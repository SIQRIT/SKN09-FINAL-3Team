{"patent_id": "10-2020-0050108", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0131698", "출원번호": "10-2020-0050108", "발명의 명칭": "발음 기관 영상을 이용한 외국어 발음 교육 방법 및 장치", "출원인": "주식회사 케이티", "발명자": "손단영"}}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서,임의의 텍스트에 대한 학습자 발음 음성과 영상을 입력받는 단계,상기 텍스트의 표준 발음 음소 리스트를 생성하는 단계, 상기 학습자 발음 음성과 영상으로부터, 상기 텍스트가 발음된 음소들을 추출하여 학습자 발음 음소 리스트를생성하는 단계, 그리고상기 표준 발음 음소 리스트를 학습된 영상 생성 모델에 입력하여 상기 표준 발음 음소 리스트를 발음하는 표준발음 시뮬레이션 영상을 생성하고, 상기 학습자 발음 음성과 영상, 상기 학습자 발음 음소 리스트를 상기 영상생성 모델에 입력하여 상기 학습자 발음 음소 리스트를 발음하는 학습자 발음 시뮬레이션 영상을 생성하고, 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상을 출력하는 단계를 포함하고,상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상은 발음 과정을 모사하는 가상의 발음기관영상인, 동작 방법."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상에 포함된 각 발음기관들의 형태 또는 움직임의 차이점을 분석하고, 상기 차이점을 포함하는 발음 가이드를 생성하는 단계 를 더 포함하는, 동작 방법."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,음소별 구강의 모양을 포함하는 제1 영상과 상기 음소별 발음기관 단면의 모양을 포함하는 제2 영상을 수집하는단계,상기 제1 영상 또는 상기 제2 영상에 해당하는 음소, 발화자의 성별과 연령대를 포함하는 발화자 정보, 상기 음소를 발음할 때의 구강의 모양과 발음기관의 모양을 태깅하여 학습 데이터를 생성하는 단계, 그리고상기 학습 데이터로 음소별 상기 구강의 모양 및 상기 발음기관 단면의 모양과의 관계를 상기 영상 생성 모델에학습시키는 단계를 더 포함하는, 동작 방법."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에서,상기 구강의 모양은 상기 구강의 크기, 입술의 모양, 아랫니와 윗니가 서로 닿는지 여부 또는 혀의 위치 중 적어도 어느 하나를 포함하는, 동작 방법."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에서,상기 음소는 음성 기호로 표시되고, 상기 음성 기호는 국제 음성 기호(International Phonetic Alphabet, IPA)를 상기 발화자 정보에 따라 변형된 것인, 동작 방법.공개특허 10-2021-0131698-2-청구항 6 제5항에서,상기 학습 데이터는,상기 발화자 정보에 따라 변형된 음성 기호와 상기 국제 음성 기호와의 차이를 수치로 나타낸 값을 더포함하는, 동작 방법."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "컴퓨팅 장치로서,메모리, 그리고상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로그램은음소별 구강의 모양과 발음기관 단면의 모양을 포함하는 학습 영상을 수집하고, 상기 학습 영상에서 발음된 음소, 상기 발음된 음소의 표준 음소, 발화자의 성별과 연령대를 포함하는 발화자 정보를 태깅하여 학습 데이터를생성하는 단계, 상기 학습 데이터로, 입력된 음소의 표준 음소가 발음되는 가상의 발음기관 시뮬레이션 영상을 출력하는 영상생성 모델을 학습시키는 단계,임의의 텍스트에 대한 학습자 발음 음성과 영상을 상기 영상 생성 모델에 입력하는 단계, 그리고상기 영상 생성 모델에서 출력된 상기 텍스트의 표준 발음 시뮬레이션 영상을 화면에 표시하는 단계를 실행하도록 기술된 명령들을 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에서,상기 학습 데이터는,상기 학습 영상에서 추출된 히스토그램 정보 또는 특징점 정보를 더 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에서,상기 학습시키는 단계는,연속하여 발음될 수 있는 음소 관계를 확률로 계산하고, 상기 표시하는 단계는,상기 확률을 바탕으로 상기 텍스트의 표준 음소 리스트와 상기 텍스트의 발음된 음소 리스트를 생성하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에서,상기 가상의 발음기관이 상기 텍스트를 발음하는 학습자 발음 시뮬레이션 영상을 생성하는 단계를 더 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0050108", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에서,상기 표준 음소 리스트와 상기 텍스트의 발음된 음소 리스트의 차이점을 분석하여 제공하는 단계공개특허 10-2021-0131698-3-를 더 포함하는, 컴퓨팅 장치."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서, 임의의 텍스트에 대한 학습자 발 음 음성과 영상을 입력받는 단계, 상기 텍스트의 표준 발음 음소 리스트를 생성하는 단계, 상기 학습자 발음 음 성과 영상으로부터, 상기 텍스트가 발음된 음소들을 추출하여 학습자 발음 음소 리스트를 생성하는 단계, 그리고 상기 표준 발음 음소 리스트를 학습된 영상 생성 모델에 입력하여 상기 표준 발음 음소 리스트를 발음하는 표준 발음 시뮬레이션 영상을 생성하고, 상기 학습자 발음 음성과 영상, 상기 학습자 발음 음소 리스트를 상기 영상 생성 모델에 입력하여 상기 학습자 발음 음소 리스트를 발음하는 학습자 발음 시뮬레이션 영상을 생성하고, 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상을 출력하는 단계를 포함하고, 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상은 발음 과정을 모사하는 가상의 발음기관 영상이다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 발음 기관 영상을 이용하여 외국어 발음을 교육하는 기술에 관한 것이다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "글로벌 시대에서 외국어를 통한 의사 소통 능력이 매우 중요해지고 있다. 특히 원활한 의사 소통을 위해, 발음 은 중요한 역할을 한다. 그러나 우리나라 외국어 교육 현실에서는 입시 위주의 교육으로 인해 발음 교육보다는, 문법이나 어휘 등에 비중을 두고 있고, 발음에 대해서도 아직도 발음을 분절하여 제공하는 위주의 발음 교육이 이루어지고 있는 실정이다. 구체적으로, 전통적인 영어 등의 외국어 발음 교육은, 학습자에게 원어민의 발음을 분절한 화면 영상을 텍스트 또는 이미지로 제공하고 발음을 따라하도록 유도하고 글로 설명하는 방식으로 제공된다. 예를 들어, 이미 정해진 규칙에 따라 발음 기관 영상을 3D로 생성하여 출력하거나, 기존에 생성된 3D 영상에 일 부 음성의 특징을 추가하여 영상을 간단히 변형하여 제공한다. 발음 기관을 3D로 시각화하여 제공하는 방법은, 음성이 발성될 때 혀의 위치나 모양, 입술의 모양 등을 사용자 가 이해하기 쉬운 그림이나 동영상으로 제공하여, 학습자에게 특정 발음에 대한 영상 정보를 제공할 수 있다. 그러나 이렇게 제공되는 정보만으로는 학습자 입장에서 스스로 어떻게 발음하는지, 어떻게 고쳐야 원어민의 발 음과 동일해지는지 알기 어렵다. 학습자가 발음할 때의 발음 기관이 3D로 어떻게 표현되는지 알 수 없기 때문이 다. 즉, 학습자의 발음 기관 영상을 화면에 출력하여, 표준 역할을 하는 원어민의 발음 기관 영상과의 차이를 직접 확인하고, 어떻게 학습자가 발음을 교정해야 하는지를 영상으로 제공하는 학습 방법이 요구된다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "해결하고자 하는 과제는 학습된 딥러닝 모델을 통해, 임의의 문장을 발음할 때의 발음 기관의 모양 및 움직임을 3차원 영상으로 나타내는 방법을 제공하는 것이다. 또한, 해결하고자 하는 과제는 동일한 텍스트에 대해 학습자의 발음 기관이 움직이는 모습과 원어민의 발음 기 관이 움직이는 모습을 동시에 제공하여, 발음 기관의 차이를 보여주고 표준 발음을 위한 발음 가이드를 제공하 는 것이다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 실시예에 따른 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치가 동작하는 방법으로서, 임의의 텍스트 에 대한 학습자 발음 음성과 영상을 입력받는 단계, 상기 텍스트의 표준 발음 음소 리스트를 생성하는 단계, 상 기 학습자 발음 음성과 영상으로부터, 상기 텍스트가 발음된 음소들을 추출하여 학습자 발음 음소 리스트를 생 성하는 단계, 그리고 상기 표준 발음 음소 리스트를 학습된 영상 생성 모델에 입력하여 상기 표준 발음 음소 리 스트를 발음하는 표준 발음 시뮬레이션 영상을 생성하고, 상기 학습자 발음 음성과 영상, 상기 학습자 발음 음 소 리스트를 상기 영상 생성 모델에 입력하여 상기 학습자 발음 음소 리스트를 발음하는 학습자 발음 시뮬레이 션 영상을 생성하고, 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상을 출력하는 단계를 포함하고, 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상은 발음 과정을 모사하는 가상 의 발음기관 영상이다. 상기 표준 발음 시뮬레이션 영상과 상기 학습자 발음 시뮬레이션 영상에 포함된 각 발음기관들의 형태 또는 움 직임의 차이점을 분석하고, 상기 차이점을 포함하는 발음 가이드를 생성하는 단계를 더 포함할 수 있다. 음소별 구강의 모양을 포함하는 제1 영상과 상기 음소별 발음기관 단면의 모양을 포함하는 제2 영상을 수집하는 단계, 상기 제1 영상 또는 상기 제2 영상에 해당하는 음소, 발화자의 성별과 연령대를 포함하는 발화자 정보,상기 음소를 발음할 때의 구강의 모양과 발음기관의 모양을 태깅하여 학습 데이터를 생성하는 단계, 그리고 상 기 학습 데이터로 음소별 상기 구강의 모양 및 상기 발음기관 단면의 모양과의 관계를 상기 영상 생성 모델에 학습시키는 단계를 더 포함할 수 있다. 상기 구강의 모양은 상기 구강의 크기, 입술의 모양, 아랫니와 윗니가 서로 닿는지 여부 또는 혀의 위치 중 적 어도 어느 하나를 포함할 수 있다. 상기 음소는 음성 기호로 표시되고, 상기 음성 기호는 국제 음성 기호(International Phonetic Alphabet, IP A)를 상기 발화자 정보에 따라 변형된 것일 수 있다. 상기 학습 데이터는, 상기 발화자 정보에 따라 변형된 음성 기호와 상기 국제 음성 기호와의 차이를 수치로 나 타낸 값을 더 포함할 수 있다. 한 실시예에 따른 컴퓨팅 장치로서, 메모리, 그리고 상기 메모리에 로드된 프로그램의 명령들(instructions)을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로그램은 음소별 구강의 모양과 발음기관 단면의 모양을 포함하는 학습 영상을 수집하고, 상기 학습 영상에서 발음된 음소, 상기 발음된 음소의 표준 음소, 발화자의 성 별과 연령대를 포함하는 발화자 정보를 태깅하여 학습 데이터를 생성하는 단계, 상기 학습 데이터로, 입력된 음 소의 표준 음소가 발음되는 가상의 발음기관 시뮬레이션 영상을 출력하는 영상 생성 모델을 학습시키는 단계, 임의의 텍스트에 대한 학습자 발음 음성과 영상을 상기 영상 생성 모델에 입력하는 단계, 그리고 상기 영상 생 성 모델에서 출력된 상기 텍스트의 표준 발음 시뮬레이션 영상을 화면에 표시하는 단계를 실행하도록 기술된 명 령들을 포함한다. 상기 학습 데이터는, 상기 학습 영상에서 추출된 히스토그램 정보 또는 특징점 정보를 더 포함할 수 있다. 상기 학습시키는 단계는, 연속하여 발음될 수 있는 음소 관계를 확률로 계산하고, 상기 표시하는 단계는, 상기 확률을 바탕으로 상기 텍스트의 표준 음소 리스트와 상기 텍스트의 발음된 음소 리스트를 생성할 수 있다. 상기 가상의 발음기관이 상기 텍스트를 발음하는 학습자 발음 시뮬레이션 영상을 생성하는 단계를 더 포함할 수 있다. 상기 표준 음소 리스트와 상기 텍스트의 발음된 음소 리스트의 차이점을 분석하여 제공하는 단계를 더 포함할 수 있다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 학습자가 발음한 텍스트에 대해 원어민의 표준 발음을 들을 수 있고, 표준 발음을 위한 발음 기관의 움직임을 시뮬레이션 영상으로도 확인할 수 있어, 발음 교육에 효과적이다. 또한, 본 발명에 따르면 원어민의 발음 기관 영상뿐만 아니라 학습자의 발음 기관 영상을 함께 제공하므로, 학 습자 스스로 교정해야 할 부분을 영상으로 확인할 수 있고 교사의 도움 없이도 발음 교정을 혼자서 진행할 수 있는바, 학습자의 편의성을 높일 수 있다."}
{"patent_id": "10-2020-0050108", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재 된 \"…부\", \"…기\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드 웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 명세서에서, 음성은 물리적인 말소리를 의미한다. 같은 문장이라도 음성은 발화하는 사람에 따라 달라질 수 있다. 국제 음성 기호(International Phonetic Alphabet, IPA)란 전 세계 언어의 다양한 소리를 포괄할 수 있도록 소 리를 문자로 표기하는 일관된 방식으로서, 국제 음성 학회에서 창안한 기호 체계를 의미한다. 예를 들어 'apple'이라는 영어 단어의 발음을 IPA로 나타내면 ['ζpl]이고, '사나이'라는 한글 단어의 발음을 IPA로 나타 내면 [sanai]이다. 음소(Phoneme)란 추상적으로 인식되는 말소리의 기본 단위를 의미하며, 같은 음소라도 발음하는 사람에 따라 다 른 음성으로 들릴 수 있다. 그러므로 같은 문장을 발음하더라도 해당 문장의 표준 발음 음소와 학습자가 발음한 문장의 음소가 서로 다를 수 있다. 따라서 본 명세서는 두 음소의 차이를 구별하고, 학습자가 음소를 발음하는 것과 표준 발음을 3D 영상으로 제공 하여, 학습자가 표준 발음을 스스로 학습할 수 있도록 도움을 주는 것을 목적으로 한다. 또한 본 명세서에서 별도의 언급이 없는 한, 발화 영상은 발화 음성을 포함하는 것으로 설명한다. 도 1은 한 실시예에 따른 발음 영상 생성 장치의 구성도이다. 도 1을 참고하면, 발음 영상 생성 장치는 학습자가 임의의 문장을 발음한 영상을 입력받는 음성 및 영상 입 력부, 입력된 문장을 구성하는 음소 리스트를 추출하는 음소 분석 모델과 발음 영상 모델을 학 습시키는 학습부, 표준 발음 영상과 발음 기관 영상을 생성하는 발음 영상 생성부, 표준 발음 영상과 학습자의 발음 영상의 차이를 분석하여 제공하는 발음 가이드 제공부, 발음 영상 DB 그리고 음소 정 보 DB를 포함한다. 설명을 위해, 음성 및 영상 입력부, 학습부, 발음 영상 생성부, 발음 가이드 제공부, 발음 영상 DB 그리고 음소 정보 DB로 명명하여 부르나, 이들은 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치이다. 여기서, 음성 및 영상 입력부, 학습부, 발음 영상 생성부, 발음 가이드 제공 부, 발음 영상 DB 그리고 음소 정보 DB는 하나의 컴퓨팅 장치에 구현되거나, 별도의 컴퓨팅 장 치에 분산 구현될 수 있다. 별도의 컴퓨팅 장치에 분산 구현된 경우, 음성 및 영상 입력부, 학습부, 발음 영상 생성부, 발음 가이드 제공부, 발음 영상 DB 그리고 음소 정보 DB는 통신 인터페 이스를 통해 서로 통신할 수 있다. 컴퓨팅 장치는 본 발명을 수행하도록 작성된 소프트웨어 프로그램을 실행할 수 있는 장치이면 충분하고, 예를 들면, 서버, 랩탑 컴퓨터 등일 수 있다. 음성 및 영상 입력부, 학습부, 발음 영상 생성부 그리고 발음 가이드 제공부 각각은 하나 의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 그리고 음소 분석 모델과 발음 영상 모델도 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 발음 영상 생성 장치는 하나의 인공지능 모델일 수 있고, 복수의 인공지능 모델로 구현될 수도 있다. 이에 따라, 상술한 구 성들에 대응하는 하나 또는 복수의 인공지능 모델은 하나 또는 복수의 컴퓨팅 장치에 의해 구현될 수 있다. 음성 및 영상 입력부는 학습자가 발음하는 임의의 문장의 음성과 영상을 입력받는다. 카메라와 마이크를 통해 입력받을 수 있으며, 학습자의 정면과 측면 영상을 획득할 수 있다. 학습부는 발음 영상 DB와 음소 정보 DB에 포함된 문장별 표준 발음 음소 정보를 이용하여 음소 분석 모델과 발음 영상 모델을 학습시킨다. 구체적으로, 음소 분석 모델은 문장이 발음된 음성 과 영상을 토대로 해당 문장을 구성하는 음소들의 리스트를 추출하는 역할을 한다. 발음 영상 모델은 음소 리스트가 입력되면 각 음소가 발음되는 조음기관의 움직임을 단면도로 생성하는 역할을 한다. 발음 영상 생성부는 학습된 발음 영상 모델을 이용하여, 입력된 학습자 영상의 문장을 발음하는 3D 영상을 생성하고, 해당 문장을 발음하는 3D 표준 발음 영상을 생성한다. 발음 가이드 제공부는 학습자의 발음 영상과 표준 발음 영상의 차이를 음소별로 비교하여, 음소별 발음 위 치, 입모양, 혀의 위치 등의 차이를 학습자에게 제공한다. 발음 영상 DB와 음소 정보 DB에 저장된 내 용을 바탕으로 발음 가이드를 생성할 수 있다. 발음 가이드는 도 12와 도 13과 같은 인터페이스를 통해 제공될 수 잇고, 학습자의 편의를 위해 음소별로 영상 또는 텍스트가 표시될 수 있다. 또한 학습자의 3D 발음 영상과 표준 발음 영상을 겹쳐서 제공할 수 있다. 발음 영상 DB는 음소를 발음할 때 발음 기관의 움직임을 신체 외부에서 촬영한 영상과 신체 내부의 단면도 영상을 포함한다. 발음 영상 DB에 저장된 영상들은 2D이거나 3D일 수 있다. 영상들은 웹 크롤링을 통해 수 집되거나, 모델 학습을 위해 촬영된 것일 수 있다. 한편, 발음 영상 DB에 저장된 영상은 원어민 또는 그에 준하는 자가 발음한 영상일 수 있다. 학습자가 발 음한 문장을 원어민이 발음하는 것과 같은 형태로 변형하여 표준 발음 영상을 생성하기 위해 사용되기 때문이다. 즉 발음 영상 DB에 포함된 영상들은 임의의 음소의 표준 발음 영상으로 간주될 수 있다. 음소 정보 DB는 각 음소를 음성 기호로 나타낸 국제 표준 IPA 발음 기호를 포함한다. 한편, 표준 IPA 발음 기호는 학습자의 발음 특성을 반영하기 위해 관리자에 의해 수정될 수 있다. 자세한 내용은 도 4를 통해 설명한 다. 도 2는 한 실시예에 따른 발음 영상 모델과 음소 분석 모델의 설명도이다. 도 2를 참고하면, 음소 분석 모델은 원어민이 특정 음소를 발음하는 영상, 발화한 문장, 발화자의 성별 및 연령대, 발음 기관의 움직임을 포함하는 단면도 영상, 해당 음소 발음시의 발음 기관의 특징 등을 포함하는 학 습 데이터를 이용하여, 임의의 문장을 구성하는 음소들을 출력하도록 학습된다. 한편 발음 영상 및 단면도 영상에 레이블링되는 정보들은 표 2의 형식일 수 있다. 학습 데이터에 대한 자세한 내용은 도 3을 통해 설명한다. 학습된 음소 분석 모델로, 학습자의 발화 영상과 발화 음성을 입력하면 음성을 구성하는 음소들이 출력될 수 있다. 또한, 입력되는 데이터의 형태는 영상, 음성에 한정되지 않고, 텍스트 형태일 수 있다. 이 경우, 학습된 음소 분석 모델에 표준 발음 영상을 생성하기 위해 텍스트 문장이 입력되면, 텍스트 문장 을 구성하는 음소들의 리스트를 출력할 수 있다. 발음 영상 모델은 위의 학습 데이터를 이용하여, 인체 단면도를 모방한 3D 발음 영상을 출력하도록 학습된 다. 학습된 발음 영상 모델에 학습자의 발화 영상 및 발화 음성을 입력하면, 학습자의 발음 기관을 3D로 시뮬 레이션하는 영상이 생성될 수 있다. 3D 발음 영상은 학습자가 발화한 문장을 발음하는 과정과 발음 기관들의 움 직임을 영상으로 출력할 수 있다. 한편 발음 영상 모델은, 영상을 생성하기 위해 음소 분석 모델에서 출력된 음소 리스트를 추가로 이 용할 수 있다. 이하에서는 각 모델의 학습 과정에 대한 자세히 설명한다. 도 3은 한 실시예에 따른 발음 영상 생성 장치가 음소 분석 모델을 학습시키는 과정의 흐름도이고, 도 4는 한 실시예에 따른 모음의 발음 위치를 나타낸 설명도이고, 도 5는 한 실시예에 따른 발음 기관의 특징을 나타낸 예 시도이고, 도 6은 한 실시예에 따른 학습 데이터를 얻는 방법의 예시도이고, 도 7은 한 실시예에 따른 음소 분 석 모델이 학습하는 음소 리스트의 예시도이다.도 3을 참고하면, 발음 영상 생성 장치는 외국인 또는 원어민의 발음 영상 및 발음 기관 단면도 영상을 수 집한다(S110). 한 예로서, 발음 영상 DB에 수집된 영상들을 이용할 수 있다. 한편, 발음 영상 DB에 모든 음소의 발음 영상과 발음 기관 단면도 영상이 없을 수 있다. 이 경우, 이미 수 집된 발음 영상과 발음 기관 단면도 영상을 기반으로 다른 음소의 발음 영상을 생성하여 학습 데이터로 사용할 수 있다. 한 예로서, 생성적 적대 신경망(Generative Adversarial Networks, GAN)을 이용할 수 있다. 발음 영상 생성 장치는 음소를 음성 기호로 표시한 음소 정보를 수집하고, 한국인 발음 특성에 따라 이를 수정한다(S120). 음소 정보란 국제 표준 IPA 발음 기호를 의미하고, 이를 그대로 사용하거나 또는 학습자의 발 음 특성에 따라 변형하여 사용할 수도 있다. 음소 정보를 변형하는 이유는, 한국인은 한국어를 주로 발음하며, 한국어에 주로 사용되는 발음 기관의 종류나 위치가 외국어와는 차이가 있으므로 이를 반영하기 위함이다. 예를 들어 도 4의 (a)는 영어의 모음을 발음할 때 사용하는 IPA 발음 기호를 발음 위치에 따라 나타낸 모음 사 각도이다. 한편, 발음 영상 생성 장치는, 도 4의 (a)와 같은 표준 IPA 기호를 그대로 사용할 수도 있으나, 각 나라의 발화자의 발음 특성에 따라 표준 IPA를 변형하여 사용할 수 있다. 한 예로서, 도 4의 (b)는 한국인이 영어의 모 음을 발음할 때의 특성을 고려하여 수정된 IPA 모음 기호를 나타낸 것이다. 이때 도 4의 (b)는 한국인의 발음 기관의 특징, 예를 들어 한국인의 구강 크기, 성대 크기, 입술 모양 등의 발 음 기관의 특징에 따라 수정된 것일 수 있고, 수정된 IPA 발음 기호의 위치는 해당 모음을 발음할 때의 구강 내 혀의 위치에 따라 변경될 수 있다. 수정된 IPA 기호는 표 1과 같을 수 있다. 표 1 일련번호 표준 IPA 수정된 IPA 1 α αː' 2 æ æ 3 4 5 6 표 1에서, 발화자가 발음한 음소를 표시할 때, 가 아니라 αː'로 바뀐 기호를 사용할 수 있다. 이는 발화자가 한국인인 경우 한국인의 발음 특성상 α로 발음되어야 하는 영어를, α로 발음하지 못하고 αː'로 발음하는 것 을 의미한다. 따라서 α가 포함된 텍스트를 발음할 때 음소 리스트는 α가 아닌 αː'를 포함할 것이다. 는 한국인의 발음 기관의 특성상, 표준 발음에 맞게 제대로 발음할 수 있는 음소로서, 를 그대로 사용할 수 있다. 한편 표 1에서는 발음된 음소가 모음인 경우를 나타내었으며, 자음을 발음한 경우에는 발화자가 발음한 음소의 기호와 함께 표준 IPA 기호에 비해 유성음 또는 무성음의 경향성, 파열음, 마찰음, 파찰음의 강도의 정도 등을 추가로 포함시킬 수 있다. 또한, 발화자의 발음 특성에 따라 임의의 음소가 여러 발음으로 소리날 수 있으므로, 하나의 음소에 대해 여러 발음이 존재할 수 있고 이를 구분하여 표현하기 위해 하나의 음소가 복수개의 수정된 IPA를 가질 수 있다. 예를 들어 aI라는 발음이 n개의 서로 다른 발음으로 소리날 수 있고, 이를 표현하기 위해 aI', aI2'부터 aIn'까지 n개 의 수정된 IPA 모음 기호를 사용할 수 있다. 발음 영상 생성 장치는 수집한 발음 영상 및 발음 기관 단면도 영상에 수정한 음소 정보, 발화자 정보, 발 음 기관 특징을 레이블링하여 학습 데이터를 생성한다(S130). 발음 영상에 레이블링하는 정보는 음소 정보뿐만 아니라 발화자 정보와 발음 기관 특징을 포함한다. 즉 해당 발 음 영상의 주인공인 발화자의 성별, 연령대 및 발화자가 해당 음소를 발음할 때의 발음 기관의 특징 등으로, 표2와 같은 값들이 레이블링될 수 있다. 표 2 구분 내용 타입 성별 여성/남성 F/M 연령 아동/10대/20~30대/40~60대/70대 이상 1~5 발음기관 특징구강/성대 크기 작다/보통/크다 1~3 입술/이 모양옆으로 벌어진 입모양/동그란 입모양/ 크게 벌어진 입술/작게 벌어진 입술/닫 힌 입술/이가 닿음1~7 혀 위치 전설/중설/후설/혀안닿음/윗니뒤/입천 장 안쪽/위아랫니사이/아랫니뒤잇몸1~8 표 2에서, 성별은 F와 M으로 레이블링되고, 연령대는 아동부터 70대 이상까지 5개의 타입으로 레이블링되고, 발 음 기관 특징은 각 특성에 따라 레이블링될 수 있다. 구체적으로, 구강과 성대의 크기는 3가지 범주로 구분될 수 있고, 입술과 이 모양에 따른 발음 기관의 특징은 도 5의 (a)와 같이 구분될 수 있다. 예를 들어, 도 5의 (a)에서 입술과 이 모양은 앞에서부터 순서대로 a, e, i, o, u, f 또는 v를 발음할 때의 모양일 수 있다. 또한 음소를 발음하는 혀의 위치는 도 5의 (b)와 같이 구분될 수 있다. 예를 들어 도 5의 (b)는 앞에서부터 순 서대로 전설, 중설, 후설, ‘r’을 발음할 때, ‘l’을 발음할 때, ‘ㄹ’을 발음할 때의 혀의 위치일 수 있다. 한편 도 4를 통해 설명한 바와 같이, 발음 영상 생성 장치는 음소 정보 DB에 저장된 표준 IPA를 그대 로 사용하거나 이를 수정하여 사용할 수 있다. 레이블링 할 정보가 준비되었으므로, 음소별 영상에 해당 정보를 레이블링해야 한다. 그러나 일반적으로 발음 영상과 발음 기관 단면도 영상은 하나의 문장을 발화하는 경우가 많으므로, 우선 발음 영상 생성 장치는 도 6과 같이 영상을 음소 단위로 분할해야 한다. 도 6을 참고하면, 발음된 문장이 'I like an apple.'이고, 'apple'의 음소가 'aepl'이다. 이 문장 전체를 발음 하는 영상을 'ae', 'p', 'l'이라는 각각의 음소들을 발음하는 영상들로 분할해야 한다. 문장 단위의 음성을 음소로 나누기 위해, 관리자가 수동으로 분할하거나 발음 영상 생성 장치가 분할을 위 한 별도의 모델을 사용할 수 있다. 한 예로서 해당 모델은 각 음소별 영상에서 음향 특징을 추출한 것을 학습 데이터로 하여, 음소별 음향 특징을 학습하고, 임의의 발화 영상을 입력하면, 해당 문장에서 발음된 음소를 출 력하는 모델일 수 있다. 이때 사용되는 음향 특징은 MFCC(Mel Frequency Cepstral Coefficient)일 수 있으며, 어느 하나에 한정되지 않는다. 이제 발음 영상 생성 장치는 음소 단위로 분할된 영상에, S120 단계에서 수정한 음소 정보, 발화자 정보, 발음 기관 특징을 레이블링한다. 레이블링할 때, 해당 영상 파일의 재생 시간, 발화한 내용의 텍스트를 더 포함 할 수 있다. 레이블링된 정보는 JSON(JavaScript Object Notation) 형태로 표현될 수 있다. 예를 들어 특정 영상의 특정 구간에서 ae45-1이라는 음소가 발음된 경우, 해당 구간의 영상 정보(image), 발음 된 전체 문장의 정보(text), 발화자 정보(talker), 발음 기관 정보(pronunciation organ)를 포함하여 표 3과 같은 데이터 구조로 음소 정보를 생성할 수 있다. 표 3 “æ45-1\": { “image\": { “Frame length\": \"2500\" “Frame data\": \"1500\", \"1700\" “Video resolution\": 1024\", \"768\" }, “text\": { “script\": \"I like an apple\" “phone\": \"æ\" }, \"voice\" : { “length\": \"2500\" “ipa phone\": \"æ45-1\" “ipa phone difference: \"-0.01\", \"+0.02\" }, \"Talker\": { “Gender\": \"female\" “Age\": \"40\" }, “Pronunciation organ\": { “timbre\": \"7\" “Jaw length\": \"10\" “Neck length\": \"12\" “Neck thickness\": \"15\" “Oral cavity\": \"25\" “vocal tract length\": \"15\" “vocal tract diameter\": \"2\" “Lip size\": \"5\" “Lip shape\": \"type 1\" “Tongue position”(Type, Type 변화): “type 1”, “1, 1” “Tongue movements”(높이): “1” “sound source strength\": \"5\" } } 표 3에서, ipa phone differenc는 도 4를 참고하여, 발음된 모음이 표준 IPA의 발음 위치와 얼마나 가까운지를 나타내는 것이다. 예를 들어 표 3에서 발화자가 발음한 음소를 æ45-1로 나타낼 수 있으며, 이는 표준 IPA 기호인 æ의 위치에 비 해 모음 사각도에서 x축 방향으로 0.01만큼 왼쪽에 위치하고, y축 방향으로 0.02만큼 높게 위치함을 알 수 있다. 한편 이러한 수치는 도 4의 모음 사각도에서, 발화자가 발음한 음소의 위치와 표준 IPA 기호의 위치의 차 이는 한국인의 구강 내 특성에 따라 결정된 것일 수 있다. 한편, 발음 영상 생성 장치는 위에서 설명한 음소 정보, 발화자 정보, 발음 기관 정보 이외에 S110 단계에 서 수집한 영상으로부터 추출한 영상학적 특징과 음성 특징들을 학습 데이터로 이용할 수 있다. 예를 들어, 영상에서 Histogram of Oriented Gradients(HOG) 또는 Local Binary Pattern(LBP) 등의 특징을 추 출하여 이용할 수 있다. 추출한 정보들은 표 3의 데이터 구조에 추가되어 레이블링 될 수 있다. 발음 영상 생성 장치는 학습 데이터를 이용하여 입력된 문장의 음소를 추출하기 위한 음소 분석 모델 을 학습시킨다(S140). 한 예로서 음소 분석 모델은 도 7과 같이 음소가 순차적으로 연결된 네트워크 형태 를 출력하도록 학습될 수 있다.한 예로서, 도 7에서 학습자가 'I like an apple.'이라는 문장을 발음한 경우, 음소 분석 모델은 문장을 구성하는 음소들이 무엇인지 확률로서 결정하기 위해 네트워크를 생성할 수 있다. 예를 들어, 가장 먼저 발음된 음소가 aIn'인 경우, 그 이후에 발음될 수 있는 음소는 ln'또는 ln+1'일 수 있다. ln'이라는 음소 다음에는 aIn+3'이 발음될 수 있고, ln+1'이라는 음소 다음에는 aIn+1'이라는 음소가 발음될 수 있 다. 음소 분석 모델은 데이터베이스(미도시)에 저장된 미리 수집된 문장 코퍼스(Corpus)를 학습 데이터로 사용 할 수 있고, n-gram 언어 모델로 구현될 수 있다. 구체적으로, SRI Language Modeling Toolkit(SRLIM)을 이용 할 수 있다. 발음 영상 생성 장치는 학습 데이터를 이용하여 3D 영상 생성을 위한 발음 영상 모델을 학습시킨다 (S150). 발음 영상 모델은 발음 기관을 3D 동영상으로 출력하는 역할을 하며, 입술, 이, 목젖, 입천장 등의 조음기 관의 구조를 표현한다. 구강, 인두, 입술, 혀의 크기와 모양, 위치는 음소를 발음할 때마다 변화할 수 있다. 발 음 기관의 3D 형태는 실제 사람의 발음 기관의 비율과 유사한 형태로 생성될 수 있으며, 발음과 무관환 눈, 코, 귀 등의 외형은 임의로 설정되거나 변경될 수 있다. 학습 데이터는 2D 또는 3D의 발음 영상과 2D 또는 3D의 발음 기관 단면도 영상에 레이블링 된 표 3의 정보들을 포함한다. 발음 영상 모델은 머신러닝으로 구현될 수 있으며, 복수의 머신러닝이 결합된 형태일 수 있다. 예를 들어 은닉 마르코프 모델(Hidden Markov Models, HMMs), 심층 신경망 (Deep Neural Network, DNN), 합성곱 신경망 (Convolution Neural Network, CNN) 등이 사용될 수 있다. 한편 도 1에서 설명한 바와 같이, S140 단계에서 학습되는 발음 영상 모델과 S150 단계에서 학습되는 음소 분석 모델은 하나의 인공지능 모델일 수 있으며, 이 경우 모델의 학습 단계는 하나의 과정으로 진행될 수 있다. 또한, 각 모델이 별개의 학습 과정을 진행한 뒤, 임의의 인공지능 모델을 통해 두 모델이 결합할 수도 있 으며, 모델의 형태는 어느 하나로 제한되지 않는다. 이하에서는 학습된 음소 분석 모델과 발음 영상 모델을 이용하여 학습자가 발화한 문장에 대한 발음 기관 단면도로 나타나는 3D 영상과 해당 문장의 표준 발음 영상을 출력하는 과정에 대해 설명한다. 도 8은 한 실시예에 따른 발음 영상 생성 장치가 동작하는 방법의 흐름도이고, 도 9는 한 실시예에 따른 음소 분석 모델이 예측한 음소 리스트의 예시도이고, 도 10은 한 실시예에 따른 발음 영상 모델이 시뮬레이션 영상을 생성하는 방법의 설명도이고, 도 11과 12는 한 실시예에 따른 발음 영상 생성 장치가 출력하는 결과 화면의 예 시도이다. 도 8을 참고하면, 발음 영상 생성 장치의 음성 및 영상 입력부는 학습자의 발화 영상을 입력받는다 (S210). 학습자가 임의의 문장을 발화하면, 발음 영상 생성 장치는 카메라와 마이크를 통해 학습자가 발화 할 때의 발음 기관의 모양과 음성을 수집한다. 발화 영상은 학습자를 정면에서 촬영한 영상과 측면에서 촬영한 영상 중 적어도 하나를 포함할 수 있다. 발음 영상 생성 장치는 학습자 발화를 음소 분석 모델에 입력하여 학습자 발화를 구성하는 음소들의 리스트를 생성한다(S220). 이때 Viterbi 알고리즘이 사용될 수 있다. Viterbi 알고리즘은 HMM 등에서 관측된 사건들의 순서를 야기한 가장 가능성 높은 은닉 상태들의 순서를 찾기 위한 알고리즘이다. 음성 인식 기술(Speech to Text)에서는, 음향 신호를 관측된 사건들의 순서라고 하면, 문자 열은 이러한 음향 신호를 야기한 숨겨진 원인(Hidden Cause)으로 간주된다. 즉, Viterbi 알고리즘은 주어진 음 향 신호에 대한 가장 가능성 높은 문자열을 찾아내는데 사용된다. 구체적으로, 발음 영상 생성 장치는 발음된 문장을 구성할 수 있는 음소들을 추출하고, 각 음소가 해당할 확률을 음소별로 계산하고, 현재 음소에 대해 다음에 나올 음소의 확률을 음소별로 계산한다. 이후 발음 영상 생성 장치는 가장 큰 확률을 갖는 네트워크를, 해당 문장을 구성하는 음소 리스트로 결정할 수 있다. 도 9를 참고하여 한 예로서, 학습자가 ‘I like an apple.'이라는 문장을 발화한 경우를 설명한다. 발음 영상 생성 장치는 공지된 음성 인식 기술(Speech to Text)을 이용하여, 학습자가 발화한 문장이 ‘I like anapple.'이라고 결정한다. 이후, ‘I like an apple.'이라는 문장을 구성할 수 있는 음소들을 네트워크 형태로 구성할 수 있다. 학습된 음소 분석 모델은 학습자가 발음한 문장이 aIn+2', ln+3', aIn+3', kn+2', , nn+3', æn+4', pn+4', ln+3'이라는 음소들이 순차적으로 발음된 것으로 분석할 수 있다. 한편, 발음 영상 생성 장치는 추가적으로 학습자가 각 음소를 발음한 길이를 추출할 수 있다. 발음 영상 생 성 장치는 음소별 발음 길이를 반영하여 발음 영상을 생성할 수 있다. 발음 영상 생성 장치는 학습자 발화의 음성 인식 결과에 따른 표준 발음 음소 리스트를 생성한다(S230). 학 습된 음소 분석 모델이 표준 발음 음소 리스트를 생성할 수 있다. 발음 영상 생성 장치는 발음 영상 모델을 이용하여 S220 단계에서 생성된 음소 리스트에 따른 학습자 발화에 대한 발음 기관 영상을 생성하여 출력한다(S240). 발음 영상 생성 장치는 발음 영상 모델을 이용하여 S230 단계에서 생성된 표준 발음 음소 리스트에 대 한 표준 발음 영상을 생성하여 출력한다(S250). 도 10을 참고하면, 발음 영상 모델은 해당 음소의 표준 발음 영상을 바탕으로 표준 발음으로 발화하기 위 한 입 모양, 혀의 위치 등의 발음 기관의 형태를 결정할 수 있다. 결정된 발음 기관의 특성과 발음 기관의 단면 도 영상을 결합하여 3D 발음 영상을 표준 발음 영상으로 생성할 수 있다. 한 예로서 발음 영상 생성 장치는 도 11 또는 도 12와 같은 화면을 출력할 수 있으며, 학습자는 출력된 화 면에서 제공되는 버튼들을 눌러 표준 발음 발음과 학습자의 발음의 차이를 구체적으로 확인할 수 있다. 발음 영상 생성 장치는 학습자 영상과 표준 발음 영상의 차이점을 분석하여 발음 가이드를 제공한다(S260). 학습자는 발음 가이드를 바탕으로, 표준 발음 영상과 자신의 발음 영상을 시청하며 발음을 스스로 교정할 수 있 다. 제공되는 발화 가이드는 미리 저장된 DB(미도시)로부터 얻거나 발음 영상 생성 장치가 표준 발음과 학습자 발음의 차이를 직접 계산하여 작성할 수 있다. DB에 미리 저장된 가이드는 음소의 차이를 교정하기 위해 학습자 가 행동해야 할 지침을 포함하며, 표 4와 같을 수 있다. 표 4 표준 발음 음소학습자 발음 음소발음 가이드 aIaIn+2'학습자는 혀의 위치를 저중설에서 고전설로 발음함 입을 벌리고 혀가 저후설에서 근고전설로 움직이도록 발음해야 함 Iln+3'입을 살짝 벌리고 혀가 윗니 바로 뒤쪽에 닿도록 해야 함 aIaIn+3'입모양을 확장해야 함 kkn+2'\"으\"발음을 빼고 혀를 고전설로 놓고 터트리는 소리로 짧게 발음해야 함 학습자는 혀의 위치를 중저후설로 발음함 혀의 위치를 중중설로 발음해야 함 nnn+3'학습자는 혀의 앞부분을 윗잇몸에 대고 발음함 혀의 앞부분을 윗잇몸에 세게 누르면서, 코로 바람을 뿜어야 함 ææn+4'학습자는 혀의 뒷 부분이 윗쪽으로 올라가고, 입이 크게 벌어짐 혀의 중간부분이 위로 올라가도록 발음해야 함 ppn+4'학습자는 입을 닫았다 살짝 열며 발음함 입을 닫았다 옆으로 살짝 벌리고 터트리듯 짧게 발음해야 함 lln+3'학습자는 혀가 입천장에 닿음 혀가 이 뒤에 닿도록 해야 함 발음의 차이점을 계산하는 한 예로서, 표준 발음 음소와 학습자가 발음한 음소의 유사도를 계산할 수 있다. 유 사도는 도 4와 같이 모음 사각도에서 음소의 위치를 좌표로 나타내어 계산될 수 있다.한편, 발음의 길이에 차이가 있는 경우 길이에 대한 내용이 발음 가이드에 추가될 수 있다. 도 13은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다. 도 13을 참고하면, 음성 및 영상 입력부, 학습부, 발음 영상 생성부, 발음 가이드 제공부, 발음 영상 DB 그리고 음소 정보 DB는 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치에서, 본 발명의 동작을 실행하도록 기술된 명령들(instructions)이 포함된 프로그램을 실행한다. 컴퓨팅 장치의 하드웨어는 적어도 하나의 프로세서, 메모리, 스토리지, 통신 인터페이스 를 포함할 수 있고, 버스를 통해 연결될 수 있다. 이외에도 입력 장치 및 출력 장치 등의 하드웨어가 포함 될 수 있다. 컴퓨팅 장치는 프로그램을 구동할 수 있는 운영 체제를 비롯한 각종 소프트웨어가 탑재될 수 있다. 프로세서는 컴퓨팅 장치의 동작을 제어하는 장치로서, 프로그램에 포함된 명령들을 처리하는 다양한 형태의 프로세서일 수 있고, 예를 들면, CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 등 일 수 있다. 메모리는 본 발명의 동작을 실행하도록 기술된 명령들이 프로세서에 의해 처리되도록 해당 프로그램을 로드한다. 메모리는 예를 들면, ROM(read only memory), RAM(random access memory) 등 일 수 있다. 스토리지는 본 발명의 동작을 실행하는데 요구되는 각종 데이터, 프로그램 등을 저장한다. 통신 인터페이스는 유/무선 통신 모듈일 수 있다. 본 발명에 따르면 학습자가 발음한 텍스트에 대해 원어민의 표준 발음을 들을 수 있고, 표준 발음을 위한 발음 기관의 움직임을 시뮬레이션 영상으로도 확인할 수 있어, 발음 교육에 효과적이다. 또한, 본 발명에 따르면 원어민의 발음 기관 영상뿐만 아니라 학습자의 발음 기관 영상을 함께 제공하므로, 학 습자 스스로 교정해야 할 부분을 영상으로 확인할 수 있고 교사의 도움 없이도 발음 교정을 혼자서 진행할 수 있는바, 학습자의 편의성을 높일 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13"}
{"patent_id": "10-2020-0050108", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 발음 영상 생성 장치의 구성도이다. 도 2는 한 실시예에 따른 발음 영상 모델과 음소 분석 모델의 설명도이다. 도 3은 한 실시예에 따른 발음 영상 생성 장치가 음소 분석 모델을 학습시키는 과정의 흐름도이다. 도 4는 한 실시예에 따른 모음의 발음 위치를 나타낸 설명도이다. 도 5는 한 실시예에 따른 발음 기관의 특징을 나타낸 예시도이다. 도 6은 한 실시예에 따른 학습 데이터를 얻는 방법의 예시도이다. 도 7은 한 실시예에 따른 음소 분석 모델이 학습하는 음소 리스트의 예시도이다. 도 8은 한 실시예에 따른 발음 영상 생성 장치가 동작하는 방법의 흐름도이다. 도 9는 한 실시예에 따른 음소 분석 모델이 예측한 음소 리스트의 예시도이다. 도 10은 한 실시예에 따른 발음 영상 모델이 시뮬레이션 영상을 생성하는 방법의 설명도이다. 도 11과 도 12는 한 실시예에 따른 발음 영상 생성 장치가 출력하는 결과 화면의 예시도이다. 도 13은 한 실시예에 따른 컴퓨팅 장치의 하드웨어 구성도이다."}
