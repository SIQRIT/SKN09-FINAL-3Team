{"patent_id": "10-2020-0188203", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0096073", "출원번호": "10-2020-0188203", "발명의 명칭": "인공신경망을 이용한 사용자 맞춤형 광고 출력 장치", "출원인": "(주)라이언로켓", "발명자": "정승환"}}
{"patent_id": "10-2020-0188203", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공신경망을 이용한 사용자 맞춤형 광고 출력 장치에 있어서,합성 타겟 얼굴을 포함하는 광고영상으로서 입모양 합성의 대상이 되는 원본광고영상 데이터를 인코딩하여 원본광고영상 임베딩 벡터를 출력하는 원본광고영상 인코더;입모양 합성의 기초가 되는 피광고자 이름 음성 데이터를 인코딩하여 음성 임베딩 벡터를 출력하는 음성인코더;상기 원본광고영상 임베딩 벡터 및 상기 음성 임베딩 벡터를 입력 데이터로 하고, 상기 합성 타겟 얼굴에 상기피광고자 이름 음성 데이터에 대응되는 입모양이 합성된 합성영상 데이터를 출력하는 합성영상 디코더; 상기 합성영상 데이터를 입력 데이터로 하고, 상기 합성영상 데이터가 상기 합성영상 디코더에서 생성된 것인지여부를 구분하는 합성영상 판별 벡터를 출력 데이터로 하는 인공신경망 모듈이고, 상기 합성영상 판별 벡터를기초로 구성되는 합성영상 손실을 출력하는 합성영상 판별기; 및상기 피광고자 이름 음성 데이터를 입력받고 음성 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 음성 임베딩 모듈; 및 상기 합성영상 데이터를 입력받고 합성영상 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 합성영상 임베딩 모듈;을 포함하고, 상기 음성 싱크 벡터와 상기 합성영상 싱크 벡터의 차이로 구성되는 싱크 손실을 출력하는 싱크 판별기;를 포함하고, 상기 싱크 판별기의 상기 음성 임베딩 모듈 및 상기 합성영상 임베딩 모듈은, 상기 피광고자 이름과 상기 합성영상 데이터의 입모양 사이의 싱크로율이 높을수록 상기 싱크 손실이 낮게 출력되도록 기학습되고, 상기 원본광고영상 인코더, 상기 음성 인코더 및 상기 합성영상 디코더의 학습 세션에서는, 상기 합성영상 데이터와 상기 원본광고영상 데이터의 차이로 구성되는 재구성 손실, 상기 합성영상 손실 및 상기 싱크 손실의 합이저감되는 방향으로 업데이트되도록 구성되는 것을 특징으로 하는, 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치에 관한 것이다. 이를 위하여, 합성 타겟 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코딩하여 원본영상 임베딩 벡터를 출력하 는 원본영상 인코더; 입모양 합성의 기초가 되는 음성 데이터를 인코딩하여 음성 임베딩 벡터를 출력하는 음성 인코더; 원본영상 임베딩 벡터 및 음성 임베딩 벡터를 입력 데이터로 하고, 합성 타겟 얼굴에 음성 데이터에 대 응되는 입모양이 합성된 합성영상 데이터를 출력하는 합성영상 디코더;가 제공될 수 있다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치에 관한 것이다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공신경망 알고리즘 및 컴퓨팅 파워의 향상으로, 딥페이크(Deepfake)와 같이 인공지능을 활용하여 제작한 콘텐츠의 생산 및 수요가 매우 빠르게 증가하고 있다. Deepfake는 Deep Learning을 이용해 원본 이미지나 동영 상 위에 다른 영상을 중첩하거나 결합하여 원본과는 다른 가공 콘텐츠(합성 영상)를 생성하는 기술이다. Deeptrace, The state of Deepfakes, Landscape, Threats and Impact에 따르면, 2018년 12월 7,964개였 던 Deepfake 콘텐츠 수가 2019년 9월에는 14,678개로 증가하였으며, 20개의 Deepfake 제작 커뮤니티가 존재하고 이용자 수는 95,791명에 달하는 수준이라고 알려져있다. 또한, Youtube, Twitter, Reddit 등 내에서도 Deepfake를 이용한 영상이 상당히 많이 생성되고 있는 실정이고, TikTok, Snap 등의 소셜 콘텐츠 플랫폼에서는 Deepfake를 이용한 서비스 개발이 진행되어 서비스되고 있는 상황이다. 또한, 영화 등 영상에 사용자 얼굴 합성 해주는 애플리케이션 'ZAO'는 2019년 9월 출시와 동시에 중국 iOS 앱스토어 다운로드 1위를 차지한 바 있다. 중 국 커뮤니케이션 서비스인 모모(陌陌)가 투자한 것으로 알려진 'ZAO'는 사용자의 정면 얼굴 사진 한 장 만 있으 면 영화TV작품이나 동영상 속의 인물에 자신의 얼굴을 붙여넣어 자신이 주인공인 동영상을 만들 수 있도록 만들어진 애플리케이션이다. 위와 같은 인공지능을 활용한 합성 영상의 제작에는 Generative Adversarial Network(이하, GAN)이 가장 많이 사용되고 있다. GAN은 생성모듈과 식별모듈이 공존하며 상호 적대적으로 손실을 학습하는 모델이다. 이러한 GAN 관련 연구는 arXiv에서 2014년 3건에서 출발하여 2019년 1,525건, 2020년 현재까지 2,037건 publication 될 정 도로 급속히 증가되고 있는 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 10-2020-0094207, 얼굴 인식에 기초한 개인화된 이모티콘들 및 립 싱킹 비 디오들을 생성하기 위한 방법들 및 시스템들, 페이스북, 인크. (특허문헌 0002) 대한민국 공개특허 10-2019-0070065, 텍스트 기반 적응적 가창 립싱크 애니메이션 생성 장치 및 방법, 중앙대학교 산학협력단"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "하지만, 기존의 합성 영상 생성을 위한 인공지능 알고리즘은 기존 영상에 새로운 얼굴을 합성하는 일반화된 모 델이 대부분이었고, 음성 입력을 통하여 입모양만을 합성하는 특수한 상황에 대한 특수 모델은 찾기 어려운 실 정이다. 이에 의해, 기존의 일반화된 합성 영상 생성 인공지능 모델을 활용하여 음성 입력을 통한 입모양 합성 을 수행하게 되면 음성과 영상의 싱크가 맞지 않거나, 부정확한 입모양이 생성되는 문제가 있었다. 특히, 기존 의 알고리즘을 통한 입모양 합성의 경우, 음성이 존재하는 구간과 음성이 존재하지 않는 구간의 전이 구간에서 현실성이 낮은 입모양이 합성되는 문제가 있었다. 따라서, 본 발명의 목적은 음성 입력을 통한 입모양의 합성의 정밀도 및 현실성이 향상된 인공신경망을 이용한 입모양 합성 장치 및 방법을 제공하는 데에 있다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이하 본 발명의 목적을 달성하기 위한 구체적 수단에 대하여 설명한다. 본 발명의 목적은, 합성 타겟 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코 딩하여 원본영상 임베딩 벡터를 출력하는 원본영상 인코더; 입모양 합성의 기초가 되는 음성 데이터를 인코딩하 여 음성 임베딩 벡터를 출력하는 음성 인코더; 상기 원본영상 임베딩 벡터 및 상기 음성 임베딩 벡터를 입력 데 이터로 하고, 상기 합성 타겟 얼굴에 상기 음성 데이터에 대응되는 입모양이 합성된 합성영상 데이터를 출력하 는 합성영상 디코더; 상기 합성영상 데이터를 입력 데이터로 하고, 상기 합성영상 데이터가 상기 합성영상 디코 더에서 생성된 것인지 여부를 구분하는 합성영상 판별 벡터를 출력 데이터로 하는 인공신경망 모듈이고, 상기 합성영상 판별 벡터를 기초로 구성되는 합성영상 손실을 출력하는 합성영상 판별기; 및 상기 음성 데이터를 입 력받고 음성 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 음성 임베딩 모듈; 및 상기 합성영상 데이터를 입력받고 합성영상 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 합성영상 임베딩 모듈;을 포함하고, 상기 음성 싱크 벡터와 상기 합성영상 싱크 벡터의 차이로 구성되는 싱크 손실을 출력하는 싱크 판별기;를 포함하고, 상기 싱크 판별기의 상기 음성 임베딩 모듈 및 상기 합성영상 임베딩 모듈은, 상기 음성 데이터와 상기 합성영 상 데이터의 입모양 사이의 싱크로율이 높을수록 상기 싱크 손실이 낮게 출력되도록 기학습되고, 상기 원본영상 인코더, 상기 음성 인코더 및 상기 합성영상 디코더의 학습 세션에서는, 상기 합성영상 데이터와 상기 원본영상 데이터의 차이로 구성되는 재구성 손실, 상기 합성영상 손실 및 상기 싱크 손실의 합이 저감되는 방향으로 업데 이트되도록 구성되는 것을 특징으로 하는, 인공신경망을 이용한 입모양 합성 장치를 제공하여 달성될 수 있다. 또한, 상기 원본영상 인코더는, 출력 데이터로서 상기 원본영상 임베딩 벡터와 함께 복수개의 감정 상태 클래스 (class)를 구분하는 감정 벡터를 출력하도록 구성되고, 상기 합성영상 판별기는, 출력 데이터로서 상기 합성영 상 판별 벡터와 함께 상기 감정 벡터를 출력하도록 구성되며, 상기 원본영상 인코더, 상기 합성영상 디코더 및 상기 합성영상 판별기의 학습 세션에서는, 상기 재구성 손실, 상기 합성영상 손실, 상기 싱크 손실 및 상기 감 정 벡터와 레이블링(Labeling)된 감정 상태 클래스와의 차이로 구성되는 감정 손실의 합이 저감되는 방향으로 업데이트되도록 구성되는 것을 특징으로 할 수 있다. 또한, 상기 합성영상 데이터를 입력 데이터로 하고, 상기 합성영상 데이터에 현실적인 얼굴이 포함되어 있는 경 우인 현실적 얼굴 클래스와 상기 합성영상 데이터에 비현실적인 얼굴이 포함되어 있는 경우인 비현실적 얼굴 클 래스를 분류하는 얼굴 판별 벡터를 출력하도록 기학습된 인공신경망 모듈이고, 상기 얼굴 판별 벡터와 레이블링 (Labeling)된 클래스와의 차이로 구성되는 얼굴 손실을 출력하는 얼굴 판별기;를 더 포함하고, 상기 원본영상 인코더, 상기 합성영상 디코더 및 상기 합성영상 판별기의 학습 세션에서는, 상기 재구성 손실, 상기 합성영상 손실, 상기 싱크 손실 및 상기 얼굴 손실의 합이 저감되는 방향으로 업데이트되도록 구성되는 것을 특징으로 할 수 있다. 또한, 상기 음성 데이터를 입력받고 음성 입크기 임베딩 벡터를 출력하는 기학습된 인공신경망 모듈인 음성 입 크기 임베딩 모듈; 및 상기 합성영상 데이터를 입력받고 영상 입크기 임베딩 벡터를 출력하는 기학습된 인공신 경망 모듈인 영상 입크기 임베딩 모듈;을 포함하고, 상기 음성 입크기 임베딩 벡터와 상기 영상 입크기 임베딩 벡터의 차이로 구성되는 입크기 손실을 출력하는 입크기 판별기;를 더 포함하고, 상기 입크기 판별기의 상기 음 성 입크기 임베딩 모듈 및 상기 영상 입크기 임베딩 모듈은, 상기 음성 데이터의 입크기 특징과 상기 합성영상 데이터의 입크기 특징의 거리가 가까울수록 상기 입크기 손실이 낮게 출력되도록 기학습되고, 상기 원본영상 인 코더, 상기 합성영상 디코더 및 상기 합성영상 판별기의 학습 세션에서는, 상기 재구성 손실, 상기 합성영상 손 실, 상기 싱크 손실 및 상기 입크기 손실의 합이 저감되는 방향으로 업데이트되도록 구성되는 것을 특징으로 할 수 있다. 또한, 상기 싱크 판별기에 구성되는 컨볼루션 필터에 랜덤하게 널링(nulling)을 적용하여 상기 음성 싱크 벡터 또는 상기 합성영상 싱크 벡터를 복수개 출력하고, 출력된 복수개의 상기 음성 싱크 벡터 또는 복수개의 상기 합성영상 싱크 벡터를 통합한 통합 벡터를 기초로 상기 싱크 손실을 계산하는 것을 특징으로 할 수 있다. 본 발명의 다른 목적은, 합성 타겟 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코딩하여 원본영상 임베딩 벡터를 출력하는 원본영상 인코더; 입모양 합성의 기초가 되는 음성 데이터를 인코 딩하여 음성 임베딩 벡터를 출력하는 음성 인코더; 및 상기 원본영상 임베딩 벡터 및 상기 음성 임베딩 벡터를 입력 데이터로 하고, 상기 합성 타겟 얼굴에 상기 음성 데이터에 대응되는 입모양이 합성된 합성영상 데이터를 출력하는 합성영상 디코더;를 포함하고, 상기 원본영상 인코더, 상기 음성 인코더 및 상기 합성영상 디코더의 학습 세션에서는, 상기 합성영상 데이터와 상기 원본영상 데이터의 차이로 구성되는 재구성 손실, 합성영상 손 실 및 싱크 손실의 합이 저감되는 방향으로 업데이트되도록 구성되며, 상기 합성영상 손실은, 상기 합성영상 데 이터를 입력 데이터로 하고 상기 합성영상 데이터가 상기 합성영상 디코더에서 생성된 것인지 여부를 구분하는 합성영상 판별 벡터를 출력 데이터로 하는 인공신경망 모듈인 합성영상 판별기에서 출력되는 손실로서, 상기 합 성영상 판별 벡터를 기초로 구성되는 손실을 의미하고, 상기 싱크 손실은, 상기 음성 데이터를 입력받고 음성 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 음성 임베딩 모듈; 및 상기 합성영상 데이터를 입력받고 합 성영상 싱크 벡터를 출력하는 기학습된 인공신경망 모듈인 합성영상 임베딩 모듈;을 포함하는 싱크 판별기에서 출력되는 손실로서, 상기 음성 싱크 벡터와 상기 합성영상 싱크 벡터의 차이로 구성되는 손실을 의미하며, 상기 싱크 판별기의 상기 음성 임베딩 모듈 및 상기 합성영상 임베딩 모듈은, 상기 음성 데이터와 상기 합성영상 데 이터의 입모양 사이의 싱크로율이 높을수록 상기 싱크 손실이 낮게 출력되도록 기학습되는 것을 특징으로 하는, 인공신경망을 이용한 입모양 합성 장치를 제공하여 달성될 수 있다. 본 발명의 다른 목적은, 인공신경망을 이용한 입모양 합성 장치를 이용한 입모양 합성 방법에 있어서, 원본영상 인코더가 합성 타겟 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코딩하여 원 본영상 임베딩 벡터를 출력하는 원본영상 인코딩 단계; 음성 인코더가 입모양 합성의 기초가 되는 음성 데이터 를 인코딩하여 음성 임베딩 벡터를 출력하는 음성 인코딩 단계; 및 합성영상 디코더가 상기 원본영상 임베딩 벡 터 및 상기 음성 임베딩 벡터를 입력 데이터로 하고, 상기 합성 타겟 얼굴에 상기 음성 데이터에 대응되는 입모 양이 합성된 합성영상 데이터를 출력하는 합성영상 디코딩 단계;를 포함하는, 인공신경망을 이용한 입모양 합성방법을 제공하여 달성될 수 있다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기한 바와 같이, 본 발명에 의하면 이하와 같은 효과가 있다. 첫째, 본 발명의 일실시예에 따르면, 음성 입력을 통한 입모양의 합성 시 음성과 영상의 싱크 정밀도가 향상되 는 효과가 발생된다. 둘째, 본 발명의 일실시예에 따르면, 음성 입력을 통한 입모양 합성의 시각적인 현실성이 향상되는 효과가 발생 된다. 기존의 알고리즘을 통한 입모양 합성의 경우, 음성이 존재하는 구간과 음성이 존재하지 않는 구간의 전이 구간에서 현실성이 낮은 입모양이 합성되는 문제가 있었다. 셋재, 본 발명의 일실시예에 따르면, 음성 입력을 통한 입모양 합성 시 얼굴 상반과 얼굴 하반의 정합이 개선되 는 효과가 발생된다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부된 도면을 참조하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 본 발명을 쉽게 실시할 수 있는 실시예를 상세히 설명한다. 다만, 본 발명의 바람직한 실시예에 대한 동작원리를 상세하게 설명함에 있 어서 관련된 공지기능 또는 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되 는 경우에는 그 상세한 설명을 생략한다. 또한, 도면 전체에 걸쳐 유사한 기능 및 작용을 하는 부분에 대해서는 동일한 도면 부호를 사용한다. 명세서 전 체에서, 특정 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적으로 연결되어 있는 경우뿐만 아니라, 그 중간에 다른 소자를 사이에 두고, 간접적으로 연결되어 있는 경우도 포함한다. 또한, 특정 구성요소를 포함한다 는 것은 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라, 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 발명의 범위는 한글 음성 및 영상에 한정되지 않고 영어, 일본어, 중국어 등 다양한 국가의 언어로 적용되는 범위를 포함할 수 있다. 또한, 본 발명의 설명에서 원본영상 데이터는 video format 뿐만 아니라 image format을 포함할 수 있으며, 원 본영상 데이터가 image format으로 구성되는 경우에는 기설정된 frame 수에 따라 복수의 이미지가 원본영상 데 이터로 입력되도록 구성될 수 있다. 인공신경망을 이용한 입모양 합성 장치 도 1, 2는 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 사용상태를 도시한 모식도이 다. 도 1, 2에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치는 노트 북, 스마트폰, 데스크탑 등의 터미널 역할을 수행하는 클라이언트(client) 또는 서버(server)에 구성되는 경우 를 모두 포함할 수 있다. 또한, 입력 데이터인 원본영상 데이터와 음성 데이터는 클라이언트 또는 서버에서 수 신할 수 있고, 출력 데이터인 합성영상 데이터는 클라이언트 또는 서버로 송신하도록 구성될 수 있다. 구체적으로, 인공신경망을 이용한 입모양 합성 장치가 서버에 구성되는 경우에는 도 1의 (a), (b), (c)에 도 시된 바와 같이 작동될 수 있다. 도 1의 (a)에서 도시된 바와 같이, 사용자 클라이언트에서 원본영상 데이 터 및 음성 데이터를 특정 앱/웹서비스를 수행하는 특정 서비스 웹서버를 통해 인공신경망을 이용한 입모 양 합성 장치로 제공하고, 인공신경망을 이용한 입모양 합성 장치에서는 합성영상 데이터를 생성하여 특 정 서비스 웹서버 및 사용자 클라이언트에 제공하도록 구성될 수 있다. 또한, 도 1의 (b)에서 도시된 바와 같이, 원본영상 데이터는 특정 서비스 웹서버에 연결된 데이터베이스에서 제공받아 인공신경망을 이용한 입모양 합성 장치로 제공하도록 구성될 수 있다. 또한, 도 1의 (c)에서 도시된 바와 같이, 사용자 클라 이언트에서는 스크립트만을 특정 서비스 웹서버에 제공하고, 특정 서비스 웹서버의 TTS(Text to Speech) 모듈에서 해당 스크립트를 기초로 음성 데이터를 생성하여 인공신경망을 이용한 입모양 합성 장치로 제공하도록 구성될 수 있다. 또한, 도면에 도시되어 있지는 않지만, 사용자 클라이언트에서 원본영상 데이 터를 특정 앱/웹서비스를 수행하는 특정 서비스 웹서버를 통해 인공신경망을 이용한 입모양 합성 장치 로 제공하고, 음성 데이터는 특정 서비스 웹서버에 연결된 데이터베이스에서 제공받아 인공신경망을 이용 한 입모양 합성 장치로 제공하며, 인공신경망을 이용한 입모양 합성 장치에서는 합성영상 데이터를 생성 하여 특정 서비스 웹서버 및 사용자 클라이언트에 제공하도록 구성될 수 있다. 또한, 인공신경망을 이용한 입모양 합성 장치가 사용자 클라이언트에 구성되는 경우에는 도 2의 (d), (e)에 도시된 바와 같이 작동될 수 있다. 도 2의 (d)에서 도시된 바와 같이, 사용자 클라이언트 내에 구성 된 애플리케이션 모듈 내에 인공신경망을 이용한 입모양 합성 장치가 구성될 수 있고, 인공신경망을 이용한 입모양 합성 장치가 원본영상 데이터 및 음성 데이터를 사용자 클라이언트의 저장모듈에서 제공받아 합 성영상 데이터를 생성하고, 생성된 합성영상 데이터를 특정 서비스 웹서버에 제공하도록 구성될 수 있다. 또는, 도 2의 (e)에 도시된 바와 같이, 원본영상 데이터는 특정 서비스 웹서버에 연결된 데이터베이스에서 제공받아 인공신경망을 이용한 입모양 합성 장치로 제공하도록 구성될 수 있다. 또한, 도 2의 (e)에 도시된 바와 같이, 사용자 클라이언트의 저장모듈에서는 스크립트만을 애플리케이션 모듈 내의 인공신경망을 이용 한 입모양 합성 장치에 제공하고, 애플리케이션 모듈 내의 TTS(Text to Speech) 모듈에서 해당 스크립트를 기초로 음성 데이터를 생성하여 인공신경망을 이용한 입모양 합성 장치로 제공하도록 구성될 수 있다. 본 발명의 범위는 위의 사용상태 이외에도 인공신경망을 이용한 입모양 합성 장치가 클라이언트 또는 서버에 구성되는 모든 경우를 포함할 수 있고, 위의 사용상태는 본 발명의 범위를 한정하지 않는다. 도 3은 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도이고, 도 4는 본 발 명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 구체적인 구조를 도시한 모식도이다. 도 3, 4에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치는 원본영상 인코 더, 음성 인코더, 합성영상 디코더, 합성영상 판별기, 싱크 판별기를 포함할 수 있다. 이 때, 본 발명의 범위는 도 3의 (a)와 같이 [원본영상 인코더, 음성 인코더, 합성영상 디코더, 합성 영상 판별기, 싱크 판별기]가 하나의 컴퓨팅 시스템 내에 구성되는 경우와, 도 3의 (b)와 같이 [원본영 상 인코더, 음성 인코더, 합성영상 디코더]와 [합성영상 판별기, 싱크 판별기]가 서로 다 른 컴퓨팅 시스템 내에 구성되어 상호 유무선 네트워크로 연결되는 경우를 포함할 수 있다. 예를 들어, 후자의 경우에는 [원본영상 인코더, 음성 인코더, 합성영상 디코더]가 사용자 클라이언트에 구성되고 [합성영상 판별기, 싱크 판별기]는 서버단에 구성되는 경우를 포함할 수 있다. 원본영상 인코더는 합성 타겟의 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코딩하여 특정 dimension의 원본영상 임베딩 벡터를 출력하는 인코딩 모듈이다. 도 5는 본 발명의 일실시예에 따른 원본영상 인코더의 구조를 도시한 모식도이다. 도 5에 도시된 바와 같이, 예를 들어, 본 발명의 일실 시예에 따른 원본영상 인코더는 특정 차원으로 표준화 된 원본영상의 특정 프레임(원본영상 데이터)을 입력 데이터로 수신하여 1 x 1 x k의 잠재변수인 원본영상 임베딩 벡터를 출력 데이터로 인코딩하는 복수개의 연속된 Convolution Layer, Pooling Layer, Fully Connected Layer를 포함하는 ConvNet으로 구성될 수 있다. 또한, 원 본영상 인코더는 합성영상 디코더와 Skip connection 구조로 구성될 수 있다. 원본영상 인코더의 학습 세션에서, 본 발명의 일실시예에 따른 원본영상 인코더에 입력되는 원본영상 데이터는 원본영상의 각 프레임에서 임의의 프레임을 n개 추출하여 원본영상 인코더 및 합성영상 디코더 의 각 Convolution layer에 대해 Channel-wise concatenation의 구조로 입력되도록 구성될 수 있다. 원본 영상 데이터로서 원본영상의 순차적인 프레임을 원본영상 데이터로 이용하게 되는 경우, 원본영상 데이터가 이 미 음성과 동기화되어 입력 데이터 자체에서 동기화된 입술 모양이 포함되게 되는 문제가 발생된다. 본 발명의 일실시예에 따른 원본영상 인코더에 따르면 이러한 문제가 해소되어, 원본영상의 순차적인 프레임을 활용하 는 경우보다 합성영상 디코더의 학습 성능이 향상되는 효과가 발생된다. 또한, 임의의 프레임을 n개 추출하 여 원본영상 인코더 및 합성영상 디코더의 각 Convolution layer에 대해 Channel-wise concatenation 의 구조로 입력되는 구성에 의해 원본영상 인코더 및 합성영상 디코더의 학습 세션에서 Vanishing Gradient가 개선되고, Feature Propagation이 강화되며, Parameter 수가 절약되어 컴퓨팅 리소스가 저감되는효과가 발생된다. 또한, 본 발명의 일실시예에 따르면, 원본영상의 각 프레임에 대해 얼굴 하반을 마스크하여 원본영상 데이터를 생성하는 얼굴 검출 모듈을 더 포함할 수 있다. 본 발명의 일실시예에 따른 얼굴 검출 모듈은 원본영상 의 각 프레임에서 임의의 프레임을 n개 추출하고, 추출된 프레임에서 입모양 합성의 타겟(target)이 되는 얼굴 (타겟 얼굴)을 검출(detect)한 뒤, 타겟 얼굴의 하반(예를 들어, 코 위치 아래 또는 입 위치)을 마스크하여, 원 본영상에서 타겟 얼굴의 하반이 마스크 된 임의의 프레임을 원본영상 데이터로서 원본영상 인코더에 입력하 는 모듈이다. 도 6은 얼굴 검출 모듈과 원본영상 인코더의 작동관계를 도시한 모식도이다. 도 6에 도시 된 바와 같이, 본 발명의 일실시예에 따른 얼굴 검출 모듈에 의해 타겟 얼굴의 하반을 마스크하여 원본영상 인코더의 입력 데이터로 활용하면, 원본영상의 입모양에 의해 발생되는 합성영상의 입모양에 대한 영향을 최소화 할 수 있는 효과가 발생된다. 이때, 얼굴 검출 모듈은 Face detection을 통해 Bounding Box를 형성 하는 기공개된 Convolutional Network을 포함하고 해당 Bounding Box의 높이 기준으로 얼굴 상하반을 구분하거 나, 코 위치/입 위치에 대한 Confidence를 출력하는 기공개된 Convolutional Network을 포함하여 구체적인 코 위치/입 위치를 기초로 얼굴 상하반을 구분하도록 구성될 수 있다. 얼굴 검출 모듈의 구체적인 구성과 관련하여, 검출된 Bounding Box의 하반을 타겟 얼굴의 하반으로 활용하 도록 구성될 수 있다. 보다 정교한 타겟 얼굴의 하반의 활용을 위한 얼굴 검출 모듈은 타겟 얼굴 검출 모듈 , 랜드마크 검출 모듈, 랜드마크 위치 보정 모듈, 타겟 얼굴 하반 마스크 모듈을 포함할 수 있다. 타겟 얼굴 검출 모듈은 원본영상의 특정 프레임을 입력 데이터로 하고 타겟 얼굴 부위를 포함하 는 바운딩 박스(타겟 얼굴 바운딩 박스)의 좌표를 출력 데이터로 하는 Convolutional Neural Network으로 구성 될 수 있다. 랜드마크 검출 모듈은 코 또는 입과 같이 타겟 얼굴의 하반을 규정하기 위한 랜드마크 위치 정보와 신뢰도 정보(confidence)를 출력하는 모듈이다. 랜드마크 위치 보정 모듈은 랜드마크 검출 모듈 에서 검출된 랜드마크 사이의 관계를 추정하여 랜드마크의 위치 정보를 더 정교하게 보정하는 모듈이다. 타겟 얼굴 하반 마스크 모듈은 랜드마크 검출 모듈에서 출력되고 랜드마크 위치 보정 모듈에 의 해 보정된 랜드마크 위치 정보를 기초로 타겟 얼굴 바운딩 박스 내에서 타겟 얼굴의 하반을 규정하고 마스크 (mask) 적용하는 모듈이다. 랜드마크 검출 모듈및 랜드마크 위치 보정 모듈의 구체적인 실시예와 관련하여, 랜드마크 검출 모듈 은 타겟 얼굴 검출 모듈에서 생성된 Feature Map(F)을 입력값으로 하여 코 또는 입과 같은 랜드마크 에 대해 특정 Label(예를 들어, Nose, Mouth)에 대응되는 Confidence 값을 예측하는 Detection Confidence Map(S)을 출력하게 된다. 1차 단계에서 랜드마크 검출 모듈의 S 출력에 대한 수학식은 아래와 같다. 수학식 1"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "위 수학식 1에서 S는 랜드마크 검출 모듈에 의해 출력되는 Detection Confidence Map을 의미하고, S1는 1 차 단계의 Detection Confidence Map를 의미한다. 또한, F는 1차 단계의 랜드마크 검출 모듈에 입력되는 원본영상의 Feature Map을 의미한다. ρ1은 1차 단계의 랜드마크 검출 모듈의 ConvNet의 inference를 의미 한다. 랜드마크 위치 보정 모듈은 F를 입력값으로 하여 각 랜드마크에 대한 관계 벡터(예를 들어, A 포인트와 B 포인트를 연결하는 벡터로서, 각 포인트의 랜드마크 상의 가까움에 대응되는 값)를 예측하는 한 세트의 관계 필 드(L)을 출력하게 된다. 1차 단계에서 랜드마크 위치 보정 모듈의 L 출력에 대한 수학식은 아래와 같다. 수학식 2"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "위 수학식 2에서 L은 랜드마크 위치 보정 모듈에 의해 출력되는 복수개의 관계 벡터를 포함하는 관계 필드 를 의미하고, L1는 1차 단계의 관계 필드를 의미한다. 또한, F는 1차 단계의 랜드마크 검출 모듈에 입력되 는 원본영상의 Feature Map을 의미한다. φ1은 1차 단계의 랜드마크 위치 보정 모듈의 ConvNet의 inference를 의미한다. 1차 단계에서 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈에 의해 출력되는 예측치인 S와 L은 최 초에 입력된 Feature map인 F와 함께 2차 단계 이후의 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈 에 입력되어 아래 수학식과 같이 랜드마크 검출 및 각 랜드마크의 관계 예측의 정확도를 향상시키는데 이 용되게 된다. 수학식 3"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 4"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "위 수학식 3에서 S는 랜드마크 검출 모듈에 의해 출력되는 Detection Confidence Map을 의미하고, St는 t 차 단계의 Detection Confidence Map를 의미하고, St-1는 1차 단계의 Detection Confidence Map, Lt-1는 1차 단 계의 관계 필드를 의미한다. 또한, F는 1차 단계의 랜드마크 검출 모듈에 입력되는 원본영상의 Feature Map을 의미한다. ρt은 t차 단계의 랜드마크 검출 모듈의 ConvNet의 inference를 의미한다. 위 수학식 4에서 L은 랜드마크 위치 보정 모듈에 의해 출력되는 복수개의 관계 벡터를 포함하는 관계 필드 를 의미하고, Lt는 t차 단계의 관계 필드를 의미하고, St-1는 1차 단계의 Detection Confidence Map, Lt-1는 1차 단계의 관계 필드를 의미한다. 또한, F는 1차 단계의 랜드마크 검출 모듈에 입력되는 원본영상의 Feature Map을 의미한다. φt은 t차 단계의 랜드마크 위치 보정 모듈의 ConvNet의 inference를 의미한다. 본 발명의 일실시예에 따르면, 랜드마크 검출 및 관계 필드의 예측의 정확도를 향상시키기 위해 각 단계의 랜드 마크 검출 모듈 및 랜드마크 위치 보정 모듈 각각의 출력값에 대해 각각 Loss Function을 적용할 수 있고, 단계가 진행될수록 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈 각각의 정확도가 향상되도 록 학습시킬 수 있다. 본 발명의 일실시예에 따라 각 단계의 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈 각각의 출력값에 대해 각각 Loss Function을 적용하는 경우 주기적으로 gradient가 보충되게 되므로 vanishing gradient problem이 해소되는 효과가 발생된다. 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈의 Loss function은 아래와 같이 구성될 수 있다. 수학식 5"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "수학식 6"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "위 수학식 5, 6에서, fst는 랜드마크 검출 모듈의 t 단계에서의 Loss function, fLt는 랜드마크 위치 보정 모듈의 t 단계에서의 Loss function을 의미한다. 랜드마크 검출 모듈의 S는 1 부터 J의 세트로 구성 될 수 있으며 Sj로 표시될 수 있다. 랜드마크 위치 보정 모듈의 L은 1 부터 C의 세트로 구성될 수 있으며 Lc로 표시될 수 있다. 각각의 손실함수에서 *는 Ground truth를 의미한다. 즉, Sj*는 S의 Ground truth, Lc*는 관계 필드 L의 Ground truth를 의미한다. p는 특정 위치, 즉 특정 랜드마크를 의미한다. W는 바이너리 마스크로 서 특정 위치 p에 레이블이 있는 경우에는 1, 특정 위치 p에 레이블이 없는 경우에는 0의 값을 갖도록 구성될 수 있다. 또한, ∥·∥2는 L2-norm을 의미한다. 이러한 바이너리 마스크는 랜드마크 검출 모듈 및 랜드마크 위치 보정 모듈의 학습 과정에서의 불이익을 최소화하는 효과를 가져온다. 음성 인코더는 특정인의 목소리을 포함하는 음성(합성 기초 음성)으로서 입모양 합성의 기초가 되는 음성 데이터를 인코딩하여 특정 dimension의 음성 임베딩 벡터를 출력하는 인코딩 모듈이다. 예를 들어, 본 발명의 일실시예에 따른 음성 인코더는 특정 차원으로 표준화 된 합성 기초 음성의 mel spectrogram(음성 데이터) 을 입력 데이터로 입력받고 1 x 1 x k의 잠재변수인 음성 임베딩 벡터를 출력 데이터로 인코딩하는 복수개의 연 속된 Convolution Layer를 포함하는 ConvNet으로 구성될 수 있다. 본 발명에서는 설명의 편의를 위하여 음성 데이터의 포맷을 멜 스케일의 스펙트로그램인 멜 스펙트로그램으로 기재하였으나 본 발명의 범위는 이에 한정되지 않으며, rare audio signal, mel-filterbank를 거치지 않은 기본 적인 spectrogram, 스펙트럼, Fundamental frequency를 의미하는 f0 등 Fourier Transform을 활용한 주파수 정 보, 신호에서의 비주기성 구성요소와 음성 신호간 비율을 의미하는 aperiodicity 등의 포맷을 포함할 수 있다. 합성영상 디코더는 원본영상 인코더에서 출력되는 원본영상 임베딩 벡터 및 음성 인코더에서 출력 되는 음성 임베딩 벡터가 결합된 결합 벡터를 입력 데이터로 하고, 합성 타겟인 타겟 얼굴에 음성 데이터에 대 응되는 입모양이 합성된 연속 n개의 특정 프레임인 합성영상 데이터를 출력하는 디코딩 모듈이다. 예를 들어, 본 발명의 일실시예에 따른 합성영상 디코더는 원본영상 임베딩 벡터 및 음성 임베딩 벡터가 결합된 결합 벡터를 입력 데이터로 수신하여 h(높이) x w(너비) x 3의 연속된 n개의 특정 프레임인 합성영상 데이터를 출력 데이터로 디코딩하는 복수개의 연속된 Convolution Layer를 포함하는 ConvNet으로 구성될 수 있다. 합성영상 디 코더의 학습 세션에서, 본 발명의 일실시예에 따른 원본영상 인코더와 합성영상 디코더는 복수개의 Convolution Layer 사이에 Skip Connection이 적용될 수 있다. 합성영상 디코더의 학습 세션에서는 합성영상 데이터와 이에 대응되는 레퍼런스 데이터(Ground Truth)의 차 이로 구성되는 재구성 손실을 저감시키는 방향으로 합성영상 디코더의 파라미터가 업데이트 되도록 구성될 수 있다. 합성영상 디코더의 손실함수 중 하나인 재구성 손실은 Mean square loss, Cross entropy loss 등으로 구성될 수 있으며, 예를 들어 아래와 같이 구성될 수 있다. 수학식 7"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "위 수학식에서 Lc는 재구성 손실, N은 합성영상 데이터와 레퍼런스 데이터의 pair의 수(배치 크기), i는 N개의 합성영상 데이터와 레퍼런스 데이터의 pair 중 특정 프레임 pair, Lg는 합성영상 데이터, LG는 Ground truth로 서의 레퍼런스 데이터, ∥·∥2은 L2-norm을 의미할 수 있다. 합성영상 판별기는 합성영상 디코더에서 출력되는 합성영상 데이터를 입력받고, 합성영상 데이터의 Real(합성영상 디코더에서 합성되지 않은 원본)과 Fake(합성영상 디코더에서 합성된 합성본)를 구분하는 합성영 상 판별 벡터를 출력하도록 학습된 인공신경망 모듈이다. 본 발명의 일실시예에 따른 합성영상 판별기는 CONCAT 함수와 복수개의 Convolution Layer를 포함하도록 구성될 수 있다. 합성영상 판별기의 학습 세션에서는 원본영상 데이터(Real로 Labeling) 및 합성영상 디코더에서 출력된 합성영상 데이터(Fake로 Labeling)를 합성영상 판별기에 입력하고, 합성영상 판별기에서는 합성영상 데 이터의 Real과 Fake를 구분하는 합성영상 판별 벡터(Real Class와 Fake Class를 포함하거나, Real Class 만을 포함하도록 구성될 수 있음)를 출력하며, 합성영상 판별 벡터와 입력 데이터의 실제 Label(Real or Fake) 사이 의 차이를 포함하는 합성영상 손실이 저감되는 방향으로 합성영상 판별기의 ConvNet의 파라미터가 업데이트 되도록 학습 세션이 구성될 수 있다. 즉, 합성영상 판별기의 학습 세션에서는 합성영상 데이터 분포에서의 합성영상 판별 벡터 Ds(x)가 최소 가 되고 이에 대응되는 레퍼런스 데이터(Ground Truth) 분포인 원본영상 데이터 분포에서의 합성영상 판별 벡터 Ds(x)가 최대가 되도록 합성영상 판별기의 파라미터가 업데이트 되도록 구성될 수 있다. 합성영상 판별 기의 손실함수인 합성영상 손실은 예를 들어 아래와 같이 구성될 수 있다. 수학식 8"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "위 수학식에서 Ls는 합성영상 손실, Lg는 합성영상 데이터, LG는 Ground truth로서의 레퍼런스 데이터, x~LG는 레퍼런스 데이터의 분포, x~Lg는 합성영상 데이터의 분포, Ds(x)는 합성영상 판별기에서 출력된 합성영상 판 별 벡터(0~1 사이의 확률값)를 의미한다. 합성영상 판별기의 학습 세션 관련하여, 합성영상 디코더에서 합성된 것으로 레이블링(Labeling) 된 학 습 데이터와 합성되지 않은 것으로 레이블링 된 학습 데이터를 합성영상 판별기의 학습 데이터로 구성하고, 학습 데이터의 특정 프레임 세그먼트를 합성영상 판별기에 입력하여 합성영상 판별 벡터를 출력하고, 합성 영상 판별 벡터를 기초로 학습 데이터의 합성영상 손실을 계산하고, 계산된 합성영상 손실을 최소화 하는 방향 으로 합성영상 판별기의 파라미터를 업데이트하도록 합성영상 판별기의 학습 세션이 구성될 수 있다. 합성영상 판별기의 작동관계와 관련하여, 합성영상 디코더이 출력한 합성영상 데이터가 합성영상이라고 판별되는 경우 Ds(x)=0, Ls=0 (최대값)에 가깝게 합성영상 손실이 출력되고, 합성영상 데이터가 원본영상이라고판별되는 경우 Ds(x)=1, Ls=-∞ (최소값)에 가깝게 합성영상 손실이 출력되도록 학습될 수 있다."}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "싱크 판별기와 관련하여, 도 7은 본 발명의 일실시예에 따른 싱크 판별기를 도시한 모식도이다. 도 7에 도시된 바와 같이, 싱크 판별기는 음성 임베딩 모듈, 합성영상 임베딩 모듈을 포함하도록 구성될 수 있다. 음성 임베딩 모듈은 음성 데이터를 입력받고 음성 싱크 벡터를 출력하는 복수개의 Convolution Layer를 포 함한 기학습된 인공신경망으로 구성될 수 있고, 합성영상 임베딩 모듈은 합성영상 데이터를 입력받고 합성 영상 싱크 벡터를 출력하는 기학습된 인공신경망으로 구성될 수 있으며, 싱크 판별기의 학습 세션에서는 음 성 싱크 벡터와 합성영상 싱크 벡터의 거리로 구성되는 싱크 손실(합성영상 데이터와 음성 데이터의 싱크로율이 높을수록 싱크 손실이 낮음)을 저감시키는 방향으로 싱크 판별기의 음성 임베딩 모듈 및 합성영상 임베 딩 모듈의 파라미터가 업데이트 되도록 구성될 수 있다. 이때, 음성 임베딩 모듈 및 합성영상 임베딩 모듈은 복수개의 Convolution layer(Conv.Layer), Pooling layer 및 Fully connected layer를 포함할 수 있으며, 음성 싱크 벡터 및 합성영상 싱크 벡터는 특정 Fully connected layer(FC Layer) 또는 특정 FC Layer 이전의 Conv.Layer에서 출력되는 벡터를 의미할 수 있다. 싱크 판별기의 손실함수인 싱크 손실은 Mean square loss, Cross entropy loss 등으로 구성될 수 있으며, 예를 들어 이진 교차 엔트로피 손실(Binary cross entropy loss)이 적용되는 경우 아래와 같이 구성될 수 있다. 수학식 9"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "위 수학식에서 Ls는 싱크 손실, N은 음성 싱크 벡터와 합성영상 싱크 벡터의 pair의 수(배치 크기), i는 N개의 음성 싱크 벡터와 합성영상 싱크 벡터의 pair 중 특정 pair, v는 합성영상 싱크 벡터, s는 음성 싱크 벡터, ∥ ·∥2은 L2-norm을 의미할 수 있다. 입력 데이터의 구체적인 예로, 싱크 판별기의 음성 임베딩 모듈의 입력 데이터인 음성 데이터는 13(MFCC 계수)x20(오디오프레임)x1의 비선형 멜 스케일 주파수에서의 파워 스펙트럼으로 구성될 수 있다. 싱크 판별기의 합성영상 임베딩 모듈의 입력 데이터인 합성영상 데이터는 얼굴 검출 모듈을 통해 대상 얼굴의 하반(코 아래 또는 입 주변)만을 입력 데이터로 활용할 수 있고, 120(하반 높이)x120(너비)x3(RGB)의 이 미지 5프레임으로 구성될 수 있다. 음성 데이터는 100Hz의 샘플링 속도로 할 때 20 오디오프레임이므로 0.2초의 구간으로 구성되고, 합성영상 데이터 또한 25Hz 프레임 속도로 할 때 5프레임이므로 0.2초의 구간으로 구성되도 록 입력할 수 있다. 음성 임베딩 모듈, 합성영상 임베딩 모듈의 구체적인 예로, 아래와 같이 구성될 수 있다. [55x55x96] CONV1 : 96@ 11x11, stride = 4, parameter = 0 [27x27x96] MAX POOL1 : 3x3, stride = 2 [27x27x256] CONV2 : 256@ 5x5, stride = 1, parameter = 2 [13x13x256] MAX POOL2 : 3x3, stride = 2 [13x13x384] CONV3 : 384@ 3x3, stride = 1, parameter = 1 [13x13x384] CONV4 : 384@ 3x3, stride = 1, parameter = 1 [13x13x256] CONV5 : 256@ 3x3, stride = 1, parameter = 1 [6x6x256] MAX POOL3 : 3x3, stride = 2 FC6 : 4096 neurons FC7 : 4096 neurons 위 예시에서, CONV는 Convolution Layer, MAX POOL은 Pooling Layer, FC는 Fully Connected Layer를 의미한다. 싱크 판별기의 학습 세션 관련하여, 입모양과 음성의 동기화가 완료된 학습 영상 데이터의 특정 프레임 세 그먼트의 얼굴 하반 이미지를 합성영상 임베딩 모듈에 입력하여 학습 영상 싱크 벡터를 출력하고, 학습 영 상 데이터의 음성 MFCC 세그먼트를 음성 임베딩 모듈에 입력하여 음성 싱크 벡터를 출력한 뒤, 학습 영상 싱크 벡터와 음성 싱크 벡터의 싱크 손실을 계산하고, 싱크 손실을 최소화 하는 방향으로 음성 임베딩 모듈 및 합성영상 임베딩 모듈의 파라미터를 업데이트하도록 싱크 판별기의 학습 세션이 구성될 수 있다. 싱크 판별기의 작동관계 관련하여, 싱크 판별기에 음성 데이터 및 합성영상 데이터가 입력되고, 음성 임베딩 모듈에서 음성 싱크 벡터, 합성영상 임베딩 모듈에서 합성영상 싱크 벡터가 각각 출력되며, 출 력된 음성 싱크 벡터 및 합성영상 싱크 벡터를 기초로 싱크 손실을 계산하도록 구성될 수 있다. 본 발명의 일실시예에 따른 싱크 판별기에 따르면, 음성과 합성영상(특히 하반)에 대한 싱크로율에 대한 손실함수가 별도로 더 포함됨으로써, 합성영상 디코더로 생성되는 합성영상의 입모양에 대한 음성 싱크로율 이 더 향상되는 효과가 발생된다. 입모양이나 얼굴을 합성하는 기존의 인공신경망에서 활용되는 손실함수는 모 두 시각적인 유사도에 집중하도록 구성되어 합성영상의 입모양에 대한 음성 싱크로율을 향상시키는데 어려움이 있었다. 또한, 본 발명의 일실시예에 따른 싱크 판별기에 따르면, [원본영상 인코더, 음성 인코더, 합성영 상 디코더, 합성영상 판별기]와는 별도로 학습 세션이 진행되고, 싱크 판별기가 기학습된 상태에서 [원본영상 인코더, 음성 인코더, 합성영상 디코더]의 학습 세션에 싱크 손실을 제공하도록 구성되 어, 상당량의 학습 데이터 추가나 상당량의 컴퓨팅 로드(Computation Load) 추가 없이도 신경망의 자유도가 향 상되기 때문에, 합성영상 디코더의 영상 합성 퀄리티가 향상되는 효과가 발생된다. 인공신경망을 이용한 입모양 합성 장치 중 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 학습 세션(training session)에서는, 영상 내에 특정인의 얼굴을 포함하고, 입모양과 음성의 동기화가 완료된 학습 데이터의 특정 영상 프레임 세그먼트(원본영상 데이터)를 원본영상 인코더에 입력하여 원본영상 임베 딩 벡터를 출력하고, 학습 데이터의 음성 MFCC 세그먼트(음성 데이터)를 음성 인코더에 입력하여 음성 임베 딩 벡터를 출력한 뒤, 원본영상 임베딩 벡터와 음성 임베딩 벡터를 합성영상 디코더에 입력하여 합성영상 데이터를 출력하도록 구성될 수 있다. 이후, 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계산된 싱크 손실 및 기학습된 합성영상 판별기에서 합성영상 데이터의 합성영상 판별 벡터를 기초로 계산된 합성영상 손실 (합성영상 데이터 분포에 대한 손실 부분)의 합이 저감되는 방향으로 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션이 구성될 수 있다. 합성영상 판별기의 학습 세션에서는, 원본영상 데이터(Real로 Labeling) 및 합성영상 디코더에서 출력 된 합성영상 데이터(Fake로 Labeling)를 합성영상 판별기에 입력하고, 합성영상 판별기에서는 합성영상 데이터의 Real과 Fake를 구분하는 합성영상 판별 벡터(Real Class와 Fake Class를 포함하거나, Real Class 만 을 포함하도록 구성될 수 있음)를 출력하며, 합성영상 판별 벡터와 입력 데이터의 실제 Label(Real or Fake) 사 이의 차이를 포함하는 손실이 저감되는 방향으로 합성영상 판별기의 ConvNet의 파라미터가 업데이트되도록 학습 세션이 구성될 수 있다. 이때, 원본영상 인코더, 음성 인코더 및 합성영상 디코더는 합성영상 판별기와 번갈아가며 학 습되도록 구성될 수 있고, 원본영상 인코더, 음성 인코더 및 합성영상 디코더는 함께 학습되도록 구성될 수 있다. 인공신경망을 이용한 입모양 합성 장치의 추론 세션에서는 원본영상 인코더, 음성 인코더 및 합성영 상 디코더만 작동될 수 있으며, 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 추론 세 션(inference session)에서는, 영상 내에 타겟 얼굴을 포함하는 원본영상 데이터의 특정 영상 프레임 세그먼트 (원본영상 데이터)를 원본영상 인코더에 입력하여 원본영상 임베딩 벡터를 출력하고, 원본영상 데이터에 합성하고자 하는 음성 MFCC 세그먼트(음성 데이터)를 음성 인코더에 입력하여 음성 임베딩 벡터를 출력한 뒤, 원본영상 임베딩 벡터와 음성 임베딩 벡터를 합성영상 디코더에 입력하여 합성영상 데이터를 출력하도록 구 성될 수 있다. 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 위와 같은 구성에 따르면, 음성 입력을 통한 입모양의 합성 시 음성과 영상의 싱크 정밀도가 향상되는 효과가 발생된다. 또한, 얼굴 판별기와 합성영상 판별기의 유기적인 결합에 의해 음성 입력을 통한 입모양 합성의 시각적인 현실성이 향상되는 효과가 발생된다. 기존의 알고리즘을 통한 입모양 합성의 경우, 음성이 존재하는 구간과 음성이 존재하지 않는 구간의 전이 구간 에서 현실성이 낮은 입모양이 합성되는 문제가 있었다. [변형예] 얼굴 판별기와 관련하여, 도 8은 본 발명의 변형예에 따른 얼굴 판별기를 더 포함하는 인공신경망을 이 용한 입모양 합성 장치를 도시한 모식도이다. 도 8에 도시된 바와 같이, 본 발명의 변형예에 따른 인공신경 망을 이용한 입모양 합성 장치는 얼굴 판별기를 더 포함할 수 있다. 얼굴 판별기는 합성영상 디코더에서 출력되는 합성영상 데이터를 입력받고, 합성영상 데이터의 얼굴의 현실성을 구분하는 얼굴 판별 벡터(0~1 사이의 확률값)를 출력하도록 학습된 인공신경망 모듈이다. 본 발명의 일실시예에 따른 얼굴 판별기는 복수개의 Convolution Layer를 포함하도록 구성될 수 있고, 얼굴 판별기 의 학습 세션에서는 합성영상 데이터 분포에서의 얼굴 판별 벡터 Df(x)가 최소가 되고 이에 대응되는 레 퍼런스 데이터(Ground Truth) 분포에서의 얼굴 판별 벡터 Df(x)가 최대가 되도록 얼굴 판별기의 파라미 터가 업데이트 되도록 구성될 수 있다. 얼굴 판별기의 손실함수인 얼굴 손실은 예를 들어 아래와 같이 구성 될 수 있다. 수학식 10"}
{"patent_id": "10-2020-0188203", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "위 수학식에서 Lf는 얼굴 손실, Lg는 합성영상 데이터, LG는 Ground truth로서의 레퍼런스 데이터, x~LG는 레퍼 런스 데이터의 분포, x~Lg는 합성영상 데이터의 분포, Df(x)는 얼굴 판별기에서 출력된 얼굴 판별 벡터(0~1 사이의 확률값)를 의미한다. 얼굴 판별기의 학습 세션 관련하여, 도 9는 본 발명의 변형예에 따른 얼굴 판별기의 학습 세션을 도시 한 모식도이다. 도 9에 도시된 바와 같이, 현실적인 얼굴이 포함된 것으로 레이블링(Labeling) 된 학습 데이터 (현실적 얼굴 Class)와 현실적인 얼굴이 포함되지 않은 것으로 레이블링 된 학습 데이터(비현실적 얼굴 Class) 를 얼굴 판별기의 학습 데이터로 구성하고, 학습 데이터의 특정 프레임 세그먼트를 얼굴 판별기에 입력 하여 얼굴 판별 벡터를 출력하고, 얼굴 판별 벡터와 학습 데이터의 레이블(Label)인 Ground Truth를 기초로 학 습 데이터의 얼굴 손실을 계산하고, 계산된 얼굴 손실을 최소화 하는 방향으로 얼굴 판별기의 파라미터를 업데이트하도록 얼굴 판별기의 학습 세션이 구성될 수 있다. 얼굴 판별기의 작동관계와 관련하여, 합성영상 디코더이 출력한 합성영상 데이터가 비현실적인 얼굴에 가까운 경우 Df(x)=0, Lf=0 (최대값)에 가깝게 얼굴 손실이 출력되고, 합성영상 데이터가 현실적인 얼굴에 가까 운 경우 Df(x)=1, Lf=-∞ (최소값)에 가깝게 얼굴 손실이 출력되도록 학습될 수 있다. 본 발명의 변형예에 따른 얼굴 판별기를 포함한 인공신경망을 이용한 입모양 합성 장치의 학습 세션과 관련하여, 영상 내에 특정인의 얼굴을 포함하고, 입모양과 음성의 동기화가 완료되며, 현실적인 얼굴이 포함되 거나 현실적인 얼굴이 포함되지 않은 것으로 각 프레임 세그먼트 별로 Labeling 된 학습 데이터의 특정 영상 프 레임 세그먼트(원본영상 데이터)를 원본영상 인코더에 입력하여 원본영상 임베딩 벡터를 출력하고, 학습 데 이터의 음성 MFCC 세그먼트(음성 데이터)를 음성 인코더에 입력하여 음성 임베딩 벡터를 출력한 뒤, 원본영상 임베딩 벡터 및 음성 임베딩 벡터를 합성영상 디코더에 입력하여 합성영상 데이터를 출력하도록 구성될 수 있다. 이후, 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 합성영상 판별기의 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계산된 싱크 손실 및 기학습된 얼굴 판별기에서 합성영상 데이터의 얼굴 판별 벡터를 기초로 계산된 얼굴 손실의 합이 저감되는 방향으로 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션이 구성될 수 있다. 이에 따르면, 입모양이나 얼굴을 합성하는 기존의 인공신경망에서 활용되는 손실함수에 비해 시각적인 퀄리티에 대한 별도의 손실함수인 얼굴 손실이 추가됨에 따라, 기존의 인공신경망 대비 생성되는 합성영상의 입모양 합성 의 시각적인 퀄리티가 향상되는 효과가 발생된다. 감정 벡터와 관련하여, 도 10은 본 발명의 변형예에 따른 감정 벡터의 결합을 도시한 모식도이다. 도 10에 도시 된 바와 같이, 본 발명의 다른 변형예에 따른 인공신경망을 이용한 입모양 합성 장치는, 음성 인코더의 출력 벡터인 음성 임베딩 벡터와 복수개의 감정 상태 Class를 구분하는 감정 벡터가 함께 출력되고, 합성영상 판별기의 출력 벡터인 합성영상 판별 벡터 또한 감정 벡터가 함께 출력되도록 구성될 수 있다. 이때, 합성 영상 판별기의 출력 벡터인 합성영상 판별 벡터는 감정 벡터가 결합되어 하나의 출력으로 구성되거나, 합성 영상 판별기의 Output Layer에 감정 벡터를 출력하는 별도의 활성화 함수(Sigmoid, ReLU 등)를 구성하여, 감정 벡터와 합성영상 판별 벡터가 별도로 출력되도록 구성될 수 있다. 또한, 원본영상 인코더, 음성 인코 더, 합성영상 디코더 및 합성영상 판별기의 손실함수에는 감정 벡터와 실제 Labeling 된 감정 상태 Class와의 차이를 기초로 구성되는 감정 손실이 더 포함될 수 있다. 감정 벡터는 복수개의 감정 Class를 포함하도록 구성될 수 있으며, 예를 들어, [행복, 슬픔, 놀람, 공포, 무감 정 등]의 복수의 감정 상태 각각에 대한 신뢰도(Confidence)가 구성될 수 있다. 본 발명의 변형예에 따른 인공신경망을 이용한 입모양 합성 장치의 학습 세션과 관련하여, 영상 내에 특정인 의 얼굴을 포함하고, 입모양과 음성의 동기화가 완료되며, 학습 데이터의 영상 포맷인 원본영상 데이터를 원본 영상 인코더에 입력하여 원본영상 임베딩 벡터를 출력하고, 음성 데이터의 감정 상태([행복, 슬픔, 놀람, 공포, 무감정 등])가 각 음성 데이터 세그먼트 별로 Labeling 된 학습 데이터의 음성 MFCC 세그먼트(음성 데이 터)를 음성 인코더에 입력하여 음성 임베딩 벡터 및 감정 벡터([행복, 슬픔, 놀람, 공포, 무감정 등의 복수 의 감정 상태 Class])를 출력한 뒤, 감정 벡터, 원본영상 임베딩 벡터 및 음성 임베딩 벡터를 합성영상 디코더 에 입력하여 합성영상 데이터를 출력하도록 구성될 수 있다. 이후, 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 합성영상 판별기에서의 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계산된 싱크 손실 및 감정 손실의 합이 저감되는 방향으로 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세 션이 구성될 수 있다. 또한, 합성영상 판별기의 학습 세션에서는, 원본영상 데이터(Real로 Labeling) 및 합 성영상 디코더에서 출력된 합성영상 데이터(Fake로 Labeling)를 합성영상 판별기에 입력하고, 합성영상 판별기에서는 합성영상 데이터의 Real과 Fake를 구분하는 합성영상 판별 벡터(Real Class와 Fake Class를 포함하거나, Real Class 만을 포함하도록 구성될 수 있음) 및 감정 벡터([행복, 슬픔, 놀람, 공포, 무감정 등의 복수의 감정 상태 Class])를 출력하며, 합성영상 판별 벡터와 입력 데이터의 실제 Label(Real or Fake) 사이의 차이 및 감정 벡터와 입력 데이터의 실제 감정 상태 Label([행복, 슬픔, 놀람, 공포, 무감정 등의 복수의 감정 상태 Class]) 사이의 차이를 포함하는 손실이 저감되는 방향으로 합성영상 판별기의 파라미터가 업데이트되 도록 학습 세션이 구성될 수 있다. 본 발명의 변형예에 따라 감정 벡터를 더 포함하는 인공신경망을 이용한 입모양 합성 장치의 추론 세션에서 는 원본영상 인코더, 음성 인코더 및 합성영상 디코더만 작동되며, 원본영상 인코더, 음성 인 코더 및 합성영상 디코더의 추론 세션(inference session)에서는, 영상 내에 타겟 얼굴을 포함하는 원 본영상 데이터의 특정 영상 프레임 세그먼트(원본영상 데이터)를 원본영상 인코더에 입력하여 원본영상 임 베딩 벡터를 출력하고, 원본영상 데이터에 합성하고자 하는 음성 MFCC 세그먼트(음성 데이터)를 음성 인코더 에 입력하여 음성 임베딩 벡터 및 감정 벡터([행복, 슬픔, 놀람, 공포, 무감정 등의 복수의 감정 상태 Class])를 출력한 뒤, 감정 벡터, 원본영상 임베딩 벡터 및 음성 임베딩 벡터를 합성영상 디코더에 입력하 여 합성영상 데이터를 출력하도록 구성될 수 있다. 이에 따르면, 합성영상 데이터에서 출력되는 타겟 얼굴의 입모양의 음성 데이터와의 동기화가 달성되는 동시에, 입모양이 음성 데이터의 각 세그먼트에 내포된 감정 상태에 맞게 합성되게 되는 효과가 발생된다. 즉, 합성의 기초가 되는 음성 데이터의 감정 상태에 맞게 입모양이 합성되게 됨으로써 합성영상의 시청자 입장에서 보다 자 연스러운 합성 영상의 생성이 가능해지는 효과가 발생된다. 감정 판별기와 관련하여, 도 11은 본 발명의 변형예에 따른 감정 판별기를 더 포함하는 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도이다. 도 11에 도시된 바와 같이, 본 발명의 다른 변형예에 따른 인공신경망을 이용한 입모양 합성 장치는, 감정 판별기 및 감정 손실을 더 포함할 수 있다. 감정 판별기는 합성영상 디코더에서 출력되는 합성영상 데이터를 입력받고, 합성영상 데이터의 감정 상 태를 구분하는 감정 벡터([행복, 슬픔, 놀람, 공포, 무감정 등의 복수의 감정 상태 Class])를 출력하도록 학습 된 인공신경망 모듈이다. 본 발명의 일실시예에 따른 감정 판별기는 복수개의 Convolution Layer를 포함하 도록 구성될 수 있다. 감정 판별기의 학습 세션에서는 감정 판별기에 감정 상태가 Labeling 된 학습 데이터를 입력하여 학습 데이터의 감정 상태를 구분하는 감정 벡터([행복, 슬픔, 놀람, 공포, 무감정 등의 복수의 감정 상태 Class])를 출력하며, 감정 벡터와 입력 데이터의 실제 감정 상태 Label 사이의 차이를 포함하는 손실인 감정 손실이 저감 되는 방향으로 감정 판별기의 파라미터가 업데이트되도록 학습 세션이 구성될 수 있다. 본 발명의 다른 변형예에 따른 인공신경망을 이용한 입모양 합성 장치의 학습 세션에서는, 영상 내에 특정인 의 얼굴을 포함하고, 입모양과 음성의 동기화가 완료되며, 영상 내의 얼굴의 감정 상태([행복, 슬픔, 놀람, 공 포, 무감정 등])가 각 프레임 세그먼트 별로 Labeling 된 학습 데이터의 특정 영상 프레임 세그먼트(원본영상 데이터)를 원본영상 인코더에 입력하여 원본영상 임베딩 벡터를 출력하고, 학습 데이터의 음성 MFCC 세그먼 트(음성 데이터)를 음성 인코더에 입력하여 음성 임베딩 벡터를 출력한 뒤, 원본영상 임베딩 벡터 및 음성 임베딩 벡터를 합성영상 디코더에 입력하여 합성영상 데이터를 출력하도록 구성될 수 있다. 이후, 각 Epoch 에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 합성영상 판별기에 서 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계산된 싱크 손실 및 기학습된 감정 판별기에서 합성영상 데이터의 감정 벡터와 원본영상 데이터의 감정 상태 Label을 기초로 계산된 감정 손실의 합이 저감되는 방향으로 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션이 구성될 수 있다. 이에 따르면, 합성영상 데이터에서 출력되는 타겟 얼굴의 입모양의 음성 데이터와의 동기화가 달성되는 동시에, 입모양이 타겟 얼굴의 각 감정 상태에 맞게 합성되게 되는 효과가 발생된다. 즉, 타겟 얼굴의 각 감정 상태에 맞게 입모양이 합성되게 됨으로써 보다 자연스럽고 심리스한 합성 영상의 생성이 가능해지는 효과가 발생된다. 또한, 합성영상 판별기와 감정 판별기의 역할이 분리됨으로써, 합성영상 판별기의 정확도가 향상되 는 효과가 발생된다. 입크기 판별기와 관련하여, 도 12는 본 발명의 변형예에 따른 입크기 판별기가 더 포함된 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 13은 본 발명의 변형예에 따른 입크기 판별기를 도시한 모식도이다. 도 12, 13에 도시된 바와 같이, 본 발명의 변형예에 따른 인공신경망을 이용한 입모양 합성 장치 는, 입크기 판별기 및 입크기 손실을 더 포함할 수 있다. 입크기 판별기는 음성 입크기 임베딩 모듈, 영상 입크기 임베딩 모듈을 포함하도록 구성될 수 있다. 음성 입크기 임베딩 모듈은 음성 데이터를 입력받고 Sigmoid, ReLU 등의 활성화 함수로 구성된 Output Layer에서 음성 입크기 벡터를 출력하는 복수개의 Convolution Layer를 포함한 기학습된 인공신경망으로 구성될 수 있고, 영상 입크기 임베딩 모듈은 합성영상 데이터를 입력받고 Sigmoid, ReLU 등의 활성화 함수 로 구성된 Output Layer에서 영상 입크기 벡터를 출력하는 기학습된 인공신경망으로 구성될 수 있다. 이때, 음 성 입크기 임베딩 모듈의 Output Layer 이전의 특정 Fully Connected Layer에서 추출된 음성 입크기 임베 딩 벡터를 출력할 수 있고, 영상 입크기 임베딩 모듈의 Output Layer 이전의 특정 Fully Connected Layer 에서 추출된 영상 입크기 임베딩 벡터를 출력하도록 구성될 수 있다. 입크기 판별기의 학습 세션에서는 입크기에 대해 Labeling 된 학습 데이터를 입크기 판별기에 입력하고, 음성 입크기 임베딩 모듈의 Output Layer에서 출력되는 음성 입크기 벡터와 실제 Label과의 차이 로 구성되는 음성 입크기 손실, 영상 입크기 임베딩 모듈의 Output Layer에서 출력되는 영상 입크기 벡터와실제 Label과의 차이로 구성되는 영상 입크기 손실, 음성 입크기 임베딩 벡터와 영상 입크기 임베딩 벡터의 거 리로 구성되는 입크기 손실(음성 데이터에서 나타나는 입크기 특징과 합성영상 데이터에서 나타나는 입크기 특 징의 차이가 적을수록 입크기 손실이 낮음)의 합을 저감시키는 방향으로 입크기 판별기의 음성 입크기 임베 딩 모듈 및 영상 입크기 임베딩 모듈의 파라미터가 업데이트 되도록 구성될 수 있다. 이때, 음성 입크 기 임베딩 모듈 및 영상 입크기 임베딩 모듈은 복수개의 Convolution layer, Pooling layer 및 Fully connected layer를 포함할 수 있으며, 음성 입크기 임베딩 벡터 및 영상 입크기 임베딩 벡터는 특정 Fully connected layer에서 출력되는 벡터를 의미할 수 있다. 입크기 판별기의 입크기 손실은 Mean square loss, Cross entropy loss 등으로 구성될 수 있다. 입크기 판별기의 작동관계 관련하여, 입크기 판별기에 음성 데이터 및 합성영상 데이터가 입력되고, 음 성 입크기 임베딩 모듈에서 음성 입크기 임베딩 벡터, 영상 입크기 임베딩 모듈에서 영상 입크기 임베 딩 벡터가 각각 출력되며, 출력된 음성 입크기 임베딩 벡터 및 영상 입크기 임베딩 벡터를 기초로 입크기 손실 을 계산하도록 구성될 수 있다. 이렇게 계산된 입크기 손실은 원본영상 인코더, 음성 인코더 및 합성영 상 디코더의 파라미터를 업데이트하도록 학습 세션에서 적용될 수 있다. 즉, 원본영상 인코더, 음성 인 코더 및 합성영상 디코더는 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재 구성 손실, 기학습된 합성영상 판별기에서 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터 와 음성 데이터를 기초로 계산된 싱크 손실 및 기학습된 입크기 판별기에서 음성 입크기 임베딩 벡터 및 영 상 입크기 임베딩 벡터를 기초로 계산된 입크기 손실의 합이 저감되는 방향으로 원본영상 인코더, 음성 인 코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션이 구성될 수 있다. 본 발명의 변형예에 따른 입크기 판별기에 따르면, 동일한 음소/음절이라고 하더라도 음성의 진폭이나 파형 등의 음성특성(입 안에서 울리는 음성, 입 밖으로 열리는 음성, 조용한 음성, 큰 음성 등)에 따라 입모양의 크 기가 다르게 합성영상이 출력될 수 있도록 하는 효과가 발생된다. 이에 따르면, 단순히 입모양의 합성이 음소/ 음절의 특징에 국한되지 않아 상황에 맞는 입모양의 합성이 가능해지고, 합성되는 입크기 특성이 강화되는 효과 가 발생된다. 각도 판별기와 관련하여, 도 14는 본 발명의 변형예에 따른 각도 판별기를 더 포함하는 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도이고, 도 15는 본 발명의 변형예에 따른 각도 판별기를 도시한 모식도이다. 도 14, 15에 도시된 바와 같이, 본 발명의 변형예에 따른 인공신경망을 이용한 입모양 합성 장치 는, 각도 판별기 및 각도 손실을 더 포함할 수 있다. 각도 판별기는 얼굴 상반 각도 검출 모듈, 얼굴 하반 각도 검출 모듈을 포함하도록 구성될 수 있다. 얼굴 상반 각도 검출 모듈은 합성영상 데이터에서 타겟 얼굴의 상반 이미지를 입력받고 Sigmoid, ReLU 등의 활성화 함수로 구성된 Output Layer에서 얼굴 상반 각도 벡터를 출력하는 복수개의 Convolution Layer를 포함한 기학습된 인공신경망으로 구성될 수 있고, 얼굴 하반 각도 검출 모듈은 합성영상 데이터에 서 타겟 얼굴의 하반 이미지를 입력받고 Sigmoid, ReLU 등의 활성화 함수로 구성된 Output Layer에서 얼굴 하반 각도 벡터를 출력하는 기학습된 인공신경망으로 구성될 수 있다. 합성영상 데이터에서 타겟 얼굴의 상반 이미지 생성 및 하반 이미지 생성은 얼굴 검출 모듈에 의해 수행될 수 있다. 각도 판별기의 학습 세션에서는 얼굴 각도(예, 얼굴 중심점에서 턱 방향의 3차원 벡터)에 대해 Labeling 된 학습 데이터를 각도 판별기에 입력하고, 얼굴 상반 각도 임베딩 모듈의 Output Layer에서 출력되는 얼 굴 상반 각도 벡터와 실제 Label과의 차이로 구성되는 얼굴 상반 각도 손실, 얼굴 하반 각도 임베딩 모듈의 Output Layer에서 출력되는 얼굴 하반 각도 벡터와 실제 Label과의 차이로 구성되는 얼굴 하반 각도 손실, 얼굴 상반 각도 벡터와 얼굴 하반 각도 벡터의 차이(예를 들어, 코사인 유사도)로 구성되는 각도 손실의 합을 저감시 키는 방향으로 각도 판별기의 얼굴 상반 각도 임베딩 모듈 및 얼굴 하반 각도 임베딩 모듈의 파라 미터가 업데이트 되도록 구성될 수 있다. 각도 판별기의 각도 손실은 코사인 유사도가 적용된 Mean square loss, Cross entropy loss 등으로 구성될 수 있다. 각도 판별기의 작동관계 관련하여, 각도 판별기에 합성영상 데이터가 입력되고, 얼굴 상반 각도 임베딩 모듈에서 얼굴 상반 각도 벡터, 얼굴 하반 각도 임베딩 모듈에서 얼굴 하반 각도 벡터가 각각 출력되며, 출력된 얼굴 상반 각도 벡터 및 얼굴 하반 각도 벡터를 기초로 각도 손실을 계산하도록 구성될 수 있 다. 이렇게 계산된 각도 손실은 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션에서 적용될 수 있다. 즉, 원본영상 인코더, 음성 인코더 및 합성영상 디코더는 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 합성영 상 판별기에서 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계 산된 싱크 손실 및 기학습된 각도 판별기에서 얼굴 상반 각도 벡터 및 얼굴 하반 각도 벡터를 기초로 계산 된 각도 손실의 합이 저감되는 방향으로 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라 미터를 업데이트하도록 학습 세션이 구성될 수 있다. 본 발명의 변형예에 따른 각도 판별기에 따르면, 합성되는 얼굴의 상하반이 바라보는 각도의 정합과 원근의 정합(얼굴 상반의 길이감와 얼굴 하반의 길이감)이 향상되는 효과가 발생된다. 이에 따르면, 원본영상에서 화자 의 얼굴 각도에 따라 어색하게 합성되던 기존의 입모양 합성 인공신경망에 비해 얼굴 상하반의 정합이 향상되는 효과가 발생된다. 상하 정합 판별기와 관련하여, 도 16은 본 발명의 변형예에 따른 상하 정합 판별기가 더 포함된 인공신 경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 17은 본 발명의 변형예에 따른 상하 정합 판별기 를 도시한 모식도이다. 도 16, 17에 도시된 바와 같이, 본 발명의 변형예에 따른 인공신경망을 이용한 입모양 합성 장치는, 상하 정합 판별기 및 상하 정합 손실을 더 포함할 수 있다. 상하 정합 판별기는 합성영상 데이터의 얼굴 상반 이미지와 얼굴 하반 이미지를 임베딩하여 입력 데이터로 입력받고, Sigmoid, ReLU 등의 활성화 함수로 구성된 Output Layer에서 얼굴 상하반의 시각적 특징(표정, 색감, 조명 영향 등)이 얼마나 정합되어 있는지에 대한 지표인 상하 정합 벡터(0~1사이의 확률값)를 출력 데이터로 출 력하는 복수개의 Convolution Layer를 포함한 기학습된 인공신경망 모듈이다. 합성영상 데이터에서 타겟 얼굴의 상반 이미지 생성 및 하반 이미지 생성은 얼굴 검출 모듈에 의해 수행될 수 있다. 상하 정합 판별기의 학습 세션과 관련하여, 도 18은 본 발명의 변형예에 따른 상하 정합 판별기의 학습 세션을 도시한 모식도이다. 도 18에 도시된 바와 같이, 상하 정합 판별기의 학습 세션에서는 상하 정합으로 Labeling 된 [얼굴 상반 이미지, 얼굴 하반 이미지] 및 상하 비정합으로 Labeling 된 [얼굴 상반 이미지, 얼굴 하반 이미지]를 학습 데이터로 입력하고, 상하 정합 판별기의 Output Layer에서 출력되는 상하 정합 벡터와 실제 Label과의 차이로 구성되는 상하 정합 손실을 저감시키는 방향으로 상하 정합 판별기의 파라미터가 업 데이트 되도록 구성될 수 있다. 상하 정합 판별기의 상하 정합 손실은 Mean square loss, Cross entropy loss 등으로 구성될 수 있다. 상하 정합 판별기의 작동관계 관련하여, 상하 정합 판별기에 합성영상 데이터의 얼굴 상반 이미지 및 얼굴 하반 이미지가 임베딩되어 입력 데이터로 입력되고, 상하 정합 벡터가 출력 데이터로 출력된다. 이렇게 출 력된 상하 정합 벡터는 원본영상 인코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트 하도록 학습 세션에서 적용될 수 있다. 즉, 원본영상 인코더, 음성 인코더 및 합성영상 디코더는 각 Epoch에 대하여 합성영상 데이터와 원본영상 데이터를 기초로 계산된 재구성 손실, 기학습된 합성영상 판별 기에서 합성영상 손실, 기학습된 싱크 판별기에서 합성영상 데이터와 음성 데이터를 기초로 계산된 싱 크 손실 및 기학습된 상하 정합 판별기에서 출력된 상하 정합 벡터의 합이 저감되는 방향으로 원본영상 인 코더, 음성 인코더 및 합성영상 디코더의 파라미터를 업데이트하도록 학습 세션이 구성될 수 있다. 본 발명의 변형예에 따른 상하 정합 판별기에 따르면, 합성되는 얼굴의 상하반의 표정, 색감, 조명 영향 등 의 시각적인 특징이 조화롭게 합성되는 효과가 발생된다. 이에 따르면, 화자의 얼굴 표정에도 입모양이 불구하 고 어색하게 합성되던 기존의 입모양 합성 인공신경망에 비해 얼굴 상하반의 표정, 색감, 조명 영향 등의 시각 적 정합이 향상되는 효과가 발생된다. 컨볼루션 필터의 랜덤 널링(nulling)과 관련하여, 도 19, 20은 본 발명의 변형예에 따른 컨볼루션 필터의 랜덤 널링(nulling)을 적용한 합성영상 임베딩 모듈를 도시한 모식도이다. 도 19, 20에 도시된 바와 같이, 본 발 명의 변형예에 따른 인공신경망을 이용한 입모양 합성 장치는, 싱크 판별기의 음성 임베딩 모듈 및/ 또는 합성영상 임베딩 모듈의 컨볼루션 필터(Convolution Filter)의 일부 또는 입력 벡터(음성 데이터/합성 영상 데이터)의 일부에 랜덤하게 널링(nulling)을 적용하여 출력 벡터를 출력한 뒤, 이 단계를 반복하여 각각 다른 랜덤 널링 컨볼루션 필터가 적용된 음성 싱크 벡터/합성영상 싱크 벡터를 복수개 출력하고, 출력된 복수개 의 출력 벡터들을 통합한 통합 벡터를 음성 임베딩 벡터/합성영상 임베딩 벡터로 구성할 수 있다. 구체적인 널링 방법과 관련하여, 컨볼루션 필터의 stride를 1로 두고 sliding window를 진행할 때마다 컨볼루션 필터에 랜덤하게 특징을 선정하는 방법으로 음성 임베딩 모듈 및/또는 합성영상 임베딩 모듈의 컨볼루 션 필터(Convolution Filter)의 일부에 랜덤하게 널링(nulling)을 적용하도록 구성될 수 있다(랜덤 널링 컨볼루 션 필터). 음성 임베딩 모듈 및/또는 합성영상 임베딩 모듈의 컨볼루션 필터(Convolution Filter)의 일 부에 랜덤하게 널링(nulling)을 적용하여 출력 벡터를 출력하고, 이 단계를 반복하여 복수의 출력 벡터를 출력 한 뒤, 복수의 출력 벡터를 통합한 통합 벡터를 음성 임베딩 벡터 및/또는 합성영상 임베딩 벡터로 구성할 수 있다. 이에 따르면, 음성의 전반적인 특징에 대하여 액티베이션 맵(Activation map)이 형성되고, 합성영상에서 입모양의 전반적인 특징에 대하여 액티베이션 맵(Activation map)이 형성되어 싱크 판별기의 정확도가 향상 되는 효과가 발생된다. 도 19에 도시된 바와 같이, 각각의 랜덤 널링 컨볼루션 필터에 의한 액티베이션 맵은 입 모양의 특징을 전반적으로 잡지 못하지만, 랜덤 널링 컨볼루션 필터에 의한 액티베이션 맵을 통합하면 입 모양 의 특징을 전반적으로 잘 확보하는 것을 확인할 수 있다. 또는, 입력 벡터(음성 데이터, 합성영상 데이터)의 일부에 랜덤하게 널링(nulling)을 적용하도록 구성될 수 있 다(랜덤 널링 음성 데이터, 랜덤 널링 합성영상 데이터). 이에 따르면, 컨볼루션 필터에 랜덤하게 널링을 적용 하는 경우에 비해, 입력 벡터 자체에 널링을 적용한 후 컨볼루션을 수행하게 되어 Random Feature Selection을 비교적 빠르게 수행할 수 있어 컴퓨팅 리소스가 저감되는 효과가 발생된다. 컨볼루션 필터에 랜덤하게 널링을 적용하는 경우에는 랜덤하게 널링이 적용된 복수의 컨볼루션 필터를 이용하여 sliding window를 수행하여야 하 는데, 이러한 방식은 일반적인 딥러닝 프레임워크(예를 들어, PyTorch, TensorFlow 등)에서 구현하기 어려운 단 점이 있다. 도 20에 도시된 바와 같이, 각각의 랜덤 널링 컨볼루션 필터에 의한 액티베이션 맵은 입 모양의 특 징을 전반적으로 잡지 못하지만, 랜덤 널링 입력 벡터에 의한 액티베이션 맵을 통합하면 입 모양의 특징을 전반 적으로 잘 확보하는 것을 확인할 수 있다. 인공신경망을 이용한 입모양 합성 방법과 관련하여, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합 성 방법(추론 세션)은 원본영상 인코딩 단계, 음성 인코딩 단계, 합성영상 디코딩 단계를 포함할 수 있다. 원본영상 인코딩 단계는, 원본영상 인코더가 합성 타겟 얼굴을 포함하는 영상으로서 입모양 합성의 대상이 되는 원본영상 데이터를 인코딩하여 원본영상 임베딩 벡터를 출력하는 단계이다. 음성 인코딩 단계는, 음성 인코더가 입모양 합성의 기초가 되는 음성 데이터를 인코딩하여 음성 임베딩 벡 터를 출력하는 단계이다. 합성영상 디코딩 단계는, 합성영상 디코더가 상기 원본영상 임베딩 벡터 및 상기 음성 임베딩 벡터를 입력 데이터로 하고, 상기 합성 타겟 얼굴에 상기 음성 데이터에 대응되는 입모양이 합성된 합성영상 데이터를 출력 하는 단계이다. 인공신경망을 이용한 실시간 방송 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 실시간 방송 입모양 합성 장치와 관련하여, 도 21은 본 발명의 일실시예에 따른 인공신경망을 이용한 실시간 방 송 입모양 합성 장치를 도시한 모식도이다. 도 21에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망 을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 실시간 방송 입모양 합성 장치는 기자회견이나 인터 뷰와 같은 방송영상을 실시간으로 다른 언어로 송출하기 위한 장치로서, 기자회견이나 인터뷰와 같은 원본방송 영상 데이터(원본영상 데이터에 대응됨), 통역가 또는 통역 모듈에 의해 실시간으로 생성되는 통역 음성 데이터 (음성 데이터에 대응됨)를 입력 데이터로 하고, 원본방송영상 데이터에서 통역 음성 데이터에 대응되는 입모양 으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 더빙 영화 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 더빙 영화 입모양 합성 장치와 관련하여, 도 22는 본 발명의 일실시예에 따른 인공신경망을 이용한 더빙 영화 입모양 합성 장치를 도시한 모식도이다. 도 22에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 더빙 영화 입모양 합성 장치는 더빙 영화의 현실감을 증대시키기 위하여 더빙 음성과 영화 화자의 입모양이 매칭되도록 합성하는 장치에 관한 것으로, 영화 영상의특정 프레임을 의미하는 원본영화영상 데이터(원본영상 데이터에 대응됨), 성우 또는 TTS 모듈에 의해 생성된 더빙 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본영화영상 데이터에서 더빙 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 온라인 강의 현지화 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 온라인 강의 현지화 장치와 관련하여, 도 23은 본 발명의 일실시예에 따른 인공신경망을 이용한 온라인 강의 현 지화 장치를 도시한 모식도이다. 도 23에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 온라인 강의 현지화 장치는 특정 언어로 촬영된 온라인 강의를 다른 언어의 음성 및 입모양으로 합성하기 위한 장치로서, 온라인 강의와 같은 원본강의영상 데이터(원본영상 데이터에 대응됨), TTS 모듈에 의해 원본강의영상 데이터와 다른 언어의 음성으로 생성되는 TTS 음성 데이터(음 성 데이터에 대응됨)를 입력 데이터로 하고, 원본강의영상 데이터에서 TTS 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치와 관련하여, 도 24는 본 발명의 일실시예에 따른 인공신경망을 이용한 인 공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치를 도시한 모식도이다. 도 24에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 애니메이션 캐릭 터 입모양 합성 장치는 애니메이션 캐릭터의 입모양과 음성을 최대한 동기화하여 애니메이션 CG 비용을 최대한 절감하기 위한 장치로서, 원본 애니메이션인 원본애니메이션 데이터(원본영상 데이터에 대응됨), 원본애니메이 션 데이터의 해당 프레임에 대응되는 스크립트의 대사 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하 고, 원본애니메이션 데이터에서 대사 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데 이터로 하도록 구성될 수 있다. 인공신경망을 이용한 화상 통화 끊김 시 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 화상 통화 끊김 시 입모양 합성 장치와 관련하여, 도 25는 본 발명의 일실시예에 따른 인공신경망을 이용한 화 상 통화 끊김 시 입모양 합성 장치를 도시한 모식도이다. 도 25에 도시된 바와 같이, 본 발명의 일실시예에 따 른 인공신경망을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 화상 통화 끊김 시 입모양 합성 장치 는 화상 통화 시 네트워크 송수신 불량으로 종종 발생되는 화상통화 영상 끊김 현상이 발생되었을 때 통화 음성 만으로 화상통화 영상을 실시간으로 생성해내어 심리스한 화상통화 사용자 경험을 창출하기 위한 장치로서, Apple사의 Facetime, Kakao사의 Voicetalk 등과 같은 원본화상통화 데이터(원본영상 데이터에 대응됨), 화상통 화 상대방에 의해 실시간으로 생성되는 통화 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본화 상통화 데이터에서 통화 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도 록 구성될 수 있다. 인공신경망을 이용한 다자간 화상 회의 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 다자간 화상 회의 입모양 합성 장치와 관련하여, 도 26은 본 발명의 일실시예에 따른 인공신경망을 이용한 다자 간 화상 회의 입모양 합성 장치를 도시한 모식도이다. 도 26에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 다자간 화상 회의 입모양 합성 장치는 다 자간 화상 회의에서 프로필 사진 만으로 화상회의를 진행할 수 있도록 프로필 사진에서 입모양이 합성된 합성영 상을 생성해주는 장치로서, 화상회의 플랫폼에서의 원본 프로필 사진 데이터(원본영상 데이터에 대응됨), 해당 화자에 의해 실시간으로 생성되는 회의 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본 프로필 사진 데이터에서 회의 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록구성될 수 있다. 인공신경망을 이용한 게임 캐릭터 입모양 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 게임 캐릭터 입모양 합성 장치와 관련하여, 도 27은 본 발명의 일실시예에 따른 인공신경망을 이용한 게임 캐릭 터 입모양 합성 장치를 도시한 모식도이다. 도 27에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망 을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 게임 캐릭터 입모양 합성 장치는 어드벤처 게임, 아 케이드 게임, 스포츠 게임, MMORPG, 롤플레잉 게임, 시뮬레이션 게임, 퍼즐 게임 등의 각종 게임 콘텐츠 내에서 특정 캐릭터의 대사가 수행되는 지점에서 대사에 대응되는 입모양이 합성된 게임 캐릭터의 얼굴을 생성하는 장 치에 관한 것으로서, 원본게임캐릭터 데이터(원본영상 데이터에 대응됨), 성우 또는 TTS 모듈에 의해 생성된 대 사 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본게임캐릭터 데이터에서 대사 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 뮤직비디오 현지화 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 뮤직비디오 현지화 장치와 관련하여, 도 28은 본 발명의 일실시예에 따른 인공신경망을 이용한 뮤직비디오 현지 화 장치를 도시한 모식도이다. 도 28에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이용한 입 모양 합성 장치로 구성된 인공신경망을 이용한 뮤직비디오 현지화 장치는 특정 언어로 촬영된 뮤직비디오를 다 른 언어의 노래 및 입모양으로 합성하기 위한 장치로서, 원본뮤직비디오 데이터(원본영상 데이터에 대응됨), 가 수에 의해 생성된 다른 언어 노래 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본뮤직비디오 데이터 에서 다른 언어 노래 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 정치인 홍보영상 합성 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 정치인 홍보영상 합성 장치와 관련하여, 도 29는 본 발명의 일실시예에 따른 인공신경망을 이용한 정치인 홍보 영상 합성 장치를 도시한 모식도이다. 도 29에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망을 이 용한 입모양 합성 장치로 구성된 인공신경망을 이용한 정치인 홍보영상 합성 장치는 정치인이 실제로 수신인의 이름을 호명하는 것과 같은 합성영상을 생성하기 위한 장치에 관한 것으로서, 특정 정치인의 홍보영상인 원본 정치인 홍보영상 데이터(원본영상 데이터에 대응됨), TTS 모듈에 수신자의 이름을 입력하여 생성된 수신자 이름 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본 정치인 홍보영상 데이터에서 수신자 이름 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 일실시예에 해당되는 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치와 관련하여, 도 30은 본 발명의 일실시예에 따른 인공신경망을 이용한 사용자 맞 춤형 광고 출력 장치를 도시한 모식도이다. 도 30에 도시된 바와 같이, 본 발명의 일실시예에 따른 인공신경망 을 이용한 입모양 합성 장치로 구성된 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치는 광고영상 내에서 실제로 피광고인의 이름을 호명하는 것과 같은 합성영상을 생성하기 위한 장치에 관한 것으로서, 특정 브랜드의 광고영상인 원본 광고영상 데이터(원본영상 데이터에 대응됨), TTS 모듈에 피광고자의 이름을 입력하여 생성된 피광고자 이름 음성 데이터(음성 데이터에 대응됨)를 입력 데이터로 하고, 원본 광고영상 데이터에서 피광고자 이름 음성 데이터에 대응되는 입모양으로 합성이 된 합성영상 데이터를 출력 데이터로 하도록 구성될 수 있다. 이상에서 설명한 바와 같이, 본 발명이 속하는 기술 분야의 통상의 기술자는 본 발명이 그 기술적 사상이나 필 수적 특징을 변경하지 않고서 다른 구체적인 형태로 실시될 수 있다는 것을 이해할 수 있을 것이다. 그러므로상술한 실시예들은 모든 면에서 예시적인 것이며 한정적인 것이 아닌 것으로서 이해해야만 한다. 본 발명의 범 위는 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 등가 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 발명의 범위에 포함하는 것으로 해석되어야 한다. 본 명세서 내에 기술된 특징들 및 장점들은 모두를 포함하지 않으며, 특히 많은 추가적인 특징들 및 장점들이 도면들, 명세서, 및 청구항들을 고려하여 당업자에게 명백해질 것이다. 더욱이, 본 명세서에 사용된 언어는 주 로 읽기 쉽도록 그리고 교시의 목적으로 선택되었고, 본 발명의 주제를 묘사하거나 제한하기 위해 선택되지 않 을 수도 있다는 것을 주의해야 한다. 본 발명의 실시예들의 상기한 설명은 예시의 목적으로 제시되었다. 이는 개시된 정확한 형태로 본 발명을 제한 하거나, 빠뜨리는 것 없이 만들려고 의도한 것이 아니다. 당업자는 상기한 개시에 비추어 많은 수정 및 변형이 가능하다는 것을 이해할 수 있다. 그러므로 본 발명의 범위는 상세한 설명에 의해 한정되지 않고, 이를 기반으로 하는 출원의 임의의 청구항들에 의해 한정된다. 따라서, 본 발명의 실시예들의 개시는 예시적인 것이며, 이하의 청구항에 기재된 본 발명의 범 위를 제한하는 것은 아니다."}
{"patent_id": "10-2020-0188203", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 명세서에 첨부되는 다음의 도면들은 본 발명의 바람직한 실시예를 예시하는 것이며, 발명의 상세한 설명과 함께 본 발명의 기술사상을 더욱 이해시키는 역할을 하는 것이므로, 본 발명은 그러한 도면에 기재된 사항에만 한정되어 해석되어서는 아니 된다. 도 1, 2는 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 사용상태를 도시한 모식도, 도 3은 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 4는 본 발명의 일실시예에 따른 인공신경망을 이용한 입모양 합성 장치의 구체적인 구조를 도시한 모식도, 도 5는 본 발명의 일실시예에 따른 원본영상 인코더의 구조를 도시한 모식도, 도 6은 얼굴 검출 모듈과 원본영상 인코더의 작동관계를 도시한 모식도, 도 7은 본 발명의 일실시예에 따른 싱크 판별기를 도시한 모식도, 도 8은 본 발명의 변형예에 따른 얼굴 판별기를 더 포함하는 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 9는 본 발명의 변형예에 따른 얼굴 판별기의 학습 세션을 도시한 모식도, 도 10은 본 발명의 변형예에 따른 감정 벡터의 결합을 도시한 모식도, 도 11은 본 발명의 변형예에 따른 감정 판별기를 더 포함하는 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 12는 본 발명의 변형예에 따른 입크기 판별기가 더 포함된 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도, 도 13은 본 발명의 변형예에 따른 입크기 판별기를 도시한 모식도, 도 14는 본 발명의 변형예에 따른 각도 판별기를 더 포함하는 인공신경망을 이용한 입모양 합성 장치를 도시한 모식도이고, 도 15는 본 발명의 변형예에 따른 각도 판별기를 도시한 모식도, 도 16은 본 발명의 변형예에 따른 상하 정합 판별기가 더 포함된 인공신경망을 이용한 입모양 합성 장치 를 도시한 모식도, 도 17은 본 발명의 변형예에 따른 상하 정합 판별기를 도시한 모식도, 도 18은 본 발명의 변형예에 따른 상하 정합 판별기의 학습 세션을 도시한 모식도,도 19, 20은 본 발명의 변형예에 따른 컨볼루션 필터의 랜덤 널링(nulling)을 적용한 합성영상 임베딩 모듈(4 2)를 도시한 모식도, 도 21은 본 발명의 일실시예에 따른 인공신경망을 이용한 실시간 방송 입모양 합성 장치를 도시한 모식도, 도 22는 본 발명의 일실시예에 따른 인공신경망을 이용한 더빙 영화 입모양 합성 장치를 도시한 모식도, 도 23은 본 발명의 일실시예에 따른 인공신경망을 이용한 온라인 강의 현지화 장치를 도시한 모식도, 도 24는 본 발명의 일실시예에 따른 인공신경망을 이용한 인공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치를 도시한 모식도, 도 25는 본 발명의 일실시예에 따른 인공신경망을 이용한 화상 통화 끊김 시 입모양 합성 장치를 도시한 모식도, 도 26은 본 발명의 일실시예에 따른 인공신경망을 이용한 다자간 화상 회의 입모양 합성 장치를 도시한 모식도, 도 27은 본 발명의 일실시예에 따른 인공신경망을 이용한 게임 캐릭터 입모양 합성 장치를 도시한 모식도, 도 28은 본 발명의 일실시예에 따른 인공신경망을 이용한 뮤직비디오 현지화 장치를 도시한 모식도, 도 29는 본 발명의 일실시예에 따른 인공신경망을 이용한 정치인 홍보영상 합성 장치를 도시한 모식도, 도 30은 본 발명의 일실시예에 따른 인공신경망을 이용한 사용자 맞춤형 광고 출력 장치를 도시한 모식도이다."}
