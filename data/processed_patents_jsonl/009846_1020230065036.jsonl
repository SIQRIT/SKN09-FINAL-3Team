{"patent_id": "10-2023-0065036", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0167249", "출원번호": "10-2023-0065036", "발명의 명칭": "Edge TPU에서 실시간 신경망 작업을 스케줄링하는 방법 및 장치", "출원인": "아주대학교산학협력단", "발명자": "오상은"}}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "실시간 신경망 작업을 스케줄링하는 방법에 있어서,적어도 하나의 프로세서를 구비하는 실시간 시스템이 처리해야 하는 복수 개의 신경망 모델을 입력받는 단계;상기 실시간 시스템이 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는신경망 모델 각각에 겹치지 않는 영역의 메모리를 할당하여 신경망 작업을 생성하는 단계; 및상기 실시간 시스템이 생성된 상기 신경망 작업을 동시에 실행하도록 유도하는 단계;를 포함하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 메모리를 할당하는 단계는,주어진 데드라인 조건에 따라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신또는 다른 신경망 모델에 할당될 메모리를 조절하는 단계; 및동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정하는 단계;를 포함하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 신경망 모델에 할당될 메모리를 조절하는 단계는,주어진 데드라인 조건에 따라 더미 모델을 개별 신경망 모델의 앞 또는 뒤 중 적어도 하나의 위치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모델을 활용한 코-컴파일을 반복함으로써 실행시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리(binary) 파일을 생성하도록 유도하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2 항에 있어서,상기 캐싱 토큰을 동일하게 설정하는 단계는,상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리해야 하는 신경망 작업들의 캐싱 토큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "실시간 신경망 작업을 스케줄링하는 방법에 있어서,적어도 하나의 프로세서를 구비하는 실시간 시스템이 처리해야 하는 복수 개의 신경망 모델을 입력받는 단계;상기 실시간 시스템이 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정하는 단계;상기 실시간 시스템이 작업의 선점(preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그먼트(segment)로분할하는 단계; 및상기 실시간 시스템이 상기 메모리 할당 및 상기 세그먼트 분할에 따라 생성된 신경망 작업을 우선순위를 고려공개특허 10-2024-0167249-3-하여 실행하도록 유도하는 단계;를 포함하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 메모리 할당을 결정하는 단계는,주어진 데드라인 조건에 따라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신또는 다른 신경망 모델에 할당될 메모리를 조절하는 단계; 및동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정하는 단계;를 포함하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 신경망 모델에 할당될 메모리를 조절하는 단계는,주어진 데드라인 조건에 따라 더미 모델을 개별 신경망 모델의 앞 또는 뒤 중 적어도 하나의 위치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모델을 활용한 코-컴파일을 반복함으로써 실행시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리(binary) 파일을 생성하도록 유도하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 6 항에 있어서,상기 캐싱 토큰을 동일하게 설정하는 단계는,상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리해야 하는 신경망 작업들의 캐싱 토큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 5 항에 있어서,상기 신경망 모델을 복수 개의 세그먼트로 분할하는 단계는,신경망 모델을 복수 개의 세그먼트로 분할하여 상기 세그먼트들 간의 경계에서 선점을 허용하는 선점 지점(preemption point)을 생성하되, 생성된 상기 선점 지점마다 신경망 작업 간의 스케줄링을 수행함으로써 우선순위에 따른 선점 스케줄링을 유도하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 신경망 모델을 복수 개의 세그먼트로 분할하는 단계는,우선순위가 높은 작업의 차단 지연이 우선순위가 낮은 작업에서 세그먼트를 실행하는 최대 시간으로 제한되도록수행되는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 9 항에 있어서,상기 신경망 모델을 복수 개의 세그먼트로 분할하는 단계는,신경망 모델의 후보 레이어(layer) 중 세그먼트의 출력 데이터를 복사할 때 발생하는 오버헤드가 가장 적은 레이어를 각 세그먼트에 대한 세그먼트 경계로 선택하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2024-0167249-4-제 5 항에 있어서,상기 신경망 작업을 우선순위를 고려하여 실행하도록 유도하는 단계는,시스템 전역(system-wide) 스케줄러가 우선순위 큐(priority queue)를 통해 신경망 작업들의 실행 요청들을 우선순위에 따라 순서대로 정렬하는 단계; 및최우선순위의 작업을 상기 큐에서 가져와 시스템 자원을 선점하여 세그먼트 단위로 바이너리 파일을 실행하는단계;를 포함하는, 작업 스케줄링 방법."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항 내지 제 12 항 중에 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "실시간 신경망 작업을 스케줄링하는 장치에 있어서,처리해야 하는 복수 개의 신경망 모델을 입력받는 입력부;입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정하는 메모리 할당부;작업의 선점(preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그먼트(segment)로 분할하는 신경망 모델분할부;메모리 할당 및 세그먼트 분할에 따라 상기 신경망 모델로부터 바이너리 파일로 구성된 신경망 작업을 생성하는컴파일러(complier); 및생성된 신경망 작업을 우선순위를 고려하여 적어도 하나의 프로세서를 통해 실행하도록 유도하는 시스템 전역(system-wide) 스케줄러;를 포함하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서,상기 메모리 할당부는,주어진 데드라인 조건에 따라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신또는 다른 신경망 모델에 할당될 메모리를 조절하고,동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 메모리 할당부는,주어진 데드라인 조건에 따라 더미 모델을 개별 신경망 모델의 앞 또는 뒤 중 적어도 하나의 위치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모델을 활용한 코-컴파일을 반복함으로써 실행시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리(binary) 파일을 생성하도록 유도하고,상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리해야 하는 신경망 작업들의 캐싱 토큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 14 항에 있어서,공개특허 10-2024-0167249-5-상기 신경망 모델 분할부는,신경망 모델을 복수 개의 세그먼트로 분할하여 상기 세그먼트들 간의 경계에서 선점을 허용하는 선점 지점(preemption point)을 생성하되, 생성된 상기 선점 지점마다 신경망 작업 간의 스케줄링을 수행함으로써 우선순위에 따른 선점 스케줄링을 유도하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 신경망 모델 분할부는,우선순위가 높은 작업의 차단 지연이 우선순위가 낮은 작업에서 세그먼트를 실행하는 최대 시간으로 제한되도록수행되고,신경망 모델의 후보 레이어(layer) 중 세그먼트의 출력 데이터를 복사할 때 발생하는 오버헤드가 가장 적은 레이어를 각 세그먼트에 대한 세그먼트 경계로 선택하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 14 항에 있어서,상기 시스템 전역 스케줄러는,우선순위 큐(priority queue)를 통해 신경망 작업들의 실행 요청들을 우선순위에 따라 순서대로 정렬하고,최우선순위의 작업을 상기 큐에서 가져와 시스템 자원을 선점하여 세그먼트 단위로 바이너리 파일을 실행하도록유도하는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 14 항에 있어서,상기 컴파일러 및 상기 프로세서는,Edge TPU(Edge Tensor Processing Units)에 기반하여 구동되는, 작업 스케줄링 장치."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서는 신경망 작업을 위한 스케줄링 기술에 관한 것으로, 실시간 신경망 작업을 스케줄링하는 방법은, 처 리해야 하는 복수 개의 신경망 모델을 입력받고, 입력된 신경망 모델 각각의 데드라인(deadline)을 고려하여 동 시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리를 할당하여 신경망 작업을 생성하며, 생성된 상기 신경망 작업을 동시에 실행하도록 유도한다."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서는 신경망 작업을 위한 스케줄링 기술에 관한 것으로, 특히 Edge TPU(Edge Tensor Processing Unit s)에서 실시간 보장을 위한 신경망 작업을 스케줄링하는 방법, 그 방법을 기록한 기록매체 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 몇 년간 심층 신경망(deep neural network, DNN) 알고리즘이 기계학습 기반 응용 서비스의 핵심 기술로 자 리 잡으면서 DNN 모델이 안전필수 실시간 시스템에 적용되는 사례가 늘어나고 있다. 예컨대, 사물인터넷 (internet of things, IoT) 기반 재난 안전 시스템에서는 재난 예측, 현장 모니터링 및 분석, 각종 액추에이터 (actuator) 제어 등을 위한 다양한 DNN 추론 애플리케이션들이 활용된다. 이와 같은 시스템에서는 각 애플리케 이션이 높은 추론 정확도로 동작해야 할 뿐만 아니라, 주어진 데드라인(deadline) 내에 실행을 완료하는 것도 매우 중요하다. 만약 각 DNN 추론 애플리케이션의 마감 시간이 정확히 지켜지지 않으면 재난 상황에 시기적절하 게 대응할 수 없게 되며, 이는 막대한 인명/재산 피해로 이어질 수 있다. 이러한 관점에서 엣지 컴퓨팅(edge computing) 패러다임은 DNN의 실시간성을 보장하기 위한 효과적인 접근법으 로 제시되었다. 원격 서버에서 DNN 추론 작업(task)을 실행시키는 기존 방식과 달리, 이 패러다임에서는 인공지 능 프로세서를 활용하여 단말 장치에서 직접 DNN 작업들을 실행시킨다. 이런 방식은 기존에 작업 실행 과정에서 발생하던 네트워크 지연을 근본적으로 피할 수 있기 때문에, 각 작업의 최악 실행시간(worst-case execution time, WCET)을 좀 더 쉽게 예측할 수 있으며, 결과적으로 DNN의 실시간성 보장에 더 유리하다. 이러한 이점으로 최근 엣지 컴퓨팅 환경에서 실시간 DNN 실행을 지원하기 위한 기법들이 이하의 선행기술문헌과 같이연구되었다. 이와 관련하여 온-디바이스 DNN 작업 부하가 급격히 증가함에 따라 특수 하드웨어 가속기는 성능과 에너지 효율 을 모두 개선할 수 있는 매력적인 솔루션이 되고 있다. 예를 들어, Google Edge TPU(Edge Tensor Processing Units) 및 Apple Neural Engines는 이미 상용 임베디드 시스템의 필수 하드웨어 구성 요소로 제시되었다. 그러 나, 특수한 DNN 가속기의 출현에도 불구하고, DNN 작업에서 길고 예측할 수 없는 실행 시간을 유발하여 실시간 시스템을 통한 배포를 제한하는 몇 가지 요인이 여전히 존재한다. 특히, 이하의 선행기술문헌을 통해 소개한 대부분의 연구들은 Nvidia Jetson이라는 인공지능 전용 임베디드 보 드 환경에서 GPU, CPU 등 컴퓨팅 자원을 어떻게 관리하고, 작업들을 어떻게 스케줄링할 것인지에 주로 초점을 맞추어 DNN 작업들의 실시간성을 보장하고자 하였다. 반면, 또 다른 대표적인 인공지능 프로세서인 Edge TPU에 대한 실시간성 보장 연구는 전무한 실정이다. Edge TPU는 Nvidia Jetson의 GPU보다 그 크기가 훨씬 작아 전력 효율이 좋으며, 가격이 낮아 여러 IoT 기기에 부착되 어 사용되고 있지만, 근본적으로 CPU, GPU와는 다른 아키텍처 특징을 지니기 때문에, 기존 연구들에서 제안한 실시간 기법들을 Edge TPU에 그대로 적용할 수 없다. 다시 말해, Edge TPU의 구조적 특징을 심도 있게 고려하고 이에 맞는 실시간성 지원 기법 개발이 시급히 필요하다. 선행기술문헌 비특허문헌 (비특허문헌 0001) H. Zhou, S. Bateni and C. Liu, \"S3 DNN: Supervised streaming and scheduling for gpu-accelerated real-time dnn workloads,\" 2018 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS), pp.190-201, 2018. (비특허문헌 0002) Y. Xiang and H. Kim, \"Pipelined data-parallel CPU/GPU scheduling for multi-DNN real-time inference,\" 2019 IEEE Real-Time Systems Symposium (RTSS), pp.392-405, 2019."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서의 실시예들이 해결하고자 하는 기술적 과제는, 종래의 실시간 시스템에서 컴퓨팅 자원을 관리하거나 작업들을 스케줄링함에 있어서 단지 메모리 접근 빈도에 비례하여 자원을 할당함으로 인해 각 작업의 데드라인 을 고려하지 못하였다는 약점을 극복하고, 또한 복수 개의 신경망 작업의 실행 요청이 수신되는 경우 작업의 우 선순위에 대한 고려없이 단지 선입선출(first-in first-out, FIFO) 원칙에 따라 비선점 방식으로 처리함으로 인 해 높은 우선순위를 갖는 작업이 상대적으로 낮은 우선순위를 갖는 작업을 기다려야 하는 우선순위 역전 (priority inversion) 현상이 발생하는 문제를 해소하고자 한다."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위하여, 본 명세서의 일 실시예에 따른 실시간 신경망 작업을 스케줄링하는 방법 은, 적어도 하나의 프로세서를 구비하는 실시간 시스템이 처리해야 하는 복수 개의 신경망 모델을 입력받는 단 계; 상기 실시간 시스템이 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하 는 신경망 모델 각각에 겹치지 않는 영역의 메모리를 할당하여 신경망 작업을 생성하는 단계; 및 상기 실시간 시스템이 생성된 상기 신경망 작업을 동시에 실행하도록 유도하는 단계;를 포함한다. 일 실시예에 따른 작업 스케줄링 방법에서, 상기 메모리를 할당하는 단계는, 주어진 데드라인 조건에 따라 신경 망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메모리를 조절하는 단계; 및 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정하는 단계;를 포함할 수 있다. 상기 기술적 과제를 해결하기 위하여, 본 명세서의 다른 실시예에 따른 실시간 신경망 작업을 스케줄링하는 방 법은, 적어도 하나의 프로세서를 구비하는 실시간 시스템이 처리해야 하는 복수 개의 신경망 모델을 입력받는 단계; 상기 실시간 시스템이 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야하는 신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정하는 단계; 상기 실시간 시스템이 작업의 선점 (preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그먼트(segment)로 분할하는 단계; 및 상기 실시간 시스템이 상기 메모리 할당 및 상기 세그먼트 분할에 따라 생성된 신경망 작업을 우선순위를 고려하여 실행하도 록 유도하는 단계;를 포함한다. 다른 실시예에 따른 작업 스케줄링 방법에서, 상기 메모리 할당을 결정하는 단계는, 주어진 데드라인 조건에 따 라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메모리를 조절하는 단계; 및 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자 인 캐싱 토큰(caching token)을 동일하게 설정하는 단계;를 포함할 수 있다. 다른 실시예에 따른 작업 스케줄링 방법에서, 상기 신경망 모델을 복수 개의 세그먼트로 분할하는 단계는, 신경 망 모델을 복수 개의 세그먼트로 분할하여 상기 세그먼트들 간의 경계에서 선점을 허용하는 선점 지점 (preemption point)을 생성하되, 생성된 상기 선점 지점마다 신경망 작업 간의 스케줄링을 수행함으로써 우선순 위에 따른 선점 스케줄링을 유도할 수 있다. 다른 실시예에 따른 작업 스케줄링 방법에서, 상기 신경망 작업을 우선순위를 고려하여 실행하도록 유도하는 단 계는, 시스템 전역(system-wide) 스케줄러가 우선순위 큐(priority queue)를 통해 신경망 작업들의 실행 요청들 을 우선순위에 따라 순서대로 정렬하는 단계; 및 최우선순위의 작업을 상기 큐에서 가져와 시스템 자원을 선점 하여 세그먼트 단위로 바이너리 파일을 실행하는 단계;를 포함할 수 있다. 나아가, 이하에서는 상기 기재된 스케줄링 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽 을 수 있는 기록매체를 제공한다. 상기 기술적 과제를 해결하기 위하여, 본 명세서의 또 다른 실시예에 따른 실시간 신경망 작업을 스케줄링하는 장치는, 처리해야 하는 복수 개의 신경망 모델을 입력받는 입력부; 입력된 상기 신경망 모델 각각의 데드라인 (deadline)을 고려하여 동시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정하는 메모리 할당부; 작업의 선점(preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그먼트(segment)로 분할 하는 신경망 모델 분할부; 메모리 할당 및 세그먼트 분할에 따라 상기 신경망 모델로부터 바이너리 파일로 구성 된 신경망 작업을 생성하는 컴파일러(complier); 및 생성된 신경망 작업을 우선순위를 고려하여 적어도 하나의 프로세서를 통해 실행하도록 유도하는 시스템 전역(system-wide) 스케줄러;를 포함한다. 또 다른 실시예에 따른 작업 스케줄링 장치에서, 상기 메모리 할당부는, 주어진 데드라인 조건에 따라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메 모리를 조절하고, 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰 (caching token)을 동일하게 설정할 수 있다. 또 다른 실시예에 따른 작업 스케줄링 장치에서, 상기 신경망 모델 분할부는, 신경망 모델을 복수 개의 세그먼 트로 분할하여 상기 세그먼트들 간의 경계에서 선점을 허용하는 선점 지점(preemption point)을 생성하되, 생성 된 상기 선점 지점마다 신경망 작업 간의 스케줄링을 수행함으로써 우선순위에 따른 선점 스케줄링을 유도할 수 있다. 또 다른 실시예에 따른 작업 스케줄링 장치에서, 상기 시스템 전역 스케줄러는, 우선순위 큐(priority queue)를 통해 신경망 작업들의 실행 요청들을 우선순위에 따라 순서대로 정렬하고, 최우선순위의 작업을 상기 큐에서 가 져와 시스템 자원을 선점하여 세그먼트 단위로 바이너리 파일을 실행하도록 유도할 수 있다."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기된 본 명세서의 실시예들에 따르면, 실시간 신경망 작업들을 스케줄링함에 있어서 각 작업의 데드라인을 고 려하여 유연하게 메모리 자원을 할당하였고 모델 분할을 통해 우선순위에 따른 선점이 가능하도록 스케줄링 동 작을 개선함으로써, 실시간 시스템의 데드라인 준수가 가능해짐과 더불어 우선순위의 역전을 방지할 수 있으며, 특히 SRAM 할당을 위해 더미 모델을 도입하고 캐싱 토큰을 수정함으로써 상용 DNN 가속기인 Edge TPU가 갖는 기 술적 장벽 및 소스 비공개의 제약을 해소하여 높은 효율의 스케줄링 가능성을 달성할 수 있다."}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서의 실시예들을 구체적으로 설명하기에 앞서, Edge TPU에 기반하여 구현되는 실시간 시스템에서 신경망 작업의 실행 과정을 조사하고, 실시간 신경망 작업에 대한 최악의 경우(worst-case)의 타이밍 보장을 제공하는 약점을 살펴보기로 한다. 이후, 이러한 약점을 극복하기 위해 안출된 본 명세서의 실시예들이 제안하는 핵심 아 이디어와 이에 수반된 기술적 수단을 순차적으로 소개하도록 한다. Edge TPU의 배경 Edge TPU는 엣지에서 DNN 작업을 실행하도록 Google에서 설계한 하드웨어 가속기이다. Edge TPU는 PCI 버스를 통해 호스트 메모리(즉, DRAM)와 연결되며 내부 아키텍처는 처리 요소(processing elements, PE)와 SRAM이라는 캐시 메모리로 구성된다. PE는 2차원 배열 구조로 구성되어 DNN 모델에서 산술 연산을 병렬 처리할 수 있다. 8MB 크기의 장치 메모리인 SRAM은 모델의 매개변수 데이터를 캐시할 수 있으므로 호스트 메모리에서 매개변수 데이터를 가져오는 것보다 빠른 추론이 가능한다. Edge TPU는 오프라인(offline)과 온라인(online)의 두 단계로 DNN 작업을 실행한다. 오프라인 단계에서, Edge TPU 컴파일러(compiler)는 작업의 DNN 모델을 Edge TPU와 호환되는 바이너리(binary) 파일로 컴파일한다. 컴파 일러는 또한 레이서-수준 세분성에서 SRAM에 캐시할 모델 매개변수 데이터의 특정 양을 결정한다. 온라인 단계 에서, DNN 작업은 Edge TPU에서 다음 단계에 따라 생성된 바이너리 파일을 실행한다: (i) 업로드 복사 (Upload Copy): 입력 데이터는 PCI 버스를 통해 호스트에서 장치 메모리로 전송되고, (ii) 추론 실행 (Run Inference): 복사된 데이터로 추론을 수행하고 출력 데이터를 장치 메모리에 저장하며, 및 (iii) 다운로드 복사 (Download Copy): 추론이 완료된 후 출력 데이터가 다시 호스트 메모리에 복사된다. 단계 (ii)에서, 캐시되지 않은 모델 매개변수 데이터(있는 경우)는 요청 시 업로드 복사가 필요하지만 캐시된 모델 매개변수 데이터는 첫 번째 추론 을 제외한 모든 연속 추론을 위해 업로드 복사가 필요하지 않다. 여러 DNN 작업이 Edge TPU에서 실행될 때 컴파일러는 SRAM을 공유하여 매개 변수 데이터를 함께 캐시할 수 있는 코-컴파일(co-compilation)을 지원한다. 각 작업에 독점적으로 할당된 SRAM의 양은 컴파일 시간에 고정되며 SRAM에서의 위치는 컴파일러 명령에 작업이 나타나는 순서대로 순차적으로 결정된다. 여러 작업의 모든 모델 매 개변수 데이터가 SRAM에 맞지 않으면 일부 데이터만 SRAM에 캐시되고 각 계층이 데이터에 액세스하는 빈도에 따 라 우선순위가 지정된다. 이제 타이밍 제약을 위반하지 않고 가능한 한 많은 실시간 DNN 작업을 수용하기 위해 Edge TPU의 효율적인 사용 에 대한 두 가지 중요한 장애 요인을 특정한다.장애 요인 #1: 데드라인을 고려하지 않는 SRAM 할당 Edge TPU 컴파일러는 작업의 데드라인과 상관없이 자주 액세스하는 레이어에 SRAM을 할당하는 데드라인을 고려 하지 않는 SRAM 할당 방식을 사용하기 때문에 실시간 DNN 작업의 타이밍 제약을 고려하지 못한다. 이러한 데드 라인을 무시하는 SRAM 할당 방식은 타이밍 제약을 보장하는 데 효율적이지 않다. 이는 Edge TPU에서 DNN 작업의 실행 시간이 SRAM에 캐시되는 모델 매개변수 데이터의 양에 따라 크게 달라지기 때문이다. 이 문제를 입증하기 위해 Google Coral 개발 보드의 Edge TPU에서 대표적인 DNN 모델(EfficientNet-{L,M,S}, MobileNetV2 및 ResNet)의 실행 시간을 측정하는 실험을 수행했다. 도 1은 Edge TPU에서 SRAM(Static Random Access Memory) 할당 크기에 따른 심층 신경망(deep neural network, DNN) 작업의 실행 시간(a) 및 실행 지연(b)을 측정한 실험 결과를 예시한 도면이다. 도 1의 (a)와 같이, 실험 결과 각 DNN 모델의 SRAM 할당 크기가 증가할수록 실행 시간이 감소하는 것으로 나타났다. 예를 들어, SRAM 할 당 없이 EfficientNet-S를 실행하면 최대 SRAM 할당으로 실행할 때보다 5.2배 느려진다. 이러한 SRAM 할당에 따 른 실행 시간 변동은 DNN 모델에 따라 다르다. 이를 위해서는 Edge TPU에 대해 기한을 인식하는 SRAM 할당이 필 요한다. SRAM 할당 메커니즘은 타이밍 제약을 위반하지 않고 가능한 한 많은 실시간 DNN 작업을 수용하기 위해 SRAM 할당 및 타이밍 제약에 따라 서로 다른 실행 시간을 모두 고려하여 작업에 할당된 SRAM의 양을 결정해야 한다. 장애 요인 #2: Edge TPU의 비선점적(non-preemptive) 특성 Edge TPU는 타이밍 제약에 따라 우선순위를 지정하지 않고 비선점 방식으로 DNN 작업을 순차적으로 예약하는 표 준 스케줄링 원칙(즉, FIFO)을 사용한다. Edge TPU의 비선점적 특성으로 인해 우선순위가 높은 작업이 현재 실 행 중인 낮은 우선순위 작업에 의해 차단될 수 있다. 이러한 차단 효과는 우선순위가 높은 DNN 작업에 예측할 수 없고 사소하지 않은 지연을 부과할 수 있다. 앞서 소개한 도 1의 (b)는 vanilla Edge TPU(비선점형 FIFO)에 서 10초 동안 세 가지 작업을 함께 실행할 때 실행 지연의 경험적 누적 분포 함수(cumulative distribution function, CDF)를 보여준다. 작업 τ1, τ2 및 τ3은 100ms, 50ms, 30ms마다 EfficientNet-L, EfficientNet-M 및 MobileNetV2에 대한 추론 요청을 생성한다. 모든 작업은 0.03ms에서 395.75ms까지의 넓은 범위에 걸쳐 길고 예측할 수 없는 실행 지연을 겪는다. 이를 위해서는 Edge TPU를 위한 새로운 시스템 전역(system-wide) 스케줄 러가 필요한다. 시스템 전역 스케줄러는 타이밍 요구 사항을 충족할 때 Edge TPU를 효율적으로 활용하기 위해 더 작은 비선점 영역에 우선순위 기반 스케줄링 및 예측 가능한 선점 지점을 제공해야 한다. 이상에서 언급한 기능이 스케줄링 가능성(schedulability)에 어떻게 부정적인 영향을 미칠 수 있는지를 보여주 기 위해 다음 두 가지 작업으로 구성된 작업 세트를 고려해 볼 수 있다: τ1 및 τ2는 각각 데드라인이 10ms 및 45ms인 MobileNetV2 및 EfficientNet-L을 사용한다. 도 2는 종래의 Edge TPU 및 본 명세서의 실시예들이 제안하는 스케줄링 시나리오를 비교하여 예시한 도면으로, (a) Vanilla Edge TPU와 (b) SPET(SRAM allocation and model Partitioning for Edge TPU)(본 명세서의 실시 예들이 제안하는 스케줄링 방법으로, 이하에서 'SPET'로 표기함)를 비교한다. Vanilla Edge TPU에서 SRAM은 MobileNetV2 (1.6MB)보다 EfficientNet-L (5.5MB)에 우선적으로 할당된다. EfficientNet-L에는 MobileNetV2보 다 더 자주 액세스되는 모델 매개변수 데이터가 있기 때문이다. 이는 최종 데드라인(10ms)보다 더 긴 실행 시간 (12ms)으로 인해 도 2의 (a)와 같이 τ1의 첫 번째 추론 요청의 기한 미스를 유발한다. τ1의 두 번째 추론 요청 도 τ2에 의한 긴 차단 지연으로 인해 데드라인을 놓쳤다. Vanilla Edge TPU와 달리 SPET는 데드라인을 고려하 여 MobileNetV2 (3.9MB)에 더 많은 SRAM을 할당하므로 τ1의 첫 번째 추론 요청의 데드라인 누락이 없다. SPET 은 또한 각 DNN 모델을 여러 세그먼트로 분할하여 더 작은 비선점 영역 사이의 경계에서 우선순위 기반 스케줄 링 및 선점 포인트(preemption points)를 지원하므로 τ2의 두 번째 추론 요청의 데드라인 누락이 없다. 따라서 스케줄링 가능성을 개선하기 위해 Edge TPU를 활용하는 데 있어 상기 지적한 두 가지 장애 요인을 모두 해결하 는 것이 중요한다. 이하에서는 도면을 참조하여 본 명세서의 실시예들을 구체적으로 설명하도록 한다. 다만, 하기의 설명 및 첨부 된 도면에서 실시예들의 요지를 흐릴 수 있는 공지 기능 또는 구성에 대한 상세한 설명은 생략한다. 덧붙여, 명 세서 전체에서, 어떤 구성 요소를 '포함'한다는 것은, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하 는 것이 아니라, 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 명세서를 한정하려는 의 도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구비하다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합 한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 특별히 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 명세서가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미이다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 인 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미 로 해석되지 않는다. Edge TPU의 동작 방식 도 3은 심층 신경망 가속기인 Edge TPU에서 신경망 작업을 스케줄링하는 과정을 설명하기 위한 도면이다. Edge TPU는 엣지 컴퓨팅을 위한 소형 인공지능 프로세서로, 도 3과 같이 크게 오프라인 단계(Offline Phase)와 온라 인 단계(Online Phase)를 걸쳐 동작한다. 첫째, 오프라인 단계에서는 TensorFlow Lite 기반의 DNN 모델(.tflite 파일)을 Edge TPU에서 실행할 수 있도록 컴파일러를 통해 컴파일하는 과정을 수행한다. 이 단계에서는 DNN 모델을 Edge TPU에서 실행 하기 위한 명령어들을 생성할 뿐만 아니라, Edge TPU 내에 있는 SRAM(Static Random Access Memory)(36 0)을 얼마나 할당할지 결정한다. SRAM은 DNN 모델의 파라미터 데이터를 저장할 수 있는 8MB 크기의 캐시 메모리의 일종으로, SRAM에 많은 파라미터 데이터가 저장될수록 DNN 작업의 실행 속도가 빨라진다. 컴파일이 완 료되면 DNN 모델에 대한 Edge TPU용 바이너리 파일이 만들어지고, 해당 바이너리 파일은 \"캐싱 토큰\"이라 고 불리는 고유 번호를 지니게 된다. 둘째, 온라인 단계에서는 컴파일 과정에서 만들어진 바이너리 파일이 Edge TPU에 전송되어 DNN 작업 이 실행된다. Edge TPU는 먼저 미리 정해둔 할당량만큼 DNN 모델의 파라미터 데이터를 SRAM에 캐싱하고, 나머지 파라미터는 Edge TPU 외부에 있는 호스트 메모리(즉, DRAM)에 저장해두고 사용한다. 이후, 작 업의 입력 데이터가 Edge TPU로 전송되어 DNN 연산이 수행되며, 해당 연산이 완료되면 그 결과 데이터를 호스트 메모리에 다시 전송해주는 방식으로 동작한다. 이때, 동일한 DNN 작업이 한 번 더 수행될 시 이전에 SRAM에 캐싱된 파라미터 데이터를 그대로 사용할 수 있다. 여기서 한 가지 주목할 점은 만약 다른 캐싱 토큰을 지니는 DNN 작업(즉, 별도로 컴파일된 바이너리 파일)이 실 행된다면 Edge TPU는 현재 SRAM에 캐싱된 데이터를 모두 삭제하는 캐시 플러싱(cache flushing)을 수행하고, 해당 작업을 위한 파라미터 캐싱을 새롭게 진행한다는 것이다. 이러한 방식은 상당한 성능 오버헤드 를 유발하기 때문에, Edge TPU에서는 다수의 DNN 작업 수행이 필요하다면 이들에 대한 모델들을 한 번에 모두 컴파일할 수 있는 \"코-컴파일(co-compile)\" 기능을 제공한다. 코-컴파일 과정에서는 DNN 작업들이 각각의 메모리 접근 빈도에 비례하여 SRAM을 나누어 할당받게 되며, 각 작업의 바이너리 파일은 동일한 캐싱 토큰 을 지니게 된다. 이러한 방식은 여러 작업이 번갈아 가며 수행되는 경우 작업 실행 간 캐시 플러싱 없이도 여러 작업이 SRAM을 효율적으로 공유할 수 있도록 하는 장점을 지닌다. 시스템 개요 Edge TPU의 동작 방식에 대한 이해를 바탕으로, 이하에서는 본 명세서의 실시예들이 앞서 지적한 두 가지 장애 요인을 해소하기 위해 도출한 기술적 수단을 순차적으로 설명한다. 앞서 언급한 장애 요인으로 인해 Edge TPU가 실시간 DNN 작업의 스케줄링 가능성을 보장하고 개선하는 것은 어 려운 일이다. Edge TPU에서 작업 실행 시간은 SRAM에 캐시된 모델 매개변수 데이터의 양에 따라 다르다. 따라서 타당성(feasibility)을 유지하면서 각 개별 작업에 대해 SRAM에서 올바른 매개변수 데이터 양을 결정하고 할당 하는 것이 중요한다. 그러나 SRAM 할당은 데드라인을 인지하지 못하는 비공개 소스(closed-source) Edge TPU 컴 파일러에 의존하므로 기한을 인식하고 유연한 SRAM 할당을 방해한다. 또한, 다른 작업에 의해 부과되는 간섭 또 는 차단 지연(blocking delay)의 양은 Edge TPU에서 실행되는 작업의 일정에 따라 달라질 수 있으며, 두 가지 모두 DNN 작업의 응답 시간에 영향을 미친다. 그러나, Edge TPU는 작업의 서로 다른 타이밍 제약 조건을 고려하 지 않고 비선점형 FIFO 방식으로 DNN 작업을 순차적으로 예약하므로 예측할 수 없는 지연이 발생하고 실시간 DNN 작업의 일정 가능성을 보장하기 어렵다. 따라서, 본 명세서의 실시예들은 이러한 문제를 해결하기 위해 Edge TPU에 대한 실시간 DNN 작업의 투명하고 예측 가능한 SRAM 할당 및 예약을 가능하게 하는 시스템 추상화를제안한다. 도 4는 본 명세서의 실시예들이 제안하는 실시간 시스템에서 데드라인(deadline) 및 우선순위를 고려하여 신경 망 작업을 스케줄링하는 과정을 개략적으로 도시한 블록도로서, 예를 들어, Edge TPU 환경에서 여러 DNN 추론 작업들의 실시간성을 보장하는 실시간 DNN 프레임워크를 제안한다. 이 프레임워크는 크게 SRAM을 할당하는 구성, 모델을 분할하는 구성 및 우선순위 스케줄링을 수행하는 구성의 세 가지 요소로 구성되며, 기존 Edge TPU 환경과 비슷하게 오프라인 단계와 온라인 단계를 걸쳐 동작한다. 먼저, 오프라인 단계에서는 처리하고자 하는 DNN 모델들이 입력되고, Edge TPU 컴파일러와 협력하여 각 DNN 작업의 마감 시간이 고려된 SRAM 자원 크기를 결정하며, 또한 작업 간의 선점 지점을 만들기 위해 DNN 모델을 여러 세그먼트들로 나누는 작업을 한다. 이후, 온라인 단계에서는 여러 DNN 작업들이 Edge TPU를 사용하기 위해 시스템 전역 스케줄러에 실행 요청을 보낸다. 스케줄러는 우선순위에 따라 작업들을 정렬하고, 가장 높은 우선순위의 작업부터 Edge TPU를 통해 실행시킨다. Edge TPU에서 의 실행은 세그먼트 단위로 이루어지며, 세그먼트 실행을 위한 입력 데이터를 호스트 메모리에서 Edge TPU(48 0)로 전송하고, SRAM 자원을 할당받아 작업을 실행한다. 실행이 끝나면 반대로 출력 데이터를 Edge TPU에 서 호스트 메모리로 전송한다. 한 세그먼트의 실행이 끝나면 우선순위에 따른 스케줄링을 다시 수행한다. 도 5는 본 명세서의 실시예들이 제안하는 작업 스케줄링 기법을 Edge TPU를 통해 구현한 실시간 시스템의 구조 를 도시한 도면으로, 유연한 SRAM 할당자(Flexible SRAM allocator), DNN 모델 파티셔너(model partitioner) 및 시스템 전역(system-wide) 스케줄러를 포함한다. 유연한 SRAM 할당자와 DNN 모델 파티셔너는 모두 오프라인 단계에서 Edge TPU 컴파일러와 긴밀하게 결합되어 중 요한 역할을 한다. SRAM의 양을 각 개별 작업에 전용으로 할당하고 더 작은 비선점 영역 간에 각각 예측 가능한 선점 지점을 제공한다. 특히 유연한 SRAM 할당자는 Edge TPU 컴파일러의 코-컴파일 기능과 함께 보조 DNN 모델 을 적절하게 활용한다. 결과적으로 유연한 SRAM 할당자는 DNN 모델의 코드 수정 및 추가 공간 오버헤드 없이 특 정 크기와 위치를 가진 SRAM의 일부를 대상 DNN 모델에 할당할 수 있다. DNN 모델 파티셔너는 선점 지점이 세그 먼트 경계에 나타나도록 대상 DNN 모델을 여러 세그먼트로 분할한다. 시스템 전역 스케줄러는 타이밍 제약을 충 족하기 위해 온라인 단계에서 우선순위에 따라 작업의 실행 및 선점을 자동으로 조율하는 우선순위 기반 스케줄 링을 지원한다. 상기된 세 가지 기능을 기반으로, 도 5의 실시간 신경망 작업 스케줄링 시스템은 스케줄링 가능성을 개선하는 방식으로 각 DNN 작업에 대한 SRAM 할당 및 세그먼트 수를 결정하는 새로운 스케줄링 정책을 개발한다. 이를 위 해 각 작업의 SRAM 할당 및 모델 분할이 실행 시간, 간섭 지연 및 차단 지연에 미치는 영향을 신중하게 고려하 여 DNN 작업이 타이밍 제약 조건을 충족하는지 확인하기 위한 응답 시간 분석을 도출한다. 그런 다음, 제안된 응답 시간 분석에 따라 혼합 정수 프로그래밍(mixed-integer programming, MIP) 문제로 SRAM 할당 및 모델 분 할을 공식화한다. 도 6은 본 명세서의 일 실시예에 따른 데드라인을 고려하여 실시간 신경망 작업을 스케줄링하는 방법을 도시한 흐름도이다. S610 단계에서, 적어도 하나의 프로세서를 구비하는 실시간 시스템은 처리해야 하는 복수 개의 신경망 모델을 입력받는다. S630 단계에서, 상기 실시간 시스템은 S610 단계를 통해 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리를 할당하여 신경망 작업을 생성 한다. 예를 들어, Edge TPU의 각 DNN 작업에 전용 SRAM 양을 할당함에 있어서, 코-컴파일 없이 여러 DNN 모델을 개별 적으로 컴파일하면 각 모델은 전체 SRAM을 독점적으로 활용하여 실행 중에 모델 매개변수 데이터를 캐시할 수 있다. 컴파일 시 Edge TPU 컴파일러는 각 모델에 고유한 캐싱 토큰을 제공한다. DNN 모델에 대한 추론 요청시 Edge TPU 런타임은 해당 토큰을 현재 캐시된 토큰과 비교한다. 토큰이 일치하는 경우 런타임은 캐시된 데이터를 사용한다. 일치하지 않으면 전체 SRAM을 플러시하고 요청된 모델의 데이터를 SRAM에 로드하여 상당한 오버헤드 를 부과한다. Edge TPU 컴파일러의 코-컴파일 기능을 사용하면 여러 모델이 SRAM을 공유하여 플러시 및 다시 로 드 오버헤드를 제거할 수 있다. 그러나, 독점적인 블랙박스 구조를 갖는 Edge TPU 컴파일러로 인해 개별 DNN 모 델에 할당된 SRAM의 크기와 위치를 현실적으로 제어할 수 없었다. DNN 태스크에 얼마나 많은 SRAM 자원을 할당할지는 컴파일 단계에서 결정되기 때문에, 근본적으로 컴파일러 자 체를 수정할 필요가 있다. 하지만 문제는 Edge TPU 컴파일러는 비공개 소스 코드 기반의 소프트웨어로 제공되어, 컴파일러 내부를 직접 수정하는 일이 불가능하다는 것이다. 이를 해결하기 위해 본 실시예가 제안하 는 S630 단계에서는 컴파일러 내부를 수정하는 대신 유연한 SRAM 할당자를 통해 코-컴파일 기능을 활용함과 더 불어 더미 모델(dummy model)이라는 특수한 DNN 모델을 생성하여 DNN 태스크에 할당되는 SRAM 영역의 크기와 위 치를 조정하는 우회적인 방법을 제안한다. 이 과정에서는, 우선 주어진 데드라인 조건에 따라 신경망 모델마다 더미 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메모리를 조절한다. 여기서, 신경망 모델에 할당될 메모리를 조절하는 과정은, 주어진 데드라인 조건에 따라 더미 모델을 개별 신경 망 모델의 앞 또는 뒤 중 적어도 하나의 위치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모 델을 활용한 코-컴파일을 반복함으로써 실행시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리 (binary) 파일을 생성하도록 유도할 수 있다. 그런 다음, 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정할 수 있다. 여기서, 캐싱 토큰을 동일하게 설정하는 과정은, 상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리해야 하는 신경망 작업들의 캐싱 토큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정할 수 있다. 각 코-컴파일을 통해 생성된 바이너리 파일들은 서로 다 른 캐싱 토큰값을 갖게 되는데, 온라인 단계에서 작업 전환 시 캐시 플러싱이 일어나는 것을 방지하기 위해 바 이너리 파일들을 모두 수정하여 동일한 캐싱 토큰을 갖도록 만든다. 이는 각각 따로 코-컴파일된 DNN 작업들이 라도 캐시 플러싱 없이 SRAM을 공유할 수 있도록 해준다. 유연한 SRAM 할당자 (Flexible SRAM allocator) 도 7은 도 5의 실시간 시스템에서 메모리를 할당하는 과정을 예시한 도면이다. 기본적으로 각 작업에 할당된 SRAM의 주어진 양에 대해 유연한 SRAM 할당자는 도 7과 같이 별도의 공동 컴파일을 통해 하나씩 작업에 SRAM을 순차적으로 할당한다. 각 작업의 SRAM 할당에 대해 최대 2개의 더미 DNN 모델과 함께 컴파일된다. 이러한 더미 DNN 모델은 가장 자주 액세스되는 레이어로 구성되어 Edge TPU 컴파일러의 기본 SRAM 할당 정책에 따라 모든 데 이터를 SRAM에 할당하도록 보장된다. 그런 다음, 유연한 SRAM 할당자는 더미 DNN 모델의 크기와 더미 및 대상 DNN 모델의 공동 컴파일 순서를 적절하게 설정하여 SRAM의 전용 크기와 위치를 각 작업에 할당할 수 있다. 보다 구체적으로, 작업 τi가 크기 및 오프셋(offset) fi를 갖는 SRAM에 할당되기를 원하는 상황을 가정하 자. 여기서, 는 이후에 논의할 MIP(mixed-integer programming) 기반 알고리즘에 의해 결정되고, 이다. 유연한 SRAM 할당자는 2개의 더미 DNN 모델(sd1 및 sd2)의 크기를 sd1 = fi 및 sd2 = M - sd1 - 로 설정한다. 여기서, M은 SRAM의 총 크기이고, sd1, 및 sd2의 순서로 코-컴파일한다. 도 7에 도시된 예와 같이, 8MB SRAM 크기 중, τ2는 두 개의 더미 DNN 모델(첫 번째 모델은 크기 sd1 = 2MB이고, 두 번 째 모델은 크기 sd2 = 2MB)을 sd1, 및 sd2의 순서로 코-컴파일하여 크기 = 4MB이고 오프셋 f2 = 2MB인 SRAM에 할당된다. 유연한 SRAM 할당자는 크기 인 τ1 을 할당하기 위해 단일 더미 DNN 모델 sd2 = M - 만 을 필요로 한다. 마찬가지로, τn은 단일 더미 DNN 모델 sd1 = M - 을 사용하여 크기 을 갖는 SRAM에 할 당될 수 있다. 여기서 n은 작업의 수이다. 각 태스크 τi의 오프셋을 로 설정함으로써, 각 태스크에 할당된 SRAM의 각 부분이 서로 겹치지 않도록 보장할 수 있다. 모든 작업에 대해 이러한 코-컴파일을 반복함으로써 유연한 SRAM 할당자는 각 작업에 할당된 SRAM의 크기와 위 치를 지정하는 실행 가능한 바이너리 파일을 생성할 수 있다. 각 작업은 다른 작업과 별도로 해당 더미 DNN 모 델로만 코-컴파일되므로 각 작업의 바이너리 파일은 서로 다른 토큰을 갖는다. 불필요한 플러시 및 재로드 오버 헤드를 방지하기 위해 유연한 SRAM 할당자는 생성된 바이너리 파일을 자동으로 계측하여 동일한 캐싱 토큰이 할 당되도록 한다. 결과적으로 유연한 SRAM 할당자는 (비공개 소스) Edge TPU 컴파일러를 수정하지 않고도 Edge TPU에서 각 DNN 작업의 투명하고 유연한 SRAM 할당을 가능하게 한다.다시 도 6으로 돌아와, S650 단계에서, 상기 실시간 시스템은 S630 단계를 통해 생성된 상기 신경망 작업을 동 시에 실행하도록 유도한다. 도 8은 본 명세서의 다른 실시예에 따른 데드라인 및 우선순위를 고려하여 실시간 신경망 작업을 스케줄링하는 방법을 도시한 흐름도로서, 도 6의 작업 스케줄링 방법에서 작업의 선점 및 우선순위를 추가로 고려하였다. S810 단계에서, 적어도 하나의 프로세서를 구비하는 실시간 시스템은 처리해야 하는 복수 개의 신경망 모델을 입력받는다. S830 단계에서, 상기 실시간 시스템은 S810 단계를 통해 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정한다. 앞서 도 6을 통해 설명한 바와 유사하게, 이 과정에서는, 주어진 데드라인 조건에 따라 신경망 모델마다 더미 (dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메모리를 조절한 다. 이를 위해, 주어진 데드라인 조건에 따라 더미 모델을 개별 신경망 모델의 앞 또는 뒤 중 적어도 하나의 위 치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모델을 활용한 코-컴파일을 반복함으로써 실행 시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리(binary) 파일을 생성하도록 유도할 수 있다. 그런 다음, 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정한다. 이를 위해, 상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리 해야 하는 신경망 작업들의 캐싱 토큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정할 수 있다. S850 단계에서, 상기 실시간 시스템은 작업의 선점(preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그 먼트(segment)로 분할한다. 이 과정에서는, 신경망 모델을 복수 개의 세그먼트로 분할하여 상기 세그먼트들 간 의 경계에서 선점을 허용하는 선점 지점(preemption point)을 생성하되, 생성된 상기 선점 지점마다 신경망 작 업 간의 스케줄링을 수행함으로써 우선순위에 따른 선점 스케줄링을 유도할 수 있다. 이를 위해, 우선순위가 높은 작업의 차단 지연이 우선순위가 낮은 작업에서 세그먼트를 실행하는 최대 시간으로 제한되도록 할 수 있다. 또한, 세그먼트 분할시 각 세그먼트의 실행 시간이 균등하게 되도록 런타임 모니터링 (runtime monitoring) 방식에 따라 신경망 모델 분할을 수행할 수 있다. 또한, 신경망 모델의 후보 레이어 (layer) 중 세그먼트의 출력 데이터를 복사할 때 발생하는 오버헤드가 가장 적은 레이어를 각 세그먼트에 대한 세그먼트 경계로 선택할 수 있다. DNN 모델 파티셔너 (DNN model partitioner) 도 9는 도 5의 실시간 시스템에서 신경망 모델을 분할하는 과정을 예시한 도면이다. DNN 모델 파티셔너는 대상 DNN 모델을 여러 세그먼트로 분할하여 세그먼트 간 경계에서 선점 지점을 제공한다. 따라서 우선순위가 높은 작 업의 차단 지연은 우선순위가 낮은 작업에서 세그먼트를 실행하는 최대 시간으로 제한된다. 이후 기술할 MIP 기반 알고리즘에 의해 각 작업 τi에 대한 세그먼트 수 가 결정되면, τi의 레이어 시퀀스 는 세그먼트로 분할되어야 총 실행 시간이 세그먼트 간에 균등하게 분할되어 최대 차단 시간이 최소화된다. 한 가지 이슈는 각 계층의 실행 시간이 SRAM 할당, 즉 해당 모델 매개변수 데이터의 캐시 여부에 따라 다르다는 것이다. 또 다른 이슈는 세그먼트 오버헤드가 존재한다는 것이다. 우선순위가 낮은 태스크의 현재 실행 중인 세 그먼트가 완료된 후 우선순위가 높은 태스크의 선점이 발생하면, 현재 실행 중인 세그먼트의 출력 데이터를 호 스트 메모리에 복사(즉, 다운로드 복사)해야 한다. 그런 다음, 출력 데이터는 장치 메모리(즉, 업로드 복사)로 다시 전송되어 예약 시 연속 세그먼트에 대한 입력으로 사용되어야 한다. 이러한 업로드 및 다운로드 복사 오버 헤드는 각 세그먼트의 끝에 어떤 레이어가 있는지에 따라 다르다. 각 세그먼트의 끝에 위치한 레이어를 세그먼 트 경계라고 한다. 따라서, 최대 차단 시간을 최소화하기 위해 세그먼트 경계를 결정할 때 이 두 가지 문제를 함께 고려해야 한다. 이를 위해, 주어진 SRAM 할당 및 세그먼트 수 에 대해, DNN 모델 파티셔너(model partitioner)는 레이 어 수준에서 실행 시간 및 업로드/다운로드 복사 비용을 포함하여 각 DNN의 프로필을 유지한다. 이러한 프로파 일에 따라 DNN 모델 파티셔너는 다음과 같이 마지막 세그먼트를 제외한 - 1 세그먼트의 세그먼트 경계를 결정한다. 도 9와 같이 평균 세그먼트 길이 를 로 계산한다. 여기서, 는 SRAM 할당 하에서 τi의 레이어의 누적 실행 시간이다. 각 세그먼트에 대한 세그먼트 경계가 될 후보 레이어 목록을 구성한다. 관심 레이어(관심 레이어 포함)보다 앞선 모든 레이어의 누적 실행 시간이 의 간격 내에 있다. 여기서, δ는 일정한 세그먼트 경계 마진(margin)이며, 경험적 으로 δ를 로 설정했다. 그런 다음, 후보 레이어 중 업로드 및 다운로드 복사 비용이 가장 적은 레이어를 각 세그먼트에 대한 세그먼트 경계로 선택한다. 이러한 방식으로 DNN 모델 파티셔너는 대상 DNN 모델을 여러 세그 먼트로 분할하여 SRAM 할당 및 모델 파티셔닝에 따른 다양한 실행 시간과 복사 오버헤드를 고려하여 최대 차단 시간을 최소화한다. 여러 세그먼트가 있는 DNN 모델이 유연한 SRAM 할당자와 호환되어 실행 가능한 바이너리 파일을 생성할 수 있다 는 우려가 있을 수 있다. 유연한 SRAM 할당자는 가장 자주 액세스되는 모델 매개변수 데이터와 함께 더미 DNN 모델을 사용하므로 작업의 세그먼트에 할당된 SRAM의 양은 변경되지 않는다. 유연한 SRAM 할당자는 각 세그먼트 에 대해 별도의 바이너리 파일을 생성하여 세그먼트 간 경계에서 선점을 허용한다. 시스템 전역 스케줄러 (System-wide scheduler) 다시 도 8로 돌아와, S870 단계에서, 상기 실시간 시스템은 상기 메모리 할당 및 상기 세그먼트 분할에 따라 생 성된 신경망 작업을 우선순위를 고려하여 실행하도록 유도한다. 이 과정에서는, 시스템 전역(system-wide) 스케 줄러가 우선순위 큐(priority queue)를 통해 신경망 작업들의 실행 요청들을 우선순위에 따라 순서대로 정렬한 다. 그런 다음, 최우선순위의 작업을 상기 큐에서 가져와 시스템 자원을 선점하여 세그먼트 단위로 바이너리 파 일을 실행할 수 있다. 여기서, 시스템 전역 스케줄러는 우선순위 기반 스케줄링 및 세그먼트 수준 선점을 지원한다. 시스템 전역 스케 줄러는 프로세스 간 통신(inter-process communication, IPC)을 통해 DNN 작업과 통신하기 위해 백그라운드 데 몬으로 실행된다. 스케줄러는 우선순위에 따라 세그먼트를 예약하기 위해 각 세그먼트의 바이너리 파일과 우선 순위 큐를 유지 관리한다. 세그먼트가 실행을 마치면 스케줄러에 완전한 메시지를 보내고, 스케줄러는 우선순위 가 가장 높은 세그먼트를 큐로부터 제거한다. MIP 기반 알고리즘 이하에서는 먼저 표기법을 사용하여 대상 시스템 모델을 설명한다. 그런 다음 주어진 SRAM 할당 및 모델 파티셔 닝에서 일련의 실시간 DNN 작업 스케줄링 가능성을 결정하는 응답 시간 분석을 도출한다. 마지막으로 SRAM 할당 크기와 각 작업에 대한 세그먼트 수를 찾는 MIP 공식을 개발하여 응답 시간 분석에 의해 설정된 작업의 스케줄 링 가능성을 산출한다. 실시간 DNN 추론 작업은 다양한 실시간 시스템에서 널리 사용되는 산발적 작업 모델로 제시된다. 각 작업이 단 일 DNN 모델을 사용하고 작업당 하나의 추론 요청을 한다고 가정한다. 각 DNN 작업 τi ∈ τ는 τi = (Ti,Ni,Si,Gi,Ci,Oi)로 지정할 수 있다. 여기서, Ti는 연속적인 작업 릴리스 사이의 최소 간격(τi의 데드라인과 동일)이고, Ni는 레이어의 수이다. 다른 매개변수는 τi의 SRAM 할당 및 모델 분할에 따라 달라진다. Si는 τi에 할당되는 가능한 SRAM 크기의 집합이며 Si = { , ..., }로 지정된다. 여기서, 및 는 각각 τi에 할당되어야 하는 최소(no) 및 최대 SRAM 크기이다. Si에서 와 사이의 요 소는 할당 가능한 최소 SRAM 크기의 배수, 즉 70KB로 결정된다. Gi는 τi에 대한 세그먼트 수의 가능한 값의 집 합이며, Gi = {1 ≤ gi ≤ Ni}로 지정된다. 여기서, gi = 1 및 gi = Ni는 각각 분할이 없고 모든 계층이 별도의 세그먼트에 속하는 경우이다. Ci는 할당된 SRAM 크기에 의존하는 τi의 누적 최악 실행 시간(worst-case execution time. WCET)이며, Ci = {Ci(si)si∈Si}로 지정된다. Oi는 할당된 SRAM 크기와 세그먼트 수 모두에 의존 하는 τi의 입력/출력을 전송하기 위한 누적 최악의 업로드/다운로드 복사 오버헤드이며, Oi = {Oi(si, gi)si∈Si,gi ∈Gi}로 특정된다. Ci 및 Oi의 값은 측정 기반 접근 방식으로 얻은 DNN 프로파일로부터 추정할 수 있다.각 작업이 모든 세그먼트에서 공유되는 정적 우선순위를 갖는 작업 수준 고정 우선순위 스케줄링을 고려한다. 각 세그먼트의 실행은 비선점형이므로 우선순위가 높은 작업의 선점은 우선순위가 낮은 작업의 현재 실행 중인 세그먼트를 완료한 후에만 허용된다. 또한, 양자 기반 시간(quantum-based time)을 가정한다. 일반성을 잃지 않 고 한 번 단위로 양자 길이를 지정한다. 먼저, SRAM 할당 및 모델 분할 문제를 설명한다. Definition 1 (SRAM 할당 및 모델 파티셔닝): 작업 세트 τ 및 Edge TPU 플랫폼이 주어지면, i) SRAM 할당 크 기 및 ii) 각 DNN 작업 τi ∈ τ에 대한 세그먼트 수 를 찾아 모든 작업은 고정 우선순위 스케줄링에 따라 데드라인을 만족시킨다. 위의 문제를 해결하기 위해, 먼저 응답 시간 분석을 개발하여 최악의 경우 응답 시간(WCRT)에 대한 SRAM 할당 및 모델 분할의 영향을 고려하여 DNN 작업의 WCRT에 대한 상한을 도출한다. 그런 다음, 제안된 응답 시간 분석 을 포함하여 필요한 조건에 따라 SRAM 할당 및 모델 분할 문제를 최적화 문제로 공식화한다. 간결한 표시를 위해 Λi는 τi에 대한 SRAM 크기와 세그먼트 수의 주어진 쌍 (si, gi)을 나타냅니다. 주어진 SRAM 할당 및 모델 파티셔닝 Λ = {{Λi}τi∈τ}에 대해, 작업 수준 고정 우선순위 스케줄링에서 각 작업 τi ∈ τ의 WCRT wi(Λ)는 다음과 같이 계산할 수 있다. 수학식 1"}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, Ii(Ti,Λ)는 우선순위가 높은 작업으로 인해 발생하는 τi의 세그먼트에 대한 누적 간섭(interference) 이고, Bi(Λ)는 우선순위가 낮은 작업으로 인해 발생하는 τi의 첫 번째 세그먼트의 차단 지연이다. 각 작업의 첫 번째 세그먼트만 작업 수준 고정된 우선순위 스케줄링에서 우선순위가 낮은 작업의 현재 실행 중인 세그먼트 에 의해 차단될 수 있다. hp(τi)를 τi보다 우선순위가 높은 모든 작업의 집합이라고 하자. τi의 세그먼트는 우선순위가 높은 작업 τh의 세그먼트에 의해 지연될 수 있다. τi의 단일 작업의 데드라인 Ti 동안 릴리즈된 τh의 작업 수는 최대 이다. 그런 다음, τi에 대한 누적 간섭 Ii(Ti,Λ)는 다음의 수학식에 의해 상한을 갖는다. 수학식 2"}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "lp(τi)는 τi보다 우선순위가 낮은 모든 작업의 집합이다. 누적 간섭에 더하여, 태스크 τi는 우선순위가 낮은 태스크 τ1의 현재 실행 중인 세그먼트에 의해 더 차단될 수 있다. 차단 지연 Bi는 다음의 수학식에 의해 상한을 갖는다. 수학식 3 여기서, 및 은 각각 주어진 Λl에 대한 τ1의 p-번째 세그먼트의 실행 시간 및 업로드/다 운로드 복사 오버헤드이다. 그런 다음, 다음 lemma에 제시된 대로 작업 세트 τ의 스케줄링 가능성을 확인할 수 있다: Lemma 1: 주어진 SRAM 할당 및 모델 파티셔닝 Λ에 대해, 작업 세트 τ는 ∀τi ∈ τ,wi(Λ) ≤ Ti인 경우, 작 업 수준 고정 우선순위 스케줄링에서 스케줄링 가능한다. Proof: 길이 Ti의 간격 내에서 실행될 수 있는 임의의 τh ∈ τ 작업은 최대 개이다. 그런 다 음, 간격 Ti에서 τi에 대한 최대 누적 간섭은 수학식 2에 의한 상한을 갖는다. 또한, 작업 τi는 우선순위가 낮 은 작업의 하나의 세그먼트에 의해 최대 한 번 차단되므로, τi의 최대 차단 시간은 수학식 3에 의한 상한을 갖 는다. 이는 수학식 1의 wi(Λ)가 τi의 작업의 응답 시간의 상한이며, 어떠한 경우에도 wi(Λ) 내에서 실행을 완 료함을 의미한다. 따라서, wi(Λ) ≤ Ti가 모든 τi ∈ τ에 대해 유지되면, τ는 스케줄링 가능한다. 응답 시간 분석에 따라 SRAM 할당 및 모델 분할을 MIP 문제로 공식화한다. 먼저 각 작업 τi에 대해 Si에서 k번 째 가능한 SRAM 크기와 Gi에서 j번째 가능한 세그먼트 계수에 해당하는 0/1 결정 변수 { } 및 { }를 각각 정의한다. 는 가 Si의 k번째 요소로 선택되면 1로 할당되고, 그렇지 않으면 0으로 할당된다. 는 가 Gi의 j번째 원소로 선택되면 1로 할당되고, 그렇지 않으면 0으로 할당된다. 그런 다음, 다음의 네 가지 제약 조건(C1-C4)을 만족하는 솔루션 {( , )τi∈τ}을 찾는다: 수학식 4"}
{"patent_id": "10-2023-0065036", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, M은 총 SRAM 크기이다. 제약 조건 C1 및 C2는 각각의 작업 τi에 대해 각각 Si 및 Gi에서 하나의 값을 선택해야 함을 지정한다. 제약 조건 C3은 각 작업에 할당된 SRAM 양의 합계가 총 SRAM 크기 M을 초과하지 않아 야 함을 지정한다. 제약 조건 C4는 응답 시간 분석에 따라 τi의 WCRT가 데드라인 Ti를 초과하지 않아야 함을 지 정한다. 예를 들어, 상용 MIP 솔버인 Gurobi를 사용하여 MIP 공식을 해결할 수 있다. 및 의 결과 값은 유연한 SRAM 할당자 및 DNN 모델 파티셔너에서 사용할 τi에 대한 SRAM 할당 및 세그먼트 수를 나타낸다. 도 10은 본 명세서의 또 다른 실시예에 따른 데드라인 및 우선순위를 고려하여 실시간 신경망 작업을 스케줄링 하는 장치를 도시한 블록도로서, 앞서 기술한 도 8의 작업 스케줄링 방법을 하드웨어 구성의 관점에서 재 구성한 것이다. 따라서, 여기서는 설명의 중복을 피하고자 각 구성요소의 동작 및 기능을 약술하도록 한다. 작 업 스케줄링 장치는 적어도 하나의 프로세서를 구비하여 이를 통해 하나 이상의 작업을 실행할 수 있으며, 프로세서는 메모리를 구비한다. 입력부는 처리해야 하는 복수 개의 신경망 모델을 입력받는 구성이다. 메모리 할당부는 입력된 상기 신경망 모델 각각의 데드라인(deadline)을 고려하여 동시에 처리해야 하는 신경망 모델 각각에 겹치지 않는 영역의 메모리 할당을 결정하는 구성이다. 보다 구체적으로, 메모리 할당부 는, 주어진 데드라인 조건에 따라 신경망 모델마다 더미(dummy) 모델을 추가함으로써 전체 메모리의 양 중에서 자신 또는 다른 신경망 모델에 할당될 메모리를 조절하고, 동시에 처리해야 하는 신경망 모델들에 동시 실행을 나타내는 고유 식별자인 캐싱 토큰(caching token)을 동일하게 설정할 수 있다. 또한, 상기 메모리 할당부는, 주어진 데드라인 조건에 따라 더미 모델을 개별 신경망 모델의 앞 또는 뒤 중 적어도 하나의 위치에 함께 배치하여 코-컴파일(co-compile)함으로써 신경망 모델에 할당되는 메모리의 크기 및 데이터가 기록될 메모리의 주소를 조절하되, 각각의 신경망 모델 별로 상기 더미 모델을 활용한 코-컴파일을 반복함으로써 실행시 신경망 작업들 간에 메모리 영역이 겹치지 않는 바이너리(binary) 파일을 생성하도록 유도 하고, 상기 더미 모델이 추가된 신경망 모델의 코-컴파일 결과 동시에 처리해야 하는 신경망 작업들의 캐싱 토 큰이 달라진 경우, 해당 신경망 작업들의 캐싱 토큰을 동일한 값으로 수정할 수 있다. 신경망 모델 분할부는 작업의 선점(preemption)을 허용하기 위해 신경망 모델을 복수 개의 세그먼트 (segment)로 분할하는 구성이다. 보다 구체적으로, 신경망 모델 분할부는, 신경망 모델을 복수 개의 세그 먼트로 분할하여 상기 세그먼트들 간의 경계에서 선점을 허용하는 선점 지점(preemption point)을 생성하되, 생 성된 상기 선점 지점마다 신경망 작업 간의 스케줄링을 수행함으로써 우선순위에 따른 선점 스케줄링을 유도할 수 있다. 또한, 상기 신경망 모델 분할부는, 우선순위가 높은 작업의 차단 지연이 우선순위가 낮은 작업에서 세그 먼트를 실행하는 최대 시간으로 제한되도록 수행되고, 세그먼트 분할시 각 세그먼트의 실행 시간이 균등하게 되 도록 런타임 모니터링(runtime monitoring) 방식에 따라 신경망 모델 분할을 수행하며, 신경망 모델의 후보 레 이어(layer) 중 세그먼트의 출력 데이터를 복사할 때 발생하는 오버헤드가 가장 적은 레이어를 각 세그먼트에 대한 세그먼트 경계로 선택할 수 있다. 컴파일러(complier)는 상기 메모리 할당 및 상기 세그먼트 분할에 따라 상기 신경망 모델로부터 바이너리 파일로 구성된 신경망 작업을 생성하는 구성이다. 시스템 전역(system-wide) 스케줄러는 생성된 신경망 작업을 우선순위를 고려하여 적어도 하나의 프로세 서를 통해 실행하도록 유도하는 구성이다. 이를 위해, 시스템 전역 스케줄러는, 우선순위 큐 (priority queue)를 통해 신경망 작업들의 실행 요청들을 우선순위에 따라 순서대로 정렬하고, 최우선순위의 작 업을 상기 큐에서 가져와 시스템 자원을 선점하여 세그먼트 단위로 바이너리 파일을 실행하도록 유도할 수 있다. 신경망 작업 실행시 앞서 결정된 메모리 할당에 따라 메모리 자원을 제공받는다. 한편, 구현의 관점에서, 상기 컴파일러 및 상기 프로세서는, Edge TPU(Edge Tensor Processing Units)에 기반하여 구동될 수 있다. 다만, 본 명세서의 실시예들은, 예시된 Edge TPU 뿐만 아니라, 데드라인을 고려하지 않는 SRAM 할당 내지 비선점적(non-preemptive) 특성을 갖는 다른 DNN 가속기에도 적용 가능하다는 점 을 알 수 있다. 평가 및 비교 실험 이하에서는 임베디드 DNN 프레임워크 중 하나인 TensorFlow Lite 위에 SPET을 구현하고 평가하였다. TensorFlow Lite로 설계된 DNN은 SPET이 Edge TPU의 소프트웨어 스택을 수정할 필요가 없기 때문에 SPET에 쉽게 적용된다. 평가를 위해 이미지 분류(EfficientNet-{L,M,S}, MobileNet-V2 및 ResNet), 객체 감지(EfficientDet) 및 차선 탐색(DAVE-2)을 위한 7개의 DNN 모델을 사용하였다. Edge TPU가 장착된 Google Coral Dev 보드에서 실험을 수 행하였다. 비교할 접근 방식으로, 다음의 7가지 접근 방식을 비교하였다: - SPET: 모든 구성 요소가 포함된 SPET; - SPET w/o MP: DNN 모델 파티셔너가 없는 SPET; - SPET w/o SA: 유연한 SRAM 할당자가 없는 SPET; - DPSA: 데드라인에 비례하는 SRAM 할당만을 포함; 데드라인에 반비례하여 작업에 SRAM을 할당하므로 기한이 짧 은 작업이 더 많은 SRAM 공간을 확보함; - Base-Seg{1, 2, 6}: 기본(데드라인 없음) SRAM 할당 및 정적 모델 분할; 각 작업의 세그먼트 수는 접근법 이 름의 접미사에 따라 1, 2 또는 6으로 설정.Base-Seg1은 바닐라 Edge TPU(Base로도 표시됨)이다. 먼저 무작위로 생성된 매개변수가 있는 합성 작업 세트에 대한 광범위한 시뮬레이션 결과를 보여준다. 스케줄링 가능성 비율을 성능 지표로 사용하였다. 생성된 작업 집합의 총 수에 대한 예약 가능한 작업 집합의 비율로 정 의된다. 실시간 작업 세트 생성에 널리 사용되는 UUniFast 알고리즘을 사용하여 8,500개의 작업 세트를 무작위 로 생성하였다. 각 작업 세트에는 5개의 이미지 분류 모델 중에서 DNN 모델이 무작위로 선택된 8개의 DNN 작업 이 포함된다. 각 작업의 최악의 실행 시간과 업로드/다운로드 복사 오버헤드는 대상 하드웨어인 Google Coral에 서 프로파일링된다. 각 작업의 우선 순위는 DM(deadline-monotonic)에 의해 결정된다. 도 11은 본 명세서의 실시예들이 제안하는 실시간 시스템의 성능을 평가하기 위한 시뮬레이션 결과를 예시한 도 면으로, Edge TPU의 스케줄링 가능성을 크게 개선하는 SPET의 기능을 검증하였다. 도 11의 (a)는 작업 집합의 총 활용률 Uτ를 0.2에서 1.0으로 변화시키면서 스케줄링 가능성 비율을 비교한 것이다. 여기서, Uτ는 로 정의된다. SPET는 Base-Seg2, Base-Seg6, DPSA 및 Base-Seg1보다 각각 34%, 42%, 69% 및 79% 더 많은 예약 가능한 작업 세트를 찾는다는 점에서 예약 가능한 작업 세트를 찾는 데 높은 기능을 나타낸다. SPET과 다른 접근 방식 간의 성능 격차는 Uτ가 증가함에 따라 커진 다. SPET의 각 구성 요소가 일정 가능성 향상에 얼마나 기여하는지 추가로 분석하였다. 먼저 SPET w/o MP, DPSA 및 Base-Seg1과 같은 서로 다른 SRAM 할당 방식을 비교하였다. SPET w/o MP는 DPSA 및 Base-Seg1보다 각각 16% 및 23% 더 예약 가능한 작업 세트를 찾는다. SPET w/o MP와 Base-Seg1의 차이점은 기한을 인지하지 못하는 SRAM 할 당의 Base-Seg1 사례와 비교하여 기한을 인지하는 유연한 SRAM 할당을 허용하는 개선 사항을 보여준다. 이러한 개선은 Uτ가 증가하면 커진다. 이는 사용률이 높은 작업이 신중한 SRAM 할당 없이는 타이밍 제약을 충족할 수 없는 경우가 더 많기 때문이다. DPSA는 또한 스케줄링 가능성에 대한 SRAM 할당의 영향을 신중하게 고려하지 않 으므로 SPET w/o MP보다 스케줄링 가능성 성능이 떨어진다. 또한 서로 다른 모델 파티셔닝 접근 방식인 SPET w/o SA 및 Base-Seg{6, 2, 1}도 비교하였다. SPET w/o SA는 Base-Seg{6, 2, 1}보다 각각 13%, 7%, 41% 더 나은 스케줄링 가능성을 보여준다. SPET w/o SA와 Base-Seg1의 차이는 선점이 허용되지 않는 경우에 비해 세그먼트 간 경계에서 선점점을 허용하는 개선점을 보여준다. Base- Seg2와 Base-Seg6은 서로 지배할 수 없다. 즉, Base-Seg6은 Uτ < 0.55일 때 Base-Seg2보다 성능이 우수하지만 Uτ ≥ 0.55일 때 성능이 역전된다. 이는 세그먼트 수를 결정할 때 차단 지연과 복사 오버헤드 간에 균형이 있기 때문이다. 세그먼트 수가 많을수록 복사 오버헤드가 높아지는 대신 우선 순위가 높은 작업에 대한 차단 지연이 줄어든다. Uτ가 증가함에 따라 각 작업의 활용도가 높아져 높은 복사 오버헤드를 수용할 수 없다. 따라서, Base-Seg6은 Base-Seg2보다 낮은 스케줄링 가능성 성능을 보인다. 그러나, SPET은 이러한 트레이드 오프를 고려 하여 각 작업에 대한 세그먼트 수를 적응적으로 결정하므로 두 접근 방식보다 더 나은 성능을 제공할 수 있다. 다음으로, 작업 세트의 작업 수를 변경하여 일정 가능성에 미치는 영향을 확인하였다. 도 11의 (b)와 같이 Uτ = 0.7일 때 작업 수를 3에서 13으로 변경하면서 총 5,500개의 추가 작업 세트를 생성하였다. SPET은 작업 수의 모 든 값에 대해 다른 접근 방식을 능가하는 것으로 나타났다. 한 가지 주목할만한 관찰은 작업 수가 변화함에 따 라 SPET w/o SA와 SPET w/o MP 사이의 우위가 변한다는 것이다. 작업 수가 줄어들수록 각 작업의 활용도가 높아 질 가능성이 높다. 이 경우 모델 분할은 차단 지연을 줄임으로써 작업을 예약 가능한 것으로 만드는 데 더 효과 적이다. 따라서 태스크 수가 10개 미만일 때는 SPET w/o SA가 SPET w/o MP보다 성능이 우수하다. 반면, 작업 수 가 증가할수록 SRAM 공간 부족으로 인해 각 작업에 적절한 SRAM 크기를 할당하는 것이 더욱 중요해진다. 따라서, SPET w/o MP는 작업 수가 10보다 클 때 SPET w/o SA보다 성능이 뛰어나다. SPET는 SRAM 할당과 모델 파티셔닝을 긴밀하게 통합하여 어떤 경우에도 더 나은 스케줄링 가능성을 제공할 수 있다. 상기된 본 명세서의 실시예들에 따르면, 실시간 신경망 작업들을 스케줄링함에 있어서 각 작업의 데드라인을 고 려하여 유연하게 메모리 자원을 할당하였고 모델 분할을 통해 우선순위에 따른 선점이 가능하도록 스케줄링 동 작을 개선함으로써, 실시간 시스템의 데드라인 준수가 가능해짐과 더불어 우선순위의 역전을 방지할 수 있으며, 특히 SRAM 할당을 위해 더미 모델을 도입하고 캐싱 토큰을 수정함으로써 상용 DNN 가속기인 Edge TPU가 갖는 기 술적 장벽 및 소스 비공개의 제약을 해소하여 높은 효율의 스케줄링 가능성을 달성할 수 있다.본 명세서에 따른 실시예는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 명세서의 일 실시예는 하나 또는 그 이상의 ASICs(application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서, 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 명세서의 일 실시예는 이상에서 설명된 능력 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리에 저장되어 프로세서에 의해 구동될 수 있다. 상기 메 모리는 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 상기 프로세서와 데이터를 주고 받을 수 있다. 한편, 본 명세서의 실시예들은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로 구현하는 것 이 가능하다. 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 데이터 저장장치 등을 포함한다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크 로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고 실시예들을 구현하기 위한 기능적인(functional) 프로그램, 코드 및 코드 세그먼트들은 본 명세서가 속 하는 기술 분야의 프로그래머들에 의하여 용이하게 추론될 수 있다. 이상에서 본 명세서에 대하여 그 다양한 실시예들을 중심으로 살펴보았다. 본 명세서에 속하는 기술 분야에서 통상의 지식을 가진 자는 다양한 실시예들이 본 명세서의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형 태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되어야 한다. 본 명세서의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동 등한 범위 내에 있는 모든 차이점은 본 명세서에 포함된 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0065036", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 Edge TPU에서 SRAM(Static Random Access Memory) 할당 크기에 따른 심층 신경망(deep neural network, DNN) 작업의 실행 시간 및 실행 지연을 측정한 실험 결과를 예시한 도면이다.도 2는 종래의 Edge TPU 및 본 명세서의 실시예들이 제안하는 스케줄링 시나리오를 비교하여 예시한 도면이다. 도 3은 심층 신경망 가속기인 Edge TPU에서 신경망 작업을 스케줄링하는 과정을 설명하기 위한 도면이다. 도 4는 본 명세서의 실시예들이 제안하는 실시간 시스템에서 데드라인(deadline) 및 우선순위를 고려하여 신경 망 작업을 스케줄링하는 과정을 개략적으로 도시한 블록도이다. 도 5는 본 명세서의 실시예들이 제안하는 작업 스케줄링 기법을 Edge TPU를 통해 구현한 실시간 시스템의 구조 를 도시한 도면이다. 도 6은 본 명세서의 일 실시예에 따른 데드라인을 고려하여 실시간 신경망 작업을 스케줄링하는 방법을 도시한 흐름도이다. 도 7은 도 5의 실시간 시스템에서 메모리를 할당하는 과정을 예시한 도면이다. 도 8은 본 명세서의 다른 실시예에 따른 데드라인 및 우선순위를 고려하여 실시간 신경망 작업을 스케줄링하는 방법을 도시한 흐름도이다. 도 9는 도 5의 실시간 시스템에서 신경망 모델을 분할하는 과정을 예시한 도면이다. 도 10은 본 명세서의 또 다른 실시예에 따른 데드라인 및 우선순위를 고려하여 실시간 신경망 작업을 스케줄링 하는 장치를 도시한 블록도이다. 도 11은 본 명세서의 실시예들이 제안하는 실시간 시스템의 성능을 평가하기 위한 시뮬레이션 결과를 예시한 도 면이다."}
