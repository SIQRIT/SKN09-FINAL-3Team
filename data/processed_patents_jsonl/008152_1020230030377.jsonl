{"patent_id": "10-2023-0030377", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0041798", "출원번호": "10-2023-0030377", "발명의 명칭": "변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법 및 시스템", "출원인": "고려대학교 산학협력단", "발명자": "박종선"}}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생성 단계;하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하SNN) 모델을 시뮬레이션하는 변환 인식 학습 단계; 및시뮬레이션 결과를 기반으로 층(layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성 단계;를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 변환 인식 학습 단계는,상기 ANN 모델의 하나 이상의 층에 상기 활성화 함수를 사용하는 것을 특징으로 하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 활성화 함수는 ReLU 함수, Clip 함수 및 TTFS(Time to First Spike) 함수 중 적어도 하나 이상을 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 활성화 함수는 하기의 수학식과 같은 TTFS 함수를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법.[수학식]T는 시간, κl는 층(layer)의 커널(kernel),τ는 층의 시간 상수, tlref는 스파이크의 시작 시간,θ0는 설정 문턱값임."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2024-0041798-3-제1항에 있어서,상기 변환 인식 학습 단계에서,상기 활성화 함수를 ReLU 함수, Clip 함수, TTFS 함수 순으로 상기 ANN 모델에 사용하는 것을 특징으로 하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 SNN 생성 단계는, 적어도 하나 이상의 상기 활성화 함수를 사용한 층에 대한 매개변수 및 가중치를 변환하여 상기 SNN 모델을 생성하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생성부;하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하SNN) 모델을 시뮬레이션하는 변환 인식 학습부; 및시뮬레이션 결과를 기반으로 층(layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성부;를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 변환 인식 학습부는,상기 ANN 모델의 하나 이상의 층에 상기 활성화 함수를 사용하는 것을 특징으로 하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 활성화 함수는 ReLU 함수, Clip 함수 및 TTFS(Time to First Spike) 함수 중 적어도 하나 이상을 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 활성화 함수는 하기의 수학식과 같은 TTFS 함수를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템.공개특허 10-2024-0041798-4-[수학식]T는 시간, κl는 층(layer)의 커널(kernel),τ는 층의 시간 상수, tlref는 스파이크의 시작 시간,θ0는 설정 문턱값임."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제7항에 있어서,상기 변환 인식 학습부에서,상기 활성화 함수를 ReLU 함수, Clip 함수, TTFS 함수 순으로 상기 ANN 모델에 적용하는 것을 특징으로 하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템."}
{"patent_id": "10-2023-0030377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제7항에 있어서,상기 SNN 생성부는,적어도 하나 이상의 상기 활성화 함수를 사용한 상기 층에 대한 매개변수 및 가중치를 변환하여 상기 SNN 모델을 생성하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법 및 시스템에 관한 것으로서, 본 발명에 따 르면 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력 시키는 ANN 생성 단계, 하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 시뮬레이션하는 변환 인식 학습 단계 및 시뮬레이션 결과를 기반으로 층 (layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성 단계를 포함한다."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 변환 인식 학습 모델을 기반으로 스파이킹 뉴럴 네트워크(Spiking Neural Network)를 학습시키는 방 법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "뉴로모픽(Neuromorphic) 기술은 인간의 신경구조를 하드웨어적으로 모방하기 위한 기술로서, 기존 컴퓨팅 아키 텍처(Computing architecture)가 인지 처리 기능을 수행하는 데 인간보다 효율성이 매우 낮고 전력 소모가 크다 는 한계를 극복하기 위해 제안되었다. 따라서 전력 및 배터리가 제한적인 에지 디바이스(Edge Device)를 저전력 /저에너지로 구동하기 위한 뉴로모픽 기술이 주목받고 있다. 뉴로모픽 기술에는 대표적으로 스파이킹 신경망(Spiking Neural Network: 이하 SNN)이 있다. SNN은 인간의 두뇌 가 뉴런(Neuron)-시냅스(Synapse) 구조로 되어 있고, 뉴런과 뉴런을 잇는 시냅스가 스파이크 형태의 전기신호로 정보를 전달한다는 특징을 모방하여 고안된 신경망이다. 이러한 SNN은 스파이크 신호가 전송되는 시간 차에 기 초하여 정보를 처리한다. 이때 SNN은 바이너리 스파이크(binary spike) 신호로 정보를 전달하며, 0 또는 1인 이 진수 스파이크(spike)가 하나의 집합으로 구성된 형태로 정보를 전달한다. 상기 신호들은 SNN 내의 뉴런들을 통 해 뉴런들에 전달되고, 스파이크의 발생 여부는 다양한 생물학적인 프로세스를 나타내는 미분방정식에 의해서 결정된다. 구체적으로, 뉴런의 입력에 스파이크가 도착하면 입력 스파이크는 디코딩되어 시냅스 가중치와 연산 되고, 그 결과가 뉴런의 막전위에 누적된다. 이때 누적된 막전위값이 문턱값(역치값) 이상의 값을 가지면 그 뉴 런은 출력 스파이크를 발생시키고 그 스파이크는 다음 뉴런으로 전달된다. 이 과정에서 스파이크를 발생시킨 뉴 런의 막전위는 0으로 초기화된다. 이처럼 스파이크가 발생한 경우에만 SNN의 연산이 동작하므로 저전력 하드웨어 구현을 가능하게 한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국 등록 특허 제10-2344678호"}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 과제를 해결하기 위한 것으로서, 본 발명의 목적은 ANN 학습 데이터를 SNN 학습 데이터로 변 환하는 도중에 발생하는 데이터 손실을 최소화하는 스파이킹 뉴럴 네트워크 학습 방법 및 시스템을 제공함에 있 다. 또한, 본 발명은 SNN 학습 모델과 유사한 ANN 학습 모델을 적용한 스파이킹 뉴럴 네트워크 학습 방법 및 시스템 을 제공한다."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예로, 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생성 단계, 하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 시뮬레이션하는 변환 인식 학습 단계 및 시뮬레이션 결과 를 기반으로 층(layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성 단계를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법을 제공한다. 또한, 상술한 목적을 달성하기 위해 본 발명의 다른 실시예에서는, 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생성부, 하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 시뮬레이션하는 변환 인식 학습부 및 시뮬레이션 결과를 기반으로 층(layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성 하는 SNN 생성부를 포함하는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 제공한다."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, ANN 학습 데이터를 SNN 학습 데이터로 변환하는 중에 발생하는 데이터 손실을 최소화할 수 있다. 또한, 기존의 ANN 학습 모델로만 처리할 수 있는 고성능의 학습을 SNN 학습 모델로 수행하면서, 저전력으로 하 드웨어 구동이 가능하다."}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 본 발명의 기술사상을 본 발명의 기술분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있을 정도로 상세히 설명하기 위하여, 본 발명의 실시 예들이 첨부된 도면을 참조하여 설명될 것이다. 도 1은 일반적인 ANN-to-SNN 변환 기법을 도시한 것이다. 도 1과 같이, 고성능의 SNN을 구현하기 위해 일반적인 아날로그 인공 신경망(Artificial Neural Network; 이하 ANN)에 역전파(Back Propagation) 학습을 적용한 사전 학습을 수행한 후, 이를 SNN으로 변환시키고 있다. 이는 SNN과 ANN의 데이터 표현 방식이 다르기 때문에 적절한 가공이 필요하므로 수행하는 기법으로, 고성능 ANN의 학 습 방식인 역전파 학습을 SNN에 적용하기 어렵기 때문에 고안한 기술이다. 즉, SNN 구동 전에 ANN을 구현하여 역전파 학습을 통해 업데이트된 가중치를 획득하고, 상기 가중치를 SNN에 적용하는 변환 기법이다. 일반적인 ANN-to-SNN 변환 기법은 SNN으로만 학습시킨 데이터에 비해 높은 정확도를 보이고 있으나, 아날로그값 인 ANN의 데이터를 특정 시점에서 발생하는 이산 스파이크(discrete spike)로 변환시키면서, 데이터 손실이 발 생하는 문제가 있다. 이러한 데이터 손실을 최소화하기 위해 ANN과 SNN을 각각 학습시킨 뒤, 두 네트워크의 결 과 값을 비교하고, 비교 결과에 따라 가중치를 정규화하거나 뉴럴 네트워크 층(layer)의 파라미터를 조정하는 방법을 사용하고 있다. 그러나 이러한 방법은 하드웨어 구동에 부담이 증가하는 문제가 있어 에너지 효율적인 ANN-to-SNN 변환 기법이 필요한 실정이다. 도 2는 본 발명의 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법에 대한 순서도이다. 도 2를 참조하면, 본 발명에 따르는 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법은 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생성 단 계(S110), 하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 시뮬레이션하는 변환 인식 학습 단계(S120) 및 시뮬레이션 결과를 기반으로 층 (layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성 단계(S130)를 포함할 수 있다. 이때 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델은 DNN(Deep Neural Network), CNN(Convolution Neural Network), RNN(Recurrent Neural Network) 등일 수 있으나 이에 한정하지 않고, 스파 이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 제외한 인공지능 뉴럴 네트워크이다. 그리고 변환 인식 학습 단계(S120)는, 상기 ANN 모델의 하나 이상의 층에 상기 활성화 함수를 사용할 수 있다. 이때 상기 활성화 함수는 ReLU 함수, Clip 함수 및 TTFS(Time to First Spike) 함수 중 적어도 하나 이상을 포 함할 수 있으나 이에 한정하지 않고, SNN 시뮬레이션을 가능하게 하는 함수이다. 이때 활성화 함수는 이전 층(layer)의 결과값을 변환하여 다른 층의 뉴런으로 신호를 전달하는 역할을 하는 함 수이고, ANN 모델의 복잡도를 향상시킬 수 있다. ReLU 함수 및 Clip 함수는 공지된 기술인 활성화 함수이며, 하 기의 수학식을 사용할 수 있다. 수학식 1 수학식 2"}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "다만, TTFS(Time to First Spike) 함수는 본 발명을 구현하기 위해 개발한 최적의 활성화 함수이며, 하기의 수 학식으로 표현할 수 있다. 수학식 3"}
{"patent_id": "10-2023-0030377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이때 T는 시간, κl는 층(layer)의 커널(kernel), τ는 층의 시간 상수, tl ref는 스파이크의 시작 시간,θ0는 설 정 문턱값이다. 상기 활성화 함수의 자세한 설명은 도 3 내지 도 5를 참조하여 후술하기로 한다. 또한, SNN 생성 단계(S130)는 적어도 하나 이상의 상기 활성화 함수를 사용한 상기 층에 대한 매개변수 및 가 중치를 변환하여 상기 SNN 모델을 생성할 수 있다. 도 3은 본 발명의 일 실시예에 따른 변환 인식 학습 단계에 대한 순서도이고, 도 4는 본 발명의 일 실시예에 따 른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법을 간략하게 도시한 도면이다. 변환 인식 학습 단계(S120)는, 상기 활성화 함수를 ReLU 함수, Clip 함수, TTFS 함수 순으로 상기 ANN 모델의 층에 사용할 수 있으나, 이에 한정하지는 않는다. 도 3을 참조하면, 상기 ANN 모델의 하나 이상의 층에 제1 활성화 함수로 ReLU 함수를 사용하여 학습 초기의 안 정화를 꾀할 수 있다(S210). 그리고 상기 ReLU 함수를 사용한 다음, 제2 활성화 함수로 Clip 함수를 사용하여 안정된 SNN 시뮬레이션 동작을 수행할 수 있다(S220). 그리고 상기 Clip 함수를 사용한 다음, 제3 활성화 함수로 본 발명을 구현하기 위해 개발한 상기 TTFS 함수를 사용하여 정확도가 향상된 SNN 시뮬레이션 동작을 수행할 수 있다(S230). 도 5의 (a) 내지 (c)는 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법 에 대한 시뮬레이션 결과 그래프이다. 도 5(a)는 상기 ReLU 함수를 활성화 함수로 사용한 경우, 데이터 입력에 대한 출력 그래프이고, 도 5(b)는 상기 Clip 함수를 활성화 함수로 사용한 경우, 데이터 입력에 대한 출력 그래 프이다. 도 5(c)는 상기 TTFS 함수를 활성화 함수로 사용한 경우, 데이터 입력에 대한 출력 그래프이다. 도 5에 도시된 바와 같이, ReLU 함수, Clip 함수, TTFS 함수 순으로 활성화 함수를 사용하면, 데이터의 정확도 가 향상됨을 알 수 있다. 상용화되어 있는 데이터셋(dataset)인 CIFAR10, CIFAR100, Tiny-ImageNet 각각에 실시예 1 내지 3을 적용하였 다. 표 1은 상기 데이터셋을 ANN 모델에 학습시킨 결과 데이터의 정확도를 비교한 표이다. 실시예 1은 ANN 모델의 모든 층(layer) 각각에 Clip 함수만을 활성화 함수로 사용하였고, 실시예 2는 ANN 모델 의 첫 번째 입력층에 TTFS 함수만을 활성화 함수로 사용하였고, 실시예 3은 ANN 모델의 모든 층에 TTFS 함수를활성화 함수로 사용하였다. 표 1 실험 T/τ CIFAR10 CIFAR100 Tiny-ImageNet 실시예 148/8 92.32(-1.33) 67.93(-4.55) 58.75(-2.28) 24/4 86.99(-6.55) 52.48(-20.23) 49.04(-12.03) 12/2 62.78(-30.69) 15.07(-57.52) 17.19(-43.84) 실시예 248/8 92.85(-0.23) 70.62(-1.06) 59.31(-1.61) 4/4 90.92(-1.80) 64.25(-6.34) 51.89(-8.52) 2/2 78.21(-12.98) 33.93(-33.27) 21.18(-37.88) 실시예 348/8 93.18(-0.02) 71.72(0.00) 60.58(-0.30) 4/4 92.45(0.04) 70.30(-0.13) 59.22(-1.05) 2/2 90.77(-0.05) 66.00(-0.56) 54.99(-3.90) 표 1을 참조하면, 실시예 1 및 2에 비교하여 실시예 3의 정확도의 하락 수치가 적은 것을 알 수 있다. 따라서 ANN 모델의 모든 층에 TTFS 함수를 활성화 함수로 사용하는 경우 데이터의 정확도가 향상됨을 알 수 있다. 표 2는 종래 기술인 T2FSNN 모델(비교예)과 본 발명에 따른 실시예 4 내지 6의 성능을 비교한 표이다. 이때, 비 교예인 T2FSNN 모델은 논문 \"T2FSNN: Deep Spiking Neural Networks with Time-to-first-spike Coding\"(저자 S. Park, S. Kim, B. Na and S. Yoon)에 개시된 종래의 ANN-to-SNN 변환 기법이다. 표 2는 상용화되어 있는 데이터셋인 CIFAR10, CIFAR100, Tiny-ImageNet 각각에 비교예 및 실시예 4 내지 6을 적용하고, 성능을 비교한 표이다. 실험 조건을 살펴보면, 네트워크는 VGG16을 사용하였고, Training Length는 200 epochs, Optimizer는 SGD(모멘 텀 0.9), Learning rate는 0.1(×0.1 on epoch 80, 120, 160)이다. 표 2 비교예 실시예 4 실시예 5 실시예 6 로그함수 base e e 2 2 T 80 80 48 24 τ 20 20 8 4 지연속도 (latency)680 1,360 816 408 CIFAR10 91.43 93.36 93.18 92.45 CIFAR100 68.79 72.14 71.72 70.30 Tiny-ImageNet - 60.63 60.58 59.22 실시예 4 내지 6은 표 2에 기재된 바와 같이, 로그함수 base 및 시간 조건을 다르게 적용하였고, 본 발명에 따 른 TTFS 함수를 활성화 함수로 사용하였다. 비교예와 실시예 4 내지 6의 성능 수치를 비교하면, 모든 조건에서 본 발명의 성능이 더 높은 것을 알 수 있다. 특히 Tiny-ImageNet과 같은 복잡한 데이터셋(dataset)에는 T2FSNN 모델을 적용할 수 없었다. 따라서 본 발명에 의하면 기존의 ANN 학습 모델로만 처리할 수 있는 고성능의 학습을 SNN 학습 모델로 용이하게 변환하여 수행할 수 있다. 도 6은 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템에 대한 구성도 이다. 도 6을 참조하면, 본 발명에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템은 아날로그 인공 뉴럴 네트워크(Artificial Neural Network; 이하 ANN) 모델을 생성하고 변수 데이터를 입력시키는 ANN 생 성부, 하나 이상의 활성화 함수를 상기 ANN 모델에 사용하여 스파이킹 뉴럴 네트워크(Spiking Neural Network; 이하 SNN) 모델을 시뮬레이션하는 변환 인식 학습부 및 시뮬레이션 결과를 기반으로 층(layer)의 매개변수 및 가중치를 보정하여 SNN 모델을 생성하는 SNN 생성부를 포함할 수 있다. 이때 변환 인식 학습부는 상기 ANN 모델의 하나 이상의 층에 상기 활성화 함수를 사용할 수 있다. 상기 활 성화 함수는 ReLU 함수, Clip 함수 및 TTFS(Time to First Spike) 함수 중 적어도 하나 이상을 포함할 수 있으 나 이에 한정하지 않고, SNN 시뮬레이션을 가능하게 하는 함수이다. 상기 활성화 함수의 자세한 설명은 전술한 바와 같다. 또한, 변환 인식 학습부에서, 상기 활성화 함수를 ReLU 함수, Clip 함수, TTFS 함수 순으로 상기 ANN 모델 의 층에 사용할 수 있으나, 이에 한정하지는 않는다. 그리고 SNN 생성부는 적어도 하나 이상의 상기 활성화 함수를 사용한 상기 층에 대한 매개변수 및 가중치 를 변환하여 상기 SNN 모델을 생성할 수 있다. 도 7은 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 적용한 처리 장치에 대한 블럭도이다. 도 7을 참조하면, 본 발명에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 적용한 처리 장치는 입력 생성기(generator), PE(Process Element) 어레이, 출력 처리 장치 및 출력 제어 장치로 크게 4가 지의 구성으로 나타낼 수 있다. 상기 입력 생성기는 48KB 및 minfind unit의 입력 버퍼(buffer)로 구성되어 있고, 입력 스파이크를 병합한다. 상기 PE 어레이는 128 개의 PE 및 4 개의 90KB 가중치 버퍼로 구성되며, 본 발명에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 포함한다. 그리고 상기 출력 처리 장치는 후처리 장치(PPU) 및 스파이크 인코더(encoder)로 구성되어 있고, PE 어레이의 출력을 스파이크로 처리하고, 출력 스파이크를 출력 버퍼에 저장한 다음, D램에 스파이크 정보를 전송한다. 또 한, 상기 출력 제어 장치는 전체 처리 장치를 제어하고, DMA 엔진은 오프칩 D램에 대한 데이터 액세스를 관리한 다. 이때 입력 스파이크는 입력 발생기에서 정렬되는 방식으로 처리되고, 정렬된 스파이크가 PE 어레이에 공급되어 막전위로 축적된다. PE 어레이의 출력은 출력 처리 장치로 전달되어 출력 스파이크(발화 단계)로 인코딩된다. 따라서 상기 처리 장치에 본 발명에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 적용함 으로써, ANN 학습 데이터를 SNN 학습 데이터로 변환하는 도중에 발생하는 데이터 손실을 최소화한다. 따라서 본 발명에 의하면 기존의 ANN 학습 모델로만 처리할 수 있는 고성능의 학습을 SNN 학습 모델로 수행하면 서, 저전력으로 하드웨어 구동이 가능하다. 상술한 내용은 본 발명을 실시하기 위한 구체적인 실시 예들이다. 본 발명은 상술한 실시 예들 이외에도, 단순 하게 설계 변경되거나 용이하게 변경할 수 있는 실시 예들도 포함될 것이다. 또한, 본 발명은 실시 예들을 이용 하여 용이하게 변형하여 실시할 수 있는 기술들도 포함될 것이다. 따라서, 본 발명의 범위는 상술한 실시 예들 에 국한되어 정해져서는 안되며, 후술하는 특허청구범위뿐만 아니라 이 발명의 특허청구범위와 균등한 것들에 의해 정해져야 할 것이다."}
{"patent_id": "10-2023-0030377", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 ANN-to-SNN 변환 기법을 도시한 도면이다. 도 2는 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법에 대한 순서도이 다. 도 3은 본 발명의 일 실시예에 따른 변환 인식 학습 단계에 대한 순서도이다. 도 4는 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법을 간략하게 도시 한 도면이다. 도 5의 (a) 내지 (c)는 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 방법 에 대한 시뮬레이션 결과 그래프이다. 도 6은 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템에 대한 구성도이다. 도 7은 본 발명의 일 실시예에 따른 변환 인식 학습 기반의 스파이킹 뉴럴 네트워크 학습 시스템을 적용한 처리 장치에 대한 블럭도이다."}
