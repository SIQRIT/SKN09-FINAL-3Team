{"patent_id": "10-2022-0160559", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0078028", "출원번호": "10-2022-0160559", "발명의 명칭": "동영상 언어 변환 방법 및 장치", "출원인": "주식회사 브레인봇", "발명자": "박태준"}}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "프로세서로 구현되는 동영상 언어 변환 방법으로서, 제1 언어로 된 동영상 컨텐츠로부터 음성을 추출하고 상기 제1 언어를 번역한 제2 언어로 된 음성을 합성하는단계; 상기 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하고, 상기 추출된 입모양 영역에 대해 딥러닝 기반 입모양 영상 생성 알고리즘 모델을 적용하여 상기 제2 언어로 된 음성에 대응하는 입모양 영상을 생성하는 단계;및 상기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상컨텐츠를 생성하는 단계;를 포함하는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 음성을 합성하는 단계는 상기 추출된 음성에 대해 음성 인식 과정, 기계 번역 과정 및 음성 합성 과정을순차적으로 수행하며, 상기 음성 인식 과정에서는 발화자 음성 특징을 추출하고, 상기 음성 합성 과정에서는 상기 추출된 발화자 음성특징과 기학습된 상기 제2 언어의 특징에 대한 임베딩을 이용해서 상기 제2 언어로 된 음성을 합성하는, 동영상언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 딥러닝 기반 입모양 영상 생성 알고리즘 모델은 상기 추출된 입모양 영역에서 발화자의 입모양 특징점들을 추출하고, 기학습된 변환 테이블에 따라서 상기 입모양 특징점들에 대응하는 변환 후 입모양 특징점들을 도출하며-상기 변환 테이블은 다수의 표본들에 대한 표본별 제1 언어 발음시의 입모양 특징점들과 그와 동일한 뜻을 갖는 제2 언어 발음시의 입모양간 특징점들간 대응관계를 포함함-,"}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 제2 언어로 된 음성의 개수와, 상기 생성된 입모양 영상의 개수는 일치하는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제2 언어로 된 음성의 영상 내 재생시간과, 상기 생성된 입모양 영상의 영상 내 재생시간은 일치하는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서, 상기 도출하는 과정에서는 인공 신경망에 기초한 알고리즘 모델을 이용하여 특징점 매칭을 수행하되, 상기 알고리즘 모델은 입력 노드로 구성된 입력 레이어, 출력 노드로 구성된 출력 레이어 및 입력 레이어와 출공개특허 10-2024-0078028-3-력 레이어 사이에 배치되며, 은닉 노드로 구성된 하나 이상의 은닉 레이어를 포함하고, 학습을 통해 노드들을연결하는 에지의 가중치 및 노드들의 바이어스가 업데이트되는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 학습된 알고리즘 모델의 입력 레이어로 서로 다른 시점에서 센싱된 센싱 데이터 및 서로 다른 시점에서 획득된 영상에 포함된 특징점의 정보 중 적어도 하나가 입력되고, 상기 학습된 알고리즘 모델의 출력 레이어로 특징점 매칭 정보가 출력되는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 음성을 합성하는 단계는 멀티 스피커 TTS 모델을 통하여 상기 제2 언어로 된 음성을 합성하는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 음성을 합성하는 단계는 상기 추출된 음성에 대해 딥러닝 기반의 STT 모델, 기계번역 모델 및 TTS 모델을통하여 상기 제2 언어로 된 음성을 합성하는, 동영상 언어 변환 방법."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항의 방법을 수행하기 위한 명령어를 포함하는 하나 이상의 컴퓨터 프로그램을 저장한 컴퓨터 판독 가능 기록 매체."}
{"patent_id": "10-2022-0160559", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "동영상 언어 변환 장치에 있어서, 동영상 컨텐츠를 획득하는 동영상 컨텐츠 획득부; 제1 언어로 된 상기 동영상 컨텐츠로부터 음성을 추출하여 상기 제1 언어를 번역한 제2 언어로 된 음성을 합성하고, 상기 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하여 상기 추출된 입모양 영역에 대해 딥러닝 기반 입모양 영상 생성 알고리즘 모델을 적용함으로써 상기 제2 언어로 된 음성에 대응하는 입모양 영상을 생성하며, 상기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨텐츠를 생성하는 프로세서; 및상기 생성된 동영상 컨텐츠를 표시하는 디스플레이;를 포함하는 동영상 언어 변환 장치."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 기술은 동영상 언어 변환 방법 및 장치에 관한 것이다. 본 기술의 동영상 언어 변환 방법은 프로세서로 구현 되는 동영상 언어 변환 방법으로서, 제1 언어로 된 동영상 컨텐츠로부터 음성을 추출하고 상기 제1 언어를 번역 한 제2 언어로 된 음성을 합성하는 단계; 상기 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하고, 상기 추 출된 입모양 영역에 대해 딥러닝 기반 입모양 영상 생성 알고리즘 모델을 적용하여 상기 제2 언어로 된 음성에 대응하는 입모양 영상을 생성하는 단계; 및 상기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨텐츠를 생성하는 단계를 포함한다. 본 기술은 외국어 영상에서 영상 과 음성을 각각 추출하여 모국어에 알맞은 새로운 영상과 음성 서비스를 지원할 수 있는 시스템을 제공할 수 있 다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 동영상 언어 변환 방법 및 장치에 관한 것으로, 보다 구체적으로는 인공지능 알고리즘을 적용한 동영 상 언어 변환 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기존에 영상 컨텐츠에서 음성을 추출하여 기계번역을 통한 연동 서비스 제공 시스템이 있다. 관련 선행문헌으로 대한민국 특허등록번호 제10-2199050호(발명명칭: 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법 및 시스템)은 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이용한 음성 번역 방법으로서, 제1 언어의 학습 텍스트 및 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터 와, 제2 언어의 학습 텍스트 및 상기 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초 하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계; 제1 언어의 입력 음성 데이터 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계; 상기 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환하는 단계; 상기 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계; 및 상기 제2 언어의 텍스트 및 상기 화자의 발성 특징을 상기 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 상기 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함함을 보여준다. 그러나 상기 선행특허는 비디오 번역시 제1 언어로 된 입력 음성 데이터로부터 화자의 음성을 모사하는 수준의 제2 언어로 된 출력 음성 데이터를 생성하는 것으로서 더빙 수준에 그친다. 더빙 수준의 비디오 번역은 시청자에게 원작의 흥미를 잃게 만든다. 자막을 읽는 불편함을 감수하더라도 더빙 영화보다는 자막 영화를 더 선호하는 것과 같은 이유이다. 이에 실제 비디오 속 등장인물이 다른 외국어로 발화하는 것과 같은 생생한 느낌을 전달할 컨텐츠 제작의 필요 성이 존재한다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 실시예는 외국어 영상에서 영상과 음성을 각각 추출하여 모국어에 알맞은 새로운 영상과 음성 서비스 를 지원할 수 있는 시스템을 제공한다. 한편, 본 발명의 명시되지 않은 또 다른 목적들은 하기의 상세한 설명 및 그 효과로부터 용이하게 추론할 수 있 는 범위 내에서 추가적으로 고려될 것이다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 프로세서로 구현되는 동영상 언어 변환 방법으로서, 제1 언어로 된 동영상 컨텐츠로부터 음성 을 추출하고 상기 제1 언어를 번역한 제2 언어로 된 음성을 합성하는 단계; 상기 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하고, 상기 추출된 입모양 영역에 대해 딥러닝 기반 입모양 영상 생성 알고리즘 모델을 적용 하여 상기 제2 언어로 된 음성에 대응하는 입모양 영상을 생성하는 단계; 및 상기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨텐츠를 생성하는 단계;를 포함할 수 있다. 상기 음성을 합성하는 단계는 상기 추출된 음성에 대해 음성 인식 과정, 기계 번역 과정 및 음성 합성 과정을 순차적으로 수행하며, 상기 음성 인식 과정에서는 발화자 음성 특징을 추출하고, 상기 음성 합성 과정에서는 상 기 추출된 발화자 음성 특징과 기학습된 상기 제2 언어의 특징에 대한 임베딩을 이용해서 상기 제2 언어로 된 음성을 합성할 수 있다. 상기 딥러닝 기반 입모양 영상 생성 알고리즘 모델은 상기 추출된 입모양 영역에서 발화자의 입모양 특징점들을 추출하고, 기학습된 변환 테이블에 따라서 상기 입모양 특징점들에 대응하는 변환 후 입모양 특징점들을 도출하 며-상기 변환 테이블은 다수의 표본들에 대한 표본별 제1 언어 발음시의 입모양 특징점들과 그와 동일한 뜻을 갖는 제2 언어 발음시의 입모양간 특징점들간 대응관계를 포함함-, 상기 변환 후 입모양 특징점들을 기반으로 상기 추출된 입모양 영역을 수정하는 과정을 포함할 수 있다. 상기 제2 언어로 된 음성의 개수와, 상기 생성된 입모양 영상의 개수는 일치할 수 있다. 상기 제2 언어로 된 음성의 영상 내 재생시간과, 상기 생성된 입모양 영상의 영상 내 재생시간은 일치할 수 있 다. 상기 도출하는 과정에서는 인공 신경망에 기초한 알고리즘 모델을 이용하여 특징점 매칭을 수행하되, 상기 알고 리즘 모델은 입력 노드로 구성된 입력 레이어, 출력 노드로 구성된 출력 레이어 및 입력 레이어와 출력 레이어 사이에 배치되며, 은닉 노드로 구성된 하나 이상의 은닉 레이어를 포함하고, 학습을 통해 노드들을 연결하는 에 지의 가중치 및 노드들의 바이어스가 업데이트될 수 있다. 상기 학습된 알고리즘 모델의 입력 레이어로 서로 다른 시점에서 센싱된 센싱 데이터 및 서로 다른 시점에서 획 득된 영상에 포함된 특징점의 정보 중 적어도 하나가 입력되고, 상기 학습된 알고리즘 모델의 출력 레이어로 특 징점 매칭 정보가 출력될 수 있다. 상기 음성을 합성하는 단계는 멀티 스피커 TTS 모델을 통하여 상기 제2 언어로 된 음성을 합성할 수 있다. 상기 음성을 합성하는 단계는 상기 추출된 음성에 대해 딥러닝 기반의 STT 모델, 기계번역 모델 및 TTS 모델을 통하여 상기 제2 언어로 된 음성을 합성할 수 있다. 또한 일 실시예에 따른 컴퓨터 판독 가능 기록 매체는 상기 어느 한 방법을 수행하기 위한 명령어를 포함하는 하나 이상의 컴퓨터 프로그램을 저장할 수 있다. 또한 일 실시예에 따른 동영상 언어 변환 장치에 있어서, 동영상 컨텐츠를 획득하는 동영상 컨텐츠 획득부; 제1 언어로 된 상기 동영상 컨텐츠로부터 음성을 추출하여 상기 제1 언어를 번역한 제2 언어로 된 음성을 합성하고, 상기 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하여 상기 추출된 입모양 영역에 대해 딥러닝 기반 입모 양 영상 생성 알고리즘 모델을 적용함으로써 상기 제2 언어로 된 음성에 대응하는 입모양 영상을 생성하며, 상 기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨 텐츠를 생성하는 프로세서; 및 상기 생성된 동영상 컨텐츠를 표시하는 디스플레이;를 포함할 수 있다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 기술은 외국어 영상에서 영상과 음성을 각각 추출하여 모국어에 알맞은 새로운 영상과 음성 서비스를 지원할 수 있는 시스템을 제공할 수 있다. 또한 본 기술은 영상 서비스의 글로벌화에 따라 다양한 언어의 영상을 모국어 음성으로 변환하여 재생산된 영상 서비스로 제공함으로써, 외국어 영상에 대한 언어적 장벽을 낮게 함으로써 컨텐츠 소비 활성화를 도모한다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는, 본 발명의 가장 바람직한 실시예가 설명된다. 도면에 있어서, 두께와 간격은 설명의 편의를 위하여 표현된 것이며, 실제 물리적 두께에 비해 과장되어 도시될 수 있다. 본 발명을 설명함에 있어서, 본 발명의 요 지와 무관한 공지의 구성은 생략될 수 있다. 각 도면의 구성요소들에 참조 번호를 부가함에 있어서, 동일한 구 성 요소들에 한해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 번호를 가지도록 하고 있음에 유의하 여야 한다. 실시예들에 대한 특정한 구조적 또는 기능적 설명들은 단지 예시를 위한 목적으로 개시된 것으로서, 다양한 형 태로 변경되어 구현될 수 있다. 따라서, 실제 구현되는 형태는 개시된 특정 실시예로만 한정되는 것이 아니며, 본 명세서의 범위는 실시예들로 설명한 기술적 사상에 포함되는 변경, 균등물, 또는 대체물을 포함한다. 제1 또는 제2 등의 용어를 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성요소 를 다른 구성요소로부터 구별하는 목적으로만 해석되어야 한다. 예를 들어, 제1 구성요소는 제2 구성요소로 명 명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 설명된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함 으로 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들 을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 해당 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해 석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 이하, 실시예들을 첨부된 도면들을 참조하여 상세하게 설명한다. 첨부 도면을 참조하여 설명함에 있어, 도면 부 호에 관계없이 동일한 구성 요소는 동일한 참조 부호를 부여하고, 이에 대한 중복되는 설명은 생략하기로 한다. 외국어로 된 영상에서 영상과 음성을 각각 추출하여 모국어에 알맞은 새로운 영상과 음성 서비스를 지원할 수 있는 시스템을 구성한다. 음성을 추출, 기계번역을 통한 모국어 변환, 그 후 모국어에 대한 음성 합성을 통해 음성출력을 지원한다. 합성된 음성을 기준으로 영상에 새롭게 립싱크에 맞추어 입 모양을 생성함으로써 기존 영 상에 생성된 타 언어 음성과 이에 맞는 입 모양이 합쳐진 새로운 영상을 제공한다. 도 1은 일 실시예에 따른 동영상 언어 변환 방법을 설명하는 흐름도이다. 도면에 도시된 바와 같이 동영상 언어 변환 방법은 음성을 합성하는 단계(S10), 입모양 영상을 생성하는 단계(S20), 및 동영상 컨텐츠를 생성하는 단 계(S30)를 포함한다. 단계(S10)에서 동영상 언어 변환 장치는 제1 언어로 된 동영상 컨텐츠로부터 음성을 추출하고 제1 언어를 번역 한 제2 언어로 된 음성을 합성한다. 동영상 컨텐츠는 컴퓨터와 같은 단말기에서 재생 가능한 상태의 멀티미디어 파일일 수 있다. 반드시 파일일 필 요는 없으며 클라우드 서버에 접속 가능한 상태의 링크 형태여도 무방하다. 단말기는 스마트폰, 스마트TV, PC, 노트북 등일 수 있다. 이하에서는 설명의 편의를 위해, 제1 언어가 영어이고, 제2 언어가 한글인 실시예를 중심 으로 설명하나 이에 한정되지 않고 다양한 언어에 대해 적용된다. 단계(S10)에서는 먼저 도 2a에 도시된 바와 같이, 동영상 컨텐츠로부터 음성(즉, 원본 영상의 음성)을 추출한다. 동영상 컨텐츠로부터 음성을 추출하기 위해 알려진 음성 추출 알고리즘이 적용될 수 있다. 또한, 추 출된 음성은 단계(S10)에서 텍스트 번역 과정에 활용되는 바, 번역의 정확도를 높이기 위해 추출된 음성에 대해 배경음이나 잡음을 제거하는 처리 과정이 함께 적용될 수 있다. 이후, 추출된 음성에 대해 도 2b에 도시된 바와 같이, 음성 인식(S12), 기계 번역(S14) 및 음성 합성(S16) 과정 들을 순차적으로 수행한다. 일 실시예에 따르면 이들 과정들은 하나의 알고리즘 모델로 구현될 수 있다. 보다 상세하게, 음성 인식(S12) 과정에서는 도 2c에 도시된 바와 같이, 화자 및 언어 인식(S122)을 통해서 현재 언어에 대한 텍스트를 출력하고(S126) 화자 음성 특징을 추출한다S128). 출력된 텍스트는 기계 번역(S14)에 활 용된다. 그리고, 추출된 화자 음성 특징은 후속하는 음성 합성 과정에 활용된다. 이때, 텍스트 출력을 위해서, 딥러닝 기반의 STT 과정이 수행될 수 있다(S124). 이를 위해 알려진 STT(Speech To Text) 모델이 적용될 수 있다. 다음으로, 기계 번역(S14) 과정에서는 도 2d에 도시된 바와 같이, 제1 언어로 된 텍스트(즉, 원본 영상의 텍스 트)를 제2 언어로 된 텍스트(즉, 타겟 언어 텍스트)로 번역하기 위해, 기계번역(Machine translation)을 수행한 다. 기계번역은 딥러닝 기반일 수 있다. 일 실시예에 따르면 기계번역 과정은 자체적으로 수행될 수도 있고, 외 부 번역 서버로 제1 언어로 된 텍스트를 제공한 후 제2 언어로 번역된 결과물을 제공받을 수도 있다. 어느 경우 나 위 과정(S14)에서 수행된다는 점에서는 공통된다. 이어서, 음성 합성(S16) 과정에서는 도 2e에 도시된 바와 같이, 출력된 제2 언어로 된 텍스트(즉, 타겟 언어 텍 스트)와 앞서 추출된 화자의 음성 특징 및 사전에 학습된 타겟 언어의 특징에 대한 임베딩을 이용해서 음성 합 성을 수행한다. 음성 합성을 위해, 딥러닝 기반의 TTS 과정이 수행될 수 있다. 이를 위해 알려진 TTS(Text To Speech) 모델이 적용될 수 있다. TTS 모델은 원본 영상에 나오는 음성과의 격차를 줄이기 위해서 멀티 스피커(Multi Speaker) TTS 모델일 수 있다. 이를 이용하여 원본 영상에서 추출된 음성과 비슷한 음성을 합성할 수 있다. 보다 상세하게, 음성 합성(S16)에서는 영상 속 발화자의 목소리 정보가 학습된 딥러닝 기반 음성합성을 통하여 발화자의 특성을 유지한 채 원하는 언어의 음성이 자연스럽게 출력될 수 있도록 하는 음성 합성을 수행할 수 있 으며, 이를 위해, 도 2e에 도시된 바와 같이, 타겟 언어 텍스트, 화자 음성 특징 및 타겟 언어 특징을 딥러닝 TTS 모델에 넣어 Mel scale로 변환된 주파수 대역을 얻고, 이를 Neural Vocoder에 넣음으로써 타겟 언어로 된 음성을 합성할 수 있다. 다음으로, 단계(S20)에서 동영상 언어 변환 장치는 도 3에 도시된 바와 같이 상술한 단계(S10)에서 합성된 음성 (즉, 타겟 언어 음성)을 토대로 원본 영상(즉, 제1 언어로 된 동영상 컨텐츠)에 립싱크를 맞출 수 있도록 영상 을 변환한다. 이를 위해, 단계(S20)에서는 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하고, 추출된 입모양 영역에 대 해 딥러닝 기반 입모양 영상 생성 알고리즘 모델을 적용하여 제2 언어로 된 음성에 대응하는 입모양 영상을 생 성한다. 즉, 단계(S20)에서는 상술한 단계(S10)에서 생성된 음성에 맞는 입모양 영상을 생성한다. 입모양 영상은 제2 언어로 된 음성에 대응하여 생성되는 바, 제2 언어로 된 음성의 재생 시간과 대체로 일치할 수 있고, 바람직하게는 정확히 일치할 수 있다. 또한, 제2 언어로 된 음성의 개수와 대체로 일치하는 입모양 영 상이 생성될 수 있고, 바람직하게는 정확히 일치할 수 있다. 예를 들어, 동영상 컨텐츠에서 문장이 100개 등장 해서 제2 언어로 된 음성이 100개인 경우 입모양 영상도 100개가 생성될 수 있다. 한편, 제2 언어로 된 음성이 영상에서 배치되는 위치와 생성된 입모양 영상이 영상에서 배치되는 위치는 각각 태그 정보로서 포함되어 원본 영상에서의 제1 언어로 된 음성과 싱크를 맞출 수 있다. 요컨대, 단계(S20)에서는 제1 언어로 발화하는 발화자의 입모양 영상으로부터 제2 언어로 발화하는 발화자의 입 모양 영상을 생성하는 것이다. 이로써, 후술하는 바와 같이, 입모양 영상은 단계(S20)에서 생성된 후 후속 단계 (S30)에서 원본 영상과 합성됨으로써 마치 원래의 발화자가 제1 언어('Hi, what a beautiful night.'와 같이) 대신 번역된 제2 언어('안녕하세요, 아름다운 밤입니다'와 같이)로 발화하는 것과 같은 시각 정보를 시청자에게 전달할 수 있다. 단계(S20)에서는 입모양 영상 생성을 위해, 먼저, 도 4 및 도 5에 도시된 바와 같이, 원본 영상으로부터 입모양 영역을 추출한다. 본 발명에서 영상이라 함은 프레임 단위를 포함하는 것일 수 있다. 동영상 컨텐츠(AVC)로부터 발화자의 입모양 영역(ROI)을 추출하기 위해 입력 영상으로부터 관심 영역(ROI)을 추 출하기 위한 알려진 영상 추출 알고리즘이 적용될 수 있다. 본 발명에서 영상 추출이라 함은 단계(S10)에서의 음성 추출과 대비되는 것으로서, 반드시 어떠한 영상 추출 과정을 포함해야 하는 것은 아니며, 동영상 컨텐츠가 편집이 가능한 상태에 놓인 것이면 된다. 즉, 동영상 컨텐츠로부터 발화자의 입모양 영역을 추출하여 분석할 수 있고, 후속 과정에서 원본 영상과 합성될 수 있는 상태에 놓인 것이면 된다. 한편, 발화자의 입모양 영역은 반드시 입에만 한정되지 않으며, 발화자의 입 주위에 해당하는 입가 영역(R1, R3), 인중 영역(R2), 턱 영역(R4) 등을 포함할 수 있다. 즉, 입모양을 표현하기 위한 입뿐만 아니라 입 주위까 지 포함할 수 있다. 도 6은 일 실시예에 따른 입모양 영상을 생성하기 위한 딥러닝기반 입모양 영상 생성 알고리즘 모델을 설명하기 위한 흐름도이다. 도 6을 참조하면, 단계(S20)에서 적용되는 딥러닝(Deep Learning) 기반 입모양 영상 생성 알 고리즘 모델은 발화자의 입모양 영역(ROI)에서 발화자의 입모양 특징점들을 추출하고(S22), 기학습된 변환 테이 블에 따라서 입모양 특징점들에 대응하는 변환 후 입모양 특징점들을 도출하며(S24), 변환 후 입모양 특징점들 을 기반으로 발화자의 입모양 영역(ROI)을 수정하는(S26) 과정을 포함한다. 이때, 변환 테이블은 다수의 표본들에 대한 표본별 제1 언어 발음시의 입모양 특징점들과 그와 동일한 뜻을 갖 는 제2 언어 발음시의 입모양간 특징점들간 대응관계를 포함한다. 다수의 표본들은 기계학습의 대상이 되며, 변 환 테이블은 기계학습의 결과물일 수 있으며, 지속적으로 업데이트될 수 있다. 예를 들어, 변환 테이블은 매우 많은 발화자들(표본들)에 대해 소정의 영어문장 발음시의 입모양 특징점 정보와 이를 번역한 한글문장 발음시의 입모양 특징점 정보간의 대응관계를 포함할 수 있다. 이때, 문장 단위가 아닌 단어 단위로 발음시의 입모양 특 징점 정보간 대응관계를 포함할 수도 있다. 이러한 변환 테이블은 입력된 제1 언어의 입모양 특징점들에 대해 가장 일치율이 높은 제2 언어의 입모양 특징점들을 도출한다. 입모양 영역(ROI)에서 특징점들을 추출하는 것은 입 주위 여러 영역들(R1, R2, R3, R4)에 대해 수행될 수 있다. 일 실시예에 따른 단계(S20)는 특징점 매칭부에 의해 수행될 수 있다. 특징점 매칭부는 인공 신경망에 기초한 알고리즘 모델을 이용하여 특징점 매칭을 수행할 수 있다. 인공 신경망(ANN: Artificial Neural Network)은 생물학적 뉴런의 동작 원리와 뉴런 간의 연결 관계를 모델링한 것으로 노드(node) 또는 처리 요소(processing element)인 다수의 뉴런(neuron)들이 레이어(layer) 구조의 형 태로 연결된 정보 처리 시스템이다. 인공 신경망은 복수의 레이어를 포함할 수 있고, 레이어들 각각은 복수의 뉴런을 포함할 수 있다. 또한, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅스(synapse)를 포함할 수 있다. 즉, 인공 신경망은 시냅스의 결합으로 네트워크를 형성한 인공 뉴런이 학습을 통해 시냅스의 결합 세기를 변화시켜 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경망은 일반적으로 다음의 세가지 인자, 즉 다른 레이어의 뉴런들 사이의 연결 패턴, 시냅스의 가중치를 갱신하는 학습 과정, 이전 레이어로부터 수신되는 입력에 대한 가중 합으로부터 출력값을 생성하 는 활성화 함수에 의해 정의될 수 있다. 인공 신경망은, DNN(Deep Neural Network), RNN(Recurrent Neural Network), BRDNN(Bidirectional Recurrent Deep Neural Network), MLP(Multilayer Perceptron), CNN(Convolutional Neural Network)와 같은 방식의 네트워크 모델들을 포함할 수 있으나, 이에 한정되지 않는다. 인공신경망은 계층 수에 따라 단층 신경망(Single-Layer Neural Networks)과 다층 신경망(Multi-Layer Neural Networks)으로 구분된다. 일반적인 단층 신경망은, 입력 레이어와 출력 레이어로 구성된다. 또한, 일반적인 다 층 신경망은 입력 레이어(input layer)와, 하나 이상의 은닉 레이어(hidden layer)와, 출력층(output layer)으 로 구성된다.입력 레이어는 외부의 자료들을 받아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어는 입력 레이어와 출력 레이어 사이에 위치하며, 입력 레이어로부터 신호를 받아 특 성을 추출하여 출력 레이어로 전달한다. 출력 레이어는 은닉 레이어로부터 신호를 받고, 수신한 신호에 기반한 출력 값을 출력한다. 뉴런 간의 입력 신호는 각각의 가중치(연결 강도)와 곱해진 후 합산되며 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 획득한 출력값을 출력한다. 한편, 입력 레이어와 출 력 레이어 사이에 복수의 은닉 레이어를 포함하는 심층 신경망은, 기계 학습 기술의 한 종류인 딥 러닝을 구현 하는 대표적인 인공 신경망일 수 있다. 인공 신경망은 훈련 데이터(training data)를 이용하여 학습(training)될 수 있다. 여기서 학습이란, 입력 데이 터를 분류(classification)하거나 회귀분석(regression)하거나 군집화(clustering)하는 등의 목적을 달성하기 위하여, 학습 데이터를 이용하여 인공 신경망의 파라미터(parameter)를 결정하는 과정을 의미할 수 있다. 시냅 스 가중치(weight)나 뉴런에 적용되는 바이어스(bias)가 인공 신경망의 파라미터의 대표적인 예이다. 훈련 데이 터에 의하여 학습된 인공 신경망은, 입력 데이터를 입력 데이터가 가지는 패턴에 따라 분류하거나 군집화할 수 있다. 한편 훈련 데이터를 이용하여 학습된 인공 신경망을, 본 명세서에서는 학습 모델(trained model)이라 명 칭할 수 있다. 인공 신경망의 학습 방식은 지도 학습(supervised learning), 비 지도 학습(unsupervised learning), 준 지도 학습(semi-supervised learning), 강화 학습(reinforcement learning)을 포함할 수 있다. 상기 내용을 참조하면, 단계(S20)의 특징점 매칭을 위한 인공 신경망에 기초한 알고리즘 모델은 입력 노드로 구 성된 입력 레이어, 출력 노드로 구성된 출력 레이어 및 입력 레이어와 출력 레이어 사이에 배치되며, 은닉 노드 로 구성된 하나 이상의 은닉 레이어를 포함한다. 이때, 알고리즘 모델은 학습 데이터에 의해 학습되며, 학습을 통해 노드들을 연결하는 에지의 가중치 및 노드의 바이어스가 업데이트될 수 있다. 한편, 일 실시예에 따르면, 상기 학습된 알고리즘 모델은 특징점 매칭부에 탑재될 수 있다. 이때, 서로 다른 시 점에서 센싱된 센싱 데이터 및 서로 다른 시점에서 획득된 영상에 포함된 특징점의 정보 중 적어도 하나가 학습 된 알고리즘 모델의 입력 레이어에 입력되고, 학습된 알고리즘 모델의 출력 레이어로 특징점 매칭 정보가 출력 될 수 있다. 다른 일 실시예에 따르면, 상기 학습된 알고리즘 모델은 외부의 AI 서버에 탑재될 수 있다. 이 경 우, 특징점 매칭부는 통신부를 통해 서로 다른 시점에서 센싱된 센싱 데이터 및 서로 다른 시점에서 획득된 영 상에 포함된 특징점의 정보 중 적어도 하나를 AI 서버로 전송한다. 서로 다른 시점에서 센싱된 센싱 데이터 및 서로 다른 시점에서 획득된 영상에 포함된 특징점의 정보 중 적어도 하나가 AI 서버의 학습된 알고리즘 모델의 입력 레이어에 입력되고, 학습된 알고리즘 모델의 출력 레이어로 특징점 매칭 정보가 출력될 수 있다. AI 서버 는 출력된 특징점 매칭 정보를 통신부를 통해 특징점 매칭부로 전송하고, 이를 통해 특징점 매칭부는 특징점을 매칭할 수 있다.상술한 과정을 통해, 도 3에 도시된 바와 같이, 원본 영상으로부터 추출된 입모양 영역에 대해 제2 언어로 된 음성에 대응하는 입모양 영상을 생성할 수 있다. 이어서, 단계(S30)에서 동영상 언어 변환 장치는 단계(S20)에서 생성된 입모양 영상 및 단계(S10)에서 생성된 제2 언어로 된 음성을 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨텐츠를 생성한다. 즉, 제1 언어로 된 원본 영상으로부터 시각적으로 및 청각적으로 제2 언어로 된 수정 영상을 생성한다. 단계(S30)에서는 음성 및 영상간 합성을 위해 알려진 합성 알고리즘을 적용할 수 있다. 단계(S30)는 일종의 동영상 컨텐츠를 재생산하는 과정이라 볼 수 있다. 재생산을 위한 시각적인 수정을 위해 단 계(S20)의 입모양 영상이 활용되고, 청각적인 수정을 위해 단계(S10)의 제2 언어로 된 음성이 활용된다. 이때, 시각적인 수정 요소와 청각적인 수정 요소가 원본 영상과 싱크가 잘 맞도록 재생산을 하는 것이 바람직하다. 기 존의 음성과 새롭게 생성된 음성은 언어가 서로 달라서 재생 지속 시간이 다르기 때문이다. 이를 위해, 제2 언 어로 된 음성이 영상에서 배치되는 위치가 저장된 태그 정보 및 생성된 입모양 영상이 영상에서 배치되는 위치 가 저장된 태그 정보를 활용할 수 있다. 이로써 기존의 외국어 영상에 대한 모국어 지원 방식은 음성 추출을 통한 기계 번역으로 제공되는 방식 대비, 일 실시예에 따르면, 모국어에 대한 음성 지원 방식의 서비스가 지원함과 동시에 모국어 음성을 통한 립싱크 영 상 재생성을 통하여 영상에 대한 새로운 서비스를 제공할 수 있다. 도 7은 일 실시예에 따른 동영상 언어 변환 장치의 하드웨어 구현의 예를 도시한 도면이다. 일 실시예에 따른 동영상 언어 변환 장치는 동영상 컨텐츠 획득부, 프로세서, 메모리, 및 디스플레이 를 포함할 수 있다. 동영상 컨텐츠 획득부는 동영상 컨텐츠를 획득할 수 있다. 동영상 컨텐츠 획득부는 동영상 컨텐츠 를 생성하는 카메라, 동영상 컨텐츠를 수신하는 통신부 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리에 저장된 뉴럴 네트워크 모델을 이용하여 제1 언어로 된 동영상 컨텐츠로부터 음성을 추출하여 제1 언어를 번역한 제2 언어로 된 음성을 생성할 수 있다. 또한 프로세서는 동영상 컨텐 츠로부터 발화자의 입모양 영역을 추출하여 상기 추출된 입모양 영역에 대해 딥러닝 기반 입모양 영상 생성 알 고리즘 모델을 적용함으로써 제2 언어로 된 음성에 대응하는 입모양 영상을 생성할 수 있다. 또한 프로세서 는 상기 생성된 입모양 영상 및 상기 제2 언어로 된 음성을 상기 동영상 컨텐츠에 합성하여 제2 언어로 된 동영상 컨텐츠를 생성할 수 있다. 다만, 프로세서의 동작을 이로 한정하는 것은 아니고, 도 1 내지 도 6에서 설명된 동작들을 수행할 수도 있다. 메모리는 뉴럴 네트워크 모델을 저장할 수 있다. 메모리는 일 실시예에 따른 동영상 언어 변환 방 법을 수행하기 위해 요구되는 데이터를 임시적으로 또는 영구적으로 저장할 수 있다. 예를 들어, 메모리 는 원본 영상, 추출된 발화자 입모양 영역, 제1 및/또는 제2 언어로 된 음성, 제1 및/또는 제2 언어로 된 텍스 트, 재생산 영상 등을 저장할 수 있다. 디스플레이는 생성된 제2 언어로 된 동영상 컨텐츠를 표시할 수 있다. 디스플레이는 입력 동영상 컨텐츠 및/또는 추출된 발화자 입모양 영역에 대한 이미지 내지는 영상도 시각화할 수 있다. 일 실시예에 따른 동영상 언어 변환 장치는 원본 영상에서 추출된 입모양 영역에서 발화자의 입모양 영역 을 추출하고, 이를 제2 언어로 된 음성과 합성함으로써 제2 언어로 된 음성에 맞는 입모양 영상을 생성할 수 있 다. 생성된 입모양 영상은 제2 언어로 된 음성과 함께 원본 영상에 합성되어 새로운 동영상 컨텐츠가 생산된다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 소프트웨어 애플리케 이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된 경우도 있지만, 해당"}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구 성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처 리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독 으로 또는 조합하여 저장할 수 있으며 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성 된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매 체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같 은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아 니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 위에서 설명한 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 또는 복수의 소프트웨어 모듈로서 작동하 도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2022-0160559", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 이를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다. 본 발명의 기술 사상은 상기 바람직한 실시예들에 따라 구체적으로 기록되었으나, 상기한 실시예는 그 설명을 위한 것이며 그 제한을 위한 것이 아님을 주의하여야 한다. 또한, 본 발명의 기술 분야의 통상의 전문가라면 본 발명의 기술 사상 범위내에서 다양한 실시예가 가능함을 이해할 수 있을 것이다."}
{"patent_id": "10-2022-0160559", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 동영상 언어 변환 방법을 설명하는 흐름도를 도시하는 도면이다. 도 2a 내지 도 2e는 일 실시예에 따른 제1 언어의 원본 영상으로부터 제2 언어로 된 음성을 합성하는 과정의 블 록도를 도시하는 도면이다. 도 3은 일 실시예에 따른 입모양 영상을 생성하는 과정의 블록도를 도시하는 도면이다. 도 4는 일 실시예에 따른 입모양 영상을 생성하기 위해 원본 영상으로부터 입모양 영역을 추출하는 과정을 설명 하는 도면이다. 도 5는 일 실시예에 따른 입모양 영상을 생성하기 위한 원본 영상으로부터 특징점을 추출하는 과정을 설명하는 도면이다. 도 6은 일 실시예에 따른 입모양 영상을 생성하기 위한 딥러닝기반 입모양 영상 생성 알고리즘 모델을 설명하기 위한 흐름도이다. 도 7은 일 실시예에 따른 동영상 언어 변환 장치의 하드웨어 구현의 예를 도시한 도면이다. 첨부된 도면은 본 발명의 기술사상에 대한 이해를 위하여 참조로서 예시된 것임을 밝히며, 그것에 의해 본 발명 의 권리범위가 제한되지는 아니한다."}
