{"patent_id": "10-2022-0078138", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0001463", "출원번호": "10-2022-0078138", "발명의 명칭": "대상물 인식 및 증강현실 콘텐츠 구현시스템", "출원인": "주식회사 파르온", "발명자": "백호암"}}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인식 대상물을 비디오 촬영 후 네트워크를 통해 대상물 인식장치에 업-로드하고, 훈련 명령을 상기 대상물 인식장치에 전송하며, 인식 대상물을 업-로드한 후 인식 대상물에 연결된 증강현실 콘텐츠를 화면에 표출해주는 사용자 단말; 및상기 사용자 단말을 통해 업-로드되는 대상물 영상을 저장하고, 상기 사용자 단말을 통해 전달되는 훈련 요청에따라 대상물 인식 네트워크를 생성하여 대상물 인식 훈련을 통해 인식 네트워크를 최적화하고, 사용자 단말을통해 인식 대상 대상물이 업-로드되면 상기 최적화된 인식 네트워크를 이용하여 대상물을 인식한 후, 인식 대상물에 연결된 콘텐츠를 증강현실을 통해 구현하여 상기 사용자 단말에 전송하는 대상물 인식장치를 포함하는 것을 특징으로 하는 대상물 인식 및 증강현실 콘텐츠 구현시스템."}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에서, 상기 사용자 단말은 인식 대상물에 관련된 콘텐츠를 상기 대상물 인식장치에 업-로드하되, 상기인식 대상물에 관련된 콘텐츠는 3D 모델, 이미지, 동영상, 소리 중 적어도 어느 하나이고, 인식 대상물에 관련된 콘텐츠 업-로드시 위치 및 방향을 지정하는 것을 특징으로 하는 대상물 인식 및 증강현실 콘텐츠구현시스템."}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에서, 상기 대상물 인식장치는,상기 사용자 단말을 통해 전송된 인식 대상물 정보를 수신하여 데이터베이스에 등록하는 인식 대상물 수신부;및대상물 인식을 위한 네트워크를 생성하는 인식 네트워크 생성부를 포함하는 것을 특징으로 하는 대상물 인식 및증강현실 콘텐츠 구현시스템."}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에서, 상기 대상물 인식장치는,상기 인식 네트워크 생성부에서 생성한 인식 네트워크를 통해 대상물에 대해 인식 훈련을 진행하고, 인식 결과를 기초로 상기 인식 네트워크를 최적화하는 인식 네트워크 훈련부를 더 포함하는 것을 특징으로 하는 대상물인식 및 증강현실 콘텐츠 구현시스템."}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "에서, 상기 인식 네트워크 훈련부는,Inception V3 네트워크를 사용해서 이미지 특징을 추출하고, 추출된 이미지 특징을 사용하여 인식을 하기 위한2-layer 네트워크를 설계하고, Tensorflow 엔진을 이용하여 인식 네트워크 훈련을 진행하며, 훈련된 네트워크를최적의 인식 네트워크(trainModel)로 디렉토리에 저장하는 것을 특징으로 하는 대상물 인식 및 증강현실 콘텐츠구현시스템."}
{"patent_id": "10-2022-0078138", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 4에서, 상기 대상물 인식장치는,상기 사용자 단말을 통해 인식 대상물 업-로드시 상기 인식 네트워크 훈련부를 통해 구축된 인식 네트워크를 이용하여 대상물을 인식하는 대상물 인식부; 및상기 대상물 인식부를 통해 인식된 인식 대상물에 연결된 콘텐츠를 증강현실을 통해 생성하고, 생성한 증강현실콘텐츠를 상기 사용자 단말에 전송해주는 증강현실 콘텐츠 생성부를 더 포함하는 것을 특징으로 하는 대상물 인식 및 증강현실 콘텐츠 구현시스템."}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "마커 없이 카메라 입력 정보를 직접 사용하여 대상물 인식 및 콘텐츠 증강을 구현하고, 모바일에 앱을 설치할 필 요 없이 웹상에서 증강현실 구현 작업이 이루어지도록 하여 사용자 편의성 증대를 도모하도록 한 대상물 인식 및 증강현실 콘텐츠 구현시스템에 관한 것으로서, 인식 대상물을 비디오 촬영 후 네트워크를 통해 대상물 인식장치 (뒷면에 계속)"}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 대상물 인식 및 증강현실 콘텐츠 구현시스템에 관한 것으로, 특히 마커 없이 카메라 입력 정보를 직 접 사용하여 대상물 인식 및 콘텐츠 증강을 구현하고, 모바일에 앱을 설치할 필요 없이 웹상에서 증강현실 구현 작업이 이루어지도록 하여 사용자 편의성 증대를 도모하도록 한 대상물 인식 및 증강현실 콘텐츠 구현시스템에 관한 것이다."}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사용자가 눈으로 보는 현실세계에 가상의 객체를 겹쳐 보여주는 기술을 '증강현실(Augmented Reality, 增强現實)'이라고 한다. 현실세계에 실시간으로 부가 정보를 갖는 가상세계를 합쳐 하나의 영상으로 보여주므로 혼합현실(Mixed Reality, MR)이라고도 한다. 현실세계를 가상세계로 보완해주는 개념인 증강현실은 컴퓨터 그래픽으로 만들어진 가상환경을 사용하지만 주역 은 현실환경이다. 컴퓨터 그래픽은 현실환경에 필요한 정보를 추가 제공하는 역할을 한다. 이러한 증강현실기술은 가상환경만으로 구성된 가상현실기술과 달리, 실제 환경과 가상의 객체가 혼합된 영상을 제공하므로, 사용자가 실제 환경을 볼 수 있게 하여 더욱 나은 현실감과 부가 정보를 제공한다. 증강현실기술은 원격의료진단·방송·건축설계·제조공정관리 등에 활용된다. 특히, 최근 스마트폰이 널리 보급 되면서 본격적인 상업화 단계에 들어섰으며, 게임 및 모바일 솔루션 업계·교육 분야 등에서도 다양한 제품을 개발하고 있다. 예를 들어, 스마트폰 카메라로 주변을 비추면 인근에 있는 상점의 위치, 전화번호 등의 정보가 입체영상으로 표 기되는 안내 서비스 등은 이미 상업적 서비스로 운영되고 있다. 한편, 증강현실의 구현에서 중요한 기술적 과제로서, 실제 환경과 가상의 객체가 시간적 및 공간적으로 정확하 게 정합된 영상이 구현되어야 하는 점이 중요하다. 이러한 영상 정합을 위해서, 일반적으로 현실환경의 객체와 함께 인식될 수 있는 다양한 형태의 마커가 사용되었다. 이러한 마커는 인위적으로 제작 또는 생성된 마커를 현 실환경의 객체에 부착 또는 인쇄하거나, 영상 인식이 가능한 객체 자체의 특징점을 마커로서 미리 등록하는 등 의 방식으로 제공되었다. 따라서 증강현실 구현을 위해서는 사전 마커 등록이 선행되어야 한다는 한계가 있다. 한편, 대상물을 인식하고 인식 결과를 증강현실 콘텐츠로 보여주기 위해 종래에 제안된 기술이 하기의 <특허문 헌 1> 내지 <특허문헌 3> 에 개시되어 있다. <특허문헌 1> 에 개시된 종래기술은 주변 환경으로부터 데이터를 수집하는 센서부, 수집된 데이터와 사용자로부 터 입력된 데이터를 변환하여 컨텍스트를 생성하는 컨텍스트 생성부, 입력되는 영상과 생성된 컨텍스트를 이용 하여 증강현실 장면을 생성하는 장면 생성부를 구비하고, 컨텍스트 인식할 수 있는 증강현실을 제공한다. 또한, <특허문헌 2> 에 개시된 종래기술은 터치 스크린을 포함하는 모바일 기기를 이용한 증강현실 서비스 제공 방법으로서, 모바일 기기의 자세에 따른 2차원 제약 평면 공간을 생성하고, 카메라를 통해 획득된 영상에 포함 된 대상물에 상응하여 디스플레이되는 가상 객체를 터치 스크린상에서의 사용자 조작에 따라 2차원 제약 평면 공간상에서 이동 또는 회전시켜 증강현실 서비스를 제공하는 방식으로, 증강현실에서 나타나는 가상 객체를 쉽 게 움직일 수 있도록 한다. 또한, <특허문헌 3> 은 스마트폰의 앱(application)을 이용하여 이미지를 스캔하고, 촬영한 이미지를 픽셀정보 로 변환하여 인식하고, 인식된 정보를 DB의 정보 형태로 변환하며, 변환된 정보자료를 DB의 정보자료와 매칭 (matching)하여 비교하고, 매칭된 DB의 정보자료를 어떤 형태로 외부로 디스플레이(display) 할 것인지를 결정 한 후, 결정된 내용에 따라 DB의 정보를 외부로 디스플레이하는 과정을 통해, 포스터나 명함 등을 스마트폰 카 메라의 앱(application)으로 인식하는 경우에는 미리 저장된 데이터베이스의 동영상이 디스플레이되도록 한다. 그러나 상기와 같은 종래기술은 증강현실 구현을 위한 마커를 사전 등록해야 하는 한계가 있으며, 스마트폰과 같은 사용자 휴대기기를 통해서 증강현실을 구현하기 때문에 사용자 모바일 기기에 증강현실 구현 앱(App)을 설 치하는 불편함이 있고, 단순 이미지 등록을 이용한 대상물 인식 방법이므로 대상물 인식률이 떨어지는 등의 단 점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 10-2009-0044700(2009.05.07. 공개)(컨텍스트 인식 증강 현실을 제공하기 위한 시스템 및 방법) (특허문헌 0002) 대한민국 등록특허 10-1470757(2014.12.02. 등록)(증강현실 서비스 제공 방법 및 장치) (특허문헌 0003) 대한민국 등록특허 10-1525409(2015.05.28. 등록)(이미지 인식모듈을 이용한 컨텐츠 증강방법)"}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서 본 발명은 상기와 같은 일반적인 증강현실 방법 및 종래기술에서 마커를 이용하여 증강현실을 구현할 때 사전에 마커를 등록해야 하는 한계 및 단순 이미지 등록을 이용하여 대상물을 인식하므로 대상물 인식률이 떨어 지는 단점 및 모바일 기기에 증강현실 앱을 설치해야 하는 불편함 등의 제반 문제점을 해결하기 위해서 제안된 것으로서, 마커 없이 카메라 입력 정보를 직접 사용하여 대상물 인식 및 콘텐츠 증강을 구현하고, 모바일에 앱 을 설치할 필요 없이 웹상(Web)에서 증강현실 구현 작업이 이루어지도록 하여 사용자 편의성 증대를 도모하도록 한 대상물 인식 및 증강현실 콘텐츠 구현시스템을 제공하는 데 그 목적이 있다. 본 발명의 다른 목적은 단순 이미지 등록 방법에 비해 강건한 사물 인식을 위한 인공지능 기술을 통해 한 장의 이미지를 등록하는 것이 비해 다양한 조건의 입력에 대해 처리가 가능하도록 한 대상물 인식 및 증강현실 콘텐 츠 구현시스템을 제공하는 것이다. 본 발명의 또 다른 목적은 사용자가 원하는 대상에 필요한 콘텐츠를 직접 연결해서 콘텐츠 증강을 구현할 수 있 도록 한 대상물 인식 및 증강현실 콘텐츠 구현시스템을 제공하는 것이다."}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 바와 같은 목적을 달성하기 위하여, 본 발명에 따른 \"대상물 인식 및 증강현실 콘텐츠 구현시스템\"은, 인식 대상물을 비디오 촬영 후 네트워크를 통해 대상물 인식장치에 업-로드하고, 훈련 명령을 상기 대상물 인식 장치에 전송하고, 인식 대상물을 업-로드한 후 인식 대상물에 연결된 증강현실 콘텐츠를 화면에 표출해주는 사 용자 단말; 상기 사용자 단말을 통해 업-로드되는 대상물 영상을 저장하고, 상기 사용자 단말을 통해 전달되는 훈련 요청에 따라 대상물 인식 네트워크를 생성하여 대상물 인식 훈련을 통해 인식 네트워크를 최적화하고, 사용자 단말을 통해 인식 대상 대상물이 업-로드되면 상기 최적화된 인식 네트워크를 이용하여 대상물을 인식한 후, 인식 대상 물에 연결된 콘텐츠를 증강현실을 통해 구현하여 상기 사용자 단말에 전송하는 대상물 인식장치를 포함하는 것 을 특징으로 한다. 상기에서 사용자 단말은 인식 대상물에 관련된 콘텐츠를 상기 대상물 인식장치에 업-로드하되, 인식 대상물에 관련된 콘텐츠는 3D 모델, 이미지, 동영상, 소리 중 적어도 어느 하나이고, 인식 대상물에 관련된 콘텐츠 업-로 드시 위치 및 방향을 지정하는 것을 특징으로 한다. 상기에서 대상물 인식장치는 대상물 인식 서버로 구현하는 것을 특징으로 한다. 상기에서 대상물 인식장치는, 상기 사용자 단말을 통해 전송된 인식 대상물 정보를 수신하여 데이터베이스에 등록하는 인식 대상물 수신부; 대상물 인식을 위한 네트워크를 생성하는 인식 네트워크 생성부; 상기 인식 네트워크 생성부에서 생성한 인식 네트워크를 통해 대상물에 대해 인식 훈련을 진행하고, 인식 결과 를 기초로 상기 인식 네트워크를 최적화하는 인식 네트워크 훈련부를 포함하는 것을 특징으로 한다. 상기에서 인식 네트워크 훈련부는, 데이터베이스에 업-로드된 각 동영상을 파일을 읽어 프레임별로 이미지를 추출한 다음 동영상 이름의 디렉토리 에 저장하고, 훈련을 위한 이미지 증강(augmentation)을 진행하고, Inception V3를 기반으로 증강된 이미지를 전이 학습(Transfer Learning)하여 최적의 인식 네트워크를 구축하는 것을 특징으로 한다. 상기에서 인식 네트워크 훈련부는, Inception V3 네트워크를 사용해서 이미지 특징을 추출하고, 추출된 이미지 특징을 사용하여 인식을 하기 위한 2-layer 네트워크를 설계하고, Tensorflow 엔진을 이용하여 인식 네트워크 훈련을 진행하며, 훈련된 네트워크를 최적의 인식 네트워크(trainModel)로 디렉토리에 저장하는 것을 특징으로 한다. 상기에서 대상물 인식장치는, 상기 사용자 단말을 통해 인식 대상물 업-로드시 상기 인식 네트워크 훈련부를 통해 구축된 인식 네트워크를 이 용하여 대상물을 인식하는 대상물 인식부; 상기 대상물 인식부를 통해 인식된 인식 대상물에 연결된 콘텐츠를 증강현실을 통해 생성하고, 생성한 증강현실 콘텐츠를 상기 사용자 단말에 전송해주는 증강현실 콘텐츠 생성부를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 마커 없이 카메라 입력 정보를 직접 사용하여 대상물 인식 및 콘텐츠 증강을 구현함으로써 기 존과 같이 대상물 인식 및 콘텐츠 증강을 위해 반드시 필요한 마커 등록 과정을 제거하여, 사용자의 편의성 향 상을 도모해주는 효과가 있다. 또한, 본 발명에 따르면 모바일에 앱을 설치할 필요 없이 웹상(Web)에서 증강현실 구현 작업이 이루어지도록 함 으로써, 사용자가 증강현실 앱을 사용자 단말에 설치해야 하는 불편함을 제거하여 사용자 편의성 증대를 도모해 주는 효과가 있다. 또한, 본 발명에 따르면 단순 이미지 등록 방법에 비해 강건한 사물 인식을 위한 인공지능 기술을 통해 한 장의 이미지를 등록하는 것이 비해 다양한 조건의 입력에 대한 처리가 가능한 장점이 있다. 또한, 본 발명에 따르면 사용자가 원하는 대상에 필요한 콘텐츠를 직접 연결해서 콘텐츠 증강을 구현함으로써, 사용자 편의 증대를 도모함과 동시에 기술적 이해도를 높여주고 접근 필요성 최소화를 도모해주는 효과도 있다."}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 본 발명의 바람직한 실시 예에 따른 대상물 인식 및 증강현실 콘텐츠 구현시스템을 첨부된 도면을 참조하 여 상세하게 설명한다. 이하에서 설명되는 본 발명에 사용된 용어나 단어는 통상적이거나 사전적인 의미로 한정해서 해석되어서는 안 되며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념으로 적절하게 정의할 수 있 다는 원칙에 입각하여 본 발명의 기술적 사상에 부합하는 의미와 개념으로 해석되어야만 한다. 따라서 본 명세서에 기재된 실시 예와 도면에 도시된 구성은 본 발명의 바람직한 실시 예에 불과할 뿐이고, 본 발명의 기술적 사상을 모두 대변하는 것은 아니므로, 본 출원 시점에서 이들을 대체할 수 있는 다양한 균등물과 변형 예들이 있을 수 있음을 이해하여야 한다. 도 1은 본 발명의 바람직한 실시 예에 따른 대상물 인식 및 증강현실 콘텐츠 구현시스템의 개념도로서, 사용자 가 사용자 단말을 대상물 비디오를 촬영한 후 웹상으로 대상물 인식장치(대상물 인식서버)에 업-로드 하고, 훈련시작 명령을 내리면, 대상물 인식장치에서 대상물에 대해 인식 네트워크 훈련을 진행하고, 최적 의 인식 네트워크(대상물 인식 모델; trainedModel)를 생성하여 저장한다. 이어, 사용자가 사용자 단말을 이용하여 대상물에 관련된 콘텐츠를 업-로드하면, 상기 대상물 인식장치에서 상기 최적화된 인식 네트워크 를 이용하여 대상물을 인식하고, 인식 대상물에 연결된 콘텐츠를 증강현실로 구현하여 사용자 단말에 전송 한다. 사용자 단말은 구현된 증강 현실 콘텐츠를 화면에 표시해주어, 사용자가 대상물을 쉽게 이해할 수 있도록 한다. 도 2는 본 발명에 따른 \"대상물 인식 및 증강현실 콘텐츠 구현시스템\"의 구성도로서, 사용자 단말 및 대상 물 인식장치를 포함할 수 있다. 사용자 단말과 대상물 인식장치는 네트워크를 통해 웹상으로 접속이 이루어져, 인식 대상물 콘텐츠는 사용자 단말에 네트워크를 통해 대상물 인식장치에 업-로드하고, 대상물 인식은 대상물 인식장치 에서 수행하고 그 결과 즉, 인식 대상물에 연결된 증강현실 콘텐츠를 생성하여 사용자 단말로 전송한 다. 사용자 단말은 인식 대상물을 비디오 촬영 후 네트워크를 통해 대상물 인식장치에 업-로드하고, 훈련 명령을 상기 대상물 인식장치에 전송하며, 인식 대상물을 업-로드한 후 인식 대상물에 연결된 증강현실 콘 텐츠를 화면에 표출해주는 역할을 한다. 이러한 사용자 단말은 사용자가 휴대한 다양한 모바일 기기로 구 현할 수 있으며, 본 발명에서는 스마트폰을 모바일 기기의 예시로 설명하기로 한다. 상기 사용자 단말은 인식 대상물에 관련된 콘텐츠를 상기 대상물 인식장치에 업-로드한다. 여기서 인 식 대상물에 관련된 콘텐츠는 3D 모델, 이미지, 동영상, 소리 중 적어도 어느 하나일 수 있으며, 인식 대상물에 관련된 콘텐츠 업-로드시 위치 및 방향을 지정할 수 있다. 대상물 인식장치는 상기 사용자 단말을 통해 업-로드되는 대상물 영상을 저장하고, 상기 사용자 단말 을 통해 전달되는 훈련 요청에 따라 대상물 인식 네트워크를 생성하여 대상물 인식 훈련을 통해 인식 네트 워크를 최적화하고, 사용자 단말을 통해 인식 대상 대상물이 업-로드되면 상기 최적화된 인식 네트워크를 이용하여 대상물을 인식한 후, 인식 대상물에 연결된 콘텐츠를 증강현실(AR)을 통해 구현하여 상기 사용자 단말 에 전송하는 역할을 한다. 이러한 대상물 인식장치는 상기 사용자 단말과는 원격에 위치하며, 대상물을 인식하고 그 결과를 사 용자 단말에 네트워크를 통해 전송해주는 대상물 인식 서버로 구현할 수 있다. 상기 대상물 인식장치는 상기 사용자 단말을 통해 전송된 인식 대상물 정보를 수신하여 데이터베이스 (DB)에 등록하는 인식 대상물 수신부, 대상물 인식을 위한 네트워크를 생성하는 인식 네트워크 생성부, 상기 인식 네트워크 생성부에서 생성한 인식 네트워크를 통해 대상물에 대해 인식 훈련을 진행하 고, 인식 결과를 기초로 상기 인식 네트워크를 최적화하는 인식 네트워크 훈련부를 포함할 수 있다. 상기 인식 네트워크 훈련부는 데이터베이스에 업-로드된 각 동영상을 파일을 읽어 프레임별로 이미지 를 추출한 다음 동영상 이름의 디렉토리에 저장하고, 훈련을 위한 이미지 증강(augmentation)을 진행하고, Inception V3를 기반으로 증강된 이미지를 전이 학습(Transfer Learning)하여 최적의 인식 네트워크를 구축할 수 있다. 이러한 인식 네트워크 훈련부는 Inception V3 네트워크를 사용해서 이미지 특징을 추출하고, 추출된 이미 지 특징을 사용하여 인식을 하기 위한 2-layer 네트워크를 설계하고, Tensorflow 엔진을 이용하여 인식 네트워 크 훈련을 진행하며, 훈련된 네트워크를 최적의 인식 네트워크(trainModel)로 디렉토리에 저장할 수 있다. 아울러 상기 대상물 인식장치는 상기 사용자 단말을 통해 인식 대상물 업-로드시 상기 인식 네트워크 훈련부를 통해 구축된 인식 네트워크를 이용하여 대상물을 인식하는 대상물 인식부, 상기 대상물 인 식부를 통해 인식된 인식 대상물에 연결된 콘텐츠를 증강현실을 통해 생성하고, 생성한 증강현실 콘텐츠를 상기 사용자 단말에 전송해주는 증강현실 콘텐츠 생성부를 포함할 수 있다. 이와 같이 구성된 본 발명에 따른 \"대상물 인식 및 증강현실 콘텐츠 구현시스템\"의 동작을 구체적으로 설명하면 다음과 같다. 먼저, 대상물 인식을 하고, 인식된 결과를 증강현실을 통해 확인하고자 하는 사용자는 사용자 단말을 이용 하여 대상물을 카메라를 통해 촬영하고, 네트워크를 통해 웹상으로 대상물 인식장치가 웹에서 운영하는 대 상물 인식 사이트에 접속한 후, 촬영된 비디오를 파일 형태로 네트워크를 통해 인식 대상물 수신부에 업- 로드한다. 이러한 비디오 파일은 인식 네트워크 훈련용으로 사용된다. 인식 대상물 비디오 파일을 업-로드한 후 사용자는 사용자 단말을 통해 훈련시작 명령을 전달한다. 상기 대상물 인식장치는 상기 사용자 단말을 통해 업-로드되는 대상물 영상을 데이터베이스에 저장한다. 이후, 사용자 단말을 통해 훈련 요청 명령이 전달되면, 대상물 인식 네트워크를 생성하여 대상물 인식 훈 련을 통해 인식 네트워크를 최적화하고, 사용자 단말을 통해 인식 대상 대상물이 업-로드되면 상기 최적화 된 인식 네트워크를 이용하여 대상물을 인식한 후, 인식 대상물에 연결된 콘텐츠를 증강현실(AR)을 통해 구현하 여 상기 사용자 단말에 전송한다. 예컨대, 상기 대상물 인식장치의 인식 대상물 수신부는 상기 사용자 단말을 통해 업-로드되는 인식 대상물 정보(파일)를 수신하여 데이터베이스(DB)에 등록한다. 다음으로, 대상물 인식장치의 인식 네트워크 생성부는 상기 사용자 단말을 통해 대상물 인식 훈 련 명령이 전달되면, 데이터베이스와 연동하여 대상물 인식을 위한 네트워크를 생성한다. 여기서 대상물 인식 네트워크는 2-layer 기반 Inception V3 네트워크일 수 있으나, 본 발명은 이것에 한정되는 것은 아니다. 상기 인식 네트워크 훈련부는 상기 인식 네트워크 생성부에서 생성한 인식 네트워크를 통해 대상물에 대해 인식 훈련을 진행하고, 인식 결과를 기초로 상기 인식 네트워크를 최적화한다. 즉, 인식 네트워크 훈련부는 도 3a에 도시한 바와 같이, 데이터베이스에 업-로드된 각 동영상을 파일 을 읽어 프레임별로 이미지를 추출한 다음, 도 3b에 도시한 바와 같이, 동영상 이름의 디렉토리에 저장한다. 이어, 훈련을 위한 이미지 증강(augmentation)을 진행한다. 여기서 이미지 증강은 이미지 크기, 방향 등을 다양 하게 변환하면서 새로운 인식용 이미지를 생성하는 것을 의미한다. 여기서 이미지 증강은 크게 Albumentations와 이미지를 읽는 데 사용할 라이브러리(예를 들어, OpenCV)를 불러 오는 제1단계(Import), 증강 파이프라인(augmentation pipeline) 정의하는 제2단계, 디스크에 있는 이미지 읽어 들이는 제3단계, 증강 파이프라인에 이미지를 넣고 증강된 이미지를 획득하는 제4단계로 이루어질 수 있다. 제1단계는 Albumentations를 Import하고, OpenCV를 Import한다. 여기서 Albumentations는 OpenCV에 의존성 (dependency)을 가지고 있는 라이브러리이다. 따라서 Albumentations가 설치되어 있다면 OpenCV도 설치되어 있 을 것이다. 제2단계는 증강 파이프라인을 정의하기 위해 Compose 클래스의 객체를 만든다. Compose 클래스의 인지 (argument)로 적용하고자 하는 증강 기법들의 리스트를 넣어주면 된다. Compose를 호출하면 이미지 증강을 실시 하는 변형 함수(transform function)을 반환한다. 증강을 생성하기 위해 필요한 증강 클래스(augmentation class)의 객체를 만들고, 증강 파라미터 등을 해당 증강 클래스에 넣어주어야 한다. 제3단계는 증강 파이프라인에 이미지를 넣기 위해 디스크(데이터베이스)에 있는 이미지를 읽어온다. 이때, 주의할 점은 Albumentations의 증강 파이프라인에 이미지를 넣을 땐 NumPy 배열 형식(in the form of a NumPy array)으로 이미지를 넣어야 한다. 또한, 이미지가 컬러 이미지라면 이미지는 Red, Green, Blue(RGB) 순 서로 된 세 개의 채널로 입력해주어야 한다. 이미지를 읽기 위해 OpenCV를 사용한다. OpenCV는 이미지를 BGR 형 식(format)으로 읽어온다. Albumentations는 가장 흔하고 인기 있는 형식인 RGB 형식의 이미지를 사용하기 때문 에, OpenCV를 사용할 경우, 이미지 형식을 명시적으로 RGB로 변환해주어야 한다. 다른 방법으로는, 파이썬의 이 미지 전처리 라이브러인 Pillow를 활용할 수도 있다. 제4단계는 이미지를 증강 파이프라인에 넣어주기 위해, 상기 Compose를 호출하여 만든 변환 함수를 불러온 후 이미지 인자에 증강하고자 하는 이미지를 넣어준다. 이렇게 하면 변환은 하나의 키(image)를 갖는 dictionary를 반환한다. 값은 해당 키(이미지)를 증강시킨 이미지이다. 이런 방식으로 다른 이미지를 증강하기 위해서는 변환 함수를 다시 호출한 후 새로운 이미지를 이미지 인자에 넣어주면 된다. 각 증강 기법들은 설정된 파라미터 P 값 의 확률에 따라 입력된 이미지를 변형시킨다. 여기서 증강 기법들은 변형 정도를 조절할 수 있는 파라미터를 가 지고 있어, 이러한 파라미터를 이용하여 변형 정도를 조절할 수 있다. 이러한 과정을 통해 이미지 증강을 진해야 인식을 위한 새로운 이미지를 생성한다. 다음으로, Inception V3를 기반으로 증강된 이미지를 전이 학습(Transfer Learning)하여 최적의 인식 네트워크 를 구축한다. 즉, Inception V3 네트워크를 사용해서 이미지 특징을 추출하고(2048 크기의 벡터로 추출), 추출된 이미지 특징 을 사용하여 인식을 하기 위한 2-layer 기반 Inception V3 네트워크에 연결한다. 이어, Tensorflow 엔진을 이용 하여 인식 네트워크 훈련을 진행하고, 도 4에 도시한 바와 같이 훈련된 네트워크를 최적의 인식 네트워크 (trainModel)로 디렉토리에 저장한다. 이러한 과정을 통해 인식 네트워크의 훈련이 종료되어 최적의 인식 네트워크를 구축한 상태에서, 사용자는 사용 자 단말을 이용하여 실제 인식할 인식 대상물에 관련된 콘텐츠를 업-로드한다. 여기서 증강현실 구현을 위한 콘텐츠는 3D 모델, 이미지, 동영상, 소리 중 적어도 어느 하나일 수 있으며, 인식 대상물에 관련된 콘텐츠 업-로드시 사용자는 사용자 단말을 이용하여 증강현실 구축을 위한 위치 및 방향을 지 정할 수 있다. 즉, 사물의 보는 방향, 위치를 설정할 수 있다. 예컨대, 왼쪽, 위, 오른쪽과 같이 바라보는 경우 를 구분하여 설정할 수 있다. 이렇게 증강현실 구축을 위한 카메라 입력 정보(위치 및 방향)를 직접 사용함으로 써, 기존과 같이 증강현실 구현을 위해 특정 마커를 사용자가 직접 등록하는 불편함을 해소할 수 있다. 증강현실 구현을 위한 콘텐츠 입력 시, 대상물 인식부는 상기 인식 네트워크 훈련부를 통해 구축된 인식 네트워크를 이용하여 대상물을 인식한다. 이어, 증강현실 콘텐츠 생성부는 상기 대상물 인식부를 통해 인식된 인식 대상물에 연결된 콘텐츠를 카메라 입력 정보(위치 및 방향)를 기초로 증강현실을 구현한다. 다음으로, 생성한 증강현실 콘텐츠를 상기 사 용자 단말에 전송해준다. 사용자 단말은 수신한 증강현실 콘텐츠를 화면에 디스플레이해줌으로써, 사용자는 단순히 사용자 단말을 이용하여 인식 대상물을 입력하는 것만으로 실시간으로 증강현실 콘텐츠를 확인할 수 있다. 이상 상술한 본 발명에 따르면 마커 없이 카메라 입력 정보를 직접 사용하여 대상물 인식 및 콘텐츠 증강을 구 현함으로써 기존과 같이 대상물 인식 및 콘텐츠 증강을 위해 반드시 필요한 마커 등록 과정을 제거할 수 있어 사용자의 편의성 향상을 도모한다. 또한, 본 발명에 따르면 모바일에 앱을 설치할 필요 없이 웹상(Web)에서 증강현실 구현 작업이 이루어지므로, 사용자가 증강현실 앱을 사용자 단말에 설치해야 하는 불편함도 제거하여 사용자 편의성 증대를 도모한다. 또한, 본 발명에 따르면 단순 이미지 등록 방법에 비해 강건한 사물 인식을 위한 인공지능 기술을 통해 한 장의 이미지를 등록하는 것이 비해 다양한 조건의 입력에 대해 처리가 가능하고, 사용자가 원하는 대상에 필요한 콘 텐츠를 직접 연결해서 콘텐츠 증강을 구현함으로써, 사용자 편의 증대를 도모함과 동시에 기술적 이해도를 높여주고 접근 필요성 최소화를 도모한다. 이상 본 발명자에 의해서 이루어진 발명을 상기 실시 예에 따라 구체적으로 설명하였지만, 본 발명은 상기 실시"}
{"patent_id": "10-2022-0078138", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되는 것은 아니고 그 요지를 이탈하지 않는 범위에서 여러 가지로 변경 가능한 것은 이 기술분야에서 통상의 지식을 가진 자에게 자명하다."}
{"patent_id": "10-2022-0078138", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 대상물 인식 및 증강현실 콘텐츠 구현시스템의 개념도, 도 2는 본 발명에 따른 대상물 인식 및 증강현실 콘텐츠 구현시스템의 구성도, 도 3a는 본 발명에서 인식 네트워크 훈련을 위한 동영상 파일의 예시이며, 도 3b는 동영상 파일로부터 추출한 이미지 파일들의 예시이고, 도 4는 본 발명에서 최적화된 훈련 네트워크의 저장 예시이다."}
