{"patent_id": "10-2019-0134227", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0050081", "출원번호": "10-2019-0134227", "발명의 명칭": "객체 인식 방법 및 이를 수행하는 객체 인식 장치", "출원인": "주식회사 에스오에스랩", "발명자": "이용이"}}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "관찰 영역(FoV: Field of View)에 대한 객체 인식을 수행하는 객체 인식 장치에 있어서,상기 관찰 영역에 레이저를 조사하여 반사광을 수신하는 센서로부터 비행시간 데이터 및 광량 데이터를 포함하는 상기 관찰 영역에 대한 데이터를 획득하는 라이다 데이터 획득 모듈; 및적어도 하나 이상의 인공 신경망을 이용하여 상기 관찰 영역 내 관심 객체에 대한 객체 인식을 수행하는 제어모듈;을 포함하되,상기 제어 모듈은,상기 광량 데이터에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하고, 제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 관심 영역 추출 모듈을 포함하고,상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대한 거리 데이터를 획득하고, 상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데이터를 획득하며, 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에대한 객체 인식을 수행하도록 학습된 제2 인공 신경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 객체 인식 모듈을 포함하는,객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 제어 모듈은 상기 거리 데이터에 기초하여 상기 인텐시티 데이터를 가공하고, 상기 제1 인공 신경망에 상기 가공된 인텐시티 데이터를 입력하여 상기 관심 영역 데이터를 획득하는,객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 가공된 인텐시티 데이터는 상기 관찰 영역이 포함하는 객체의 반사계수(albedo)에 기초하여 가공되는객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 제1 인공 신경망 및 제2 인공 신경망은 각각 DNN(Deep Neural Network), CNN(Convolution NeuralNetwork), R-CNN(Regions with CNN), fast R-CNN, faster R-CNN, YOLO(You Only Look Once) 및 SSD(SingleShot Multi-box Detector) 중 적어도 하나를 포함하는객체 인식 장치.공개특허 10-2021-0050081-3-청구항 5 제1 항에 있어서,상기 제어 모듈은 상기 인텐시티 데이터에 대해 히스토그램 균일화 및 업샘플링을 수행하는,객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 관심 객체는 이동 수단, 보행자 및 건물을 포함하며,상기 객체 인식 데이터는 상기 이동 수단, 보행자 및 건물 중 적어도 하나를 상기 관심 객체에 대응시키는객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 관찰 영역은 차량 내 적어도 일부를 포함하고,상기 관심 객체는 상기 차량 내 사용자 신체의 적어도 일부를 포함하며,상기 제어 모듈은 상기 사용자 신체의 적어도 일부를 인식하여 상기 차량 내 미리 설정된 기능을 수행하는,객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항에 있어서,상기 관찰 영역에 대응하는 이미지를 표시하는 디스플레이를 포함하고,상기 제어 모듈은 상기 관심 영역 데이터에 기초하여 상기 이미지에 상기 관심 영역이 표시된 이미지를 상기 디스플레이를 통해 출력하는,객체 인식 장치."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센서로부터 비행시간(ToF: Time ofFlight) 데이터 -상기 비행시간 데이터는 상기 레이저가 조사된 시점 및 반사광이 수신된 시점에 기초함- 를 획득하여 상기 관찰 영역 내에서 관심 객체를 인식하는 객체 인식 방법에 있어서,상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대응하는 거리 데이터를 획득하는 단계;상기 반사광의 광량에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하는 단계;제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는데이터를 출력하도록 학습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심영역을 지시하는 관심 영역 데이터를 획득하는 단계; 및상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데이터를 획득하고 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 단계;를 포함하는공개특허 10-2021-0050081-4-객체 인식 방법."}
{"patent_id": "10-2019-0134227", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센서로부터 비행시간(ToF: Time ofFlight) 데이터 및 상기 반사광의 광량 데이터를 획득하여 상기 관찰 영역 내에서 관심 객체를 인식하는 객체인식 방법에 있어서,상기 관찰 영역에 분포하는 점들에 대응하며 상기 비행시간 데이터에 기초한 거리 데이터 및 상기 광량 데이터에 기초한 인텐시티 데이터를 포함하는 점 구름(point cloud) 데이터를 획득하는 단계;상기 점 구름 데이터의 상기 인텐시티 데이터에 기초하여 상기 관찰 영역에 대한 인텐시티 이미지를 획득하는단계; 제1 이미지를 입력 받아 상기 제1 이미지 중 관심 영역을 지시하는 데이터를 출력하도록 학습된 영역 제안 네트워크 형태의 제1 인공 신경망에 상기 인텐시티 이미지를 입력하여 상기 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 단계; 및상기 관심 영역 데이터에 기초하여 상기 관심 영역 내 위치하는 점들의 거리 데이터에 기초하여 상기 관심 영역에 대응하는 이미지를 획득하고, 제2 이미지를 입력 받아 상기 제2 이미지에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망에 상기 관심 영역에 대응하는 이미지를 입력하여 상기 관찰 영역 내 상기 관심 객체를 인식하는 단계;를 포함하는 객체 인식 방법."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 관찰 영역(FoV: Field of View)에 대한 객체 인식을 수행하는 객체 인식 장치로, 관찰 영역에 레이저 를 조사하여 반사광을 수신하는 센서로부터 관찰 영역에 대한 데이터를 획득하는 라이다 데이터 획득 모듈 및 인 공 신경망을 이용하여 관찰 영역 내 관심 객체에 대한 객체 인식을 수행하는 제어 모듈을 포함하되, 제어 모듈은 관찰 영역에 대해 획득한 인텐시티(intensity) 데이터를 기초로 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 관심 영역 추출 모듈을 포함하고, 관심 영역 데이터에 기초하여 관심 객체에 대한 관심 데이 터를 획득하며 인공 신경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영 역에 대한 상기 관심 객체를 인식하는 객체 인식 모듈을 포함하는 객체 인식 장치에 관한 발명이다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 객체 인식 방법 및 이를 수행하는 객체 인식 장치에 관한 것으로, 보다 상세하게는 관찰 영역에 빛을 조사하여 수신되는 광을 이용하여 관심 영역을 추출하고 관찰 영역 내 관심 객체를 인식하는 객체 인식 방법 및 이를 수행하는 객체 인식 장치에 관한 것이다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "객체 인식 기술 분야에서는 객체를 탐지하고 추적하는 데에 있어 지속적으로 정밀도가 보다 높아지는 것을 요구 되는 한편, 객체를 인식하는 시간, 즉 데이터 연산량은 상대적으로 줄어드는 것이 요구되고 있다. 이러한 추세 속에서 객체를 인식하는 장치의 성능 향상과 함께 인공 신경망을 이용하여 장비에 의해 측정된 데이터를 처리하 는 기술이 차세대 객체 인식 기술로 각광을 받고 있다. 객체 인식 기술 분야에서는 이미지 전체에 대해 객체 인식을 수행하는 대신, 이미지로부터 객체에 있는 것으로 추정되는 관심 영역을 설정한 뒤 관심 영역에 대해서 객체 인식을 수행하는 방식이 연산량 감소를 위한 방식 중 하나로 이용되고 있으며, 특히 라이다(LiDAR: Light Detection And Ranging or Laser Imaging, Detection And Ranging) 분야에서는 가시광 카메라에서 얻어진 이미지로 관심 영역을 추출한 뒤 해당 관심 영역에 대한 라이다 이미지로부터 객체를 인식하는 멀티 모달 형태의 기법이 주로 활용되고 있다. 그러나, 종래에 복수의 장치를 이용하는 멀티 모달 객체 인식의 경우 장치들 사이의 동기화(synchronization) 또는 캘리브레이션(calibration)이 필요하거나 특정 장치, 예를 들어 RGB 카메라의 경우 야간이나 악천후와 같 이 객체의 조도가 낮은 환경에서는 성능이 저하되는 등 객체 인식 효율이나 성능이 어느 한 장치의 성능에 의존 적이 되는 한계점을 가지고 있다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 일 과제는, 빛을 이용하는 라이다 장치를 이용하여 관찰 영역에 대해 데이터를 추출 하여 관심 객체를 인식하는 객체 인식 방법 및 이를 수행하는 객체 인식 장치를 제공하는 것이다.본 발명이 해결하고자 하는 일 과제는, 단일 센서를 이용하여 관찰 영역에서 관심 영역 추출 및 관심 객체를 인 식하는 객체 인식 방법 및 이를 수행하는 객체 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 라이다 장치를 이용하여 관찰 영역에 대한 관심 영역을 설정하고 객체 인식을 수행하는 객체 인식 방법 및 이를 수행하는 객체 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 라이다 장치에서 측정된 데이터 중 관심 영역에 해당하는 데이터만 이용 하여 객체 인식을 수행하는 객체 인식 방법 및 이를 수행하는 객체 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 과제가 상술한 과제로 제한되는 것은 아니며, 언급되지 아니한 과제들은 본 명세서"}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 명세서의 일 양상에 따르면, 관찰 영역(FoV: Field of View)에 대한 객체 인식을 수행하는 객체 인식 장치에 있어서, 상기 관찰 영역에 레이저를 조사하여 반사광을 수신하는 센서로부터 비행시간 데이터 및 광량 데이터를 포함하는 상기 관찰 영역에 대한 데이터를 획득하는 라이다 데이터 획득 모듈 및 적어도 하나 이상의 인공 신경 망을 이용하여 상기 관찰 영역 내 관심 객체에 대한 객체 인식을 수행하는 제어 모듈을 포함하되, 상기 제어 모 듈은 상기 광량 데이터에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하고, 제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심 영역을 지 시하는 관심 영역 데이터를 획득하는 관심 영역 추출 모듈을 포함하고, 상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대한 거리 데이터를 획득하고, 상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데 이터를 획득하며, 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 객체 인식 모듈을 포함하는 객체 인식 장치가 제공될 수 있다. 본 명세서의 다른 양상에 따르면, 관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센서 로부터 비행시간(ToF: Time of Flight) 데이터 -상기 비행시간 데이터는 상기 레이저가 조사된 시점 및 반사광 이 수신된 시점에 기초함- 를 획득하여 상기 관찰 영역 내에서 관심 객체를 인식하는 객체 인식 방법에 있어서, 상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대응하는 거리 데이터를 획득하는 단계, 상기 반사광의 광 량에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하는 단계, 제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학 습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 단계 및 상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데이터를 획 득하고 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에 대한 객체 인식을 수행하도록 학습된 제2 인공 신 경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 단계를 포함하는 객체 인식 방법이 제공될 수 있다. 본 발명의 또 다른 양상에 따르면, 관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센 서로부터 비행시간(ToF: Time of Flight) 데이터 및 상기 반사광의 광량 데이터를 획득하여 상기 관찰 영역 내 에서 관심 객체를 인식하는 객체 인식 방법에 있어서, 상기 관찰 영역에 분포하는 점들에 대응하며 상기 비행시 간 데이터에 기초한 거리 데이터 및 상기 광량 데이터에 기초한 인텐시티 데이터를 포함하는 점 구름(point cloud) 데이터를 획득하는 단계, 상기 점 구름 데이터의 상기 인텐시티 데이터에 기초하여 상기 관찰 영역에 대 한 인텐시티 이미지를 획득하는 단계, 제1 이미지를 입력 받아 상기 제1 이미지 중 관심 영역을 지시하는 데이 터를 출력하도록 학습된 영역 제안 네트워크 형태의 제1 인공 신경망에 상기 인텐시티 이미지를 입력하여 상기 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 단계 및 상기 관심 영역 데이터에 기초하여 상기 관심 영역 내 위치하는 점들의 거리 데이터에 기초하여 상기 관심 영역에 대응하는 이미지를 획득하고, 제 2 이미지를 입력 받아 상기 제2 이미지에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망에 상기 관심 영 역에 대응하는 이미지를 입력하여 상기 관찰 영역 내 상기 관심 객체를 인식하는 단계를 포함하는 객체 인식 방 법이 제공될 수 있다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 발명의 과제의 해결 수단이 상술한 해결 수단들로 제한되는 것은 아니며, 언급되지 아니한 해결 수단들은 본"}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "명세서 및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수있을 것이다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 야간이나 악천후와 같이 관찰 영역의 조도가 낮은 환경에서도 관심 객체를 추출하고 객체 인 식을 수행할 수 있다. 본 발명에 의하면, 단일 센서 또는 하나의 장치를 이용하는 객체 인식 시스템이 제공되므로, 복수의 센서 또는 복수의 장치를 이용하는 경우 보다 구성이 단순해져 비용이 절감될 수 있다. 단일 센서 또는 하나의 장치를 이용하는 객체 인식 시스템이 제공되므로, 복수의 센서 또는 복수의 장치를 이용 하는 경우와 달리 특정 장치에 의한 성능 저하를 방지할 수 있다. 본 발명에 의하면, 하나의 장치를 이용하여 객체 인식을 수행하므로, 복수의 장치를 사용하는 경우와 달리 장치 들 간의 캘리브레이션 또는 동기화 과정이 생략되어 객체 인식 속도가 향상될 수 있다. 본 발명에 의하면, 관찰 영역에 해당하는 데이터 중 관심 영역에 해당하는 데이터를 선별하여 선택된 관심 데이 터에 대해서만 객체 인식을 수행하므로 전체 데이터에 대해 객체 인식을 수행하는 것보다 객체 인식에 소요되는 시간이 단축될 수 있다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과가 상술한 효과들로 제한되는 것은 아니며, 언급되지 아니한 효과들은 본 명세서 및 첨부된 도면"}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 상술한 목적, 특징들 및 장점은 첨부된 도면과 관련된 다음의 상세한 설명을 통해 보다 분명해질 것 이다. 다만, 본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예들을 가질 수 있는 바, 이하에서는 특정 실시예들을 도면에 예시하고 이를 상세히 설명하고자 한다. 본 명세서에 기재된 실시예는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명의 사상을 명 확히 설명하기 위한 것이므로, 본 발명이 본 명세서에 기재된 실시예에 의해 한정되는 것은 아니며, 본 발명의 범위는 본 발명의 사상을 벗어나지 아니하는 수정예 또는 변형예를 포함하는 것으로 해석되어야 한다. 본 명세서에 첨부된 도면은 본 발명을 용이하게 설명하기 위한 것으로 도면에 도시된 형상은 본 발명의 이해를 돕기 위하여 필요에 따라 과장되어 표시된 것일 수 있으므로 본 발명이 도면에 의해 한정되는 것은 아니다. 본 발명과 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 이하의 설명에서 사용되는 구성요소에 대한 접미사 \"유닛\", \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 본 명세서의 일 양상에 따르면, 관찰 영역(FoV: Field of View)에 대한 객체 인식을 수행하는 객체 인식 장치에 있어서, 상기 관찰 영역에 레이저를 조사하여 반사광을 수신하는 센서로부터 비행시간 데이터 및 광량 데이터를 포함하는 상기 관찰 영역에 대한 데이터를 획득하는 라이다 데이터 획득 모듈 및 적어도 하나 이상의 인공 신경 망을 이용하여 상기 관찰 영역 내 관심 객체에 대한 객체 인식을 수행하는 제어 모듈을 포함하되, 상기 제어 모 듈은 상기 광량 데이터에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하고, 제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심 영역을 지 시하는 관심 영역 데이터를 획득하는 관심 영역 추출 모듈을 포함하고, 상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대한 거리 데이터를 획득하고, 상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데 이터를 획득하며, 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 객체 인식 모듈을 포함하는 객체 인식 장치가 제공될 수 있다. 여기서, 상기 제어 모듈은 상기 거리 데이터에 기초하여 상기 인텐시티 데이터를 가공하고, 상기 제1 인공 신경 망에 상기 가공된 인텐시티 데이터를 입력하여 상기 관심 영역 데이터를 획득할 수 있다. 또 여기서, 상기 가공된 인텐시티 데이터는 상기 관찰 영역이 포함하는 객체의 반사계수(albedo)에 기초하여 가 공될 수 있다. 또 여기서, 상기 제1 인공 신경망 및 제2 인공 신경망은 각각 DNN(Deep Neural Network), CNN(Convolution Neural Network), R-CNN(Regions with CNN), fast R-CNN, faster R-CNN, YOLO(You Only Look Once) 및 SSD(Single Shot Multi-box Detector) 중 적어도 하나를 포함할 수 있다. 또 여기서, 상기 제어 모듈은 상기 인텐시티 데이터에 대해 히스토그램 균일화 및 업샘플링을 수행할 수 있다. 또 여기서, 상기 관심 객체는 이동 수단, 보행자 및 건물을 포함하며, 상기 객체 인식 데이터는 상기 이동 수단, 보행자 및 건물 중 적어도 하나를 상기 관심 객체에 대응시킬 수 있다. 또 여기서, 상기 관찰 영역은 차량 내 적어도 일부를 포함하고, 상기 관심 객체는 상기 차량 내 사용자 신체의 적어도 일부를 포함하며, 상기 제어 모듈은 상기 사용자 신체의 적어도 일부를 인식하여 상기 차량 내 미리 설 정된 기능을 수행할 수 있다. 또 여기서, 상기 관찰 영역에 대응하는 이미지를 표시하는 디스플레이를 포함하고, 상기 제어 모듈은 상기 관심 영역 데이터에 기초하여 상기 이미지에 상기 관심 영역이 표시된 이미지를 상기 디스플레이를 통해 출력할 수 있다. 본 명세서의 다른 양상에 따르면, 관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센서 로부터 비행시간(ToF: Time of Flight) 데이터 -상기 비행시간 데이터는 상기 레이저가 조사된 시점 및 반사광 이 수신된 시점에 기초함- 를 획득하여 상기 관찰 영역 내에서 관심 객체를 인식하는 객체 인식 방법에 있어서, 상기 비행시간 데이터에 기초하여 상기 관찰 영역에 대응하는 거리 데이터를 획득하는 단계, 상기 반사광의 광 량에 기초하여 상기 관찰 영역에 대한 인텐시티(intensity) 데이터를 획득하는 단계, 제1 입력 데이터를 입력 받아 상기 제1 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학 습된 제1 인공 신경망을 이용하여 상기 인텐시티 데이터로부터 상기 관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 단계 및 상기 관심 영역 데이터에 기초하여 상기 거리 데이터로부터 관심 데이터를 획 득하고 제2 입력 데이터를 입력 받아 상기 제2 입력 데이터에 대한 객체 인식을 수행하도록 학습된 제2 인공 신 경망을 이용하여 상기 관심 데이터로부터 객체 인식 데이터를 획득함으로써 상기 관찰 영역에 대한 상기 관심 객체를 인식하는 단계를 포함하는 객체 인식 방법이 제공될 수 있다. 본 발명의 또 다른 양상에 따르면, 관찰 영역(FoV: Field of View)에 레이저를 조사하여 반사광을 수신하는 센 서로부터 비행시간(ToF: Time of Flight) 데이터 및 상기 반사광의 광량 데이터를 획득하여 상기 관찰 영역 내 에서 관심 객체를 인식하는 객체 인식 방법에 있어서, 상기 관찰 영역에 분포하는 점들에 대응하며 상기 비행시 간 데이터에 기초한 거리 데이터 및 상기 광량 데이터에 기초한 인텐시티 데이터를 포함하는 점 구름(point cloud) 데이터를 획득하는 단계, 상기 점 구름 데이터의 상기 인텐시티 데이터에 기초하여 상기 관찰 영역에 대 한 인텐시티 이미지를 획득하는 단계, 제1 이미지를 입력 받아 상기 제1 이미지 중 관심 영역을 지시하는 데이 터를 출력하도록 학습된 영역 제안 네트워크 형태의 제1 인공 신경망에 상기 인텐시티 이미지를 입력하여 상기관찰 영역 내 관심 영역을 지시하는 관심 영역 데이터를 획득하는 단계 및 상기 관심 영역 데이터에 기초하여 상기 관심 영역 내 위치하는 점들의 거리 데이터에 기초하여 상기 관심 영역에 대응하는 이미지를 획득하고, 제 2 이미지를 입력 받아 상기 제2 이미지에 대한 객체 인식을 수행하도록 학습된 제2 인공 신경망에 상기 관심 영 역에 대응하는 이미지를 입력하여 상기 관찰 영역 내 상기 관심 객체를 인식하는 단계를 포함하는 객체 인식 방 법이 제공될 수 있다. 본 명세서는 객체 인식 방법 및 이를 수행하는 객체 인식 장치에 관한 것이다. 구체적으로, 본 명세서의 일 실 시예에 따르면 라이다를 이용하여 관찰 영역 내 제안되는 관심 영역을 통해 관심 객체를 인식하는 객체 인식 방 법 및 이를 수행하는 장치가 제공될 수 있다. 여기서, 라이다(LiDAR: Light Detection And Ranging 또는 Laser Imaging, Detection And Ranging)는 레이저를 관찰 영역(FoV: Field of View)에 조사하여 반사되는 광이 수신되기까지 걸리는 시간 및 강도 등을 측정하여 거 리, 방향, 속도, 온도, 물질 분포 및 농도 특성을 감지하는 기술이다. 여기서, 관찰 영역(FoV: Field of View)은 라이다 기술을 이용하여 데이터를 획득하고자 하는 대상 영역이며, 라이다 장치에서 레이저를 조사하는 가상 평면 또는 가상 공간을 의미할 수 있다. 여기서, 관심 영역(RoI: Region of Interest)은 이미지 분석 또는 영상 분석에 있어서 분석 대상을 축소시키기 위해 관찰 영역 내에서 선택되는 일부 영역을 의미할 수 있다. 여기서, 관심 객체(Object of Interest)는 관찰 영역 내에 존재하는 객체 인식의 대상을 의미할 수 있다. 다만, 관심 객체는 포괄적으로 관찰 영역 내에 존재하는 객체 인식 대상뿐만 아니라 라이다 기술을 이용하여 인식하고 자 하는 대상 자체를 포함할 수도 있다. 여기서, 객체 인식(Object Recognition)은 대상 객체에 대한 이미지 또는 영상 데이터를 처리하는 것으로 객체 탐지, 객체 인지, 객체 감지, 객체 추적(Object Tracking), 객체 식별(Object classification), 객체 세그멘테 이션(Object Segmentation) 또는 객체 분류(Object Identification) 등을 포괄하는 개념으로 본 명세서에서는 객체 인식의 목적에 따라 앞서 나열된 용어들 중 적어도 하나로 대체되거나 앞서 나열된 용어들 중 적어도 하나 와 혼용하여 사용될 수 있다. 한편, 이하에서는 본 명세서의 일 실시예에 따른 객체 인식 시스템에 대해 서술하는 데에 있어 라이다 장치를 이용하는 방법을 주로 설명하지만 본 명세서의 발명의 사상이 이에 한정되는 것은 아니며, 라이다 장치를 대신 하여 레이더(RaDAR: Radio Detection And Ranging) 장치 또는 RGB 카메라 등 객체 인식에 이용될 수 있는 장치 들이 이용될 수 있음은 물론이다. 이하에서는 본 명세서의 일 실시예에 따른 객체 인식 시스템에 관하여 도 1을 참조하여 설명한다. 도 1은 본 명세서의 일 실시예에 따른 객체 인식 시스템을 도시한 도면이다. 도 1을 참조하면, 객체 인식 시스템은 라이다 유닛 및 분석 유닛을 포함할 수 있다. 객체 인 식 시스템에서 라이다 유닛은 관찰 영역에 레이저를 조사하여 수신된 반사광에 기초하여 데이 터를 획득하고 분석 유닛이 라이다 유닛에서 획득된 데이터에 기초하여 관찰 영역에 대한 객 체 인식을 수행할 수 있다. 라이다 유닛은 관찰 영역에 레이저를 조사하고 반사되는 광을 수신할 수 있다. 이 때, 라이다 유닛 은 조사하는 광, 반사되는 광 및 수신된 광의 반사강도에 기초하여 관찰 영역에 대한 데이터를 생성 할 수 있다. 구체적으로, 본 명세서의 일 실시예에 따르면, 라이다 유닛은 광조사 모듈(미도시)을 통해 광을 관찰 영 역에 조사하고 광수신 모듈(미도시)을 통해 반사되는 광을 수신하며, 라이다 제어 모듈(미도시) 및 라이다 통신 모듈(미도시)을 통해 수신된 반사광에 기초한 데이터를 분석 유닛에 제공할 수 있다. 여기서, 관찰 영역에 대한 데이터는 비행시간(ToF: Time of Flight) 데이터, 거리 데이터, 깊이(depth) 데이터 및 반사강도(intensity) 데이터 등을 포함할 수 있다. 관찰 영역에 대한 데이터에 대해서는 추후 구체적으로 서술하도록 한다.라이다 유닛은 분석 유닛과 유/무선으로 통신할 수 있다. 라이다 유닛은 분석 유닛으 로 관찰 영역에 대한 데이터를 제공할 수 있다. 분석 유닛은 관찰 영역에 대한 객체 인식을 수행할 수 있다. 구체적으로 분석 유닛은 라이다 유닛으로부터 획득한 관찰 영역에 대한 데이터에 기초하여 관찰 영역에 대한 객체 인식을 수행 할 수 있다. 분석 유닛은 통신 모듈(미도시), 메모리(미도시), 입력 모듈(미도시), 출력 모듈(미도시) 및 제어 모듈 (미도시)을 포함할 수 있다. 통신 모듈은 외부 기기와 통신을 수행할 수 있다. 분석 유닛은 통신 모듈을 통해 라이다 유닛이나 외부 서버와 데이터 송수신을 할 수 있다. 여기서, 통신 모듈은 유/무선 방식을 포함할 수 있다. 메모리는 각종 정보를 저장할 수 있다. 메모리에는 분석 유닛을 구동하기 위한 운용 프로그램이나 분석 유닛의 각 구성을 동작시키기 위한 프로그램을 비롯해 분석 유닛의 동작에 필요한 각종 데이터가 임시적으로 또는 반영구적으로 저장될 수 있다. 예를 들어, 메모리에는 관찰 영역에 대한 데이터를 가공 및 처리하기 위한 프로그램 및 데이터 분석을 위한 인공 신경망이 저장될 수 있다. 메모리는 분석 유닛에 내장되는 형태나 탈부착 가능한 형태로 제공될 수 있다. 입력 모듈은 사용자로부터 사용자 입력을 수신할 수 있다. 사용자 입력은 키 입력, 터치 입력, 음성 입력을 비 롯한 다양한 형태로 이루어 질 수 있다. 입력 모듈은 전통적인 형태의 키패드나 키보드, 마우스는 물론, 사용자 의 터치를 감지하는 터치 센서 및 그 외의 다양한 형태의 사용자 입력을 감지하거나 입력 받는 다양한 형태의 입력 수단을 모두 포함하는 포괄적인 개념이다. 출력 모듈은 각종 정보를 출력해 사용자에게 이를 제공할 수 있다. 출력 모듈은 영상을 출력하는 디스플레이, 소리를 출력하는 스피커, 진동을 발생시키는 햅틱 장치 및 그 외의 다양한 형태의 출력 수단을 모두 포함하는 포괄적인 개념이다. 제어 모듈은 분석 유닛의 전반적인 동작을 제어할 수 있다. 예를 들어, 제어 모듈은 메모리로부터 데이터 가공 및 분석을 위한 프로그램을 로딩하여 라이다 유닛으로부터 획득한 데이터를 가공 및 분석하고 그 결 과를 출력 모듈을 통해 사용자에게 제공하도록 제어 신호를 생성할 수 있다. 분석 유닛은 별도의 전원부를 가지거나 유선 혹은 무선으로 외부로부터 전원을 공급받을 수 있으며 전원 부를 제어하는 스위치를 별도로 가질 수 있다. 분석 유닛은 객체 인식을 수행하기 위해 빅데이터(big data), 기계 학습 모델(machine learning model), 인공 지능(artificial intelligence) 또는 인공 신경망(ANN: Artificial Neural Network) 등의 기술을 이용할 수 있다. 예를 들어, 분석 유닛은 기계 학습된 프로그램을 구동하여 관찰 영역에 대한 객체 인식을 수행할 수 있다. 분석 유닛에서 객체 인식이 수행되는 예들에 대한 보다 구체적인 설명은 후술하도록 한 다. 이상에서 설명한 객체 인식 시스템은 물리적으로 단일한 장치로 제공되거나 복수의 장치로 제공될 수 있다. 예를 들어, 라이다 유닛과 분석 유닛은 물리적으로 통합된 단일한 객체 인식 장치로 제공될 수 있 다. 또 다른 예로, 라이다 유닛과 분석 유닛은 분리된 복수의 장치로 제공될 수 있다. 이 때, 관찰 영역에 대한 데이터 생성, 가공 및 처리와 관찰 영역에 대한 객체 인식은 라이다 유닛 및 분석 유닛 중 적어도 어느 하나에서 수행될 수 있다. 예를 들어, 라이다 유닛에서 관찰 영역에 대한 데이터를 가공하여 관찰 영역에 대한 이미지 또는 영상을 분석 유닛에 제공하고, 분석 유닛은 획득한 이미지 또는 영상 데이터를 이용하여 관찰 영역에 대한 객체 인식을 수행할 수 있다. 또 다른 예로, 라이다 유닛에서 관찰 영역에 대한 데이터를 가공 및 이용하여 객체 인식을 수 행할 수 있으며 분석 유닛을 통해 결과를 디스플레이할 수 있다. 한편, 객체 인식 장치는 분석 유닛으로 구성될 수 있다. 이 때, 객체 인식 장치는 라이다 데이터 획득 모 듈, 관심 영역 추출 모듈 및 객체 인식 모듈을 포함하는 제어 모듈을 포함할 수 있다.여기서, 라이다 데이터 획득 모듈은, 라이다 유닛으로부터 관찰 영역에 대한 데이터를 획득하는 모 듈이다. 라이다 데이터 획득 모듈은 예를 들어, 상술한 통신 모듈로 구현될 수 있다. 통신 모듈로 구현되는 라 이다 획득 모듈은, 라이다 유닛로부터 라이다 데이터를 통신하여 획득할 수 있다. 다른 예를 들어, 라이 다 데이터 획득 모듈은 라이다 유닛의 광 수신 모듈과 라이다 제어 모듈로 구현될 수 있다. 여기서, 관심 영역 추출 모듈과 객체 인식 모듈은 기능적으로 구분되는 구성 요소로서, 물리적으로는 단일한 하 나의 제어 모듈로 제공되거나 또는 물리적으로 분리된 복수의 제어 모듈(CPU, 칩, 전자 회로)로 제공될 수 있다. 도 2 및 도 3은 본 명세서의 일 실시예에 따른 라이다 유닛이 관찰 영역에 대한 데이터를 획득하는 방법에 관한 예시도이다. 도 2를 참조하면, 라이다 유닛은 관찰 영역에 레이저를 조사하고 반사광을 수신함으로써 관찰 영역 에 대한 데이터를 획득할 수 있다. 구체적으로, 도 3과 같이 라이다 유닛은 관찰 영역에 조사 된 레이저의 반사광이 수신 되는 시점 정보 및 광량에 기초한 반사강도(intensity) 정보를 획득할 수 있다. 여기서, 관찰 영역은 다양한 형상으로 형성될 수 있다. 예를 들어, 관찰 영역은 라이다 유닛으 로부터 일정 거리 이격된 가상 평면 또는 가상 곡면을 포함할 수 있다. 또 다른 예로, 관찰 영역은 라이다 유닛으로부터 제1 거리 이격된 면과 제2 거리 이격된 면 사이의 가상 공간을 포함할 수 있다. 또 다른 예 로, 관찰 영역은 라이다 유닛으로부터 미리 설정된 수평 각도와 미리 설정된 수직 각도 및 이격 거 리에 의해 정의되는 임의의 면 또는 공간을 포함할 수 있다. 한편, 다시 도 2를 참조하면 관찰 영역은 객체(Obj)를 포함할 수 있다. 여기서, 객체(Obj)는 관찰 영역 내 존재하는 물체로 라이다 유닛으로부터 조사된 광을 반사시킬 수 있다. 예를 들어, 객체(Obj)는 사람, 나무, 이동 수단, 건물 등의 가시적 물체뿐만 아니라 액체, 분자 단위의 기체 등 비가시적 물질 등을 포 함할 수 있으며 객체 시스템에서 수행되는 객체 인식의 대상을 포함할 수 있다. 이하에서는 라이다 유닛이 관찰 영역에 대한 데이터를 획득하는 방법에 대해서 서술한다. 라이다 유닛은 관찰 영역에 광을 조사하여 수신한 반사광을 기초로 광의 비행시간을 이용하여 거리 데이터를 획득할 수 있다. 구체적으로, 도 2 및 도 3을 참고하면 라이다 유닛에서 관찰 영역에 광을 조사하는 경우 관찰 영역 내에 위치하는 객체(Obj)에서 조사된 광이 반사하여 라이다 유닛으로 제공 되고, 라이다 유닛은 광이 조사된 시점 및 반사광이 수신된 시점에 기초하여 광의 비행시간을 산출하여 비행시간 데이터를 생성할 수 있다. 여기서, 다시 도 3을 참조하면, 라이다 유닛은 수신된 반사광의 반사 강도가 반사강도 임계값(Ith) 보다 커지는 경우 반사광이 수신된 것으로 인식하고 반사광의 반사강도가 반사강 도 임계값(Ith) 보다 커지는 시점을 기초로 비행시간을 산출할 수 있다. 따라서, 라이다 유닛은 보다 정 확한 반사광 수신 시점을 산출하기 위해 반사광 수신 시점을 측정된 시점보다 앞당겨 산출하거나, 라이다 유닛 이 반사광이 수신된 것으로 인식하기 위한 반사강도 임계값(Ith)을 감소시킬 수 있다. 한편, 라이다 유닛은 주파수 변조(FMCW: Frequency Modulated Continuous Wave)) 방식 또는 위상 변이 (phase shift) 방식으로도 거리 데이터를 획득할 수 있다. 여기서, 주파수 변조 방식은 라이다 유닛이 관 찰 영역에 조사하는 광의 주파수를 변화시키고 수신되는 반사광의 주파수 차이를 측정하여 거리 데이터를 획득하는 방식을 의미할 수 있다. 또 여기서, 위상 변이 방식은 라이다 유닛이 관찰 영역에 미리 설 정된 주파수를 갖는 광을 연속적으로 조사하고 수신되는 반사광의 위상 변화량을 측정하여 거리 데이터를 획득 하는 방식을 의미할 수 있다. 라이다 유닛은 비행시간 데이터를 이용하여 관찰 영역에 대한 거리 데이터 또는 깊이 데이터를 산출 하거나 관찰 영역에 대응하는 2D 이미지, 3D 이미지 또는 영상을 생성할 수 있다. 구체적으로, 라이다 유 닛은 비행시간 데이터를 이용하여 관찰 영역을 구성하는 점(point)들의 거리 또는 깊이를 나타내는 데이터들의 집합인 점 구름(point cloud) 이미지 또는 깊이 이미지(depth map)를 생성할 수 있다. 라이다 유닛은 반사광의 광량에 기초하여 반사강도 데이터를 획득할 수 있다. 구체적으로, 라이다 유닛 은 객체(Obj)로부터 반사된 광의 수신 시점뿐만 아니라 반사광의 반사강도 데이터도 획득할 수 있다. 여 기서, 반사강도 데이터는 객체(Obj)가 광을 반사하는 정도에 따라 같은 거리에 있더라도 다르게 측정될 수있다. 예를 들어, 다시 도 2 및 도 3을 참조하면 제2 객체(Obj2) 및 제3 객체(Obj3)는 라이다 유닛으로부 터 같은 거리만큼 이격되어 있으나 반사광의 반사강도는 제2 객체(Obj2)가 제3 객체(Obj3)보다 크게 측정될 수 있다. 또 다른 예로, 제1 객체(Obj1)는 제2 객체(Obj2) 보다 라이다 유닛으로부터 짧은 거리에 위치하고 있으나 반사율 차이로 인하여 제2 객체(Obj2)의 반사강도가 더 크게 측정될 수 있다. 한편, 라이다 유닛은 관찰 영역에 대해 광을 조사하여 수신된 반사광에 기초한 데이터들을 가공하지 않고 분석 유닛에 제공하여 분석 유닛에서 필요에 따라 제공받은 데이터를 가공 및 처리하거나, 관 찰 영역에 대한 데이터들을 가공 및 처리하여 분석 유닛에 제공할 수도 있다. 이하에서는 도 4 내지 도 9를 참조하여 라이다 유닛에서 측정된 관찰 영역에 대한 데이터에 기초하 여 분석 유닛이 객체 인식을 수행하는 방법에 대해 서술한다. 도 4는 본 명세서의 일 실시예에 따른 객체 인식 방법에 관한 순서도이다. 본 명세서의 일 실시예에 따른 분석 유닛은 라이다 유닛으로부터 획득한 데이터를 이용하여 관찰 영역에 대한 객체 인식을 수행할 수 있다. 도 4를 참조하면 객체 인식 방법은 분석 유닛이 라이다 유닛으로부터 관찰 영역에 대한 데이 터를 획득하는 단계(S1100), 관찰 영역에 대한 데이터를 이용하여 거리 데이터를 획득하는 단계(S1200), 관심 영역 데이터를 이용하여 거리 데이터 중 관심 데이터를 획득하는 단계(S1300) 및 인공 신경망을 이용하여 객체 인식을 수행하는 단계(S1400)를 포함할 수 있다. 이하에서는 상술한 각 단계들에 대해서 보다 구체적으로 설명한다. 분석 유닛은 라이다 유닛으로부터 관찰 영역에 대한 데이터를 획득할 수 있다(S1100). 여기서, 관찰 영역에 대한 데이터는 라이다 유닛에서 관찰 영역으로부터 수신한 반사광에 관한 데이터 또는 라이다 유닛에서 반사광에 관한 데이터를 가공 및 처리한 데이터를 포함할 수 있다. 예를 들 면, 분석 유닛이 획득하는 관찰 영역에 대한 데이터는 비행시간 데이터, 거리 데이터, 깊이 데이터, 반사강도 데이터 및 관찰 영역에 대응하는 점 구름 데이터 또는 이미지 데이터 중 적어도 하나를 포함할 수 있다. 여기서, 점 구름 데이터는 관찰 영역을 형성하는 점들 각각의 거리 데이터를 포함하는 데이터를 의미하거나 관찰 영역을 형성하는 점들 각각의 거리 데이터 및 반사강도 데이터를 포함하는 데이터를 의미 할 수도 있다. 분석 유닛은 관찰 영역에 대한 데이터를 이용하여 거리 데이터를 획득할 수 있다(S1200). 여기서, 거리 데이터는 관찰 영역에 대한 비행시간에 기초한 데이터로, 관찰 영역에 대응하는 2D 또는 3D 이 미지로 표시될 수 있다. 한편, 분석 유닛에서 객체 인식을 수행하기 위해 거리 데이터 외에 깊이 데이터 또는 관찰 영역 내 객체에 대한 정보를 나타내는 데이터가 이용될 수도 있다. 또한, 라이다 유닛에 서 관찰 영역에 대해 측정한 데이터를 가공하여 거리 데이터를 생성하여 분석 유닛에 제공한 경우 본 단계는 생략될 수 있다. 분석 유닛은 관찰 영역에 대한 데이터로부터 관심 영역 데이터를 획득할 수 있다(S1300). 여기서, 관심 영역 데이터는 관찰 영역 중 관심 영역을 지시하는 데이터를 포함할 수 있다. 예를 들어, 관 심 영역 데이터는 거리 데이터의 적어도 일부를 포함하도록 형성된 관찰 영역 내 관심 영역을 지시하는 좌 표값을 포함할 수 있다. 분석 유닛은 관심 영역 데이터를 획득함으로써 관찰 영역 중 객체 인식을 수행할 영역을 제안할 수 있다. 관찰 영역으로부터 관심 영역 데이터를 획득하는 방법에 대해서는 추후 구체적으로 서술하도록 한다. 분석 유닛은 관심 영역 데이터를 이용하여 거리 데이터 중 관심 데이터를 획득할 수 있다(S1400). 여기서, 관심 데이터는 객체 인식 시스템에서 객체 인식을 수행하고자 하는 관심 영역에 대한 데이터를 의 미할 수 있다. 또는, 관심 데이터는 추후 서술하는 객체 인식을 위한 인공 신경망의 입력 데이터로 이용되는 데 이터를 의미할 수 있다. 예를 들어, 분석 유닛은 관찰 영역 중 관심 영역 데이터가 지시하는 관심 영역에 속하는 거리 데이터를 객체 인식 대상 데이터로 선택할 수 있다.분석 유닛은 인공 신경망을 이용하여 객체 인식을 수행할 수 있다(S1400). 구체적으로 인공 신경망의 대 표적인 예로는 데이터를 입력받는 입력 레이어, 결과를 출력하는 출력 레이어 및 입력 레이어와 출력 레이어 사 이에서 데이터를 처리하는 히든 레이어(hidden layer)를 포함하는 딥 러닝 계열의 인공 신경망이 있다. 인공 신 경망의 세부적인 예시들로는, 합성곱 인공 신경망(Convolution Neural Network), 순환신경망(Recurrent Neural Network), 심층신경망(Deep Neural Network) 등이 있으며, 본 명세서에서 인공 신경망은 상술된 인공 신경망, 그 외의 다양한 형태의 인공 신경망 및 이들이 조합된 형태의 인공 신경망을 모두 포함하는 포괄적인 의미로 해 석되어야 하며, 반드시 딥 러닝 계열이어야만 하는 것도 아니다. 일 예로, 분석 유닛은 입력 데이터를 입력 받아 입력 데이터에 대한 객체 인식을 수행하도록 학습된 인공 신경망을 이용하여 관심 데이터로부터 객체 인식 데이터를 획득함으로써 관찰 영역에 대한 관심 객체 인식 을 수행할 수 있다. 한편, 분석 유닛은 인공 신경망 모델 외에 최근접 이웃 알고리즘(KNN), 랜덤 포레스트(Random Forest), 서포트 벡터 머신(SVM), 주성 분분석법(PCA) 등이 포함될 수 있으며, 이상에서 언급된 기법들이 앙상블된 형태 나 그 외에 다양한 방식으로 조합된 형태까지도 전부 포함할 수 있다. 한편, 인공 신경망을 중심으로 언급되는 실시예들에서 특별한 언급이 없는 한 인공 신경망이 다른 기계학습 모델로 대체될 수 있음을 미리 밝혀둔다. 도 5는 본 명세서의 일 실시예에 따른 관찰 영역 내 관심 영역을 추출하는 방법에 관한 순서도이다. 도 5를 참조하면, 관심 영역 추출 방법은 분석 유닛이 라이다 유닛으로부터 반사강도 데이터를 획 득하는 단계(S2100), 가상 평면에서 반사강도 이미지를 생성하는 단계(S2200), 전처리 과정을 통해 반사강도 이 미지를 가공하는 단계(S2300) 및 인공 신경망을 이용하여 관심 영역 데이터를 획득하는 단계(S2400)를 포함할 수 있다. 이하에서는 상술한 각 단계들에 대해서 보다 구체적으로 설명한다. 분석 유닛은 라이다 유닛으로부터 반사강도 데이터를 획득할 수 있다(S2100). 반사강도 데이터는 라이다 유닛에서 수신한 관찰 영역에 대한 반사광의 반사강도에 기초한 데이터를 의미할 수 있다. 반사강도 데이터는 관찰 영역 내 객체(Obj)로부터 반사되는 광에 기초하므로 광이 반사된 위치 정보를 포 함할 수 있다. 예를 들어, 반사강도 데이터는 관찰 영역 내 광을 반사하는 객체(Obj)의 공간 좌표와 반사 광의 세기값을 포함할 수 있다. 분석 유닛은 가상 평면에서 반사강도 이미지를 생성할 수 있다(S2200). 구체적으로, 분석 유닛은 반사강도 데이터에 기초하여 관찰 영역에 대한 반사강도 이미지(intensity map)을 생성할 수 있다. 여기서, 반사강도 이미지는 2D 이미지 또는 3D 이미지로 표시될 수 있다. 예를 들어, 반사강도 이미지는 관찰 영역에 대한 반사강도 데이터를 라이다 유닛으로부터 일정 거리 이격되어 있는 가상 평면에 투영하 여 생성한 2D 이미지를 포함할 수 있다. 또 다른 예로, 반사강도 이미지는 관찰 영역에 대한 반사강도 데 이터를 이용하여 생성한 관찰 영역에 대응되는 3D 이미지 또는 라이다 유닛으로부터 일정 거리 이격 되어 있거나 이격되어 있지 않은 가상 공간에 대응되는 3D 이미지를 포함할 수 있다. 여기서, 반사강도 데이터 또는 반사강도 이미지는 객체(Obj)가 라이다 유닛으로부터 이격되어 있는 거리 및 객체(Obj)의 반사율 또는 알베도 등 객체(Obj)의 위치, 상태 및 특성 중 적어도 하나를 고려할 수 있다. 예 를 들어, 반사강도 데이터는 객체(Obj)의 반사도를 포함할 수 있다. 본 명세서에서 반사강도 데이터는 일반적으 로 수신광의 세기값을 직접 반영할 수도 있으나, 반드시 그러해야 하는 것은 아니다. 예를 들어, 반사강도 데이 터로부터 관심 영역을 검출하기 위해 반사율이나 재질 등을 이용할 수도 있으며, 이때에는 수신광의 세기값을 거리 데이터에서 얻어진 거리값으로 보정한 결과를 반사강도 데이터로 이용할 수도 있다. 한편, 라이다 유닛에서 반사강도 데이터에 기초하여 반사강도 이미지를 생성하여 분석 유닛에 제공 하는 경우 앞서 서술한 라이다 유닛으로부터 반사강도 데이터를 획득하는 단계(S2100) 및 가상 평면에서 반사강도 이미지를 생성하는 단계(S2200)는 생략될 수 있다. 분석 유닛은 전처리 과정을 통해 반사강도 이미지를 가공할 수 있다(S2300). 구체적으로, 분석 유닛 은 반사강도 이미지의 명도, 채도, 대비, 해상도, 화질 등의 이미지 특성을 변경할 수 있다. 전처리 과정 을 통해 가공된 반사강도 이미지는 추후 서술하는 인공 신경망에 의해 객체 인식이 보다 정확히 수행될 수 있다.분석 유닛은 히스토그램 변형(histogram modification)을 통해 반사강도 이미지의 밝기, 명도 등을 변경 할 수 있다. 여기서 히스토그램 변형은 이미지 각 픽셀값들의 분포를 변경하는 이미지 처리 기법으로, 히스토그 램 균일화(histogram equalization) 기법을 포함할 수 있다. 한편, 분석 유닛은 히스토그램 변형 외에 그 레이 스케일 변형(gray-scale modification) 또는 영상 대수(image algebra) 등의 이미지 처리 기법을 이용하 여 반사강도 이미지의 밝기, 명도 등을 변경할 수 있다. 분석 유닛은 업샘플링(upsampling) 기법을 이용하여 반사강도 이미지의 해상도를 증가시켜 화질을 개선할 수 있다. 또는, 분석 유닛은 보간법(interpolation)을 이용하여 반사강도 이미지의 해상도를 증가시킬 수 도 있다. 분석 유닛은 인공 신경망을 이용하여 관심 영역 데이터를 획득할 수 있다(S2400). 구체적인 예로, 분석 유닛은 CNN(Convolution Neural Network), R-CNN(Regions with CNN), fast R-CNN, faster R-CNN, YOLO(You Only Look Once) 알고리즘 및 SSD(Single Shot Multi-box Detector) 중 적어도 하나의 인공 신경망을 이용하여 관심 영역 데이터를 획득할 수 있다. 일 예로, 분석 유닛은 입력 데이터를 입력 받아 입력 데이터 중 미리 설정된 조건을 만족하는 데이터의 범위를 지시하는 데이터를 출력하도록 학습된 인공 신경망을 이용하여 반사강도 데이터 또는 반사강도 이미지로 부터 관심 영역 데이터를 획득할 수 있다. 한편, 본 단계에서 이용되는 인공 신경망은 상술한 바와 같이 분석 유닛이 객체 인식을 수행하기 위해 이 용하는 인공 신경망을 포함할 수도 있다. 이하에서는 도 6 내지 도 8을 참조하여 본 명세서의 일 실시예에 따른 분석 유닛이 관심 영역 데이터를 획득하는 방법에 대해서 서술한다. 도 6 및 도 7은 본 명세서의 일 실시예에 따른 관찰 영역 내 관심 영역을 추출하는 방법에 관한 예시도이다. 도 6을 참조하면, 분석 유닛은 경계 박스를 이용하여 반사강도 이미지로부터 관심 영역을 추출할 수 있다. 구체적으로, 분석 유닛은 적어도 하나 이상의 경계 박스를 이용하여 반사강도 이미지를 미리 설정된 방향으로 슬라이딩하여 스캔하고 반사강도 이미지에 영역별 점수를 부여하여 미리 설정된 점수 이 상이 되는 영역을 관심 영역으로 추출할 수 있다. 여기서, 반사강도 이미지은 반사강도 데이터에 기초하여 생성된 2D 또는 3D 이미지이거나 전처리 과정을 거쳐 가공된 반사강도 이미지를 포함할 수 있다. 또는, 분석 유닛은 반사강도 이미지을 대신하 여 CNN을 이용하여 반사강도 이미지로부터 획득한 특징 맵(feature map)에 대해 적어도 하나 이상의 경계 박스를 이용하여 관심 영역을 추출할 수 있다. 또 여기서, 분석 유닛은 반사강도 이미지에 대해 가로, 세로 또는 대각선 방향으로 경계 박스를 슬 라이딩 시켜 영역별 점수를 획득할 수 있다. 분석 유닛이 관심 영역 데이터를 얻기 위해 이용하는 인공 신경망은 상술한 바와 같이 경계 박스를 이용 하여 관심 영역을 추출하도록 학습된 인공 신경망을 포함할 수 있다. 도 7을 참조하면, 분석 유닛은 그리드 셀을 이용하여 반사강도 이미지에서 관심 영역을 추출할 수 있다. 구체적으로, 분석 유닛은 반사강도 이미지를 적어도 하나 이상의 그리드 셀로 나누고 각 그리드 셀에서 적어도 하나 이상의 객체(Obj)가 있을 것으로 추정되는 경계 박스를 설정하고 점수를 부 여한 후 미리 설정된 점수 이상의 점수를 갖는 경계 박스를 관심 영역으로 획득할 수 있다. 여기서, 반사강도 이미지은 반사강도 데이터에 기초하여 생성된 2D 또는 3D 이미지이거나 전처리 과정을 거쳐 가공된 반사강도 이미지를 포함할 수 있다. 또는, 분석 유닛은 반사강도 이미지을 대신하 여 CNN을 이용하여 반사강도 이미지로부터 획득한 특징 맵(feature map)에 대해 적어도 하나 이상의 경계 박스를 이용하여 관심 영역을 추출할 수 있다. 보다 구체적으로, 다시 도 7을 참조하면 분석 유닛은 반사강도 이미지를 mxn의 그리드 셀로 나 누고, 각 그리드 셀에 제1 경계 박스 및 제2 경계 박스를 설정할 수 있다. 이 때, 그리드 셀 에 대한 데이터는 데이터 네트워크 형태로 저장될 수 있다. 여기서, 데이터 네트워크는 제1 경 계 박스에 대한 제1 경계 박스 데이터(311a), 제2 경계 박스에 대한 제2 경계 박스 데이터(311b) 및그리드 셀에 대한 클래스(class) 데이터(311c) 등을 포함할 수 있다. 여기서, 경계 박스 데이터는 경계 박 스에 대한 데이터로 각 경계 박스의 좌표 및 크기 정보를 포함할 수 있다. 또 여기서, 클래스 데이터(311c)는 그리드 셀이 각 클레스에 해당될 수 있는 확률 정보를 포함할 수 있다. 분석 유닛은 상술한 바와 같이 각 그리드 셀에서의 경계 박스들을 이용하여 반사강도 이미지 내 관심 영역을 추출하거나 객체(Obj) 분류를 할 수 있다. 이 때, 분석 유닛이 관심 영역 데이터를 획득 하기 위해 이용하는 인공 신경망은 상술한 바와 같이 반사강도 이미지를 그리드 셀로 나누어 관심 영 역을 추출하도록 학습된 인공 신경망을 포함할 수 있다. 한편, 분석 유닛은 상술한 방법 외에 다른 영역 제안 네트워크를 이용하여 반사강도 이미지로부터 관심 영역 데이터를 획득할 수 있음은 물론이다. 이하에서는 도 8 및 도 9를 참조하여 본 명세서의 일 실시예에 따른 관심 영역 데이터를 이용하는 객체 인식 방 법에 대해 서술한다. 도 8은 본 명세서의 일 실시예에 따른 관심 영역을 표시하는 방법에 관한 예시도이다. 분석 유닛은 반사강도 데이터를 이용하여 획득한 관심 영역 데이터를 이용하여 거리 데이터 중 관심 데이 터를 선택할 수 있다. 구체적으로, 분석 유닛은 라이다 유닛으로부터 획득한 거리 데이터 중 관심 영역 데이터를 이용하여 관심 데이터를 선택할 수 있다. 도 8을 참조하면, 분석 유닛은 거리 데이터 및 선택된 관심 데이터를 표시하는 관심 영역을 이미지로 출력할 수 있다. 여기서, 거리 데이터는 2D 이미지 또는 3D 이미지로 표시될 수 있다. 또 여기서, 관심 영역은 관심 영역 데이터에 기초하여 관찰 영역 내 관심 객체를 지시하는 영역을 포 함하며 관찰 영역에 대한 이미지에서 경계 박스 등으로 표시될 수 있다. 여기서, 관찰 영역에 대한 이미지는 점 구름 이미지, 깊이 이미지, 거리 이미지 등을 포함할 수 있다. 도 8을 참조하면, 관찰 영역 내 관심 객체는 자동차, 보행자 등을 포함하고, 제1 내지 제3 관심 영역(510, 520, 530)이 각각의 관심 객체을 지시하도록 표시될 수 있다. 한편, 본 명세서에 따른 일 예로 관심 영역은 반사강도 데이터로부터 획득된 관심 영역 데이터 외에 RGB 카메라에 의한 이미지 데이터 또는 라이다 유닛 외의 다른 장비에 의해 측정되어 획득된 관심 영역 데이 터에 기초하여 산출될 수도 있다. 이러한 멀티 모달 방식의 경우 거리 데이터와 관심 영역 데이터는 서로 다른 관찰 영역에 대한 정보를 포함할 수 있으므로 관심 데이터를 추출하는 데에 있어서 캘리브레이션 (calibration)이 필요할 수 있다. 구체적으로, 거리 데이터 및 관심 영역 데이터가 서로 다른 장비에서 측정된 데이터로부터 획득된 경우 각 데이터의 형식(format), 예를 들어 공간 좌표 등을 일치시킬 필요가 있다. 따라서, 분석 유닛은 객체 인식을 수행하는 데에 있어서 서로 다른 장비를 이용하는 경우 각 장비가 상호 호환되도록 데이터 형식을 일치시킨 후 관심 데이터를 획득할 수 있다. 다른 예로, 같은 장비를 이용하여 객체 인식을 수행하는 경우, 즉 싱글 모달 방식은 상술한 캘리브레이션 작업 은 생략될 수 있다. 예를 들어, 분석 유닛이 라이다 유닛으로부터 같은 관찰 영역에 대해 획 득한 거리 데이터 및 반사강도 데이터 등을 이용하여 객체 인식을 수행하는 경우 분석 유닛은 관찰 영역 에 대한 반사강도 데이터로부터 획득한 관심 영역 데이터를 이용하여 같은 관찰 영역에 대한 거리 데 이터 중 관심 데이터를 선택할 수 있다. 이 때, 객체 인식 시스템의 구성이 단순해짐에 따라 캘리브레이션 작업이 생략됨으로써 객체 인식 시스템의 구조 또는 프로세스 알고리즘이 간소해질 수 있다. 결과적으로, 객체 인식 시스템을 구현하는 데에 소모되는 비용이 절감될 수 있고 객체 인식을 수행하는데 소요되는 시 간이 감소할 수 있다. 또 다른 예로, 같은 장비를 이용하는 싱글 모달 객체 인식의 경우 분석 유닛은 라이다 유닛으로부 터 관찰 영역에 대한 비행시간 데이터에 기초하여 관찰 영역에 대응되는 2D 또는 3D 이미지를 생성하 고, 반사강도 데이터에 기초하여 반사강도 이미지를 생성하며, 반사강도 이미지로부터 관심 영역 을 추출하되, 상술한 2D 또는 3D 이미지 중 추출된 관심 영역에 대응되는 영역에 대해 객체 인식을 수행할 수 있다. 도 9는 본 명세서의 일 실시예에 따른 관심 객체에 대한 객체 인식을 수행하는 방법에 관한 예시도이다. 도 9를 참조하면, 인공 신경망은 관심 영역에 의해 선택된 관심 데이터를 입력받는 입력 레이어, 객체 인 식 결과를 출력하는 출력 레이어 및 입력 레이어와 출력 레이어 사이에 배치되는 적어도 하나의 히든 레이어를 포함할 수 있다. 입력 레이어에는 관심 데이터가 입력될 수 있다. 입력 레이어는 복수의 입력 노드를 포함할 수 있다. 입력 노드 각각에는 관심 데이터의 좌표 정보 및 거리 정보 등이 입력될 수 있다. 출력 레이어는 객체 인식 결과를 출력할 수 있다. 예를 들어, 인공 신경망이 이진 분류기(binary classification) 형태로 객체 인식 결과를 출력하는 경우에는 출 력 레이어에 하나 또는 두 개의 출력 노드가 포함될 수 있다. 이진 분류기 형태로 결과값을 출력하는 인공 신경 망은 주로 관심 객체가 특정 종류의 객체에 해당하는지 여부를 판단할 수 있다. 또 다른 예를 들어, 인공 신경망이 다중 분류기(multi classification) 형태로 객체 인식 결과를 출력하는 경우 에는 출력 레이어는 복수 개의 출력 노드를 포함할 수 있다. 다중 분류기 형태로 결과값을 출력하는 인공 신경 망은 주로 관심 객체가 복수의 종류의 객체에 해당하는지 여부를 판단할 수 있다. 또 다른 예를 들어, 인공 신경망이 회귀분석(regression) 형태로 진단 결과를 출력하는 경우에는 출력 레이어에 는 적어도 하나의 출력 노드가 포함될 수 있다. 인공 신경망은 기 확인된 객체 인식 결과값과 관심 데이터가 서로 태그된 라벨링 데이터를 학습용 데이터셋으로 이용하여 학습될 수 있다. 이에 따라, 충분히 학습된 인공 신경망은 관심 데이터가 입력되면 객체 인식 결과를 출력할 수 있다. 분석 유닛은 상술한 인공 신경망을 이용하여 관심 영역에 대한 객체 인식을 수행할 수 있다. 구체적 으로, 다시 도 9를 참조하면 분석 유닛은 제1 관심 영역에 의해 선택된 관심 데이터를 인공 신경망 의 입력 레이어에 입력하고, 인공 신경망의 히든 레이어 및 출력 레이어를 통해 제1 관심 영역에 의해 선 택된 관심 객체가 각 클래스에 해당할 확률값을 획득하여 결과적으로 관심 객체가 사람임을 판단할 수 있다. 이상에서 서술한 객체 인식 시스템은 현대 산업의 다양한 기술 분야에서 활용될 수 있다. 특히, 촬영된 이 미지 또는 영상에 포함된 객체에 관한 정보가 필요한 경우 본 명세서의 일 실시예에 따른 객체 인식 시스템 이 이용될 수 있다. 이처럼, 관찰 영역에 대한 객체 인식을 수행하여 획득한 관찰 영역 내 존재하는 객체의 정보를 이용 하는 대표적인 기술 분야로 자율 주행 기술이 있다. 본 명세서의 일 실시예에 따른 객체 인식 시스템은 차량에 장착되어 차량에 자율 주행에 필요한 정보를 제 공할 수 있다. 예를 들어, 객체 인식 시스템은 차량으로부터 미리 설정된 범위를 관찰 영역으로 하고, 관찰 영역에 대한 객체 인식을 수행하고 그로부터 획득한 객체 인식 결과를 차량에 제공하며 차량은 객체 인식 시스템으로부터 객체 인식 결과를 획득하고 이를 이용하여 자율 주행을 수행할 수 있다. 구체적 으로, 차량 외부에 탑재된 라이다 유닛은 차량의 주변에 광을 조사하고 반사광을 수신하여 획득한 관찰 영역에 대한 데이터를 분석 유닛에 제공하고, 분석 유닛은 객체 인식을 수행하여 차량 주변에 보행자의 위치, 다른 차량의 위치, 차선 정보, 교통 신호 정보 등 차량의 주행을 위한 정보를 획득하여 차량에 제공할 수 있다. 한편, 객체 인식 시스템의 활용이 상술한 경우에 한정되는 것은 아니며, 이미지 또는 영상 분석이 요구되 는 기술 분야에 활용될 수 있음은 물론이다. 예를 들어, 후술하는 바와 같이 객체 인식 시스템은 차량 내 부에서 운전자 등의 제스처를 인식하는 데에도 이용될 수 있다. 이하에서는 도 10 내지 도 12를 참조하여 상술한 객체 인식 시스템이 활용되는 경우에 대해 예시적으로 서 술한다. 도 10은 본 명세서의 일 실시예에 따른 차량에 탑재된 객체 인식 시스템에 관한 예시도이다. 도 10을 참조하면, 객체 인식 시스템은 차량에 탑재되어 차량 내부에 대한 객체 인식을 수행할 수 있다. 라이다 유닛은 차량 내부에 탑재되어 관찰 영역에 광을 조사할 수 있다. 구체적으로, 라이다 유닛 은 차량 천장 또는 차량 내 리어 뷰 미러(rear view mirror)에 탑재되어 차량 내부를 관찰 영역으로 하여 객체 인식을 위한 데이터를 획득할 수 있다. 차량 내에서 관찰 영역은 다양하게 설정될 수 있다. 예를 들어, 관찰 영역은 차량 내 탑승자의 제스 처를 인식하기 위해 기어를 중심으로 형성되거나 차량 내 운전석과 조수석 사이 공간에 형성될 수 있다. 또 다 른 예를 들어, 관찰 영역은 차량 내 각 좌석을 중심으로 형성되거나 차량 내부 전체에 대해 형성될 수도 있다. 라이다 유닛은 차량 내 관찰 영역에 광을 조사하고 반사광을 수신하여 획득한 관찰 영역에 대 한 데이터를 분석 유닛에 제공할 수 있다. 분석 유닛은 차량 내 탑재될 수도 있으나 차량 외부의 서버에서 객체 인식을 수행하는 경우 생략될 수 있 다. 이 때, 라이다 유닛은 서버에 관찰 영역에 대한 데이터를 제공하고, 서버는 라이다 유닛 으로부터 획득한 데이터에 기초하여 객체 인식을 수행하고 결과 데이터를 차량에 제공할 수 있다. 도 11은 본 명세서의 일 실시예에 따른 관심 영역을 표시하는 방법에 관한 예시도이다. 도 11을 참조하면, 차량 내 제스처 인식을 수행하기 위해 객체 인식 시스템이 동작할 수 있다. 여기서, 관 찰 영역은 운전석과 조수석 사이를 중심으로 형성되고 관심 객체는 운전석 및 보조석에 탑승한 사람의 손 을 포함할 수 있다. 분석 유닛은 라이다 유닛으로부터 차량 내 관찰 영역에 대한 데이터를 획득하여 객체 인식을 수행할 수 있다. 다시 도 11을 참조하면, 분석 유닛은 관찰 영역에 대한 데이터에 기초하여 획득한 관심 영역 데이터를 이용하여 차량 내 관찰 영역에 대응하는 이미지에 운전석 및 보조석에 탑승한 사람의 손을 각각 지시하는 제5 및 제6 관심 영역(550, 560)을 표시할 수 있다. 도 12는 본 명세서의 일 실시예에 따른 차량 내 제스처 인식을 나타내는 예시도이다. 분석 유닛은 관심 객체를 지시하는 관심 영역에 대한 객체 인식을 수행할 수 있다. 구체적으로, 도 12를 참조하면, 분석 유닛은 라이다 유닛으로부터 획득한 차량 내 관찰 영역에 대한 거리 데 이터 중 제6 관심 영역을 입력 데이터로 인공 신경망에 입력하여 결과 값을 획득할 수 있다. 여기서, 차량 내 제스처 인식에서 분석 유닛이 획득하는 결과값은 관심 객체 종류 및 관심 객체의 제스처 종류에 관한 값을 포함할 수 있다. 구체적으로, 다시 도 12를 참조하면, 분석 유닛은 제6 관심 영역(56 0)에 대해 객체 인식 수행에 따른 결과값을 기초로 관심 객체가 '보조석', '숫자 2'를 의미하는 것으로 판단할 수 있다. 또는 분석 유닛은 객체 인식 수행으로 획득한 결과값 또는 객체 인식 데이터를 차량을 제어하는 차량 제어 모듈에 제공하고, 차량 제어 모듈은 획득한 객체 인식 데이터를 이용하여 차량 내 적어도 하나의 기 능을 제어할 수 있다. 한편, 본 명세서의 일 실시예에 따른 객체 인식 시스템은 차량 내 제스처 인식에 활용되는 것으로 한정되 는 것은 아니며, 차량 내/외부뿐만 아니라 이미지 프로세싱이 요구되는 다양한 분야에서 활용될 수 있음은 물론 이다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도"}
{"patent_id": "10-2019-0134227", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "록 구성될 수 있으며, 그 역도 마찬가지이다.이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2019-0134227", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 명세서의 일 실시예에 따른 객체 인식 시스템을 도시한 도면이다. 도 2 및 도 3은 본 명세서의 일 실시예에 따른 라이다 유닛이 관찰 영역에 대한 데이터를 획득하는 방법에 관한 예시도이다. 도 4는 본 명세서의 일 실시예에 따른 객체 인식 방법에 관한 순서도이다. 도 5는 본 명세서의 일 실시예에 따른 관찰 영역 내 관심 영역을 추출하는 방법에 관한 순서도이다. 도 6 및 도 7은 본 명세서의 일 실시예에 따른 관찰 영역 내 관심 영역을 추출하는 방법에 관한 예시도이다. 도 8은 본 명세서의 일 실시예에 따른 관심 영역을 표시하는 방법에 관한 예시도이다. 도 9는 본 명세서의 일 실시예에 따른 관심 객체에 대한 객체 인식을 수행하는 방법에 관한 예시도이다. 도 10은 본 명세서의 일 실시예에 따른 차량에 탑재된 객체 인식 시스템에 관한 예시도이다. 도 11은 본 명세서의 일 실시예에 따른 관심 영역을 표시하는 방법에 관한 예시도이다. 도 12는 본 명세서의 일 실시예에 따른 차량 내 제스처 인식을 나타내는 예시도이다."}
