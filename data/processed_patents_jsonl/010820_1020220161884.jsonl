{"patent_id": "10-2022-0161884", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0079310", "출원번호": "10-2022-0161884", "발명의 명칭": "텍스트 분석을 위한 인공 신경망 모델의 앙상블 방법", "출원인": "주식회사 토브앤바나", "발명자": "박영재"}}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 상기 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득하는 단계;상기 복수의 인공 신경망 모델들 각각으로부터, 상기 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하는 단계;상기 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 상기 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득하는 단계; 및상기 입력 텍스트 데이터에 대응하는 레이블링 데이터와 상기 출력 데이터 사이의 차이에 기초하여, 상기 복수의 인공 신경망 모델들 및 상기 앙상블 가중치를 학습하는 단계를 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 출력 데이터를 획득하는 단계는상기 복수의 인공 신경망 모델들 각각으로부터, 상기 복수의 변환 임베딩 벡터들에 대응하는 출력 벡터를 획득하는 단계; 및상기 복수의 인공 신경망 모델들 각각에 대응하는 상기 출력 벡터에 상기 앙상블 가중치를 적용하여 상기 출력데이터를 획득하는 단계를 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 임베딩 벡터를 획득하는 단계는하나의 입력 텍스트 데이터를 상기 복수의 인공 신경망 모델들에 입력하여, 상기 복수의 인공 신경망 모델들 각각으로부터 상기 임베딩 벡터를 획득하는 단계를 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 복수의 변환 임베딩 벡터들을 획득하는 단계는미리 정해진 드롭 아웃 비율에 기초하여, 상기 임베딩 벡터에 대하여 복수 회 드롭 아웃을 수행하여 상기 복수의 변환 임베딩 벡터들을 획득하는 단계를 포함하는, 학습 방법.공개특허 10-2024-0079310-3-청구항 5 제1항에 있어서,상기 학습하는 단계는상기 차이에 기초하여 결정된 제1 손실 함수와 상기 복수의 인공 신경망 모델들 사이의 크로스 엔트로피에 기초하여 결정된 제2 손실 함수에 기초하여, 상기 복수의 인공 신경망 모델들 및 상기 앙상블 가중치를 학습하는 단계를 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,입력 텍스트 데이터는한국어 리뷰 데이터를 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 복수의 인공 신경망 모델들 각각은BERT 모델 및 분류 모델을 포함하는, 학습 방법."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "하드웨어와 결합되어 제1항 내지 제7항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2022-0161884", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 명령어를 저장하는 메모리; 및상기 메모리에 저장된 명령어를 실행함으로써,하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 상기 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득하고,상기 복수의 인공 신경망 모델들 각각으로부터, 상기 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하고,상기 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 상기 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득하는프로세서를 포함하는 전자 장치."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "텍스트 데이터의 감정 분석을 위한 인공 신경망 모델의 동작 방법이 개시된다. 일 실시예에 따른 학습 방법은 하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 복수의 인공 신경망 모델들 각각으로 부터 임베딩 벡터를 획득하는 단계, 복수의 인공 신경망 모델들 각각으로부터, 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하는 단계, 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득하는 단계 및 입력 텍스트 데이터에 대응하는 레이블 링 데이터와 출력 데이터 사이의 차이에 기초하여, 복수의 인공 신경망 모델들 및 앙상블 가중치를 학습하는 단 계를 포함한다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래 실시예들은 텍스트 데이터의 감정 분석을 위한 인공 신경망 모델의 동작 방법에 관한 것으로, 보다 구체적 으로는 한국어 리뷰 데이터의 감정 분석을 위한 인공 신경망 모델의 앙상블 방법에 관한 것이다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인터넷 웹 서비스의 발달로 인해 전자상거래의 규모가 증가하게 되었고, 이에 따라서 판매자의 수와 상품의 수 도 증가 하게 되었다. 고객이 요구하는 상품은 시대와 문화를 반영하여 변화하고 있으며 그에 따라서 판매자도 새로운 상품을 발굴해야만 한다. 판매자들은 고객의 수요에 맞는 상품을 제공, 판매하기 위해 고객이 상품에 대해 느끼는 리뷰, 후기를 분석할 필요가 있으나, 고객의 리뷰 또는 피드백 데이터 분석에 시간이 많이 소요되고, 비용이 많이 필요하기 때문에 딥러닝 기술을 활용하여 고객이 원하는 제품을 발굴하기 위한 기술 개발이 요구되고 있다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "일 실시 예에 의하면, 텍스트 데이터로부터 감정 분석을 위한 모델의 앙상블 학습 방법이 제공될 수 있다. 보다 상세하게는, 텍스트 데이터의 감정 분석을 위한 복수의 인공 신경망 모델의 학습 방법 및 이를 수행하는 학습 장치가 제공될 수 있다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 따른 학습 방법은 하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 상기 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득하는 단계; 상기 복수의 인공 신경망 모델들 각각 으로부터, 상기 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하는 단계; 상기 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 상기 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득하는 단계; 및 상기 입력 텍스트 데이터에 대응하는 레이블링 데이터와 상기 출력 데이터 사이의 차이에 기 초하여, 상기 복수의 인공 신경망 모델들 및 상기 앙상블 가중치를 학습하는 단계를 포함한다. 상기 임베딩 벡터를 획득하는 단계는 상기 복수의 인공 신경망 모델들에 서로 다른 입력 텍스트 데이터를 입력 하여, 상기 복수의 인공 신경망 모델들 각각으로부터 상기 임베딩 벡터를 획득하는 단계를 포함하고, 상기 복수 의 변환 임베딩 벡터들을 획득하는 단계는 상기 복수의 인공 신경망 모델들 각각에 대응하는 드롭 아웃 비율을 결정하는 단계; 및 상기 드롭 아웃 비율에 기초하여, 상기 복수의 인공 신경망 모델들 별로 상기 임베딩 벡터에 대하여 복수 회 드롭 아웃을 수행하여 상기 복수의 변환 임베딩 벡터들을 획득하는 단계를 포함하고, 상기 드롭 아웃 비율을 결정하는 단계는 상기 복수의 인공 신경망 모델들 각각에 대응하는 입력 텍스트 데이터에 포함된 하나 이상의 레이블링 값을 식별하는 단계; 상기 식별 결과에 기초하여, 상기 입력 텍스트 데이터에 대응하는 레이블링 비중을 결정하는 단계; 및 상기 레이블링 비중에 기초하여, 상기 드롭 아웃 비율을 결정하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "한글 텍스트 데이터에 나타나는 사용자의 감정 값을 정확하게 식별할 수 있다. 복수의 인공 신경망 모델들과 자연어 처리 모델들을 이용하여 한글 텍스트 데이터에서 나타나는 사용자의 감정 값을 정확하게 식별할 수 있다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 개시되어 있는 특정한 구조적 또는 기능적 설명들은 단지 기술적 개념에 따른 실시예들을 설명하 기 위한 목적으로 예시된 것으로서, 실제로 구현된 형태는 다양한 다른 모습을 가질 수 있으며 본 명세서에 설 명된 실시예로만 한정되지 않는다. 제1 또는 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 이런 용어들은 하나의 구성요소 를 다른 구성요소로부터 구별하는 목적으로만 이해되어야 한다. 예를 들어 제1 구성요소는 제2 구성요소로 명명 될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 표현들, 예를 들어 \"~간의\"와 \"바로~간의\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 실시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함 을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 해당 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되 는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 실시예들은 퍼스널 컴퓨터, 랩톱 컴퓨터, 태블릿 컴퓨터, 스마트 폰, 텔레비전, 스마트 가전 기기, 지능형 자동 차, 키오스크, 웨어러블 장치 등 다양한 형태의 제품으로 구현될 수 있다. 이하, 실시예들을 첨부된 도면을 참 조하여 상세하게 설명한다. 각 도면에 제시된 동일한 참조 부호는 동일한 부재를 나타낸다. 도 1a는 인공 신경망을 이용한 딥러닝 연산 방법을 설명하기 위한 도면이다. 딥러닝(Deep Learning) 등을 포함하는 인공지능(AI) 알고리즘은 인공 신경망(Artificial Neural Network, AN N)에 입력 데이터를 입력시키고, 컨볼루션 등의 연산을 통해 출력 데이터를 학습하고, 학습된 인공 신경망을 이 용하여 특징을 추출할 수 있다. 인공 신경망은 생물학적 뇌를 모델링한 컴퓨터 과학적 아키텍쳐(Computational Architecture)를 의미할 수 있다. 인공 신경망 내에서, 뇌의 뉴런들에 해당되는 노드들은 서로 연결되어 있고, 입력 데이터를 처리하기 위하여 집합적으로 동작한다. 다양한 종류의 뉴럴 네트워크들을 예로 들면, 컨볼루션 뉴럴 네트워크(Convolutional Neural Network, CNN), 회귀 뉴럴 네트워크(Recurrent Neural Network, RNN), 딥 빌리프 네트워크(Deep Belief Network, DBN), 제한된 볼츠만 기계(Restricted Boltzman Machine, RBM) 방식 등이 있으나, 이에 제한되지 않는다. 피드-포워드 (feed-forward) 뉴럴 네트워크에서, 뉴럴 네트워크의 뉴런들은 다른 뉴런들과의 연결들(links)을 갖는다. 이와 같은 연결들은 뉴럴 네트워크를 통해, 한 방향으로, 예를 들어 순방향(forward direction)으로 확장될 수 있다. 도 1a는 입력 데이터를 입력 받아 출력 데이터를 출력하는 인공 신경망(예를 들어, 컨볼루션 뉴럴 네트워크 (Convolution Neural Network, CNN))의 구조를 도시한다. 인공 신경망은 1개 이상의 레이어(layer)를 보유한 딥 뉴럴 네트워크(deep neural network)일 수 있다.도 1b는 일 실시예에 따른 인공 신경망 모델의 학습 및 추론 방법을 설명하기 위한 도면이다. 도 1b를 참조하면, 일 실시예에 따른 리뷰 데이터 분석 시스템은 학습 장치 및 분석 장치를 포함할 수 있다. 일 실시예에 따른 학습 장치는 뉴럴 네트워크를 생성하거나, 뉴럴 네트워크를 훈련(train)(또는 학습(learn))하거나, 뉴럴 네트워크를 재훈련(retrain)하는 기능들과 같은 다양한 프로세싱 기능들을 갖는 컴퓨 팅 디바이스에 해당된다. 예를 들어, 학습 장치는 PC(personal computer), 서버 디바이스, 모바일 디바이 스 등의 다양한 종류의 디바이스들로 구현될 수 있다. 일 실시예에 따른 학습 장치는 주어진 초기 뉴럴 네트워크를 반복적으로 훈련(학습)시킴으로써, 하나 이상 의 훈련된 뉴럴 네트워크를 생성할 수 있다. 하나 이상의 훈련된 뉴럴 네트워크를 생성하는 것은 뉴 럴 네트워크 파라미터를 결정하는 것을 의미할 수 있다. 여기서, 파라미터들은 예를 들어 뉴럴 네트워크의 입/ 출력 액티베이션들, 웨이트들, 바이어스들 등 뉴럴 네트워크에 입/출력되는 다양한 종류의 데이터를 포함할 수 있다. 뉴럴 네트워크의 반복적인 훈련이 진행됨에 따라, 뉴럴 네트워크의 파라미터들은 주어진 입력에 대해 보 다 정확한 출력을 연산하기 위해 조정될(tuned) 수 있다. 일 실시예에 따른 학습 장치는 훈련된 하나 이상의 뉴럴 네트워크를 분석 장치에 전달할 수 있 다. 분석 장치는 모바일 디바이스, 임베디드(embedded) 디바이스 등에 포함될 수 있다. 일 실시예에 따 른 분석 장치는 뉴럴 네트워크의 구동을 위한 전용 하드웨어로 프로세서, 메모리, 입출력(I/O) 인터페이스, 디스플레이, 통신 인터페이스, 또는 센서 중 적어도 하나를 포함하는 전자 장치일 수 있다. 일 실시예에 따른 분석 장치는 태블릿 PC, 스마트폰, 개인용 컴퓨터(예를 들어, 노트북 컴퓨터 등), 인공 지능 스피커, 스마트 TV, 이동 전화기, 내비게이션, 웹 패드, PDA, 워크스테이션 등과 같이 메모리 수단을 구비 하고 마이크로 프로세서를 탑재하여 연산 능력을 갖춘 디지털 기기를 모두 포함하는 개념일 수 있다. 일 실시예에 따른 분석 장치는 하나 이상의 훈련된 뉴럴 네트워크를 그대로 구동하거나, 하나 이상의 훈련된 뉴럴 네트워크가 가공(예를 들어, 양자화)된 뉴럴 네트워크를 구동할 수 있다. 가공된 뉴럴 네트워크를 구동하는 분석 장치는, 학습 장치와는 별도의 독립적인 디바이스에서 구현될 수 있 다. 하지만, 이에 제한되지 않고, 분석 장치는 학습 장치와 동일한 디바이스 내에도 구현될 수 있다. 도 2는 일 실시예에 따른 리뷰 데이터 분석 시스템을 설명하기 위한 도면이다. 도 1a 내지 도 1b를 참조하여 설명한 내용은 도 2에도 동일하게 적용될 수 있다. 도 2를 참조하면, 일 실시예에 따른 분석 장치(예: 도 1b의 분석 장치)는 복수의 인공 신경망 모델들(220- 1 내지 220-n, n은 2이상의 자연수)을 통해, 입력 텍스트 데이터에 대응하는 분석 결과를 출력할 수 있다. 복수의 인공 신경망 모델들(220-1 내지 220-n)은 학습 장치(예: 도 1b의 학습 장치)에서 학습이 완 료된 후, 분석 장치에 탑재될 수 있다. 일 실시예에 따른 입력 텍스트 데이터는 복수의 인공 신경망 모델들(220-1 내지 220-n)에 입력되는 텍스트 데이터로, 예를 들어, 특정 언어(예: 한국어)로 작성된 사용자 리뷰(예: 쇼핑몰 구매 후기) 데이터일 수 있다. 그러나, 입력 텍스트 데이터가 한국어 리뷰 데이터로 한정되는 것은 아니다. 예를 들어, 입력 텍스트 데 이터는 평가의 대상이 될 수 있는 모든 유형의 텍스트 데이터를 포함할 수 있다. 일 실시예에 따른 분석 결과는 입력 텍스트 데이터에 대응하는 평가 정보를 의미할 수 있다. 분석 결과는 수치화된 데이터로 현출될 수 있다. 예를 들어, 분석 장치는 입력 텍스트 데이터를 긍정 평 가와 부정 평가 두 클래스로 분류할 수 있고, 긍정 평가일 경우 분석 결과로 1을 출력할 수 있고, 반대로 부정 평가일 경우 분석 결과로 0을 출력할 수 있다. 예시에서, 분석 결과가 두 개의 클래스로 표현 되는 것으로 설명하였으나, 이에 한정되는 것은 아니고, 분석 결과는 복수의 클래스로 표현될 수도 있다. 일 실시예에 따른 리뷰 데이터 분석 시스템은 복수의 인공 신경망들의 앙상블(ensemble)에 기초하여 분석 결과 를 생성할 수 있다. 아래에서 도 3 내지 도 4를 참조하여 복수의 인공 신경망들의 앙상블에 기초하여 분 석 결과를 생성하는 방법을 상세히 설명한다. 도 3은 일 실시예에 따른 복수의 인공 신경망 모델들을 학습하는 방법을 설명하기 위한 도면이다. 도 1a 내지 도 2를 참조하여 설명한 내용은 도 3에도 동일하게 적용될 수 있다. 도 3을 참조하면, 일 실시예에 따른 학습 장치(예: 도 1b의 학습 장치)는 학습 데이터를 이용하여 복수의 인공 신경망 모델들(320-1 내지 320-n)을 학습할 수 있다. 일 실시예에 따른 학습 데이터는 입력 텍스트 데이터와 입력 텍스트 데이터에 레이블링된 평가 데이터를 포함할 수 있다. 예를 들어, 입력 텍스트 데 이터가 영화 리뷰 데이터일 경우, 영화 리뷰 데이터에 대응하는 점수(예를 들어, 1점 내지 5점 중)가 평가 데이터일 수 있다. 또는, 입력 텍스트 데이터의 평가 데이터는 입력 텍스트 데이터에 고유하게 부여 된 점수를 가공하여 획득될 수도 있다. 예를 들어, 입력 텍스트 데이터가 영화 리뷰 데이터일 경우, 영화 리뷰 데이터에 대응하는 점수가 4점 이상일 경우 평가 데이터는 1에 레이블링되고, 2점 이하일 경우 0에 레이블 링될 수 있다. 다만, 학습 데이터를 획득하는 방법은 위 예시에 한정되지 않고, 다양한 방법을 통해 학습 데이 터가 획득될 수 있다. 아래에서, 레이블링된 평가 데이터는 레이블링 데이터로 지칭될 수도 있다. 입력 텍스트 데이터는 복수의 인공 신경망 모델들(320-1 내지 320-n) 각각에 입력될 수 있다. 제1 인공 신경망 모델(320-1)은 제1 자연어 처리 모델(321-1) 및 제1 분류 모델(327-1)을 포함할 수 있고, 제2 인공 신경 망 모델(320-2)은 제2 자연어 처리 모델(321-2) 및 제2 분류 모델(327-2)을 포함할 수 있고, 제n 인공 신경망 모델(320-n)은 제1 자연어 처리 모델(321-n) 및 제n 분류 모델(327-n)을 포함할 수 있다. 복수의 인공 신경망 모델들(320-1 내지 320-n)은 전처리 모듈(미도시)을 더 포함할 수 있다. 일 실시예에 따른 전처리 모듈은 입력 텍스트 데이터에 대하여 공백 삭제 등 정제한 후 입력 텍스트 데이터를 자연어 처리 모델에 입력 가능한 토큰들로 분리할 수 있다. 복수의 자연어 처리 모델들(321-1 내지 321-n)은 입력 텍스트 데이터(또는, 전처리된 입력 텍스트 데이 터)를 수신하여 입력 텍스트 데이터에 대응하는 임베딩 벡터(323-1 내지 323-n)를 출력할 수 있다. 임베 딩 벡터(323-1 내지 323-n)는 입력 텍스트 데이터의 문맥 정보를 갖는 벡터일 수 있다. 복수의 자연어 처 리 모델들(321-1 내지 321-n)은 BERT 모델일 수 있으나, 이에 한정되지는 않는다. 예를 들어, 복수의 자연어 처리 모델들(321-1 내지 321-n)은 ELMo 모델 및 OpenAI GPT 모델을 포함하는 다양한 유형의 자연어 처리 모델일 수 있다. 학습 장치는 입력 텍스트 데이터에 대응하는 임베딩 벡터(323-1 내지 323-n)를 출력할 수 있도록 복수의 자연어 처리 모델들(321-1 내지 321-n) 각각을 학습할 수 있고, 이 과정을 사전 학습(pre-training) 과정이라 지칭할 수 있고, 복수의 자연어 처리 모델들(321-1 내지 321-n) 각각을 사전 학습된 모델(pre-trained model)로 지칭할 수 있다. 학습 장치는 사전 학습된 복수의 인공 신경망 모델들(320-1 내지 320-n) 각각에 대응하는 임베딩 벡터(323-1 내 지 323-n)를 변환하여 변환 임베딩 벡터(325-1 내지 325-n)를 획득할 수 있다. 임베딩 벡터(323-1 내지 323- n)에 기초하여 획득된 변환 임베딩 벡터(325-1 내지 325-n)는 복수 개일 수 있다. 예를 들어, 학습 장치는 임 베딩 벡터(323-1)를 변환하여 복수 개(예: 3개)의 변환 임베딩 벡터들을 획득할 수 있다. 학습 장치는 복수의 인공 신경망 모델들(320-1 내지 320-n) 별로 동일한 수의 변환 임베딩 벡터가 획득할 수도 있고 서로 다른 수의 변환 임베딩 벡터를 획득될 수 있다. 학습 장치는 미리 정해진 드롭 아웃 비율에 기초하여 임베딩 벡터(323-1 내지 323-n)에 대하여 드롭 아웃을 수 행하여 변환 임베딩 벡터(325-1 내지 325-n)를 획득할 수 있다. 예를 들어, 드롭 아웃 비율이 80%이고, 입력 텍스트 데이터가 (a, b, c, d, e) 5개의 엘리먼트(예: 토큰)로 구성되었으며, 3개의 변환 임베딩 벡터를 획득한다고 가정하면, 학습 장치는 3번에 걸쳐 4개(5*80/100)의 임베딩 벡터를 임의로 드롭 아웃하여 1개의 엘 리먼트로 구성된 3개의 변환 임베딩 벡터를 획득할 수 있다. 복수의 자연어 처리 모델들(321-1 내지 321-n)은 모두 동일한 드롭 아웃 비율을 갖을 수 있다. 예를 들어, 복 수의 자연어 처리 모델들(321-1 내지 321-n) 모두 80%의 드롭 아웃 비율로 드롭 아웃을 수행할 수 있다. 복수의 분류 모델들(327-1 내지 327-n) 각각은 변환 임베딩 벡터(325-1 내지 325-n)를 수신하여 출력 벡터(329- 1 내지 329-n)를 출력할 수 있다. 복수의 분류 모델들(327-1 내지 327-n) 각각은 변환 임베딩 벡터(325-1 내지 325-n) 뿐만 아니라 임베딩 벡터(323-1 내지 323-n)를 수신하여 각각에 대응하는 출력 벡터(329-1 내지 329- n)를 출력할 수도 있다. 출력 벡터(329-1 내지 329-n)는 복수의 분류 모델들(327-1 내지 327-n) 각각이 출력한 평가 데이터일 수 있다. 출력 벡터는 복수의 클래스들 각각에 대응하는 확률 값을 포함할 수 있다. 예를 들어, 제1 분류 모델(327-1)은 입력 텍스트 데이터가 제1 클래스(예: 긍정 평가)로 분류될 확률이 0.7이고, 제2 클래스(예: 부정 평가)로 분류될 확률이 0.3이라고 결정할 수 있고, 제2 분류 모델(327-2)은 입력 텍스트 데이터가 제1 클래스(예: 긍정 평가)로 분류될 확률이 0.4이고, 제2 클래스(예: 부정 평가)로 분류 될 확률이 0.6이라고 결정할 수 있고, 제n 분류 모델(327-n)은 입력 텍스트 데이터가 제1 클래스(예: 긍정 평가)로 분류될 확률이 0.6이고, 제2 클래스(예: 부정 평가)로 분류될 확률이 0.4라고 결정할 수 있다.학습 장치는 복수의 인공 신경망 모델들(320-1 내지 320-n) 각각에 대응하는 앙상블 가중치에 기초하여 복수의 출력 벡터들(329-1 내지 329-n)을 가중합하여 출력 데이터를 출력할 수 있다. 예를 들어, 제1 인공 신경 망 모델(320-1)에 대응하는 앙상블 가중치가 0.2이고, 제2 인공 신경망 모델(320-2)에 대응하는 앙상블 가중치 가 0.1이고, 제n 인공 신경망 모델(320-n)에 대응하는 앙상블 가중치가 0.3이라고 가정하면, 학습 장치는 입력 텍스트 데이터가 제1 클래스(예: 긍정 평가)로 분류될 확률은 (0.7*0.2+0.4*0.1+?+0.6*0.3)이고, 제2 클 래스(예: 부정 평가)로 분류될 확률은 (0.3*0.2+0.6*0.1+?+0.4*0.3)이라고 결정할 수 있다. 학습 장치는 입력 텍스트 데이터에 대응하는 레이블링 데이터와 출력 데이터 사이의 차이에 기초하여, 복수의 인공 신경망 모델들 및 앙상블 가중치를 학습할 수 있다. 학습 장치는 레이블링 데이터와 출 력 데이터 사이의 차이에 기초하여 제1 손실함수를 결정할 수 있고, 나아가 추가로 복수의 인공 신경망 모 델들(320-1 내지 320-n) 사이의 크로스 엔트로피 손실에 기초하여 제2 손실함수를 결정할 수 있으며, 제1 손실 함수 및 제2 손실함수에 따라 결정된 최종 손실함수가 최소가 되도록 역전파를 통해 복수의 인공 신경망 모델들 각각의 파라미터 및 앙상블 가중치를 조정할 수 있다. 도 4는 일 실시예에 따른 복수의 인공 신경망 모델들을 학습하는 방법을 설명하기 위한 도면이다. 도 1a 내지 도 2를 참조하여 설명한 내용은 도 4에도 동일하게 적용될 수 있다. 도 4를 참조하면, 일 실시예에 따른 복수의 인공 신경망 모델들(320-1 내지 320-n)은 모두 동일한 입력 데이터 가 아닌 적어도 하나는 다른 학습 데이터에 기초하여 학습될 수 있다. 예를 들어, 입력 텍스트 데이터(410- 1)는 인공 신경망 모델들(320-1)에 입력될 수 있고, 입력 텍스트 데이터(410-2)는 인공 신경망 모델들(320-2)에 입력될 수 있고, 입력 텍스트 데이터(410-n)는 인공 신경망 모델들(320-n)에 입력될 수 있다. 다만, 도 4에는 복수의 인공 신경망 모델들(320-1 내지 320-n)이 모두 다른 입력 텍스트 데이터(401-1 내지 410-n)를 수신하는 것처럼 도시되었으나, 복수의 인공 신경망 모델들(420-1 내지 420-n) 중 일부는 동일한 입력 텍스트 데이터를 수신할 수도 있다. 복수의 인공 신경망 모델들(420-1 내지 420-n)은 전처리 모듈(미도시)을 더 포함할 수 있다. 일 실시예에 따른 전처리 모듈은 입력 텍스트 데이터(401-1 내지 410-n)에 대하여 공백 삭제 등 정제한 후 입력 텍스트 데이터 (401-1 내지 410-n)를 자연어 처리 모델에 입력 가능한 토큰들로 분리할 수 있다. 복수의 자연어 처리 모델들(421-1 내지 421-n)은 입력 텍스트 데이터(401-1 내지 410-n)(또는, 전처리된 입력 텍스트 데이터)를 수신하여 입력 텍스트 데이터(401-1 내지 410-n) 각각에 대응하는 임베딩 벡터(423-1 내지 423-n)를 출력할 수 있다. 임베딩 벡터(423-1 내지 423-n)는 입력 텍스트 데이터(401-1 내지 410-n) 각 각의 문맥 정보를 갖는 벡터일 수 있다. 복수의 자연어 처리 모델들(421-1 내지 421-n)은 BERT 모델일 수 있으나, 이에 한정되지는 않는다. 예를 들어, 복수의 자연어 처리 모델들(421-1 내지 421-n)은 ELMo 모델 및 OpenAI GPT 모델을 포함하는 다양한 유형의 자연어 처리 모델일 수 있다. 학습 장치는 입력 텍스트 데이터(401-1 내지 410-n)에 대응하는 임베딩 벡터(423-1 내지 423-n)를 출력할 수 있도록 복수의 자연어 처리 모델들(421-1 내지 421-n) 사전 학습할 수 있다. 학습 장치는 복수의 인공 신경망 모델들(420-1 내지 420-n) 각각에 대응하는 드롭 아웃 비율을 결정하고, 결정 된 드롭 아웃 비율에 기초하여 복수의 인공 신경망 모델들(420-1 내지 420-n) 별로 임베딩 벡터(423-1 내지 423-n)에 대하여 복수 회 드롭 아웃을 수행하여 복수의 변환 임베딩 벡터들(425-1 내지 425-n)을 획득할 수 있 다. 예를 들어, 학습 장치는 인공 신경망 모델(420-1)의 드롭 아웃 비율을 20%라고 결정할 수 있고, 입력 텍스 트 데이터(410-1)가 (a, b, c, d, e) 5개의 엘리먼트(예: 토큰)로 구성되었으며, 3개의 변환 임베딩 벡터를 획 득한다고 가정하면, 학습 장치는 3번에 걸쳐 1개(5*20/100)의 임베딩 벡터를 임의로 드롭 아웃하여 4개의 엘리 먼트로 구성된 3개의 변환 임베딩 벡터를 획득할 수 있다. 학습 장치는 인공 신경망 모델(420-2)의 드롭 아웃 비율을 40%라고 결정할 수 있고, 입력 텍스트 데이터(410-2)가 (a', b', c', d', e') 5개의 엘리먼트(예: 토큰)로 구성되었으며, 3개의 변환 임베딩 벡터를 획득한다고 가정하면, 학습 장치는 3번에 걸쳐 2개(5*40/10 0)의 임베딩 벡터를 임의로 드롭 아웃하여 3개의 엘리먼트로 구성된 3개의 변환 임베딩 벡터를 획득할 수 있다. 다시 말해, 학습 장치는 복수의 인공 신경망 모델들(420-1 내지 420-n) 별로로 서로 상이한 드롭 아웃 비율을 결정할 수 있고, 드롭 아웃 비율은 입력 텍스트 데이터(401-1 내지 410-n)의 특성에 기초하여 결정될 수 있다. 이를 통해, 학습 장치는 복수의 인공 신경망 모델들(420-1 내지 420-n)이 편향된 예측을 하는 것을 방지할 수 있다. 보다 구체적으로, 학습 장치는 복수의 인공 신경망 모델들(420-1 내지 420-n) 각각에 대응하는 입력 텍스트 데 이터(401-1 내지 410-n)에 포함된 하나 이상의 레이블링 값을 식별하고, 식별 결과에 기초하여 입력 데이터 (401-1 내지 410-n)에 대응하는 레이블링 비중을 결정할 수 있다. 예를 들어, 학습 장치는 입력 텍스트 데이터(401-1)를 구성하는 복수의 엘리먼트들(예를 들어, 5개)에 대응하는 레이블링 값을 식별(예를 들어, 긍정 평가가 4개, 부정 평가가 1개)할 수 있고, 전체 엘리먼트에서 특정 레이블 링 값이 차지하는 비중(예를 들어, 긍정 평가의 비중은 0.8, 부정 평가의 비중은 0.2)을 결정할 수 있다. 또한, 학습 장치는 입력 텍스트 데이터(401-2)를 구성하는 복수의 엘리먼트들(예를 들어, 5개)에 대응하는 레이 블링 값을 식별(예를 들어, 긍정 평가가 2개, 부정 평가가 3개)할 수 있고, 전체 엘리먼트에서 특정 레이블링 값이 차지하는 비중(예를 들어, 긍정 평가의 비중은 0.4, 부정 평가의 비중은 0.6)을 결정할 수 있다. 학습 장치는 결정된 레이블링 비중에 기초하여 드롭 아웃 비율을 결정할 수 있다. 학습 장치는 레이블링 비중 을 임계값과 비교하여 입력 텍스트 데이터가 특정 레이블링으로 치우쳐졌다고 판단될 경우 드롭 아웃 비율을 높 게 결정할 수 있고, 입력 텍스트 데이터가 특정 레이블링으로 치우쳐지지 않고 분포가 균형잡힌 경우에는 드롭 아웃 비율을 낮게 결정할 수 있다. 예를 들어, 학습 장치는 입력 텍스트 데이터(401-1)의 긍정 평가의 비중(예: 0.8)이 임계값(예: 0.7) 이상이기 때문에, 입력 텍스트 데이터(401-1)에 대응하는 드롭 아웃 비율을 높게(예: 80%) 결정할 수 있고, 입력 텍스트 데이터(401-2)의 부정 평가의 비중(예: 0.6)이 임계값(예: 0.7) 미만이기 때문에, 입력 텍스트 데이터(401-2)에 대응하는 드롭 아웃 비율을 낮게(예: 40%) 결정할 수 있다. 복수의 분류 모델들(427-1 내지 427-n) 각각은 변환 임베딩 벡터(425-1 내지 425-n)를 수신하여 출력 벡터(429- 1 내지 429-n)를 출력할 수 있다. 복수의 분류 모델들(427-1 내지 427-n) 각각은 변환 임베딩 벡터(425-1 내지 425-n) 뿐만 아니라 임베딩 벡터(423-1 내지 423-n)를 수신하여 각각에 대응하는 출력 벡터(429-1 내지 429- n)를 출력할 수도 있다. 학습 장치는 복수의 인공 신경망 모델들(420-1 내지 420-n) 각각에 대응하는 앙상블 가중치에 기초하여 복수의 출력 벡터들(429-1 내지 429-n)을 가중합하여 출력 데이터를 출력할 수 있다. 학습 장치는 입력 텍스트 데이터(410-1 내지 410-n)에 대응하는 레이블링 데이터와 출력 데이터 사이의 차 이에 기초하여, 복수의 인공 신경망 모델들 및 앙상블 가중치를 학습할 수 있다. 학습 장치는 레이블링 데이터 와 출력 데이터 사이의 차이에 기초하여 제1 손실함수를 결정할 수 있고, 나아가 추가로 복수의 인공 신경 망 모델들(420-1 내지 420-n) 사이의 크로스 엔트로피 손실에 기초하여 제2 손실함수를 결정할 수 있으며, 제1 손실함수 및 제2 손실함수에 따라 결정된 최종 손실함수가 최소가 되도록 역전파를 통해 복수의 인공 신경망 모 델들 각각의 파라미터 및 앙상블 가중치를 조정할 수 있다. 도 5는 일 실시예에 따른 한국어 리뷰 데이터에 대한 감정 분석 결과를 출력하는 인공 신경망 모델의 예시를 도 시한 도면이다. 도 1a 내지 도 4를 참조하여 설명한 내용은 도 5에도 동일하게 적용될 수 있다. 도 5를 참조하면, 일 실시예에 따른 분석 장치는 학습이 완료된 복수의 인공 신경망 모델들(520-1 내지 520- 3)을 이용하여, 한국어 리뷰 데이터에 대응하는 감정 분석 결과(예: 부정적)를 출력할 수 있다. 복 수의 인공 신경망 모델들(520-1 내지 520-3)은 BERT 모델을 포함할 수 있다. 인공 신경망 모델(520-1)은 한국어 리뷰 데이터를 수신하여, 한국어 리뷰 데이터가 \"부정적\"임을 지 칭하는 출력 벡터를 출력할 수 있고, 인공 신경망 모델(520-2)은 한국어 리뷰 데이터를 수신하여, 한국어 리뷰 데이터가 \"부정적\" 임을 지칭하는 출력 벡터를 출력할 수 있고, 인공 신경망 모델(520-3)은 한국어 리뷰 데이터를 수신하여, 한국어 리뷰 데이터가 \"긍정적\" 임을 지칭하는 출력 벡터를 출력할 수 있다. 분석 장치는 복수의 인공 신경망 모델들(520-1 내지 520-3) 각각의 출력 벡터를 앙상블하여 감정 분석 결과인 최종 출력 데이터(예: 부정적)를 출력할 수 있다. 도 6은 일 실시예에 따른 텍스트 데이터의 감정 분석을 위한 인공 신경망 모델의 학습 방법을 설명하기 위한 순 서도이다. 설명의 편의를 위해, 단계들(610 내지 640)은 도 1b에 도시된 학습 장치를 사용하여 수행되는 것으로 기술 된다. 그러나 이 단계들(610 내지 640)은 어떤 다른 적절한 전자 기기를 통해, 그리고 어떤 적절한 시스템 내에 서도 사용될 수 있을 것이다.나아가, 도 6의 동작은 도시된 순서 및 방식으로 수행될 수 있지만, 도시된 실시예의 사상 및 범위를 벗어나지 않으면서 일부 동작의 순서가 변경되거나 일부 동작이 생략될 수 있다. 도 6에 도시된 다수의 동작은 병렬로 또는 동시에 수행될 수 있다. 단계에서, 학습 장치(예: 도 1b의 학습 장치)는 하나 이상의 입력 텍스트 데이터를 복수의 인공 신경 망 모델들에 입력하여, 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득할 수 있다. 단계에서, 학습 장치는 복수의 인공 신경망 모델들 각각으로부터, 임베딩 벡터를 변환하여 복수의 변환 임 베딩 벡터들을 획득할 수 있다. 단계에서, 학습 장치는 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 복수의 변 환 임베딩 벡터들에 대응하는 출력 데이터를 획득할 수 있다. 단계에서, 학습 장치는 입력 텍스트 데이터에 대응하는 레이블링 데이터와 출력 데이터 사이의 차이에 기 초하여, 복수의 인공 신경망 모델들 및 앙상블 가중치를 학습할 수 있다. 도 7은 일 실시예에 따른 학습 장치의 구성을 도시하는 도면이다. 도 7을 참조하면, 일 실시예에 따른 학습 장치는 하나 이상의 프로세서 및 메모리를 포함할 수 있다. 일 실시예에 따른 메모리는 컴퓨터에서 읽을 수 있는 명령어들(instructions)을 저장할 수 있다. 메모리 에 저장된 명령어들이 프로세서에 의해 실행되면, 프로세서는 명령어들에 의해 정의되는 동작들 을 처리할 수 있다. 메모리는 예를 들어 RAM(random access memories), DRAM(dynamic random access memories), SRAM(static random access memories) 또는 이 기술 분야에서 알려진 다른 형태의 비휘발성 메모리 를 포함할 수 있다. 메모리는 기 학습된 인공 신경망 모델을 저장할 수 있다. 일 실시예에 따른 하나 이상의 프로세서는 학습 장치의 전체적인 동작을 제어한다. 프로세서는 목적하는 동작들(desired operations)을 실행시키기 위한 물리적인 구조를 갖는 회로를 가지는 하드웨어로 구현 된 장치일 수 있다. 목적하는 동작들은 프로그램에 포함된 코드(code) 또는 명령어들을 포함할 수 있다. 하드 웨어로 구현된 장치는 마이크로프로세서(microprocessor), 중앙 처리 장치(Central Processing Unit; CPU), 그 래픽 처리 장치(Graphic Processing Unit; GPU), 프로세서 코어(processor core), 멀티-코어 프로세서(multi- core processor), 멀티프로세서(multiprocessor), ASIC(Application-Specific Integrated Circuit), FPGA(Field Programmable Gate Array), NPU(Neural Processing Unit) 등을 포함할 수 있다 일 실시예에 따른 프로세서는 하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득하고, 복수의 인공 신경망 모델들 각각으로부터, 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하고, 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득하고, 입력 텍스트 데이 터에 대응하는 레이블링 데이터와 출력 데이터 사이의 차이에 기초하여, 복수의 인공 신경망 모델들 및 앙상블 가중치를 학습할 수 있다. 도 8은 일 실시예에 따른 분석 장치의 구성을 도시하는 도면이다. 도 8을 참조하면, 일 실시예에 따른 분석 장치는 하나 이상의 프로세서 및 메모리를 포함할 수 있다. 일 실시예에 따른 메모리는 컴퓨터에서 읽을 수 있는 명령어들(instructions)을 저장할 수 있다. 메모리 에 저장된 명령어들이 프로세서에 의해 실행되면, 프로세서는 명령어들에 의해 정의되는 동작들 을 처리할 수 있다. 메모리는 예를 들어 RAM(random access memories), DRAM(dynamic random access memories), SRAM(static random access memories) 또는 이 기술 분야에서 알려진 다른 형태의 비휘발성 메모리 를 포함할 수 있다. 메모리는 기 학습된 인공 신경망 모델을 저장할 수 있다. 일 실시예에 따른 하나 이상의 프로세서는 분석 장치의 전체적인 동작을 제어한다. 프로세서는 목적하는 동작들(desired operations)을 실행시키기 위한 물리적인 구조를 갖는 회로를 가지는 하드웨어로 구현 된 장치일 수 있다. 목적하는 동작들은 프로그램에 포함된 코드(code) 또는 명령어들을 포함할 수 있다. 하드 웨어로 구현된 장치는 마이크로프로세서(microprocessor), 중앙 처리 장치(Central Processing Unit; CPU), 그 래픽 처리 장치(Graphic Processing Unit; GPU), 프로세서 코어(processor core), 멀티-코어 프로세서(multi-core processor), 멀티프로세서(multiprocessor), ASIC(Application-Specific Integrated Circuit), FPGA(Field Programmable Gate Array), NPU(Neural Processing Unit) 등을 포함할 수 있다 일 실시예에 따른 프로세서는 하나 이상의 입력 텍스트 데이터를 복수의 인공 신경망 모델들에 입력하여, 복수의 인공 신경망 모델들 각각으로부터 임베딩 벡터를 획득하고, 복수의 인공 신경망 모델들 각각으로부터, 임베딩 벡터를 변환하여 복수의 변환 임베딩 벡터들을 획득하고, 복수의 인공 신경망 모델들 각각에 대응하는 앙상블 가중치에 기초하여, 복수의 변환 임베딩 벡터들에 대응하는 출력 데이터를 획득할 수 있다. 이상에서 설명된 실시예들은 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨 어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치, 방법 및 구성요소는, 예를 들 어, 프로세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마 이크로컴퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령(instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 소프트웨어 애플리 케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처 리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설명된 경우도 있지만,"}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치는 복수 개의 프로세서 또는 하 나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들의 조합을 포함 할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로(collectively) 처 리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치, 또는 전송되는 신호 파(signal wave)에 영구적으로, 또는 일시적으로 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터는 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독 으로 또는 조합하여 포함할 수 있다. 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성 된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2022-0161884", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가진 자라면 상기를 기초로 다양한 기술적 수정 및 변형을 적용할 수 있다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1a 도면1b 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2022-0161884", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 인공 신경망을 이용한 딥러닝 연산 방법을 설명하기 위한 도면이다. 도 1b는 일 실시예에 따른 인공 신경망 모델의 학습 및 추론 방법을 설명하기 위한 도면이다. 도 2는 일 실시예에 따른 리뷰 데이터 분석 시스템을 설명하기 위한 도면이다. 도 3은 일 실시예에 따른 복수의 인공 신경망 모델들을 학습하는 방법을 설명하기 위한 도면이다. 도 4는 일 실시예에 따른 복수의 인공 신경망 모델들을 학습하는 방법을 설명하기 위한 도면이다. 도 5는 일 실시예에 따른 한국어 리뷰 데이터에 대한 감정 분석 결과를 출력하는 인공 신경망 모델의 예시를 도시한 도면이다. 도 6은 일 실시예에 따른 텍스트 데이터의 감정 분석을 위한 인공 신경망 모델의 학습 방법을 설명하기 위한 순 서도이다. 도 7은 일 실시예에 따른 학습 장치의 구성을 도시하는 도면이다. 도 8은 일 실시예에 따른 분석 장치의 구성을 도시하는 도면이다."}
