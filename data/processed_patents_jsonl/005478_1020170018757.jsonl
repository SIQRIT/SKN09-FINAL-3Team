{"patent_id": "10-2017-0018757", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2017-0099763", "출원번호": "10-2017-0018757", "발명의 명칭": "인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 방법 및 장치", "출원인": "바이두 온라인 네트웍 테크놀러지", "발명자": "첸, 치지에"}}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 방법에 있어서, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(screening)하는 단계; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크로 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 업데이트하는 단계;및상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 평균값 처리하는 단계;를 포함하는자소 음소 변환 모델 생성 방법."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드를 획득하기 위하여, 상기 미리 설정된 노드의 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(Screening)하는 단계는, 상기 각 히든 계층의 노드 중 하이딩(hiding)되지 않은 노드를 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 하기 위하여, 미리 설정된 노드의 하이딩(hiding) 비율에 기초하여 랜덤으로 상기 신경 네트워크 중 각 히든 계층의 노드 중 일부를 하이딩(hiding)하는 단계;를 포함하되상기 히든 계층의 노드 중 하이딩(hiding)되는 노드의 수는 상기 미리 설정된 노드의 하이딩(hiding) 비율에 대응되는자소 음소 변환 모델 생성 방법."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 각 단어를 트레이닝 하기 위한 리테이닝(retaining) 노드를 획득하기 위하여, 상기 미리 설정된 노드의 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(Screening)하는 단계는,상기 리테이닝(retaining)된 노드를 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 하기 위하여, 상기 미리 설정된 노드의 리테이닝(retaining) 비율에 기초하여 랜덤으로 상기 신경 네트워크의 각 히든 계층의노드 중 일부를 리테이닝(retaining)하는 단계;를 포함하고상기 리테이닝(retaining)된 노드의 수는 상기 미리 설정된 리테이닝(retaining) 노드의 비율에 대응하는자소 음소 변환 모델 생성 방법."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항 내지 제3 항 중 어느 한 항에 있어서, 상기 트레이닝 데이터 내의 단어 수에 기초하여 상기 신경 네트워크의 계층 수, 각 계층의 노드 수, 및 상기 미공개특허 10-2017-0099763-3-리 설정된 노드의 비율을 조정하는 단계;를 더 포함하는자소 음소 변환 모델 생성 방법."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 장치에 있어서, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(Screening)하는 처리모듈; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크를 응용하여 상기 각 단어를 트레이닝하고, 상기서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치를 업데이트하기 위한 트레이닝 모듈; 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 평균값 처리하는 생성모듈;을 포함하는자소 음소 변환 모델 생성 장치."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 상기 처리모듈은, 상기 각 히든 계층의 노드 중 하이딩(hiding)되지 않은 노드를 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 하기 위하여, 미리 설정된 노드의 하이딩(hiding) 비율에 기초하여 랜덤으로 상기 신경 네트워크 중 각 히든 계층의 노드 중 일부를 하이딩(hiding)하되,상기 히든 계층의 노드 중 하이딩(hiding)되는 노드의 수는 상기 미리 설정된 노드의 하이딩(hiding) 비율에 대응되는자소 음소 변환 모델 생성 장치."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5 항에 있어서, 상기 처리모듈은, 상기 리테이닝(retaining)된 노드를 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 하기 위하여, 상기 미리 설정된 노드의 리테이닝(retaining) 비율에 기초하여 랜덤으로 상기 신경 네트워크의 각 히든 계층의노드 중 일부를 리테이닝(retaining)하되,상기 리테이닝(retaining)된 노드의 수는 상기 미리 설정된 리테이닝(retaining) 노드의 비율에 대응하는자소 음소 변환 모델 생성 장치."}
{"patent_id": "10-2017-0018757", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제5 항 내지 제7 항 중 어느 한 항에 있어서,상기 처리 모듈은 상기 트레이닝 데이터 내의 단어 수에 기초하여 상기 신경 네트워크의 계층 수, 각 계층의 노드 수, 및 상기 미리 설정된 노드의 비율을 조정하는자소 음소 변환 모델 생성 장치.공개특허 10-2017-0099763-4-"}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 방법 및 장치를 제공하는 바, 그중 당해 방법은, 신경 네트워크를 응용하여 트레이닝 데이터 중의 각 단어에 대하여 자소 음소 변환을 행하는 매번의 트 레이닝 과정에서, 미리 설정한 노드의 비례에 따라 상기 신경 네트워크의 히든 계층 노드에 대하여 랜덤 스크리 (뒷면에 계속)"}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 자소 음소 변환 모델 생성 방법 및 장치에 관한 것으로서, 특히 인공 지능을 기반으로 하는 자소 음 소 변환 모델 생성 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능(Artificial Intelligence, 영어 약자는 AI임)은 사람의 지능을 시뮬레이션하고 확장하기 위한 이론, 방법, 기술, 및 어플리케이션을 연구 개발하는 새로운 기술 과학이다. 인공 지능은 컴퓨터 과학 기술의 한 분야 로서, 인류 지능의 본질을 요해하고 인류와 같이 행동할 수 있는 새로운 지능형 로봇을 구현하는데 이용된다. 상기 인공 지능분야에는 로봇, 음성 인식, 이미지 인식, 자연 언어 처리, 및 전문가 시스템 등이 포함된다. 상 기 인공 지능의 연구 분야 중 인공 지능의 가장 중요한 측면은 음성 식별 기술이다. 한편, 자소 음소 변환 모델(grapheme-to-phoneme, g2p)은 영문 음성 합성을 위한 중요 처리 모듈로서, 수신한 단어 중의 자소를 대응되는 음소로 변환하기 위한 모듈이다. 심층 신경 네트워크(Deep Neural Networks) 기술을 기반으로 음향 g2p 모델을 트레이닝함으로써 기존의g2p 모델에 비하여 더 좋은 응용 효과를 얻는 g2p 모델이 획 득 될 수 있다. 구체적으로 기존의 g2p모델의 음향 모델에 의한 트레이닝과 상이한 심층 신경 네트워크를 통한 g2p 모델을 트레이닝함에 따라 더 좋은 응용 효과를 가지는 g2p 모델이 획득될 수 있다. g2p 모델의 목표는 단어를 상기 단어에 대응되는 음소 시퀀스로 전환하는 것이고 실제적으로 존재하는 단어 수 도 상대적으로 고정된 것으로 10 만개의 단어에 불과하기에, g2p 트레이닝 모델이 필요하는 데이터 량은 상대적 으로 고정된 것이다. 이 때, 심층 신경 네트워크를 통하여 음향 모델을 트레이닝한다면, 트레이닝 데이터를 끊 임없이 증가할 수 있기에, 따라서 신경 네트워크의 계층 수 및 각 계층의 유닛 수는 끊임없이 증가할 수 있으므 로, 이로써 더좋은 기능을 가져 올 수 있다. 그러나 상기 신경 네트워크를 통한 g2p 모델(즉, 자소 음소 변환 모델)의 트레이닝은 데이터량이 변하지 않는 상태에서 네트워크 계층 수 및 각 유닛 수를 증가시키므로, 과적합(over-fitting) 현상을 빈번하게 발생시키는 문제점을 갖는다. 이른바 과적합(over-fitting) 현상이 발생하면 트레이닝을 통해 얻은 G2P모델의 트레이닝 데 이터의 성능은 좋지만, 상대적으로 테스트 데이터는 상기 트레이닝 데이터의 성능보다 떨어지게 될 수 있다. 상기 과적합 현상을 해결하기 위해, 만약 비교적 작은 네크워크를 선택하여 g2p 모델을 트레이닝하면, 기능을 상대적으로 받아들 일 수 있는 g2p 모델을 획득할 수 있다. 다만 상기 획득된 이러한 g2p 모델은 네트워크의 계 층 수 및 각 계층의 유닛 수가 모두 상대적으로 비교적 작아심층 더욱 강한 학습 능력과 일반화능력을 소유하지 못하게 된다."}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 출원은 적어도 일정한 정도에서 관련 기술에서의 기술적 과제 중의 하나를 해결하려는데 있다. 이를 위하여, 본 출원의 제 1 목적은 인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 방법을 제공하는 바, 당해 방법은 노드를 동태적으로 하이딩(hiding)하는 심층 신경 네트워크를 응용하여 데이터를 트레이닝하여, 자소 음소 변환 모델을 생성하고, 자소 음소 변환 모델의 학습능력과 일반화능력을 향상하고, 과 적합 현상이 해결된 자소 음소 변환 모델 생성 방법 및 장치를 제공하는 것에 있다. 본 출원의 제2 목적은 인공 지능을 기반으로 하는 과적합 현상을 해결하는 자소 음소 변환 모델 생성 방법 및 장치를 제공하는 것에 있다."}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위하여, 본 출원의 제1 측면 실시예에서는 자소 음소 변환 모델 생성 방법을 제공하는 바, 상기 방법은, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비 율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(screening)하는 단계; 상기 리테이닝 (retaining) 노드에 대응되는 서브 신경 네트워크로 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 업데이트하는 단계;및 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 평균값 처리하는 단계;를 포함할 수 있다."}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 출원명의 일 실시예에 따른 자소 음소 변환 모델 생성 방법은, 신경 네트워크에 의한 자소 음소 변환 트레이 닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝 (retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(screening)하는 단계; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크로 상 기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 업데이트하는 단계;및 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노 드의 가중치를 평균값 처리하여, 자소 음소 변환 모델을 생성하는 것을 실현하여, 자소 음소 변환 모델의 학습 능력과 일반화능력을 향상시키고, 또한 과적합 현상을 해결할 수 있다. 상기 목적을 달성하기 위하여, 본 발명의 제2 측면의 실시예는 자소 음소 변환(혹은 자소 음소 변환) 모델 생성 장치를 제공하는 바, 상기 장치는 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미 리 설정된 노드 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(Screening)하는 처리 모듈; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크를 응용하여 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치를 업데이트하기 위한 트 레이닝 모듈; 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝 (retaining) 노드의 가중치를 평균값 처리하는 생성모듈;을 포함할 수 있다. 본 출원의 실시예에 따른 자소 음소 변환(혹은 자소 음소 변환) 모델 생성 장치는, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(screening)하는 단계; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워 크로 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 업데 이트하는 단계;및 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝 (retaining) 노드의 가중치를 평균값 처리하여, 자소 음소 변환 모델을 생성한다. 이로써, 노드를 동태적으로 하이딩(hiding)하는 심층 신경 네트워크를 응용하여 데이터를 트레이닝하여, 자소 음소 변환 모델을 생성하는 것을 실현하여, 자소 음소 변환 모델의 학습능력과 일반화능력을 향상시키고, 또한 과적합 현상을 해결할 수 있 다."}
{"patent_id": "10-2017-0018757", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는, 본 발명의 실시예에 대하여 상세하게 설명하도록 한다. 상기 실시예의 예시는 첨부된 도면에 도시 되며, 그중 동일하거나 또는 유사한 부호는 동일하거나 유사한 소자 또는 동일하거나 유사한 기능을 구비한 소 자를 나타낸다. 아래에 첨부된 도면을 참조하여 설명되는 실시예는 예시적인 것으로서, 그 의의는 본 발명을 설 명하기 위한 것으로서, 본 발명에 대한 한정으로 이해하여서는 안된다. 아래의 도면을 참조하여 본 발명의 실시예에 따른 자소 음소 변환 모델 생성 방법 및 장치를 설명하기로 한다. 도 1은 본 발명의 실시예에 따른 자소 음소 변환(혹은 자소 음소 변환) 모델 의 생성 방법의 흐름도이다. 도 1에서 도시하는 바와 같이, 당해 자소 음소 변환(혹은 자소 음소 변환) 모델은 다음의 단계 S101 내지 단계 S103을 포함한다. 단계 S101, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(screening)할 수 있다. 단계 S102, 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크로 상기 각 단어를 트레이닝하고, 상 기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 업데이트할 수 있다. 단계 S103, 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 가중치를 평균값 처리하여, 자소 음소 변환 모델을 생성할 수 있다. 구체적으로, g2p가 더 강한 일반화능력을 소유하도록, 본 출원에서는 심층 신경 네트워크를 적용하여 트레이닝 한 g2p 모델을 얻는다. 또한 신경 네트워크를 응용하여 트레이닝 데이터 중의 각 단어에 대하여 자소 음소 전환 트레이닝을 행하는 과정에서, 네트워크 중의 노드 중 일부를 하이딩(hiding)함으로써 모델의 과적합을 방지한다. 즉, 노드를 동태적으로 하이딩(hiding)하는 신경 네트워크를 응용하여 데이터를 트레이닝하여, 자소 음소 변환 모델을 생성하고, 또한 생성된 자소 음소 변환 모델이 과적합 현상이 나타나는 것을 방지할 수 있다. 본 출원의 일 실시예예 따른 트레이닝을 위한 신경 네트워크는 심층 신경 네트워크인 것으로 설명하고 있으나, 이에 국한 되지 않고 다른 트레이닝 어플리케이션 시나리오가 적용될 수도 있는 바, 본 출원에서는 이에 대하여 한정하지 않기로 한다. 예를 들어, 상기 다른 트레이닝 어플리케이션 시나리오로는 장-단기 메모리(Long-Short Term Memory, LSTM) 등이 응용 될 수 있다. 심층 신경 네트워크는 3개 계층을 포함하는 바, 제1 계층은 입력계층이고, 맨 마지막 계층은 출력계층이며, 중 간 계층은 히든 계층인데, 일부 신경 네트워크는 하나의 히든 계층을 구비하고, 일부 신경 네트워크는 복수의 히든 계층을 구비하며, 각 계층 네트워크마다 노드가 있을 수 있다. 또한, 트레이닝 데이터 중 단어양에 따라, 트레이닝을 위한 심층 신경 네트워크의 크기가 적당히 조정될 수 있는바, 본 출원의 일 실시에예 따른 상기 심 층 신경 네트워크 구조의 계층 수, 및 각 계층의 노드의 수를 유동적으로 조정하는 것이 포함될 수 있다. 모델 과적합을 방지하기 위하여, 본 출원에서는 입력된 단어를 트레이닝하는 각각의 프로세스 과정에서, 입력된 단어를 트레이닝하기 전에 트레이닝 데이터 중의 단어 수에 기초하여 신경 네트워크의 계층 수, 각 계층의 노드 수, 및 미리 설정된 노드의 비율을 조정할 수 있다. 상기 미리 설정된 노드의 비율에는, 트레이닝 프로세스 동안 혹은 트레이닝 프로세스 전의, 심층 신경 네트워크 구조의 단순화 정도가 반영될 수 있다. 또한, 상기 신경 네트워크의 계층 수, 각 계층의 노드수, 미리 설정된 노드의 비율은 트레이닝할 단어 수에 의 해 조정될 수 있다. 한편, 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드를 획득하기 위하여, 다른 트레이닝 어플리케이션 시나리오에서는 전술한 방식과 다른 노드의 비율 조정 방식에 따라 랜덤으로 신경 네트워크의 히든 계층 노드를 스크리닝(Screening)할 수 있다. 이하에서는 상기 다른 어플리케이션 시나리오의 노드의 비율 조정 방식을 설명 하도록 한다.제1 실시예: 노드의 하이딩(hiding) 비율 구체적으로, 미리 설정된 노드의 하이딩(hiding) 비율(hiding ratio, 하이딩(hiding)율)에 의해, 랜덤으로 신경 네트워크 중 각 히든 계층의 노드에서 노드의 하이딩(hiding) 비율에 대응하는 수의 노드의 일부가 하이딩 (hiding)될 수 있다. 이에 따라(futher), 제1 예시에서 본 출원의 일 실시예예 따른 자소 음소 변환 모델 생성 방법 및 장치는 각 히든 계층의 노드에서 하이딩(hiding)되지 않은 노드 중 일부를 각리 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 사용할 수 있다. 제2 실시예: 노드의 리테이닝(retaining) 비율 미리 설정된 노드의 리테이닝(retaining) 비율에 기초하여, 랜덤으로 신경 네트워크 중 각 히든 계층의 노드의 일부를 리테이닝(retaining) 처리하여, 상기 리테이닝(retaining) 처리된 노드 중 일부를 각 단어를 트레이닝하 기 위한 리테이닝(retaining) 노드로 할 수 있다. 상기 리테이닝(retaining)된 히든 계층의 노드의 수는 미리 설정된 노드의 리테이닝(retaining) 비율에 대응된다. 이에 따라, 리테이닝(retaining) 노드에 대응하는 서브 신경 네트워크를 응ㅊ하여 노드에 대응되는 각 단어를 트레이닝하고, 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치를 업데이트 할 수 있다. 즉, 본 출원의 일 실시예에 따른 모델 트레이닝에서 하이딩(hiding)된 노드(비 리테이닝(retaining) 노드)를 사 용되지 않고, 상기 하이딩(hiding)된 노드에 대응되는 가중치가 리테이닝(retaining)되며 상기 가중치는 더 업 데이트되지 않을 수 있다. 다음 트레이닝 시, 또 다시 랜덤으로 소정의 비율에 따라 히든 계층의 일부 노드가 하이딩(hiding)될 수 있다. 상기 트레이닝 방식은 끊임없이 순환될 수 있다. 전술한 프로세스에 따른 트레이닝 방식은 매번 트레이닝 할 때 일부 노드를 감춤으로써 실제적으로 남은 네트워크가 원 네트워크에 비하여 상대적 으로 작은 네트워크가 되도록하여, 과적합 현상을 해결할 수 있는 효과를 가질 수 있다. 또한, 모든 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치에 대하여 평균값 처리가 수 행되어, 자소 음소 변환 모델이 생성될 수 있다. 매번 트레이닝시 랜덤으로 다른 노드를 하이딩(hiding)함으로써, 실제적으로 다수의 소규모 네트워크들이 획득 될 수 있다. 결과적으로 최종 결과물은 상기 소규모 네트워크들에 대하여 평균을 취하는 방식으로 인정될 수 있 는 바, 이는 아주 효과적으로 과적합을 해결하는 방식일 수 있다. 주의하여야 할 것은: 상기 하이딩(hiding)된 노드는, 실제로 삭제된 것이 아니라, 단지 당시 트레이닝하는 동안 사용되지 않는 것이고, 다음 트레이닝 할 때 에는 다시 사용되는 것이다. 상기 과정을 더욱 명확히 설명하기 위하여, 도 2 및 도3을 예를 들어 설명하기로 한다. 도 2는 본 출원의 일 실시예에 따른 3 개 계층의 신경 네트워크의 개략도이다. 도 2에서 도시하는 바와 같이, 입력계층에는 3개 노드, 히든 계층에는 6개 노드, 출력계층에는 2개 노드가 있다. 도 3은 본 출원의 일 실시예에 따른 과적합 방지 기술 트레이닝을 추가한 신경 네트워크의 개략도이다. 도 3을 참조하면, 도 3에서 점선으로 그려진 노드가 표시하는 것은 당해 트레이닝에서 하이딩(hiding)된 노드이다. 당 해 트레이닝 중 상기 하이딩(hiding)된 노드의 관련 가중치는 고려되지도, 업데이트 되지도 않는다. 이를 표현 하기 위하여, 상기 하이딩(hiding)된 노드에 대응되는 가중치는 점선으로 표시된다. 실선으로 그린 노드가 표시 하는 것은 당해 트레이닝 중에 리테이닝(retaining)된 노드로서, 즉 리테이닝(retaining) 노드를 사용하여 데이 터에 대하여 트레이닝이 수행되고, 상기 리테이닝(retaining) 노드의 가중치가 업데이트된다. 신경 네트워크를 이용하여 데이터를 트레이닝하고 자소 음소 변환 모델을 생성하는 과정에서, 노드 중 일부를 하이딩(hiding)하는 기술을 통해 상기 모델을 생성하고 과적합을 방지하는 것과 전부의 노드를 사용하여 상기 모델을 생성하는 것의 비교 결과를 더욱 명확히 설명하기 위하여, 표 1의 실험결과를 통하여 설명하도록 한다. 표 1 모델 네트워크 노드의 히든하이딩(hiding)된 여부음소 착오율(error rate) 256-BLSTM 아니오 과적합 256-BLSTM 예 9.37% 128-BLSTM+64-BLSTM+64-BLSTM 아니오 과적합 128-BLSTM+64-BLSTM+64-BLSTM 예 8.81% 128-BLSTM+128-BLSTM+64-BLSTM 아니오 과적합128-BLSTM+128-BLSTM+64-BLSTM 예 8.38% 표 1을 참조하면, 자소 음소 변환 모델의 히든 계층의 신경 네트워크 모델의 히든 계층만 도시되었다. 예컨대, 하나의 히든 계층은 모델128-BLSTM + 64-BLSTM + 64-BLSTM 3개의BLSTM (Bidirectional Long-Short Term Memory)의 신경 네트워크 모델을 소유하고 있음을 도시하여 설명하는 것이다. 256-BLSTM 모델에 대하여, 파라미터는 64만이고, 128-BLSTM+64-BLSTM+64-BLSTM 모델에 대하여, 파라미터는 44 만이며, 128-BLSTM+128-BLSTM+64-BLSTM 모델에 대하여, 파라미터는 74만 이다. 상기 모델의 파라미터는 노드의 가중치를 가리키는 바, 상기 파라미터의 양은 트레이닝 데이터 양에 비하여 현저히 많은 양이다. 만약 이 모델 을 이용하여 트레이닝 할 때 노드 중 일부를 감추지 않는다면, 모델은 과적합될 것이다. 신경 네트워크 중의 노 드 중 일부를 감추어 모델 트레이닝을 수행함으로써, 모델 과적합을 효과적으로 방지할 수 있으며, 또한 고성능 의 트레이닝 기능을 갖는 모델을 획득 할 수 있다. 상기의 설명 및 실험으로부터 알 수 있듯이, 본 출원의 장점은 다음과 같다. 본 출원의 일 실시예에 따른 자소 음소 변환 모델 생성 방법 및 장치는 신경 네트워크의 노드 중 일부를 감추어 g2p 모델을 트레이닝하여, 모델을 트레이닝 할 때 네트워크 계층 수 및 각 계층의 노드 수가 적당히 증가 될 수 있도록 할 수 있다. 이로써, 상기 자소 음소 변화 모델 생성 방법 및 장치는 모델의 일반화 능력을 향상시키고, 또한 과적합 현상이 발생하지 않도록 하여, 비교적 큰 규모의 신경 네트워크가 실제적인g2p 모델의 트레이닝에 이용될 수 있도록 한다. 본 출원의 일 실시예에 따른 자소 음소 변환 모델 생성 방법 및 장치는, 신경 네트워크를 응용하여 트레이닝 데 이터 중의 각 단어에 대하여 자소 음소 변환 트레이닝을 행하는 과정에서, 미리 설정된 노드의 비율에 따라 상 기 신경 네트워크의 히든 계층 노드를 스크리닝(Screening)함으로써 각 단어를 트레이닝 하기 위한 리테이닝 (retaining) 노드를 획득하고; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크를 이용하여 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중을 업데이트하며; 모든 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중에 대하여 평균값 처 리를 행하여, 자소 음소 변환 모델을 생성한다. 전술한 노드를 동태적으로 하이딩(hiding)하는 심층 신경 네트 워크를 이용하여 데이터를 트레이닝하여, 자소 음소 변환 모델을 생성함으로써 자소 음소 변환 모델의 학습능력 과 일반화능력을 향상시키고, 또한 과적합 현상을 해결할 수 있다. 상기의 실시예를 실현하기 위하여, 본 출원에서는 인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 장치가 더 제공될 수 있다. 도 4는 본 출원의 일 실시예에 따른 자소 음소 변환 모델 생성 장치의 구조의 블록도이다. 도 4에서 도시된 바와 같이, 자소 음소 변환 모델 생성 장치는, 신경 네트워크에 의한 자소 음소 변환 트레이닝을 트레이닝 데이터 내의 각 단어에 수행하는 과정 동안, 상기 각 단어의 트레이닝을 위한 리테이닝(retaining) 노드를 획득하기 위하여 미리 설정된 노드 비율에 따라 상기 신경 네트워크의 히든 계층의 노드를 랜덤으로 스크리닝(Screening)하는 처리모듈을 포함하되; 구체적으로, 상기 처리모듈은, 미리 설정된 노드의 비율에 따라 랜덤으로 상기 신경 네트워크의 히든 계층 노드에 대하여 스크리닝(Screenin g)하기 위한 것이다. 하나의 실시예에서, 상기 처리모듈은, 상기 각 히든 계층의 노드 중 하이딩(hiding)되지 않은 노드를 각 단어를 트레이닝하기 위한 리테이닝 (retaining) 노드로 하기 위하여, 미리 설정된 노드의 하이딩(hiding) 비율에 기초하여 랜덤으로 상기 신경 네 트워크 중 각 히든 계층의 노드 중 일부를 하이딩(hiding)하되, 상기 히든 계층의 노드 중 하이딩(hiding)되는 노드의 수는 상기 미리 설정된 노드의하이딩(hiding) 비율에 대응되는 것이다. 다른 하나의 실시예에서, 상기 처리모듈은 상기 리테이닝(retaining)된 노드를 각 단어를 트레이닝하기 위한 리테이닝(retaining) 노드로 하기 위하여, 상 기 미리 설정된 노드의 리테이닝(retaining) 비율에 기초하여 랜덤으로 상기 신경 네트워크의 각 히든 계층의 노드 중 일부를 리테이닝(retaining)하되, 상기 리테이닝(retaining)된 노드의 수는 상기 미리 설정된 리테이닝(retaining) 노드의 비율에 대응하는 것이다. 트레이닝 모듈은 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크를 응용하여 상기 각 단어 를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치를 업데이트하기 위한 것이다. 생성 모듈은 상기 자소 음소 변환 모델을 생성하기 위하여, 상기 서브 신경 네트워크의 각 리테이닝 (retaining) 노드의 가중치를 평균값 처리하기 위한 것이다. 상기 실시예를 기반으로하여, 또 다른 하나의 실시예에서 상기 처리모듈은 또 상기 트레이닝 데이터 중의 단어 수에 따라 상기 신경 네트워크의 계층 수, 각 계층의 노드 수, 및 상기 미리 설정된 노드의 비율을 조정하기 위한 것이다. 설명드리고자 하는 것은, 전술한 자소 음소 변환 모델 생성 방법의 실시예에 대한 해석과 설명은 당해 실시예의 인공 지능을 기반으로 하는 자소 음소 변환 모델 생성 장치에도 적용되는 바, 여기에서 더는 기술하지 않기로 한다. 본 발명의 실시예에 따른 자소 음소 변환 모델 생성 장치는, 신경 네트워크를 응용하여 트레이닝 데이터 중의 각 단어에 대하여 자소 음소 변환 트레이닝을 행하는 과정에서, 미리 설정된 노드의 비율에 따라 상기 신경 네 트워크의 히든 계층 노드에 대하여 랜덤 스크리닝(Screening)을 수행함으로써, 각 단어를 트레이닝 하기 위한 리테이닝(retaining) 노드를 획득하고; 상기 리테이닝(retaining) 노드에 대응되는 서브 신경 네트워크를 응용 하여 상기 각 단어를 트레이닝하고, 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치를 업데이트하며; 모든 상기 서브 신경 네트워크의 각 리테이닝(retaining) 노드의 관련 가중치에 대하여 평균값 처리를 행하여, 자소 음소 변환 모델을 생성한다. 이로써, 노드를 동태적으로 하이딩(hiding)하는 심층 신경 네 트워크를 응용하여 데이터를 트레이닝함으로써, 자소 음소 변환 모델을 생성하는 것을 실현하여, 자소 음소 변 환 모델의 학습능력과 일반화능력을 향상시키고, 또한 과적합 현상을 해결할 수 있다. 본 명세서의 설명에서 참조 용어 \"일 실시예\", \"일부 실시예\", \"예시\", \"구체적 예시\" 또는 \"일부 예시\" 등의 설명은 당해 실시예 또는 예시를 결부하여 설명하는 구체적인 특징, 구조, 재료 또는 특징이 본 발명의 적어도 하나의 실시예 또는 예시에 포함된다는 것을 의미한다. 본 명세서에서 상술한 용어에 대한 함축적인 표달이 반 드시 동일한 실시예 또는 예시를 가리키는 것은 아니다. 그리고, 설명된 구체적 특징, 구조, 재료 또는 특징은 임의의 하나 또는 복수개의 실시예 또는 예시에서 적합한 방식으로 결합될 수 있다. 이 외에, 서로 모순되지 않 는 상황하에서, 본발명이 속하는 기술 분야의 통상의 지식을 가진 자들은 본 명세서에서 설명한 다른 실시예 또 는 예시 및 다른 실시예 또는 실시예의 특징들을 결합하거나 조합할 수 있다. 이 외에, 용어 \"제1\", \"제2\" 등은 단지 설명의 목적을 위한 것일 뿐, 상대적인 중요성을 지시하거나 암시하는 것으로 이해해서는 안된다. 이로써, \"제1\", \"제2\"로 한정한 특징은 명시적이거나 또는 묵시적으로 적어도 하나 의 당해 특징을 포함할 수 있다. 본 발명의 설명에서, 별도의 설명이 없는 한, \"복수개\"의 함의는 적어도 두개 임을 가리킨다. 흐름도에서 또는 여기에서 기타 방식으로 설명되는 어떠한 과정 또는 방법 설명은 하나의 또는 복수개의 특정 로직 기능 또는 과정의 단계를 실현하기 위한 수행가능 명령의 코드의 모듈, 세그먼트 또는 부분을 포함하는 것 을 나타내는 것으로 이해할 수 있다. 그리고 본 발명의 바람직한 실시 방식의 범위는 별도의 실현을 포함하는바, 여기서 제시되거나 토론된 순서대로가 아닌, 거의 동시의 방식 또는 상반되는 순서를 포함한 순서 에 따라 기능을 수행될 수 있는바, 이는 본 발명의 실시예가 속하는 기술 분야의 통상의 지식을 가진 자들에 의 해 이해될 수 있다."}
{"patent_id": "10-2017-0018757", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 발명의 상기 및/또는 부가적 측면과 장점은 아래의 첨부된 도면을 결합하여 실시예에 대하여 설명하는 것으 로부터 명확해지고 쉽게 이해될 수 있다. 그중, 도 1은 본 출원의 실시예에 따른 자소 음소 변환(혹은 자소 음소 변환) 모델 생성 방법의 흐름도이다. 도 2는 본 출원의 일 실시예에 따른 세개 계층 신경 네트워크의 개략도이다. 도 3은 본 출원의 일 실시예에 따른 과적합 방지 기술 트레이닝을 추가한 신경 네트워크의 개략도이다. 도 4는 본 출원의 일 실시예에 따른 자소 음소 변환(혹은 자소 음소 변환) 모델 의 생성 장치의 구조 개략도이다."}
