{"patent_id": "10-2017-0133466", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0041790", "출원번호": "10-2017-0133466", "발명의 명칭": "신경망 번역 모델 구축 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "이요한"}}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "신경망 번역 모델을 생성하는 컴퓨팅 장치에서의 신경망 번역 모델 구축 방법에서,불특정 분야에서 사용되는 소스 도메인 데이터의 특징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제1 신경망 번역 모델을 생성하는 단계;특정 분야에서 사용되는 타겟 도메인 데이터의 특징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제2 신경망 번역 모델을 생성하는 단계;상기 소스 도메인 데이터와 상기 타겟 도메인 데이터의 공통된 특징을 학습하는 인코더-디코더 구조의 신경망을갖는 제3 신경망 번역 모델을 생성하는 단계; 및상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 조합기를 생성하는 단계 를 포함하는 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서, 상기 도메인 적응형 신경망 번역 모델은,상기 조합기의 조합 연산에 의해, 상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 앙상블 모델로 기능하는 것인 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서, 상기 제3 신경망 번역 모델을 생성하는 단계에서 상기 공통된 특징은,상기 소스 도메인 데이터가 표현하는 문장 내의 단어 분포 및 의미와 상기 타겟 도메인 데이터가 표현하는 문장내의 단어 분포 및 의미가 유사한 특징인 것인 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서, 상기 제3 신경망 번역 모델을 생성하는 단계상기 공통된 특징을 인코딩한 공통 특징 벡터값을 출력하는 인코더를 생성하는 단계;상기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에 속하는지를 분류하는 도메인 분류기를 생성하는 단계; 및상기 도메인 분류기에 의해 분류된 상기 공통 특징 벡터값를 디코딩하여 상기 공통된 특징의 번역 결과에 대응하는 출력 벡터값을 출력하는 디코더를 생성하는 단계를 포함하는 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에서, 상기 인코더를 생성하는 단계는,상기 도메인 분류기가 상기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에속하는지를 정확하게 분류하지 못하도록 상기 공통된 특징을 인코딩하는 단계인 것인 신경망 번역 모델 구축 방공개특허 10-2019-0041790-3-법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서, 상기 공통된 특징을 인코딩하는 단계는,상기 어느 도메인에 속하는지를 정확하게 분류하지 못하도록 상기 인코더를 구성하는 신경망 내의 노드들을 상호 연결하는 연결 가중치를 조정하는 단계인 것인 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에서, 상기 제3 신경망 번역 모델을 생성하는 단계는,상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공통된 특징에 대응하는 공통 특징 벡터값과 상기 제1신경망 번역 모델의 인코더에서 인코딩한 상기 소스 도메인 데이터의 특징에 대응하는 소스 특징 벡터값이 벡터공간에서 수직하도록 상기 공통된 특징을 학습하는 단계; 및상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공통된 특징에 대응하는 공통 특징 벡터값과 상기 제2신경망 번역 모델의 인코더에서 인코딩한 상기 타겟 도메인 데이터의 특징에 대응하는 타겟 특징 벡터값이 상기벡터 공간에서 수직하도록 상기 공통된 특징을 학습하는 단계를 포함하는 것인 신경망 번역 모델 구축 방법."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "불특정 분야에서 사용되는 소스 도메인 데이터의 특징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제1 신경망 번역 모델, 특정 분야에서 사용되는 타겟 도메인 데이터의 특징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제2 신경망 번역 모델, 상기 소스 도메인 데이터와 상기 타겟 도메인 데이터의 공통된 특징을 학습하는인코더-디코더 구조의 신경망을 제3 신경망 번역 모델 및 상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 조합기를 수학적으로 모델링한 신경망 번역 모델을 생성하는 프로세서; 및상기 프로세서의 명령에 의해 상기 신경망 번역 모델을 저장하는 저장 유닛을 포함하는 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에서, 상기 공통된 특징은,상기 소스 도메인 데이터가 표현하는 문장 내의 단어 분포 및 의미와 상기 타겟 도메인 데이터가 표현하는 문장내의 단어 분포 및 의미가 유사한 특징인 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에서, 상기 프로세서에 의해 생성된 상기 제3 신경망 번역 모델은,상기 공통된 특징을 인코딩한 공통 특징 벡터값을 출력하는 인코더;상기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에 속하는지를 분류하는 도메인 분류기; 및상기 도메인 분류기에 의해 분류된 상기 공통 특징 벡터값를 디코딩하여 상기 공통된 특징의 번역 결과에 대응하는 출력 벡터값을 출력하는 디코더공개특허 10-2019-0041790-4-를 포함하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에서, 상기 인코더는,상기 프로세서의 프로세스에 따라, 상기 도메인 분류기가 상기 공통 특징 벡터값이 상기 소스 도메인 및 상기타겟 도메인 중에서 어느 도메인에 속하는지를 정확하게 분류하지 못하도록 상기 공통된 특징을 인코딩하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에서, 상기 프로세서는,상기 어느 도메인에 속하는지를 정확하게 분류하지 못하도록 상기 인코더를 구성하는 신경망 내의 노드들을 상호 연결하는 연결 가중치를 조정하는 프로세스를 수행하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에서, 상기 프로세서는,상기 도메인 분류기를 상기 신경망 번역 모델의 학습 과정에서 실행하고, 상기 신경망 번역 모델에 의해 수행되는 실제 번역 과정에서는 실행하지 않는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에서, 상기 도메인 분류기는,상기 프로세서의 프로세스에 따라, 신경망의 계층 구조에서 다수의 은닉층으로 구현되는 것인 신경망 번역 모델구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제8항에서, 상기 제3 신경망 번역 모델은 상기 프로세서의 프로세스에 따라, 상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공통된 특징에 대응하는 공통 특징 벡터값과 상기 제1신경망 번역 모델의 인코더에서 인코딩한 상기 소스 도메인 데이터의 특징에 대응하는 소스 특징 벡터값이 벡터공간에서 수직하도록 상기 공통된 특징을 학습하고,상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공통된 특징에 대응하는 공통 특징 벡터값과 상기 제2신경망 번역 모델의 인코더에서 인코딩한 상기 타겟 도메인 데이터의 특징에 대응하는 타겟 특징 벡터값이 상기벡터 공간에서 수직하도록 상기 공통된 특징을 학습하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제8항에서, 상기 신경망 번역 모델은,상기 조합기의 조합 연산에 의해, 상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 앙상블 모델로 기능하는 것인 신경망 번역 모델 구축 장치.공개특허 10-2019-0041790-5-청구항 17 제8항에서, 상기 제1 신경망 번역 모델은 상기 프로세서의 프로세스에 따라, 상기 소스 도메인 데이터의 특징을 인코딩한 소스 특징 벡터값을 출력하는 인코더; 및상기 소스 특징 벡터값을 디코딩하여 상기 소스 도메인 데이터의 번역 결과에 대응하는 소스 출력 벡터값을 출력하는 디코더를 포함하도록 생성된 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에서, 상기 제1 신경망 번역 모델의 상기 인코더는,벡터 공간에서, 상기 제3 신경망 번역 모델의 인코더가 상기 공통된 특징을 인코딩한 공통 특징 벡터값에 수직하도록 상기 소스 도메인 데이터의 특징을 인코딩하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제8항에서, 상기 제2 신경망 번역 모델은 상기 프로세서의 프로세스에 따라,타겟 도메인 데이터의 특징을 인코딩한 타겟 특징 벡터값을 출력하는 인코더; 및상기 타겟 특징 벡터값을 디코딩하여 상기 타겟 도메인 데이터의 번역 결과에 대응하는 타겟 출력 벡터값을 출력하는 디코더를 포함하도록 생성된 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에서, 상기 제2 신경망 번역 모델의 상기 인코더는,벡터 공간에서, 상기 제3 신경망 번역 모델의 인코더가 상기 공통된 특징을 인코딩한 공통 특징 벡터값에 수직하도록 상기 타겟 도메인 데이터의 특징을 인코딩하는 것인 신경망 번역 모델 구축 장치."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "신경망 번역 모델 구축 방법이 개시된다. 이 방법은, 불특정 분야에서 사용되는 소스 도메인 데이터의 특징을 학 습하는 제1 신경망 번역 모델을 생성하는 단계; 특정 분야에서 사용되는 타겟 도메인 데이터의 특징을 학습하는 제2 신경망 번역 모델을 생성하는 단계; 상기 소스 도메인 데이터와 상기 타겟 도메인 데이터 간의 공통된 특징 을 학습하는 제3 신경망 번역 모델을 생성하는 단계; 및 상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 조합기를 생성하는 단계를 포함한다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 기술에 관한 것으로, 상세하게는 번역 분야에서 활용되는 신경망 모델에 관한 것이다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "신경망 번역 모델(neural network translation model)은 학습 데이터에 의해 학습된다. 따라서, 신경망 번역 모델은 학습 데이터의 스타일을 추종하는 번역 결과를 출력한다. 이러한 신경망 번역 모델의 성능은 특정 분야 의 테스트 데이터를 입력하여, 그 테스트 데이터에 대한 번역결과를 평가하는 방식으로 측정할 수 있다. 따라서, 학습 데이터의 스타일과 테스트 데이터(또는 실제 데이터)의 스타일 간의 유사도(similarity)에 따라 신경망 번역 모델의 성능이 크게 달라질 수 있다. 성능 개선을 위해, 기존의 신경망 번역 모델은 학습 데이터에 의해 학습된 번역 모델과 테스트 데이터에 의해 학습된 번역 모델이 조합된 앙상블 모델로 설계되고 있다. 그러나 이러한 앙상블 모델로 설계되었음에도, 여전 히 번역 성능을 개선하는 데 한계가 있다. 성능 측정을 목적으로 하는 테스트 데이터는 특정 분야에서 사용되는 데이터이고, 학습 데이터는 특정 분야보다 더 넓은 분야에서 사용되는 데이터이기 때문에, 테스트 데이터의 양은 학습 데이터의 양에 비해 현저히 적을 수 밖에 없다. 이것은 테스트 데이터가 사용되는 특정 분야와 동일한 분야에서 사용되는 실제 데이터에 대한 번역 성능의 저하를 의미한다. 또한, 분야별 데이터양의 차이가 큰 학습 환경에서 구축된 신경망 번역 모델은 오히려그 번역 성능을 저하하는 결과를 초래한다. 따라서, 기존의 신경망 번역 모델은 분야별 데이터의 양을 조율하는 작업 과정에서 막대한 시간과 비용을 유발한다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "전술한 문제점을 해결하기 위한, 본 발명의 목적은 데이터 양이 풍부한 소스 도메인에서의 번역 모델의 번역 성 능을 저하하지 않고, 동시에 소스 도메인에서의 번역 모델의 번역 결과를 이용하여 상대적으로 데이터의 양이 적은 타겟 도메인에서의 번역 모델의 번역 성능을 향상시킬 수 있는 신경망 번역 모델을 구축하는 데 있다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위한 본 발명의 일면에 따른 신경망 번역 모델 구축 방법은, 불특정 분야에서 사용되는 소스 도메인 데이터의 특징을 학습하는 제1 신경망 번역 모델을 생성하는 단계; 특정 분야에서 사용되는 타겟 도메인 데이터의 특징을 학습하는 제2 신경망 번역 모델을 생성하는 단계; 상기 소스 도메인 데이터와 상기 타 겟 도메인 데이터의 공통된 특징을 학습하는 제3 신경망 번역 모델을 생성하는 단계; 및 상기 제1 내지 제3 신 경망 번역 모델의 각 번역 결과를 조합하는 조합기를 생성하는 단계를 포함한다. 본 발명의 다른 일면에 따른 신경망 번역 모델 구축 장치는 불특정 분야에서 사용되는 소스 도메인 데이터의 특 징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제1 신경망 번역 모델, 특정 분야에서 사용되는 타겟 도메 인 데이터의 특징을 학습하는 인코더-디코더 구조의 신경망을 갖는 제2 신경망 번역 모델, 상기 소스 도메인 데 이터와 상기 타겟 도메인 데이터의 공통된 특징을 학습하는 인코더-디코더 구조의 신경망을 제3 신경망 번역 모 델 및 상기 제1 내지 제3 신경망 번역 모델의 각 번역 결과를 조합하는 조합기를 수학적으로 모델링한 신경망 번역 모델을 생성하는 프로세서; 및 상기 프로세서의 명령에 의해 상기 신경망 번역 모델을 저장하는 저장 유닛 을 포함한다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 도메인별로 특화된 다수의 신경망 번역 모델과 각 도메인 별 학습 데이터의 유사한 특징을 학습하는 신경망 번역 모델이 조합된 앙상블 모델을 구축함으로써, 소스 도메인에서의 번역 모델의 번역 성능을 저하하지 않고, 동시에 소스 도메인에서의 번역 모델의 번역 결과를 이용하여 상대적으로 데이터의 양이 적은 타겟 도메인에서의 번역 모델의 번역 성능을 향상시킬 수 있다. 나아가 소스 도메인에서의 학습 데이터의 양과 타겟 도메인에서의 학습 데이터의 양을 조율하는 작업 과정에서 소요되는 막대한 시간과 비용을 줄일 수 있다."}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 실시 예가 첨부된 도면과 연관되어 기재된다. 본 발명의 실시 예는 다양한 변경을 가할 수 있 고 여러 가지 실시 예를 가질 수 있는바, 특정 실시 예들이 도면에 예시되고 관련된 상세한 설명이 기재되어 있 다그러나 이는 본 발명의 다양한 실시 예를 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 다양한 실시예의 사상 및 기술 범위에 포함되는 모든 변경 및/또는 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용되었다. 먼저, 본 명세서에서는 언급되는 몇 가지 용어들을 아래와 같이 정의한다. 소스 도메인(source domain) 본 발명에서 소스 도메인은 어느 한 분야(field)를 지칭하는 용어로 지칭될 수 있다. 상기 어느 한 분야는, 예 를 들면, 정치, 경제, 사회, 과학, 교육, 철학, 문학, 스포츠, 엔터테인먼트 중 어느 하나일 수 있다. 또한, 본 발명에서 소스 도메인은 불특정 분야를 지칭하는 용어로 사용될 수 있다. 타겟 도메인(target domain) 본 명세서에서 타겟 도메인은 소스 도메인에서 정의된 어느 한 분야를 세분화한 특정 분야를 지칭될 수 있다. 예를 들면, 소스 도메인이 과학 분야를 지칭하는 경우, 타겟 도메인은 물리학, 화학, 생물학, 지구과학 중 어느 하나를 지칭하는 분야로 해석할 수 있다. 또한, 본 발명에서 타겟 도메인은 특정 분야를 지칭하는 용어로 사용될 수 있다. 소스 도메인 데이터(source domain data) 본 명세서에서 소스 도메인 데이터는 소스 도메인에서 정의된 어느 한 분야에서 주로 사용되는(또는 특화된) 학 습 데이터를 의미하며, 예를 들면, 소스 도메인이 과학 분야인 경우, 과학 분야에서 주로 사용되는 단어(word) 또는 문체(style)로 이루어진 문장(sentence) 또는 말뭉치(corpus)로 지칭될 수 있다. 타겟 도메인 데이터(target domain data) 본 명세서에서 타겟 도메인 데이터는 소스 도메인에서 정의된 어느 한 분야를 세분화한 특정 분야에서 주로 사 용되는(또는 특화된) 학습 데이터로 지칭될 수 있으며, 소스 도메인이 과학 분야인 경우, 물리학에서 주로 사용 되는 단어(word) 또는 문체(style)로 이루어진 문장(sentence) 또는 말뭉치(corpus)로 지칭될 수 있다. 또한, 타겟 도메인 데이터는 소스 도메인에서 정의된 어느 한 분야와 다른 분야에서 주로 사용되는(또는 특화된) 학습 데이터로 지칭될 수 있다. 또한, 타겟 도메인 데이터는 번역 결과를 알고 싶은 실제 문장이 사용되는 분야에 특화된 학습 데이터로 지칭될 수 있다. 또한, 타겟 도메인 데이터는 본 발명의 신경망 번역 모델의 성능 테스트를 목적으로 하는 '학습 데이터'로 지칭 될 수 있다. 또한, 포괄적인 의미에서 타겟 도메인 데이터는 소스 도메인 데이터에 비해 상대적으로 데이터 량이 적은 학습 데이터로 지칭될 수 있다. 신경망 번역 모델은 양질(good quality)의 데이터 량이 많은 학습 데이터로 학습될 때, 우수한 번역 성능을 제 공할 수 있다. 이러한 측면에서, 타겟 도메인 데이터는 소스 도메인 데이터에 비해 번역 성능이 낮은 학습 데이 터로 지칭될 수 있다. 또한, 타겟 도메인 데이터는 소스 도메인 데이터가 표현하는 문장 스타일과 다른 학습 데이터로 지칭될 수 있다. 예를 들면, 소스 도메인 데이터가 표현하는 문장 스타일은 대화체이고, 소스 도메인 데이터가 사용되는 분야와 동일한 분야에서 타겟 도메인 데이터가 표현하는 문장 스타일은 문어체일 수 있다. 대화체의 문장으로 학습된 신경망 번역 모델은 문어체의 문장에 대한 번역에서 양호한 번역 성능을 제공하지 못 한다. 또한 현대의 문장으로 학습된 신경망 번역 모델은 고문서에 대한 번역에서 양호한 번역 성능을 제공하지못한다. 인공 신경망(이하, 신경망) 신경망은 인간의 계산 능력을 모방하는 소프트웨어 또는 하드웨어로 구현된 인식 모델이다. 신경망은 상호 연결 된 뉴런 형태의 노드(node)들을 포함한다. 노드는 인공 뉴런으로 지칭될 수 있다. 노드들은 입력층(input layer), 출력층(output layer) 및 이들 사이에 개재된 다수의 은닉층(hidden layer)으로 분류될 수 있다. 한 층의 노드들은 연결선을 통해 다른 층의 노들과 연결될 수 있다. 연결선은 연결 가중치(connection weight)를 갖는다. 연결 가중치는 두 노드를 연결하는 노드들 간의 연관 정도를 수치화한 특정한 값으로서 '연결 강도'로 지칭될 수도 있다. 학습 학습은 노드 사이의 연결과 관련된 연결 가중치를 조정(갱신)하는 프로세스로 지칭될 수 있다. 이러한 연결 가 중치의 조정은 신경망 모델의 학습 과정에 포함될 수 있다. 상기 학습 과정은, 특정의 입력/출력 태스크의 예를 신경망 모델에 반복적으로 제공하는 단계, 오차를 측정하기 위해 신경망 모델의 출력과 원하는 출력을 비교하는 단계, 및 오차를 감소시키기 위해 연결 가중치를 조정하는 단계를 포함할 수 있다. 상기 학습과정은 더 이상의 반복이 오차를 감소시키지 못하게 될 때까지(또는 에러가 미리 정해진 최소값 이하 로 떨어질 때까지) 반복된다. 그러면, 신경망 모델은 '훈련된(trained)' 것으로 말해진다. 상기 학습 과정에서 연결 가중치를 조정하는 단계는 아래의 인코더 및 디코더를 수학적으로 나타낸 알고리즘의 조정을 의미한다. 상기 알고리즘의 조정은 상기 알고리즘을 구성하는 각 함수들의 결과값을 조정하는 의미로 해 석한다. 인코더 및 디코더 인코더는 소프트웨어 또는 하드웨어로 구현된 신경망으로 지칭될 수 있다. 소프트웨어로 구현된 경우, 인코더는 프로그램 코드 및 프로그램 코드들의 집합으로 이루어진 수학적 알고리즘으로 지칭될 수 있다. 본 발명에서 인코더는 학습 데이터(또는 입력 문장)를 인코딩하여 그에 대응하는 특징 벡터(또는 문장 벡터)를 출력한다. 특징 벡터(또는 문장 벡터)는 학습 데이터(또는 입력 문장)를 구성하는 단어의 분포와 단어의 의미 정보를 벡터 공간(vector space)에서 표현한 값으로 지칭될 수 있다. 단어의 분포는 N-gram 분포(N-gram distribution)를 의미한다. N-gram은 입력 문장을 구성하는 단어를 N개의 서브-스트링(sub-string)으로 나누는 분포 구조를 의미한다. 디코더는 소프트웨어 또는 하드웨어로 구현된 신경망으로 지칭될 수 있다. 소프트웨어로 구현된 경우, 디코더는 프로그램 코드 및 프로그램 코드들의 집합으로 이루어진 수학적 알고리즘이다. 본 발명에서 디코더는 특징 벡터 (또는 문장 벡터)를 디코딩하여 번역 결과에 대응하는 출력 벡터값을 출력한다. 여기서, 출력 벡터값은 특정 단 어를 번역한 결과가 정답일 확률을 나타내는 값으로 지칭될 수 있다. Sequence-to-Sequence 신경망 모델 본 발명에 따라 구축된 신경망 번역 모델은 Sequence-to-Sequence 신경망 모델을 기반으로 하며, Sequence-to- Sequence 신경망 모델은 인코더와 디코더를 포함하도록 구성된 신경망 모델을 지칭하는 용어로 사용될 수 있다. 이상 용어들의 설명은 이해를 돕기 위함이지, 본 발명의 기술적 사상을 한정하고자 하는 의도로 해석해서는 안 된다. 또한, 일부 용어들(인코더 및 디코더, Sequence-to-Sequence 신경망 모델, 연결 가중치)은 인공 신경망분야에서 자주 언급되는 공지의 용어들과 동일하게 해석될 수 있다. 특별한 언급이 없는 한, 이들 용어들에 대 한 설명은 공지의 설명으로 대신한다. 도 1은 본 발명의 실시 예에 따른 신경망 번역 모델 구축 장치의 블록도이다. 도 1을 참조하면, 본 발명의 실시 예에 따른 신경망 번역 모델 구축 장치는 신경망 번역 모델을 구축하는 컴퓨팅 장치로 구현될 수 있다. 컴퓨팅 장치는, 예를 들면, 인공지능 로봇, 컴퓨터 비전 장치, 음성 인식 장치, 모바일 폰, 웨어러블 디바이스, 가전 기기, 차량 내의 전자 장치 등일 수 있으며, 이에 한정하지 않고, 번역 기능을 갖는 모든 종류의 전자 장 치를 포함할 수 있다. 컴퓨팅 장치로 구현되는 신경망 번역 모델 구축 장치는 기본적으로, 하나 이상의 프로세서, 메모리 , 입력 장치, 출력 장치, 저장 유닛, 네트워크 인터페이스 및 이들을 연결하는 버스 를 포함할 수 있다. 추가로, 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있다. 상기 프로세서는 메모리 및/또는 저장 유닛에 저장된 처리 명령어, 신경망 번역 모델 구축 장치 의 운영체제, 각종 어플리케이션 및 신경망 번역 모델을 실행하는 중앙 처리 유닛 또는 반도체 장치일 수 있다. 상기 프로세서는 신경망 번역 모델을 학습하여 번역 성능이 개선된 신경망 번역 모델을 구축한다. 이를 위 해, 상기 프로세서는 번역 성능이 개선된 신경망 번역 모델을 구성하는 프로그램 코드 및 이들의 집합을 생성하고, 프로그램 코드 및 이들의 집합을 메모리 및/또는 저장 유닛에 저장하는 작업을 수행할 수 있다. 상기 메모리 및 상기 저장 유닛은 휘발성 저장 매체 또는 비 휘발성 저장 매체를 포함할 수 있다. 상기 입력 장치는 사용자 또는 외부 장치로부터 데이터를 수신하는 것으로, 사용자로부터 데이터가 입력되 는 경우, 키보드, 마우스, 터치 기능을 갖는 장치일 수 있고, 외부 장치로부터 데이터가 입력되는 경우, 외부 장치와의 인터페이싱을 기능을 갖는 장치일 수 있다. 상기 출력 장치는 상기 프로세서에 의해 처리된 결과를 출력하는 것으로, 영상 출력 수단 및 음성 출 력 수단을 포함할 수 있다. 영상 출력 수단은, 예를 들면, 번역 결과를 시각적으로 출력하는 LCD 또는 LED 표시 장치일 수 있다. 음성 출력 수단은, 예를 들면, 번역 결과를 청각적으로 출력하는 스피커일 수 있다. 상기 프로세서에 의해 구축되는 신경망 번역 모델은 인코더 및 디코더 구조를 갖는 Sequence-to-Sequence 신경망 모델을 기반으로 한다. Sequence-to-Sequence 신경망 모델을 기반으로 하는 본 발명의 실시 예에 따른 신경망 번역 모델의 개략적인 구성이 도 2에 도시된다. 도 2를 참조하면, 본 발명의 실시 예에 따른 신경망 번역 모델은 인코더-디코더 구조 기반의 3개의 신경망 번역 모델들(210, 230 및 250)과 조합기를 포함한다. 제1 신경망 번역 모델 상기 제1 신경망 번역 모델은 소스 도메인 데이터의 특징을 학습한다. 즉, 상기 제1 신경망 번역 모델 은 상기 소스 도메인에 특화된 학습 데이터의 특징만을 학습한다. 이러한 제1 신경망 번역 모델은 상 기 프로세서(도 1의 110)에 의해 생성, 실행 및 상기 저장 유닛에 저장되는 프로그램 코드들의 집합으로 이루어진 수학적 알고리즘으로 구현된다. 프로그램 코드들의 집합은 함수로 지칭될 수 있다. 상기 제1 신경망 번역 모델은 인코더 및 디코더를 포함한다. 상기 인코더는 소스 도메인 데이터가 표현하는 소스 문장을 k 차원 공간의 벡터로 인코딩하여 상기 소 스 문장의 특징을 벡터 공간에서 표현하는 소스 특징 벡터값을 출력한다. 상기 인코더는 순환 신경망 (Recurrent Neural Network) 또는 컨볼루션 신경망(Convolutional Neural Network)으로 구성될 수 있다. 상기 디코더는 상기 인코더로부터의 상기 소스 특징 벡터값을 상기 소스 문장을 구성하는 단어들의 나열 순서로 순차적으로 디코딩하여 상기 소스 문장의 번역 결과에 대응하는 소스 출력 벡터값을 출력한다. 상 기 디코더는 순환 신경망(Recurrent Neural Network) 또는 컨볼루션 신경망(Convolutional Neural Network)으로 구성될 수 있다. 상기 제2 신경망 번역 모델 상기 제2 신경망 번역 모델은 상기 소스 도메인 데이터에 비해 상대적으로 데이터 양이 적은 타겟 도 메인 데이터의 특징을 학습한다. 즉, 상기 제2 신경망 번역 모델은 상기 타겟 도메인에 특화된 학습 데이터의 특징만을 학습한다. 이러한 제2 신경망 번역 모델은 상기 프로세서(도 1의 110)에 의해 생성, 실 행 및 상기 저장 유닛에 저장되는 프로그램 코드들의 집합으로 이루어진 수학적 알고리즘으로 구현된다. 프로그램 코드들의 집합은 함수로 지칭될 수 있다. 상기 제2 신경망 번역 모델은 인코더 및 디코더를 포함한다. 상기 인코더는 상기 타겟 도메인 데이터가 표현하는 타겟 문장을 k 차원 공간의 벡터로 인코딩하여 상 기 타겟 문장의 특징을 벡터 공간에서 표현하는 타겟 특징 벡터값을 출력한다. 상기 인코더는 순환 신경망 (Recurrent Neural Network) 또는 컨볼루션 신경망(Convolutional Neural Network)으로 구성될 수 있다. 상기 디코더는 상기 인코더로부터의 타겟 특징 벡터값을 상기 타겟 문장을 구성하는 단어들의 나열 순서로 순차적으로 디코딩하여 상기 타겟 문장의 번역 결과에 대응하는 타겟 출력 벡터값을 출력한다. 상기 디 코더는 순환 신경망(Recurrent Neural Network) 또는 컨볼루션 신경망(Convolutional Neural Network)으 로 구성될 수 있다. 제3 신경망 번역 모델 상기 제3 신경망 번역 모델은 상기 소스 도메인 데이터와 상기 타겟 도메인 데이터의 공통된 특징을 학습한다. 즉, 상기 제3 신경망 번역 모델은 상기 소스 도메인에 특화된 학습 데이터의 특징과 상기 타겟 도메인에 특화된 학습 데이터의 특징 간의 공통된 특징만을 학습한다. 여기서, 상기 공통된 특징은 상기 소스 도메인 데이터에 대응하는 문장의 단어의 분포 및 의미와 상기 타겟 도메인 데이터에 대응하는 문장의 단어의 분포 및 의미가 유사한 특징으로 정의될 수 있다. 이러한 제3 신경망 번역 모델은 상기 프로세서(도 1의 110)에 의해 생성, 실행 및 상기 저장 유닛에 저장되는 프로그램 코드들의 집합으로 이루어진 수학적 알고 리즘으로 구현된다. 프로그램 코드들의 집합은 함수로 지칭될 수 있다. 상기 제3 신경망 번역 모델은 인코더, 도메인 분류기 및 디코더를 포함한다. 상기 인코더는 상기 소스 도메인 데이터가 표현하는 소스 문장과 상기 타겟 도메인 데이터가 표현하는 타 겟 문장 간의 공통된 특징을 k 차원 공간의 벡터로 인코딩하여 상기 공통된 특징을 벡터 공간에서 표현하는 공 통 특징 벡터값을 출력한다. 상기 인코더는 순환 신경망(Recurrent Neural Network) 또는 컨볼루션 신경망 (Convolutional Neural Network)으로 구성될 수 있다. 상기 도메인 분류기는 상기 공통된 특징이 속한 도메인을 분류한다. 즉, 상기 도메인 분류기는 상기 공통된 특징이 속한 도메인을 지시하는 도메인 분류 결과값을 출력한다. 출력된 도메인 분류 결과값은 상기 인 코더로 피드백된다. 상기 인코더는 상기 도메인 분류 결과값에 따라 학습 종료를 결정한다. 아래에서 설명하겠지만, 상기 인코더는 상기 도메인 분류기가 상기 공통된 특징이 속한 도메인을 정확하게 분 류하지 못할 때까지 상기 공통된 특징에 대한 학습을 반복적으로 수행한다. 즉, 상기 도메인 분류 결과값이 상 기 타겟 도메인을 지시하는 값이 아니고, 동시에 상기 소스 도메인을 지시하는 값이 아닐 때, 상기 인코더(25 2)는 충분한 학습을 수행한 것으로 보고, 학습을 종료한다. 상기 타겟 도메인을 지시하는 값이 아니고, 동시에 상기 소스 도메인을 지시하는 값이 아닌 경우를 나타내는 상 기 도메인 분류 결과값은, 예를 들면, 소스 도메인을 지시하는 도메인 분류 결과값을 '1', 타겟 도메인을 지시 하는 도메인 분류 결과값을 '0'이라 할 때, 0과 1의 중간값에 해당하는 0.5이다. 0.5는 이상적인 값이다. 실제 상기 인코더의 학습 종료를 결정하는 도메인 분류 결과값은 기 설정된 허용 오차 범위 내에서 소스 도메인 을 지시하는 도메인 분류 결과값과 타겟 도메인을 지시하는 도메인 분류 결과값의 중간값에 근사치로 설정하는 것이 바람직하다.결국, 본 발명에서 상기 인코더의 학습은 상기 도메인 분류기가 상기 공통된 특징의 도메인을 분류하 지 못하도록 상기 소스 도메인 데이터의 특징과 상기 타겟 도메인 데이터의 특징을 유사하게 만드는 학 습 과정이다. 따라서, 상기 도메인 분류기는 상기 인코더의 학습 과정에서만 실행되고, 실제 번역 과 정에서는 실행되지 않는다. 이러한 도메인 분류기는 신경망의 계층 구조에서 한 개 이상의 은닉층으로 구성될 수 있다. 상기 은닉층으 로 구성되는 상기 도메인 분류기는 상기 프로세서(도 1의 110)에 의해 생성, 실행 및 상기 저장 유닛(15 0)에 저장되는 프로그램 코드들의 집합으로 이루어진 수학적 알고리즘으로 구현된다. 상기 디코더는 상기 공통 특징 벡터값을 상기 공통된 특징을 구성하는 단어들의 나열 순서로 순차적으로 디코딩하여 상기 공통된 특징의 번역 결과에 대응하는 공통된 특징 기반의 출력 벡터값을 출력한다. 상기 디코 더는 순환 신경망(Recurrent Neural Network) 또는 컨볼루션 신경망(Convolutional Neural Network)으로 구성될 수 있다. 조합기 상기 조합기는 상기 제1 내지 제3 신경망 번역 모델(210, 230 및 250)의 각 번역 결과를 조합하여 최종 번 역 결과를 출력한다. 즉, 상기 조합기는 상기 디코더로부터의 소스 출력 벡터값, 상기 디코더로 부터의 타겟 출력 벡터값 및 상기 디코더로부터의 공통 출력 벡터값을 조합하여 상기 최종 번역 결과에 대 응하는 최종 출력 벡터값을 출력한다. 상기 조합기가 각 디코더로부터의 출력 벡터값을 조합하는 방법은, 예를 들면, 평균치 계산 방법이 이용될 수 있다. 즉, 상기 최종 출력 벡터값은 상기 소스 출력 벡터값, 상기 타겟 출력 벡터값 및 상기 공통 출력 벡터 값의 평균치일 수 있다. 이러한 조합기의 조합 연산에 의해, 본 발명의 신경망 번역 모델은 도메인 별로 구분되는 신경망 번 역 모델들의 각 번역 결과를 조합하는 앙상블 모델로 지칭될 수 있다. 상기 조합기는 상기 프로세서(도 1의 110)에 의해 생성, 실행 및 상기 저장 유닛에 저장되는 프로그 램 코드들의 집합으로 이루어진 수학적 알고리즘을 구현된다. 프로그램 코드들의 집합은 때때로 함수로 지칭될 수 있다. 신경망 번역 모델의 학습 본 발명의 일 실시 예에 따른 신경망 번역 모델은 각 인코더(212, 232, 252)에서 손실 함수(loss function)의 결과값이 최소화되는 방향으로 학습한다. 손실 함수는 도메인 분류기의 분류 성능을 나타내는 수학식이다. 손실 함수(Loss)는 아래와 수학식 1로 나타낼 수 있다. 수학식 1"}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 는 데이터를 나타내고, Gd는 도메인 분류기를 나타내는 함수이고, Gf는 소스 도메인 데이터와 타겟 도메인 데이터의 공통된 특징을 인코딩하는 인코더를 나타내는 함수이다. 그리고, d는 데이터 ( )가 속한 도메인을 인덱싱한 값이다.수학식 1에서, d = 1일 때, 우변에서 우측항 은 제거되고, 좌측항"}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "만이 잔존하게 된다. 잔존하는 좌측항 에서 log 안에 있는 은 데이터( )가 도메인 분 류기에 의해 분류된 결과값 즉, 소스 도메인 및 타겟 도메인 중 어느 하나를 지시하는 값이며, 타겟 도메 인으로 분류된 도메인 분류 결과값을 0, 소스 도메인으로 분류된 도메인 분류 결과값 1이라 가정할 때, 0과 1 사이의 값을 갖는다. 도메인 분류기가 정답 (d)을 찾으면, log 안의 값이 1이 되므로, 손실 함수(loss) 값은 최대가 되고, 반대 로 도메인 분류기가 정답을 찾지 못하면 log 안의 값이 0이 되므로, 손실 함수(loss)의 결과값은 최소가 된다. d=0 일 때, 동일한 현상이 일어난다. 결과적으로, 손실 함수(loss function) 결과값이 최소화되는 방향은 도메인 분류기가 데이터의 도메인을 정확하게 분류하지 않도록 인코더가 공통된 특징을 반복 학습하는 것이고, 이것은 인코더의 가중치 (weight) 조정을 통해 달성될 수 있다. 본 발명의 다른 실시 예에 따른 신경망 번역 모델의 학습은 소스 도메인 데이터에 특화된 인코더 및 타겟 도메인 데이터에 특화된 인코더의 각 출력 벡터값과 소스 도메인 데이터와 타겟 도메인 데이 터의 공통된 특징에 특화된 인코더의 출력 벡터값이 수직하도록 손실 함수를 정의하는 것이다. 본 발명에서 인코더는 소스 도메인 테이터와 타겟 도메인 데이터의 공통된 특징을 학습해야 하고, 다른 인코더들(212, 232)은 소스 도메인 테이터에 특화된 특징과 타겟 도메인 데이터에 특화된 특징을 각각 학 습해야 한다. 이러한 학습 전략이 완벽하게 수행되기 위해, 인코더에서 출력되는 공통 특징 벡터값과 인코더에서 출력되는 소스 특징 벡터값은 가장 낮은 유사도를 가져야 하며, 동시에 인코더에서 출력되는 공통 특징 벡 터값과 인코더에서 출력되는 타겟 특징 벡터값도 가장 낮은 유사도를 가져야 한다. 두 벡터값들의 유사도가 가장 낮은 경우는, 두 벡터값들이 벡터 공간에서 수직 관계에 있는 경우이다. 따라서, 인코더는 각 도메인에 특화된 인코더들(212, 232)이 출력하는 특징 벡터값에 수직한 특징 벡터를 출력하도록 소스 도메인 데이터와 타겟 도메인 데이터의 공통된 특징을 학습한다. 이러한 학습 과정은 아래의 수학식 2로 표현할 수 있는 도메인 분류기의 손실 함수로 나타낼 수 있다. 수학식 2"}
{"patent_id": "10-2017-0133466", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "H는 데이터에 대한 인코더의 특징 벡터를 의미하고, S는 소스 도메인을 지시하고, T는 타겟 도메인을 지시한다. 그리고, v는 도메인 특화된 인코더를 지시하고, s는 소스 도메인 데이터와 타겟 도메인 데이터 간의 공통 특징 을 인코딩하는 인코더를 의미한다. 즉 우변의 좌측항은 소스 도메인에 특화된 인코더가 출력하는 특 징 벡터와 상기 공통 특징에 특화된 인코더가 출력하는 벡터가 서로 수직 관계에 있음을 나타낸 것이고, 우변의 우측항은 타겟 도메인에 특화된 인코더가 출력하는 특징 벡터와 상기 공통 특징에 특화된 인코더 가 출력하는 특징 벡터가 서로 수직한 관계에 있음을 나타내 것이다. 이상과 같이 학습 단계에서 각 도메인에 특화된 인코더(212, 232)와 공통된 특징에 특화된 인코더가 도메 인 분류기의 손실 함수를 최소화하는 방향으로 학습하고, 나아가 각 도메인 별 특징 벡터가 수직관계에 있 도록 정의된 새로운 손실 함수를 이용하여 각 도메인별 구분되는 인코더들을 학습함으로써, 타겟 도메인 데이터 의 양이 적어서 타겟 도메인의 번역 성능을 보장하지 못하는 실제 환경을 반영하고, 막대한 시간과 비용이 소모되는 데이터 조율 작업 없이, 소스 도메인과 타겟 도메인의 번역 성능을 보장할 수 있다. 도 3은 본 발명의 실시 예에 따른 신경망 번역 모델 구축 방법의 흐름도로서, 설명의 이해를 돕기 위해, 도 1 및 2가 참조될 수 있다. 또한, 도 1 및 2를 설명한 내용과 중복되는 내용은 간략히 기재하거나 삭제한다. 또한 아래의 각 단계는, 특별히 언급하지 않는 한, 도 1에 도시된 프로세서에서 수행한다. 도 3을 참조하면, 먼저, 단계 S310에서, 불특정 분야에서 사용되는 소스 도메인 데이터의 특징을 학습하는 인코 더-디코더 구조의 신경망을 갖는 제1 신경망 번역 모델을 생성하는 과정이 수행된다. 이어, 단계 S320에서, 특정 분야에서 사용되는 타겟 도메인 데이터의 특징을 학습하는 인코더-디코더 구조 의 신경망을 갖는 제2 신경망 번역 모델을 생성하는 과정이 수행된다. 여기서, 상기 단계 S320는 상기 단 계 S310 보다 먼저 수행되거나 상기 단계 S310과 병렬적으로 수행될 수 있다. 이어, 단계 S330에서, 상기 소스 도메인 데이터와 상기 타겟 도메인 데이터의 공통된 특징을 학습하는 인코더-디코더 구조의 신경망을 제3 신경망 번역 모델을 생성하는 과정이 수행된다. 여기서, 상기 공통된 특징은, 상기 소스 도메인 데이터가 표현하는 문장 내의 단어 분포(또는 N-gram 분포) 및 의미와 상기 타겟 도메인 데이터가 표현하는 문장 내의 단어 분포 및 의미가 유사한 특징을 의미한다. 이어, 단계 S340에서, 상기 제1 내지 제3 신경망 번역 모델(210, 230 및 250)의 각 번역 결과를 조합하는 조합 기를 생성하는 과정이 수행된다. 이러한 조합기의 조합 연산에 의해, 본 발명의 신경망 번역 모델은 상기 제1 내지 제3 신경망 번역 모델(210, 230 및 250)의 각 번역 결과가 조합된 앙상블 모델로 기능할 수 있게 된다. 도 4는 본 발명의 일 실시 예에 따른 도 3의 단계 S330의 세부 흐름도이다. 도 4를 참조하면, 먼저, 단계 S331에서, 상기 소스 도메인 데이터와 상기 타겟 도메인 데이터의 공통된 특징을 k차원 공간의 벡터로 인코딩한 공통 특징 벡터값을 출력하는 인코더를 생성하는 과정이 수행된다. 이어, 단계 S333에서, 상기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에 속하는지를 분류하는 도메인 분류기를 생성하는 과정이 수행된다. 이때, 상기 인코더는 상기 도메인 분류기가 상기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에 속하는지 를 정확하게 분류하지 못하도록 상기 공통된 특징을 인코딩함을 특징으로 한다. 상기 도메인 분류기가 상 기 공통 특징 벡터값이 상기 소스 도메인 및 상기 타겟 도메인 중에서 어느 도메인에 속하는지를 정확하게 분류 하지 못한 값을 출력할 때까지, 인코더의 학습은 반복적으로 수행된다. 상기 인코더의 학습은 상기 인코더를 구성하는 신경망 내의 노드들을 상호 연결하는 연결 가중치 조정을 통해 달성될 수 있다. 이와 같이, 상기 인코더가 상기 도메인 분류기의 분류 작업을 방해하는 방향으로 학습함으로써, 손실 함수 의 결과값이 최소화될 수 있음은 전술한 바와 같다. 이어, 단계 S335에서, 상기 공통 특징 벡터값를 디코딩하여 상기 공통된 특징의 번역 결과에 대응하는 출력 벡 터값을 출력하는 디코더를 생성하는 과정이 수행된다. 도 5는 본 발명의 다른 실시 예에 따른 도 3의 단계 S330의 세부 흐름도이다. 도 5를 참조하면, 먼저, 단계 S337에서, 상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공 통된 특징에 대응하는 공통 특징 벡터값과 상기 제1 신경망 번역 모델의 인코더에서 인코딩한 상기 소스 도메인 데이터의 특징에 대응하는 소스 특징 벡터값이 벡터 공간에서 수직하도록 상기 공통된 특징을 학습하는 과정이 수행된다. 이어, 단계 S339에서, 상기 제3 신경망 번역 모델의 인코더에서 인코딩한 상기 공통된 특징에 대응하 는 공통 특징 벡터값과 상기 제2 신경망 번역 모델의 인코더에서 인코딩한 상기 타겟 도메인 데이터 의 특징에 대응하는 타겟 특징 벡터값이 상기 벡터 공간에서 수직하도록 상기 공통된 특징을 학습하는 과정 이 수행된다. 본 발명에서는 상기 학습 과정들(S337, S339)을 표현하는 전술한 손실 함수(수학식 2)가 새롭게 정의되고, 새롭 게 정의된 손실함수를 최소화하는 방향으로 인코더가 학습을 수행함으로써, 소스 도메인에서의 번역 모델 의 번역 성능을 저하시키지 않고, 동시에 소스 도메인에서의 번역 모델의 번역 결과를 이용하여 상대 적으로 데이터의 양이 적은 타겟 도메인에서의 번역 모델의 번역 성능을 향상시킬 수 있게 된다. 이상에서 본 발명에 대하여 실시 예를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아 니며, 본 발명이 속하는 분야의 통상의 지식을 가진 자라면 본 발명의 본질적인 특성을 벗어나지 않는 범위에서 이상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 예를 들어, 본 발명의 실시 예 에 구체적으로 나타난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2017-0133466", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 신경망 번역 모델 구축 장치의 블록도이다. 도 2는 도 1에 도시된 신경망 번역 모델 구축 장치에 의해 구축된 신경망 번역 모델의 구조도이다. 도 3은 본 발명의 실시 예에 따른 신경망 번역 모델 구축 방법의 흐름도이다. 도 4는 본 발명의 일 실시 예에 따른 도 3의 단계 S330의 상세 흐름도이다. 도 5는 본 발명의 다른 실시 예에 따른 도 3의 단계 S330의 상세 흐름도이다."}
