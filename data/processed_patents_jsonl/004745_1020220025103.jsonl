{"patent_id": "10-2022-0025103", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0127613", "출원번호": "10-2022-0025103", "발명의 명칭": "메타버스 기반의 실습형 교육 시스템", "출원인": "(주)엑스알터치", "발명자": "최진규"}}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "가상 클래스를 강의실이 필요한 교육기관 또는 단체에 임대하는 서비스를 제공하고, 가상 클래스 상에서 일어나는 학습 행동을 수집 및 저장하며, 가상 클래스의 임대 신청이나 사용 정보, 가상 클래스를 이용한 사용자의 정보를 기록 및 관리하는 관리자 웹 서버;메타버스 기반의 가상공간을 구축하여 메타버스를 활용한 화상기능과 가상공간이 결합하여 가상 클래스의 임대를 신청하는 사용자에게 비대면 교육 서비스를 제공하는 메타버스 플랫폼 서버; 및미리 설치되거나 메타버스 플랫폼 서버로부터 다운로드된 전용 어플리케이션을 통하여 관리자 웹 서버에 가상클래스의 임대를 신청하고, 메타버스 플랫폼 서버에 접속하여 가상 클래스에 참여하는 사용자 단말을 포함하는것을 특징으로 하는 메타버스 기반의 실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 관리자 웹 서버는 메타버스 플랫폼 서버 상에 구축된 가상클래스를 강의실이 필요한 교육기관 또는 단체에임대한 후, 가상 클래스를 수행하는 수강생 현황 관리, 수강생 수동 입과 및 개별 입과 등의 출석 정보 제공 기능, 출석 인정에 대하여 기준값 부여 기능 및 정산 관리 기능을 수행하는 것을 특징으로 하는 메타버스 기반의실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 메타버스 플랫폼 서버는 교육 참가자의 플랫폼에 맞는 네트워크 환경을 구축하여 사용자 간 통신 네트워크를 구축하고, 가상공간 상에서교육 참가자의 위치, 각도, 가상공간 내 개체와 상호작용을 통신하여 동일한 경험을 공유하며, 기본 수업과 강사 요청에 따라 맞춤 제작된 수업으로 각 강의들은 DB에 보관하는 네트워크 관리부; XR 컨텐츠를 기반으로 제작되며, 강의실 및 강의자료를 사용자가 어플리케이션에서 요청하는 경우에 해당 컨텐츠만 다운받아 사용하는 방식으로 동작하고, 로그인/회원가입/회원정보 조회 및 수정을 포함하는 기본 기능과다운받은 강의자료를 사용/관리하는 기능을 구비하고, 다운받은 수강자료는 어플리케이션이 켜질 때 자료 만료기한을 검토하여 자동삭제하고, 수업 내용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분은기록을 남기는 교육 컨텐츠 제공부; 및 다수의 플랫폼을 동시에 지원 가능하도록 설계되고, 각 플랫폼에 맞는 네트워크 구성을 진행하여 플랫폼에 제한받지 않고 한 공간에 모여 있을 수 있도록 하고, 각 플랫폼은 실시간으로 자신의 위치, 사람/사물간 상호작용을공유하도록 하며, 실습 중에도 실습 지침, 유의사항, 권장 수치 정보를 명시하여 실습 내용 및 과정을 강사나다른 학생들과 공유하도록 하는 다중 플랫폼 실습 지원부를 포함하는 것을 특징으로 하는 메타버스 기반의 실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 교육 컨텐츠 제공부는같이 학습하는 구성원과 하나의 가상 공간에서 함께 학습을 진행하는 가상공간을 제작하고, 가상공간 상에서 교사, 칠판, 책상, 수강생들과 함께 공존하도록 하며, 수강생들과 조별 협업이 가능하도록 조별 학습공간을 구성하는 강의실 제작부;공개특허 10-2023-0127613-3-사용자 단말에 배포되는 전용 어플리케이션에서 요청하는 경우에 해당 컨텐츠만 다운받아 사용하는 방식으로 동작하는 강의실 배포부;교육 컨텐츠를 사용자 선호에 따라 모바일 기기에서 동작할 수 있도록 강의를 진행하는 과정 중에 자원 관리를수행하되, 모바일 기기의 사양을 맞추어 해당 컨텐츠를 제작하고 수업 중 각 단원들이 씬(Scene)처럼 동작하도록 편성하는 강의 편성부; 및수업 내용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분을 기록을 남겨 향후 실습 중 자주실수하는 내용/사례 또는 통계 자료로 사용하도록 기록하는 학습 행동 기록부를 포함하는 것을 특징으로 하는메타버스 기반의 실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 강의실 제작부는 가상공간에서 파일을 공유하도록 하고, 이때 관리자는 관리자(교수자)는 파일을 구성원동의없이 공유 가능하며, 구성원은 관리자의 권한 부여에 따라 공유 가능하게 하며, 교육목적과 형태에 따른 클래스 선택(이론클래스, 팀단위 프로젝트 클래스), 실습형 교육이 가능한 가상교육장으로 특화, 가상클래스에서활용가능한 온라인 교구재 구축, 온라인 협업 SW 연동 기능을 제공하고, 마이크 음소거, 화면 노출에 대한 제어기능과, 보팅 기능(공식, 비공식 투표 기능)과, 채팅창(채팅을 통하여 메시지를 전달할 수 있음) 기능을 제공하며, 관리자(교수자) 권한에서 수강생의 수업 화면을 실시간 모니터링 기능(단, 수강생이 권한을 설정한 경우에한정)과, 수강생이 강의 화면을 임의로 녹화하지 못하도록 방지하는 기능과, 관리자(강사)의 조건부 음성 전달기능(특정 수강생에게만 음성 전달 기능(실습 현황에 대한 개별 코칭 시 활용))과, 움직임 감지 기능(수강생의움직임을 감지하여, 수업의 이탈이 높은 경우 관리자(교수자)와 수강생에게 알림으로 안내)과, 수강생 코칭 및피드백 기능(수강생의 이해 불가한 페이지(교재, 자료 등)를 HMD 장비로 스캔 또는 사진 캡쳐하여 전송하고 강사가 해당 화면을 가상 공간에 띄워서 설명 진행하는 기능을 제공하는 것을 특징으로 하는 메타버스 기반의 실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항에 있어서,상기 다중 플랫폼 실습 지원부는 핸들이 없는 PC나 모바일의 경우도 마우스 클릭이나 화면터치로 상호작용을 지원하며 PC의 경우 가상 손을 제공하는 것을 특징으로 하는 메타버스 기반의 실습형 교육 시스템."}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 가상 클래스를 강의실이 필요한 교육기관 또는 단체에 임대하는 서비스를 제공하고, 가상 클래스 상에 서 일어나는 학습 행동을 수집 및 저장하며, 가상 클래스의 임대 신청이나 사용 정보, 가상 클래스를 이용한 사 용자의 정보를 기록 및 관리하는 관리자 웹 서버; 메타버스 기반의 가상공간을 구축하여 메타버스를 활용한 화상 (뒷면에 계속)"}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 메타버스 기반의 실습형 교육 시스템에 관한 것이다."}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "정보 통신 기술의 발달 및 컴퓨터, 스마트폰의 대중화에 따라, 종래의 대면 교육이 여러 가지 형태의 이러닝(e- Learning)으로 대체되고 있다. 초창기의 이러닝에서는 교과서와 같은 텍스트 기반 매체에 담긴 내용들을 단순히 디지털 콘텐츠로 옮기는 것과 같은 접근 방법이 많이 사용되었지만, 단순한 텍스트, 동영상 또는 플래시 기반의 일방형 교육 콘텐츠로 얻을 수 있는 교육 효과에는 여러 가지 한계점이 존재한다. 이러한 점을 극복하기 위해서, 최근 증강 현실(Augmented Reality) 또는, 혼합 현실(Mixed Reality)이라 불리 는 기술에 대한 관심이 높아지고 있다. 증강 현실이란, 콘텐츠 사용자가 실제 관찰하고 있는 사물이나 장소에 대한 부가적인 정보나 의미를 함께 제공 하는 기술을 의미하는데, 학습 콘텐츠에 적용할 경우, 학습 장면에 대한 맥락인식(context-awareness)을 높이고, 학습자의 실제감과 몰입감을 촉진함으로써 학습효과를 향상시킬 수 있을 것으로 기대되고 있다.이러한 증강 현실 콘텐츠의 사용자는 종래의 학습 콘텐츠에서처럼 마우스나 키보드 등과 같은 범용 입력 장치로 콘텐츠를 조작하는 것이 아니라, 실감형 인터페이스, 즉, 현실 세계의 객체에 대한 조작을 통해 디지털 콘텐츠 와 상호 작용하게 된다. 이를 학습 콘텐츠에 적용할 경우, 실감형 인터페이스를 통한 조작 활동이 학습자의 학습 경험을 증진시킬 수 있 으며, 실제 교육에서는 여러 가지 감각을 통하여 학습이 이루어지기 때문에, 이러닝에서도 이와 같은 체험 또는 학습을 할 수 있는 환경을 만들어 주는 것이 중요하다. 일 예를 들자면, 실감형 인터페이스로 특정 디스플레이 장치를 머리에 쓰거나(HMD, head mounted display) 또는 손에 들고(handheld display) 실제 객체를 볼 때, 실체 객체에 가상 콘텐츠를 혼합하여 보여주는 방법이나, 또 다른 예로, 카메라 등을 사용해 인식한 영상과 가상 콘텐츠를 혼합하여 일반 모니터나 스크린 위에 보여주는 방 법이 있다. 하지만, 이러한 기술들을 실제 교육에 적용하는 과정에 있어, 장비의 안정성과 실시간성, 고가 장비로 인한 제 한 등의 문제점이 있기 때문에, 아직까지 교육내용을 단순 시뮬레이션하는 것에 머물러 있는 현실이다. 또한, 두가지 방법 모두 주로 카메라를 통한 영상 인식을 통해 실감형 인터페이스를 구현함으로써, 조도와 같은 주변 환경에 민감하고 영상 인식의 불안정성이 있다는 문제점이 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-2018-0046313호(공개일자: 2018.05.08.) (특허문헌 0002) 등록특허공보 제10-0912369호(공고일자: 2009.08.19.)"}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "전술한 문제점을 개선하기 위한 본 발명 실시예들의 목적은 메타버스를 활용한 화상기능과 가상공간이 결합된 현장 중심의 비대면 교육 서비스를 가능하게 하는 메타버스 기반의 실습형 교육 시스템을 제공하는 것이다."}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위하여, 본 발명의 일 실시예에 의한 메타버스 기반의 실습형 교육 시스템은 가상 클래스를 강의실이 필요한 교육기관 또는 단체에 임대하는 서비스를 제공하고, 가상 클래스 상에서 일어나는 학 습 행동을 수집 및 저장하며, 가상 클래스의 임대 신청이나 사용 정보, 가상 클래스를 이용한 사용자의 정보를 기록 및 관리하는 관리자 웹 서버; 메타버스 기반의 가상공간을 구축하여 메타버스를 활용한 화상기능과 가상공 간이 결합하여 가상 클래스의 임대를 신청하는 사용자에게 비대면 교육 서비스를 제공하는 메타버스 플랫폼 서 버; 및 미리 설치되거나 메타버스 플랫폼 서버로부터 다운로드된 전용 어플리케이션을 통하여 관리자 웹 서버에 가상 클래스의 임대를 신청하고, 메타버스 플랫폼 서버에 접속하여 가상 클래스에 참여하는 사용자 단말을 포함 할 수 있다. 상기 관리자 웹 서버는 메타버스 플랫폼 서버 상에 구축된 가상클래스를 강의실이 필요한 교육기관 또는 단체에 임대한 후, 가상 클래스를 수행하는 수강생 현황 관리, 수강생 수동 입과 및 개별 입과 등의 출석 정보 제공 기 능, 출석 인정에 대하여 기준값 부여 기능 및 정산 관리 기능을 수행할 수 있다. 상기 메타버스 플랫폼 서버는 교육 참가자의 플랫폼에 맞는 네트워크 환경을 구축하여 사용자 간 통신 네트워크 를 구축하고, 가상공간 상에서 교육 참가자의 위치, 각도, 가상공간 내 개체와 상호작용을 통신하여 동일한 경 험을 공유하며, 기본 수업과 강사 요청에 따라 맞춤 제작된 수업으로 각 강의들은 DB에 보관하는 네트워크 관리 부; XR 컨텐츠를 기반으로 제작되며, 강의실 및 강의자료를 사용자가 어플리케이션에서 요청하는 경우에 해당컨텐츠만 다운받아 사용하는 방식으로 동작하고, 로그인/회원가입/회원정보 조회 및 수정을 포함하는 기본 기능 과 다운받은 강의자료를 사용/관리하는 기능을 구비하고, 다운받은 수강자료는 어플리케이션이 켜질 때 자료 만 료기한을 검토하여 자동삭제하고, 수업 내용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분은 기록을 남기는 교육 컨텐츠 제공부; 및 다수의 플랫폼을 동시에 지원 가능하도록 설계되고, 각 플랫폼에 맞는 네트워크 구성을 진행하여 플랫폼에 제한받지 않고 한 공간에 모여 있을 수 있도록 하고, 각 플랫폼은 실시간으 로 자신의 위치, 사람/사물간 상호작용을 공유하도록 하며, 실습 중에도 실습 지침, 유의사항, 권장 수치 정보 를 명시하여 실습 내용 및 과정을 강사나 다른 학생들과 공유하도록 하는 다중 플랫폼 실습 지원부를 포함할 수 있다. 상기 교육 컨텐츠 제공부는 같이 학습하는 구성원과 하나의 가상 공간에서 함께 학습을 진행하는 가상공간을 제 작하고, 가상공간 상에서 교사, 칠판, 책상, 수강생들과 함께 공존하도록 하며, 수강생들과 조별 협업이 가능하 도록 조별 학습공간을 구성하는 강의실 제작부; 사용자 단말에 배포되는 전용 어플리케이션에서 요청하는 경우 에 해당 컨텐츠만 다운받아 사용하는 방식으로 동작하는 강의실 배포부; 교육 컨텐츠를 사용자 선호에 따라 모 바일 기기에서 동작할 수 있도록 강의를 진행하는 과정 중에 자원 관리를 수행하되, 모바일 기기의 사양을 맞추 어 해당 컨텐츠를 제작하고 수업 중 각 단원들이 씬(Scene)처럼 동작하도록 편성하는 강의 편성부; 및 수업 내 용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분을 기록을 남겨 향후 실습 중 자주 실수하는 내용/사례 또는 통계 자료로 사용하도록 기록하는 학습 행동 기록부를 포함할 수 있다. 상기 강의실 제작부는 가상공간에서 파일을 공유하도록 하고, 이때 관리자는 관리자(교수자)는 파일을 구성원 동의없이 공유 가능하며, 구성원은 관리자의 권한 부여에 따라 공유 가능하게 하며, 교육목적과 형태에 따른 클 래스 선택(이론클래스, 팀단위 프로젝트 클래스), 실습형 교육이 가능한 가상교육장으로 특화, 가상클래스에서 활용가능한 온라인 교구재 구축, 온라인 협업 SW 연동 기능을 제공하고, 마이크 음소거, 화면 노출에 대한 제어 기능과, 보팅 기능(공식, 비공식 투표 기능)과, 채팅창(채팅을 통하여 메시지를 전달할 수 있음) 기능을 제공하 며, 관리자(교수자) 권한에서 수강생의 수업 화면을 실시간 모니터링 기능(단, 수강생이 권한을 설정한 경우에 한정)과, 수강생이 강의 화면을 임의로 녹화하지 못하도록 방지하는 기능과, 관리자(강사)의 조건부 음성 전달 기능(특정 수강생에게만 음성 전달 기능(실습 현황에 대한 개별 코칭 시 활용))과, 움직임 감지 기능(수강생의 움직임을 감지하여, 수업의 이탈이 높은 경우 관리자(교수자)와 수강생에게 알림으로 안내)과, 수강생 코칭 및 피드백 기능(수강생의 이해 불가한 페이지(교재, 자료 등)를 HMD 장비로 스캔 또는 사진 캡쳐하여 전송하고 강 사가 해당 화면을 가상 공간에 띄워서 설명 진행하는 기능을 제공할 수 있다. 상기 다중 플랫폼 실습 지원부는 핸들이 없는 PC나 모바일의 경우도 마우스 클릭이나 화면터치로 상호작용을 지 원하며 PC의 경우 가상 손을 제공할 수 있다."}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 메타버스 기반의 실습형 교육 시스템은 메타버스를 활용한 화상기능과 가상공간이 결합된 현장 중심의 비대면 교육 서비스로 같이 학습하는 구성원과 각자 독립된 개인 공간에 있지만 마치 같이 있는 것처럼 느낄 수 있는 하나의 가상 공간에서 학습을 진행할 수 있다. 또한, 본 발명의 일 실시예는 코로나 19와 같은 상황으로 대면 교육이 불가한 경우 가상 클래스를 활용함으로써 수강생들에게 몰입감이 높은 교육환경을 제공할 수 있다. 또한, 본 발명의 일 실시예는 본사와 지점의 거리가 먼 기업체로 현장 교육이 어려운 경우 가상 클래스를 활용 함으로써 교육효과를 극대화시킴과 동시에 비용 절감이 가능하고, 수도권 중심으로 형성된 특화된 교육 참여를 위하여 먼 거리를 감수해야 하는 수강생에게 교육 참여 기회를 제공할 수 있다."}
{"patent_id": "10-2022-0025103", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "상기한 바와 같은 본 발명을 첨부된 도면들과 실시예들을 통해 상세히 설명하도록 한다. 본 발명에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려 는 의도가 아님을 유의해야 한다. 또한, 본 발명에서 사용되는 기술적 용어는 본 발명에서 특별히 다른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 의미로 해석되어야 하며, 과도하게 포괄적인 의미로 해석되거나, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에 는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용 되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함한다. 본 발명에서, \"구성된다\" 또는 \"포함한다\" 등의 용어는 발명에 기재된 여러 구성 요소들, 또는 여러 단계를 반 드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 발명에서 사용되는 제 1, 제 2 등과 같이 서수를 포함하는 용어는 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요소들은 용어들에 의해 한정되어서는 안 된다. 용어들은 하나의 구성 요소를 다른 구성 요소 로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성 요소는 제 2 구성 요소로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 바람직한 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동 일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 발명의 사상을 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아니 됨을 유의해야 한다. 도 1 내지 도 2는 본 발명의 일 실시예에 따른 메타버스 기반의 실습형 교육 시스템을 개략적으로 나타내는 블 록도이고, 도 3은 도 1 내지 도 2의 메타버스 플랫폼 서버의 구성을 개략적으로 나타내는 블럭도이다. 도 1 내지 도 2를 참조하면, 본 발명의 실시예에 따른 메타버스 기반의 실습형 교육시스템은 메타버스를 활용한 화상기능(ex. zoom)과 가상공간이 결합된 현장 중심의 비대면 교육 서비스로 같이 학습하는 구성원과 각자 독립 된 개인 공간에 있지만 마치 같이 있는 것처럼 느낄 수 있는 하나의 가상 공간에서 학습을 진행할 수 있는 시스 템이고, 기본적으로, 관리자 웹 서버, 메타버스 플랫폼 서버 및 사용자 단말을 포함할 수 있다. 관리자 웹 서버는 메타버스 플랫폼 서버 상에 구축된 가상 클래스를 강의실이 필요한 교육기관 또는 단 체, 즉 사용자 단말에 임대하는 서비스를 제공하는 업체의 서버 장치로, 가상 클래스 상에서 일어나는 학습 행동(성공 및 실패시 행동, 시간측정 등)을 수집 및 저장하고, 사용자 단말과 연계되어 가상클래스 임대 신 청이나 해당 서비스 사용, 사용자의 정보 조회 및 관리가 가능하도록 설계되어 있다. 관리자 웹 서버는 성인 중심의 국내 직업훈련기관, 민간교육학원 및 대학교를 대상으로 교육 목적 및 교육 방식에 맞는 가상 교육장을 직접 선택하여 교육을 운영할 수 있도록 서비스를 제공할 수 있고, 전용 어플리케이 션 상에서 간단한 수강 관리를 진행할 수 있으며, 웹으로 접속하여 LMS(Learning Management System) 방식의 자 세한 수업 관리도 가능하도록 설계되어 있다. 이때, 관리자 웹 서버는 가상 클래스를 임대함에 있어서, 사용자(또는 사용처)가 보유중인 장비에 따라 보 급형과 고급형으로 가상 공간을 선택 가능하도록 해당 정보를 할 수 있다. 이를 통하여, 관리자 웹 서버는 가상현실(VR) 기반의 강의장 임대서비스 제공을 통한 수익창출, 즉 강의장 상품 수익과 가상클래스 내 유료상품 수익 창출이 가능하게 할 수 있다. 이를 위하여, 관리자 웹 서버는 공간형태, 정원, 이용기간 3개 기준요소 로 이용료를 부과하는 형태로도 구현가능하다. 또한, 관리자 웹 서버는 메타버스 플랫폼 서버 상에 구축된 가상클래스를 강의실이 필요한 교육기관 또 는 단체에 임대한 후, 가상 클래스를 수행하는 수강생 현황 관리(수강현황, 접속시간 등), 수강생 수동 입과 및 개별 입과 등의 출석 정보 제공 기능, 출석 인정에 대하여 기준값 부여 기능(부여된 강의실만 적용됨), 정산 관리 기능(결제 상품, 결제 금액, 결제 방식 등)을 수행할 수 있다. 이때, 관리자 웹 서버 상에 수집되거나 저장된 정보는 메타버스 플랫폼 서버 상에도 저장할 수도 있으 며, 이때 각각의 데이터들은 제3자가 도용하지 못하도록 또는 권한이 없는 자가 확인할 수 없도록 암호화하여 안전하게 관리되는 것이 바람직하다. 한편, 본 명세서 내의 가상 클래스는 가상현실(VR) 기반의 교육 공간을 의미한다. 예를 들어, 가상 클래스는 성 인 중심의 국내 직업훈련기관, 민간교육학원 및 대학교를 대상으로 하는 교육 목적 및 교육 방식에 맞는 가상 교육장일 수 있다. 이러한 가상현실 기반의 교육 공간 제공 서비스는 증강현실 또는 혼합현실을 사용자에게 제공하는 일종의 메타 버스(metabus) 서비스로 볼 수 있다. 메타버스는 가공, 추상을 의미하는 '메타(Meta)'와 현실세계를 의미하는 '유니버스(Universe)'의 합성어로 3차원 가상 세계를 의미한다. 메타버스는 기존의 가상현실 환경(Virtual reality environment)이라는 용어보다 진보된 개념으로서, 웹과 인터넷 등의 가상세계가 현실세계에 흡수된 증 강 현실 환경을 제공할 수 있다. 또한, 본 발명의 실시예에 따른 인공지능 기반 가상현실 서비스 시스템은 인터넷 클라우드 기술에 기반하여 관 리자 웹 서버 상에서 데이터를 관리, 운용하며, 이에 기초하여 메타버스 플랫폼 서버 상에 구축된 가상 클래스에 참석한 강사나 수강생들 간의 만남 및 대화가 가능하도록 한다. 특히, 가상 클래스에 참석한 강사나 수강생들 간의 만남에서는 실시간 물리적 접촉 피드백이 가능한 상호작용 그래픽 엔진 연동기술을 이용할 수 있 다. 그리고, 서비스 운영자는 본 발명의 서비스를 통해 개인이 메타버스 플랫폼 서버 상에 구축된 가상 클래스 의 데이터에 대한 관리, 운영과, 가상 클래스에 의한 콘텐츠 제공을 통합 관리함으로써, 사용자에게 공개, 비공 개 등 서비스 계약 요건에 따라 정보를 제공할 수 있도록 한다. 메타버스 플랫폼 서버는 메타버스 기반의 가상공간(학습 공간 및 실습 공간)를 구축하여 메타버스를 활용한 화상기능(ex. zoom)과 가상공간이 결합된 현장 중심의 비대면 교육 서비스를 제공하는 서버 장치로서, 메타버스 환경에서 같이 학습하는 구성원과 각자 독립된 개인 공간에 있지만 마치 같이 있는 것처럼 느낄 수 있는 하나의 가상 공간에서 학습을 진행할 수 있는 서비스를 제공할 수 있다. 이러한 메타버스 플랫폼 서버는, 사용자 단말과 연결되어 메타버스 서비스를 제공하기 위한 구성으로, 하드웨어적으로 통상적인 웹 서버와 동일한 구성을 가지며, 소프트웨어적으로는 C, C++, Java, Visual Basic, Visual C 등과 같은 다양한 형태의 언어를 통해 구현되어 여러 가지 기능을 하는 프로그램 모듈을 포함할 수 있 다. 또한, 일반적인 서버용 하드웨어에 도스(dos), 윈도우(window), 리눅스(linux), 유닉스(unix), 매킨토시 (macintosh), 안드로이드(Android), 아이오에서(iOS) 등의 운영 체제에 따라 다양하게 제공되고 있는 웹 서버 프로그램을 이용하여 구현될 수 있다. 본 실시예에 따른 사용자 단말 및 메타버스 플랫폼 서버 간을 연결하는 인터넷 네트워크의 무선 통신망 의 일 예로는, 이동통신을 위한 기술표준들 또는 통신방식(예를 들어, GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EV- DO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G 등)에 따라 구축된 이동 통신망을 포함할 수 있으나, 특별히 한정하는 것은 아니 다. 또한, 유선 통신망의 일 예로는, LAN(Local Area Network), WAN(Wide Area Network)등의 폐쇄형 네트워크 일 수 있으며, 인터넷과 같은 개방형 네트워크인 것이 바람직하다. 인터넷은 TCP/IP 프로토콜 및 그 상위계층에 존재하는 여러 서비스, 즉 HTTP(HyperText Transfer Protocol), Telnet, FTP(File Transfer Protocol), DNS(Domain Name System), SMTP(Simple Mail Transfer Protocol), SNMP(Simple Network Management Protocol), NFS(Network File Service), NIS(Network Information Service)를 제공하는 전세계적인 개방형 컴 퓨터 네트워크 구조를 의미한다. 이러한 메타버스 플랫폼 서버는 가상공간 상에서의 강의 참여가 가능한 전용 어플리케이션을 구비하여 해당 메타버스 플랫폼 서버에 진입한 사용자간 위치 정보를 공유하고, 사용자의 행동을 동기화시킨다. 사용자간 위치 정보는 가상공간 상에서의 강의 참여가 가능한 전용 어플리케이션을 구비하여 해당 메타버스 플랫폼 서버 에 등록하여 접속한 사용자들에게만 노출되며, 해당 사용자와 관련이 없는 자에게는 노출되지 않는다. 다만, 가상공간에서의 강의 또는 실습 기간이 지나면, 자동으로 삭제되거나, 사용자의 요청에 따라 임의로 삭제되지 않고 SNS(Social Network Service) 상에 보존될 수 있다. 이를 위하여, 메타버스 플랫폼 서버는 도 3에 도시된 바와 같이 네트워크 관리부, 교육 컨텐츠 제공부 및 다중 플랫폼 실습 지원부를 포함한다. 네트워크 관리부는 교육 참가자(즉, 교육기관, 단체 또는 개인 등)가 참여하는 가상 클랙스가 구축된 플랫 폼에 맞는 네트워크 환경을 구축하여 사용자 간 통신 네트워크를 구축하고, 가상공간 상에서 교육 참가자의 위 치, 각도, 가상공간 내 개체와 상호작용 등을 통신하여 동일한 경험을 공유하며, 기본 수업과 강사 요청에 따라 맞춤 제작된 수업으로 각 강의들은 DB(미도시)에 보관하는 역할을 수행한다. 네트워크 관리부는 통합 게임 엔진에서 출시를 지원하는 플랫폼 중 예상사용자가 많이 이용할 것으로 예상 되는 플랫폼(예를 들어, Windows PC, Android, iOS, Oculus, HoloLens)에 각 플랫폼에 맞는 네트워크 환경을 구축하여 다른 사용자와 통신하도록 할 수 있고, 사용자들 중에 MR(Mixed Reality) 사용자들은 머리와 손의 움 직임을 공유하게 하고, PC 및 모바일 AR(Augmented Reality) 이용자들은 머리 움직임을 대체(손은 표현하지 않 음)하도록 할 수 있다. 또한, 네트워크 관리부는 가상공간 상에 사용자가 있는 위치, 각도, 가상공간 내 개체와 상호작용 등을 통 신하여 동일한 경험을 공유하도록 하고, 수업은 기본 이론을 토대로 제작되어 상시 제공되는 기본 수업과 강사 요청에 따라 맞춤 제작된 수업으로 각 강의들은 DB에 보관되도록 할 수 있다. 교육 컨텐츠 제공부는 XR 컨텐츠를 기반으로 제작되며, 강의실 및 강의자료를 사용자가 어플리케이션에서 요청하는 경우에 해당 컨텐츠만 다운받아 사용하는 방식으로 동작한다. 이를 위하여, 교육 컨텐츠 제공부 는 로그인/회원가입/회원정보 조회 및 수정 등의 기본 기능과 다운받은 강의자료를 사용/관리하는 기능을 구비 한다. 또한, 교육 컨텐츠 제공부는 다운받은 수강자료는 어플리케이션이 켜질 때 자료 만료기한을 검토하 여 자동삭제하고, 수업 내용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분은 메타버스 플랫 폼 서버에 기록을 남기는 역할을 수행한다. 한편, 교육 컨텐츠 제공부에서 제공되는 강의 내용은 XR컨텐츠, 즉 VR(Virtual Reality, 가상현실), AR(Augmented Reality, 증강현실), MR(Mixed Reality, 혼합현실)과 미래에 등장할 신기술까지 포괄하는 확장현 실(eXtended Reality)로 제작된다. 또한, 교육 컨텐츠 제공부는 강의실 제작부, 강의실 배포부, 강의 편성부 및 학습 행동 기 록부를 포함한다. 강의실 제작부는 같이 학습하는 구성원과 하나의 가상 공간에서 함께 학습을 진행하는 가상공간을 제작하 고, 가상공간 상에서 교사, 칠판, 책상, 수강생들과 함께 공존하도록 한다. 또한, 강의실 제작부는 수강생들과 조별 협업이 가능하도록 조별 학습공간을 구성할 수 있다. 이때, 관리 자(교수자)가 조별 소규모 공간으로 별도 분리가 가능하고, 관리자는 조별 실습이 종료된 경우 메인 공간으로 가상 클래스의 수강생들을 모두 불러올 수 있다. 강의실 제작부는 가상공간에서 파일(교안, 화이트보드, 공유자 컴퓨터 화면, 기타 파일 등)을 공유하도록 하고, 이때 관리자는 관리자(교수자)는 파일을 구성원 동의없이 공유 가능하며, 구성원은 관리자의 권한 부여에 따라 공유 가능하게 할 수 있다. 또한, 강의실 제작부는 다양한 교육 구성에 맞는 가상 공간을 제작하여, 교육목적과 형태에 따른 클래스 선택(이론클래스, 팀단위 프로젝트 클래스), 실습형 교육이 가능한 가상교육장으로 특화, 가상클래스에서 활용 가능한 온라인 교구재 구축, 온라인 협업 SW 연동(GitHub, Google Docs, 비캔버스 등) 기능을 제공할 수 있다. 또한, 강의실 제작부는 마이크 음소거, 화면 노출에 대한 제어 기능과, 보팅 기능(공식, 비공식 투표 기능)과, 채팅창(채팅을 통하여 메시지를 전달할 수 있음) 기능을 제공할 수 있다. 이때, 화이트 보드나 교안에 관리자, 수강생 모두 필기 가능하도록 구현하고, 관리자(교수자) 권한에서 수강생의 수업 화면을 실시간 모니터 링 기능(단, 수강생이 권한을 설정한 경우에 한정)과, 수강생이 강의 화면을 임의로 녹화 하지 못하도록 방지하 는 기능을 제공할 수 있다. 나아가, 관리자(강사)의 조건부 음성 전달 기능(특정 수강생에게만 음성 전달 기능 (실습 현황에 대한 개별 코칭 시 활용)), 움직임 감지 기능(수강생의 움직임을 감지하여, 수업의 이탈이 높은 경우 관리자(교수자)와 수강생에게 알림으로 안내), 수강생 코칭 및 피드백 기능(수강생의 이해 불가한 페이지 (교재, 자료 등)를 HMD 장비로 스캔 또는 사진 캡쳐하여 전송하고 강사가 해당 화면을 가상 공간에 띄워서 설명진행하는 기능을 제공할 수 있다. 강의실 배포부는 사용자 단말에 배포되는 전용 어플리케이션의 용량을 효율적으로 줄이기 위해 각각의 강의실 및 강의자료를 사용자가 전용 어플리케이션에서 요청하는 경우에 해당 컨텐츠만 다운받아 사용하는 방식 으로 동작하도록 한다. 이때, 사용자 단말에 배포되는 전용 어플리케이션에는 로그인/회원가입/회원정보 조 회 및 수정 등의 기본 기능과 다운받은 강의자료를 사용/관리하는 기능만을 가지고 있다. 또한, 사용자가 수강 하는 강의가 많아질수록 어플리케이션의 크기가 비례하게 커지는 것을 방지하기 위해, 다운받은 수강자료는 어 플리케이션이 켜질 때 자료 만료기한(수업이 제공되는 마지막 날)을 검토하여 자동으로 삭제하도록 할 수 있다. 강의 편성부는 교육 컨텐츠를 사용자 선호에 따라 모바일 기기(휴대폰, Oculus, HoloLens 등)에서 동작할 수 있도록 강의를 진행하는 과정 중에 자원 관리를 수행하게 된다. 이를 위하여, 강의 편성부는 전반적으 로 모바일 기기에 대한 최적화 사양을 맞추어 해당 컨텐츠를 제작하고 수업 중 각 단원들이 마치 씬(Scene)처럼 동작하도록 설계한다. 예를 들어, 강의 편성부는 1단원이 끝나고 2단원으로 넘어갈 때 1단원 정보 중 2단 원에서 이어서 사용할 정보를 제외한 나머지 전부 지우는 식으로 진행하고, 각 단원이 담고 있는 내용이 많을 경우 동일한 방식으로 하위 챕터들이 동작하도록 컨텐츠를 설계할 수 있다. 학습 행동 기록부는 수업 내용 중 평가가 이루어지는 부분이나 기록을 누적할 필요가 있는 부분을 기록을 남겨 향후 실습 중 자주 실수하는 내용/사례 또는 통계 자료로 사용하도록 할 수 있다. 이때, 이 기능은 교육 컨텐츠를 구성할 때 저장할 내용과 자료를 형식에 맞추어 전달해주도록 구성된다. 다중 플랫폼 실습 지원부는 핸들이 없는 PC나 모바일의 경우도 마우스 클릭이나 화면터치로 상호작용을 지 원하며 PC의 경우 가상 손을 제공하여 만약 핸들이 있을 때 동작하게 된다면 어떻게 될 지를 최대한 가늠할 수 있도록 하는 역할을 수행한다. 다중 플랫폼 실습 지원부는 XR에 대한 관심에 부응하면서도 일반 사용자층의 편안한 접근을 위해 주로 사 용되는 플랫폼(예를 들어, 윈도우 PC, 모바일 등)을 동시에 지원 가능하도록 설계된다. 이를 통하여, 다중 플랫 폼 실습 지원부는 각 플랫폼에 맞는 네트워크 구성을 진행하여 플랫폼에 제한받지 않고 한 공간에 모여 있 을 수 있도록 하고, 각 플랫폼은 실시간으로 자신의 위치, 사람/사물간 상호작용을 공유할 수 있도록 한다. 또 한, 다중 플랫폼 실습 지원부는 실습 중에도 실습 지침, 유의사항, 권장 수치 등을 명시하여 실습 내용의 이해도를 높이고 이 과정을 강사나 다른 학생들과 공유할 수 있도록 한다. 이때, 학습자는 XR 컨텐츠의 특성상 소모품으로 구성된 실습도 원하는 만큼 반복할 수 있게 된다. 또한, 다중 플랫폼 실습 지원부는 핸들이 없 는 PC나 모바일의 경우도 마우스 클릭이나 화면터치로 상호작용을 지원하며 PC의 경우 가상 손을 제공하여 만약 핸들이 있을 때 동작하게 된다면 어떻게 될 지를 최대한 가늠할 수 있도록 구성될 수 있다. 사용자 단말은 강의실이 필요한 교육기관, 단체 또는 개인들이 메타버스 기반의 가상클래스 공간을 임대하 여 학습 또는 실습하는 사용자(교육기관, 단체 또는 개인 등)가 소지한 통신단말로, 전용 어플리케이션의 설치 및 실행이 가능한 스마트 폰(smart phone), 태블릿 PC, 노트북 컴퓨터(laptop computer), 전자북 단말기, 디지 털방송용 단말기, PDA(Personal Digital Assistants), PMP(Portable Multimedia Player), MP3 플레이어, 디지 털 카메라, 네비게이션(Navigation) 디바이스 일 수 있으나, 이에 한정되지 않는다. 사용자 단말은 본 메타버스 기반의 실습형 교육 서비스를 제공받기 위하여, 미리 설치되거나 메타버스 플랫 폼 서버로부터 다운로드된 전용 어플리케이션을 통하여 메타버스 플랫폼 서버에 접속한다. 전용 어플리 케이션은, 사용자 단말에 설치되고, 메타버스 플랫폼 서버와 연결되고, 메타버스 플랫폼 서버 상에 구축된 가상 클래스에 참여하여 메타버스 기반의 실습형 교육 서비스를 제공받을 수 있다. 이러한 전용 어플리케이션은 메타버스 플랫폼 서버 상에서 아래와 같은 XRTouch의 기술이 구현될 수 있도록 설계되어 있고, 이하에서는 기능별로 보다 상세하게 설명하기로 한다. 이하에서 설명되는 기술은 메타버스 기반 의 실습형 교육서비스를 제공하는 업체나 관리자, 그리고 메타버스 기반의 실습형 교육서비스를 제공받는 사용 자가 이용할 수 있다. XRTouch 인터렉션 기술 XRTouch의 기술은 사용자의 실제 환경에서 가상 콘텐츠를 원활하게 운영하기 위해 사실적인 혼합현실(Mixed Reality), 상호 작용 및 음성 경험을 구축하고, 광범위한 머신 지각 및 AI 기능인 Presence Platform을 이용하 여 \"Spatial Anchors Experimental, Voice SDK, Passthrough\"의 인터랙션 기술(Interaction Technology)을 확 장현실 환경에 구현한다.인터랙션 기술은 안정적인 모듈식이며 컴포지트 가능한 기능을 통해 손 및 컨트롤러 상호 작용을 어플리케이션 에 쉽게 빌드하고 통합할 수 있게 한다. 이는 기존의 상호 작용 프레임 워크보다 더 유연하며 필요한 모듈만 사 용하여 기존 아키텍처에 통합할 수 있다 인터랙션 기술의 기능의 종류 Grab, Resize & Throw 크기 조정, 던지기, 손으로 전달, 심지어 소환 및 멀리서 잡을 수 있는 가상 객체와 직접 상호 작용한다. Grab 의 기능은 현실세계의 좌표계(World Coordinate)에서와 같이 물체를 잡거나 놓을 수 있도록 한다. Hand Grab 손은 가상 객체를 자연스럽게 잡을 수 있게 하여 시간과 비용(노동의 시간)을 줄일 수 있게 한다. Pose Detection 손가락의 구부림 및 굴곡과 같은 손가락 정보당 사용자 지정 제스처를 쉽게 구축하고 여러 제스처를 구현할 수 있다. Direct Touch 가상 표면을 구성하여 버튼을 구축할 수 있으며, 버튼 터치의 역할은 오류(false) 및 잘못 누르는 행동을 자동 으로 인지하고, 버튼은 가상 손이 버튼을 통과하지 못하게 하는 \"touch limiting\"을 통해 햅틱과 물리적 성질을 시뮬레이션한다. 스크롤 제스처를 사용하면 평면 및 곡면에 포함되는 있는 보다 정교한 2D UI를 쉽게 만들 수 있다. Targeting and Selection 곡면을 포함하여 시스템 UI에서 타겟팅 및 선택 역할을 구현한다. 추적 키보드(Tracked Keyboard) 추적 키보드(Tracked Keyboard SDK)를 이용하여 실제 키보드를 가상현실 환경으로 가져와 원활한 타이핑 기능을 제공한다. 컴퓨터 비전 추적을 사용하여 키보드를 찾아 렌더링하고, 블루투스는 키보드 출력(선택 사항)을 수신 하고, 핸드 트래킹과 패스스루를 조합하여 키보드 위에 손을 렌더링한다. 추적 키보드를 사용하여 Unity 및 네이티브 앱에서 다양한 키보드를 지원할 수 있다. Oculus Multiplayer Development Quick Start Guide 멀티플레이어 환경에서 새로운 기능을 테스트하거나 버그를 수정하는 동안 여러 장치에 대한 변경 사항을 푸시 하는 프로세스를 지원하며, 여러 헤드셋에서 빌드를 빠르고 쉽게 시작하는 데 필요한 계정 및 도구를 설정하여 Unity에서 여러 장치를 연결하는 단계와 링크를 지원한다. 이를 위하여, 하드웨어는 Quest Multiplayer(2개 또는 그 이상의 퀘스트 헤드셋), Link Multiplayer(2개 또는 그 이상의 퀘스트 또는 리프트 헤드셋(Can be a mix of the two), A PC for each headset with the Oculus app installed)가 요구될 수 있다. 멀티플레이의 경우 각 헤드셋이 고유한 사용자로 로그인해야 하므로 여러 계정이 있어야 한다. 사용자 계정은 개인 계정에 연결되어 있으므로 단일 개발자가 여러 개인 계정을 만들 수 없다. ADB 여러 안드로이드(Android) 장치가 연결되어 있을 때 ADB가 대상을 지정할 장치를 요청할 수 있다. 즉, ADB 장치 를 실행하고 \"adb-s\"로 전달할 일련 번호를 복사하는 것을 의미한다. 특히, UE4에서 생성된 일괄 처리 파일을 사용하여 빌드를 설치하는 경우, 단일 장치와 마찬가지로 ADB가 작동하도록 하려면 한 장치를 제외한 모든 장치 를 분리하거나 기본 장치의 일련 번호로 ANDROID_SERIAL 위한 환경 변수를 설정할 수 있다. 또한, XRTouch를 위한 오큘러스 통합 SDK는 유니티에서 오큘러스 앱을 개발할 수 있는 지원을 제공한다. SDK v35는 OVRPlugin 1.69, 오디오 공간화 32.0, 플랫폼 통합 37.0, 아바타 통합 20.0, 립싱크 통합 29.0, 음성 SDK 37.0과 함께 제공된다. 상호 작용 SDK는 컨트롤러와 핸즈를 위해 강력하고 표준화된 상호 작용(그랩, 찌르기, 레이캐스트 등)을 구현할 수 있는 모듈식 컴포넌트 라이브러리이다. 또한, 자신의 손 포즈를 구축하는 데 도움이 되는 툴링이 포함되어 있다. 물리적 추적 키보드 SDK는 사용자가 VR 환경 내부에 있는 동안 실제 키보드와 상호 작용하는 효율적인 방법을 제공한다. SDK는 사용자의 키보드 표현 위에 사용자의 손을 렌더링하여 가상 키보드와 블라인드 터치 타이핑의 한계를 극복할 수 있다. 오큘러스 링크를 통해 통과하면 Oculus 링크(Oculus Air Link 포함)를 사용하는 동안 통과 가능한 응용 프로그 램을 실행할 수 있다. 런타임 컨트롤러 모델은 OpenXR 백엔드로 빌드된 앱에 사용하는 컨트롤러와 일치하도록 Oculus 소프트웨어에서 컨트롤러 모델을 동적으로 로드한다. OVRInput.GetLocalControllerStatesStates예측()은 지정된 컨트롤러의 현재 위치, 회전 및 속도를 예측없이 추 적 공간으로 검색한다. OpenXR 포즈방법에서 Pinch(엄지와 검지로 잡는 것)는 시스템 제스처를 사용할 때 검지 손가락과 엄지 손가락 사이의 인치 간격의 약 1/8로 설정한다. OpenXR에서는 Rift에 대한 혼합 현실 캡처가 지원되지 않는 경우, 유니티 에디터에서 OpenXR - 레거시 OVRPlugin 메뉴로 전환하여 VRAPI 백엔드로 전환하여 리프트 MRC를 계속 사용할 수 있는 오큘러스, 도구로 이동 한다. GetAppPerfStats 및 리셋앱퍼프스타트는 현재 OpenXR 백엔드에서 지원되지 않는 경우, 유니티 에디터에서 OpenXR - Oculus - 도구로 이동하여 레거시 OVRPlugin 메뉴로 전환하여 VRAPI 백엔드로 전환하여 리프트 MRC를 계속 사용한다. Unity에서 응용 프로그램 공간 왜곡을 사용하면 빠른 이동 또는 순간 이동이 일부 아티팩트가 있을 수 있는 데, 이때 XR 플러그인에 새 앱 공간 포즈를 보내기 전에 모든 이동 논리가 실행되었음을 guarentee에 SetAppSpacePosition / SetAppSpace회전을 늦게 업데이트로 이동한다. Unity 전체 오큘러스 핸드 트래킹 지원이 OpenXR 백엔드로 복원되는 경우, 이것은 오픈 XR 기반의 OVRPlugin 기능적으 로 레거시 OVRPlugin에 제공된 핸드 트래킹에 맞춰 다시 제공한다. 이제 손 골격, 캡슐, 메쉬, 축척, 핀치, 포 인터 포즈, 입력 메타데이터 플래그 및 활성 컨트롤러를 포함한 모든 손 추적 속성이 지원된다. 레벨 변경 간에 레이어를 삭제하는 데 필요한 작업은 이제 조기 삭제가 GPU 충돌을 일으키지 않도록 큐를 사용 하여 수행된다. 이제 모바일 멀티뷰를 사용할 때 왼쪽 눈과 오른쪽 눈 모두 동일한 사전 노출 값을 사용한다. 또한, 비대칭 UV 사각형을 처리하는 방식이 변경된 후 렌더링속도가 약간 빨라지게 된다. 스페이셜 앵커(Spatial Anchors) 애저 스페이셜 앵커(Azure Spatial Anchors)를 사용하면 다양한 디바이스 간에 전 세계 앵커를 공유할 수 있다. 스페이셜 앵커는 공간 인식 혼합 현실 어플리케이션을 빌드하는 데 꼭 필요한 기능을 제공한다. 이러한 어플리 케이션은 Microsoft HoloLens, ARKit 지원 iOS 기반 디바이스, ARCore 지원 Android 기반 디바이스를 지원할 수 있다. 개발자는 Spatial Anchors를 통해 혼합 현실 플랫폼을 사용하여 공간을 인지하고, 정확한 관심 지점을 지정하며, 지원되는 디바이스에서 해당 관심 지점을 회수할 수 있다. 이러한 정확한 관심 지점을 스페이셜 앵커 (Spatial Anchors)라고 한다. 스페이셜 앵커를 통해 동일한 장소에 있는 사람은 다중 사용자 혼합 현실 어플리케이션에 쉽게 참여할 수 있다. 예를 들어, 두 명의 사람이 가상 체스 보드를 테이블에 배치하여 혼합 현실 체스 게임을 시작할 수 있다. 그런 다음, 테이블에서 디바이스를 가리켜서 가상 체스 보드를 보고 함께 상호 작용할 수 있다. 관리자 또는 개발자는 스페이셜 앵커를 함께 연결하여 이들 간의 관계를 만들 수도 있다. 예를 들어, 어플리케 이션에는 사용자가 작업을 완료하기 위해 반드시 상호 작용해야 하는 두 개 이상의 관심 지점이 있는 환경이 포 함될 수 있다. 이러한 관심 지점은 연결된 방식으로 만들 수 있다. 나중에 사용자가 다단계 작업을 완료하면 어 플리케이션은 현재 주변의 앵커를 요청하여 사용자를 작업의 다음 단계로 리디렉션할 수 있다. 또한, 사용자는 어플리케이션을 통해 가상 달력을 회의실 벽에 배치할 수 있으며, 사람들은 스마트폰 어플리케 이션 또는 HoloLens 디바이스를 사용하여 이를 볼 수 있다. 산업용 설정에서는 사용자가 지원되는 디바이스 카 메라로 대상을 가리켜서 머신에 대한 컨텍스트 정보를 받을 수 있다. 이러한 스페이셜 앵커는 지원되는 디바이스 플랫폼에 대한 관리 서비스 및 클라이언트 SDK로 구성된다. 보다 구체적으로, 지정된 애저 스페이셜 앵커(Azure Spatial Anchors) 계정에 액세스하려면 클라이언트는 먼저 Azure Mixed Reality STS(보안 토큰 서비스)에서 액세스 토큰을 가져와야 한다. STS에서 가져온 토큰의 수명은 24시간이다. 토큰에 포함된 스페이셜 앵커 서비스가 사용하는 정보를 통해 계정에 대한 권한 부여를 결정하고 권한 있는 사용자만 계정에 액세스할 수 있도록 한다. 액세스 토큰은 계정 키 또는 Azure AD에서 발급한 토큰에서 교환하여 얻을 수 있다. 계정 키를 사용하면 애저 스페이셜 앵커 서비스를 빠르게 사용할 수 있다. 하지만 어플리케이션을 프로덕션 환 경에 배포하기 전에 Azure AD 인증을 사용하도록 어플리케이션을 업데이트하는 것이 바람직하다. 다음 두 가지 방법으로 Azure AD 인증 토큰을 얻을 수 있다. 엔터프라이즈 어플리케이션을 빌드하고 회사가 Azure AD를 ID 시스템으로 사용하는 경우 어플리케이션에서 사용 자 기반 Azure AD 인증을 사용할 수 있다. 그런 다음 기존 Azure AD 보안 그룹을 사용하여 스페이셜 앵커 계정 에 대한 액세스 권한을 부여한다. 조직의 사용자에게 직접 액세스 권한을 부여할 수도 있다. 그렇지 않은 경우에는 어플리케이션을 지원하는 웹 서비스에서 Azure AD 토큰을 가져오는 것이 바람직하다. 클 라이언트 어플리케이션에 애저 스페이셜 앵커에 액세스하기 위한 자격 증명을 포함하지 않도록 할 수 있으므로 프로덕션 어플리케이션에 이 방법을 사용하는 것이 바람직하다. Azure AD(Azure Active Directory) 사용자를 대상으로 하는 어플리케이션의 경우 사용자에 대한 Azure AD 토큰 을 사용하는 것이 바람직하다. MSAL을 사용하여 이 토큰을 가져올 수 있다. 다음과 같이 앱 등록에 대한 빠른 시작의 단계를 수행한다. 우선, Azure Portal 상에서, Azure AD에서 어플리케이션을 네이티브 어플리케이션으로 등록한다. 등록하는 과정 에서 어플리케이션이 다중 테넌트인지 여부를 확인한다. 또한, 어플리케이션에 허용되는 리디렉션 URL도 제공한 다. 그런 다음, API 사용 권한 탭으로 이동한 후, 사용 권한 추가를 선택한다. 이때, 내 조직에서 사용하는 API 탭에서 혼합 현실 리소스 공급자를 선택하고, 위임된 권한을 선택하며, mixedreality아래에서 mixedreality.signin을 선택한 후, 권한 추가를 선택한다. 그런 다음, 관리자 동의 허용을 선택한다. 그런 다음, 리소스에 대한 액세스 권한을 부여할 어플리케이션 또는 사용자에게 ASA RBAC 역할을 할당한다. 어 플리케이션의 사용자에게 ASA 계정에 다른 역할을 지정하려면 Azure AD에 여러 어플리케이션을 등록하고 각 사 용자에게 별도 역할을 할당한다. 그런 다음 사용자에게 올바른 역할을 사용하도록 권한 부여 논리를 구현한다. 한편, 코드에서 적용하는 방법을 설명하자면, Azure AD 어플리케이션의 어플리케이션 ID 및 리디렉션 URI를 MSAL의 클라이언트 ID 및 RedirectUri 매개 변수로 사용해야 한다. 그런 다음, 테넌트 정보를 설정하는 과정, 즉 어플리케이션에서 내 조직만을 지원하는 경우 이 값을 테넌트 ID 또는 테넌트 이름으로 변경하고, 어플리케이션이 모든 조직 디렉터리의 계정을 지원하는 경우 이 값을 조직으로 변경하며, 어플리케이션이 모든 Microsoft 계정 사용자를 지원하는 경우 이 값을 공용으로 변경하는 과정을 거친다. 그런 다음, 토큰 요청에서 범위를.로 설정한다. 이 범위는 어플리케이션이 Mixed Reality STS(보안 토큰 서비 스)에 대한 토큰을 요청하고 있음을 Azure AD에 나타낸다. 이러한 단계를 완료한 후 어플리케이션은 MSAL에서 Azure AD 토큰을 얻을 수 있다. Azure AD 서비스 인증 여기서는 어플리케이션이 자체 메커니즘을 사용하여 백 엔드 서비스를 인증하는 것으로 가정한다.(예: Microsoft 계정, PlayFab, Facebook, Google ID 또는 사용자 지정 사용자 이름 및 암호). 사용자가 백 엔드 서 비스에 인증되면 해당 서비스는 Azure AD 토큰을 검색하고, Azure Spatial Anchors에 대한 액세스 토큰으로 교 환하고, 클라이언트 어플리케이션에 다시 반환할 수 있다. Azure AD 액세스 토큰은 MSAL을 통해 검색된다. 앱 빠른 시작 등록에 나열된 다음 단계를 수행한다. 우선, Azure Portal에서, Azure AD에 어플리케이션을 등록한다. 그런 다음, Azure Portal에서 Azure Active Directory를 선택한 다음, 앱 등록을 선택한다. 그럼 다음, 새 등록을 선택하고, 어플리케이션 이름을 입력하고 어플리케이션 유형으로 웹앱/API를 선택한 후 서비스의 인증 URL을 입력한 후, 만들기를 선택한다. 그런 다음, 어플리케이션에서 설정을 선택한 다음 인증서 및 암호 탭을 선택한다. 새 클라이언트 암호를 만 들고 기간을 선택한 다음 추가를 선택한다.이때, 암호 값을 저장하고, 웹 서비스의 코드에 포함해야 한다. 그런 다음, 리소스에 대한 액세스 권한을 부여할 어플리케이션 또는 사용자에게 ASA RBAC 역할을 할당한다. 어 플리케이션의 사용자에게 ASA 계정에 다른 역할을 지정하려면 Azure AD에 여러 어플리케이션을 등록하고 각 사 용자에게 별도 역할을 할당한다. 그런 다음 사용자에게 올바른 역할을 사용하도록 권한 부여 논리를 구현한다. Azure 역할 기반 액세스 제어 어플리케이션, 서비스 또는 서비스의 Azure AD 사용자에게 부여되는 액세스 수준을 제어할 수 있도록 만든 다음 역할을 필요에 따라 애저 스페이셜 앵커(Azure Spatial Anchors) 계정에 할당할 수 있다. 이때, 스페이셜 앵커 계정 소유자인 이 역할의 어플리케이션 또는 사용자는 공간 앵커를 만들고, 이를 쿼리하 고, 삭제할 수 있다. 계정 키를 사용하여 계정에 인증하는 경우 스페이셜 앵커 계정 소유자 역할이 인증된 보안 주체에 할당됩니다. 또한, 스페이셜 앵커 계정 기여자인 이 역할의 어플리케이션 또는 사용자는 공간 앵커를 만들고, 이를 쿼리할 수 있지만 삭제할 수 없다. 또한, 스페이셜 앵커 계정 읽기 권한자인 이 역할의 어플리케이션 또는 사용자는 공간 앵커에 대한 쿼리 작업만 수행할 수 있다. 새 공간 앵커를 만들거나 기존 항목을 삭제하거나 메타데이터를 업데이트할 수 없다. 이 역할 은 일반적으로 일부 사용자가 환경을 큐레이팅하는 어플리케이션에 사용되지만, 다른 사용자는 이전에 환경에 배치된 앵커만 회수할 수 있다. 원격 렌더링(Remote Rendering) 원격 렌더링(Remote Rendering)은 복잡한 렌더링 작업을 클라우드로 오프로딩하는 방식으로 작동한다. 대부분의 클라우드 서버에 GPU가 없기 때문에 아무 서버나 이러한 렌더링 작업을 수행할 수 없다. 더 일반적인 웹 트래픽 과 달리, 관련 데이터 양과 대화식 프레임 속도로 결과 생성에 대한 까다로운 요구 사항으로 인해 어느 서버가 어느 사용자 요청을 처리하는지에 대한 책임을 다른 머신으로 즉시 넘길 수 없다. 즉, 원격 렌더링(Remote Rendering)을 사용하는 경우 렌더링 요청을 처리하기 위해 필요한 하드웨어 기능이 있 는 클라우드 서버를 독점 예약해야 한다. 세션은 이 서버와의 상호 작용과 관련된 모든 것을 가리킨다. 세션은 사용할 머신을 예약(임대)하는 초기 요청으로 시작하고, 모델을 로드하고 조작하는 모든 명령을 계속 진행하고, 클라우드 서버에서 임대를 해제하는 것으로 끝난다. 이러한 세션을 관리하고 상호 작용하는 방법에는 여러 가지가 있다. 세션을 만들고, 업데이트하고, 종료하는 언 어 독립 방법은 세션 관리 REST API를 사용하는 것이다. 예를 들어, C# 및 C++에서 이러한 작업은 클래스 RemoteRenderingClient 및 RenderingSession을 통해 노출된다. Unity 어플리케이션의 경우 ARRServiceUnity 구성 요소에서 제공하는 추가 유틸리티 기능이 있다. 활성 세션에 연결되면 모델 로드 및 장면 상호 작용 등의 작업이 클래스를 통해 노출된다. 한편, 하나의 디바이스에서 여러 세션에 완전히 연결하는 것은 불가능하나, 단일 어플리케이션에서 원하는 만 큼 세션을 만들고, 관찰하고, 종료할 수는 있다. 어플리케이션이 세션에 연결되지 않는 한 HoloLens 2와 같은디바이스에서 실행할 필요가 없다. 이러한 구현의 사용 사례로 중앙 메커니즘을 통해 세션을 제어하려는 경우가 있다. 예를 들어, 여러 태블릿과 HoloLens 디바이스가 로그인할 수 있는 웹앱을 빌드할 수 있다. 그러면 어플리 케이션은 표시할 CAD 모델과 같은 옵션을 태블릿에 표시할 수 있다. 사용자가 선택하면 이 정보는 공유 환경을 만들기 위해 모든 HoloLens 디바이스에 전달된다. 또한, 모든 세션은 아래와 같이 여러 단계를 거치게 된다. 세션 시작 ARR에 새 세션을 만들도록 요청하면 먼저 세션 UUID가 반환된다. 이 UUID를 사용하여 세션에 대한 정보를 쿼 리할 수 있다. UUID와 세션에 대한 일부 기본 정보는 30일 동안 유지되므로 세션이 중지된 후에도 해당 정보를 쿼리할 수 있다. 이때 세션 상태는 시작 중으로 보고된다. 원격 렌더링(Remote Rendering)이 세션을 호스트할 수 있는 서버를 찾으려고 한다. 이 검색에는 2개의 매개 변 수가 사용된다. 먼저 지역의 서버만 예약된다. 지역 간의 네트워크 대기 시간이 너무 길어서 적절한 환경을 보 장할 수 없기 때문이다. 두 번째 요소는 사용자가 지정한 원하는 크기이다. 각 지역에는 표준 또는 프리미 엄 크기 요청을 수행할 수 있는 제한된 수의 서버가 있다. 따라서 요청된 크기의 서버가 지역에서 모두 현재 사용 중인 경우 세션을 만들지 못한다. 실패 이유를 쿼리할 수 있다. 서비스에서 적합한 서버를 찾으면 적절한 VM(가상 머신)을 복사하여 애저 원격 렌더링(Azure Remote Rendering) 호스트로 전환해야 한다. 이 프로세스가 종료된 이후에 VM이 부팅되고 세션 상태가 준비로 전환된다. 이때, 서버는 입력을 단독으로 대기한다. 세션에 연결 세션이 준비되면 해당 세션에 연결할 수 있다. 연결되어 있는 동안 디바이스는 모델을 로드하고 수정하는 명 령을 보낼 수 있다. 모든 ARR 호스트는 한 번에 하나의 클라이언트 디바이스만 서비스하므로 클라이언트가 세션 에 연결할 때 렌더링된 콘텐츠를 독점적으로 제어할 수 있다. 즉, 렌더링 성능은 사용자가 제어할 수 없는 이유 로 변경되지 않는다. 한편, 한 클라이언트만 세션에 연결할 수 있지만 현재 상태와 같은 세션에 대한 기본 정보는 연결하지 않고 쿼 리할 수 있다. 디바이스가 세션에 연결되어 있는 동안 다른 디바이스의 연결 시도는 실패한다. 그러나 연결된 디바이스가 자발 적으로 또는 어떤 종류의 오류로 인해 연결을 끊은 후에는 세션에서 다른 연결 요청을 수락한다. 이전 상태(로 드된 모델 등)는 다음 연결 디바이스가 깨끗 한 슬레이트를 갖도록 모두 버려진다. 따라서 여러 디바이스에서 세션을 여러 번 재사용할 수 있으며 대부분의 경우 최종 사용자에게서 세션 시작 오버헤드를 숨길 수 있다. 원격 서버는 클라이언트 쪽 데이터의 상태를 변경하지 않는다. 클라이언트 어플리케이션이 변환 업데이트와 로 드 요청 등의 모든 데이터 변형을 수행해야 한다. 모든 작업은 클라이언트 상태를 즉시 업데이트한다. 엔터티 엔터티는 공간에서 이동할 수 있는 개체를 가리키며, 원격으로 렌더링되는 콘텐츠의 기본 구성 요소이다. 엔터티는 위치, 회전 및 크기로 정의되는 변환을 갖는다. 엔터티 그 자체로는 눈에 보이는 기능을 갖지 않으며, 엔터티에 연결되는 구성 요소를 통해 동작이 추가된다. 엔터티 자체의 가장 중요한 측면은 계층 구조와 그로 인한 계층 변환이다. 예를 들어, 공유 부모 엔터티에 여러 엔터티가 자식 요소로 연결된 경우, 부모 엔터티의 변환을 변경하여 모든 엔터티를 이동, 회전 및 크기 조정할 수 있다. 또한 엔터티의 enabled 상태를 사용하여 계층의 전체 하위 그래프에 대한 표시 유형 및 광선 캐스트 에 대한 응답을 해제할 수 있다. 엔터티는 부모가 고유하게 소유한다. 엔터티는 서버가 콘텐츠를 로드하거나 사용자가 장면에 개체를 추가하려는 경우에 만들어진다. 예를 들어, 사용자가 메시의 내부를 시각화하기 위해 잘린 평면을 추가하려는 경우, 평면을 배치하려는 곳에 엔터티를 만들고 여기에 잘린 평면 구성 요소를 추가하면 된다. 구성요소(Components) Entity Component System 패턴을 사용합니다. 엔터티가 개체의 위치 및 계층적 컴퍼지션을 나타낸다면 구성 요소는 동작 구현을 담당한다. 가장 자주 사용되는 구성 요소 형식은 메시를 렌더링 파이프라인에 추가하는 mesh components이다. 이와 유사 하게 광원 구성 요소는 광원을 추가하는 데, 절단면 구성 요소는 열린 메시를 잘라내는 데 사용한다. 이러한 모든 구성 요소는 참조 지점으로 연결된 엔터티의 변환(위치, 회전, 배율)을 사용한다. 모델 모델은 엔터티 및 구성 요소로 구성된 전체 개체 표현을 나타낸다. 모델은 원격 렌더링 서비스에 사용자 지 정 데이터를 가져오는 주요 방법이다. 모델에는 루트 노드인 엔터티가 하나만 있다. 그 아래에 자식 엔터티의 임의 계층 구조가 있을 수 있다. 모델 을 로드하면 이 루트 엔터티에 대한 참조가 반환된다. 각 엔터티에는 연결된 구성 요소가 있을 수 있다. 일반적으로 엔터티는 메시 리소스를 참조하는 MeshComponents를 포함한다. 또한, FBX 및 GLTF와 같은 파일 형식의 입력 모델을 변환하여 런타임용 모델을 만들 수 있다. 변환 프로세스는 질감, 재질 및 메시와 같은 모든 리소스를 추출하고 최적화된 런타임 형식으로 변환한다. 또한 구조적 정보를 추출하고 이를 ARR의 엔터티/구성 요소 그래프 구조로 변환한다. 메시(Mesh) 메시는 모델 변환을 통해서만 만들 수 있는 변경 불가능한 공유 리소스이다. 메시는 raycast 쿼리를 위한 물 리적 표현과 함께 하나 또는 여러 개의 하위 메시가 포함된다. 각 하위 메시는 기본적으로 렌더링되어야 하는 재질을 참조한다. 3D 공간에 메시를 배치하려면 Entity에 MeshComponent를 추가한다. 메시 클래스 속성은 재질과 경계를 포함한다. 재질은 배열이다. 각 재질은 다른 하위 메시에서 사용된다. 배열의 여러 항목은 동일한 재질을 참조할 수 있다. 이 데이터는 런타임 시 수정할 수 없다. 경계는 메시 꼭짓점의 로컬 공간 AABB(축 정렬 경계 상자)이다. MeshComponent 클래스는 메시 리소스의 인스턴스를 배치하는 데 사용된다. 각 MeshComponent는 단일 메시를 참 조한다. 각 하위 메시를 렌더링하는 데 사용되는 재질을 재정의할 수 있다. MeshComponent 속성을 살펴보면, 메시는 이 구성 요소에서 사용하는 메시 리소스이고, 재질은 메시 구성 요소 자체에 지정된 재질의 배열이다. 배열의 길이는 항상 메시 리소스의 재질 배열과 동일하고, 메시 기본값에서 재정의되지 않아야 하는 재질은 이 배열에서 null로 설정된다. 또한, UsedMaterials은 각 하위 메시에 실제로 사용되는 재질의 배열이고, null이 아닌 값에 대한 재질 배열의 데이터와 동일하다. 그렇지 않으면 메시 인스 턴스의 재질 배열 값이 포함된다. 질감(Textures) 텍스처는 변경할 수 없는 공유 리소스이다. 텍스처는 Blob 스토리지에서 로드되고 모델에 직접 적용될 수 있 다. 그러나 가장 일반적으로 텍스처는 변환된 모델의 일부로 해당 재료에 의해 참조된다. ARR에 지정된 모든 텍스처는 DDS 형식이어야 한다. mipmaps 및 텍스처 압축을 사용하는 것이 바람직하다. 컨테이너 파일 형식(파일 이름 확장자 DDS사용)은 GPU에 의해 하드웨어에서 압축 해제될 수 있는 이전 독점 S3 텍스처 압축(S3TC) 알고리즘으로 압축된 데이터를 저장하기 위한 Microsoft 형식이다. 따라서 압축 및 비압축 데이터 파일 로 그래픽 텍스처 및 큐빅 환경 맵을 저장하는 데 형식이 유용하다. 이 데이터 형식의 파일 확장자는 '.dds'입니다. 또한, 텍스처를 로드하는 경우 예상되는 형식을 지정해야 한다. 형식이 일치하지 않으면 텍스처 로드가 실패한 다. 동일한 URI를 사용하여 텍스처를 두 번 로드하면 동일한 텍스처 개체가 반환된다. 이는 공유 리소스이기 때문이다. 재질(Materials) 재질은 메시가 렌더링되는 방법을 정의하는 공유리소스이다. 재질은 적용할 질감, 개체를 투명하게 만들지 여부, 조명이 계산되는 방법을 지정하는 데 사용된다. 재질은 모델 변환 중에 자동으로 생성되며 런타임에 액세스할 수 있다. 코드에서 사용자 지정 재질을 만들어 기존 재질을 대체할 수도 있다. 이 시나리오는 동일한 재질을 여러 메시에서 공유하려는 경우에 특히 적합하다. 재질을 참조하는 모든 메시에서 재질 수정 내용이 표시되므로 이 메서드를 사용하여 변경 내용을 쉽게 적용할 수 있다. 애저 원격 렌더링(Azure Remote Rendering)에는 두 가지 고유한 재질 유형이 있다. PBR 재질은 가능한 한 물리적으로 올바르게 렌더링되어야 하는 표면에 사용된다. 이러한 재질에 대한 실제 조명 은 PBR(물리적 기반 렌더링)을 사용하여 계산된다. 이 재질 유형을 최대한 활용하려면 거칠기 및 법선 맵과 같 은 고품질의 입력 데이터를 제공하는 것이 중요하다. 색 재질은 추가 조명이 필요하지 않은 경우에 사용된다. 이러한 재질은 항상 전체적으로 밝아서 더 쉽게 설정할 수 있다. 색 재질은 조명이 아예 없어야 하거나 photogrammetry를 통해 가져온 모델과 같이 고정 조명이 이미 통합된 데이터에 사용된다. 한편, 메시에는 하나 이상의 하위 메시가 있다. 각 하위 메시는 하나의 재질을 참조한다. 메시에서 직접 사용할 재질을 변경하거나 MeshComponent에서 하위 메시에 사용할 재질을 재정의할 수 있다. 메시 리소스에서 재질을 직접 수정하는 경우 이 변경 내용은 해당 메시의 모든 인스턴스에 영향을 준다. 그러나 MeshComponent에 대한 변경 내용은 하나의 메시 인스턴스에만 영향을 준다. 원하는 동작에 따라 적합한 방법이 다르지만 MeshComponent를 수정하는 것이 더 일반적인 방법이다. 또한, 변환 중에 속성과 질감이 동일한 여러 재질은 자동으로 중복 제거되어 단일 재질이 된다. 변환 설정에서 이 기능을 사용하지 않도록 설정할 수 있지만 최상의 성능을 위해 이 기능을 설정하는 것이 바람직하다 개체 및 리소스 수명 개체는 사용자의 재량에 따라 생성, 수정 및 제거할 수 있는 것으로 간주된다. 개체는 자유롭게 복제할 수 있으 며 각 인스턴스는 시간이 지남에 따라 변형될 수 있다. 따라서 엔터티 및 구성 요소는 개체이다. 개체의 수명은 전적으로 사용자 제어에 달려있다. 그러나 클라이언트 쪽 표현의 수명과는 관련이 없다. Entity 및 Component와 같은 클래스에는 원격 렌더링 호스트에서 개체의 할당을 취소하기 위해 호출해야 하는 Destroy 함수가 있다. 또한 Entity.Destroy()는 해당 계층의 엔터티, 자식 및 모든 구성 요소를 제거한다. 리소스의 수명은 완전히 원격 렌더링 호스트에 의해 관리된다. 리소스는 내부적으로 참조 횟수가 계산된다. 더 이상 참조되지 않는 경우 리소스의 할당이 취소된다. 대부분의 리소스는 일반적으로 파일에서 로드하여 간접적으로만 만들 수 있다. 동일한 파일이 여러 번 로드되면 Azure Remote Rendering은 동일한 참조를 반환하고 데이터를 다시 로드하지 않는다. 많은 리소스는 변경이 불가능하다(예: 메시 및 질감). 그러나 일부 리소스는 변경 가능하다(예: 재질). 리 소스는 자주 공유되므로 리소스를 수정하면 여러 개체에 영향을 줄 수 있다. 예를 들어 재질의 색을 변경하면 메시를 사용하는 모든 개체의 색이 변경되어 해당 재질을 참조한다. 기본 제공 리소스 애저 원격 렌더링(Azure Remote Rendering)에는 RenderingSession.Connection.LoadXYZAsync()를 호출할 때 builtin://으로 각 식별자를 추가하여 로드할 수 있는 몇 가지 기본 제공 리소스가 포함되어 있다. 모든 개체 및 리소스의 수명은 연결에 바인딩된다. 연결 해제 시 모든 항목은 무시된다. 동일한 세션에 다시 연 결하는 경우 장면 그래프가 비어 있고 모든 리소스가 제거된다. 실제로 세션에 동일한 리소스를 로드하는 경우 연결을 끊은 후에는 일반적으로 처음보다 빠르다. 대부분의 리소 스를 Azure Storage에서 처음 다운로드해야 하기 때문이다. 이 경우 두 번째는 필요하지 않으므로 상당한 시간 을 절약할 수 있다. 개체 경계(Object bounds) 개체 경계는 엔터티 및 해당 자식이 차지하는 볼륨을 나타낸다. 애저 원격 렌더링(Azure Remote Rendering)에 서 개체 경계는 항상 AABB(축 정렬 경계 상자)로 제공된다. 개체 경계는 로컬 공간 또는 월드 공간 중 하나일 수 있다. 어느 쪽이든 항상 축에 정렬된다. 즉, 범위와 볼륨이 로컬 및 월드 공간 표현 사이에 다를 수 있 다. 또한, 메시의 로컬 축 정렬 경계 상자는 메시 리소스에서 직접 쿼리할 수 있다. 이러한 경계는 엔터티의 변형을 사용하여 엔터티의 로컬 공간 또는 월드 공간으로 변환될 수 있다. 렌더링 모드 원격 렌더링(Remote Rendering)은 TileBasedComposition 모드 및 DepthBasedComposition 모드라는 두 가지 주요 작업 모드를 제공한다. 이러한 모드는 워크로드가 서버의 여러 GPU에 분산되는 방법을 결정한다. 모드는 연결 시 지정해야 하며 런타임 중에는 변경할 수 없다. 두 모드는 모두 장점이 있지만 고유한 기능 제한 사항이 있으므로 가장 적합한 모드를 선택하는 것은 사용 사례 에 따라 달라지게 된다. TileBasedComposition 모드에서 관련된 모든 GPU는 특정 하위 사각형(타일)을 화면에 렌더링한다. 주 GPU는 비 디오 프레임으로 클라이언트에 보내기 전에 타일에서 최종 이미지를 작성한다. 따라서 모든 GPU는 렌더링을 위 해 동일한 리소스 집합을 사용해야 하므로 로드된 자산은 단일 GPU의 메모리에 적합해야 한다. MSAA가 모든 GPU의 전체 기하 도형 집합에서 작동할 수 있으므로 이 모드의 렌더링 품질은 DepthBasedComposition 모드보다 약간 더 높다. 또한 이 모드에서는 각 부분을 투명한 재질로 전환하거나 HierarchicalStateOverrideComponent를 통해 투명하 게 표시 모드로 전환할 수 있다. DepthBasedComposition 모드에서 관련된 모든 GPU는 전체 화면 해상도로 렌더링되지만 메시의 하위 집합만 렌 더링된다. 주 GPU의 최종 이미지 작성은 깊이 정보에 따라 부분이 적절히 병합되도록 고려한다. 기본적으로 메 모리 페이로드는 GPU 전체에 분산되므로 단일 GPU의 메모리에 적합하지 않는 모델을 렌더링할 수 있다. 모든 단일 GPU는 MSAA를 사용하여 로컬 콘텐츠를 앤티앨리어싱한다. 그러나 고유한 GPU의 가장자리 간에 고유한 앨리어싱이 있을 수 있다. 이 효과는 최종 이미지를 후 처리하여 완화되지만 MSAA 품질은 TileBasedComposition 모드보다 낮다. MSAA 아티팩트는 조각과 커튼이 모두 동일한 GPU에 렌더링되므로 앤티앨리어싱이 두 부분 사이에서 제대로 작동 한다. 반면에 커튼과 벽이 별개의 GPU로 구성되어 있으므로 이러한 두 부분 사이의 가장자리에서 약간의 앨리어 싱을 보여준다. 이 모드의 가장 큰 제한 사항은 기하 도형 부분을 투명한 재질로 동적으로 전환할 수 없고 HierarchicalStateOverrideComponent에 대해 투명하게 표시 모드가 작동하지 않는다는 것이다. 그러나 다른 상태 재정의 기능(윤곽선, 색조 등)은 작동한다. 또한 변환 시 투명으로 표시된 재질도 이 모드에서 제대로 작 동한다. 두 모드의 성능 특성은 사용 사례에 따라 달라지며, 일반적인 권장 사항을 추론하거나 제공하기가 어렵다. 위에 서 언급한 제한 사항(메모리 또는 투명도/투명하게 표시)의 제약을 받지 않는 경우 두 모드를 모두 사용해 보고 다양한 카메라 위치를 사용하여 성능을 모니터링하는 것이 바람직하다. 그래픽 바인딩 그래픽 바인딩은 렌더링된 이미지에 영향을 주는 다양한 함수에 대한 액세스를 제공한다. 이러한 함수는 항상 사용할 수 있는 일반 함수와 선택한 Microsoft.Azure.RemoteRendering.GraphicsApiType에만 관련된 특정 함수 의 두 가지 범주로 구분할 수 있다. 모델 형식에 대한 재질 매핑 원본 자산이 모델로 변환될 때 변환기는 각 메시의 재질을 만든다. 재질을 만드는 방법은 재정의할 수 있다. 하지만 기본적으로 변환은 PBR 재질을 만든다. 모든 원본 파일 형식(예: FBX)은 자체 규칙을 사용하여 재질을 정의하므로 이러한 규칙은 Azure Remote Rendering의 PBR 재질 매개 변수에 매핑되어야 한다. 이상에서는 본 발명에 따른 바람직한 실시예들에 대하여 도시하고 또한 설명하였다. 그러나 본 발명은 상술한 실시예에 한정되지 아니하며, 특허 청구의 범위에서 첨부하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속 하는 기술 분야에서 통상의 지식을 가진 자라면 누구든지 다양한 변형 실시가 가능할 것이다."}
{"patent_id": "10-2022-0025103", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 내지 도 2는 본 발명의 일 실시예에 따른 메타버스 기반의 실습형 교육 시스템을 개략적으로 나타내는 블 럭도이다. 도 3은 도 1 내지 도 2의 메타버스 플랫폼 서버의 구성을 개략적으로 나타내는 블럭도이다."}
