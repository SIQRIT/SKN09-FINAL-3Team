{"patent_id": "10-2023-0144354", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0072920", "출원번호": "10-2023-0144354", "발명의 명칭": "이종 소규모 데이터셋을 위한 멀티태스크 학습 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "양지원"}}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "상이한 태스크들에 대해 생성된 학습 데이터 쌍을 하나의 특징공간으로 투영시켜서 특징벡터를 생성하는 제1레이어;상기 투영된 특징벡터로부터 공통 특징을 추출하는 제2레이어; 상기 추출된 공통 특징에 대해 개별적인 추론을 수행하는 제3레이어를 포함하며, 상기 제1레이어와 제3레이어는 태스크별 레이어이며, 상기 제2레이어는 태스크들간에 공유되는 레이어이고,상기 제1레이어, 제2레이어, 및 제3레이어는 하나의 인공신경망에서의 순전파를 실행하는 멀티태스크 학습장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서, 상기 제1레이어는 투영 인코더(Projection Encoder)를 포함하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서, 상기 제1레이어는 상기 특징벡터를 생성시에 태스크마다 할당된 별개의 웨이트 매트릭스를 이용하는 것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서, 상기 제2레이어는 융합 인코더(Fusion Encoder)를 포함하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서, 상기 제2레이어는 상기 공통 특징을 추출시에 동일 웨이트 매트릭스를 이용하는 것을 특징으로 하는 이종 소규모 데이터셋을 위한멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에서, 상기 제3레이어는 독립 분류기(Independent Classifier)를 포함하는 이종 소규모 데이터셋을 위한멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에서, 상기 학습 데이터셋은 Data augmentation 기법을 이용하여 생성되는 것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에서, 상기 인공신경망의 역전파를 위한 손실함수로 태스크별(task-specific) 추론 에러 및 데이터쌍별 표현 손실(Pairwise Representation Loss)이 사용되는 것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 장치."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2024-0072920-3-제1레이어, 제2레이어, 및 제3레이어가 포함된 인공신경망에서 실행되는 멀티태스크 학습 방법으로,상기 제1레이어가, 상이한 태스크들에 대해 생성된 학습 데이터 쌍을 하나의 특징공간으로 투영시켜서 특징벡터를 생성하고;상기 제2레이어가, 상기 투영된 특징벡터로부터 공통 특징을 추출하고;상기 제3레이어가, 상기 추출된 공통 특징에 대해 개별적인 추론을 수행하는 것을 포함하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 방법."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에서, 상기 제1레이어가 상기 특징벡터를 생성시에 태스크마다 할당된 별개의 웨이트 매트릭스를 이용하는것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 방법."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에서, 상기 제2레이어가 상기 공통 특징을 추출시에 동일 웨이트 매트릭스를 이용하는 것을 특징으로 하는이종 소규모 데이터셋을 위한 멀티태스크 학습 방법."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에서, 상기 학습 데이터셋은 Data augmentation 기법을 이용하여 생성되는 것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 방법."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에서, 상기 Data augmentation 기법은각 태스크 데이터셋에서 무작위로 추출한 데이터를 짝으로 맞추되, 각 데이터셋마다의 모든 데이터가 적어도 하나 이상의 학습 데이터 쌍에 포함되도록 하는 것을 특징으로 하는 이종 소규모 데이터셋을 위한 멀티태스크 학습 방법."}
{"patent_id": "10-2023-0144354", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에서, 태스크별(task-specific) 추론 에러 및 데이터쌍별 표현 손실(Pairwise Representation Loss)을 사용하여 상기 인공신경망의 역전파를 수행하는 것을 추가로 포함하는 이종 소규모 데이터셋을 위한 멀티태스크학습 방법."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 따른 이종의 소규모 데이터셋의 학습 성능 고도화를 위한 멀티태스크 학습 장치 및 방법은, 상이한 태 스크들에 대해 생성된 학습 데이터 쌍을 하나의 특징공간으로 투영시켜서 특징벡터를 생성하는 제1레이어; 상기 투영된 특징벡터로부터 공통 특징을 추출하는 제2레이어; 상기 추출된 공통 특징에 대해 개별적인 추론을 수행하 는 제3레이어를 포함한다. 여기서, 상기 제1레이어와 제3레이어는 태스크별 레이어이고 상기 제2레이어는 태스크 들간에 공유되는 레이어이다. 그리고 상기 제1레이어, 제2레이어, 및 제3레이어는 하나의 인공신경망에서 순전파 를 실행할 수 있다. 본 발명에 따르면 이질적인(heterogenous) 구조와 형태의 소규모 데이터셋들을 동시에 상호 보완적으로 학습하여 데이터 부족으로 인한 모델 성능 저하를 보완하고 성능 개선 및 균형 있는 성능을 보장할 수 있다."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 및 기계학습 방법론에 관한 것으로, 데이터 양은 적으면서 상이한 도메인에 속하는 이종의 소규모 데이터셋들에 대한 동시 학습을 수행하는 멀티태스크 학습 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "근래 들어 빅데이터 시장과 기술이 크게 확대됨에 따라 딥러닝 기반 시스템의 고도화가 이뤄지고 있지만, 여전 히 가용할 수 있는 규모의 데이터를 확보하기 어려운 산업 분야들도 존재한다. 학습 데이터 소스를 직접 마련하거나 인프라 구축이 어려울 시, 부분적으로 쪼개진 소규모 공공·상용 데이터 소스에 의존하게 된다. 그러나 데이터의 양이 부족하면 인공지능 모델의 복잡도를 아무리 높여도 실증할 수 있 는 수준의 추론 성능을 기대하기 어렵다. 또한, 여러 데이터 세트(데이터셋)들을 취합하여 전체 데이터 규모를 늘리더라도 각각의 데이터셋마다 데이터 수집 시기, 대상, 환경적 요인들이 다르기에 이들을 융합적으로 활용하 기는 결코 쉽지 않다.위와 같이 학습 데이터의 양이 충분하지 않거나 보유한 데이터가 차원 이미지 또는 자연어와 같이 복잡한 구조 를 가진 경우에 멀티태스크 학습(multi-task learning) 기술을 도입할 수 있다. 멀티태스크 학습은 비슷한 여러 데이터셋을 동시에 학습함으로써 주어진 모든 태스크에 대한 성능 향상을 위한 인공지능 방법론이다. 멀티태스 크 학습은, 각각의 데이터셋의 크기가 작을지라도 이들을 끌어 모아 하나의 데이터셋처럼 학습을 수행한다. 멀 티태스크 학습은 공유 레이어로 각 태스크에 속하는 데이터들의 특성을 먼저 추출하고, 이후에 학습 레이어를 태스크 개수만큼 분리해 추출한 공통 특성으로 개별 태스크를 수행하는 구조를 갖는다. 이러한 모델 구조를 통 해 각 태스크를 학습하는 과정에서 얻은 정보가 다른 태스크의 학습 성능 개선에도 큰 영향을 줄 수 있다. 그러나, 일반적인 멀티태스크 학습에는 크게 2가지 기술적 문제가 존재한다. 첫째, 각각의 태스크에서 학습하고자 하는 데이터가 같은 특징공간(feature space)에 존재해야 한다는 제약이다. 데이터의 특징벡터(feature vector)의 차원이 같아야 하며 각 특징벡터에서 표현할 수 있는 특징들의 구성, 또 이들이 가질 수 있는 값의 범위 등이 태스크마다 상이해선 안 된다. 이는 데이터셋 간의 공통 특징을 추출하는 과정에서 같은 네트워크 구조를 공유해야 하기 때문이며, 데이터 특성이 이질적인(heterogenous) 경우 는 공유 레이어로의 순전파가 불가능하다. 현실에서 수집된 복수의 데이터셋이 모두 같은 구조와 특성을 따를 것을 기대하기는 어려운 것이다. 둘째, 실제로 공유 레이어가 공통 특징을 학습하는 데 관여하는지 확인할 방법이 부재하다. 실제로 각각의 태스 크가 매우 상이한 데이터 분포 또는 라벨링 기준을 갖고 있을 경우가 존재할 수 있다. 이때, 학습 모델은 데이 터 간 공통된 특징을 추출하기보다는 개별 태스크마다 경쟁적으로 자신의 추론 성능만을 높이려는 방향으로 학 습이 유도된다. 이는 각 데이터셋에 대한 학습이, 관련 없는 태스크에 대한 추론 성능을 저하시키기도 한다. 특 히 태스크별로 학습 난이도가 크게 차이가 날 경우, 분류 손실 값을 쉽게 줄일 수 있는 태스크 학습에 과적합되 는 현상이 발생하고, 이는 결국 태스크별로 불균형한 성능을 얻게 되는 문제를 일으킨다."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 데이터 양은 적으면서 상이한 도메인에 속하는 이질적인(heterogenous) 구조와 형태의 소규모 데이터셋들을 동시에 상호보완적으로 학습하여 데이터 부족으로 인한 모델 성능 저하를 보완하고 성능 개선 및 균형 있는 성능을 보장하는 멀티태스크 학습 방법 및 이 방법을 수행하는 장치를 제공하는 것이다. 본 발명에서 해결하고자 하는 기술적 과제를 정리하면 아래와 같다. 1) 기존 멀티태스크 학습 메커니즘에서 특징공간이 다른 데이터셋을 공통으로 학습하지 못하는 문제 2) 상이한 데이터셋 학습 시, 공유 레이어가 공통된 특징을 추출하는 기능을 제대로 수행하지 못하는 문제, 또 그로 인해 태스크별 추론 성능 차이가 크게 벌어지는 문제"}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에서는 상기 목적을 달성하기 위해 아래와 같은 3가지 기술을 제시한다. 1) 특징공간(feature space)이 상이한 이종(heterogenous) 데이터셋을 동시에 학습할 수 있도록 batch를 생성 하는 Data Augmentation 기법 2) 이종 데이터셋이 비슷한 특징공간을 갖도록 투영(projection)하는 기법 3) Representation loss(표현 손실)와 Task-specific loss(태스크별 손실)의 최적화 강도 조절을 통한 공유 레 이어 및 개별 태스크 추론 레이어 간의 학습 성능 불균형을 해결하는 기술 구체적으로, 본 발명에 따른 이종의 소규모 데이터셋의 학습 성능 고도화를 위한 멀티태스크 학습 장치 및 방법 은, 상이한 태스크들에 대해 생성된 학습 데이터 쌍을 하나의 특징공간으로 투영시켜서 특징벡터를 생성하는 제 1레이어; 상기 투영된 특징벡터로부터 공통 특징을 추출하는 제2레이어; 상기 추출된 공통 특징에 대해 개별적 인 추론을 수행하는 제3레이어를 포함한다. 여기서, 상기 제1레이어와 제3레이어는 태스크별 레이어이고 상기제2레이어는 태스크들간에 공유되는 레이어이다. 그리고 상기 제1레이어, 제2레이어, 및 제3레이어는 하나의 인 공신경망에서 순전파를 실행할 수 있다. 상술한 과제 해결 수단은 이후에 도면과 함께 설명하는 실시예를 통하여 더욱 명확해질 것이다."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에서 제안하는 기술은 종래 기술과 비교하여 다음과 같은 이점들을 갖는다. 1) 공유 레이어를 거치기 전, 개별 태스크마다 투영 인코더를 추가하고 이를 학습하기 위해 임의로 각 데이터셋 의 표본을 묶어 Data Augmentation을 수행하는 본 발명은 데이터 특성의 개수, 구조, 분포가 상이한 여러 데이 터셋을 한 모델로써 동시 학습을 가능하게 한다. 2) 1번의 특징에 따라 본 발명은 종래 기술에 비해 공통 학습을 위한 데이터셋 선정에 제약이 적다. 이로써 다 양한 데이터셋을 여럿 모아 데이터 특성을 풍부하게 학습시킬 수 있어, 데이터 규모가 작은 태스크가 포함되더 라도 좋은 성능을 기대할 수 있다. 3) 학습하고자 하는 여러 데이터셋 중 특정 몇몇에만 편향되어 학습이 이뤄지는 경우가 있는 종래의 기술과는 달리, 본 발명은 공유 레이어에서 각 태스크별로 산출되는 중간 결과물로부터 군집간 거리를 구하고 이를 최적 화하는 손실함수를 적용하여 일부 태스크의 성능 개선으로만 학습 방향이 쏠리지 않도록 유도한다."}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 바람직한 실시예를 첨부 도면을 참조하여 상세히 설명한다. 아래의 설명에서 사용된 용어는 본 발명의 바람직한 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것이 아니다. 본 명세서에서, 단수 형은 특별히 언급하지 않는 한 복수형도 포함한다. 또한 명세서에 사용된 '포함한다(comprise, comprising 등)'라는 용어는 언급된 구성요소, 단계, 동작, 및/또는 소자 이외의 하나 이상의 다른 구성요소, 단계, 동작, 및/또는 소자의 존재 또는 추가를 배제하지 않는 의미로 사용된 것이다. 본 발명의 일 실시예에 따른 '이종의 소규모 데이터셋의 학습 성능 고도화를 위한 멀티태스크 학습'을 수행하는 장치 및 방법의 작용을 다음과 같이 정리할 수 있다. 1) 각 태스크별 데이터를 동시 학습 가능한 입력 데이터 쌍으로 재구성 2) 상기 재구성된 입력 데이터 쌍을 하나의 특징공간(feature space)으로 투영(projection) 3) 투영된 특징벡터 쌍을 공유 레이어를 통과시켜 공통 특징을 추출 4) 공유 레이어를 거친 각각의 특징벡터를 원래 속한 태스크에 해당하는 추론 레이어로 배분하여 개별적인 추론 을 수행. 상기 2)~4)의 연산은 하나의 인공신경망 모델에서의 순전파 과정에 의해 순차적으로 진행될 수 있다. 상기 인공 신경망은 하나 혹은 둘 이상의 레이어로 구성된 심층신경망일 수 있다. 상기 인공신경망은 Fully-connected neuron, Convolutional neural network(CNN), Recurrent neural network(RNN), 또는 이와 유사한 구조를 가진신경망 구조를 포함할 수 있다. 본 발명에서 태스크는 분류, 회귀, 생성 등의 문제에 관련된 태스크들일 수 있다. 도 1은 본 발명의 일 실시예에 따른 이종의 소규모 데이터셋의 학습 성능 고도화를 위한 멀티태스크 학습 장치 및 방법을 구성하는 신경망의 블록도이다. 도 1에 도시된 신경망은 아래부터 위로 제1, 제2, 제3레이어가 3단계로 쌓인 형태를 갖는다. 도 1에서는 두 종 류의 태스크 A, B를 예시로 들어 설명하고 있으나, 본 발명은 3 종류 이상의 태스크 학습에도 적용이 가능하다. 입력 학습 데이터는 제1, 제2, 제3레이어로 순전파된다. 제1레이어는 투영 인코더(Projection Encoder)(10a,10b)를 포함할 수 있다. 제2레이어는 융합 인코더(Fusion Encoder)를 포함할 수 있다. 제3레 이어는 독립 분류기(Independent Classifier)(30a,30b)일 수 있다. 여기서, 융합 인코더는 태스크들에 공 유되는 레이어(shared layer)이고, 투영 인코더(10a,10b)와 독립 분류기(30a,30b)는 각 태스크마다 개별적으로 연산을 수행하는 레이어(task-specific layer)이다. 먼저 투영 인코더(10a,10b)는 각 태스크에 포함된 데이터 표본 에 개별적인 연산을 적용하여 모든 데이터가 같은 특징공간(feature space)에 투영(projection)되도록 하는 역할을 한다. 도 1에서는 상이한 구조의 데이터 가 모두 1차원 벡터 형태를 갖는 것으로 예시하고 있으나, 본 발 명에 의하면 이미지 정보와 같은 고차원의 데이터도 역시 학습 가능하다. 도 1에서 투영 인코더(10a,10b)를 거 친 결과물을 각각 로 표현했을 때, 이 두 특징벡터의 크기는 사용자가 정의한 상수 로 동일해야 한다. 다음으로 융합 인코더는 태스크 유형과는 관계없이 모든 데이터에 동일한 연산을 적용하여 데이터 간 공통 된 특징이 반영된 특징벡터 를 산출한다. 마지막으로 독립 분류기(30a,30b)는 상기 얻은 를 원래 입력 데이터가 속해 있던 태스크 그룹으로 분 리하고 태스크마다 개별적인 레이어 연산을 하여 최종 결과물 를 독립적으로 추론한다. 일부 실시예에 서, 독립 분류기(30a,30b)는 상이한 유형의 숫자 이미지로부터 해당 숫자가 0~9까지 중 어느 것에 속하는지를 판별할 수 있다. 상기 개략적으로 설명한 본 발명의 실시예에 따른 멀티태스크 학습 구조에 대해 보다 상세하게 설명한다. 학습하고자 하는 데이터는 제각기 다른 데이터 형태, 데이터 양, 특성 값 분포를 갖는다. 상기 예시로 들었던 태스크 A, B를 기준으로 이어 설명한다. 주어진 데이터셋 와 는 각각 n, m 개의 데이터로 구성되어 있다. 여기서 상기 데이터셋 간의 이질성을 보 장하기 위해, 입력 데이터 와 는 특징공간이 다르다는 조건 을 만족해야 한다. 일부 실시예에서, 각 데이터셋별로 클래스 간의 비중 및 각 특성 값 의 분포가 다를 수 있다. 이렇듯 구조적으로 상이한 데이터셋을 하나의 모델로 학습하기 위해서 일차적으로 각 데이터를 동일한 특징공간 으로 투영시키는 과정이 수행된다. 투영 인코더(10a,10b)의 학습이 태스크별로 동시에 이뤄져야 하기에, 두 데 이터셋의 표본 의 쌍을 맞춰주는 작업이 선행되는 것이다. 이 작업은 Data augmentation 기법으로 행 할 수 있다.도 2는 이질적인 데이터셋으로부터 모델 학습을 위한 데이터 쌍을 생성하는 Data augmentation 개념을 도시한 것이다. 각 데이터셋의 샘플 수가 다르고 의 k번째 데이터와 의 k번째 데이터는 직접적으로 상관관계가 존재하지 않으므로 각 데이터셋에서 무작위로 추출한 데이터를 짝으로 맞춘다. 이 때, 각 데이터셋마다 모든 데 이터가 적어도 하나 이상의 입력 데이터 쌍에 포함될 수 있도록 한다. 즉, 도 2에서처럼 각 데이터 표본 하 나하나마다 다른 데이터셋에서 복원추출한 표본 를 쌍으로 맺어준다. 상기 예에서는 데이터셋 A에 속한 n개 의 표본으로부터 입력 데이터쌍 ( 는 1~n 사이에서 임의로 추출한 정수)을 얻을 수 있으며, 반대로 데이터 셋 B에 속한 m개의 표본으로부터 입력 데이터쌍 ( 는 1~m 사이에서 임의로 추출한 정수)을 얻을 수 있다. 결과적으로 모델은 두 종류의 입력 데이터쌍을 통합한 n+m 쌍의 입력 데 이터쌍을 학습 데이터셋으로 갖게 된다. 다음에, 상기 Data Augmentation 과정을 통해 생성한 입력 데이터쌍을 투영 인코더(10a,10b)를 거쳐 개별적으로 압축된 특징벡터를 추출하는 과정이 따른다. 추출된 특징벡터는 이후 모든 태스크가 공유하는 융합 인코더 라는 레이어로 순전파된다. 도 3은 입력 데이터쌍이 투영 인코더(10a,10b)와 융합 인코더를 순차적으로 거쳐 개별 및 공통 특징이 추출 된 잠재벡터(latent vector) 와 를 연산하는 과정을 나타낸다. 투영 인코더(10a,10b)는 각 태스크 입력 데이터에 대한 차원 축소 및 융합 인코더 학습을 위한 특징공간 통일의 목표를 갖는다. 투영 인코더 (10a,10b)는 태스크마다 할당된 별개의 웨이트 매트릭스 의 연산을 적용하여 중간 산물인 와 를 계산한다. 그에 반해 융합 인코더는 동일한 웨이트 매트릭스 V를 상기 중간 산물에 적용하여 및 를 계산한다. 본 발명의 멀티태스크 학습 장치의 마지막 레이어는 개별 태스크 추론을 위한 신경망으로 구성된다. 도 4는 융합 인코더를 거쳐 추출된 잠재벡터를 해당되는 추론 태스크에 재할당하고 추론을 수행하는 과정을 나타낸다. 각 태스크마다 할당된 독립 분류기(30a,30b)를 거쳐 추론 결과 및 를 최종적으로 얻어내며, 이때 독립 분류기(30a,30b) 간 웨이트 파라미터는 서로 공유되지 않는다. 도 4는 두 태스크 모두가 분류 문제에 해당하는 것으로 예시하였으나, 다른 실시예에서는 회귀, 생성과 같은 다 른 태스크들의 혼합도 가능하다. 이하, 본 발명에서 제안하는 멀티태스크 학습의 최적화 강도 조절을 위한 역전파에 대해 설명한다. 이로써 공유 레이어 및 개별 태스크 추론 레이어 간의 학습 성능 불균형을 해소할 수 있다. 이를 위한 손실함수 및 역전파 과정을 도 5에 나타내었다. 본 발명에서 멀티태스크 학습 모델의 역전파에 사용하는 손실함수는 2가지이다. 첫 번째 손실함수는 각 태스크 별(task-specific) 추론 에러의 합산이다. 도 5에서 두 데이터셋 A와 B에 대한 추론 에러가 각각 라 할 때, 종합 추론 에러는 아래 수학식 1과 같이 표현된다.수학식 1"}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상기 식에서 CE는 크로스 엔트로피(cross entropy) 함수를, α는 전체 데이터 양에서 데이터셋 A가 차지하는 비 중을 의미한다. 두 번째 손실함수는 Pairwise Representation Loss(데이터쌍별 표현 손실) Δ이다. 이 손실함수 Δ는 융합 인코 더를 통해 산출된 특징벡터들이 태스크 유형과 상관없이 얼마나 유사한지를 나타낸다. 또한 이 손실함수는 이질적인 데이터셋을 투영 인코더(10a,10b)가 얼마나 비슷한 특징공간으로 투영시켰는지 알 수 있는 척도이다. 또한 이 손실함수는 투영 인코더(10a,10b)를 통해 얻은 투영벡터(projected vector) 의 공통 특징을 융합 인코더가 얼마나 잘 추출하였는지 알 수 있는 척도이기도 하다. Pairwise Representation Loss는 아래 수학식 2와 같이 표현될 수 있다. 수학식 2"}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2에서 태스크 간 잠재벡터(latent vector) 유사도는 δ와 같은 Gaussian distance 형태로 표현되었다. Pairwise Representation Loss Δ는 δ를 이용하여 전체 유사도를 0과 1 사이의 값으로 나타낸다. Δ가 0에 가 까울수록 각기 다른 태스크로부터 산출된 잠재벡터들의 유사도가 큼을 의미하며, 반대로 유사도가 적어질수록 Δ는 1에 가까운 값을 갖는다. 본 발명의 일 실시예에 따른 멀티태스크 학습 모델이 사용하는 상기 두 손실함수를 종합하여 아래 수학식 3과 같은 최종 손실함수로 역전파를 수행한다. 수학식 3"}
{"patent_id": "10-2023-0144354", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이상에서 설명한 본 발명의 다중학습 장치 및 방법은 도 6에 예시한 컴퓨터 시스템에 기반하여 구현될 수 있다. 도 6에 나타낸 컴퓨터 시스템(computer system)은, 공통버스(common bus)를 통해 통신하는 프로세서 (processor), 메모리(memory), 입력 인터페이스 장치(input interface device), 출력 인터페이스 장치(outputinterface device), 및 저장 장치(storage device) 중 적어도 하나를 포함할 수 있다. 컴퓨터 시스템은 또한 네트워크에 연결되는 통신 장치(communication device)를 포함할 수 있다. 프로세서는 CPU(central processing unit)이거나, 또는 메모리 또는 저장 장치에 저장된 명령을 실행하는 반도체 장치일 수 있다. 통신 장치는 유선 신호 또는 무선 신호를 송신 또는 수신할 수 있다. 메모리 및 저장 장치는 다양한 형태의 휘발성 또는 비휘발성 저장 매체를 포함할 수 있다. 그리고 메모리는 ROM(read only memory) 및 RAM(random access memory)를 포함할 수 있다. 메모리는 프로세서의 내부 또는 외부에 위치할 수 있고, 공지의 다양한 수단을 통해 프로세서와 연결 될 수 있다. 따라서, 본 발명은 컴퓨터에 구현된 방법으로 구현되거나, 컴퓨터 실행가능 명령이 저장된 비일시적 컴퓨터 판 독가능 매체로 구현될 수 있다. 한 실시예에서, 프로세서에 의해 실행될 때, 컴퓨터 판독가능 명령은 본 명세서 에 기재된 적어도 하나의 양태에 따른 방법을 수행할 수 있다. 또한, 본 발명에 따른 방법은 다양한 컴퓨터 수단을 통해 수행될 수 있는 프로그램 명령 형태로 구현되어, 컴퓨 터 판독가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 컴퓨터 판독 가능 매체에 기록되는 프로그램 명령은, 본 발 명의 실시예를 위해 특별히 설계되어 구성된 것이거나, 컴퓨터 소프트웨어 분야의 통상의 기술자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독가능 기록 매체는 프로그램 명령을 저장하고 수행하도록 구성된 하드 웨어 장치를 포함할 수 있다. 예를 들어, 컴퓨터 판독가능 기록 매체는 하드 디스크, 플로피 디스크 및 자기 테 이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크 (floptical disk)와 같은 자기광 매체(magneto-optical media), 롬(ROM), 램(RAM), 플래시 메모리 등일 수 있 다. 프로그램 명령은 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라, 인터프리터 등을 통해 컴 퓨터에 의해 실행될 수 있는 고급언어 코드를 포함할 수 있다. 이상에서 본 발명의 사상을 구체적으로 구현한 실시예를 설명하였다. 그러나 본 발명의 기술적 범위는 이상에서 설명한 실시예 및 도면에 한정되는 것이 아니라 특허청구범위의 합리적 해석에 의해 정해지는 것이다."}
{"patent_id": "10-2023-0144354", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 이종 데이터셋 학습을 위한 멀티태스크 학습 장치 및 방법의 구성도 도 2는 이종 데이터셋의 학습을 위한 입력 데이터 쌍을 생성하는 Data augmentation의 설명도 도 3은 개별 특징 및 공통 특징을 추출하기 위한 순전파 과정 설명도 도 4는 개별 태스크 수행을 위한 독립 분류기의 기능 설명도 도 5는 멀티태스크 학습의 역전파 과정 설명도 도 6은 본 발명의 구현 기반인 컴퓨터 시스템의 예시도"}
