{"patent_id": "10-2020-0034093", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0032266", "출원번호": "10-2020-0034093", "발명의 명칭": "전자 장치 및 이의 제어 방법", "출원인": "삼성전자주식회사", "발명자": "앱델패다 모하메드"}}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 가속기(Accelerator)와 복수의 신경망(Neural network)을 저장하는 메모리를 포함하는 전자 장치의 제어방법에 있어서,상기 복수의 신경망 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상기 제1 신경망을 구현할 제1 가속기를선택하는 단계;상기 제1 가속기 상에서 상기 제1 신경망을 구현(implement)하여 상기 구현 결과와 관련된 정보를 획득하는 단계;상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제1 신경망에 대한 제1 보상 값(reward value)을획득하는 단계;상기 복수의 신경망 중 상기 제1 가속기에서 구현할 제2 신경망을 선택하는 단계;상기 제1 가속기 상에서 상기 제2 신경망을 구현하여 상기 구현 결과와 관련된 정보를 획득하는 단계;상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제2 신경망에 대한 제2 보상 값을 획득하는단계; 및상기 제1 보상 값 및 상기 제2 보상 값을 바탕으로 상기 복수의 신경망 및 상기 복수의 가속기 중 가장 큰 보상값을 가지는 신경망 및 가속기를 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 제1 가속기를 선택하는 단계는,제1 예측 모델에 상기 제1 가속기 및 상기 제1 신경망을 입력하여 획득된 상기 제1 가속기 및 상기 제1 신경망의 하드웨어 성능이 제1 기설정된 기준을 만족하는지 여부를 식별하는 단계; 및상기 획득된 하드웨어 성능이 상기 제1 하드웨어 기준을 만족한다고 식별되면, 상기 제1 가속기 상에서 상기 제1 신경망을 구현하여 상기 구현과 관련된 정보를 획득하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 식별하는 단계는,상기 획득된 하드웨어 성능이 상기 제1 하드웨어 기준을 만족하지 못한다고 식별되면, 상기 제1 가속기를 제외한 가속기 중 상기 제1 신경망을 구현할 제2 가속기를 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 구현과 관련된 정보는 구현의 정확도 및 효율성 지표(metrics)를 포함하는 것을 특징으로 하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "상기 제1 보상 값을 획득하는 단계는,상기 획득된 정확도 및 효율성 지표를 정규화하는 단계; 및상기 정규화된 지표에 대해 가중 합(weighted sum) 연산을 수행하여 상기 제1 보상 값을 획득하는 단계;를 포함공개특허 10-2021-0032266-3-하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 복수의 신경망 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상기 제1 신경망을 구현할 제1 가속기를선택하는 단계는,상기 복수의 신경망 각각에 포함된 제1 구성 가능한 파라미터에 대응되는 제1 확률 값을 획득하는 단계; 및상기 복수의 신경망 중 상기 제1 확률 값에 따라 상기 제1 신경망을 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 제1 가속기를 선택하는 단계는,상기 복수의 가속기 각각에 포함된 제2 구성 가능한 파라미터에 대응되는 제2 확률값을 획득하는 단계; 및상기 제2 확률 값에 따라 상기 복수의 가속기 중 상기 제1 신경망이 구현될 상기 제1 가속기를 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 복수의 가속기 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상기 제1 신경망을 구현할 제1 가속기를선택하는 단계는,상기 제1 신경망을 선택한 후, 상기 제1 신경망을 구현할 상기 제1 가속기를 선택하기 전, 제2 예측 모델을 통해 상기 선택된 제1 신경망의 하드웨어 성능을 예측하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 예측하는 단계는,상기 예측된 제1 신경망의 하드웨어 성능이 제2 하드웨어 기준을 만족하는지 여부를 식별하고,상기 예측된 제1 신경망의 하드웨어 성능이 상기 제2 하드웨어 기준을 만족한다고 식별되면, 상기 제1 신경망을구현할 수 있는 상기 제1 가속기를 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서상기 식별하는 단계는,상기 선택된 제1 신경망의 하드웨어 성능이 상기 제2 하드웨어 기준을 만족하지 않는다고 식별되면, 상기 제1신경망을 제외한 복수의 신경망 중 하나의 신경망을 다시 선택하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "전자 장치에 있어서,복수의 가속기와 복수의 신경망을 저장하는 메모리; 및상기 복수의 신경망 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상기 제1 신경망을 구현할 제1 가속기를선택하고,상기 제1 가속기 상에서 상기 제1 신경망을 구현하여 상기 구현 결과와 관련된 정보를 획득하고,상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제1 신경망에 대한 제1 보상 값을 획득하고,공개특허 10-2021-0032266-4-상기 복수의 신경망 중 상기 제1 가속기에서 구현할 제2 신경망을 선택하고,상기 제1 가속기 상에서 상기 제2 신경망을 구현하여 상기 구현 결과와 관련된 정보를 획득하고,상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제2 신경망에 대한 제2 보상 값을 획득하고,상기 제1 보상 값 및 상기 제2 보상 값을 바탕으로 상기 복수의 신경망 및 상기 복수의 가속기 중 가장 큰 보상값을 가지는 신경망 및 가속기를 선택하는 프로세서;를 포함하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 프로세서는,제1 예측 모델에 상기 제1 가속기 및 상기 제1 신경망을 입력하여 획득된 상기 제1 가속기 및 상기 제1 신경망의 하드웨어 성능이 제1 기설정된 기준을 만족하는지 여부를 식별하고,상기 획득된 하드웨어 성능이 상기 제1 하드웨어 기준을 만족한다고 식별되면, 상기 제1 가속기 상에서 상기 제1 신경망을 구현하여 상기 구현과 관련된 정보를 획득하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서, 상기 프로세서는,상기 획득된 하드웨어 성능이 상기 제1 하드웨어 기준을 만족하지 못한다고 식별되면, 상기 제1 가속기를 제외한 가속기 중 상기 제1 신경망을 구현할 제2 가속기를 선택하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 구현과 관련된 정보는 구현의 정확도 및 효율성 지표(metrics)를 포함하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 프로세서는,상기 획득된 정확도 및 효율성 지표를 정규화하고,상기 정규화된 지표에 대해 가중 합 연산을 수행하여 상기 제1 보상 값을 획득하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11에 있어서,상기 프로세서는,상기 복수의 신경망 각각에 포함된 제1 구성 가능한 파라미터에 대응되는 제1 확률 값을 획득하고,상기 복수의 신경망 중 상기 제1 확률 값에 따라 상기 제1 신경망을 선택하는 전자 장치"}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서,상기 프로세서는,상기 복수의 가속기 각각에 포함된 제2 구성 가능한 파라미터에 대응되는 제2 확률값을 획득하고,상기 제2 확률 값에 따라 상기 복수의 가속기 중 상기 제1 신경망이 구현될 상기 제1 가속기를 선택하는 전자장치.공개특허 10-2021-0032266-5-청구항 18 제11항에 있어서,상기 프로세서는,상기 제1 신경망을 선택한 후, 상기 제1 신경망을 구현할 상기 제1 가속기를 선택하기 전, 제2 예측 모델을 통해 상기 선택된 제1 신경망의 하드웨어 성능을 예측하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 프로세서는,상기 예측된 제1 신경망의 하드웨어 성능이 제2 하드웨어 기준을 만족하는지 여부를 식별하고,상기 예측된 제1 신경망의 하드웨어 성능이 상기 제2 하드웨어 기준을 만족한다고 식별되면, 상기 제1 신경망을구현할 수 있는 상기 제1 가속기를 선택하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19항에 있어서상기 프로세서는,상기 선택된 제1 신경망의 하드웨어 성능이 상기 제2 하드웨어 기준을 만족하지 않는다고 식별되면, 상기 제1신경망을 제외한 복수의 신경망 중 하나의 신경망을 다시 선택하는 전자 장치."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치 및 이의 제어 방법이 개시된다. 본 개시의 전자 장치는 복수의 가속기와 복수의 신경망을 저장하는 메 모리 및 복수의 신경망 중 제1 신경망을 선택하고, 복수의 가속기 중 제1 신경망을 구현할 제1 가속기를 선택하 고, 제1 가속기 상에서 제1 신경망을 구현하여 구현 결과와 관련된 정보를 획득하고, 구현과 관련된 정보를 바탕 으로 제1 가속기 및 제1 신경망에 대한 제1 보상 값을 획득하고, 복수의 신경망 중 제1 가속기에서 구현할 제2 신경망을 선택하고, 제1 가속기 상에서 제2 신경망을 구현하여 구현 결과와 관련된 정보를 획득하고, 구현과 관 련된 정보를 바탕으로 제1 가속기 및 제2 신경망에 대한 제2 보상 값을 획득하고, 제1 보상 값 및 제2 보상 값을 바탕으로 복수의 신경망 및 복수의 가속기 중 가장 큰 보상 값을 가지는 신경망 및 가속기를 선택하는 프로세서 를 포함할 수 있다."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 이의 제어 방법에 관한 것으로, 더욱 상세하게는 최적의 정확도 및 효율성 지표를 출력 할 수 있는 한 쌍의 가속기 및 신경망을 결정하는 전자 장치 및 이의 제어 방법에 관한 것이다."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "FPGA 가속기는 low-batch DNN 추론 작업, 맞춤형 하드웨어(HW) 구성 및 성김(sparsity) 또는 맞춤형 정밀도와 같은 DNN의 특정 속성들에 맞춰질 때 효과적일 수 있다. 그리고, FPGA 가속기의 경우, 맞춤형 ASIC과 비교할 때 HW 설계 주기가 상대적으로 짧다는 장점이 있다. 다만, FPGA 가속기는 일반적으로 가속기 상에서 구현될 인공 신경망(예를 들어, DNN)이 결정되어 고정된 후에 설계된다는 한계가 존재한다. 예를 들어, DNN 이 설계(또는, 디자인)되고 난 뒤에, DNN이 구현될 FPGA 가속기가 설계될 수 있다. 가속기가 소프트웨어로 프로그래밍이 가능하더라도, 가속기의 하드웨어는 일반적으로 특정 DNN의 효율이 극대화 할 수 있도록 과도 최적화(over-optimized)될 수 있다. 결과적으로, 상이한 DNN들이 동일한 가속기에서 구현될 경우, 비효율적인 결과를 도출할 수 있다는 단점이 있다. 과도 최적화 문제를 해결하기 위하여, FPGA 설계(또는, 디자인)는 일반적으로 HW 수준에서 구성할 수 있다. 이 경우, 새로운 DNN이 발견 또는 개발되면, 가 속기 하드웨어의 파라미터를 새로운 DNN에 맞게 조정하여 HW 효율을 극대화할 수 있다. 다만, 위 기술의 경우, 하드웨어 구성 기능이 있더라도, FPGA 가속기는 항상 새로운 DNN에 맞게 조정되어야 한다는 단점이 있다. 한편, DNN을 설계하는 방식은 자동화될 수 있으며, NAS(neural architecture search)로 지칭될 수 있다. NAS 는 이미지 분류, 초고해상도, 음성 인식 및 기계 번역에서 최첨단 정확도를 달성하는 DNN 모델들을 성공적으로 발견해 왔다. 그리고, FNAS로 불리는 추가 개발이 Jiang 등에 의해서 arXiv e-prints (Jan 2019)에 게재된 \"Accuracy vs. Efficiency: Achieving Both Through FPGA-Implementation Aware Neural Architecture Search\"에 기술되어 있다. FNAS는 HW 인식 NAS이며, 이것은 주어진 FPGA 가속기에서 대기 시간을 최소화하는 DNN을 발견하기 위해 사 용되었다. FNAS는 특정 FPGA 가속기에 적합한 CNN(Convolutional Neural Network)을 성공적으로 발견할 수 있 다. 그리고, FNAS는 보상 기능에 대기 시간을 추가함으로써 발견된 모델들이 모바일 장치에서 실행될 때 정확도 와 추론 대기 시간을 모두 최적화할 수 있다. 한편, 기존 CPU와 GPU의 경우, 알고리즘은 기존 HW에 맞게 최적화되어 있으며 성공적인 ASIC을 위해, 미래-보장 정확도를 달성하도록 많은 유연성과 프로그래밍 기능을 내장해야 한다는 한계가 존재하였다. 본 개시의 일 실시예에 따른, 복수의 가속기와 복수의 신경망을 저장하는 메모리를 포함하는 전자 장치의 제어 방법에 있어서, 상기 복수의 가속기 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상기 제1 신경망을 구현 할 제1 가속기를 선택하는 단계, 상기 제1 가속기 상에서 상기 제1 신경망을 구현(implement)하여 상기 구현 결 과와 관련된 정보를 획득하는 단계, 상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제1 신경망 에 대한 제1 보상 값(reward value)을 획득하는 단계, 상기 복수의 신경망 중 상기 제1 가속기에서 구현할 제2 신경망을 선택하는 단계, 상기 제1 가속기 상에서 상기 제2 신경망을 구현하여 상기 구현 결과와 관련된 정보를 획득하는 단계, 상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제2 신경망에 대한 제2 보상 값 을 획득하는 단계 및 상기 제1 보상 값 및 상기 제2 보상 값을 바탕으로 상기 복수의 신경망 및 상기 복수의 가 속기 중 가장 큰 보상 값을 가지는 신경망 및 가속기를 선택하는 단계를 포함할 수 있다. 그리고, 본 개시의 일 실시예에 따른, 전자 장치는 복수의 가속기와 복수의 신경망을 저장하는 메모리 및 프로 세서를 포함하고, 상기 프로세서는, 상기 복수의 가속기 중 제1 신경망을 선택하고, 상기 복수의 가속기 중 상 기 제1 신경망을 구현할 제1 가속기를 선택하고, 상기 제1 가속기 상에서 상기 제1 신경망을 구현하여 상기 구 현 결과와 관련된 정보를 획득하고, 상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제1 신경망 에 대한 제1 보상 값을 획득하고, 상기 복수의 신경망 중 상기 제1 가속기에서 구현할 제2 신경망을 선택하고, 상기 제1 가속기 상에서 상기 제2 신경망을 구현하여 상기 구현 결과와 관련된 정보를 획득하고, 상기 구현과 관련된 정보를 바탕으로 상기 제1 가속기 및 상기 제2 신경망에 대한 제2 보상 값을 획득하는 단계 및 상기 제1 보상 값 및 상기 제2 보상 값을 바탕으로 상기 복수의 신경망 및 상기 복수의 가속기 중 가장 큰 보상 값을 가 지는 신경망 및 가속기를 선택할 수 있다."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부된 도면을 참조하여 본 개시에 대해 구체적으로 설명하도록 한다. 도 1은 본 개시의 일 실시예에 따른, 전자 장치의 구성 및 동작을 설명하기 위한 블록도이다. 도 1에 도시 된 바와 같이, 전자 장치는 메모리 및 프로세서를 포함할 수 있다. 다만, 도 1에 도시된 구성은 본 개시의 실시예들을 구현하기 위한 예시도이며, 통상의 기술자에게 자명한 수준의 적절한 하드웨어 및 소프트 웨어 구성들이 전자 장치에 추가로 포함될 수 있다. 메모리는 전자 장치의 적어도 하나의 다른 구성요소에 관계된 인스트럭션(instruction) 또는 데이터 를 저장할 수 있다. 인스트럭션은 프로그램 작성 언어에서 프로세서가 직접 실행할 수 있는 하나의 동작 문장(action statement)을 의미하며, 프로그램의 실행 또는 동작에 대한 최소 단위이다. 그리고, 메모리는 프로세서에 의해 액세스되며, 프로세서에 의한 데이터의 판독(readout)/기록/수정/삭제/갱신 등이 수 행될 수 있다. 그리고, 메모리는 복수의 가속기(10-1, 10-2, … 10-N) 및 복수의 신경망(20-1, 20-2, … 20-N)를 저장할 수 있다. 구체적으로, 메모리에는 복수의 가속기(10-1, 10-2, … 10-N)를 포함하는 가속기 서브 검색 공간 및 복수의 신경망(20-1, 20-2, … 20-N)을 포함하고 있는 신경망 서브 검색 공간을 저장할 수 있다. 전체 검색 공간은 하기 수학식 1과 같이 정의될 수 있다.수학식 1"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "SNN은 신경망에 대한 서브 검색 공간이고, SFPGA는 FPGA에 대한 서브 검색 공간이다. 한편, 가속기가 FPGA가 아닌 다른 종류의 가속기로 구현된 경우, 메모리는 구현된 종류의 가속기를 검색하고 선택할 수 있는 서브 검색 공간을 저장할 수 있음은 물론이다. 그리고, 메모리에 저장된 각 검색 공간에 프로세서는 액세스하여 신경망 또는 가속기를 검색하고 선택할 수 있다. 이와 관련된 실시예는 후술하는 부분에서 설명하도록 한다. 신경망(또는, 인공 신경망(Artificial Neural Network))은 인공 지능 알고리즘을 이용하여 입력된 데이터를 처 리할 수 있는 모델이다. 신경망은 복수의 레이어를 포함할 수 있으며, 레이어는 신경망의 각 단계를 의미할 수 있다. 신경망에 포함된 복수의 레이어는 복수의 가중치(weight values)을 갖고 있으며, 이전(previous) 레이어 의 연산 결과와 복수의 가중치의 연산을 통해 레이어의 연산을 수행할 수 있다. 구체적으로, 신경망은 여러 개 의 레이어의 조합으로 구성될 수 있으며, 레이어는 복수개의 가중치로 표현될 수 있다. 신경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트워크 (Deep Q-Networks)이 있다. 그리고, CNN은 conv1x1, conv3x3 및 pool3x3에서 선택된 서로 다른 블록들로 구성될 수 있다. 또 다른 예로, 신경망은 LZ77 압축 및 허프만 (Huffman) 인코딩을 수행하는 두 가지 주요 계산 블록(computation block)을 포함하는 알고리즘인 GZIP 압축 방 식의 신경망을 포함할 수 있다. 한편, LZ77 계산 블록은 압축 창 크기 및 최대 압축 길이와 같은 파라미터들을 포함한다. 허프만 계산 블록은 허프만 트리 크기, 트리 업데이트 빈도 등과 같은 파라미터들을 갖는다. 이 파라 미터들은 GZIP 압축 알고리즘의 최종 결과에 영향을 주며, 일반적으로 압축 비율과 압축 속도에는 트레이드 오 프(trade-off)가 존재할 수 있다. 그리고, 복수의 신경망 각각은 제1 구성 가능한 파라미터(configurable parameter)를 포함할 수 있다. 복수의 신경망 각각의 하드웨어 또는 소프트웨어 특성은 신경망 각각에 포함한 구성 가능한 파라미터에 대응되는 수치 (또는, 가중치)에 의해 결정될 수 있다. 제1 구성 가능한 파라미터는 각 신경망의 연산 방식, 또는 레이어 연결 방식 중 적어도 하나를 포함할 수 있다. 연산 방식은 신경망에 포함된 레이어 간에 수행되는 연산의 종류, 횟수 등을 포함할 수 있다. 그리고, 레이어 연결 방식은 각 연산망에 포함된 레이어의 개수, 레이어에 포함된 스택 또는 셀의 개수, 레이어 간의 연결 관계 등을 포함할 수 있다. 그리고, 가속기는 인공 지능 알고리즘 바탕으로 학습된 신경망(Neural Network)이 처리할 수 있는 데이터의 양 또는 처리 속도를 증가시킬 수 있는 하드웨어 장치이다. 일 예로, 가속기는 뉴럴 네트워크를 구현하기 위한 플 랫폼인 FPGA(Field-Programmable Gate-Array) 가속기 또는 ASIC(application-specific integrated circuit) 등으로 구현될 수 있다. 그리고, 복수의 가속기 각각은 제2 구성 가능한 파라미터(configurable parameter)를 포함할 수 있다. 복수의 가속기 각각의 하드웨어 또는 소프트웨어 특성은 각각이 포함하고 있는 제2 구성 가능한 파라미터에 대응되는 수치에 따라 결정될 수 있다. 복수의 가속기 각각이 포함하고 있는 제2 구성 가능한 파라미터는 병렬화 파라미 터(parallelization parameter) (예를 들어, 병렬 출력 기능들 또는 병렬 출력 픽셀들), 버퍼 깊이(예를 들어, 입력, 출력 및 가중치 버퍼들에 대한 버퍼 깊이), 풀링 엔진 파라미터(pooling engine parameter), 메모리 인터 페이스 폭 파라미터(memory interface width parameter) 및 컨볼루션 엔진 비율 파라미터 중 적어도 하나를 포 함할 수 있다. 그리고, 메모리는 평가 모델을 저장할 수 있다. 평가 모델은 프로세서에 의해 선택된 가속기 및 신경망에 대한 보상 값을 출력할 수 있는 인공지능 모델이며, 프로세서에 의해 제어될 수 있다. 구체적 으로, 평가 모델은 선택된 가속기 상에서 선택된 신경망이 구현됨으로써 획득되는 구현과 관련된 정보(예를 들어, 정확도 지표(metrics) 및 효율성 지표)에 대해 정규화(normalization)를 수행할 수 있다. 그리고, 평가 모델은 정규화된 정확도 지표 및 효율성 지표에 대해 가중 합(weighted sum) 연산을 수행하여 보상 값을 출력할 수 있다. 평가 모델이 각 지표를 정규화하고, 이에 대해 가중 합 연산을 수행하는 과정은 후술하는 부분에서 구체적으로 설명하도록 한다. 평가 모델이 출력한 한 쌍의 가속기 및 신경망에 대한 보상 값이 클수록, 한 쌍의 가속기 및 신경망은 더욱 정확하고 효율적인 구현 및 연산을 수행할 수 있다. 한편, 평가 모델은 정확도 지표 및 효율성 지표 각각에 대응되는 임계값을 통해 평가 모델이 출력할 수 있는 값을 제한할 수 있다. 예를 들어, 평가 모델이 보상값을 출력하기 위해 정확도 지표 및 효율성 지표 에 적용하는 알고리즘은 수학식 2와 같이 구현될 수 있다. 수학식 2"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2에서 m은 정확도 지표 또는 효율성 지표의 벡터이고, w는 m의 가중치 벡터이며, th는 m의 임계값 벡터 이다. 그리고, 평가 모델은 하기 수학식 3을 이용하여 보상 값을 출력할 수 있다. 수학식 3"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3에서 ar은 가속기의 면적, lat는 대기 시간, acc는 정확도 수치이며, w1, w2, w3은 각 면적, 대기 시간 및 정확도에 대한 가중치 세트이다. 그리고, 검색 공간 sεS에 대해 최적화가 수행되면, 평가 모델 출력 Ε (s)=m은 주어진 제약 조건(예를 들면, 특정 값 미만의 대기 시간)을 충족한다. 한편, 정확도 지표는 가속기 상에서 신경망이 어느 정도로 정확하게 구현되었는지를 나타내는 수치를 의미한다. 효율성 지표는 가속기 상에서 신경망이 어느 정도로 최적화된 구현을 수행할 수 있는지를 나타내는 수치를 의미 한다. 한편, 효율성 지표는 가속기 상에 신경망이 구현될 때 가속기의 레이턴시(latency) 지표, 전력(power) 지 표 및 가속기의 면적(area) 지표 중 적어도 하나를 포함할 수 있다. 한편, 메모리는 제1 예측 모델 및 제2 예측 모델을 포함할 수 있다. 제1 예측 모델은 입력된 가속기 및 신경망에 대응되는 하드웨어 성능의 추정값을 출력할 수 있는 인공지능 모델이다. 제1 가속기 및 제1 신경망에 대응되는 하드웨어 성능은 제1 가속기 상에 제1 신경망이 구현될 때 소요되는 레이턴시 또는 전력을 포함할 수 있다. 즉, 제1 예측 모델은 제1 가속기 상에서 제1 신경망이 구현될 때 소요될 수 있는 레이턴시 또는 전력의 추정값을 출력할 수 있다. 제1 하드웨어 기준은 제1 예측 모델의 설계 당시에 기설정된 값일 수 있으나, 프로세서에 의해 업데이트 될 수 있다. 제1 예측 모델과 관련된 실시예는 후술하는 부분에 서 구체적으로 설명하도록 한다. 제2 예측 모델은 신경망에 대응되는 하드웨어 성능의 추정값을 출력할 수 있는 인공지능 모델이다. 구체적 으로, 제1 신경망이 입력되면, 제2 예측 모델은 제1 신경망에 대응되는 하드웨어 성능의 추정값을 출력할 수 있다. 제1 신경망에 대응되는 하드웨어 성능의 추정값은 제1 신경망이 특정 가속기에서 구현될 때 소요될 것으로 예측되는 레이턴시, 제1 신경망의 메모리 풋 프린트(memory footprint) 중 적어도 하나를 포함할 수 있다. 제1 신경망의 메모리 풋 프린트는 제1 신경망이 메모리 또는 제1 가속기 상에서 차지하고 있는 공간의 크 기를 의미할 수 있다. 제2 예측 모델과 관련된 실시예는 후술하는 부분에서 구체적으로 설명하도록 한다. 한편, 제1 예측 모델 및 제2 예측 모델은 프로세서에 의해 제어될 수 있다. 또한, 각 모델은 프로 세서에 의해 학습될 수 있다. 예를 들어, 프로세서는 제1 예측 모델에 제1 가속기 및 제1 신경망을 입력하여 제1 가속기 및 제1 신경망의 하드웨어 성능의 추정값을 획득할 수 있다. 그리고, 프로세서는 실 제 제1 가속기 상에서 제1 신경망을 구현하였을 때 획득할 수 있는 하드웨어 성능 값과 획득된 추정값의 차이가 최소화될 수 있는 최적의 추정 값을 출력하도록 제1 예측 모델을 학습시킬 수 있다. 그리고, 예를 들어, 프로세서는 제2 예측 모델에 제1 신경망을 입력하여 제1 신경망의 하드웨어 성능 의 추정값을 획득할 수 있다. 그리고, 프로세서는 실제 제1 신경망이 특정 가속기에 구현되었을 때 제1 신 경망을 통해 획득할 수 있는 하드웨어 성능 값과 획득된 추정값의 차이가 최소화될 수 있는 최적의 추정 값을출력하도록 제2 예측 모델을 학습시킬 수 있다. 그리고, 메모리는 Policy Function 모델을 포함할 수 있다. Policy Function 모델은 신경망 및 가속기 각각에 포함된 구성 가능한 파라미터에 대응되는 확률 값을 출력할 수 있는 인공지능 모델이며, 프로세 서에 의해 제어될 수 있다. 일 실시예로, 복수의 신경망이 입력되면, Policy Function 모델은 신경망 각각에 포함된 제1 구성 가능한 파라미터에 policy function을 적용하여 제1 구성 가능한 파라미터 각각에 대응 되는 확률 값을 출력할 수 있다. policy function은 구성 가능한 파라미터 중 높은 보상 값을 출력할 수 있게 하는 파라미터에 대해 높은 확률 값을 부여할 수 있는 함수로 복수의 파라미터를 포함할 수 있다. 그리고, Policy function에 포함된 복수의 파라미터는 프로세서 제어에 의해 업데이트 될 수 있다. 그리고, 제1 구성 가능한 파라미터에 대응되는 확률 값은 제1 구성 가능한 파라미터를 포함하는 신경망이 다른 신경망에 비해 높은 보상 값을 출력할 수 있는 신경망인지에 대한 확률 값을 의미할 수 있다. 예를 들어, 제1 구성 가능한 파라미터가 연산 방식이고, 제1 신경망이 제1 연산 방식을 수행할 수 있고, 제2 신경망이 제2 연산 방식을 수행할 수 있는 경우를 가정한다. 그리고, 제1 신경망 및 제2 신경망이 입력되면, policy function 모델 은 각 신경망에 포함된 연산 방식에 policy function을 적용하여 각 연산 방식에 대응되는 확률 값을 출력 할 수 있다. 제1 연산 방식에 대응되는 확률이 40%이고, 제2 연산 방식에 대응되는 확률이 60%인 경우는 프로세 서가 복수의 신경망 중 제1 연산 방식을 포함하는 제1 신경망을 선택할 확률이 40%이고, 제2 연산 방식을 포함하는 제2 신경망을 선택할 확률이 60%인 경우를 의미할 수 있다. 또 다른 실시예로, 복수의 가속기가 입력되면, Policy Function 모델은 가속기 각각에 포함된 제2 구성 가 능한 파라미터에 policy function을 적용하여 제2 구성 가능한 파라미터 각각에 대응되는 확률 값을 출력할 수 있다. 제2 구성 가능한 파라미터에 대응되는 확률 값은 제2 구성 가능한 파라미터를 포함하는 가속기가 다른 가 속기에 비해 높은 보상 값을 출력할 수 있는 가속기인지에 대한 확률 값을 의미할 수 있다. 예를 들어, 가속기 에 포함된 제2 구성 가능한 파라미터가 컨볼루션 엔진 비율 파라미터이고, 제1 가속기가 컨볼루션 엔진 비율 파 라미터를 포함하고, 제2 신경망이 제2 가속기가 컨볼루션 엔진 비율 파라미터를 포함한 경우, 제1 가속기 및 제 2 가속기가 입력되면, policy function 모델은 제1 및 제2 컨볼루션 엔진 비율 파라미터 각각을 포함하는 가속기에 policy function을 적용하여 각 컨볼루션 엔진 비율 파라미터에 대응되는 확률 값을 출력할 수 있다. 제1 컨볼루션 엔진 비율 파라미터를 선택할 확률이 40%이고, 제2 컨볼루션 엔진 비율 파라미터를 선택할 확률이 60%인 경우는 프로세서가 복수의 가속기 중 제1 컨볼루션 엔진 비율 파라미터를 포함하는 제1 가속기를 선 택할 확률이 40%이고, 제2 컨볼루션 엔진 비율 파라미터를 포함하는 제2 가속기를 선택할 확률이 60%인 경우를 의미할 수 있다. 한편, 평가 모델, 제1 예측 모델, 제2 예측 모델 및 Policy function 모델은 비휘발성 메모리 에 저장되어 있다가 프로세서 제어에 의해 휘발성 메모리로 로딩될 수 있다. 휘발성 메모리는 도 1에 도시 된 바와 같이 프로세서에 일 구성요소로서 프로세서에 포함될 수 있으나 이는 일 실시예에 불과하며, 프로세서와 별개의 구성 요소로 구현될 수 있다. 휘발성 메모리는 전력 공급이 중단되더라도 저장된 정보를 유지할 수 있는 메모리를 말한다. 예를 들어, 비휘발 성 메모리는 플래시 메모리(Flash Memory), PROM(Programmable Read-Only Memory), MRAM(Magnetoresistive Random-Access Memory) 및 RRAM(Resistive RAM) 중 적어도 하나를 포함할 수 있다. 휘발성 메모리는 저장된 정 보를 유지하기 위해서는 지속적인 전력 공급이 필요한 메모리를 말한다. 예를 들어, 휘발성 메모리는 DRAM(Dynamic Random-Access Memory) 및 SRAM(Static RAM) 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리와 전기적으로 연결되어 전자 장치의 전반적인 동작을 제어할 수 있다. 특히, 프로세서는 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써, 신경망 서브 검색 공간에 저 장된 복수의 신경망 중 하나를 선택할 수 있다. 구체적으로, 프로세서는 메모리에 저장된 신경망 서 브 검색 공간에 액세스할 수 있다. 그리고, 프로세서는 신경망 서브 검색 공간에 포함된 복수의 신경망을 Policy function 모델에 입력하여 복수의 신경망 각각에 포함된 제1 구성 가능한 파라미터에 대응되는 확률 값을 획득할 수 있다. 예를 들어, 제1 구성 가능한 파라미터가 레이어의 연결 방식인 경우, 프로세서는 복 수의 신경망을 Policy function 모델에 입력하여 복수의 신경망 각각의 레이어 연결 방식에 대응되는 확률 값을 획득할 수 있다. 제1 신경망 및 제2 신경망 각각의 레이어 연결 방식에 대응되는 확률 값이 각각 60% 및 40%인 경우, 프로세서는 복수의 신경망 중 제1 신경망 및 제2 신경망을 각각 60% 및 40%의 확률로 선택할 수 있다. 그리고, 프로세서는 복수의 가속기 중 선택된 신경망을 구현할 가속기를 선택할 수 있다. 구체적으로, 프 로세서는 메모리에 저장된 가속기 서브 검색 공간에 액세스할 수 있다. 그리고, 프로세서는 가 속기 서브 검색 공간에 저장된 복수의 가속기를 Policy function 모델에 입력하여 복수의 가속기 각각이 포 함하는 제2 구성 가능한 파라미터에 대응되는 확률 값을 획득할 수 있다. 예를 들어, 제2 구성 가능한 파라미터 가 병렬화 파라미터인 경우, 프로세서는 복수의 가속기를 Policy function 모델에 입력하여 복수의 가 속기 각각에 포함된 병렬화 파라미터에 대응되는 확률 값을 획득할 수 있다. 제1 가속기 및 제2 가속기 각각이 포함하는 병렬화 파라미터에 대응되는 확률 값이 각각 60% 및 40%인 경우, 프로세서는 복수의 가속기 중 제1 가속기 및 제2 가속기 각각 60% 및 40%의 확률로 제1 신경망을 구현할 가속기로 선택할 수 있다. 한편, 본 개시의 일 실시예로, 복수의 신경망 중 제1 신경망을 선택한 경우, 프로세서는 복수의 가속기 중 제1 신경망을 구현할 가속기를 선택하기 전에 제2 예측 모델을 통해 제1 신경망에 대응되는 하드웨어 성능 의 추정값을 획득할 수 있다. 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하지 않을 경우, 프로세서는 제1 신경망을 제외한 복수의 신경망 중 하나를 다시 선택할 수 있다. 구체적으로, 프로세서는 제1 신경망을 제2 예측 모델에 입력하여 제1 신경망에 대응되는 하드웨어 성능의 추정값을 획득할 수 있다. 제1 신경망에 대응되는 하드웨어 성능의 추정값은 제1 신경망이 특정 가속기에서 구현될 때 소 요될 것으로 예측되는 레이턴시 또는 제1 신경망의 메모리 풋 프린트 중 적어도 하나를 포함할 수 있다. 그리고, 프로세서는 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하는지 여부를 식별할 수 있다. 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족한다고 식별되면, 프로세서는 복수의 가속기 중 제1 신경망을 구현할 가속기를 선택할 수 있다. 한편, 제1 신경망에 대응되 는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하지 못한다고 식별되면, 프로세서는 제1 신경망을 제외한 복수의 신경망 중 하나의 신경망을 선택할 수 있다. 제1 신경망에 대응되는 하드웨어의 성능이 제2 하드 웨어 기준을 만족하지 못한다는 것은 제1 신경망을 통해서는 높은 보상 값을 획득하지 못한다는 것을 의미할 수 있다. 따라서, 제1 신경망의 하드웨어 성능이 제2 하드웨어 기준을 만족하지 못한다고 식별되면, 프로세서(12 0)는 제1 신경망을 제외함으로써 불필요한 동작을 최소화할 수 있다. 다만, 이는 일 실시예에 불과하며, 프로세 서는 복수의 신경망 중 제1 신경망을 선택한 뒤에 바로 복수의 가속기 중 제1 신경망을 구현할 제1 가속기 를 선택할 수 있다. 또 다른 실시예로, 복수의 신경망 중 제1 신경망을 선택하고, 복수의 가속기 중 제1 신경망이 구현될 제1 가속 기가 선택된 경우, 프로세서는 제1 예측 모델에 제1 가속기 및 제1 신경망을 입력하여 제1 가속기 및 제1 신경망에 대응되는 하드웨어 성능의 추정값을 획득할 수 있다. 제1 가속기 및 제1 신경망에 대응되는 하드 웨어 성능은 제1 가속기 상에 제1 신경망이 구현될 때 소요되는 레이턴시 또는 전력을 포함할 수 있다. 그리고, 프로세서는 획득된 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족하는지 여부를 식별할 수 있다. 획득된 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족한다고 식별되면, 프로세서는 제1 가속 기 상에서 제1 신경망을 구현하고, 구현과 관련된 정보를 획득할 수 있다. 한편, 획득된 하드웨어 성능이 제1 하드웨어 기준을 만족하지 않는다고 식별되면, 프로세서는 제1 가속기를 제외한 복수의 가속기 중 제1 신 경망을 구현할 다른 가속기를 선택할 수 있다. 제1 신경망 및 제1 가속기의 하드웨어 성능이 제1 하드웨어 기준 을 만족하지 못한다는 것은 제1 가속기 상에서 제1 신경망을 구현하여 획득한 구현과 관련된 정보를 통해서는 높은 보상 값을 획득하지 못한다는 것을 의미할 수 있다. 따라서, 제1 신경망 및 제1 가속기의 하드웨어 성능이 제1 하드웨어 기준을 만족하지 못한다고 식별되면, 프로세서는 제1 신경망 및 제1 가속기를 바로 제외함으 로써 불필요한 동작을 최소화할 수 있다. 다만, 이는 일 실시예에 불과하며, 제1 가속기 및 제1 신경망이 선택 되면, 프로세서는 선택된 가속기 및 신경망을 제1 예측 모델에 입력하지 않고 바로 구현하여 구현과 관련된 정보를 획득할 수 있다. 그리고, 제1 하드웨어 기준 및 제2 하드웨어 기준은 실험 또는 통계를 통하여 획득된 기설정된 값일 수 있으나, 프로세서에 의해 업데이트될 수 있다. 예를 들면, 제1 하드웨어 기준 중 임계 레이턴시가 100ms라고 설정 되었으나, 복수의 신경망에 대응되는 레이턴시의 추정 값의 평균 값이 50ms 라고 식별되면, 프로세서는 임 계 레이턴시를 축소(예를 들어, 60ms로)시킬 수 있다. 즉, 프로세서는 복수의 신경망 또는 복수의 가속기 의 하드웨어 성능의 추정값을 바탕으로 제1 하드웨어 기준 또는 제2 하드웨어 기준을 업데이트할 수 있다. 그리고, 프로세서는 선택된 가속기 상에 선택된 신경망을 구현하여 구현과 정확도 및 효율성 지표를 포함 하는 구현과 관련된 정보를 획득할 수 있다. 그리고, 프로세서는 구현과 관련된 정보를 평가 모델에 입력하여 선택된 가속기 및 신경망에 대응되는 보상 값을 획득할 수 있다. 한편, 상술한 바와 같이, 평가 모델 은 정확도 지표 및 효율성 지표를 정규화하고, 정규화된 지표에 대해 가중합 연산을 수행하여 보상 값을 출력할 수 있다. 그리고, 제1 가속기 상에서 제1 신경망을 구현함으로써 제1 보상 값을 획득하면, 프로세서는 복수의 신경 망 중 제1 가속기 상에서 구현할 제2 신경망을 선택할 수 있다. 즉, 프로세서는 복수의 신경망 중 제1 가 속기 상에서 제1 신경망을 구현할 때보다 더 높은 보상 값을 획득할 수 있는 신경망을 검색하여 제2 신경망을 선택할 수 있다. 프로세서는 복수의 신경망 중 제1 신경망을 선택하는 방식과 동일하게 제1 신경망을 제외 한 복수의 신경망 중 제2 신경망을 선택할 수 있다. 그리고, 프로세서는 제1 가속기 상에서 선택된 제2 신경망을 구현함으로써 구현과 관련된 정보를 획득할 수 있다. 한편, 제1 가속기 상에서 제2 신경망을 구현하기 전, 프로세서는 제1 가속기 및 제2 신경망을 제 1 예측 모델에 입력하여 제1 가속기 및 제2 신경망에 대응되는 하드웨어 성능이 제1 하드웨어 기준을 만족 하는지 여부를 식별할 수 있다. 제1 가속기 및 제2 신경망에 대응되는 하드웨어 성능이 제1 하드웨어 기준을 만 족한다고 식별되면, 프로세서는 제1 가속기에 제2 신경망을 구현하여 구현과 관련된 정보를 획득할 수 있 다. 다만, 이는 일 실시예에 불과하며, 프로세서는 제1 예측 모델에 제1 가속기 및 제2 신경망을 입력 하지 않고 바로 구현하여 구현과 관련된 정보를 획득할 수 있음은 물론이다. 그리고, 프로세서는 제1 가속기 및 제2 신경망을 구현하여 획득된 정확도 지표 및 효율성 지표를 바탕으로 제2 보상 값을 획득할 수 있다. 그리고, 프로세서는 제1 보상값 및 제2 보상값을 바탕으로 복수의 가속기 중 가장 큰 보상 값을 가지는 신경망 및 가속기를 선택할 수 있다. 구체적으로, 제2 보상값이 제1 보상값보다 크다는 것은 제1 가속기 상에는 제1 신경망을 구현하는 것보다 제2 신경망을 구현하는 것이 더욱 효율적이고 정 확하다는 것을 의미할 수 있다. 따라서, 프로세서는 제1 가속기 및 제2 신경망 한 쌍이 제1 가속기 및 제1 신경망 한 쌍보다 더 최적화된 한 쌍임을 식별할 수 있다. 그리고, 프로세서는 제1 가속기를 제외한 복수의 가속기 중 제2 신경망을 구현할 가속기를 선택할 수 있다. 제2 신경망을 구현할 가속기로 제2 가속기가 선택되면, 프로세서는 제2 가속기에 제2 신경망을 구현 하여 구현과 관련된 정보를 획득하고, 획득된 구현과 관련된 정보를 바탕으로 제3 보상값을 획득할 수 있다. 그 리고, 프로세서는 제2 보상값과 제3 보상값을 비교하여 더 높은 보상 값을 출력할 수 있는 한 쌍의 가속기 및 신경망을 선택할 수 있다. 그리고, 프로세서는 위 동작을 반복하여 저장된 가속기 및 신경망 중에서 가 장 큰 보상값을 출력할 수 있는 한 쌍의 신경망 및 가속기를 선택할 수 있다. 가장 큰 보상값을 출력할 수 있는 한 쌍의 신경망 및 가속기는 다른 쌍보다 이미지 분류, 음성 인식 등과 같이 특정 작업을 정확하고 효율적으로 수행할 수 있다. 한편, 프로세서는 디지털 신호를 처리하는 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어(computer executable instructions)를 실행함으로써 다양한 기능을 수행할 수 있다. 뿐만 아니라, 프로세서는 인공 지능 기능을 수행하기 위하여, 별도의 AI 전용 프로세서인 GPU(graphics-processing unit), NPU(Neural Processing Unit), VPU(Visual Processing UniT) 중 적어도 하나를 포함할 수 있다. 구체적으로, 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인 공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조 로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의 해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 도 2는 본 개시의 일 실시예에 따른, 전자 장치가 제1 예측 모델을 통해 제1 가속기 상에서 제1 신경망을 구현할지 여부를 결정하는 과정을 설명하기 위한 순서도이다. 우선, 전자 장치는 복수의 신경망 중 제1 신경망을 선택하고, 복수의 가속기 중 제1 신경망을 구현할 제1 가속기를 선택할 수 있다(S210). 전자 장치가 제1 신경망 및 제1 가속기를 선택하는 과정은 도 1을 참조하 여 구체적으로 설명하였으므로 중복되는 설명은 생략하도록 한다. 그리고, 전자 장치는 제1 신경망 및 제1 가속기를 제1 예측 모델을 통해 제1 신경망 및 제1 가속기에 대응 되는 하드웨어 성능의 추정값을 획득할 수 있다(S220). 구체적으로, 제1 신경망 및 제1 가속기가 입력되면, 제1 예측 모델은 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값을 출력할 수 있다. 예를 들어, 제1 예측 모델은 제1 가속기 상에 제1 신경망을 구현할 때 소요될 것으로 추정되는 레이턴시 및 전력을 출력할 수 있다. 그리고, 전자 장치는 획득된 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족하는지 여부를 식별할 수 있다(S230). 예를 들어, 제1 가속기 상에서 제1 신경망을 구현할 때 소요될 것으로 추정되는 레이턴시가 제1 하 드웨어 기준을 초과하는 경우, 전자 장치는 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값 이 제1 하드웨어 기준을 만족하지 못하는 것으로 식별할 수 있다. 또 다른 예로, 제1 가속기 상에서 제1 신경망 을 구현할 때 소요될 것으로 추정되는 전력이 제1 하드웨어 기준을 초과하지 않는 경우, 전자 장치는 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족하는 것으로 식별할 수 있다. 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족하지 않는다고 식별 되면, 전자 장치는 제1 가속기를 제외한 가속기 중 제1 신경망을 구현할 제2 가속기를 선택할 수 있다 (S240). 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족하지 않는다 는 것은 제1 신경망 및 제1 가속기를 통해 높은 보상 값을 획득하지 못한다는 것을 의미할 수 있다. 따라서, 전 자 장치는 제1 신경망 및 제1 가속기 한 쌍을 제외하고 다른 한 쌍의 신경망 및 가속기를 선택함으로써 불 필요한 동작을 최소화할 수 있다. 그리고, 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값이 제1 하드웨어 기준을 만족한다고 식별 되면, 전자 장치는 제1 가속기 상에서 제1 신경망을 구현할 수 있다(S250). 즉, 전자 장치는 제1 신 경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정 값이 제1 하드웨어 기준을 만족하므로 실제 제1 가속기 상에 제1 신경망을 구현함으로써 구현과 관련된 정보를 획득할 수 있다. 도 3은 본 개시의 일 실시예에 따른, 전자 장치가 제2 예측 모델을 통해 제1 신경망을 구현할 가속기를 선 택할지 여부를 결정하는 과정을 설명하기 위한 순서도이다. 우선, 전자 장치는 복수의 신경망 중 제1 신경망을 선택할 수 있다(S310). 전자 장치가 복수의 신경 망 중 제1 신경망을 선택하는 과정은 상술하였으므로 중복되는 설명은 생략하도록 한다. 그리고, 전자 장치는 제2 예측 모델을 통해 제1 신경망에 대응되는 하드웨어 성능의 추정값을 획득할 수 있다(S320). 구체적으로, 제1 신경망이 입력되면, 제2 예측 모델은 제1 신경망에 대응되는 하드웨어 성능의 추정값을 출력할 수 있다. 예를 들어, 제2 예측 모델은 제1 신경망이 특정 가속기 상에서 구현될 때 소요될 것으 로 추정되는 레이턴시 또는 제1 신경망의 메모리 풋 프린트 등을 추정할 수 있다. 그리고, 전자 장치는 획득된 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족 하는지 여부를 식별할 수 있다(S330). 예를 들어, 특정 가속기 상에서 제1 신경망을 구현할 때 소요될 것으로 추정되는 레이턴시가 제2 하드웨어 기준을 초과하는 경우, 전자 장치는 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하지 못하는 것으로 식별할 수 있다. 또 다른 예로, 제1 신경망의 용 량이 제2 하드웨어 기준을 만족하는 경우, 전자 장치는 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하는 것으로 식별할 수 있다. 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하지 않는다고 식별되면, 전자 장치 는 제1 신경망을 제외한 복수의 신경망 중 하나를 선택할 수 있다(S340). 제1 신경망에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족하지 않는다는 것은 제1 신경망을 통해 높은 보상 값을 획득하지 못 한다는 것을 의미할 수 있다. 따라서, 전자 장치는 제1 신경망을 제외한 복수의 신경망 중 다른 신경망을 선택함으로써 불필요한 동작을 최소화할 수 있다. 그리고, 제1 신경망 및 제1 가속기에 대응되는 하드웨어 성능의 추정값이 제2 하드웨어 기준을 만족한다고 식별 되면, 전자 장치는 복수의 가속기 중 제1 신경망을 구현할 가속기를 선택할 수 있다(S350). 전자 장치 가 제1 신경망을 구현할 가속기를 선택하는 과정은 도 1을 참조하여 설명하였으므로 중복되는 설명은 생략 하도록 한다. 도 4a 내지 도 4c는 본 개시의 일 실시예에 따른, 전자 장치가 가속기 및 파라미터화 가능 알고리즘 (parameterizable algorithm)을 설계하는 방식을 설명하기 위한 순서도이다. 도 4a 내지 도 4b는 파라미터화 가 능 알고리즘이 컨볼루션 뉴럴 네트워크(convolution neural network; CNN)로 구현된 경우를 도시하였으나 이는 일 실시예에 불과하다. 즉, 파라미터화 가능 알고리즘은 다른 종류의 신경망으로도 구현될 수 있다. 도 4a에 도시된 바와 같이, 전자 장치는 메모리에 저장된 신경망 서브 검색 공간으로부터 제1 CNN을 선택할 수 있다(S400). 이와 동시 또는 임계 시간 범위 내에, 전자 장치는 가속기 서브 검색 공간으로부터 제1 가속기 아키텍처를 선택할 수 있다(S402). 그리고, 전자 장치는 선택된 제1 가속기 아키텍쳐 상에서 선택된 제1 CNN을 구현할 수 있다(S404). 전자 장치는 선택된 제1 가속기 상에 제1 CNN을 구현함으로써 정 확도 지표 및 효율성 지표를 포함하는 구현과 관련된 정보를 획득할 수 있다(S406). 효율성 지표는 가속기 상에 서 신경망이 구현되면서 소요되는 대기 시간, 전력, 가속기의 면적 등을 포함할 수 있다. 그리고, 전자 장치 는 획득된 구현과 관련된 정보를 바탕으로 보상값을 획득할 수 있다(S408). 그리고, 전자 장치는 획 득된 보상값을 최적화된 한 쌍의 CNN 및 가속기(예를 들어, FPGA)를 선택 또는 업데이트하는데 사용할 수 있다 (S410). 즉, 전자 장치는 최적의 CNN 및 FPGA의 쌍을 선택될 때까지 상술한 프로세서를 반복할 수 있다. 도 4b는 전자 장치가 도 4a의 방법을 구현하기 위한 방식을 개략적으로 도시한 것이다. 전자 장치에 포함된 프로세서는 CNN 서브 검색 공간 및 가속기 서브 검색 공간(또는, 가속기 디자인 공간)으로부터 제1 CNN 및 제1 FPGA를 선택하고, 선택된 제1 CNN을 제1 FPGA 상에서 구현하여 획득된 구현과 관련된 정보를 평가 모델에 입력할 수 있다. 그리고, 평가 모델은 구현과 관련된 정보를 바탕으로 획득된 보상 값을 출력할 수 있다. 이 방법은 기본 FPGA 가속기와 CNN의 구조를 공동으로 최적화하기 위한 강화 학습 시스템으로 설명될 수 있다. 상술한 바와 같이, 기존의 NAS 는 CNN을 특정 FPGA 가속기로 조정하거나 새롭게 발견된 CNN에 대해 FPGA 가속기를 조정하였다. 그러나, 본 개시에 따른 NAS는 CNN 및 이에 대응하는 FPGA 가속기 모두를 공동으로 설계할 수 있다. 도 4c는 프로세서의 일 구성을 상세를 도시한 것이다. 도 4c에 도시된 바와 같이, 프로세서는 복수의 단일 LSTM(long short-term memory) 셀들을 포함하고, 그에 대응하는 특수화된 완전-연결(fully-connected; FC) 층이 연결되어 있다(출력당 하나의 셀 및 하나의 FC 층). 도 4c에 도시된 바와 같이, 하나의 단일 LSTM 셀 에 연결된 FC 층에서 출력된 결과는 다음 LSTM 셀에 입력될 수 있다. 이 때, FC 층에서 출력되는 결과는 CNN 또 는 가속기 하드웨어를 구성하기 위한 파라미터 일 수 있다. 일 실시예로, 도 4c에 도시된 바와 같이, 프로세서 는 복수의 단일 LSTM 셀 및 이에 연결된 FC 층을 통해 CNN을 구성하는 파라미터를 먼저 획득할 수 있으며, 그 뒤에 FPGA 가속기의 하드웨어 파라미터들을 획득할 수 있다. CNN 및 FPGA 가속기 각각의 제1 및 제2 구성 가 능한 파라미터는 출력으로서 처리되며 자신의 셀 및 FC 층을 갖는다. 구성 가능한 모든 파라미터들이 획득되면,프로세서는 CNN 및 가속기의 평가를 위해 CNN 및 가속기를 평가 모델에 전송할 수 있다. 도 4c에 도시된 프로세서는 기존의 RL-기반 NAS의 확장이며, RL 에이전트로 지칭될 수도 있다. 따라서 프 로세서는 LSTM 셀을 기반으로 한다. 그러나, 제어기는 완전히 상이한 알고리즘, 예를 들어 제네틱 알고리 즘(genetic algorithm)을 구현할 수도 있고 따라서 상이한 구조를 가질 수도 있다. 프로세서는 모델 구조 로 변환되는 유한 시퀀스의 액션들을 취하는 것을 담당한다. 각 액션은 도 4c에 도시된 예들과 같은 결정이라고 할 수 있다. 각 결정은 유한 옵션 세트에서 선택되며 동일한 반복에서 제어기에 의해 선택된 다른 결정들과 함 께 모델 구조 시퀀스 s를 형성한다. 가능한 모든 세트 s를 검색 공간이라고 하며, 공식적으로 다음과 같이 정의 할 수 있다:"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 Oi 는 i 번째 결정에 사용 가능한 옵션들의 세트이다. 반복을 t번 할경우, 프로세서는 구조 시퀀스 st를 생성한다. 시퀀스 st가 전달되면, 평가 모델는 제안된 구조를 평가하고, 평가된 지표(또는, 메트릭스 (metrics))에 기초하여 보상 함수 R(st)에 의해 생성되는 보상 값(rt)를 생성한다. 보상은 (t→∞일 때) 보상 기능을 최대화하는 st를 선택하도록 평가 모델 등을 업데이트하는데 사용된다. 프로세서는 각 모델을 업데이트하는 문제에 대한 다양한 접근 방식이 존재할 수 있다. 딥 RL에서는 DNN이 학습형 구성 요소로서 사용되며, 프로세서는 각 모델에 역전파 방식(backpropagation)을 사용하여 업데이 트할 수 있다. 구체적으로, 도 4a를 참조하여 설명한 방식의 경우, 프로세서는 DNN(본 발명의 경우 단일 LSTM 셀)이 결정당 하나의 확률 분포 시퀀스를 생성하는 Policy function π를 구현하며, 이것은 각각의 O 세트 들로부터 요소들을 선택하도록 샘플링되며 이에 따라 시퀀스 s를 결정한다. 그리고, 프로세서는 다음 관측 된 보상 값r과 시퀀스 s를 선택할 수 있는 전체 확률의 프로덕트의 기울기를 계산하여 네트워크를 업데이트한다. 하기 수학식 4를 참조하여 설명될 수 있다. 수학식 4"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 D={D1, D2, …. , Dn}는 각 결정에 대한 확률 분포의 세트이다. s는 독립적으로 샘플링된 결정들의 시퀀 스 s1, s2, …, sn에서 생성되므로, 전체 확률 p(s|D)는 다음과 같이 쉽게 계산될 수 있다:"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "RL 기반 알고리즘은 s 요소가 무엇인지(사용 가능한 옵션이 무엇인지) 또는 s로부터 어떠한 방식으로 보상 신호 가 계산되는지에 대해 어떠한 제한도 부여하지 않기 때문에 편리하다. 따라서, 일반성을 잃지 않고 일부 세부 사항을 추상화할 수 있으며, 실제에 있어서는, 사용 가능한 각 옵션을 그것의 인덱스에 의해서 간단히 식별할 수 있다. 그리고, 프로세서에 의해 선택된 인덱스들의 시퀀스가 모델로 변환된 후에 평가됨으로써 본 섹션 에서 설명된 알고리즘과 독립적으로 보상 신호를 구성한다. 기본 방법을 손상시키지 않으면서 일반적으로 다른 전략들을 사용할 수도 있음에 유의한다. 이 속성에 따라, 본 개시의 전반적으로 걸쳐진 다음과 같은 단축 표기 법을 사용하여 하기 수학식 5와 같이 검색 공간을 설명한다: 수학식 5"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이것은 |Oi| = ki인(여기서 k는 파라미터에 사용 가능한 옵션 수), 수학식 1에서 정의된 검색 공간 S로서 이해 되어야 한다. 일반적인 알고리즘의 개요가 하기 알고리즘을 참고하여 설명될 수 있다. 알고리즘 1: 강화(REINFORCE)를 이용한 일반 검색 알고리즘"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "입력: 정책(Policy) 가중치 θ, 실행할 단계 개수 T, 결정할 개수 n 출력: 업데이트된 θ 및 탐색된 포인트들의 세트"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "REINFORCE 알고리즘 또는 유사한 알고리즘이 지표(또는, 메트릭스)를 평가하고 보상 함수를 생성하는 것과 함께 검색을 수행하는데 사용될 수 있다. 이 알고리즘은 가중치/파라미터를 취하는 정책 함수를 포함할 수 있고, 분 포 Dt가 Policy function으로부터 획득될 수 있다. 그런 다음, 이 분포로부터의 시퀀스 st가 샘플링될 수 있다. 결합된 공간을 검색할 때, 시퀀스에는 FPGA 파라미터 및 CNN 파라미터가 모두 포함된다. 그 후, 시퀀스는 예를 들어 (선택한 CNN을 선택한 FPGA에서 실행하거나 아래에 자세히 설명되는 바와 같이 성능을 시뮬레이션하여) 평 가 모델에 의해 평가된다. 구현 시에 소요되는 대기 시간, 전력, 가속기의 면적과 같은 효율성 지표 및 정 확도 지표는 평가 모델에 의해 측정될 수 있다. 상술한 효율성 지표 및 정확도 지표는 보상 함수 R(mt)를 획득하기 위해 평가 모델에 대한 입력으로 사용된다. 평가 모델이 보상 값을 획득하기 위해 사용하는 보상 함수는 해당 시퀀스를 선택할 확률과 함께 policy function의 파라미터/가중치를 업데이트하는데 사용된다. 이에 따라 policy function가 보상을 극대화하는 시퀀스를 선택하는 것을 학습하게 된다. 도 4a에 도시된 방법은 FPGA 가속기의 선택과 관련된 여러 결정을 포함하는 것에 의하여 기존 NAS를 확장한다. 검색 공간은 신경망 서브 검색 공간(SNN)과 FPGA 하위 검색 공간(SFPGA)의 카테시안 프로덕트로서 정의되며 수 식은 상술한 수학식 1과 같이 정의될 수 있다. SNN은 신경망을 검색할 수 있는 신경망 검색 공간이고, SFPGA는 FPGA 가속기 설계와 관련된 확장 부분이다. 이와 같이 정의된 검색 공간은 수학식 5에 제공된 정의와 근본적으로 다르지 않으며 검색 알고리즘에 대한 변경 을 의미하지는 않는다는 점에 유의한다. 그러나 두 부분에 대한 검색 도메인이 다르기 때문에, 두 부분을 명시 적으로 구별하고 해당 차이점을 사용하여 시너지 효과에 대해 논의하는 것이 도움이 된다. 각 하위 검색 공간 에 대해서는 아래에서 자세히 설명된다. 도 5는 도 4a의 방법에서 사용될 수 있는 양호하게 정의된 CNN 서브 검색 공간을 개략적으로 도시한 것이다. 이 것은 사용될 수 있는 양호하게 정의된 검색 공간의 일 예일뿐이라는 것이 이해될 것이다. 검색 공간은 Ying 등 에 의해 arXiv e-prints (Feb 2019)에 게재된 \"NASBench 101: Towards Reproducible Neural Architecture Search\"에 자세히 설명되어 있으며 NASBench라고 불릴 수도 있다. 도 5는 CNN 서브 검색 공간 내에 포함된 CNN 의 구조를 보여준다. 도시된 바와 같이, CNN은 그 각각이 3개의 셀들(312,314,316)을 포함하는 3개의 스택들 (302,304,306)을 포함할 수 있다. 각 스택은 동일한 셀 설계를 사용하지만 스택들과 인터리브된 다운샘플링 모 듈로 인해 상이한 차원을 가진 데이터에서 작동할 수 있다. 구체적으로, 각 스택의 입력 데이터는 X 및 Y 차원 에서 x2 더 작지만 이전 것에 비해 x2 더 많은 기능을 포함할 수 있으며, 이것은 분류 모델의 표준 사례이다.이 스켈레톤이 단일 셀의 가장 안쪽에 있는 설계인 각 모델의 다양한 부분과만 고정될 수 있다. 셀 설계의 검색 공간은 최대 7개의 연산(첫 번째 및 마지막 고정) 및 9개의 연결로 제한된다. 연산은 다음과 같 은 사용 가능한 옵션 중에서 선택될 수 있다: 3x3 또는 1x1 컨볼루션 및 3x3 최대 풀링(모두 스트라이드 1을 가 짐), 연결은 \"forward\"일 수 있다(즉, 기본 계산 그래프의 인접 행렬은 상부-삼각 행렬일 수 있다). 또한, 하나 이상의 연결이 연산에 입력되면, 병합(concatenation) 및 요소별 추가 연산이 자동으로 삽입된다. 수학식 1에서 와 같이, 검색 공간은 옵션들(즉, 구성 가능한 파라미터들)의 리스트로서 정의되며, 이 경우, CNN 검색 공간에 는 각각 3개의 옵션이 있는 5개의 연산과 트루(true) 또는 폴스(false)(2 옵션)일 수 있는 21개의 연결이 포함 된다 -- 21개의 연결은 7개의 연산 사이의 인접 행렬의 0이 아닌 값이다."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "검색 공간은 최대 9개의 연결이 있어야 하는 요구사항을 직접 캡처하지 않고, 따라서 유효하지 않은 포인트, 즉 유효한 모델을 생성할 수 없는 검색 공간의 포인트들이 포함됨을 유의한다. 또한, 셀의 출력 노드가 입력에서 분리되는 경우 포인트가 유효하지 않을 수 있다 섹션에서 이러한 문제를 해결하는 방법에 대해서는 후술하는 부 분에서 설명하도록 한다. 도 6은 연결된 시스템 온 칩 및 외부 메모리와 함께 FPGA 가속기를 도시한 것이다. FPGA 가속기 는 하나 이상의 컨볼루션 엔진, 풀링 엔진, 입력 버퍼, 가중치 버퍼 및 출력 버퍼 를 포함한다. 도 6에 표시된 것과 같은 시스템 온 칩 FPGA 상의 DNN 가속화를 위한 라이브러리는 Xilinx Inc 2019의 \"Chaidnn v2 - HLS based Deep Neural Network Accelerator Libray for Xilinx Ultrascale+ MPSoCs\"에 설명되어 있으며, 이하에서는 ChaiDNN 라이브러리라고 지칭한다. FPGA 가속기의 서브 검색 공간은 양호하게 정의되어 있으며 FPGA 가속기의 각 주요 구성 요소에 대해 제2 구성 가능한 파라미터들로 정의된다. 가속기 서브 검색 공간을 정의하는 제2 구성 가능한 파라미터는 병렬화 파라미 터(예를 들어, 병렬 출력 기능들 또는 병렬 출력 픽셀들), 버퍼 깊이(예를 들어, 입력, 출력 및 가중치 버퍼들 에 대한), 메모리 인터페이스 폭, 풀링 엔진 사용 및 컨볼루션 엔진 비율을 포함할 수 있다. 컨볼루션 엔진(들)의 구성 가능한 파라미터는 각각 병렬로 생성될 수 있는 출력 기능 맵들의 수 및 출력 픽셀들 의 수를 결정하는 병렬화 파라미터(예를 들어, \"filter_par\",\"pixel_par\" 등)를 포함할 수 있다. 파라미터 컨볼 루션 엔진 비율은 \"ratio_conv_engines\"로 구성될 수 있다. 파라미터 컨볼루션 엔진 비율은 각 컨볼루션 엔진에 할당된 DSP 수를 결정할수 있다. 1로 설정되면, 이는 모든 타입의 컨볼루션을 실행하는 단일 일반 컨볼루션 엔 진이 있으며 1 값은 ChaiDNN 라이브러리에서 사용되는 기본 설정으로 간주될 수 있다. 1 미만의 임의의 숫자로 설정하면, 듀얼 컨볼루션 엔진들이 있을 수 있다. 예를 들어, 듀얼 컨볼루션 중 하나는 3x3 필터에 특화되고 조 정되고 다른 하나는 1x1 필터에 적합할 수 있다. 풀링 엔진 사용을 위한 구성 가능한 파라미터는 \"pool_enable\"이다. 이 파라미터가 트루인 경우, 추가 FPGA 리 소스가 독립형 풀링 엔진을 만드는데 사용될 수 있다. 그렇지 않으면 컨볼루션 엔진의 풀링 기능이 사용될 수 있다. 도 6을 참조하면, 가속기 상에는 3개의 버퍼, 즉 입력 버퍼, 가중치 버퍼 및 출력 버퍼를 포함 할 수 있다. 버퍼들 각각은 구성 가능한 깊이(configurable depth)를 가지며 FPGA의 내부 블록 메모리에 상주한 다. 현재 CHaiDNN 구현에서, 버퍼들은 입력 기능 맵들, 출력 기능 맵들 및 각 레이어의 가중치를 수용하기에 충 분한 공간을 가져야 한다. 버퍼 크기가 클수록 느린 외부 메모리에서 데이터를 가져오지 않고도 더 큰 이미지들 과 필터들을 사용할 수 있다. 후술하는 바와 같이, 기능 및 필터 슬라이싱은 가속기의 유연성을 향상시킬 수 있 다. FPGA는 AXI 버스를 통해 CPU 및 외부 DDR4 메모리와 통신한다. CHaiDNN 라이브러리에서와 같이, 구성 가능한 파 라미터를 사용하면 리소스와 성능 사이의 균형을 이루도록 메모리 인터페이스 폭을 구성할 수 있다. 다음은 파라미터들(filter_par, pixel_par, input, output, weights buffer depths, mem_interface_width, pool_en and ratio_conv_engines)에 대한 FPGA 가속기 검색 공간을 정의한다."}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "평가 모델의 세부 사항을 보다 상세하게 고려하면, 가속기의 면적 및 대기 시간은 가속기 설계 공간의 파라미터 들에 의해 결정됨에 유의한다. 각 컴파일 작업에는 시간이 걸리고 CNN 모델을 동시에 실행하려면 수천개의 FPGA 가 필요하므로, NAS에서 온라인으로 면적 및 대기 시간을 측정하기 위해 설계 공간의 모든 구성을 컴파일하는 것은 실용적이지 않다. 따라서, 고속 평가 모델이 효율성 지표를 찾는데 유용할 수 있다. 각 가속기 아키텍처에 대해, 도 4a의 단계(S406)는 면적 모델을 사용하여 단계적으로 완료될 수 있다. CLB들, DSP들 및 BRAM들의 관점에서 FPGA 리소스 활용은 각 서브 컴포넌트에 대한 CLB, DSP 및 BRAM 사용량을 모델링하 기 위해 수학식을 사용하여 추정될 수 있다. 서브 컴포넌트의 예는 구성 가능한 파라미터들 \"filter_par\" 및 \"pixel_par\"의 크기에 따라 달라지는 컨볼루션 엔진 내의 라인 버퍼이다. 수학식은 이 두 변수를 입력으로 사 용하고 BRAM들의 수를 제공한다. 구성 가능한 파라미터 \"ratio_conv_engines\"가 1보다 작게 설정되면, 2개의 특수 컨볼루션 엔진이 있다. 이 경 우, 컨볼루션 엔진들의 CLB들 및 DSP들 사용량은 일반 컨볼루션 엔진에 비해 25% 감소한다. 이것은 전문화로 인 해 발생할 수 있는 잠재적인 면적 절감의 합리적인 추정치이며, 훨씬 더 많은 비용 절감이 문헌에서 입증되었다. 또한, 독립형 풀링 엔진을 사용하고 구성 가능한 파라미터 \"pool_enable\"가 1로 설정된 경우, 고정 량의 CLB들 및 DSP들이 소모된다. BRAM은 컨볼루션 및 풀링 엔진들의 데이터를 버퍼링한다. 입력, 출력 및 가중치 버퍼의 크기는 깊이를 통해 구 성할 수 있다. 이 데이터는 이중 버퍼링되므로 BRAM 양의 두 배를 소비한다. 고정된 수의 BRAM들은 풀링(활성화된 경우), 바이어스, 스케일, 평균, 분산 및 베타 전용이다. BRAM들의 수는 각 BRAM이 36 Kbit인 것으로 가정하여 계산된다. FPGA 리소스 활용에 기초하여, 다음 단계는 그 면적이 단일 수(실리콘 면적)로 정량화되도록 mm2 단위의 FPGA 크기를 추정하는 것이다. 각 리소스의 면적은 CLB를 기준으로 조정된다. 이 데이터는 사용 중인 장치에 대해 사용할 수 없으므로, 유사한 장치에 대한 데이터는 Abdelfattah 등에 의해 International Conference on Field Programmable Technology 95-103 에 게재된 \"Design Tradeoffs for Hard and Soft FPGA-based Network on Chips\"로부터 사용된다. 더 작은 프로세스 노드(20nm 대 40nm)와 상이한 블록 속성들(10개 대신 CLB당 8개의 LUT, 9 Kbit 대신 BRAM당 36 Kbit)도 고려한다. 아래의 표 1은 본 실시예에서 사용될 수 있는 장치의 추정된 블록 면적을 도시한 것이다. 표 1"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 13, "content": "도 7a는 다양한 가속기 아키텍처의 면적을 보여준다. 도 7a에 도시된 그래프는 구성 가능한 파라미터 \"filter_par\"= 8 및 \"filter_par\"= 16에 대한 면적별 예상 리소스 사용량을 표시하고 있다. 면적 측정치들도 계 산되어 그래프에 나타나 있다. 이 도면은 면적 모델의 예측이 실제 측정과 관련하여 유효함을 보여준다. 이 모 델은 현재 CHaiDNN에서 지원하지 않는 가속기 아키텍처의 면적을 예측한 것임을 유의해야 하며, 예를 들어 구성 가능한 파라미터 \"filter_par\"=8, \"pixel_par\"=4를 갖는 가장 작은 아키텍처는 96.43 mm2 크기이며, 구성 가능 한 파라미터 \"filter_par\"=16, \"pixel_par\"=64를 갖는 가장 큰 아키텍처는 218.62 mm2 크기이다. 일단, CLB들, DSP들 및 BRAM들 측면에서 FPGA 리소스 활용도가 추정되었다. 대기 시간은 도 4a의 단계(S406)의 일부로서, 예를 들어 대기 시간 모델을 사용하여 추정될 수 있다. 이 예에서 활용도는 대기 시간 이전에 추정되 지만 이 추정은 임의의 순서로 수행될 수 있음이 이해될 것이다. 대기 시간 모델은 두 개의 부분(대기 시간 룩 업(look-up) 테이블 및 스케줄러)으로 나뉠 수 있다. NASBench 검 색 공간에서, 3x3 및 1x1 컨볼루션들, 최대 플링 및 다양한 차원의 요소별 추가 작업을 포함하여 85개의 작업이 수행될 수 있다. 대기 시간 모델은 상이한 구성들로 FPGA 가속기에서 각 작업을 실행하고 CHaiDNN에서 제공하는 성능 평가 API를 사용하여, 대기 시간 숫자를 프로파일링한 다음 룩업 테이블에 저장할 수 있다. 스케줄러는 병 렬 계산 유닛에 작업을 할당하고, 룩업 테이블의 작업 대기 시간을 사용하여 CNN 모델의 총 대기 시간을 계산할 수 있다.컨볼루션 작업의 대기 시간은 병렬화 팩터들 \"filter_par\" 및 \"pixel_par\"에 따라 다를 수 있다. CHaiDNN은 \"filter_par=8\", \"pixel_par=4\" 및 \"filter_par=16\", \"pixel_par=64\" 아키텍처를 지원하지 않기 때문에, 이들 의 대기 시간은 다른 아키텍처의 측정값들을 사용하여 보간될 수 있다. 듀얼 컨볼루션 엔진의 경우, 그 중 하나 가 3x3 필터에 특화되고, 다른 하나는 1x1 필터에 특화될 수 있다. 한편, 각 컨볼루션의 성능은 사용 가능한 엔 진들의 수에 비례하여 조정된다. 예를 들어, ratio_conv_engines = 0.75 파라미터인 경우, 3x3 컨볼루션의 대기 시간이 1/0.75 증가하고 1x1 컨볼루션의 대기 시간이 1/0.25 증가할 수 있다. 원래 CHaiDNN 가속기에서, 데이터 버퍼들은 가능한 최대 처리량을 달성하기 위해 전체 입력, 출력 및 필터 텐서 들에 맞게 크기를 조정해야 한다. 그러나, 이미지 해상도가 증가하고 CNN이 더 복잡하고 깊게 구성될 경우, 상 술된 할당 방식은 실현 불가능하고 가속기의 실현 가능성을 제한할 수 있다. 도 7a를 참조하며 상술한 바와 같 이, 입력 텐서의 슬라이스들이 외부 메모리로부터 입력 버퍼로 페치(fetch)되고 가속기에 의해 독립적으로 처리 되는 방식이 추가될 수 있다. 또한, 출력 및 가중치 버퍼들이 가득 차면 출력 계층들 및 필터 가중치들이 외부 메모리에 유출되며, 따라서, 성능은 구성 가능한 파라미터 \"mem_interface_width\"에 의존하는 메모리 대역폭에 의해 제한될 수 있다. 따라서, 현재 CHaiDNN 구현에 대한 제한으로 인해 대기 시간 모델을 구축할 때 몇 가지 가정이 이루어질 수 있 다. 먼저, 성능 평가 API는 독립형 엔진에서 실행되는 최대 풀링을 지원하지 않으므로 대기 시간은 컨볼루션 엔 진에서 실행되는 것보다 2배 더 빠르도록 모델링될 수 있다. 둘째, 메모리 인터페이스 폭은 독립적으로 구성될 수 없다. 파라미터 세트를 포함하는 DIET_CHAI_Z 구성과 관련이 있으며, 메모리 인터페이스 폭은 DIET_CHAI_Z가 활성화될 때 폭이 줄어든 AXI 버스에 의존할 수 있다. 모델은 모든 파라미터들을 가속기 설계 공간으로 가져오 지 않고, 파라미터 \"mem_interface_width\"가 512 비트에서 256 비트로 감소할 때 대기 시간이 4% 증가한다고 가 정한다. 마지막으로, 모델에 사용된 접근 방식은 가속기의 런타임에서 대기 시간을 최적화하는데 사용되는 작업 융합을 고려하지 않는다. 도 7b는 대기 시간 모델의 검증 결과를 도시한 것이다. 먼저 대기 시간은 상이한 가속기 아키텍처에 대한 모델 에 의해 추정되며, 그 결과는 도 7b에서 도시된 그래프들로 표시된다. 그리고, FPGA 가속기에서 신경망 모델을 실행하고 도 7b에 도시된 대로 종단 간 대기 시간(end-to-end latency)을 측정한다. 도 7b는 대기 시간 모델이 가정된 가정에도 불구하고 병렬 처리 수준과 관련하여 대기 시간 추세를 설명할 수 있음을 보여줄 수 있다. 도 7a 및 도 7b의 경우, HW 풀링이 가능하고, 메모리 인터페이스 폭은 512 비트이며, 버퍼 크기는 [8192,2048,2048]이고, 배치 크기(batch size)는 2이고 클록 주파수(clock frequency)는 200MHz임에 유의한다. 도 8 및 도 9는 본 개시의 일 실시예에 따른, GFLOPS(크기) 및 pixel_par 파라미터들과 관련된 룩업 테이블로부 터 모든 컨볼루션 작동의 추출된 대기 시간 숫자를 도시한 도면이다. 도 8에 도시된 바와 같이, 대기 시간은 데 이터 크기에 따라 증가하고 컨볼루션 엔진들에서 더 많은 병렬 처리에 따라 감소할 수 있다. 도 4a에 도시된 바와 같이, 전자 장치는 효율성 지표 및 정확도 지표를 바탕으로 보상 값을 획득할 수 있 고(S208), 획득된 보상 값을 CNN 및 FPGA의 선택을 업데이트하기 위해 사용할 수 있다(S210). 일 실시예로, 위 구현의 복잡성을 설명하기 위해 도 9는 Branke 등에 의해 Springer 2008에 게재된 \"Multiobjective Optimization, Interactive and Evolutionary Approaches\"에서 설명된 일부 파레토 최적 포인 트들을 보여준다. NASBench의 CNN 정확도는 사전 계산되어 데이터베이스에 저장되며, 위에서 설명한 FPGA 가속 기 모델은 데스크탑 컴퓨터에서 빠르게 실행될 수 있다. 이를 통해 전체 공동 설계 검색 공간을 37 억 데이터 포인트로 열거할 수 있다. 검색 공간에서 지배적 포인트들을 반복적으로 필터링하여 37 억 포인트 내의 파레토 최적 포인트 위치를 찾아낸다. 지배적 포인트들은 3 가지 지표(면적, 대기 시간, 정확도) 모두에서 하나 이상의 다른 포인트보다 열등한 포인트이다. 나머지 (지배적이지 않은) 포인트들은 평가 지표(면적, 대기 시간 또는 정 확도) 중 하나 이상에서 최적화되었다. 여기서의 검색 공간의 경우, 3096개의 파레토 최적화 모델 가속기 쌍만 존재하였으며, 이것이 도 9에 나와 있다. 도 9에 도시된 바와 같이, 면적, 대기 시간 및 정확도 지표는 3 가지 방식의 트레이드 오프가 있다. 어느 하나 의 지표를 향상시키려면 다른 두 개의 지표를 저하시켜야 한다. 산점도(scatter plot)에 도시된 바와 같이, 검 색 공간은 각각 다른 가속기 면적에서 대략 동심원의 정확도-대기 시간 트레이드 오프 곡선으로 이루어질 수 있 다. CNN을 수정함으로써, 동심원 정확도-대기 시간 곡선들을 따라 대략적으로 이동할 수 있다. 가속기 하드웨어 를 변경하면, 수평선을 가로 질러 이동할 수 있다. 따라서 지연 시간과 면적 모두에 영향을 미침). 도 10는 공동 설계된 CNN 및 FPGA의 성능을 GoogLeNet, ResNet 및 SqueezeNet와 같은 다른 방법들을 사용하여 찾은 모델 및 가속기들과 비교한 것이다. ChaiDNN은 GoogLeNet과 ResNet을 모두 실행하도록 수동으로 최적화되 었으며, 도 10에서 볼 수 있듯이, GoogLeNet의 대기 시간은 파레토 프론트에 매우 가깝다(즉, 위에서 설명한 방 법). 그러나, ResNet의 경우 파레토 프론트에서 훨씬 멀리 떨어져 있다. GoogLeNet에 비해 정확도가 향상되었 지만, 도 10에 도시된 바와 같이 대기 시간이 파레토 프론트에서 3배 떨어져 있다. 이것은 모델 이후 가속기의 순차적 설계와 비교하여 모델과 가속기를 공동 설계하는 능력을 보여준다. 도 11a 내지 도 11d는 본 개시의 일 실시에에 따른, 상이한 면적 제약에서 단일 및 듀얼 컨볼루션 엔진에 대한 정확도-대기 시간 파레토 프론티어를 도시한 도면이다. 전술한 바와 같이, 구성 가능한 파라미터 ratio_conv_engines는 단일 엔진 또는 듀얼 엔진이 있는지 여부와 각 듀얼 엔진에 할당된 DSP의 비율을 결정한 다. 위 결정된 요소들은 1x1 및 3x3 컨볼루션 실행 속도에 영향을 줄 수 있다. 이 가속기 파라미터는 CNN 검색 공간과 흥미로운 트레이드 오프를 만들 수 있다. 먼저, CNN 셀은 파라미터 ratio_conv_engines가 1보다 작다는 이점을 얻기 위해 쉽게 병렬화할 수 있어야 한다. 둘째, CNN 셀에서 3x3:1x1 연산의 비율에 따라, 상이한 ratio_conv_engines가 더 효율적일 수 있다. 이 파라미터의 경우, 공동 설계가 어떻게 최적의 결과를 이끌어 내 고 최상의 정확도 및 효율성을 위한 CNN과 가속기의 올바른 조합을 찾아내는지를 보여줄 수 있다. 도 11a 내지 도 11d를 참조할 때, 각각의 도면들은 면적 제약이 엄격할수록 듀얼 엔진들이 더 효율적이며 면적 제약이 클 때 일반적으로 단일 일반 엔진이 더 좋다는 것을 보여준다. 즉, 듀얼 엔진들이 실제로 유용한 가속기 기능임을 보여줄 수 있다. 이것은 CNN 모델 병렬 처리, 듀얼 엔진들을 위한 스케줄링 알고리즘 및 각 타입의 컨 볼루션 엔진에 할당된 DSP들의 비율 사이의 상호 작용을 고려할 때 명백하지 않은 결론이다. 이 가속기 기능을 단일 CNN 모델 또는 소수의 수작업으로 설계된 모델들로 연구한다면 이 결론에 도달하는 것은 불가능할 것이다. 듀얼 엔진들은 이러한 소수의 수작업으로 설계된 이러한 모델에도 적합하지 않을 수 있다. 그러나, 공동 설계를 통해, 수십만 개의 CNN 모델 중 주어진 가속기 기능에 맞는 최상의 모델을 검색할 수 있다. 듀얼 특수 엔진이 유용한 가속기 컴퓨팅 코어가 될 수 있음을 확인한 후, 1x1 및 3x3 컨볼루션들에 할당된 DSP들의 실제 비율을 자세히 살펴보았다. 실제 NAS 검색 시나리오에서, 특정 FPGA 장치의 면적을 제한하고, 특정 정확도 임계값을 능 가하는 가장 빠른 모델을 찾을 수 있다. 도 12a는 본 개시의 일 실시예에 따른, 파레토 최적 포인트들을 검색할 때 이러한 제약 조건의 결과를 보여준다. 각각의 상이한 ratio_conv_engines 값에 대해 발견된 상위 4개의 모델들이 강조 표시되어 있다. 발견 된 포인트들은 CNN 모델과 가속기 아키텍처들 간의 독립성을 보여준다. 예를 들어, 가속기가 1x1 컨볼루션들에 대해 더 많은 계산을 포함하는 경우 CNN 셀에 더 많은 conv1x1 연산이 존재하며, conv3x3의 경우도 마찬가지이 다. 도 12b 및 도 12c는 각각 0.33 및 0.67과 같은 ratio_conv_engines에 대응하는 CNN 셀들을 도시한 것이다. 도 시된 바와 같이, ratio_conv_engines=0.67일 때, 최상의 모델은 3개의 1x1 컨볼루션 및 4개의 3x3를 갖는 반면, ratio_conv_engines=0.33의 경우, 카운트들은 5개의 1x1 및 2개의 3x3으로 이동했다. 도 12d는 공동 설계 가속기 또는 \"상이한\" 가속기, 즉 도 12c의 CNN에 대해 공동 설계한 가속기에서 실행되는 도 12c의 CNN 실행 스케줄을 비교한 것이다. 두 설계 모두 동일한 면적 제약 조건이 적용되었다. 도면에서 알 수 있듯이 공동 설계된 가속기의 대기 시간은 훨씬 짧아지고(48 ms 대 72 ms) 컨볼루션 엔진들의 활용도가 훨씬 높아지는 반면에, \"상이한\" 가속기에서는 1x1 엔진의 활용률이 낮고, 3x3 엔진의 병목 현상이 발생한다. 도 13은 파라미터 ratio_conv_engines에 대한 Pareto-optimal 공동 설계 CNN 모델 가속기 쌍의 전체 환경을 보 여준다. 도 13에 도시된 바와 같이, 1x1 컨볼루션에 더 많은 DSP가 할당되면(비율=0.25), 파레토 최적 설계는 정확도가 낮아진다. 반대로, 3x3 컨볼루션에 더 많은 계산이 할당되면(비율=0.67) 정확도가 높아진다. 실제로 이것은 3x3 컨볼루션의 사용 증가로 인해 정확도가 높아진다는 것을 의미할 수 있다. 또한, 단일 컨볼루션 엔진 이 낮은 대기 시간 설계에 비해 우수할 수 있다. 또한 비율=0.5 또는 0.33인 경우 유사한 점을 찾아냈다. 이러 한 가속기의 수동 설계를 안내하는데 도움을 주는 방식으로 유용한 관측을 계속할 수 있다. 그러나, 위에서 설 명한 것처럼, 본 개시의 목표는 NAS를 사용하여 검색을 자동화하는 것이다. 기계 학습 작업(예를 들어, 이미지 분류)은 DNN 검색 공간으로 표현될 수 있으며, 하드웨어 가속기는 파라미터 들을 통해 표현될 수 있다(FPGA 검색 공간을 형성). 도 4a에 도시된 바와 같이, 전자 장치는 획득된 효율 성, 정확도 지표를 바탕으로 보상 값을 획득하고(S208), 획득된 보상 값은 CNN 및 FPGA의 선택을 업데이트하기 위해 사용할 수 있다(S210). 이러한 단계들은 대기 시간, 정확도 및 면적의 다목적 최적화(multiobjective optimization; MOO) 및 후술하는 공동 설계 검색 공간을 탐색하기 위한 상이한 검색 알고리즘을 사용하여 수행 될 수 있다. 전술한 바와 같이, 세가지 지표간에 근본적인 균형이 있으므로 최적화 문제에 대한 사소한 해결책이 없다. 따라 서 추가 단계들은 \"더 양호한\" 및 \"더 양호하지 않은\" 공동 설계를 정의하려면 추가 단계를 수행해야 한다. 궁 극적으로, 본 개시는 관심있는 메트릭스를 가져와 관련 공동 설계의 품질로 해석되는 스칼라 값을 반환하는 함 수를 요규할 수 있다. 이 함수를 위에서 나타낸 알고리즘 REINFORCE로부터의 보상 함수 R로서 사용하도록 한다. MOO 문제에 대한 두 가지 표준 접근 방식이 고려된다. 첫 번째 접근 방식은 Branke 등에 의해 Springer 2008에 게재된 \"Multiobjective Optimization, Interactive and Evolutionary Approaches\"에서 설명되는 바와 같이 가 중치 합계를 사용하여 세 가지 메트릭을 하나의 목적 함수로 통합하는 것이다. 두 번째 접근 방식은 하나의 메 트릭을 제외한 모든 메트릭이 특정 임계값 아래/위에 있게 만든 다음에 나머지 메트릭에 대해 최적화되는 포인 트들의 세트만 고려하는 것이다(ε-제약 방식). 그런 다음 더 적은 수의 지표가 제약되는 하이브리드 방식을 고 려하고/하거나 보상 함수를 계산할 때 제약이 있는 지표를 고려한다. 공식적으로, 이 작업에서 사용하는 일반 적인 MOO 보상 함수는 하기 수학식 6과 같이 구현될 수 있다. 수학식 6"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "여기서 m은 최적화하려는 지표들의 벡터이고, w는 이들의 가중치의 벡터이며, th는 함수의 도메인을 제한하는데 사용되는 임계값들의 벡터이다. 두 개 이상의 지표가 합산된 경우, 상이한 지표가 상이한 유닛들을 사용하고 상이한 범위의 값을 가지므로 값을 정규화하여 서로 비교할 수 있다. 가중치를 절대값으로 상대적으로 조정하면 비슷한 효과를 얻을 수 있지만 정 규화된 값을 추론하기가 더 쉬울 수 있다. 즉, 정규화 후에도 주어진 가중치 세트에 대해 상이한 지표가 목적 함수에 어떻게 기여하는지는 여전히 명확하지 않다. 직면해야 할 작은 기술은 RL 알고리즘들이 보상 함수를 최대화하여 작동하지만, 지표마다 다른 유형의 최적화가 필요하다는 것이다(정확도는 최대, 면적 및 대기 시간은 최소). 보상 함수에 대한 입력으로 음수의 면적과 대기 시간을 취함으로써 이것을 처리한다. 가중치 합계를 수행할 때마다, 정규화 중에 음수 값들을 처리하여 모든 지 표들에 대해 양수 값을 생성하여야 한다. Marlez 등에 의해 Engineering Optimization 37, 6 , 551-570에 게재된 \"Function-Transformation Methods for multi-objective optimization\"에서 보다 상세히 설명되어 있는 세 가지 정규화 전략을 활용할 수 있다. 첫 번째 정규화 방식은 가장 일반적인 방법 중 하나인 최대 정규화이며 이것은 달성 가능한 최대값과 관 련하여 값들을 정규화하는 방식이다. 음수 값들의 경우, 이들의 절대 값을 고려하여 유사하게 처리한다. 이 경 우, 정규화 함수는 하기 수학식 7과 같이 정의될 수 있다. 수학식 7"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "두 번째 정규화 방식은 지표의 최소값과 최대값을 모두 고려하는 최소-최대 정규화 방식이다. 위 방식의 경우, [0,1] 범위에서 선형으로 매핑될 수 있다 특정 함수는 수학식 8과 같이 정의될 수 있다.수학식 8"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "세 번째 정규화 방식은 표준 편차를 사용하여 값들을 정규화하는 표준 편차 정규화 방식이다. 세 번째 정규화 방식은 수학식 9에 정의된 수식을 활용할 수 있다. 수학식 9"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "일반 가중 합계 수학식(수학식 6)을 선택된 정규화 함수(수학식 7-9 중 하나, 예를 들어 수학식 8)와 조합하여, MOO 문제를 다음과 같이 수학식 10으로 정의할 수 있다: 수학식 10"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "여기서 ar은 면적, lat는 대기 시간, acc는 정확도이고, w1, w2, w3은 각 면적, 대기 시간 및 정확도에 대한 가 중치 세트이며 검색 공간 sεS에 대해 최적화가 수행되어 평가 모델의 출력 Ε(s)=m는 주어진 제약 조건(예를 들면, 특정 값 미만의 대기 시간)을 충족한다. 검색 포인트가 지정된 제약 조건을 충족하지 않으면, 퍼니시먼트 함수 Rv가 제어기의 피드백으로 사용되어 요구 사항들보다 낮은 유사한 포인트들을 검색하지 못하게 한다. 표준 보상 함수가 양수이고 제어기가 유효하지 않은 포인트를 선택하지 못하도록 하기 위해, 간단한 솔루션은 퍼니시먼트 함수를 음수로 만드는 것이다. 여기서 표 준 보상 함수 R과 동일한 함수를 사용하지만 다음과 같은 두 가지 변경 사항이 있다: 1) (ar, lat, acc) 대신에, (ar-arth, lat-latth, acc-accth)를 취하고 2) 상기와 반대의 과정을 취함으로써 Rv를 음수로 만들어 서 프로세서에 잘못된 선택임을 알릴 수 있다. 이들의 선택이 검색 결과에 어떤 영향을 미치는지 탐구하기 위해 MOO 문제에 대한 다른 가중치들이 고려될 수도 있다. 예를 들어, 가중치들은 각 지표에 대해 동일하게 설정될 수 있으며(예를 들어 1/3) 또는 가중치들이 어떤 메트릭의 우선 순위를 지정하도록 설정될 수 있다(예를 들면, 최적화 문제를 해결할 때 w1을 0.5로, w2와 w3을 0.25로 설정하여 면적의 우선 순위를 지정함). 각 가중치는 가중치의 합이 1이며, [0,1] 범위에 있을 수 있다. 보상 값을 바탕으로 CNN 및 FPGA의 선택을 업데이트하는 방식은 두 가지 방식이 있을 수 있다(S210). 첫 번째 접근 방식에서는, CNN 서브 검색 공간 및 FPGA 서브 검색 공간을 동시에 또는 임계 시간 범위 내에 접근하여 CNN 및 FPGA를 업데이트 하는 방식이다. 이러한 접근 방식을 통합 검색이라고 한다. 이 전략은 각 단계에서 CNN 과 가속기를 모두 업데이트할 수 있으므로, 보상 함수에 맞게 더 빠르게 변경할 수 있다. 그러나, 통합 검색 공 간(예를 들면, SNNxSFPGA)이 훨씬 더 커서, 최상의 포인트들(즉, 최상의 선택들)을 찾아내기가 더 어려울 수 있 다. 따라서, 보상 함수가 계산될 수 있도록, 실험이 최대 단계 수(예를 들면, 10,000 단계)에 대해 실행되고 지표들이 평가된다. 실제 검색을 실행하는 경우, 하나 이상의 프로세서에 의해 선택될 수 있는 유효하지 않은 제약이 있는 포 인트들 및 그러한 포인트들이 식별될 때의 적절한 반응을 고려해야 한다. MOO에는 탐색 개념이 없으므로 이 동 작은 표준 MOO 공식에 맞지 않으며 오히려 이것은 단순히 대등한 방식으로 다차원적 포인트들을 수식하는 방법을 제공할 수 있다. 그러나, 검색을 실행할 때, 보상 함수는 프로세서를 원하는 결과로 안내하는데 직접 사용되므로 추가 의미가 있다. 따라서, 단순히 유효하지 않고 제약이 있는 포인트들을 무시하면 제어기의 피드 백이 하나의 지표에만 관련되는 상황이 발생할 수 있으며, 이로 인해 제어기는 다른 두 개의 포인트를 고려하지 않고 그것을 최대화하는 더 많은 포인트들을 선택하게 될 수 있다. 따라서, 표준 보상 함수 내의 일부 지표들에 대해 가중치를 0으로 사용할 때마다 유효하지 않은 제약이 있는 포인트와 함께 사용할 보완 보상 함수를 제공하 는 것이 바람직하다. 그렇지 않으면, 프로세서가 공간 탐색을 학습할 때 일부 지표들을 고려하지 않는 상 황이 발생할 위험이 있다. 전술한 바와 같이, 본 방식은 예를 들어 결합된 검색의 사용에 의해 FPGA 및 CNN을 공동 설계하는 방식이다. 통 합 검색의 대안으로, 검색은 한 부분(예를 들어, FPGA 설계)이 고정되거나 정지되어 특정 부분(예를 들어, CNN 설계)에 중점을 두거나 그 반대인 특정 페이즈(phase)들을 명시적으로 정의했을 수 있다. 도 13은 전자 장치 의 페이즈 검색을 구현하는데 사용할 수 있는 대안적인 아키텍처를 도시한 것이다. 도 14에 도시된 바와 같이, 전자 장치는 2개의 상이한 프로세서(1400, 1420) 및 평가 모델을 포함할 수 있다. 한편, 도 14는 평가 모델이 프로세서(1400, 1420)가 아닌 별개의 휘발성 메모리에 로딩된 것으 로 도시하였으나, 이는 일 실시예에 불과하며 각 프로세서에 로딩될 수 있음은 물론이다. 제 1 프로세서 는 CNN 구조를 최적화하도록 학습될 수 있으며, 제2 프로세서는 FPGA 설계를 위한 최상의 옵션 조합을 선 택하도록 학습될 수 있다. CNN 서브 검색 공간 상에서 CNN을 검색 및 선택하는 CNN 페이즈에 대한 단계 수는 FPGA 서브 검색 공간 상에서 FPGA를 검색 및 선택하는 FPGA 페이즈에 대한 단계 수보다 클 수 있다. 예를 들면, FPGA 페이즈에 대한 단계가 200 단계일 경우, CNN 페이즈에 대한 단계는 1000 단계일 수 있다. 총 단계 수(예를 들면, 10,000 단계)에 도달 할 때까지, 두 페이즈들은 인터리브(interleave)될 수 있고 여러 번 반복된다. 이 페이즈 솔루션은 전반적으로 최적인 솔루션을 찾는데 사용된다. 이러한 분할-및-정복(divide-and-conquer) 기법은 두 검색 공간을 개별적으 로 고려하므로 (검색 공간마다에 대한) 로컬에서 최적의 포인트들을 쉽게 찾아낼 수 있다. 그러나, 페이즈들 사 이의 상호 영향은 제한적이므로, 예를 들어 특정 작업을 수행하기 위해 CNN과 가속기를 서로 최적으로 적응시키 는 것이 더 어려울 수도 있다. 도 15a 내지 도 15c는 상위 100개의 파레토 최적 포인트들과 비교한 상위 검색 결과들을 도시한 것이다. 각 도 면은 위에서 설명한 통합 및 페이즈 검색 결과들을 보여준다. 일 기준으로서, 이러한 제안된 검색들은 CNN 검색 공간이 먼저 CNN을 위해 검색된 후 가속기 설계 공간이 검색되는 별도의 검색 전략(예컨대, 종래 기술의 순차적 검색 방법)과 비교될 수 있다. 전술한 바와 같이, 복수의 인터리빙된 페이즈들 아닌 2개의 개별 페이즈들이 존 재할 수 있다. 제1 프로세서에 의한 CNN 검색은 8,333 단계만큼, 제2 프로세서에 의한 FGPA는 1,334 단계만큼 수행할 수 있다. 도 15a 내지 도 15c에 도시된 각각의 상위 검색 결과는 3 가지 실험 변형 중 하나에 대한 보상 함수를 최대화할 수 있다. 각 실험은 10 번 반복되므로 각 전략마다 최대 10개의 포인트들이 존재한다. 양호한 검색 알고리즘은 파레토의 최적 포인트들 근처의 결과들을 만들어낼 것으로 예상된다. 도 15a는 상기 수학식 10의 보상 함수에 부과되는 제약이 없는 \"비제약 조건\" 실험에 대한 결과를 보여준다. 가중치는 임의로 w(area, lat, acc) = (0.1, 0.8, 0.1)로 선택된다. 도 15a에 도시된 바와 같이, 이 실험은 공 동 설계 공간을 이해하기 위해 많은 양호한 포인트를 간단히 검색하는데 유용할 수 있다. 도 15b는 단일 제약 조건이 적용된 실험, 즉 대기 시간이 100 ms 미만인 실험 결과를 보여준다. 가중치는 w(area, lat, acc) = (0.1, 0, 0.9)로 선택된다. 이 실험은 최종 사용자가 작업 및 실시간 요구 사항을 알 수 있는 시나리오를 모방 하지만 어떤 FPGA 장치를 선택할지 확실하지 않으며 각 장치 크기에서 얻을 수 있는 정확도가 그러한 결정에 도 움이 될 수 있을지 확실하지 않다. 도 15c는 두 가지 제약 조건, 즉 정확도가 0.92보다 크고 면적이 100mm2보다 작은 실험 결과를 보여준다. 가중치는 대기 시간을 최적화하기 위해 w(area, lat, acc) = (0, 1, 0)으로 선택 된다. 두 가지 제약 조건을 적용하여 실험을 단일 목표로 한다. 이러한 실험은 응용에 대한 최대 FPGA 면적 버 짓과 최소 허용 정확도가 있을 때 유용할 수 있다. 도 16a 내지 도 16c는 세 가지 실험 시나리오에서 개별, 통합 및 페이즈 검색 전략들 각각에 대한 보상 값들을 도시한 것이다. 도 16a는 제약 조건이 없는 \"무제약' 실험 결과들을 도시하고, 도 16b는 단일 제약 조건이 적용 된 실험 결과들을 도시하며, 도 15c는 두 가지 제약 조건이 적용된 실험 결과를 도시한 것이다. 퍼니시먼트 함 수(Rv)가 아닌 보상 기능(R)만 도표에 표시된다. 도 15a 내지 도 16c는 개별 검색이 제약 조건 내에서 양호한 포인트들을 일관되게 찾을 수 없음을 도시한 것이다. HW 타겟 플랫폼의 컨텍스트없이 가장 정확한 CNN 모델을 검색하기 때문이다. 도 15b는 다룬 검색들보다 우월한 2개의 \"운이 좋은\" 개별 포인트들을 도시하고, 도 16b는 더 높은 보상을 도시한 것이다. 그러나, 도표에 남아있는 여덟 개의 포인트들이 모두 제한보다 훨씬 높은 대기 시간이 있음을 나타내지는 않는다. 이것은 도 15a 내지 도 15c의 모든 경우에 있어서, 표시된 축 내에 소수의 분리된 포인트만이 적합하고 나머지 포인트는 일반적으로 정확도는 높지만 효율은 매우 낮다. 이것은 HW 컨텍스트없이 설계된 CNN의 임의성을 보여준다. 그것 들은 우연에 기초한 효율 제약에 속할 수도 있고 그렇지 않을 수도 있으며, 공동의 공동 설계 방법론의 필요성 을 더욱 자극한다. 도 15a 내지 도 16c는 페이즈 및 통합 검색 전략들이 HW 가속기를 고려하고, 더 중요하게는 하드웨어 가속기의 모든 변형 및 CNN의 모든 변형을 동시에 고려하기 때문에 별도의 검색을 개선한다는 것을 도시한 것이다. 도 16a 내지 도 16c는, 통합 검색 전략이 도 16a에 도시된 제한되지 않은 실험에서 일반적으로 더 우수한 반면, 페 이즈 검색 전략은 도 16b 및 도 16c에 도시된 제한된 실험 둘 다에 대해 더 높은 보상에 도달한다는 것을 도시 한 것이다. 이는 페이즈 검색이 이상적인 포인트들에 가까워지는 도 15c에도 도시되어 있다. 그러나 도 15c는 페이즈 검색의 단점을 보여주며, 즉, 지정된 제한 조건이 누락되기 쉽다. 아마도 실험의 10,000 단계 한계 내에 서 CNN 검색 페이즈에서 FPGA 검색 페이즈로 전환할 수 있는 기회가 제한되어 있기 때문이다. 검색 단계들의 수 를 늘리면 페이즈 검색이 제약 조건 내에서 포인트를 찾을 수 있지만 실험의 런타임이 증가한다는 것을 의미해 야 한다. 보다 일반적으로, 페이즈 검색은 통합 검색에 비해 수렴하는 것이 느리다. 이것은 페이즈 검색이 최상의 결과 를 찾기 전에 몇 가지 탐색 페이즈들을 검친다는 것을 보여주는 도 16a 내지 도 16c에 강조 표시되어 있다. 따 라서, 페이즈 검색 및 통합 검색 모두 서로에 대한 장점이 있는 것으로 보인다. 통합 검색은 검색이 제한적이지 않을 때 더 잘 작동하고, 일반적으로 솔루션에 수렴하는 것이 더 빠르다. 페이즈 검색은 제약 조건이 있지만 일 반적으로 더 많은 검색 단계들이 필요한 경우 더 나은 포인트들을 찾는다. 한편, 도 5을 참조하여 위에서 설명한 바와 같이, 전술한 분석에 사용되는 CNN 서브 검색 공간을 NASBench라고 한다. 이 검색 공간에서, CNN은 ImageNet 분류를 수행하도록 트레이닝되었다. 위에서 나타낸 결과들을 검증하기 위해, 공동 설계 방법을 사용하여 다른 작업(예를 들면, Cifar-100 이미지 분류)을 최적화하는 CNN 모델 가속기 쌍을 탐색한다. Cifar-100 이미지 분류는 일반적으로 ImageNet와 유사한 상위-1 정확도 수치에 의해 반영되는 ImageNet 분류만큼 어렵다. 그러나, Cifar-100은 훨씬 작은 트레이닝 세트(60K vs 1M)를 가지고 있으므로, Cifar-100 이미지 분류를 수행하도록 CNN을 트레이닝하는 것은 ImageNet 분류보다 약 2 배 정도 더 빠르다. 이 를 통해 본 출원에서 설명된 실험들에 대하여 인프라스트럭처를 보다 효과적으로 사용할 수 있다. 이러한 작업을 수행하려면 탐색된 모든 CNN들을 처음부터 트레이닝해야 한다. 그럼에도 불구하고, 위에서 설명 된 동일한 검색 공간 SCNN이 여전히 사용될 수 있다. Ying 등에 의해 2019년 2월 arXiv e-prints에 게재된 \"NAS-Bench-101: Towards Reproducible Neural Architecture Search\"에 이러한 트레이닝이 기재되어 있다. 표 준 데이터 확장(패딩(padding), 랜덤 크롭(random crop) 및 플립핑flipping)), 코사인 감소 및 10-4의 가중치 감소를 갖는 0.1의 초기 학습 레이트를 사용하는 108개의 트레이닝 이포크(epoch)들이 존재할 수 있다. 각각의 새로운 CNN을 트레이닝하는데 약 1-GPU 시간이 걸리므로, 많은 모델들을 트레이닝할 수 있도록, 그 각각이 48개 의 모델을 병렬적으로 트레이닝할 수 있는 8개의 Nvidia-1080 GPU를 갖는 6개의 머신에 대하여 공동 설계 NAS를 병렬화할 수 있다. 공동 설계 검색은 하나로 통합된 2개의 제약 조건에 의해서 실행된다. 구체적으로, 대기 시간과 면적이 면적당 성능을 지칭하는 지표(perf/area)으로 통합되며, 이 지표는 임계값에 대한 제약 조건으로 된다. 그 다음 이 제 약 조건 하에서 정확도가 최대화된다. 면적당 성능 임계값은 (2, 8, 16, 30, 40)에 따라 점차 증가하며, 첫 번 째 임계값에서의 300개 포인트부터 시작하여 마지막 임계값에서의 약 1000 포인트까지, 총 약 2300개의 유효 포 인트에 대해 검색이 실행된다. 이것은 제어기가 고정밀 CNN의 구조를 보다 쉽게 학습할 수 있는 것으로 나타났 다. 전술한 통합 검색 전략은 솔루션에 더 빨리 수렴되기 때문에 사용된다. 도 17은 통합 검색을 사용하여 검색된 다양한 포인트들의 상위-1 정확도 및 성능/면적을 나타낸다. 각 임계값에 서 방문된 모델 가속기 포인트들 중 상위 10개 포인트가 나타나 있다. 이 도면은 또한 도 5에 도시된 CNN 스켈 레톤 내의 ResNet 및 GoogLeNet 셀들을 보여주며, 이들은 성능/면적의 측면에서 가장 최적의 가속기와 쌍을 이 룬다. FPGA 검색 공간에서 가장 적합한 대응 가속기에서 구현될 때 2개의 공지된 고정확도 CNN 셀과 비교하여 이것은 능가하기 어려운 기준이다. 그러나, 이 도면에서 알 수 있는 바와 같이, ResNet 및 GoogLeNet 기준들의 정확도와 효율성을 모두 초과하는 많은 포인트들이 존재한다. 최적의 2개의 포인트가 각각 Cod-1과 Cod-2로 표시되어 있다. 이들의 성능이 하기 표 2에 나와 있다. 표 2"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "Cod-1은 ResNet에 비해 정확도를 1.8% 향상시키는 동시에 성능/면적을 41% 향상시킨다. 이들은 정확도 및 효율 성 모두에 있어서 상당한 이득이다. Cod-2는 GoogLeNet에 비해 작은 개선을 보여 주지만, 절대 대기 시간 측면 에서 4.2 % 더 빠르게 실행되는 동시에 효율성과 정확도 모두를 능가한다. 도 18a 및 도 18b는 각각 Cod-1 및 Cod-2의 모델 구조를 나타낸 것이며, 하기의 표 3은 HW 파라미터들을 나열한 것이다. 표 3"}
{"patent_id": "10-2020-0034093", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "Cod-1은 ResNet 정확도를 능가하지만 다음과 같은 중요한 ResNet 특징을 사용한다: 도 18a에서 셀의 가장 오른 쪽 브랜치에 도시된 바와 같은 스킵 연결들 및 요소별 추가될 수 있다. 하드웨어 측면에서, Cod-1과 Cod-2는 가 장 큰 컨볼루션 엔진을 사용하며 전용 풀링 엔진을 사용하지 않을 수 있다. 그러나, 다른 HW 파라미터들은 각 CNN에 맞게 조정된다. 예를 들어, 입력 버퍼 크기와 메모리 인터페이스 너비가 Cod-1의 경우 Cod-2보다 작다. 이것은 Cod-1 CNN이 Cod-2에 비해 더 작은 컨볼루션들을 더 많이 사용하기 때문일 수 있다. 검색 공간의 총 개수는 약 37 억 포인트들이므로, Cod-1 및 Cod-2보다 더 나은 포인트들이 존재할 수 있다. Cod-1을 찾아내기 전에 약 1000개의 포인트들만이 탐색되었지만, Cod-2를 찾아내기 전에는 약 2000개의 포인트 들이 탐색되었다. 이것은 통합 검색을 사용할 때 제어기의 수렴 속도를 강조한다. 특히 전술한 바와 같은 대 표적인 보상 함수들과 검색 전략들에 의해서 적절히 조정될 경우, 좋은 설계들을 찾아내는데 효과적이다. 도 19은 도 4a의 구성에 통합되는 CNN 서브 검색 공간을 검색하는데 사용될 수 있는 대안적인 시스템을 도시한 것이다. 일 실시예로, 전자 장치에 포함된 프로세서는 컷 오프 모델에 공급되는 CNN에 대한 모델 아키텍처를 입력할 수 있다. 컷 오프 모델는 프로세서에 빠른 피드백을 제공하기 위한 컷 오 프로서, 지연 시간 및 메모리 풋프린트에 대한 임계값들과 같은, 하드웨어 지표들을 사용할 수 있다. 프로세서 에 의해 제안된 모델이 하드웨어 기준을 만족하지 않으면, 유사한 성능이 떨어지는 모델들을 제안하지 않 도록 피드백을 받을 수 있다. 따라서, 프로세서가 하드웨어 제약 조건을 만족시키는 모델들을 제안하는데 집중할 수 있게 한다. 제안된 모델이 하드웨어 기준을 만족하면, 프로세서는 제안된 모델에 대한 보다 상 세한 평가를 받기 위해(예를 들어, 전술한 바와 같이, 보상 함수를 생성하기 위해) 제안된 모델을 평가 모델 로 전송된다. 컷 오프 모델은 동적일 수 있으며, 이에 따라 검색에 의해 위치되는 모델들을 개선하기 위해 검색이 진행 됨에 따라 하드웨어 지표들이 변경될 수 있다. 예를 들어, 초기 대기 시간 임계값이 100 ms이지만 많은 모델들 이 50 ms인 대기 시간을 갖는 경우, 대기 시간 임계값은 온 더 플라이로(예컨대, 실시간으로) 약 60 ms로 업데 이트될 수 있다. 이러한 방식으로, 프로세서는 더 많은 모델들이 검색에서 제외되어 전체 검색 프로세스 가 신속하게 처리할 수 있다.개략적으로 도시된 바와 같이, 컷 오프 모델은 모든 장치에 맞는 모델들을 검색하기 위해, 복수의 하드웨어 장 치 H/W 1, H/W 2, … H/W N을 동시에 사용할 수 있다. 도 20는 본 개시의 일 실시예에 따른, 컷 오프 모델이 하드웨어 런타임 추정기(H/W runtime estimator) 및 검증기(validator)를 포함하는 도면으로, 도 19에 도시된 시스템보다 복잡한 버전 일 수 있다. 하드웨어 런타임 추정기는 타겟 하드웨어 플랫폼(들)에서 제어기에 의해 제안된 모델의 하드 웨어 성능, 예를 들면, 대기 시간을 예측하는데 사용될 수 있다. 제안된 모델 아키텍처를 실행하는데 필요한 총 FLOPS 수 또는 그것의 파라미터 크기 간의 관계는 온/오프 칩 메모리 활용도, 메모리 풋프린트, 병렬 처리 수준, 면적 사용, 클럭 속도 또는 임의의 다른 관련 작업 또는 하드웨어 메트릭의 변화로 인하여 특정 하드웨어 플랫폼에서 대기 시간과 비선형 관계를 가질수 있다. 하드웨어 런타임 추정기는 예측 모듈(또는, 통계 모듈(statistical module), 판별기 (dsicriminator), 이론적 하드웨어 모듈 및 배치 모듈을 포함할 수 있다. 도 20은 각 모델 은 프로세서와는 별개의 휘발성 메모리에 로딩된 상황을 도시하고 있으나 이는 일 실시예에 불과하며, 프로세서 에 포함된 휘발성 메모리에 로딩될 수 있음은 물론이다. 그리고, 각 모델은 프로세서에 의해 제어 및 학습될 수 있다. 예측 모듈은 하드웨어 지표들을 예측(즉, 추정)하고 이들을 판별 기에 전송하는데 사용될 수 있다. 초기에, 예측 모델은 이론적 하드웨어 모델 모듈에서 계산되어 예측 모듈로 전송되는 기준 예측을 제공하는 이론 모델에 기반한다. 모델들은 예측 품질이 좋지 않을 수 있다(특히 초기 모델들). 따라서, 판별기은 예측 모듈로부터의 결과들에 대한 신뢰도를 모니터링할 수 있다. 추정된 하드웨어 지표의 신뢰도가 낮을 경우(예를 들어, 신뢰 임계값 미만), 제안된 아키텍처는 타겟 하드웨어 (예를 들어, 하드웨어 장치들, H/W 1, H/W 2, … H/W N 중 하나)상의 배치를 위해 배치 모듈로 전송될 수 있다. 대기 시간(latency)(또는 다른 하드웨어 지표)이 측정되며, 획득된 측정치가 예측 모델 모듈로 전 송됨으로써 예측 모델은 업데이트될 수 있다. 그리고, 획득된 측정치가 판별기로 전송됨으로써 판별기 내 의 모니터링 프로세스 동작을 최적화하도록 판별기가 학습될 수 있다. 추정된 값이 아닌 실제 측정치가 모델과 함께 검증기로 전송될 수 있다. 추정된 하드웨어 지표들에 대한 신뢰도가 양호할 경우(예를 들어, 임계값 초과), 모델이 검증기로 바로 전송된다. 추정된 하드웨어 값(들) 또는 측정된 하드웨어 값(들)을 갖는 모델을 수신하면, 검증기은 제안된 아키텍 처가 모든 하드웨어 지표들을 만족하는지 여부를 검사할 수 있다. 다시 말해, 검증기은 하드웨어 값(들) 을 정의된 임계값들과 비교하여 하드웨어 제약 조건이 만족되는지 여부를 결정할 수 있다. 제안된 모델이 하드 웨어 기준을 만족하는 경우, 이 모델은 전술한 바와 같이 보다 상세한 평가를 위해(예를 들면, 보상 함수를 생 성하기 위해) 평가 모델로 전송된다. 따라서, 이러한 구성에서, 프로세서는 CNN에 대해 제안된 모 든 모델 아키텍처들을 하드웨어 런타임 추정기로 전송한다는 것이 명백하다. 구체적으로, 도면에 도시된 바와 같이, 제안된 모델 아키텍처들이 예측 모델 및 판별기로 전송된다. 도 20에서 설명된 방법은 도 4a의 구현 및 평가 단계들(단계 S204 및 단계 S206)을 모델링하는데 사용될 수 있 다. 이로 인해 반복할 때마다 하드웨어를 풀링할 필요가 없기 때문에 런타임이 더 빨라질 수 있다. 또한, 전체 검색 절차는 전체 GPU 시간 버짓을 제공함으로써 구성될 수 있음에 유의한다. 따라서, 계산 버짓이 끝나면, 모 든 요구 사항들을 충족하는 최상의 모델을 얻게 된다. 도 21은 본 개시의 일 실시예에 따른, 예측 모델이 지속적으로 업데이트되는 방식을 도시한 도면이다. 해당 방 식은 내부의 모듈들 중 하나 이상의 것을 사용하여 런타임 추정기에서 수행될 수 있다. 제1 단계에 나타낸 바와 같이, 예측 모델은 제안되는 신경망 모델(예를 들어, CNN 모델)을 프로세서로부터 수신할 수 있다(S1500). 그리 고, 예측 모델을 실행하기 전에, 프로세서는 제안된 신경망 모델들이 예측 모델에 몇 번 전송되었는지 여부를 식별할 수 있다. 예를 들어, 프로세서는 예측 모델에 신경망 모델이 N 번 반복하여 전송되었는지 여부를 식별할 수 있다(S1502). 이 때, N 번은 임계 횟수를 의미하며 기설정된 숫자일 수 있으며 실험 또는 통계 등을 통해 도 출된 횟수 일 수 있다. 만약, 예측 모델에 신경망 모델이 전송되는 횟수가 N번 미만인 경우, 선택된 신경망 모 델이 FPGA에서 실행될 때, 예측 모델은 발생하는 대기 시간과 같은 하드웨어 파라미터들을 예측하기 위해 수신 된 신경망 모델에 적용된다(단계 S1504). 그 다음 프로세스는 다음 수신된 신경망 모델에 대해 반복하기 위해 시작으로 되돌아간다. 이미 예측 모델의 N 반복 이상이 있었던 경우, 제안된 신경망 모델은, 예를 들어 하드웨어 파라미터의 실제 측 정을 제공하기 위해 도 19에 도시된 배치 모듈 및 복수의 하드웨어 모듈들 중 하나를 사용함으로써, 실제 가속 기 하드웨어 상에서 구현될 수 있다(단계 S1506). 그리고, 하드웨어 파라미터들을 예측하기 위해 예측 모델이 적용될 수 있다(단계 S1508). 이들 단계들이 순차적으로 도시되어 있지만, 동시에 또는 다른 순서로 수행될 수 도 있음을 이해할 것이다. 예측된 파라미터와 측정된 파라미터 사이에 불일치가 있는 경우, 측정된 파라미터들 을 사용하여 예측 모델을 업데이트할 수 있다(단계 S1510). 그 다음, 프로세스는 다음 수신 모델에 대해 반복하 기 위해 시작으로 되돌아간다. 이러한 방법은 항상 실제 하드웨어를 사용하여 성능을 결정하는 방법과 비교할 때 스케일링을 가능하게 하고 런 타임을 향상시킨다. 예를 들어, 복수의 스레드들 또는 프로세스들이 통계 모델을 사용하여 새로운 CNN 모델들을 검색할 수 있으며, 단일의 실제 하드웨어 장치가 통계 모델을 간헐적으로 업데이트하는데 사용된다. 정기적 측 정들을 사용하면 통계 모델이 더 정확하고 최신 상태일 수 있다. 예측 모델은 수행뿐만 아니라 그것이 생성되 는 트레이닝 데이터를 포함한다. 새로운 CNN 모델들에 대한 검색이 수행됨에 따라, 원래 모델이 트레이닝되지 않은 데이터를 포함하여 상이한 검색 공간들로 이동할 수 있다. 따라서, 측정치들로 예측 모델을 업데이트하면 통계 모델이 검색을 안내하는데 사용되는 대표적인 하드웨어 메트릭들을 계속 예측할 수 있게 된다. 예측된 하 드웨어 지표들 및 측정된 하드웨어 지표들 사이의 모든 오류는 하드웨어에서 CNN 모델 구현 사이의 반복 횟수를 조정하는데 사용될 수도 있다. 예를 들어, 오류가 증가하면, 하드웨어 풀링 사이의 반복 횟수가 줄어들 수 있으 며 그 반대도 마찬가지이다. 도 22는 판별기이 신뢰할 수 있는 예측과 유효하지 않은 예측을 구별하는 방법을 학습하도록 돕기 위해, 도 20 의 판별기에 의해서 도 21에 나타낸 것과 유사한 방법이 어떻게 사용될 수 있는지를 도시한 것이다. 본 제안된 기술은 선택 프로세스의 런타임에 크게 영향을 주지 않으면서 훨씬 더 나은 예측 모델을 생성함으로써 선택 프 로세스 내의 하드웨어 인식을 향상시킬 수 있다. 본 개시의 일 실시예로, S1600 및 S1602로 도시된 바와 같이, 판별기는 프로세서로부터 제안된 모델 및 예측 모 델로부터 예측된 하드웨어 지표들을 수신할 수 있다. 위 단계들이 특정 순서로 나타나 있지만 이 정보는 동시에 또는 상이한 순서로 수신될 수도 있다. 판별기는 예측된 하드웨어 지표들이 신뢰될 수 있는지의 여부를 결정할 수 있다(S1604). 일 실시예로, 판별기는 예측된 하드웨어 지표들을 신뢰할 수 있는지 여부를 결정하는 동안, 예 측된 지표들이 검증될 필요가 있는지 여부를 식별할 수 있다(단계 S1606). 판별기는 검증 결정은 상이한 정책들 에 따라, 예를 들어 고정된 횟수의 반복 이후에, 임의의 간격으로 또는 시스템의 출력들을 평가함으로써 이루어 질 수 있다. 일 실시예로, 검증이 필요하지 않은 경우, 예측된 하드웨어 파라미터들은 검증기에게 출력됨으로써 (S1608), 전술한 바와 같이 평가 모델에게 전달할지 여부를 결정한다. 판별기가 예측된 지표들을 신뢰할 수 없는 것으로 결정하는 경우, 제안된 신경망 모델이 실제 가속기 하드웨어 상에서 구현 또는 실행됨으로써, 관심 대상인 하드웨어 지표(예를 들어, 대기 시간)의 측정치들을 획득한다 (S1610). 도 21에서 상술한 바와 같이, 예측된 파라미터와 측정된 파라미터 사이에 불일치가 있으면, 측정된 파 라미터들을 사용하여 예측 모델을 업데이트할 수 있다(S1612). 측정된 HW 파라미터들은, 예를 들어 전술한 바와 같이 모델을 평가 모델에게 전달할지 여부를 결정하기 위해 검증 모델에 출력된다(단계 S1614). 유사하게, 판별 기가 예측된 지표들이 검증될 필요가 있는 것으로 결정하는 경우, 제안된 모델을 하드웨어 상에서 실행하고 (S1610), 필요한 예측 모델을 업데이트하며(S1612), 또한 측정된 파라미터들을 출력하는(S1614) 단계들이 수행 된다. 모든 경우에, 측정되거나 예측된 파라미터들이 출력되면, 프로세스는 다음 수신 모델에 대해 반복하기 위 해 시작으로 되돌아간다. 상기 설명에서, 하드웨어 지표들 및 하드웨어 파라미터라는 용어는 상호 교환적으로 사용된다. 대기 시간과 같 은 특정 지표들을 추정하거나 측정하기 어려울 수 있으므로, FLOP 및 모델 크기와 같은 프록시 지표들이 원하는 지표들에 대한 추정치들로서 사용될 수 있다. 전술한 통계 모델들은 특정 타입의 CNN에 대해 이전에 캡처된 하 드웨어 측정치들을 사용하여 트레이닝될 수 있다. 통계 모델들은 모델 속성들(예를 들어, 파라미터들의 수, FLOP들, 계층간 연결, 작업 유형 등)로부터 하드웨어 메트릭들(예를 들어, 대기 시간)을 근사화하는 이론적 모 델들을 사용하여 구축될 수 있다. 이론적 모델들은 각 계층에 대한 다양한 정확도/충실도와 함께 각 계층 유형 (예를 들어, 컨볼루션, 최대 풀, relu 등)에 대해 별개의 수학식들을 가질 수 있다. 통계 모델들 대신 이론적 모델들을 사용할 수도 있다. 상기 설명에서, CNN 및 FPGA 제어기를 공동 설계 또는 설계하는 것이 참조되었다. 그러나, 이 방법은 CNN들에 적용될 수 있을 뿐만 아니라 전술한 기술들을 사용하여 임의의 뉴럴 네트워크로 쉽게 확장될 수 있음을 이해할 것이다. 이 방법은 또한 하드웨어, 예를 들어 압축 알고리즘 및 암호화 알고리즘에서 유리하게 구현되는 임의의파라미터화 가능 알고리즘에 보다 광범위하게 적용 가능하다. 이 방법이 작동하기 위해서는, 양호하게 정의된 알고리즘 검색 공간을 가질 수 있어야 한다는 것이 이해될 것이다(예를 들어, 파라미터화 가능 알고리즘은 하나 이상의 구성 가능한 파라미터에 의해 정의될 수 있어야 한다). 예를 들어, 전술한 방법에서는, 검색 공간이 도 4과 관련하여 설명된 모델을 사용하여 정의된다. 그러나, 이 모델은 단지 예시적인 것이며, 모델링될 뉴럴 네트 워크의 파라미터들을 설정함으로써, 파라미터화 가능 알고리즘들의 다른 모델들이 사용될 수 있음을 이해할 것 이다. 유사하게, 이 방법은 FPGA 제어기뿐만 아니라 다른 타입의 하드웨어에도 적용될 수 있음을 이해할 것이다. 본 개시의 특정 양태들이 예시적인 실시예들에 따른 시스템, 방법, 장치 및/또는 컴퓨터 프로그램 제품의 블록 도 및 흐름도를 참조하여 위에서 설명되었다. 블록도 및 흐름도의 하나 이상의 블록, 및 블록도 및 흐름도의 블록의 조합은 각각 컴퓨터 실행 가능 프로그램 명령어들의 실행에 의해 구현될 수 있음을 이해할 것이다. 마 찬가지로, 블록도 및 흐름도의 일부 블록들이 반드시 제시된 순서대로 수행될 필요는 없으며, 일부 실시예들에 따르면, 반드시 수행될 필요가 없을 수도 있다. 또한, 블록도 및/또는 흐름도의 블록들에 도시된 것 이외의 추 가 구성 요소 및/또는 동작이 특정 실시예들에서 제공될 수도 있다. 따라서, 블록도 및 흐름도의 블록들은 특정 기능들을 수행하기 위한 수단들의 조합, 특정 기능들을 수행하기 위 한 요소들 또는 단계들의 조합, 및 특정 기능들을 수행하기 위한 프로그램 명령어 수단을 지원한다. 블록도 및 흐름도의 각 블록, 및 블록도 및 흐름도의 블록들의 조합은 특정 기능들, 요소들 또는 단계들 또는 특수 목적 하드웨어 및 컴퓨터 명령어들의 조합을 수행하는 특수 목적 하드웨어 기반 컴퓨터 시스템들에 의해 구현될 수 있음을 이해해야 한다. 본 개시의 특정 실시예들이 설명되었지만, 당업자는 다수의 다른 수정들 및 대안적인 실시예들이 본 개시의 범 위 내에 있음을 인식할 것이다. 예를 들어, 특정 시스템, 시스템 구성 요소, 장치 또는 장치 구성 요소와 관련 하여 설명된 기능 및/또는 처리 능력들 중 임의의 것이다른 시스템, 장치 또는 구성 요소에 의해 수행될 수도 있다. 또한, 다양한 예시적인 구현들 및 아키텍처들이 본 개시의 실시예들에 따라 설명되었지만, 당업자는 여기 에 설명된 예시적인 구현들 및 아키텍처들에 대한 다수의 다른 수정들이 또한 본 개시의 범위 내에 있음을 이해 할 것이다. 당업자는 본 기술들이 광범위한 응용들을 가지며, 실시예들은 첨부된 청구 범위에 정의된 바와 같은 임의의 본 발명의 개념을 벗어나지 않으면서 광범위한 수정들을 취할 수 있음을 인식할 것이다. 한편, 본 개시에 첨부된 도면은 본 개시에 기재된 기술을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 개시의 실시예의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물(alternatives)을 포함하 는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사용될 수 있다. 본 개시에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. 본 개시에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\"등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 개시에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중 요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 상기 어떤 구성요소가 상기 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요 소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소(예: 제1 구성요소)가 다른 구성 요소(예: 제2 구성요소)에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 상기 어떤 구성요 소와 상기 다른 구성요소 사이에 다른 구성요소(예: 제 3 구성요소)가 존재하지 않는 것으로 이해될 수 있다. 본 개시에서 사용된 표현 \"~하도록 구성된(또는 설정된)(configured to)\"은 상황에 따라, 예를 들면, \"~에 적합 한(suitable for),\" \"~하는 능력을 가지는(having the capacity to),\" \"~하도록 설계된(designed to),\" \"~하도록 변경된(adapted to),\" \"~하도록 만들어진(made to),\" 또는 \"~를 할 수 있는(capable of)\"과 바꾸어 사용될 수 있다. 용어 \"~하도록 구성된(또는 설정된)\"은 하드웨어적으로 \"특별히 설계된(specifically designed to)\" 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, \"~하도록 구성된 장치\"라는 표현은, 그 장치가 다른 장치 또는 부품들과 함께 \"~할 수 있는\" 것을 의미할 수 있다. 예를 들면, 문구 \"A, B, 및 C를 수행하도록 구성된(또는 설정된) 부프로세서\"는 해당 동작을 수행하기 위한 전용 프로세서(예: 임베디드 프로세서), 또는 메모리 장치에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 한편, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전자 장 치)를 지칭할 수 있다. 이하에서는 도면을 참조하여 본 개시에 대해 더욱 상세히 설명하도록 한다. 본 개시의 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명 령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시 예들에 따른 전자 장치(예: 전자 장치)를 포함할 수 있다. 상기 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 상기 프로세 서의 제어하에 다른 구성요소들을 이용하여 상기 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또 는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적 (non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적 저장매체'는 신호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시적으로 저장됨을 구분하 지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시 예에 따르면, 본 개시에 개시된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래 될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD- ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예를 들어, 다운로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임 시적으로 생성될 수 있다. 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으 며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나 의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행 할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다.도면 도면1 도면2 도면3 도면4a 도면4b 도면4c 도면5 도면6 도면7a 도면7b 도면8 도면9 도면10 도면11a 도면11b 도면11c 도면11d 도면12a 도면12b 도면12c 도면12d 도면13 도면14 도면15a 도면15b 도면15c 도면16a 도면16b 도면16c 도면17 도면18a 도면18b 도면19 도면20 도면21 도면22"}
{"patent_id": "10-2020-0034093", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 개시의 일 실시예에 따른, DNN을 설계하는 시스템을 설명하기 위한 블록도, 도 2는 본 개시의 일 실시예에 따른, 전자 장치의 구성 및 동작을 설명하기 위한 블록도, 도 3a은 본 개시의 일 실시예에 따른, CNN 및 FPGA를 공동 설계하는 방법의 흐름도, 도 4a, 도 4b 및 도 4c는 본 개시의 일 실시예에 따른, 전자 장치의 구성 및 동작을 설명하기 위한 도면, 도 5는 본 개시의 일 실시예에 따른, 도 4a의 순서도에서 사용될 수 있는 CNN 서브 검색 공간을 개략적으로 도 시한 도면, 도 6는 본 개시의 일 실시예에 따른, FPGA 가속기의 구성 요소들의 개략도, 도 7a는 본 개시의 일 실시예에 따른, 2가지 타입의 가속기 아키텍처를 위한 리소스 사용에 대한 면적을 나타내 는 도면, 도 7b는 본 개시의 일 실시예에 따른, 도 7a에 도시된 타입들의 가속기 아키텍처의 병렬 처리에 대한 이미지당 대기 시간을 나타내는 도면, 도 8은 본 개시의 일 실시예에 따른, 크기 및 픽셀-파에 대한 대기 시간을 나타내는 도면, 도 9은 본 개시의 일 실시예에 따른, 정확도, 대기 시간 및 면적에 대한 일부 파레토-최적점들을 나타내는 도면, 도 10은 도 9에 도시된 파레토-최적점들에 대한 대기 시간에 대한 정확도를 도시한 도면, 도 11a, 도 11b, 도 11c 및 도 11d는 본 개시의 일 실시예에 따른, 55 mm2 미만, 70 mm2 미만, 150 mm2 미만 및 220 mm2 미만의 각각의 면적 제한에서의 단일 및 듀얼 컨볼루션 엔진들에 대한 정확도-대기 시간 파레토 프론티어를 도시한 도면, 도 12a는 본 개시의 일 실시예에 따른, 제약 조건을 적용하여 대기 시간에 대한 정확도를 도시한 도면, 도 12b 및 도 12c는 본 개시의 일 실시예에 따른, 도 12a로부터 선택된 CNN의 2가지 예시적인 구성을 도시한 도 면, 도 12d는 본 개시의 일 실시예에 따른, 도 12c의 공동 설계 가속기 및 상이한 가속기에서 실행되는 CNN 실행 스 케줄을 비교한 도면, 도 13은 본 개시의 일 실시예에 따른, 파라미터 ratio_conv_engines에 대한 파레토 최적점의 전체적인 랜드스케 이프를 나타내기 위한 대기 시간에 대한 정확도를 도시한 도면, 도 14는 본 개시의 일 실시예에 따른, 페이즈 검색을 구현하는데 사용될 수 있는 대안적인 아키텍처를 도시한 도면, 도 15a는 본 개시의 일 실시예에 따른, 대기 시간에 대한 정확도를 도시하고, 비제약 조건 검색에 대한 상위 검 색 결과를 강조 표시한 도면, 도 15b는 본 개시의 일 실시예에 따른, 대기 시간에 대한 정확도를 도시하고, 하나의 제약 조건이 있는 검색에 대한 상위 검색 결과를 강조 표시한 도면, 도 15c는 본 개시의 일 실시예에 따른, 대기 시간에 대한 정확도를 도시하고, 두 가지 제약 조건이 있는 검색에 대한 상위 검색 결과들을 강조 표시한 도면, 도 16a, 도 16b 및 도 16c는 본 개시의 일 실시예에 따른, 도 15a 내지 도 15c의 비제약 조건 및 제약 조건 검 색들에서의 각각의 개별, 통합 및 페이즈 검색 전략들에 대한 보상 값들을 나타내는 도면, 도 17은 본 개시의 일 실시예에 따른, 조합 검색을 사용하여 검색된 다양한 포인트들에 대해 성능/면적에 대한 상위-1 정확도(top-1 accuracy)를 도시한 도면, 도 18a 및 도 18b는 본 개시의 일 실시예에 따른, 도 15에서 선택된 CNN의 2개의 예시적인 구성을 도시한 것이다. 도 19 및 도 20은 본 개시의 일 실시예에 따른, 도 4a의 방법과 함께 사용될 수 있거나 독립적 검색을 수행하는 대안적인 아키텍처를 도시한 도면, 도 21은 본 개시의 일 실시예에 따른, 도 20의 아키텍처에 구현될 수 있는 방법의 순서도, 도 22은 본 개시의 일 실시예에 따른, 도 20의 아키텍처에 구현될 수 있는 대안적인 방법의 순서도이다."}
