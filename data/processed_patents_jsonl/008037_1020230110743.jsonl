{"patent_id": "10-2023-0110743", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0029612", "출원번호": "10-2023-0110743", "발명의 명칭": "모션 기반 사용자 감정 상호 작용 방법 및 시스템", "출원인": "주식회사 피씨엔", "발명자": "송광헌"}}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨팅 장치에서 수행되는 사용자 감정 상호 작용 방법에 있어서,사용자의 움직임을 촬영한 사용자영상을 취득하는 단계;상기 사용자영상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감정관련모션인 경우 해당 사용자 모션에 상응하는 사용자감정을 결정하는 단계; 및상기 사용자 감정을 제공서비스에 반영하는 단계를 포함하는, 모션 기반 사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 사용자 모션에 상응하는 사용자감정의 결정에 해당 시점의 상기 제공서비스의 내용을 활용하는, 모션 기반사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 사용자 모션에 대한 모션종료후 유지시간을 이용하여 감정관련모션인지 여부를 결정하는, 모션 기반 사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,상기 사용자 모션이 감정관련모션인 경우 일정시간 내로 연속된 다음 모션이 존재하는지 여부를 확인하고, 연속된 다음 모션이 감정무관모션인 경우 상기 사용자 모션은 감정관련 모션이 아닌 것으로 결정하는, 모션 기반 사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 4에 있어서,상기 연속된 다음 모션도 감정관련모션인 경우, 각 모션에 설정된 감정에 대한 상관관계를 확인하여 상기 사용자감정을 결정하는, 모션 기반 사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 1에 있어서,복수개의 감정을 유발하는 동영상 또는 사진을 포함하는 감정유발콘텐츠를 사용자에게 제공하고, 상기 감정유발콘텐츠가 재생되는 동안 변화되는 사용자의 모션과 그 시점의 감정유발콘텐츠의 내용을 이용하여 사용자감정정보를 생성 및 저장하며, 저장된 상기 사용자감정정보를 더 이용하여 상기 사용자감정을 결정하는, 모션 기반 사용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 1에 있어서,상기 사용자영상에서 사용자의 얼굴 표정변화를 더 확인하고, 얼굴 표정변화 여부에 따라 상기 사용자감정의 강도 수치를 산출하며, 상기 강도 수치에 따라 상기 사용자감정에 대한 상기 제공서비스로의 반영방식을 달리 적용하는, 모션 기반 사공개특허 10-2025-0029612-3-용자 감정 상호 작용 방법."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "모션 기반 사용자 감정 상호 작용 방법을 수행하도록 하는 컴퓨터-판독 가능 매체에 저장된 컴퓨터 프로그램으로서, 상기 컴퓨터 프로그램은 컴퓨터로 하여금 이하의 단계들을 수행하도록 하며, 상기 단계들은,사용자의 움직임을 촬영한 사용자영상을 취득하는 단계;상기 사용자영상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감정관련모션인 경우 해당 사용자 모션에 상응하는 사용자감정을 결정하는 단계; 및상기 사용자 감정을 제공서비스에 반영하는 단계를 포함하는, 컴퓨터-판독 가능 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2023-0110743", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "사용자의 움직임을 촬영한 사용자영상을 취득하기 위한 통신부;상기 사용자영상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감정관련모션인 경우 해당 사용자 모션에 상응하는 사용자감정을 결정하는 감정인식부; 및상기 사용자 감정을 제공서비스에 반영하는 상호작용부를 포함하는, 모션 기반 사용자 감정 상호 작용 시스템."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "모션 기반 사용자 감정 상호 작용 방법 및 시스템이 개시된다. 본 발명의 일측면에 따른 컴퓨팅 장치에서 수행되 는 모션 기반 사용자 감정 상호 작용 방법은, 사용자의 움직임을 촬영한 사용자영상을 취득하는 단계; 상기 사용 자영상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감 정관련모션인 경우 해당 사용자 모션에 상응하는 사용자감정을 결정하는 단계; 및 상기 사용자 감정을 제공서비 스에 반영하는 단계를 포함한다."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 모션 기반 사용자 감정 상호 작용 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에는 메타버스 등 가상현실을 넘어 확장현실에 대한 서비스가 확장되고 있다. 종래의 확장현실을 위한 사용자 인터페이스는 키보드, 마우스와 같은 언어적 입력장치만을 이용함으로써 사용자 표현(감정)을 반영하기에는 극히 한정적이고 제한적이었다. 또한, 게임제작자, 방송관계자 등의 전문인력들은 사용자 감정인 인식하기 위한 제스처 인식, 표정인식 등을 수 행하기 위해 다수의 고가 장비를 이용하게 되는데, 이는 다양한 사용자를 위한 메타버스 서비스 등에 적용하기 에는 한계가 있다. 또한, 최근에는 온라인을 이용한 수업, 화상채팅 방식의 고객 상담 서비스 등 영상 기반의 서비스가 다양화되고 있어, 상대방의 감정을 영상분석에 의해 자동인식하여 고객 맞춤형 서비스를 제공하고자 하는 니즈가 많다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 제10-2368300 음성 및 표정에 기반한 캐릭터의 동작 및 감정 표현 시스템"}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "따라서, 본 발명은 상술한 문제점을 해결하기 위해 안출된 것으로서, 영상에서 사용자 모션을 분석하여 감정을 인식하는, 모션 기반 사용자 감정 상호 작용 방법 및 시스템을 제공하기 위한 것이다. 또한, 본 발명은 각 모션의 연결성을 분석하여 감정에 대한 인식률을 높일 수 있는, 모션 기반 사용자 감정 상 호 작용 방법 및 시스템을 제공하기 위한 것이다. 본 발명의 다른 목적들은 이하에 서술되는 바람직한 실시예를 통하여 보다 명확해질 것이다."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 측면에 따르면, 컴퓨팅 장치에서 수행되는 사용자 감정 상호 작용 방법에 있어서, 사용자의 움직 임을 촬영한 사용자영상을 취득하는 단계; 상기 사용자영상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감정관련모션인 경우 해당 사용자 모션에 상응하는 사용자감정 을 결정하는 단계; 및 상기 사용자 감정을 제공서비스에 반영하는 단계를 포함하는, 모션 기반 사용자 감정 상 호 작용 방법 및 그 방법을 실행하는 프로그램이 제공된다. 여기서, 상기 사용자 모션에 상응하는 사용자감정의 결정에 해당 시점의 상기 제공서비스의 내용을 활용할 수 있다. 또한, 상기 사용자 모션에 대한 모션종료후 유지시간을 이용하여 감정관련모션인지 여부를 결정할 수 있다. 또한, 상기 사용자 모션이 감정관련모션인 경우 일정시간 내로 연속된 다음 모션이 존재하는지 여부를 확인하고, 연속된 다음 모션이 감정무관모션인 경우 상기 사용자 모션은 감정관련 모션이 아닌 것으로 결정할 수 있다. 또한, 상기 연속된 다음 모션도 감정관련모션인 경우, 각 모션에 설정된 감정에 대한 상관관계를 확인하여 상기 사용자감정을 결정할 수 있다. 또한, 복수개의 감정을 유발하는 동영상 또는 사진을 포함하는 감정유발콘텐츠를 사용자에게 제공하고, 상기 감 정유발콘텐츠가 재생되는 동안 변화되는 사용자의 모션과 그 시점의 감정유발콘텐츠의 내용을 이용하여 사용자 감정정보를 생성 및 저장하며, 저장된 상기 사용자감정정보를 더 이용하여 상기 사용자감정을 결정할 수 있다. 또한, 상기 사용자영상에서 사용자의 얼굴 표정변화를 더 확인하고, 얼굴 표정변화 여부에 따라 상기 사용자감 정의 강도 수치를 산출하며, 상기 강도 수치에 따라 상기 사용자감정에 대한 상기 제공서비스로의 반영방식을 달리 적용할 수 있다. 본 발명의 다른 측면에 따르면, 사용자의 움직임을 촬영한 사용자영상을 취득하기 위한 통신부; 상기 사용자영 상에서 사용자 모션을 인식하면, 인식된 사용자 모션이 미리 등록된 감정관련모션인지 여부를 확인하고, 감정관 련모션인 경우 해당 사용자 모션에 상응하는 사용자감정을 결정하는 감정인식부; 및 상기 사용자 감정을 제공서 비스에 반영하는 상호작용부를 포함하는, 모션 기반 사용자 감정 상호 작용 시스템이 제공된다. 전술한 것 외의 다른 측면, 특징, 이점이 이하의 도면, 특허청구범위 및 발명의 상세한 설명으로부터 명확해질 것이다."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 영상에서 사용자 모션을 분석하여 감정을 인식함으로써, 확장현실, 온라인 수업, 화상기반 고객상담 등 다양한 화상 기반 서비스에 활용할 수 있다. 또한, 본 발명에 따르면 각 모션의 연결성을 분석하여 감정인식에 대한 정확도를 높일 수 있다."}
{"patent_id": "10-2023-0110743", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아 니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의 해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된 다. 예를 들어, 후술될 제1 임계값, 제2 임계값 등의 용어는 실질적으로는 각각 상이하거나 일부는 동일한 값인 임계값들로 미리 지정될 수 있으나, 임계값이라는 동일한 단어로 표현될 때 혼동의 여지가 있으므로 구분의 편 의상 제1, 제2 등의 용어를 병기하기로 한다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들 을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요 소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 또한, 각 도면을 참조하여 설명하는 실시예의 구성 요소가 해당 실시예에만 제한적으로 적용되는 것은 아니며, 본 발명의 기술적 사상이 유지되는 범위 내에서 다른 실시예에 포함되도록 구현될 수 있으며, 또한 별도의 설명 이 생략될지라도 복수의 실시예가 통합된 하나의 실시예로 다시 구현될 수도 있음은 당연하다. 또한, 첨부 도면을 참조하여 설명함에 있어, 도면 부호에 관계없이 동일한 구성 요소는 동일하거나 관련된 참조 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 도 1은 본 발명의 일 실시예에 따른 모션 기반 사용자 감정 상호 작용 시스템의 구성을 도시한 기능블록도이다. 도 1을 참조하면, 본 실시예에 따른 모션 기반 사용자 감정 상호 작용 시스템(이하 감정상호작용 시스템이라 칭 함)은 저장부, 통신부 및 제어부를 포함하되, 제어부는 사용자관리부, 감정인식부 및 상호작용부를 포함할 수 있다. 저장부에는 제어부가 기능하기 위해 필요한 데이터들이 저장되며, 또한 사용자 맞춤형 서비스를 위한 사용자감정정보가 저장된다. 사용자감정정보에 대해서는 차후 상세히 설명하기로 한다. 통신부는 사용자감정에 대한 정보를 활용한 서비스를 통신망 등을 통해 접속한 사용자단말로 제공하기 위한 통신수단이다. 예를 들어, 통신부를 통해 사용자 단말로부터 사용자의 움직임을 촬영한 촬영영상을 취득하 며, 또한 인식된 사용자 감정이 반영된 서비스 데이터를 사용자 단말로 전송한다. 이러한 통신수단은 당업자에 게는 자명할 것이므로 더욱 상세한 설명은 생략한다. 제어부는 사용자 단말로부터 사용자의 움직임을 촬영한 촬영영상이 취득되면, 학습된 감성인식기술(인공지 능을 활용)을 기반으로 촬영영상을 분석하여 사용자 모션에 상응하는 사용자감정을 인식하며, 인식된 사용자 감 정을 제공서비스에 반영한다. 제어부의 감정인식부는 각종 감정상태에 따른 다양한 사용자 모션에 대한 학습데이터를 학습함으로써 촬영영상의 분석으로 사용자 감정을 결정한다. 또한, 감정인식부는 인공지능을 이용하여 서비스를 제공하면 서도 계속적인 학습으로 사용자감정의 인식 정확도를 높일 수 있다. 특히, 상술한 바와 같은 사용자에 대응되도 록 미리 등록된 사용자감정정보를 더 이용함으로써 모션에 따른 사용자감정의 인식 정확도를 높일 수 있다. 사 용자감정정보는 사용자 맞춤형 서비스를 제공하기 위한 것으로, 각 감정에 따른 사용자 모션에 대한 정보를 사용자감정정보로서 미리 저장하여 활용하는 것이다. 사용자관리부는 이러한 사용자감정정보를 저장, 삭제, 갱신하는 관리 기능을 수행한다. 이에 대한 상세한 설명은 차후 설명하기로 한다. 제어부의 상호작용부는 사용자모션을 이용하여 인식한 사용자 감정을 제공서비스에 반영한다. 예를 들 어, 아바타 서비스를 제공하는 중이라면, 사용자의 아바타에 사용자 감정에 상응하는 제스처와 표정을 적용하는 것이다(도 7참조). 물론 이는 하나의 예시일 뿐 아바타 서비스 외에도 확장현실, 온라인상담, 온라인교육 등 영 상을 이용한 모든 서비스에 사용자 감정을 적용할 수 있을 것이다. 참고로, 확장현실(eXtended Reality)은 가상현실(VR)과 증강현실(AR)을 아우르는 혼합현실(MR) 기술을 망라하 는 용어이다. 가상현실(VR)이 360도 영상을 바탕으로 새로운 현실을 경험하도록 하는 기술이라면 증강현실(AR) 은 실제 사물 위에 컴퓨터그래픽(CG)을 통해 정보와 콘텐츠를 표시한다. 증강현실(AR)과 가상현실(VR)은 별개이 지만 이 두 기술은 각자 단점을 보완하며 상호 진화를 하고 있다. 그러나 현 단계에서는 차이가 분명히 드러난 다. 가상현실(VR)은 눈 전체를 가리는 헤드셋 형(HMD) 단말기가 필요하고, 증강현실(AR)은 구글 글라스와 같은 안경으로 표현이 가능하다. 확장현실(XR)은 가상·증강현실(VR·AR) 기술의 개별 활용 또는 혼합 활용을 자유롭게 선택하며, 확장된 현실을 창조한다. 마이크로소프트(MS)가 개발한 홀로 렌즈는 안경 형태의 기기지만 현실 공간과 사물 정보를 파악하여 최적화된 3D 홀로그램을 표시한다는 점에서 확장현실(XR)의 한 형태로 볼 수 있다. 확장현실(XR)은 교육은 물론 헬스케어, 제조업 등 다양한 분야에 적용될 것으로 기대된다. 이러한 확장현실에서 사용자와의 상호 작용을 위해서는 사용자의 실시간 감정상태를 인식하여 서비스에 반영하 는 기술이 중요하다. 또한, 도면에는 도시되지 않았으나, 사용자를 촬영한 영상뿐 아니라, 손목시계형 장치와 같은 최소화된 웨어러 블 디바이스를 더 활용하여 사용자모션에 상응하는 사용자감정을 인식할 수도 있다. 도 2는 본 발명의 일 실시예에 따른 모션 기반 사용자 감정 상호 작용 과정을 개략적으로 도시한 흐름도이다. 도 2를 참조하면, 감정상호작용 시스템은 사용자의 움직임을 촬영한 사용자영상을 취득한다(S210). 사용자영상 은 시스템 자체에 구비된 카메라장치에 의해 직접 생성할 수 있으며, 또는 통신망을 통해 사용자영상을 촬영한 사용자단말 등으로부터 수신하는 형태일 수도 있다. 사용자영상은 일정 시간단위로 파일형태로 취득할 수 있으 며, 또는 스트리밍(streaming) 방식으로 실시간 취득할 수도 있으며, 이는 당업자에게는 자명할 것이므로 더욱 상세한 설명은 생략한다. 사용자영상을 분석하여 사용자 모션을 인식한다(S220). 즉, 사용자의 움직임에 따른 임의의 모션을 인식하는 것 이다. 예를 들어, 양팔을 머리위로 들어올리는 행위를 사용자모션으로서 인식한다. 인식된 사용자 모션이 미리 설정된 감정관련모션인지 여부를 확인한다(S230). 차후 상세히 설명하겠으나, 도 4 를 참조하면, 인식된 모션이 미리 감정관련모션으로 등록되어 있는지를 확인하는 것이다. 또한, 일례에 따르면, 사용자 모션에 대한 모션종료후 유지시간을 이용하여 감정관련모션인지 여부를 결정할 수 도 있다. 즉, 각 감정관련모션마다 유지시간이 설정될 수 있으며, 설정된 유지시간을 만족할 경우에만 감정관련 모션으로 인식하는 것이다. 예를 들어, 손을 어깨까지 올리는 모션이 감정관련모션이고 유지시간이 2초라면, 손 을 어깨까지 올리는 모션을 수행하더라도 2초 내에 다른 모션을 수행한다면, 해당 사용자모션은 감정관련모션으 로 인식하지 않는 것이다. 인식된 사용자모션이 감정관련모션인 경우 사용자모션에 상응하는 사용자 감정을 결정하여 제공서비스에 반영한 다(S240). 이해의 편의를 위해 예를 들면, 온라인 수업중인 학생을 촬영한 영상에서 인식된 사용자 모션이 오른 팔을 이마에 가져다대는 모션인 경우, 학생의 사용자감정을 [당혹감, 혼란]으로 인식하여 제공서비스인 인공지 능 교육서비스로서 현재 제공중인 수업내용에 대한 보다 상세한 설명을 추가하는 형태로 서비스를 진행할 수 있 다. 여기서, 사용자 모션에 상응하는 사용자감정의 결정에 해당 시점의 제공서비스의 내용을 활용할 수도 있다. 예 를 들어, 현재 제공서비스가 온라인교육이고 현재 제공하는 서비스의 내용이 수학과 관련되고 난이도가 높은 내 용이라 가정하면, 학생이 바로 문제를 풀지 못하는 경우 문제가가 어려워 [고민, 혼란]과 같은 부정적인 감정을 느낄 확률이 높다 판단하고, 또는 바로 문제풀이를 수행한 경우에는 [쾌감]과 같은 긍정적인 감정을 느낄 확률이 높다 판단할 수 있을 것이다. 보다 정확한 사용자 감정을 인식하기 위해서는 연속된 사용자모션에 대한 상관관계를 확인할 필요가 있다. 예를 들어, Step1로서 두 팔을 올리고 있는 경우, Step2로서 두손이 얼굴에 머무는 동작A의 경우 감정모션으로 인식할 수 있으나, Step3으로서 두팔이 얼굴을 넘어 머리위로 올라가는 모션은 비감정모션으로 처리될 수도 있 다. 도 3은 본 발명의 일 실시예에 따른 사용자 모션들의 연결성을 이용한 사용자감정 인식 과정을 도시한 흐름도이 고, 도 4는 본 발명의 일 실시예에 따른 감정관련모션 정보를 도시한 테이블이다. 도 3을 참조하면, 사용자영상에서 인식된 사용자모션이 감정관련모션인지를 판단하고(S310~320), 감정관련모션 인 경우 일정시간(예를 들어, 5초 등) 내로 연속되어 이어지는 다음모션이 감정관련모션인지를 판단한다(S330). 다시 말해, 인식된 사용자 모션이 감정관련모션인 경우 일정시간 내로 연속된 다음 모션이 존재하는지 여부를 확인하고, 연속된 다음 모션이 감정관련모션인지 또는 감정무관모션인지를 확인하는 것이다. 만일, 연속된 다음 모션이 감정무관모션인 경우 인식된 사용자모션을 일반모션(즉,비감정모션)으로 처리한다 (S340). 이와 달리, 연속된 다음 모션도 감정관련모션인 경우, 각 모션에 설정된 감정에 대한 상관관계를 확인하여 사용 자감정을 결정한다(S350). 미리 설정되어 저장되는 감정관련모션에 대한 정보를 예시한 도 4를 참조하면, 감정관련모션 정보로서 각 연결 모션에 따라 인식되는 사용자감정이 다르게 설정될 수도 있다. 즉, 얼굴로 양손을 올리는 감정관련모션에 대해 서도 그 이후 어떠한 연결모션을 수행하는지에 따라 사용자감정을 다르게 인식할 수 있는 것이다. 전술한 바와 같이 일례에 따르면, 각 사용자 맞춤형 감정인식의 정확도를 높이기 위해, 사용자감정보를 활용할 수 있다. 이를 위해, 복수개의 감점을 유발하는 동영상 또는 사진을 포함하는 감정유발콘텐츠를 사용자에게 제공하고, 감 정유발콘텐츠가 재생되는 동안 사용자를 촬영한 촬영영상을 취득하여 분석한다. 그리고, 사용자 모션에 변화가 발생하였는지를 판단하고, 변화가 있었다면 현 시점의 감정유발콘텐츠 내용과 그때의 사용자 모션을 사용자감정 정보로서 생성하여 저장한다. 예를 들어, 감정유발콘텐츠의 내용으로 유머스러운 내용이 나올 때, 사용자가 임의의 모션을 수행했다면 그때의 사용자 모션에 따른 영상특징정보와 웃음이라는 감정상태에 대한 정보가 사용자감정정보로서 저장될 수 있다. 물론 이때에도 사용자 모션이 어떤 감정상태인지에 대해 학습데이터를 활용한 인식과정을 더할 수 있음은 당연 하다. 그리고, 이때 사용자의 연령, 성별 및 얼굴 형태에 상응하는 학습데이터를 기반으로 사용자감정정보를 생성할 수도 있다. 예를 들어, 10대의 행동과 40대의 행동에는 차이가 있을 수 있으므로, 사용자의 연령, 성별 및 체형 등에 상응하는 학습데이터를 우선하여 활용하여 사용자 모션을 분석함으로써 보다 활용성이 높은 사용자감정정 보를 생성할 수 있다. 본 실시예에 따르면, 다양한 감정상태를 유발하는 콘텐츠를 사용자에게 미리 제공하여 시청토록 하고, 그때의 사용자행동을 관찰하여 각 감정상태에서의 사용자 모션에 대한 특징을 구체화하고, 차후 감정인식 시에 활용함 으로써 감정인식 정확도를 높일 수 있다. 도 5는 본 발명의 일 실시예에 따른 사용자 얼굴표정에 의한 강도를 이용한 사용자감정 적용 과정을 도시한 흐 름도이다. 도 5를 참조하면, 사용자모션에 따른 사용자감정을 인식하면, 해당 사용자모션의 인식 시점에 대응하여 사용자 영상에서 사용자의 얼굴 표정변화를 더 확인한다(S510). 그리고, 얼굴 표정변화에 따라 사용자감정의 강도 수치를 산출한다(S520). 물론, 표정변화가 클수록 강도 수치 는 높게 산출될 수 있을 것이다. 강도 수치에 따라 사용자감정에 대한 제공서비스로의 반영방식을 달리 적용한다. 예를 들어, 강도 수치가 높다 면 감정상태인 [웃음]에 상응하는 아바타의 행동을 크게 표현하고 특수효과를 삽입하며, 이와 달리 강도 수치가 낮다면 특수효과 없이 아바타에 [웃음]과 관련된 작은 제스처만을 반영한다. 상술한 본 발명에 따른 모션 기반 사용자 감정 상호 작용 방법을 수행하도록 하는 컴퓨터-판독 가능 매체에 저 장된 컴퓨터 프로그램이 제공될 수 있다. 또한, 상술한 모션 기반 사용자 감정 상호 작용 방법은 컴퓨터로 읽을 수 있는 기록 매체에 컴퓨터가 읽을 수 있는 코드로서 구현되는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록매체로는 컴퓨터 시스템에 의하여 해독될 수 있는 데이터가 저장된 모든 종류의 기록 매체를 포함한다. 예를 들어, ROM(Read Only Memory), RAM(Random Access Memory), 자기 테이프, 자기 디스크, 플래쉬 메모리, 광 데이터 저장장치 등이 있을 수 있다. 또한, 컴 퓨터가 읽을 수 있는 기록매체는 컴퓨터 통신망으로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 읽을 수 있는 코드로서 저장되고 실행될 수 있다. 또한, 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야에서 통상의 지식을 가 진 자라면 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0110743", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 모션 기반 사용자 감정 상호 작용 시스템의 구성을 도시한 기능블록도. 도 2는 본 발명의 일 실시예에 따른 모션 기반 사용자 감정 상호 작용 과정을 개략적으로 도시한 흐름도. 도 3은 본 발명의 일 실시예에 따른 사용자 모션들의 연결성을 이용한 사용자감정 인식 과정을 도시한 흐름도. 도 4는 본 발명의 일 실시예에 따른 감정관련모션 정보를 도시한 테이블. 도 5는 본 발명의 일 실시예에 따른 사용자 얼굴표정에 의한 강도를 이용한 사용자감정 적용 과정을 도시한 흐름도."}
