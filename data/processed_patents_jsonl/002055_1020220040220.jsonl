{"patent_id": "10-2022-0040220", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0141103", "출원번호": "10-2022-0040220", "발명의 명칭": "AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템", "출원인": "주식회사 비솔", "발명자": "박형오"}}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "소정의 공간 상에 선택적으로 배치된 복수 개의 제1카메라 모듈로부터 오브젝트의 모션에 대한 영상 정보를 획득하는 영상 획득 유닛;상기 소정의 공간 상에 선택적으로 배치된 복수 개의 제2카메라 모듈로부터 상기 오브젝트의 모션에 대한 조인트 포인트(joint point)를 획득하는 조인트 획득 유닛; 및상기 오브젝트의 모션에 따른 상기 영상 정보와 상기 조인트 포인트를 선택적으로 매핑하여, 소정의 컴포징(composing) 영상을 생성하는 영상 매핑 유닛을 포함하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 영상 획득 유닛은,상기 복수 개의 제1카메라 모듈 각각은 개별적으로 2차원 영상을 획득하며, 상기 복수 개의 제1카메라 모듈로부터 획득된 상기 2차원 영상을 선택적으로 취합하여 소정의 영상 정보를 생성하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 조인트 획득 유닛은,상기 복수 개의 제2카메라 모듈로부터 상기 오브젝트의 복수 개의 미리 결정된 부위에 선택적으로 부착된 소정의 마커(marker)를 각각 추적하여 상기 소정의 마커의 위치 정보를 검출하는 트레킹(tracking)부를 포함하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 소정의 마커는,상기 복수 개의 미리 결정된 부위의 일측과 타측에 각각 부착되는 한 쌍의 구성인 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 조인트 획득 유닛은,상기 복수 개의 미리 결정된 부위의 상기 일측의 소정의 마커와 상기 타측의 소정의 마커를 상기 오브젝트의 소정의 피지컬 정보를 기반으로 선택적으로 보간하여 상기 조인트 포인트(joint point)를 추출하는 조인트 보정부를 더 포함하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 조인트 획득 유닛은,공개특허 10-2023-0141103-3-상기 소정의 공간 상에서 상기 오브젝트의 모션에 따른 상기 복수 개의 미리 결정된 부위의 각각의 상기 조인트포인트의 좌표를 추출하여 상기 조인트 포인트의 위치 정보를 선택적으로 산출하는 익스트랙션(extraction)부를더 포함하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 영상 매핑 유닛은, 상기 영상 획득 유닛으로부터 획득된 상기 소정의 영상 정보에 상기 조인트 포인트와 상기 조인트 포인트의 위치 정보를 선택적으로 매핑하여 상기 소정의 컴포징 영상을 생성하는 것을 특징으로 하는, AI 학습을 위한 배경데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서, 상기 영상 매핑 유닛은,상기 소정의 영상 정보와 상기 조인트 포인트 사이에는 어트랙티브 포스(attractive force)를 발생시켜, 상기어트랙티브 포스에 상기 소정의 영상 정보 위에 상기 조인트 포인트가 상호 동기화되는 것을 특징으로 하는, AI학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 영상 매핑 유닛은,상기 소정의 영상 정보의 미리 설정된 바운더리(boundary) 안으로 상기 조인트 포인트가 진입하면, 상기 어트랙티브 포스에 의하여 상기 조인트 포인트는 자동으로 픽싱(fixing)되는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템."}
{"patent_id": "10-2022-0040220", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서, 상기 영상 매핑 유닛은,소정의 시점 이전에 미리 저장된 컴포징 영상을 기반으로 상기 오브젝트의 후속 모션에 따른 상기 조인트 포인트의 후속 위치를 전산적으로 예측하여 상기 소정의 컴포징 영상의 후속되는 컴포징 영상을 선택적으로 구현하는 모션 프리딕팅부를 포함하는 것을 특징으로 하는, AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성시스템."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템에 관한 것으로서, 소정의 공간 상에 선택적으로 배치된 복수 개의 제1카메라 모듈로부터 오브젝트의 모션에 대한 영상 정보를 획득하는 영상 획득 유 닛; 소정의 공간 상에 선택적으로 배치된 복수 개의 제2카메라 모듈로부터 오브젝트의 모션에 대한 조인트 포인 트를 획득하는 조인트 획득 유닛; 및 오브젝트의 모션에 따른 영상 정보와 조인트 정보를 선택적으로 매핑하여, 소정의 컴포징 영상을 생성하는 영상 매핑 유닛을 포함하는 기술적 사상을 개시한다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템에 관한 것이다. 보다 자세하게는, 소 정의 공간 상에 제1카메라 모듈과 제2카메라 모듈이 선택적으로 배치되고, 각각의 카메라 모듈을 통하여 오브젝 트의 모션에 따른 동작 영상 데이터와 조인트 포인트를 각각 추출하여, 영상 데이터 상에 조인트 포인트를 투사"}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "하여 AI학습을 위한 배경 데이터에 적용될 수 있는 오브젝트를 생성하는 시스템에 관한 기술분야이다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝(Deep learning)은 다층 구조 형태의 신경망을 기반으로 하는 머신 러닝(Machine learning)의 한 분야로 서 다량의 데이터로부터 높은 수준의 추상화 모델을 구축하는 것으로 정의할 수 있다. 딥러닝은 데이터를 컴퓨터가 처리 가능한 형태인 벡터나 그래프 등으로 표현하고 이를 학습하는 모델을 구축할 수 있으며, 학습을 위한 더 나은 표현 방법과 효율적인 모델 구축에 초점을 맞춰 신경 시스템의 정보 처리나 통신 패턴에 기반을 두고 있다. 최근 딥러닝 기술의 급속한 발전과 함께 학습데이터가 주목을 받고 있다. 딥러닝 방식에서 모델 훈련을 위해서 는 충분한 학습데이터가 필요하다. 한 조사기관(Cognilytica)에 따르면 일반적인 AI 프로젝트에서 학습데이터의 준비과정이 전체 프로젝트의 80% 시간을 소모하고 있다고 한다. 또한, AI 프로젝트에서 학습데이터의 시장이 2023년까지 $1.2B에 이를 것으로 예 상하고 있다. 특히, 학습데이터 제작을 위한 레이블링 시장은 급증하는 수요에 대응하기 위해 크게 확대될 것으 로 예상하고 있다. 학습데이터는 딥러닝 기반의 인공지능 개발에서 전문가에 의해 설계된 모델을 훈련시키는 교재와 같은 역할을 담당하여, 인공지능이 해결해야 하는 문제별로 필요에 따라 제작해야 한다. 따라서 일반적으로 학습데이터는 이 미지 단위로 레이블링을 수작업으로 하고 있어 많은 시간과 노력이 들어가기 때문에 단기간에 대량으로 확보하 기 어렵다는 문제점을 안고 있다. 이와 관련된 선행 특허문헌의 예로서 “자율 주행 데이터의 데이터 레이블링 방법, 장치 및 컴퓨터프로그램 (등 록번호 제10-2325367호, 이하 특허문헌1이라 한다.)”이 존재한다. 특허문헌1에 따른 발명의 경우, 컴퓨팅 장치에 의해 수행되는 방법에 있어서, 3D 포인트 클라우드 데이터(Point Cloud Data)를 변환하여 생성된 2D 형태의 데이터 상에서 레이블링(Labeling) 대상을 가리키는 사용자 입력을 얻는 단계, 2D 형태의 데이터와 기 매칭된 센서 데이터에서 레이블링 대상의 3D 포인트 클라우드 데이터 좌표 값과 대응되는 센서 데이터 좌표 값을 선택하는 단계 및 센서 데이터 좌표 값을 이용하여 센서 데이터 상에 레 이블링을 수행하는 단계를 포함한다. 또 다른 특허문헌의 예로서 “이동하는 물체의 3차원 데이터를 생성하는 방법 및 장치 (등록번호 제10- 2160340호, 이하 특허문헌2이라 한다.)”이 존재한다. 특허문헌2에 따른 발명의 경우, 관측시야에서 이동하는 물체를 검사하기 위하여 두 대의 디지털 카메라 및 하나 의 광 프로젝터를 포함하는 머신 비전 시스템을 이용한 3차원 데이터 생성방법으로서, 복수의 디지털 패턴을 관 측시야로 순차적으로 투영하는 단계; 디지털 카메라로 복수의 이미지를 캡쳐하는 단계로서, 복수의 이미지 중 각 이미지는 순차적으로 투영된 패턴 중 어느 하나와 연관되는 것; 복수의 이미지 중 각 이미지에 모션 파라미 터를 보상하는 단계; 캡쳐된 이미지로부터 합성된 제1 및 제2 패턴 이미지를 생성하는 단계; 제7 및 제8 패턴으 로부터 경계를 탐지하는 단계; 제1내지 제7 패턴 이미지에 의해 경계를 디코딩하는 단계; 합성 패턴 및 캡쳐된 패턴의 코드를 매칭하는 단계; 및 두 개의 디지털 카메라의 대응 쌍에 기초하여 3차원 데이터를 생성하는 단 계;를 포함한다. 또 다른 특허문헌의 예로서 “보행자 검출 시스템 및 모듈, 방법, 컴퓨터프로그램 (등록번호 제10-1995223 호, 이하 특허문헌3이라 한다.)”이 존재한다. 특허문헌3에 따른 발명의 경우, 레이저 센서가 대상 영역을 센싱하여 획득한 라이다 데이터에서 노이즈를 제거 하여 보행자를 검출하는 보행자 검출부; 노이즈가 제거된 라이다 데이터를 2차원 직교 좌표계 형식으로 변환하 고, 2차원 직교 좌표계 형식으로 변환된 라이다 데이터를 그리드 맵에 맵핑시켜 맵핑 영상 데이터를 획득하며, 맵핑 영상 데이터에 2차원 직교 좌표계 형식으로 변환된 라이다 데이터의 수평 좌표를 거리 정보로 추가하여 맵 핑 영상을 생성하는 맵핑 영상 생성부; 거리 정보를 이용하여 보행자에 대응되는 경계 박스의 크기를 설정하는 경계 박스 설정부; 및 카메라 센서가 대상 영역을 촬영하여 획득한 촬영 영상에서 보행자에 대응되는 부분에 설 정된 크기의 경계 박스를 표시하는 경계 박스 표시부를 포함한다. 선행기술문헌 특허문헌 (특허문헌 0001) 등록번호 제10-2325367호 (특허문헌 0002) 등록번호 제10-2160340호 (특허문헌 0003) 등록번호 제10-1995223호"}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템은 상기한 바와 같은 종래 문제 점을 해결하기 위해 안출된 것으로서, 다음과 같은 해결하고자 하는 과제를 제시한다. 첫째, 소정의 공간에 복수 개의 카메라 모듈을 배치하고, 오브젝트의 모션에 따른 영상 정보와 조인트 정보를 획득하고자 한다. 둘째, 복수 개의 카메라 모듈을 소정의 공간 상에서 캘리브레이션 하여 오브젝트의 모션에 따른 영상 정보를 수 집하고자 한다. 셋째, 복수 개의 카메라 모듈로부터 오브젝트의 모션에 대한 조인트의 포인트를 추출하여, 영상 정보에 매핑하 여 ai 학습을 위한 영상을 생성하고자 한다. 본 발명의 해결 과제는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 해결과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템은 상기의 해결하고자 하는 과제 를 위하여 다음과 같은 과제 해결 수단을 가진다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템은 소정의 공간 상에 선택적으로 배치된 복수 개의 제1카메라 모듈로부터 오브젝트의 모션에 대한 영상 정보를 획득하는 영상 획득 유닛; 상기 소정의 공간 상에 선택적으로 배치된 복수 개의 제2카메라 모듈로부터 상기 오브젝트의 모션에 대한 조인트 포 인트(joint point)를 획득하는 조인트 획득 유닛; 및 상기 오브젝트의 모션에 따른 상기 영상 정보와 상기 조인 트 포인트를 선택적으로 매핑하여, 소정의 컴포징(composing) 영상을 생성하는 영상 매핑 유닛을 포함하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 영상 획득 유닛은, 상기 복수 개의 제1카메라 모듈 각각은 개별적으로 2차원 영상을 획득하며, 상기 복수 개의 제1카메라 모듈로부터 획 득된 상기 2차원 영상을 선택적으로 취합하여 소정의 영상 정보를 생성하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 조인트 획득 유닛은, 상 기 복수 개의 제2카메라 모듈로부터 상기 오브젝트의 복수 개의 미리 결정된 부위에 선택적으로 부착된 소정의 마커(marker)를 각각 추적하여 상기 소정의 마커의 위치 정보를 검출하는 트레킹(tracking)부를 포함하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 소정의 마커는, 상기 복 수 개의 미리 결정된 부위의 일측과 타측에 각각 부착되는 한 쌍의 구성인 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 조인트 획득 유닛은, 상 기 복수 개의 미리 결정된 부위의 상기 일측의 소정의 마커와 상기 타측의 소정의 마커를 상기 오브젝트의 소정 의 피지컬 정보를 기반으로 선택적으로 보간하여 상기 조인트 포인트(joint point)를 추출하는 조인트 보정부를 더 포함하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 조인트 획득 유닛은, 상 기 소정의 공간 상에서 상기 오브젝트의 모션에 따른 상기 복수 개의 미리 결정된 부위의 각각의 상기 조인트 포인트의 좌표를 추출하여 상기 조인트 포인트의 위치 정보를 선택적으로 산출하는 익스트랙션(extraction)부를 더 포함하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 매핑 유닛은, 상기 영상 획득 유닛으로부터 획득된 상기 소정의 영상 정보에 상기 조인트 포인트와 상기 조인트 포인트의 위치 정보를 선택적으로 매핑하여 상기 소정의 컴포징 영상을 생성하는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 매핑 유닛은, 상기 소정 의 영상 정보와 상기 조인트 포인트 사이에는 어트랙티브 포스(attractive force)를 발생시켜, 상기 어트랙티브 포스에 의해 상기 소정의 영상 정보 위에 상기 조인트 포인트가 상호 동기화되는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 매핑 유닛은, 상기 소정 의 영상 정보의 미리 설정된 바운더리(boundary) 안으로 상기 조인트 포인트가 진입하면, 상기 어트랙티브 포스 에 의하여 상기 조인트 포인트는 자동으로 픽싱(fixing)되는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 매핑 유닛은, 상기 소정 의 영상 정보 내에서 상기 오브젝트의 상기 복수 개의 미리 결정된 부위의 위치 정보가 선택적으로 변동되면, 상기 조인트 포인트는 상기 어트랙티브 포스에 의해 자동으로 시프팅되는 것을 특징으로 할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 상기 매핑 유닛은, 소정의 시 점 이전에 미리 저장된 컴포징 영상을 기반으로 상기 오브젝트의 후속 모션에 따른 상기 조인트 포인트의 후속 위치를 전산적으로 예측하여 상기 소정의 컴포징 영상의 후속되는 컴포징 영상을 선택적으로 구현하는 모션 프 리딕팅부를 포함하는 것을 특징으로 할 수 있다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 구성의 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템은 다음과 같은 효과를 제공한다. 첫째, 소정의 공간에 복수 개의 카메라 모듈을 배치하고, 오브젝트의 모션에 따른 영상 정보와 조인트 정보를 획득할 수 있게 된다. 둘째, 복수 개의 제1카메라 모듈은 소정의 공간 상의 미리 설정된 포지션 배치되어, 오브젝트의 모션에 따른 영 상 정보를 수집할 수 있게 된다. 셋째, 복수 개의 제2카메라 모듈로부터 오브젝트의 모션에 대한 조인트의 포인트를 추출하여, 영상 정보에 매핑 하여 ai 학습을 위한 영상을 소정의 컴포징 영상을 생성할 수 있게 된다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급한 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0040220", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한 다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 기술적 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 도1은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 개념도이 다. 도2는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 블록 도이다. 도3은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 획득 유닛의 개념도이다. 도4는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오 브젝트 생성 시스템의 조인트 획득 유닛의 개념도이다. 도5는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 획득 유닛의 블록도이다. 도6은 본 발명의 일 실시예에 따 른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 트레킹부의 개념도이다. 도7은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 보정부의 개념도이 다. 도8은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 익스 트랙션부의 개념도이다. 도9는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 매핑 유닛의 개념도이다. 도10은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터 의 실사 기반 오브젝트 생성 시스템의 영상 매핑 유닛의 미리 설정된 바운더리에 대한 개념도이다. 도11은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 모션 프리딕팅부의 개념도이다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 경우, 도1에 도시된 바와 같 이, 소정의 공간 상에 제1카메라 모듈과 제2카메라 모듈이 선택적으로 배치되고, 각각의 카메라 모듈을 통하여 오브젝트의 움직임에 따른 동작 영상 데이터와 조인트 데이터를 추출하여, 동작 영상 데이터 상에 조인 트 데이터를 투사하여 ai 학습을 위한 데이터를 생성하는 기술적 사상을 개시한다. 여기서 말하는 소정의 공간은 현실에서 실제 존재하는 모든 공간, 3차원 공간으로 정의할 수 있으며, 소정의 공 간은 가상 공간과 연동되는 것이 바람직하다. 소정의 공간에는 제1카메라 모듈과 제2카메라 모듈이 마련되어, 오브젝트의 동작에 따른 각종 데이터를 얻을 수 있게 된다. 먼저, 제1카메라 모듈은 소정의 공간에 존재하는 오브젝트를 감지하고 오브젝트의 동작 영상 데이터를 확보 하기 위한 것으로 예컨대, FHD(Full High Definition), UHD, 4K UHD, 8K UHD 이상의 영상을 촬영할 수 있는 고 속 비디오 카메라로서 구성된다. 제1카메라 모듈은 ai 학습에 사용 가능하도록 1/1000초까지 시간 동기화를 하여 취득하는 것이 바람직하다. 제2카메라 모듈은 소정의 공간에 존재하는 오브젝트를 감지하여 광학 방식의 모션 캡쳐 시스템을 구축하기 위한 것으로, 소정의 공간에 적외선을 조사하고, 오브젝트로부터 반사되는 빛을 수신하여 오브젝트의 움직임을 기록할 수 있는 광학식 모션 캡쳐 시스템을 활용할 수 있다. 예컨대, 제2카메라 모듈은 VICON사의 모션 캡 쳐 카메라로 구성될 수 있으며, 타사의 상용화된 모션 캡쳐 카메라와 시스템을 통해 제2카메라 모듈을 구축 할 수 있도록 하는 것이 바람직하다.제1카메라 모듈과 제2카메라 모듈은 이미 상용화된 기술과 원리를 통해 구현되는 것으로 자세한 기작은 생략하고자 한다. 오브젝트의 경우, 제1카메라 모듈과 제2카메라 모듈로부터 촬영되는 움직이는 대상체, 물체 등으로 정 의할 수 있다. 오브젝트는 실제 공간 또는 가상 공간에 존재하면서 움직이는 모든 물체로서, 오브젝트에는 사람이 포함될 수 있다. 예컨대, 자동차 기계 학습을 위한 가상 공간 상에서 자동차를 기준으로 사람은 하나의 대상체로서 오브젝 트가 될 수 있다. 또한, 조인트 포인트는 오브젝트를 구성하는 구성 요소 각각을 연결하는 부위를 조인트 포인트로 정의할 수 있 다. 예컨대, 오브젝트가 사람일 경우, 사람의 뼈와 뼈가 서로 맞닿아 연결되는 관절을 조인트 포인트로서 인식 하여 오브젝트에 존재하는 각각의 조인트 포인트를 추출할 수 있게 된다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 경우, 도2에 도시된 바와 같 이, 영상 획득 유닛, 조인트 획득 유닛, 및 영상 매핑 유닛을 포함하게 된다. 먼저 이들 구성요소에서 언급하는 유닛(unit)의 의미는 특정한 제품, 부품, 혹은 기타 유형물의 단위체를 언급 하는 것이 아니라, 이들 유닛을 형용하는 기술적 사상이 구현되는 기술적 사상의 개념적 단위를 의미하게 된다. 따라서, 이들 유닛은 특정한 제품, 부품, 혹은 기타 유형물 그 자체가 아니라 제품, 부품 혹은 기타 유형물 상 에서 구현되는 기술적 기능의 개념으로 이해되어야 한다. 또한, 이들 유닛은 인터넷 상의 서버 또는 클라우드 서버로서 네트워크를 통해 제1카메라 모듈 또는 제2카 메라 모듈와 상호 연결되며, 각각의 카메라 모듈로부터 획득된 정보와 송수신하여, 상호 병합할 수 있는 역 할을 수행하는 기술적 사상의 개념적 단위로서 이해하는 것이 바람직하다. 아울러, 이들 유닛과 제1카메라 모듈 또는 제2카메라 모듈로부터 측정, 입력, 생성된 자료나 데이터를 목적에 맞게 처리되어 2차 가공할 수 있으며, 가공된 2차 데이터는 정보로서 정의할 수 있으며, 예컨대, 후술하 게 될 오브젝트 모션에 따른 영상 정보, 2차원 영상, 소정의 영상, 조인트 포인트, 복수 개의 미리 결정된 부위, 소정의 마커, 소정의 컴포징 영상, 어트랙티브 포스, 미리 설정된 바운더리 등의 정보 역시 이러한 개념 적 단위로서 인식하는 것이 바람직하다. 먼저, 영상 획득 유닛의 경우, 도3에 도시된 바와 같이, 소정의 공간 상에 선택적으로 배치된 복수 개의 제1카메라 모듈로부터 오브젝트의 모션에 대한 영상 정보를 획득하는 구성이다. 복수 개의 제1카메라 모듈은 n대의 제1카메라 모듈로 이루어진 것으로 위에서 상술한 바와 같이 고속 비디오 카메라가 소정의 공간 곳곳에 설치되어, 오브젝트의 모션에 대한 영상을 촬영할 수 있도록 한다. 복수 개의 제1카메라 모듈은 소정의 공간 소정의 공간 상에 복수 개의 미리 설정된 포지션에 선택적으로 배 치되는 것이 바람직하다. 이때, 복수 개의 미리 설정된 포지션은 소정의 공간에서 모든 각도에 따른 오브젝트의 모션에 따른 영상 정보를 얻기 위한 위치로 정의할 수 있다. 즉 복수 개의 미리 설정된 포지션에 배치된 복수 개의 제1카메라 모듈을 통해 복수 개의 앵글에 대한 영상 정보를 획득할 수 있다. 또한, 복수 개의 미리 설정된 포지션에 배치된 복수 개의 제1카메라 모듈은 카메라 캘리브레이션(camera calibration)를 진행하여 복수 개의 제1카메라 모듈 각각으로부터 획득된 2차원 영상의 왜곡을 보정할 수 있게 된다. 카메라 캘리브레이션은 이미 상용화된 기술로 자세한 기작은 생략한다. 영상 획득 유닛의 복수 개의 제1카메라 모듈 각각은 개별적으로 2차원 영상을 획득한다. 영상 획득 유닛은 복수 개의 제1카메라 모듈로부터 각각 동시 획득된 복수 개의 2차원 영상을 선택적 으로 취합하여 소정의 영상 정보로서 획득할 수 있다. 소정의 영상 정보는 개별적으로 획득된 2차원 영상을 조합하여 형성된 것으로 정의할 수 있다. 영상 획득 유닛에서 생성된 소정의 영상 정보는 복수 개의 미리 설정된 포지션으로부터 촬영된 2차원 영상 을 토대로 형성된 것으로 어느 각도에서나 오브젝트의 모션 영상 정보를 얻을 수 있다. 또한, 영상 획득 유닛으로부터 생성된 소정의 영상 정보는 물리 엔진 기반으로 생성된 가상 공간 상에 오 버랩될 수 있도록 하여, 버추얼 카메라를 통해 특정 각도에서의 모션 정보를 얻을 수 있는 것이 바람직하다. 여 기서 말하는 물리 엔진은 게임 엔진, 렌더링 엔진, 물리 엔진 등을 포함하며, 예컨대, 언리얼 엔진(unreal engine), 크라이 엔진(CryENGINE), 소스 엔진(source engine), 하복 엔진(havok engine), 유니티3d 엔진(unity 3d engine) 등을 기반으로 하는 것이며, 위와 같은 물리 엔진은 이미 상용화된 것으로 기술적 원리와 프로세스 에 대한 자세한 기작은 생략하고자 한다. 또한, 가상 공간은 컴퓨터, 인터넷이나 PC통신 등과 같은 정보 통신망을 통해 대량의 디지털 정보가 교환되고 공유되는 가상의 활동 공간이 되어, 물리 엔진을 기반으로 현실에서 적용되는 물리 법칙이 그대로 적용되도록 하는 공간으로 정의할 수 있다. 즉, 영상 획득 유닛으로부터 획득된 영상 정보를 통해 가상의 그래픽 영상을 적용할 수 있게 되는 것이 바 람직하다. 조인트 획득 유닛의 경우, 도4에 도시된 바와 같이, 소정의 공간 상에 선택적으로 배치된 복수 개의 제2카 메라 모듈로부터 오브젝트의 모션에 대한 조인트 포인트(joint point)를 획득하는 구성이다. 복수 개의 제2카메라 모듈은 n대의 제2카메라 모듈로 이루어진 것으로 위에서 상술한 바와 같이 모션 캡쳐 카메라가 소정의 공간 곳곳에 설치되어 오브젝트의 모션에 대한 위치 정보를 추출할 수 있도록 한다. 소정의 공간 상에 복수 개의 미리 설정된 포지션에 선택적으로 배치되는 것이 바람직하다. 이때, 복수 개의 미리 설정된 포지션은 소정의 공간에서 모든 각도에 따른 오브젝트의 모션에 따른 조인트 포인 트 정보를 얻기 위한 위치로 정의할 수 있다. 즉, 복수 개의 미리 설정된 포지션에 배치된 복수 개의 제2카메라 모듈을 통해 복수 개의 앵글에 대한 조인트 포인트 정보를 획득할 수 있다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 획득 유닛은 도5 에 도시된 바와 같이, 트레킹(tracking)부, 조인트 보정부, 및 익스트랙션(extraction)부를 포 함하게 된다. 먼저, 트레킹부의 경우, 도6에 도시된 바와 같이, 복수 개의 제2카메라 모듈로부터 오브젝트의 복수 개의 미리 결정된 부위에 선택적으로 부착된 소정의 마커를 각각 추적하여, 소정의 마커의 위치 정보를 검출하 는 구성이다. 여기서 말하는 복수 개의 미리 결정된 부위는 예컨대, 오브젝트가 사람일 경우, 왼쪽 눈, 오른쪽 눈, 왼쪽 귀, 오른쪽 귀, 코, 목, 가슴, 허리, 왼쪽 어깨, 왼쪽 팔꿈치, 왼쪽 손목, 왼쪽 손바닥 엄지, 왼쪽 손바닥 약지, 오 른쪽 어깨, 오른쪽 팔꿈치, 오른쪽 손목, 오른쪽 손바닥 엄지, 오른쪽 손바닥 약지, 가운데 엉덩이, 왼쪽 엉덩 이, 왼쪽 무릎, 왼쪽 발목, 왼쪽 엄지 발가락, 왼쪽 새끼 발가락, 오른쪽 엉덩이, 오른쪽 무릎, 오른쪽 발목, 오른쪽 엄지 발가락, 오른쪽 새끼 발가락 중 적어도 적어도 하나 이상을 포함할 수 있다. 오브젝트가 사람일 경우에 복수 개의 미리 결정된 부위 각각에 소정의 마커가 부착되며, 오브젝트의 모션에 따 라서 위치가 실시간으로 변동되므로 복수 개의 제2카메라 모듈을 소정의 마커를 추적하게 된다. 여기서 말하는 소정의 마커는 복수 개의 제2카메라 모듈를 통해 오브젝트를 인식하도록 사용되는 광학 마커 로서 패시브 마커(passive marker) 또는 액티브 마커(active marker) 등이 있다. 소정의 마커는 복수 개의 미리 결정된 부위의 일측과 타측에 각각 부착되는 한 쌍의 구성으로 이루어지는 것이 바람직하다. 조인트 보정부의 경우, 도7에 도시된 바와 같이, 복수 개의 미리 결정된 부위의 일측의 소정의 마커와, 타 측의 소정의 마커를 오브젝트의 소정의 피지컬 정보를 기반으로 선택적으로 보간하여 조인트 포인트를 추출하는 구성이다. 먼저, 조인트 보정부에서는 소정의 피지컬 정보를 기반으로 조인트 포인트를 추정하게 되는데 여기서 말하 는 소정의 피지컬 정보는 오브젝트의 각 구성 요소의 사이즈, 크기, 비율 정보가 포함될 수 있다. 예컨대, 오브 젝트가 사람일 경우, 사람의 몸무게, 키, 팔 길이, 다리 길이, 발 사이즈, 몸통 길이, 머리 둘레 또는 신체 비 율 사이즈 중 적어도 하나 이상이 정보와, 각각의 신체 부위별 사이즈와 더불어 신체의 머리, 몸통, 팔 다리 등 의 관절 위치 정보가 포함된다. 조인트 보정부는 소정의 피지컬 정보를 기반으로 복수 개의 미리 결정된 부위 일측과 타측에 부착된 소정 의 마커의 위치 정보를 추출하고, 일측의 소정의 마커와 타측의 소정의 마커 사이에 존재하는 조인트 포인트의 위치를 보간하여 추정할 수 있게 된다. 또한, 조인트 보정부의 경우, 도7에서 도시된 바와 같이 general midpoint 방식을 통해 조인트 포인트를 추정하는 것으로 설명하였으나, 여기에 한정하지 않고 복수 개의 미리 결정된 부위의 기하학적 특성 분석을 기 반으로 손실 데이터 보정 방법을 통해 조인트 포인트를 추정하는 것이 바람직하다. 익스트랙션부의 경우, 도8에서 도시된 바와 같이, 소정의 공간 상에서 오브젝트의 모션에 따른 복수 개의 미리 결정된 부위 각각의 조인트 포인트의 좌표 정보를 추출하여 조인트 포인트의 위치 정보를 선택적으로 산출 하는 구성이다. 익스트랙션부는 소정의 공간을 3차원의 직교 좌표계, 원통 좌표계, 구면 좌표계로서 오브젝트의 조인트 포 인트 위치를 나타낼 수 있게 된다. 예컨대, 소정의 공간 차원을 x축, y축, z축의 3개를 사용해서 나타낼 수 있으며, 3차원의 x, y, z의 위치를 표 시하는 경우는 (x, y, z)의 순으로 표시하여 조인트 포인트의 위치 정보로 할당할 수 있게 된다. 영상 매핑 유닛의 경우, 도9에 도시된 바와 같이, 오브젝트의 모션에 따른 영상 정보와 조인트 포인트를 선택적으로 매핑하여 소정의 컴포징(composing) 영상을 생성하는 구성이다. 영상 매핑 유닛은 영상 획득 유닛으로부터 획득된 소정의 영상 정보에 조인트 포인트와 조인트 포인 트의 위치 정보를 선택적으로 매핑하여, 소정의 컴포징 영상을 생성한다. 영상 매핑 유닛은 영상 획득 유닛으로부터 복수 개의 제1카메라 모듈을 통해 촬영 및 취합된 소 정의 영상 정보와 복수 개의 제1카메라 모듈과 동일한 위치에서 복수 개의 제2카메라 모듈을 통해 조인 트 포인트의 좌표 정보를 각각 획득하여 소정의 영상 정보에 조인트 포인트를 오버레이 함으로써 정확한 키포인 트 데이터가 포함된 소정의 컴포징 영상을 생성하게 된다. 여기서 말하는 소정의 컴포징 영상은 소정의 영상 정보와 조인트 포인트 정보가 병합 또는 합성되어 형성된 영 상으로, ai 학습을 위한 영상으로서 key point annotation 영상이 되는 것으로, 소정의 영상과 조인트 포인트를 병합하고 매핑하는 것은 이미 상용화된 기술을 토대로 생성하는 것으로 자세한 기작은 생략하고자 한다. 영상 매핑 유닛은 소정의 영상 정보와 조인트 포인트의 시점, 시간을 동기화하는 것이 바람직하다. 영상 매핑 유닛에서는 소정의 시점(viewpoint)에 따른 소정의 영상 정보로부터 오브젝트의 복수 개의 미리 결정된 부위의 위치 정보와 복수 개의 미리 결정된 부위로부터 추출된 조인트 포인트의 위치 정보를 전산적으로 상호 매칭하여, 소정의 영상 정보 상에 조인트 포인트를 투영할 수 있도록 한다. 또한, 영상 매핑 유닛은 소정의 영상 정보와 조인트 포인트 사이에는 어트랙티브 포스(attractive force) 를 발생시켜, 어트랙티브 포스에 의해 소정의 영상 정보 위에 조인트 포인트가 상호 동기화 될 수 있다. 어트랙티브 포스는 소정의 영상 정보 위에 조인트 포인트가 매핑되면, 상호 매핑된 소정의 영상 정보와 조인트 포인트 사이에 서로 끌어당기도록 하는 힘, 즉 인력이 작용되는 것으로 정의할 수 있다. 즉, 소정의 영상 정보의 어느 한 포인트와 조인트 포인트 사이에 인력이 발생하면, 소정의 영상 정보 상에 조인 트 포인트의 위치가 고정될 수 있게 된다. 또한, 오브젝트의 모션에 변화가 발생할 경우, 소정의 영상 정보의 미리 설정된 바운더리(boundary) 안으로 조 인트 포인트가 진입하면, 어트랙티브 포스에 의하여 조인트 포인트는 자동으로 픽싱될 수 있도록 한다. 여기서 말하는 미리 설정된 바운더리는 도10에 도시된 바와 같이, 오브젝트의 복수 개의 미리 결정된 부위를 중 심으로 영역이 임의 설정되며, 임의 설정된 영역에 경계 라인으로 정의할 수 있다. 즉, 소정의 영상 정보 상의 미리 설정된 바운더리 내에 해당하는 조인트 포인트가 진입하면, 어트랙티브 포스에 의해 인력이 발생하여 자동으로 픽싱될 수 있다. 아울러, 영상 매핑 유닛은 소정의 영상 정보 내에서 오브젝트의 복수 개의 미리 결정된 부위의 위치 정보 가 선택적으로 변동되면, 조인트 포인트는 어트랙티브 포스에 의해 자동으로 시프팅(shifting) 될 수 있도록 한 다. 예컨대, 오브젝트의 모션에 따라서 오브젝트의 복수 개의 미리 결정된 부위의 위치가 실시간으로 변동될 때, 어 트랙티브 포스가 작용되어, 소정의 영상 정보의 미리 결정된 부위가 조인트 포인트가 끌어당기게 되어, 조인트 포인트는 자동으로 시프팅될 수 있게 되는 것이다. 본 발명에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 매핑 유닛의 경우, 모션 프리딕팅(motion predicting)부를 포함하게 된다. 먼저, 모션 프리딕팅부의 경우, 도11에 도시된 바와 같이, 소정의 시점 이전에 미리 저장된 컴포징 영상을 기반으로, 오브젝트의 후속 모션에 따른 조인트 포인트의 후속 위치를 전산적으로 예측하여, 소정의 컴포징 영 상에 후속되는 컴포징 영상을 선택적으로 구현하는 구성이다. 미리 저장된 컴포징 영상은 현재 시점을 기준으로 이전까지 수행된 ai 학습을 통해 서버에 이미 저장되어 있는 영상 정보 또는 외부 서버로부터 저장된 영상을 획득하여, 미리 저장된 컴포징 영상을 기준으로 오브젝트의 후 속 모션을 예측할 수 있게 된다. 예컨대, 도11에 도시된 바와 같이, 현재 시점의 오브젝트 모션으로부터 소정의 컴포징 영상 중 각각의 조인트 포인트의 다음 위치는 미리 저장된 컴포징 영상을 기반으로 예측하여, 후속되는 조인트 포인트 위치를 추적하여 후속되는 컴포징 영상을 구현할 수 있게 된다. 또한, 모션 프리딕팅부는 현재의 조인트 포인트로부터 단위 시간당 속도 변화율을 산출하여, 후속되는 조 인트 포인트의 위치를 예측할 수 있다. 영상 매핑 유닛으로부터 획득된 소정의 컴포징 영상은 인공지능 학습 데이터를 위해 활용될 수 있게 된다. 본 발명의 권리 범위는 특허청구범위에 기재된 사항에 의해 결정되며, 특허 청구범위에 사용된 괄호는 선택적 한정을 위해 기재된 것이 아니라, 명확한 구성요소를 위해 사용되었으며, 괄호 내의 기재도 필수적 구성요소로 해석되어야 한다."}
{"patent_id": "10-2022-0040220", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도1은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 개념도이 다. 도2는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 블록도이 다. 도3은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 획득 유닛의 개념도이다. 도4는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 획 득 유닛의 개념도이다. 도5는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 획 득 유닛의 블록도이다. 도6은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 트레킹부 의 개념도이다. 도7은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 조인트 보정부의 개념도이다. 도8은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 익스트랙 션부의 개념도이다. 도9는 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 매핑 유닛의 개념도이다. 도10은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 영상 매 핑 유닛의 미리 설정된 바운더리에 대한 개념도이다. 도11은 본 발명의 일 실시예에 따른 AI 학습을 위한 배경 데이터의 실사 기반 오브젝트 생성 시스템의 모션 프 리딕팅부의 개념도이다."}
