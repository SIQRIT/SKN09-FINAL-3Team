{"patent_id": "10-2018-0157352", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0075071", "출원번호": "10-2018-0157352", "발명의 명칭": "불확실성 예측을 위한 샘플링 모델 생성 장치 및 방법, 불확실성 예측 장치", "출원인": "서울대학교산학협력단", "발명자": "이정우"}}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "딥러닝 기반으로 학습된 인공 신경망 모델과,인공 신경망 모델의 학습 과정에서 샘플링된 적어도 둘 이상의 가중치들 각각에 의해 모델링된 샘플링모델들과, 인공 신경망 모델 및 샘플링 모델들 각각에 동일한 하나의 데이터가 입력된 후, 각각으로부터 출력되는 출력값들을 종합하여 불확실성 정도가 반영된 결과값을 생성하는 출력 생성부를 포함하는 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 샘플링된 적어도 둘 이상의 가중치들은국소 최소(local minimum)에 근접한 것인 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 샘플링된 적어도 둘 이상의 가중치들은인공 신경망 모델의 훈련 데이터들 중 미니 배치의 데이터들 각각 대한 가중치의 경사 벡터들 간 내적값들의 합산값이 소정 임계치 이하일 때 샘플링된 것인 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1항에 있어서, 인공 신경망 모델은 N개의 레이블들 중 하나로 라벨링된 훈련 데이터들에 의해 학습되고, 샘플링 모델들 각각의 출력값은 N개의 레이블들 각각에 대해 상응하는 확률값들인 N차원 정보이되,출력 생성부는샘플링 모델들 각각으로부터 산출된 N차원 정보들을 합산한 하나의 N차원 정보를 출력하는 불확실성 정도 산출부를 포함하는 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1항에 있어서, 인공 신경망 모델은 N개의 레이블들 중 하나로 라벨링된 훈련 데이터들에 의해 학습되고, 샘플링 모델들 각각의 출력값은 N개의 레이블들 각각에 대해 상응하는 확률값들인 N차원 정보이되,출력 생성부는샘플링 모델들 각각으로부터 산출된 N차원 정보들 각각으로부터 최대 확률값을 가지는 하나의 레이블을 선별하고, 레이블들 별로 선별된 갯수를 카운팅한 값인 N차원 정보를 출력하는 불확실성 정도 산출부를 포함하는 불확공개특허 10-2020-0075071-3-실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 4항 또는 제 5항에 있어서, 출력 생성부는인공 신경망 모델로부터 출력된 정답 추정값와 함께 불확실성 정도 산출부로부터 출력된 정답 추정값에 대한 불확실성 정도를 별개로 출력하는 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 4항 또는 제 5항에 있어서, 출력 생성부는인공 신경망 모델로부터 출력된 정답 추정값 및 불확실성 정도 산출부로부터 출력된 불확실성 정도를 기반으로하나의 불확실성 반영된 정답 추정값을 산출하는 결과값 조합부를 더 포함하는 불확실성 예측 장치."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "인공 신경망에 훈련 데이터들 중 해당 순번의 미니 배치 데이터를 입력하는 단계와, 인공 신경망의 출력값으로부터 가중치의 국소 최소(local minimum) 근접 여부를 판단하는 단계와, 가중치가 국소 최소에 근접한 것으로 판단될 경우, 해당 가중치로 모델링된 샘플링 모델을 생성하고, 가우시안노이즈를 추가하여 가중치를 업데이트시키는 단계와, 가중치가 국소 최소에 근접한 것이 아닌 것으로 판단될 경우, 가중치를 업데이트시키는 단계를 포함하되, 상기 단계들을 순차적으로 반복하는 불확실성 예측을 위한 샘플링 모델 생성 방법."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서, 판단하는 단계는미니 배치의 데이터들 각각 대한 가중치의 경사 벡터를 산출하는 단계와, 경사 벡터들 간의 내적값들의 합산값을 산출하는 단계와, 합산값이 소정 임계치 이하인지를 판단하는 단계를 포함하는 불확실성 예측을 위한 샘플링 모델 생성 방법."}
{"patent_id": "10-2018-0157352", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 8항 내지 제 9항 중 하나에 기재된 단계들로 동작되는 불확실성 예측을 위한 샘플링 모델 생성 장치."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 불확실성 예측 장치로, 딥러닝 기반으로 학습된 인공 신경망 모델과, 인공 신경망 모델의 학습 과정에 서 샘플링된 적어도 둘 이상의 가중치들 각각에 의해 모델링된 샘플링 모델들과, 인공 신경망 모델 및 샘플링 모 델들 각각에 동일한 하나의 데이터가 입력된 후, 각각으로부터 출력되는 출력값들을 종합하여 불확실성가 반영된 결과값을 생성하는 출력 생성부를 포함한다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 기술에 관한 것으로, 특히 불확실성을 예측할 수 있는 딥러닝 기반 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 딥 러닝(Deep learning) 모델들은 컴퓨터 비전, 자연 언어 처리, 기계 제어 등 다양한 분야에서 사람을 능 가하는 성능을 보인다. 하지만 현재 모델들은 불확실성을 예측하지 못하는 한계가 있다. 그런데, 이러한 불확실성을 예측하는 것은 의학 진단이나 자율 주행 자동차와 같은 분야에서 매우 중요하다. 즉, 딥 러닝 모델이 정답을 예측함과 아울러 자신이 예측한 정답에 대한 불확실성에 대한 정보까지 제공해준다 면, 의료 진단 전문가 또는 자율 주행 전문가가 불확실성을 기반으로 더 정확한 판단을 할 수 있을 것이다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 확률적 그래디언트(stochastic gradient)를 이용하여 신경망 가중치(Neural Network weight)의 근사 사후 분포(approximate posterior distribution)를 탐색하는 불확실성 예측을 위한 샘플링 모델 생성 장치 및 방법을 제시한다. 본 발명은 샘플링 모델들을 이용하여 인공 신경망의 정답 추정값에 대한 불확실성 정보를 제공하는 불확실성 예 측 장치를 제공한다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명은 불확실성 예측 장치로, 딥러닝 기반으로 학습된 인공 신경망 모델과, 인공 신경망 모델의 학습 과정 에서 샘플링된 적어도 둘 이상의 가중치들 각각에 의해 모델링된 샘플링 모델들과, 인공 신경망 모델 및 샘플링 모델들 각각에 동일한 하나의 데이터가 입력된 후, 각각으로부터 출력되는 출력값들을 종합하여 불확실성가 반 영된 결과값을 생성하는 출력 생성부를 포함한다. 본 발명은 불확실성 예측을 위한 샘플링 모델 생성 방법으로, 인공 신경망에 훈련 데이터들 중 해당 순번의 미 니 배치 데이터를 입력하는 단계와, 인공 신경망의 출력값으로부터 가중치의 국소 최소(local minimum) 근접 여 부를 판단하는 단계와, 가중치가 국소 최소에 근접한 것으로 판단될 경우, 해당 가중치로 모델링된 샘플링 모델 을 생성하고, 가우시안 노이즈를 추가하여 가중치를 업데이트시키는 단계와, 가중치가 국소 최소에 근접한 것이 아닌 것으로 판단될 경우, 가중치를 업데이트시키는 단계를 포함하되, 상기 단계들을 순차적으로 반복한다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따라 확률적 그래디언트(stochastic gradient)를 이용하여 신경망 가중치(Neural Network weight)의 근사 사후 분포(approximate posterior distribution)를 표현해 줄 수 있는 샘플링 모델을 통해, 인공 신경망 의 정답 추정값에 대한 불확실성 정보를 제공해 줄 수 있다."}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 첨부된 도면을 참조하여, 바람직한 실시 예에 따른 불확실성 예측을 위한 샘플링 모델 생성 장치 및 방법, 불확실성 예측 장치에 대해 상세히 설명하면 다음과 같다. 여기서, 동일한 구성에 대해서는 동일부호를 사용하며, 반복되는 설명, 발명의 요지를 불필요하게 흐릴 수 있는 공지 기능 및 구성에 대한 상세한 설명은 생략한다. 발명의 실시형태는 당업계에서 평균적인 지식을 가진 자에게 본 발명을 보다 완전하게 설명하기 위해 서 제공되는 것이다. 따라서, 도면에서의 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 첨부된 블록도의 각 블록과 흐름도의 각 단계의 조합들은 컴퓨터 프로그램인스트럭션들(실행 엔진)에 의해 수행 될 수도 있으며, 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 블록도의 각 블록 또는 흐름도의 각 단계에서 설명된 기능 들을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능 하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트럭션들은 블록도의 각 블록 또는 흐 름도의 각 단계에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 그리고 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재되 는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계들이 수 행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 수 행하는 인스트럭션들은 블록도의 각 블록 및 흐름도의 각 단계에서 설명되는 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하다. 또한, 각 블록 또는 각 단계는 특정된 논리적 기능들을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있으며, 몇 가지 대체 실시 예들에서는 블록들 또는 단 계들에서 언급된 기능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 또는 단계들은 사실 실질적으로 동시에 수행되는 것도 가능하며, 또한 그 블록들 또는 단 계들이 필요에 따라 해당하는 기능의 역순으로 수행되는 것도 가능하다. 이하, 첨부 도면을 참조하여 본 발명의 실시 예를 상세하게 설명한다. 그러나 다음에 예시하는 본 발명의 실시 예는 여러 가지 다른 형태로 변형될 수 있으며, 본 발명의 범위가 다음에 상술하는 실시 예에 한정되는 것은 아 니다. 본 발명의 실시 예는 당업계에서 통상의 지식을 가진 자에게 본 발명을 보다 완전하게 설명하기 위하여 제공된다. 딥 러닝 모델에서 불확실성을 표현하는 방법으로 네트워크의 변수를 결정론적인 값(deterministic value)이 아 닌 확률론적인 값(stochastic value)로 바라보는 베이지안 신경망(Bayesian Neural Network)이 있다. 이러한 베이지안 딥 러닝 방법은 크게 두 가지가 존재하는데, 하나는 variational Bayesian 방법이고, 다른 하나는 MCMC(Markov Chain Monte Carlo) 방법을 신경망에 적용한 방법이다. 여기서, MCMC 방법은 샘플링 방법의 일종으로, 이를 통해 점진적으로 정확한 사후 분포(posterior distribution)를 알아낼 수 있다. 해당 샘플들을 이용하여 분포의 평균, 분산과 같이 정확한 사후 분포 (posterior distribution)에 대한 대표 값들을 대략적으로 알 수 있다. 하지만 MCMC 방법은 대표값들을 얻기 위 해 전체 데이터를 봐야하고, 수용 확률(acceptance probability)이 존재하여 데이터 셋이 클 경우 시간이 오래 걸린다는 단점이 존재한다. 이러한 종래의 MCMC의 단점을 보완하기 위한 방법으로 SG-MCMC(Stochastic Gradient MCMC)이 제안되었는데, 이 는 샘플링을 위해 일부 데이터만을 사용하고 수용 단계(acceptance step)가 존재하지 않는다. 이 방법을 이용하 여 큰 데이터 셋에 대하여도 사후 분포(posterior distribution)을 효율적으로 얻을 수 있는데, 대표적으로 SGlD(Stochastic-Gradient langevin Dynamics) 방법이 있다. SGlD는 확률적 최적화(stochastic optimization) 에 langevin dynamics 개념을 접목한 방법으로 네트워크를 학습시킬 때 그래디언트(gradient)에 가우시안 노이 즈(Gaussian noise)를 추가하는 방법으로, 이는 확률적 그래디언트(stochastic gradient)를 이용한 첫 번째 샘 플링 알고리즘이다. 본 발명은 이러한 확률적 그래디언트(stochastic gradient)를 응용한 불확실성 예측을 위한 샘플링 모델 생성 장치 및 방법, 불확실성 예측 장치를 제안한다. 이때, 본 발명에서 신경망(Neural Network)을 학습하면서 국소 최소(local minimum) 근접 여부 판단을 위한 그래디언트 확실성(gradient certainty)값을 기준으로 가중치(weight)를 샘플링하여 근사 사후 분포(approximate posterior distribution)를 구하는 새로운 방식이 적용된 다. 도 1은 본 발명의 일 실시 예에 따른 불확실성 예측을 위한 샘플링 모델 생성 장치를 설명하기 위한 블록 구성 도이고, 도 2는 본 발명의 일 실시 예에 따른 불확실성 예측을 위한 샘플링 모델 생성 방법을 설명하기 위한 순 서도이다. 도 1 및 2를 참조하면, 학습 제어부는 인공 신경망 모델의 가중치 및 미니 배치(mini-batch) 순번 (l)을 초기화(S210)하고, 학습 제어부는 l번째 미니 배치 데이터들을 인공 신경망 모델에 입력시킨다 (S220). 그런 후, 학습 제어부는 인공 신경망 모델의 출력값으로부터 l번째 미니 배치에 포함된 데이터들 각각 에 대한 가중치의 그래디언트 벡터(gradient vector) 시퀀스를 산출한다(S230). 이는 다음의 <수학식 1>과 같이 표현된다. 수학식 1"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "<수학식 1>에서 l은 미니 배치의 순번을 의미하고, m은 미니 배치 사이즈(mini-batch size)를 의미한다. 또한, 가중치의 그래디언트 벡터는 <수학식 2>와 같이 산출된다. 수학식 2"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "그런 후, 국소 최소 근접 산출부는 다음의 <수학식 3>과 같이 정의된 그래디언트 확실성(Gradient Certainty) 값을 산출한다(S240). 수학식 3"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "<수학식 3>에서 는 두 벡터 간의 내적을 의미한다. 이러한 그래디언트 확실성(Gradient Certainty) 값은 가중치(weight)가 국소 최적(local optimum) 근접 여부를 알려주는 지표이다. 즉, 그래디언트 확실성(Gradient Certainty) 값이 작다는 것은 그래디언트(gradient) 값들이 대체로 작고 그래디언트 벡터 (gradient vector)가 다양한 방향을 가리키고 있음을 뜻하므로, 해당 가중치(weight)가 국소 모드(local mod e)에 도달했다고 판단할 수 있다. 반면, 그래디언트 확실성(Gradient Certainty) 값이 크다는 것은 그래디언트 (gradient) 값들이 대체로 크고 그래디언트 벡터(gradient vector)가 일정한 방향을 가리키고 있음을 뜻하므로, 해당 가중치(weight)가 특정 방향을 향해 학습되고 있는 상태로 판단할 수 있다. 따라서, 본 발명에 따라, 국소 모드(local mode)에 도달된 가중치를 샘플링하기 위해, 가중치 샘플링부는 그래디언트 확실성 값이 소정 임계치 미만인지를 판단한다(S250). 즉, S235에서 그래디언트 불확실 값이 소정 임계치 미만일 경우, 즉 해당 가중치(weight)가 국소 모드(local mode)에 도달했다고 판단될 경우에만 가중치 샘플링부는 해당 가중치를 샘플링한다(S240). 그러면, 학습 제어부는 가중치 샘플링부로부터의 전달되는 샘플링 여부에 따라 가중치를 업데이트를 다 르게 한다. 즉, 가중치가 샘플링된 경우, 학습 제어부는 국소 모드(local mode)에서 빠져나오도록 다음의<수학식 4>와 같이 그래디언트(gradient)에 가우시안 노이즈(Gaussian noise)를 가산하여 가중치를 업데이트한 다(S270). 수학식 4"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "반면, 가중치가 샘플링되지 않은 경우, 즉 S250에서 그래디언트 불확실 값이 소정 임계치 미만이 아닐 경우, 학 습 제어부는 다음의 <수학식 5>와 같이 가중치를 업데이트한다(S280). 수학식 5"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "<수학식 4> 및 <수학식 5>에서 은 다음의 <수학식 6>과 같이 l번째 미니 배치에 포함된 데이터들 각각에 대한 가중치의 그래디언트 벡터(gradient vector) 들의 평균값으로, 학습 제어부에 의해 산출되는 값이다. 수학식 6"}
{"patent_id": "10-2018-0157352", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "그런 후, 학습 제어부는 미니 배치 순번 l이 마지막 T개에 도달하는지의 여부를 판단(S290)하고, 훈련할 미 니배치 데이터가 더 있을 경우, S320을 수행한 후, S220으로 되돌아가서 인공 신경망 모델의 학습을 진행하 여 새로운 국소 모드(local mode)를 찾아나선다. 전술한 바와 같은 과정에 의해 샘플링된 가중치들은 DB에 저장되고, 샘플링 모델 생성부는 샘플링된 가 중치들 각각에 상응하는 인공 신경망 모델과 동일한 구조의 샘플링 모델들(110-1, 110-2,...., 110-K)을 생 성한다. 이와 같이 본 발명에서 생성된 샘플링 모델들(110-1, 110-2,...., 110-K) 각각에서의 출력값들을 통해 가중치 (weight)의 사후 분포(posterior distribution) 및 인공 신경망 모델의 불확실성을 표현해낼 수 있다. 그 러면, 이러한 샘플링 모델들(110-1, 110-2,...., 110-K)을 이용한 불확실성 예측 장치에 대해 도 3 및 도 5를 참조하여 살펴보기로 한다. 도 3는 본 발명의 일 실시 예에 따른 불확실성 예측 장치의 블록 구성도이고, 도 4는 본 발명의 다른 실시 예에 따른 불확실성 예측 장치의 블록 구성도이다. 도 3 및 도 4를 참조하면, 불확실성 예측 장치는 인공 신경망 모델, 적어도 둘 이상의 샘플링 모델들 (110-1, 110-2,...., 110-K) 및 출력 생성부를 포함한다. 인공 신경망 모델은 라벨링된 훈련 데이터를 입력받아 레이블값에 근접한 출력값을 생성하도록 훈련되어 생 성된 딥러닝 기반 인공 지능 학습 모델이다. 예컨대, 인공 신경망 모델은 숫자 '4'의 형상 이미지 데이터가 입력됨에 따라, 훈련 데이터 셋의 레이블들 중 하나의 값 또는 레이블값이 나올 확률 벡터값을 출력하도록 훈련 된 분류 모델일 수 있다. 샘플링 모델들(110-1, 110-2,...., 110-K)은 인공 신경망 모델의 학습 과정에서 샘플링된 적어도 둘 이상의 가중치들 각각에 의해 모델링된 인공 신경망 모델일 수 있다. 즉, 전술한 바와 같이, 샘플링 모델들(110-1, 110-2,...., 110-K)은 인공 신경망 모델의 불확실성을 나타내 줄 수 있는 것으로, 가중치의 근사 사후 분포 (approximate posterior distribution)를 표현하기 위해 구현된 모델들이다. 여기서, 샘플링된 적어도 둘 이상의 가중치들은 국소 최소(local minimum)에 근접한 것으로 판단된 것일 수 있다. 더 상세하게는, 도 1 및 도 2 을 참조하여 전술한 바와 같이, 샘플링된 적어도 둘 이상의 가중치들은 인공 신경망 모델의 훈련 데이터들 중 미니 배치의 데이터들 각각 대한 가중치의 그래디언트 벡터들 간 내적값들의 합산값이 소정 임계치 이하일 때 샘플링된 것일 수 있다. 출력 생성부는 인공 신경망 모델 및 샘플링 모델들(110-1, 110-2,...., 110-K) 각각에 동일한 하나의 데이터를 입력시킨 후, 각각으로부터 출력되는 출력값들을 종합하여 가중치 사후 분포로 표현되는 불확실성가 반영된 결과값을 출력한다. 이를 위해, 출력 생성부는 샘플링 모델들(110-1, 110-2,...., 110-K) 각각으로 부터 출력값들을 취합하는 불확실성 산출부를 포함한다. 여기서, 인공 신경망 모델이 N개의 레이블들 중 하나로 라벨링된 훈련 데이터들에 의해 학습되고, 샘플링 모델들(110-1, 110-2,...., 110-K) 각각의 출력값은 N개의 레이블들 각각에 대해 상응하는 확률값들인 N차원 정 보일 수 있다. 이럴 경우, 일 양상에 따라, 불확실성 산출부는 샘플링 모델들(110-1, 110-2,...., 110-K) 각각으로부터 산출된 N차원 정보들 각각으로부터 샘플링 모델들 각각으로부터 산출된 N차원 정보들을 합산한 하나의 N차원 정 보를 출력할 수 있다. 예컨대, 인공 신경망 모델이 '0'에서 '9'로 라벨링된 MNIST 데이터들로 학습된 모델 일 경우, 샘플링 모델들(110-1, 110-2,...., 110-K) 각각은 입력된 데이터가 '0'에서 '9'까지의 레이블들 각각 에 대한 유사도를 확률값으로 출력해낼 수 있다. 즉, 샘플링 모델들(110-1, 110-2,...., 110-K) 각각은 10차원 확률 정보를 출력할 수 있다. 그러면, 불확실성 산출부는 K 개의 10차원 확률 정보들을 합산하여 출력한다. 다른 양상에 따라, 불확실성 산출부는 샘플링 모델들(110-1, 110-2,...., 110-K) 각각으로부터 산출된 N차 원 정보들 각각으로부터 샘플링 모델들 각각으로부터 산출된 N차원 정보들 각각으로부터 최대 확률값을 가지는 하나의 레이블을 선별하고, 레이블들 별로 선별된 갯수를 카운팅한 값인 N차원 정보를 출력할 수 있다. 출력 생성부는 이러한 불확실성 산출부에 의해 출력된 불확실성가 반영된 결과값을 생성하여 출력하 는데 다양한 실시 예들이 가능하다. 일 실시 예에 따라, 도 3에 도시된 바와 같이, 출력 생성부는 불확실성가 반영된 결과값으로 인공 신경망 모델로부터 출력된 정답 추정값 및 불확실성 산출부로부터 출력된 정답 추정값에 대한 불확실성을 별 개로 출력할 수 있다. 예컨대, 도 3에 도시된 입력 데이터에 대해 '7'이라는 정답 추정값과 아울러 '0'에서 ' 9'까지의 레이블들 각각에 대한 확률 분포도를 출력할 수 있다. 그러면, 비록 인공 신경망 모델이 '7'이라 고 정답 추정값을 출력하였으나, 입력된 데이터는 사람이 인식하는 것처럼 '3' 또는 '7' 로 판단될 확률이 엇비 슷하므로, 정답 추정값에 대한 불확실성가 크다고 짐작해 볼 수 있다. 다른 실시 예에 따라, 도 4에 도시된 바와 같이, 출력 생성부는 결과값 조합부를 더 포함하여, 인공 신경망 모델로부터 출력된 정답 추정값 및 불확실성 산출부로부터 출력된 정답 추정값에 대한 불확실 성을 기반으로 하나의 불확실성가 반영된 정답 추정값을 출력할 수도 있다. 예컨대, 인공 신경망 모델 및 불확실성 산출부로부터 출력된 N차원 확률 정보들을 합산하여 출력할 수도 있고, 합산된 N차원 확률 정보 로부터 상위 복수의 정답값들을 출력할 수도 있다. 즉, 사람이 인식하는 것과 같이 '입력 이미지가 '3'인것 같 기도 하고 '7'인것 같기도 하다'는 의미의 결과값을 출력할 수도 있다. 전술한 바와 같은 불확실성 예측 장치의 성능 평가를 위한 테스트 결과에 대해 다음의 도 5 내지 6을 참조하여 설명하기로 한다. 여기서, 가중치(weight)의 사후 분포(Posterior distribution)을 대략적으로 나타내고 불확실 성을 잘 표현하는지 확인하기 위해, 숫자 이미지인 MNIST 데이터 셋을 사용하여 인공 신경망 모델을 학습하 고, 판단하기 애매한 MNIST 데이터 및 숫자 이미지가 아닌 notMNIST 데이터 셋을 사용하여 불확실성 예측 장치 의 성능을 평가하였다. 도 5는 판단하기 애매한 MNIST 데이터에 대한 불확실성 테스트 결과 도면이고, 도 6는 notMNIST 데이터에 대한 불확실성 테스트 결과 도면이다. 도 5 및 도 6을 참조하면, MNIST 데이터 셋은 28*28 흑백 이미지로 손으로 작성한 숫자(0-9)가 적혀 있다. notMNIST 데이터 셋은 MNIST와 동일하게 28*28 흑백 이미지이지만 다양한 형태의 알파벳(A-J)이 그려져 있다. 그리고, 기존에 존재하는 SGLD(도 5 및 도 6의 (a)), Dropout(도 5 및 도 6의 (b))에 의해 생성된 모델에 의한 결과값과, 본 발명에서 제안하는 방법((도 5 및 도 6의 (c))의 결과값과 비교하였다.MNIST 테스트 데이터중에서 하나의 숫자라고 판단하기 애매한 데이터에 대하여 실험해보았다. 도 5에서와 같이 MNIST 데이터 중에서 하나의 숫자로 판단하기 애매한 그림들에 대해서 각각의 알고리즘을 실험해보았다. 첫 번 째 그림의 경우 정답 숫자는 '4'이지만 '9'처럼 보이기도 한다. 두 번째 그림의 경우 정답 숫자는 '7'이지만 '3'처럼 보이기도 한다. 두 데이터에 대해서 본 발명은 숫자 정답을 맞추었을 뿐만 아니라 불확실성을 잘 표현 하였다. 반면에 SGlD와 MNIST 데이터로 학습한 모델에 notMNIST 데이터로 테스트한 경우, Dropout 알고리즘의 경우 정답을 맞추지 못하거나, 맞추더라도 큰 확신을 가지고 정답을 맞추었기 때문에 불확실을 잘 표현하지 못 한 것으로 판단된다. 다음으로, MNIST로 학습한 모델에 notMNIST 데이터로 테스트한 경우를 살펴보았다. 도 6에 도시된 바와 같이, SGlD와 Dropout의 경우 A와 H에 대해서 특정 숫자를 나타낸다고 확신하는 반면, 본 발명에서는 0-9까지 여러 개 의 결과를 나타냄으로써 데이터에 대한 정답이 불확실함을 표현해줄 수 있다."}
{"patent_id": "10-2018-0157352", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 불확실성 예측 모델 생성 장치를 설명하기 위한 블록 구성도이다. 도 2는 본 발명의 일 실시 예에 따른 불확실성 예측 모델 생성 방법을 설명하기 위한 순서도이다. 도 3은 본 발명의 일 실시 예에 따른 불확실성 예측 장치의 블록 구성도이다. 도 4는 본 발명의 다른 실시 예에 따른 불확실성 예측 장치의 블록 구성도이다. 도 5는 판단하기 애매한 MNIST 데이터에 대한 불확실성 테스트 결과 도면이다. 도 6은 notMNIST 데이터에 대한 불확실성 테스트 결과 도면이다."}
