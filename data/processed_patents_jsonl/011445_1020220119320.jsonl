{"patent_id": "10-2022-0119320", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0040387", "출원번호": "10-2022-0119320", "발명의 명칭": "전자 장치 및 그 오디오 트랙 획득 방법", "출원인": "삼성전자주식회사", "발명자": "데그티아렌코 일리야"}}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "디스플레이; 스피커;적어도 하나의 인스트럭션(instruction)이 저장된 메모리; 및상기 디스플레이, 상기 스피커 및 상기 메모리와 연결되어 상기 전자 장치를 제어하는 하나 이상의 프로세서;를포함하며,상기 하나 이상의 프로세서는,상기 적어도 하나의 인스트럭션을 실행함으로써, 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하도록 상기 디스플레이를 제어하고, 상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부터 화자(speaker)의 컨텍스트 정보를획득하고, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하고, 상기 획득된 오디오 트랙 데이터를 출력하도록 상기 스피커를 제어하는, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 하나 이상의 프로세서는, 학습된 인공 지능 모델에 상기 자막 데이터 및 상기 화자의 컨텍스트 정보를 입력하여 상기 오디오 트랙 데이터를 획득하며, 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 텍스트 데이터를 화자 적응형(speaker adaptive) 오디오 데이터로변환하여 출력하도록 학습된 Text-to-Speech(TTS) 인공 지능 모델인, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미터를 획득하고, 상기 화자의 특성 파라미터에 기초하여 상기 자막 데이터를 상기 화자 적응형 오디오 데이터로 변환한 상기 오디오 트랙 데이터를 출력하도록 학습되는, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 하나 이상의 프로세서는, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미터를 식별하고, 상기 화자의 특성 파라미터에기초하여 상기 자막 데이터를 화자 적응형 오디오 데이터로 변환하여 상기 오디오 트랙 데이터를 획득하는, 전자 장치. 공개특허 10-2024-0040387-3-청구항 5 제3항 또는 제4항에 있어서, 상기 화자의 특성 파라미터는, 음성 타입(voice type), 음성 억양(voice intonation), 음성 피치(voice pitch), 음성 발화 속도(voice speechspeed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 포함하고, 상기 화자의 컨텍스트 정보는, 화자의 성별 정보, 나이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 중 적어도 하나를 포함하는, 전자장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서, 상기 하나 이상의 프로세서는, 상기 비디오 데이터 또는 상기 오디오 데이터 중 적어도 하나에 기초하여 발화 시작과 관련된 타이밍 데이터,상기 화자의 식별 데이터, 상기 화자의 감정 데이터를 획득하고, 상기 타이밍 데이터, 상기 식별 데이터 및 상기 감정 데이터에 기초하여 상기 화자의 특성 파라미터를 식별하는, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 하나 이상의 프로세서는, 특정 데이터 채널로부터 스트리밍되는 자막 데이터를 획득하거나, 상기 비디오 데이터에 포함된 프레임들에 대한 텍스트 인식을 통해 상기 자막 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 하나 이상의 프로세서는, 상기 비디오 데이터에 대응되는 오디오 데이터를 배경 오디오 데이터 및 스피치 오디오 데이터로 분리하고, 상기 스피치 오디오 데이터로부터 상기 화자의 컨텍스트 정보를 획득하며, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 획득된 상기 오디오 트랙 데이터 및 상기 배경 오디오 데이터를 믹싱하여 출력하도록 상기 스피커를 제어하는, 전자 장치."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서, 상기 하나 이상의 프로세서는, 상기 비디오 데이터에 제1 화자 및 제2 화자가 포함된 경우 상기 제1 화자의 발화 시작과 관련된 제1 타이밍 데이터 및 상기 제2 화자의 발화 시작과 관련된 제2 타이밍 데이터를 획득하고,상기 제1 타이밍 데이터, 상기 제1 화자의 제1 식별 데이터 및 상기 제1 화자의 제1 감정 데이터에 기초하여 상기 제1 화자에 대응되는 제1 자막 데이터를 TTS 변환하여 상기 제1 화자에 대응되는 제1 오디오 트랙 데이터를획득하고, 상기 제2 타이밍 데이터, 상기 제2 화자의 제2 식별 데이터 및 상기 제2 화자의 제2 감정 데이터에 기초하여 상기 제2 화자에 대응되는 제2 자막 데이터를 TTS 변환하여 상기 제2 화자에 대응되는 제2 오디오 트랙 데이터를획득하는, 전자 장치.공개특허 10-2024-0040387-4-청구항 10 전자 장치의 오디오 트랙 획득 방법에 있어서, 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하는 단계;상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부터 화자(speaker)의 컨텍스트 정보를획득하는 단계; 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하는 단계; 및상기 획득된 오디오 트랙 데이터를 출력하는 단계;를 포함하는 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 오디오 트랙 데이터를 획득하는 단계는, 학습된 인공 지능 모델에 상기 자막 데이터 및 상기 화자의 컨텍스트 정보를 입력하여 상기 오디오 트랙 데이터를 획득하며, 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 텍스트 데이터를 화자 적응형(speaker adaptive) 오디오 데이터로변환하여 출력하도록 학습된 Text-to-Speech(TTS) 인공 지능 모델인, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미터를 획득하고, 상기 화자의 특성 파라미터에 기초하여 상기 자막 데이터를 상기 화자 적응형 오디오 데이터로 변환한 상기 오디오 트랙 데이터를 출력하도록 학습되는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서, 상기 오디오 트랙 데이터를 획득하는 단계는, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미터를 식별하고, 상기 화자의 특성 파라미터에기초하여 상기 자막 데이터를 화자 적응형 오디오 데이터로 변환하여 상기 오디오 트랙 데이터를 획득하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항 또는 제13항에 있어서, 상기 화자의 특성 파라미터는, 음성 타입(voice type), 음성 억양(voice intonation), 음성 피치(voice pitch), 음성 발화 속도(voice speechspeed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 포함하고, 상기 화자의 컨텍스트 정보는, 화자의 성별 정보, 나이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 중 적어도 하나를 포함하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "공개특허 10-2024-0040387-5-제13항에 있어서, 상기 화자의 컨텍스트 정보를 획득하는 단계는, 상기 비디오 데이터 또는 상기 오디오 데이터 중 적어도 하나에 기초하여 발화 시작과 관련된 타이밍 데이터,상기 화자의 식별 데이터, 상기 화자의 감정 데이터를 획득하는 단계; 및 상기 타이밍 데이터, 상기 식별 데이터 및 상기 감정 데이터에 기초하여 상기 화자의 특성 파라미터를 식별하는 단계;를 포함하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제10항에 있어서, 특정 데이터 채널로부터 스트리밍되는 자막 데이터를 획득하거나, 상기 비디오 데이터에 포함된 프레임들에 대한 텍스트 인식을 통해 상기 자막 데이터를 획득하는 단계;를 더 포함하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제10항에 있어서, 상기 화자의 컨텍스트 정보를 획득하는 단계는, 상기 비디오 데이터에 대응되는 오디오 데이터를 배경 오디오 데이터 및 스피치 오디오 데이터로 분리하는단계;상기 스피치 오디오 데이터로부터 상기 화자의 컨텍스트 정보를 획득하는 단계;를 포함하며, 상기 오디오 트랙 데이터를 출력하는 단계는, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 획득된 상기 오디오 트랙 데이터 및 상기 배경 오디오 데이터를 믹싱하여 출력하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제10항에 있어서, 상기 오디오 트랙 데이터를 획득하는 단계는, 상기 비디오 데이터에 제1 화자 및 제2 화자가 포함된 경우 상기 제1 화자의 발화 시작과 관련된 제1 타이밍 데이터 및 상기 제2 화자의 발화 시작과 관련된 제2 타이밍 데이터를 획득하는 단계; 상기 제1 타이밍 데이터, 상기 제1 화자의 제1 식별 데이터 및 상기 제1 화자의 제1 감정 데이터에 기초하여 상기 제1 화자에 대응되는 제1 자막 데이터를 TTS 변환하여 상기 제1 화자에 대응되는 제1 오디오 트랙 데이터를획득하는 단계; 및상기 제2 타이밍 데이터, 상기 제2 화자의 제2 식별 데이터 및 상기 제2 화자의 제2 감정 데이터에 기초하여 상기 제2 화자에 대응되는 제2 자막 데이터를 TTS 변환하여 상기 제2 화자에 대응되는 제2 오디오 트랙 데이터를획득하는 단계;를 포함하는, 오디오 트랙 획득 방법."}
{"patent_id": "10-2022-0119320", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "전자 장치의 프로세서에 의해 실행되는 경우 상기 전자 장치가 동작을 수행하도록 하는 컴퓨터 명령을 저장하는비일시적 컴퓨터 판독 가능 매체에 있어서,상기 동작은,대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하는 단계;상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부터 화자(speaker)의 컨텍스트 정보를획득하는 단계; 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하는 단계; 및공개특허 10-2024-0040387-6-상기 획득된 오디오 트랙 데이터를 출력하는 단계;를 포함하는, 비일시적 컴퓨터 판독 가능 매체."}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 전자 장치는, 디스플레이, 스피커, 적어도 하나의 인스트럭션이 저장된 메모리 및 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하도록 디스플레이를 제어하고, 비디오 데이터 및 비디오 데 이터에 대응되는 오디오 데이터로부터 화자의 컨텍스트 정보를 획득하고, 자막 데이터 및 화자의 컨텍스트 정보 에 기초하여 대상 언어에 대응되는 오디오 트랙 데이터를 획득할 수 있다."}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 오디오 트랙 획득 방법에 관한 것으로, 더욱 상세하게는 자막 데이터에 기초하여 오 디오 트랙을 획득하는 전자 장치 및 그 오디오 트랙 획득 방법에 관한 것이다."}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 기술의 발달에 힘입어 다양한 유형의 전자기기들이 개발 및 보급되고 있다. 특히, TV 또는 스마트폰과 같 은 디스플레이 장치의 보급이 활발하게 진행되고 있다. 연구에 따르면 2023년까지 동영상이 모든 소비자 인터넷 트래픽의 82% 이상을 차지할 것이며, 자막(Subtitles)/ 캡션(Closed Caption)을 제공하는 경우 80%의 사용자들이 전체 비디오를 시청할 가능성이 더 높다고 한다. 자막 은 더 많은 사용자들에서 다가갈 수 있는 키(key)이며 영상 접근성에 있어 중요한 역할을 할 수 있다. 동영상에 자막이 있으면 청중 참여도가 높아지고 사용자 만족도가 향상되며 사용자가 동영상을 시청 방식을 선택할 수 있 게 된다. Verizon의 보고서에 따르면 50% 이상의 사람들이 캡션을 선호하며, 80%의 사용자는 자막을 사용할 수 있을 때 전체 비디오를 시청할 가능성이 더 높다고 하였다. 자막에 대한 의존도가 높아짐에 따라 전 세계 자막 및 자막 솔루션 시장 규모는 2019년 2억 6,340만 달러에서 2025년 3억 5,010만 달러로 CAGR 7.4%로 성장할 것으 로 예상된다. 이와 같이 비디오 스트림에 대한 번역 기능에 대한 필요도는 점차 증가하고 있다. 비디오 콘텐츠의 현지화는 마 케팅 콘텐츠를 해외 시장에 적응시키는 과정이며, 자막은 일반적으로 비디오를 현지화하는 가장 쉬운 방법이다. 자막을 음성으로 제공하는 오디오 트랙은 자막에 비해 비디오 컨텐츠에 대한 용이한 시청을 가능하게 하고 시각 장애가 있는 사람들이 자막이 있는 영화/TV 콘텐츠를 볼 수 있도록 도와줄 수 있다. 하지만, 일반적으로 사용 가능한 오디오 트랙 옵션의 수는 사용 가능한 자막 언어의 수보다 훨씬 적다."}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시 예에 따르면, 전자 장치는, 디스플레이, 스피커, 적어도 하나의 인스트럭션(instruction)이 저장된 메 모리 및 상기 디스플레이, 상기 스피커 및 상기 메모리와 연결되어 상기 전자 장치를 제어하는 하나 이상의 프 로세서를 포함할 수 있다. 상기 하나 이상의 프로세서는, 상기 적어도 하나의 인스트럭션을 실행함으로써, 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하도록 상기 디스플레이를 제어하고, 상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부터 화자(speaker)의 컨텍스트 정보를 획득하고, 상기 자막 데 이터 및 상기 화자의 컨텍스트 정보에 기초하여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하고, 상기 획득된 오디오 트랙 데이터를 출력하도록 상기 스피커를 제어할 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 학습된 인공 지능 모델에 상기 자막 데이터 및 상기 화자의 컨텍스 트 정보를 입력하여 상기 오디오 트랙 데이터를 획득할 수 있다. 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 텍스트 데이터를 화자 적응형(speaker adaptive) 오디오 데이터로 변환하여 출 력하도록 학습된 Text-to-Speech(TTS) 인공 지능 모델일 수 있다. 일 예에 따른 상기 학습된 인공 지능 모델은, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미 터를 획득하고, 상기 화자의 특성 파라미터에 기초하여 상기 자막 데이터를 상기 화자 적응형 오디오 데이터로 변환한 상기 오디오 트랙 데이터를 출력하도록 학습될 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 상기 화자의 컨텍스트 정보에 기초하여 상기 화자의 특성 파라미터 를 식별하고, 상기 화자의 특성 파라미터에 기초하여 상기 자막 데이터를 화자 적응형 오디오 데이터로 변환하 여 상기 오디오 트랙 데이터를 획득할 수 있다. 일 예에 따른 상기 화자의 특성 파라미터는, 음성 타입(voice type), 음성 억양(voice intonation), 음성 피치 (voice pitch), 음성 발화 속도(voice speech speed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 포함하 고, 상기 화자의 컨텍스트 정보는, 화자의 성별 정보, 나이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 중 적어도 하나를 포함할 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 상기 비디오 데이터 또는 상기 오디오 데이터 중 적어도 하나에 기 초하여 발화 시작과 관련된 타이밍 데이터, 상기 화자의 식별 데이터, 상기 화자의 감정 데이터를 획득하고, 상기 타이밍 데이터, 상기 식별 데이터 및 상기 감정 데이터에 기초하여 상기 화자의 특성 파라미터를 식별할 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 특정 데이터 채널로부터 스트리밍되는 자막 데이터를 획득하거나, 상기 비디오 데이터에 포함된 프레임들에 대한 텍스트 인식을 통해 상기 자막 데이터를 획득할 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 상기 비디오 데이터에 대응되는 오디오 데이터를 배경 오디오 데이 터 및 스피치 오디오 데이터로 분리하고, 상기 스피치 오디오 데이터로부터 상기 화자의 컨텍스트 정보를 획득 하며, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 획득된 상기 오디오 트랙 데이터 및 상기 배 경 오디오 데이터를 믹싱하여 출력하도록 상기 스피커를 제어할 수 있다. 일 예에 따른 상기 하나 이상의 프로세서는, 상기 비디오 데이터에 제1 화자 및 제2 화자가 포함된 경우 상기 제1 화자의 발화 시작과 관련된 제1 타이밍 데이터 및 상기 제2 화자의 발화 시작과 관련된 제2 타이밍 데이터 를 획득하고, 상기 제1 타이밍 데이터, 상기 제1 화자의 제1 식별 데이터 및 상기 제1 화자의 제1 감정 데이터 에 기초하여 상기 제1 화자에 대응되는 제1 자막 데이터를 TTS 변환하여 상기 제1 화자에 대응되는 제1 오디오 트랙 데이터를 획득하고, 상기 제2 타이밍 데이터, 상기 제2 화자의 제2 식별 데이터 및 상기 제2 화자의 제2 감정 데이터에 기초하여 상기 제2 화자에 대응되는 제2 자막 데이터를 TTS 변환하여 상기 제2 화자에 대응되는 제2 오디오 트랙 데이터를 획득할 수 있다. 일 실시 예에 따른 전자 장치의 오디오 트랙 획득 방법은, 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시하는 단계, 상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부터 화자(speaker)의 컨텍스트 정보를 획득하는 단계, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하는 단계 및, 상기 획득된 오디오 트랙 데이터를 출력 하는 단계를 포함할 수 있다. 일 실시 예에 따른 전자 장치의 프로세서에 의해 실행되는 경우 상기 전자 장치가 동작을 수행하도록 하는 컴퓨 터 명령을 저장하는 비일시적 컴퓨터 판독 가능 매체에 있어서, 상기 동작은, 대상 언어의 자막 데이터가 포함 된 비디오 데이터를 표시하는 단계, 상기 비디오 데이터 및 상기 비디오 데이터에 대응되는 오디오 데이터로부 터 화자(speaker)의 컨텍스트 정보를 획득하는 단계, 상기 자막 데이터 및 상기 화자의 컨텍스트 정보에 기초하 여 상기 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득하는 단계 및, 상기 획득된 오디오 트랙 데이터를 출력하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 널리 사용되는 일반적인 용어 들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 일 실시 예를 보다 상세하게 설명한다. 도 1은 일 실시 예에 따른 전자 장치의 구현 예를 설명하기 위한 도면이다. 전자 장치는 도 1에 도시된 바와 같이 TV, 스마트 폰, 태블릿 PC, HMD(Head mounted Display)와 같은 AR/VR 기기 등로 구현될 수 있으나, 이에 한정되는 것은 아니며 노트북 PC, 비디오 월(video wall), LFD(largeformat display), Digital Signage(디지털 간판), DID(Digital Information Display), 프로젝터 디스플레이, NED(Near Eye Display), 전자 액자, 카메라, 캠코더, 등과 같이 디스플레이 기능을 갖춘 장치라면 한정되지 않 고 적용 가능하다. 일 예에 따라 전자 장치는 다양한 압축 영상 및/또는 다양한 해상도의 비디오 데이터를 스트리밍 방식으로 수신하여 디스플레이할 수 있다. 다만, 이에 한정되는 것은 아니며 다운로드된 비디오 데이터를 디스플레이하는 것도 가능하다. 예를 들어, 전자 장치는 MPEG(Moving Picture Experts Group)(예를 들어, MP2, MP4, MP7 등), JPEG(joint photographic coding experts group), AVC(Advanced Video Coding), H.264, H.265, HEVC(High Efficiency Video Codec) 등으로 압축된 형태로 영상을 수신할 수 있다. 또는 영상 처리 장치 는 SD(Standard Definition), HD(High Definition), Full HD, Ultra HD 영상 중 어느 하나의 영상을 수신할 수 있다. 예를 들어, 전자 장치는 스트리밍되는 비디오 데이터, 특히, 자막 데이터를 포함하는 비디오 데이터를 함 께 표시할 수 있다. 자막 데이터는 음성 오디오 데이터를 사용자가 이해할 수 있는 언어로 번역한 텍스트 데이 터를 의미한다. 일 예에 따라 자막 데이터는 비디오 데이터에 포함된 프레임의 일 영역에 포함되어 수신될 수 있다. 다른 예에 따라 전자 장치는 별도의 데이터 채널을 통해 수신된 자막 데이터를 비디오 데이터에 대 응되는 오디오 데이터와 동기화하여 표시할 수 있다. 일 실시 예에 따라 전자 장치는 자막 데이터에 대한 실시간 오디오 트랙 생성을 통해 자막 데이터에 대응 되는 언어의 음성 데이터를 출력할 수 있다. 특히, 전자 장치는 고정된 음성의 오디오 트랙 데이터가 아닌 원본 비디오 데이터 및/또는 원본 오디오 데이터의 컨텍스트가 반영된 음성의 오디오 트랙 데이터를 생성하여 출력할 수 있다. 비디오 데이터가 실시간 스트리밍으로 수신되는 경우, 오디오 트랙 데이터 또한 실시간으로 생 성되어 출력될 수 있다. 일 에에 따라 전자 장치가 스피커를 구비하는 경우 스피커를 통해 오디오 트랙 데 이터를 출력하고 스피커를 구비하지 않는 경우 오디오 트랙 데이터를 외부 스피커 장치로 전송할 수 있다. 이하에서는 자막 데이터를 기초로 원본 비디오 데이터 및/또는 원본 오디오 데이터의 컨텍스트가 반영된 음성의 오디오 트랙 데이터를 생성하는 다양한 실시 예에 대해 설명하도록 하낟. 도 2a는 일 실시 예에 따른 전자 장치의 구성을 나타내는 블럭도이다. 도 2a에 따르면, 전자 장치는 디스플레이, 스피커, 메모리 및 하나 이상의 프로세서 를 포함한다. 디스플레이는 자발광 소자를 포함하는 디스플레이 또는, 비자발광 소자 및 백라이트를 포함하는 디스플레 이로 구현될 수 있다. 예를 들어, LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플 레이, LED(Light Emitting Diodes), 마이크로 LED(micro LED), Mini LED, PDP(Plasma Display Panel), QD(Quantum dot) 디스플레이, QLED(Quantum dot light-emitting diodes) 등과 같은 다양한 형태의 디스플레이 로 구현될 수 있다. 디스플레이 내에는 a-si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 한 편, 디스플레이는 플렉시블 디스플레이(flexible display), 롤러블 디스플레이(rollable display), 3차원 디스플레이(3D display), 복수의 디스플레이 모듈이 물리적으로 연결된 디스플레이 등으로 구현될 수 있다. 스피커는 오디오 데이터를 출력한다. 예를 들어, 스피커는 하나 이상의 프로세서에서 처리된 디 지털 오디오 데이터를 아날로그 오디오 데이터로 변환하고 증폭하여 출력할 수 있다. 예를 들어, 스피커는 적어도 하나의 채널을 출력할 수 있는, 적어도 하나의 스피커 유닛, D/A 컨버터, 오디오 앰프(audio amplifier) 등을 포함할 수 있다. 일 예에 따라 스피커는 다양한 멀티 채널 음향 신호를 출력하도록 구현될 수 있다. 예를 들어, 하나 이상의 프로세서는 생성된 오디오 트랙 데이터을 인핸스 처리하여 출력하도록 스피커 를 제어할 수 있다. 일 예에 따라 전자 장치는 스피커를 포함하지 않을 수 있으며, 생성된 오디오 트 랙 데이터이 외부 스피커 장치를 통해 출력되도록 오디오 트랙 데이터를 외부 스피커 장치로 전송할 수도 있다. 메모리는 하나 이상의 프로세서와 전기적으로 연결되며, 본 개시의 다양한 실시 예를 위해 필요한 데 이터를 저장할 수 있다. 메모리는 데이터 저장 용도에 따라 전자 장치(100')에 임베디드된 메모리 형태로 구현되거나, 전자 장치(100')에 탈부착이 가능한 메모리 형태로 구현될 수도 있다. 예를 들어, 전자 장치(10 0')의 구동을 위한 데이터의 경우 전자 장치에 임베디드된 메모리에 저장되고, 전자 장치(100')의 확장 기 능을 위한 데이터의 경우 전자 장치(100')에 탈부착이 가능한 메모리에 저장될 수 있다. 한편, 전자 장치(10 0')에 임베디드된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non-volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현될 수 있다. 또 한, 전자 장치(100')에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구 현될 수 있다. 일 예에 따라 메모리는 전자 장치(100')를 제어하기 위한 적어도 하나의 인스트럭션(instruction) 또는 인 스트럭션들을 포함하는 컴퓨터 프로그램을 저장할 수 있다. 일 예에 따라 메모리는 외부 장치(예를 들어, 소스 장치), 외부 저장 매체(예를 들어, USB), 외부 서버(예 를 들어 웹 하드) 등으로부터 수신된 영상, 즉 입력 영상, 다양한 데이터, 정보 등을 저장할 수 있다. 일 예에 따라 메모리는 복수의 레이어를 포함하는 인공 지능 모델(또는신경망 모델)에 관한 정보를 저장할 수 있다. 여기서, 인공 지능 모델에 관한 정보를 저장한다는 것은 인공 지능 모델의 동작과 관련된 다양한 정보, 예를 들어 인공 지능 모델에 포함된 복수의 레이어에 대한 정보, 복수의 레이어 각각에서 이용되는 파라 미터(예를 들어, 필터 계수, 바이어스 등)에 대한 정보 등을 저장한다는 것을 의미할 수 있다. 일 예에 따라 메모리는 화질 처리에 필요한 다양한 정보, 예를 들어 Noise Reduction, Detail Enhancement, Tone Mapping, Contrast Enhancement, Color Enhancement 또는 Frame rate Conversion 중 적어 도 하나를 수행하기 위한 정보, 알고리즘, 화질 파라미터 등을 저장할 수 있다. 또한, 메모리는 영상 처리 에 의해 생성된 최종 출력 영상을 저장할 수도 있다. 일 실시 예에 따르면, 메모리는 본 개시에 따른 다양한 동작들에서 생성되는 데이터를 저장하는 단일 메모 리로 구현될 수 있다. 다만, 다른 실시 예에 따르면, 메모리는 상이한 타입의 데이터를 각각 저장하거나, 상이한 단계에서 생성되는 데이터를 각각 저장하는 복수의 메모리를 포함하도록 구현될 수도 있다. 상술한 실시 예에서는 다양한 데이터가 하나 이상의 프로세서의 외부 메모리에 저장되는 것으로 설명 하였으나, 상술한 데이터 중 적어도 일부는 전자 장치(100') 또는 하나 이상의 프로세서 중 적어도 하나의 구현 예에 따라 프로세서 내부 메모리에 저장될 수도 있다. 하나 이상의 프로세서는 메모리에 저장된 적어도 하나의 인스트럭션(instruction)을 실행함으로써, 다양한 실시 예에 따른 전자 장치의 동작을 수행할 수 있다. 하나 이상의 프로세서는 전자 장치의 동작을 전반적으로 제어한다. 구체적으로, 프로세서는 전 자 장치의 각 구성과 연결되어 전자 장치의 동작을 전반적으로 제어할 수 있다. 예를 들어, 프로세서 는 디스플레이 및 메모리와 전기적으로 연결되어 전자 장치1l00)의 전반적인 동작을 제어할 수 있다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 본 개시에 따른 인공 지능과 관련된 기능은 하나 이상의 프로세서와 메모리를 통해 동작될 수 있다. 하나 이상의 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 이상의 프로세서(140는 CPU(Central Processing Unit), GPU(Graphic Processing Unit), NPU(Neural Processing Unit) 중 적어도 하나 를 포함할 수 있으나 전술한 프로세서의 예시에 한정되지 않는다. CPU는 일반 연산뿐만 아니라 인공 지능 연산을 수행할 수 있는 범용 프로세서로서, 다계층 캐시(Cache) 구조를 통해 복잡한 프로그램을 효율적으로 실행할 수 있다. CPU는 순차적인 계산을 통해 이전 계산 결과와 다음 계산 결과의 유기적인 연계가 가능하도록 하는 직렬 처리 방식에 유리하다. 범용 프로세서는 전술한 CPU로 명시한 경 우를 제외하고 전술한 예에 한정되지 않는다. GPU는 그래픽 처리에 이용되는 부동 소수점 연산 등과 같은 대량 연산을 위한 프로세서로서, 코어를 대량으로 집적하여 대규모 연산을 병렬로 수행할 수 있다. 특히, GPU는 CPU에 비해 컨볼루션(Convolution) 연산 등과 같 은 병렬 처리 방식에 유리할 수 있다. 또한, GPU는 CPU의 기능을 보완하기 위한 보조 프로세서(co-processor)로 이용될 수 있다. 대량 연산을 위한 프로세서는 전술한 GPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않 는다. NPU는 인공 신경망을 이용한 인공 지능 연산에 특화된 프로세서로서, 인공 신경망을 구성하는 각 레이어를 하드 웨어(예로, 실리콘)로 구현할 수 있다. 이때, NPU는 업체의 요구 사양에 따라 특화되어 설계되므로, CPU나 GPU 에 비해 자유도가 낮으나, 업체가 요구하기 위한 인공 지능 연산을 효율적으로 처리할 수 있다. 한편, 인공 지 능 연산에 특화된 프로세서로, NPU 는 TPU(Tensor Processing Unit), IPU(Intelligence Processing Unit), VPU(Vision processing unit) 등과 같은 다양한 형태로 구현 될 수 있다. 인공 지능 프로세서는 전술한 NPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 또한, 하나 이상의 프로세서는 SoC(System on Chip)으로 구현될 수 있다. 이때, SoC에는 하나 이상의 프 로세서 이외에 메모리, 및 프로세서와 메모리 사이의 데이터 통신을 위한 버스(Bus)등과 같은 네트워크 인 터페이스를 더 포함할 수 있다. 전자 장치에 포함된 SoC(System on Chip)에 복수의 프로세서가 포함된 경우, 전자 장치는 복수의 프로세서 중 일부 프로세서를 이용하여 인공 지능과 관련된 연산(예를 들어, 인공 지능 모델의 학습(learning)이나 추론 (inference)에 관련된 연산)을 수행할 수 있다. 예를 들어, 전자 장치는 복수의 프로세서 중 컨볼루션 연 산, 행렬 곱 연산 등과 같은 인공 지능 연산에 특화된 GPU, NPU, VPU, TPU, 하드웨어 가속기 중 적어도 하나를 이용하여 인공 지능과 관련된 연산을 수행할 수 있다. 다만, 이는 일 실시예에 불과할 뿐, CPU 등과 범용 프로 세서를 이용하여 인공 지능과 관련된 연산을 처리할 수 있음은 물론이다. 또한, 전자 장치는 하나의 프로세서에 포함된 멀티 코어(예를 들어, 듀얼 코어, 쿼드 코어 등)를 이용하여 인공 지능과 관련된 기능에 대한 연산을 수행할 수 있다. 특히, 전자 장치는 프로세서에 포함된 멀티 코어를 이 용하여 병렬적으로 컨볼루션 연산, 행렬 곱 연산 등과 같은 인공 지능 연산을 수행할 수 있다. 이하 하나 이상의 프로세서는 설명의 편의를 위하여 프로세서로 명명하도록 한다. 도 2b는 본 개시의 일 실시 예에 따른 전자 장치의 일 구현 예의 세부 구성를 나타내는 도면이다. 도 2b에 따르면, 전자 장치(100')는 디스플레이, 스피커, 메모리, 하나 이상의 프로세서, 통신 인터페이스, 사용자 인터페이스 및 카메라를 포함한다. 도 2b에 도시된 구성 중 도 2a에 도시된 구성과 중복되는 구성에 대해서는 자세한 설명을 생략하도록 한다. 통신 인터페이스는 외부 장치와 통신을 수행할 수 있다. 통신 인터페이스는 AP 기반의 Wi-Fi(와이파 이, Wireless LAN 네트워크), 블루투스(Bluetooth), 지그비(Zigbee), 유/무선 LAN(Local Area Network), WAN(Wide Area Network), 이더넷(Ethernet), IEEE 1394, MHL(Mobile High-Definition Link), AES/EBU(Audio Engineering Society/ European Broadcasting Union), 옵티컬(Optical), 코액셜(Coaxial) 등과 같은 통신 방식 을 통해 외부 장치(예를 들어, 소스 장치), 외부 저장 매체(예를 들어, USB 메모리), 외부 서버(예를 들어 웹 하드) 등으로부터 스트리밍 또는 다운로드 방식으로 입력 영상을 수신할 수 있다. 여기서, 입력 영상은 SD(Standard Definition), HD(High Definition), Full HD 또는 Ultra HD 영상 중 어느 하나의 디지털 영상이 될 수 있으나 이에 한정되는 것은 아니다. 사용자 인터페이스는 버튼, 터치 패드, 마우스 및 키보드와 같은 장치로 구현되거나, 상술한 디스플레이 기능 및 조작 입력 기능도 함께 수행 가능한 터치 스크린으로 구현될 수 있다. 일 실시 예에 따라 사용자 인터 페이스는 리모콘 송수신부로 구현되어 원격 제어 신호를 수신할 수 있다. 리모콘 송수신부는 적외선 통신, 블루투스 통신 또는 와이파이 통신 중 적어도 하나의 통신 방식을 통해 외부 원격 제어 장치로부터 리모콘 신호 를 수신하거나, 리모콘 신호를 송신할 수 있다. 카메라는 기 설정된 이벤트에 따라 턴 온 되어 촬영을 수행할 수 있다. 카메라는 촬상된 영상을 전기 적인 신호로 변환하고 변환된 신호에 기초하여 영상 데이터를 생성할 수 있다. 예를 들어, 피사체는 반도체 광 학소자(CCD; Charge Coupled Device)를 통해 전기적인 영상 신호로 변환되고, 이와 같이 변환된 영상 신호는 증 폭 및 디지털 신호로 변환된 후 신호 처리될 수 있다. 전자 장치 (100')는 구현 예에 따라 튜너 및 복조부를 추가적으로 포함할 수 있다. 튜너(미도시)는 안테나를 통 해 수신되는 RF(Radio Frequency) 방송 신호 중 사용자에 의해 선택된 채널 또는 기 저장된 모든 채널을 튜닝하 여 RF 방송 신호를 수신할 수 있다. 복조부(미도시)는 튜너에서 변환된 디지털 IF 신호(DIF)를 수신하여 복조하 고, 채널 복호화 등을 수행할 수도 있다. 도 3은 일 실시 예에 따른 오디오 트랙 데이터 획득 방법을 설명하기 위한 도면이다. 도 3에 도시된 일 실시 예에 따르면, 프로세서는 대상 언어의 자막 데이터가 포함된 비디오 데이터를 표시 하도록 디스플레이를 제어할 수 있다(S310). 예를 들어, 프로세서는 자막 데이터를 포함하는 비디오 데이터를 수신하여 표시하거나, 별도의 데이터 채널을 통해 수신된 자막 데이터를 비디오 데이터에 대응되는 오 디오 데이터와 동기화하여 표시할 수 있다. 이어서, 프로세서는 비디오 데이터 또는 비디오 데이터에 대응되는 오디오 데이터 중 적어도 하나로부터, 비디오 데이터에 포함된 화자(speaker)의 컨텍스트 정보를 획득할 수 있다. 여기서, 화자의 컨텍스트 정보는 화 자의 성별 정보, 나이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 중 적어도 하나를 포함할 수 있다. 이어서, 프로세서는 자막 데이터 및 화자의 컨텍스트 정보에 기초하여 대상 언어에 대응되는 오디오 트랙 데이터(audio track data)를 획득할 수 있다. D여기서, 오디오 트랙이란 오디오 신호를 디지털로 기록한 트랙일 수 있다. 즉, 프로세서는 오디오 데이터/비디오 데이터에 기초하여 획득된 자막 데이터에 대응되는 대상 언어의 텍스트 데이터 뿐 아니라 화자의 메타 데이터(성별, 나이 감정 등)가 반영된 오디오 트랙 데이터를 획득 할 수 있다. 이 후, 프로세서는 획득된 오디오 트랙 데이터를 출력하도록 스피커를 제어할 수 있다. 도 4는 일 실시 예에 따른 오디오 트랙 데이터 획득 방법을 설명하기 위한 도면들이다. 도 4에 도시된 바와 같이 프로세서는 자막 데이터 획득 모듈, 화자의 컨텍스트 데이터 획득 모듈 및 TTS(Text-To-Speech) 모듈을 이용하여 오디오 트랙을 생성할 수 있다. 여기서, 각 모듈은 적어도 하나의 소프트웨어, 적어도 하나의 하드웨어 및/또는 이들의 조합으로 구현될 수 있다. 예를 들어, 자막 데이터 획득 모듈, 화자의 컨텍스트 데이터 획득 모듈 및 TTS(Text-To-Speech) 모듈 중 적어도 하나는 학습된 인공 지능 모델 및/또는 기 정의된 알고리즘을 이용하도록 구현될 수 있다. 자막 데이터 획득 모듈 , 화자의 컨텍스트 데이터 획득 모듈 및 TTS(Text-To-Speech) 모듈은 전자 장치 내에 포 함될 수 있으나, 일 예에 따라 적어도 하나의 서버에 분산될 수 있다. 예를 들어, 자막 데이터 획득 모듈 은 제1 서버에 포함될 수 있으며, 화자의 컨텍스트 데이터 획득 모듈은 제2 서버에 포함될 수 있으 며,TTS(Text-To-Speech) 모듈은 제3 서버에 포함될 수 있다. 이때, 제1 서버 내지 제3 서버는 서로 직접적 으로 연결되거나 전자 장치(또는 중개 서버)를 통해 연결되어 데이터를 송수신할 수 있다. 일 예에 따르면, 프로세서는 자막 데이터 획득 모듈을 이용하여 특정 데이터 채널로부터 스트리밍되 는 자막 데이터를 획득하거나, 비디오 데이터에 포함된 프레임들에 대한 텍스트 인식을 통해 자막 데이터를 획 득할 수 있다. 예를 들어, 프로세서는 특정 데이터 채널로부터 스트리밍되는 디지털 스트림에서 자막 데이 터를 획득할 수 있다. 일 예에 따르면, 프로세서는 화자의 컨텍스트 데이터 획득 모듈을 이용하여 비디오 데이터 및 오디오 데이터로부터 화자의 컨텍스트 정보를 획득할 수 있다. 여기서, 화자의 컨텍스트 정보는 화자의 성별 정보, 나 이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 중 적어도 하나를 포함할 수 있다. 일 예에 따르면, 프로세서는 TTS(Text-To-Speech) 모듈을 이용하여 자막에 대응되는 텍스트 데이터 (즉, 대상 언어의 텍스트 데이터)를 음성으로 변환할 수 있다. 이 경우, TTS 모듈은 자막에 대응되는 텍스 트 데이터를 기 정의된 음성으로 변환하는 것이 아닌, 화자의 컨텍스트가 반영된 음성으로 변환할 수 있다. 즉, TTS 모듈은 화자의 성별 정보, 나이 정보, 감정 정보, 캐릭터 정보 또는 발화 음량 정보 등이 반영된 오디 오 트랙 데이터를 획득할 수 있다. 그 외에 TTS 모듈은 오디오 데이터에 기초하여 획득된 음성에 대한 정 보, 예를 들어 주파수 정보(주파수 대역, 주파수 스펙트럼, 주파수 파형 등) 등에 기초하여 가능한 화자의 음색 과 유사한 음고/음색의 오디오 트랙 데이터를 획득할 수 있다. 도 5는 도 4에 도시된 오디오 트랙 데이터 획득 방법을 자세히 설명하기 위한 도면이다. 도 5에 도시된 일 실시 예에 따르면, 화자의 컨텍스트 데이터 획득 모듈(이하 컨텍스트 데이터 획득 모 듈)은 오디오 데이터 및 비디오 데이터에 기초하여 화자의 데이터 예를 들어, 컨텍스트 데이터를 식별할 수 있 다. 여기서, 오디오 데이터 및/또는 비디오 데이터는 비디오 데이터와 동기화되어 수신된 데이터로 일부 전처리 또는/및 후처리된 데이터일 수 있으나 이에 한정되는 것은 아니다. 예를 들어, 인코딩된 오디오 데이터 및/또는 비디오 데이터가 수신된 경우, 디코딩된 후 화자의 컨텍스트 정보를 획득하는데 이용될 수 있다. 예를 들어, 오 디오 데이터 및/또는 비디오 데이터는 디코딩되어 출력되기 전 출력 버퍼에 저장된 데이터일 수 있으나, 이에 한정되는 것은 아니다. 예를 들어, 수신 버퍼에 버퍼링된 오디오 데이터 및/또는 비디오 데이터가 디코딩된 후 화자의 컨텍스트 정보를 위해 출력 버퍼가 아닌 별도의 메모리에 저장될 수 있다. 다만, 이에 한정되는 것은 아 니며 비디오 데이터가 화면 상에 표시되고 있는 상태에서 화면을 캡쳐하고 캡쳐된 화면에 포함된 자막 데이터를인식하여 자막 텍스트 데이터를 획득하는 것도 가능할 수 있다. 일 예에 따라 컨텍스트 데이터 획득 모듈은 비디오 데이터 및 오디오 데이터로부터 발화 시작과 관련된 타 이밍 데이터, 화자의 식별 데이터 및 화자의 감정 데이터를 포함하는 화자의 컨텍스트 데이터를 식별할 수 있다. 여기서, 화자의 식별 데이터는 화자의 성별 정보, 나이 정보, 캐릭터 정보, 배우 정보 등 화자를 식별할 수 있는 다양한 타입의 식별 데이터를 포함할 수 있다. 화자의 감정 데이터는 기쁨, 슬픔, 화남, 놀람, 지루함, 평온 등 다양한 타입의 감정 데이터를 포함할 수 있다. 일 예에 따라 컨텍스트 데이터 획득 모듈은 비디오 데이터에 복수의 화자, 예를 들어 제1 화자 및 제2 화 자가 포함된 경우 각 화자와 관련된 컨텍스트 데이터를 각각 획득할 수 있다. 컨텍스트 데이터 획득 모듈 은 제1 화자의 발화 시작과 관련된 제1 타이밍 데이터, 제1 화자의 제1 식별 데이터 및 제1 화자의 제1 감정 데 이터를 획득하고, 제2 화자의 발화 시작과 관련된 제2 타이밍 데이터, 제2 화자의 제2 식별 데이터 및 제2 화자 의 제2 감정 데이터를 획득할 수 있다. 일 예에 따라 컨텍스트 데이터 획득 모듈은 오디오 데이터에 포함 된 음성 데이터의 특징(예를 들어, 억양, 피치, 주파수 등)을 분석하여 음성 데이터가 동일한 화자의 음성인지 다른 화자의 음성인지 식별하고, 그에 기초하여 제1 타이밍 데이터 및 제2 타이밍 데이터를 획득할 수 있다. 또한, 컨텍스트 데이터 획득 모듈은 화자의 컨텍스트 정보에 기초하여 화자의 특성 파라미터를 획득할 수 있다. 여기서, 화자의 특성 파라미터는, 음성 타입(voice type), 음성 억양(voice intonation), 음성 피치 (voice pitch), 음성 발화 속도(voice speech speed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 포함할 수 있다. 그 외, 음성의 주파수 대역, 주파수 스펙트럼, 주파수 파형 등의 정보를 더 포함할 수 있다. 자막 데이터 획득 모듈은 비디오 데이터에 포함된 프레임들에 대한 텍스트 인식을 통해 자막 텍스트 데이 터를 획득할 수 있다. 예를 들어, 프로세서는 비디오 프레임에 대한 텍스트 영역(또는 자막 영역) 인식 및 OCR(optical character reader) 인식을 통해 자막 데이터를 획득할 수 있다. 예를 들어, 프로세서는 비디 오 데이터를 학습된 인공 지능 모델(또는 알고리즘)에 입력하여 자막 데이터를 획득할 수 있다. 여기서, 학습된 인공 지능 모델은 비디오 데이터에 포함된 텍스트 데이터를 감지 및 인식하여 출력하도록 학습될 수 있다. 일 예에 따라 자막 데이터가 특정 데이터 채널로부터 디지털 스트림으로 스트리밍되는 경우 자막 데이터 획득 모듈을 거치지 않고 TTS(Text-To-Speech) 모듈로 제공될 수도 있다. 다만, 이 경우에도 화자의 특성 데이터 및 자막 데이터 간 동기화가 필요하므로 자막 데이터 획득 모듈이 해당 기능을 수행하는 것도 가능 할 수 있다. TTS(Text-To-Speech) 모듈을 자막 텍스트 데이터 및 화자의 특성 파라미터에 기초하여 화자 적응형 오디오 트랙 데이터를 생성할 수 있다. 일 예에 따라, 자막 데이터 획득 모듈로부터 출력되는 자막 텍스트 데이터 및/또는 특정 데이터 채널로부터 스트리밍되는 자막 텍스트 데이터는 컨텍스트 데이터 획득 모듈로부터 출 력되는 화자의 특성 파라미터와 동기화되어 TTS 모듈로 입력될 수 있다. 이 경우 동기화 작업은 자막 데이 터 획득 모듈 및 컨텍스트 데이터 획득 모듈의 출력단에서 수행되거나, TTS 모듈의 입력단에서 수행될 수 있다. 예를 들어, 비디오 데이터 및 오디오 데이터로부터 획득된 발화 시작과 관련된 타이밍 데이터 에 기초하여 자막 텍스트 데이터 및 화자의 특성 파라미터가 동기화되어 TTS 모듈로 입력될 수 있다. TTS 모듈은 일 예에 따라 비디오 데이터에 복수의 화자, 예를 들어 제1 화자 및 제2 화자가 포함된 경우 각 화자의 컨텍스트 데이터에 기초하여 대응되는 자막 텍스트를 변환할 수 있다. 예를 들어, TTS 모듈은 제1 타이밍 데이터, 제1 화자의 제1 식별 데이터 및 제1 화자의 제1 감정 데이터에 기초하여 제1 화자에 대응되 는 제1 자막 텍스트 데이터를 TTS 변환하여 제1 화자에 대응되는 제1 오디오 트랙 데이터를 획득할 수 있다. 또 한, TTS 모듈은 제2 타이밍 데이터, 제2 화자의 제2 식별 데이터 및 제2 화자의 제2 감정 데이터에 기초하 여 제2 화자에 대응되는 제2 자막 데이터를 TTS 변환하여 제2 화자에 대응되는 제2 오디오 트랙 데이터를 획득 할 수 있다. 도 6a 및 도 6b는 일 실시 예에 따라 화면을 캡쳐하여 자막 텍스트 데이터를 획득하는 방법을 설명하기 위한 도 면들이다. 일 실시 예에 따르면, 자막 데이터 획득 모듈은 비디오 데이터에 포함된 자막 데이터를 인식하여 자막 텍 스트 데이터를 획득할 수 있다. 도 6a에 도시된 일 예에 따르면, 자막 데이터 획득 모듈은 화면을 캡쳐하고(141-1), 캡쳐된 화면 이미지에 서 자막 영역이 포함된 것으로 식별되면(S141-2), 자막 영역의 위치를 식별할 수 있다(141-3). 이어서, 자막 데 이터 획득 모듈은 식별된 자막 영역 위치에서 텍스트 인식(예를 들어, OCR 인식)을 수행하여(141-4) 텍스트를 획득할 수 있다. 이 후, 자막 데이터 획득 모듈은 획득된 텍스트에 대한 문법 수정(141-5)을 거쳐 최 종 자막 텍스트를 획득할 수 있다(141-6). 예를 들어, 도 6b에 도시된 바와 같이 디스플레이에 비디오 데 이터가 표시되는 동안 화면을 캡쳐하고, 캡쳐된 화면 이미지에 포함된 자막 영역(610, 620)에 대한 텍스트 인식 을 통해 자막 텍스트를 획득할 수 있다. 도 7은 일 실시 예에 따른 오디오 트랙 획득 방법을 설명하기 위한 도면이다. 도 7에 따르면, 프로세서는 자막 데이터 획득 모듈, 화자의 컨텍스트 데이터 획득 모듈, TTS(Text-To-Speech) 모듈 및 어댑티브 필터 모듈 및 오디오 믹서를 이용하여 오디오 트랙을 생성할 수 있다. 도 7에 도시된 구성 중 도 4, 5에 도시된 구성과 중복되는 구성에 대해서는 자세한 설명을 생 략하도록 한다. 여기서, 각 모듈은 적어도 하나의 소프트웨어, 적어도 하나의 하드웨어 및/또는 이들의 조합으 로 구현될 수 있다. 자막 데이터 획득 모듈은 도 5에서 충분히 설명하였으므로 설명의 편의를 위하여 생략 도시하였다. 프로세서는 어댑티브 필터를 이용하여 오디오 데이터를 배경 오디오 데이터 및 스피치 오디오 데이터 로 분리할 수 있다. 어댑티브 필터를 통해 분리된 배경 오디오 데이터는 오디오 믹서로 제공되고, 스 피치 오디오 데이터는 화자의 컨텍스트 데이터 획득 모듈로 제공될 수 있다. 일 예에 따라 컨텍스트 데이터 획득 모듈은 비디오 데이터 및 스피치 오디오 데이터로부터 발화 시작과 관 련된 타이밍 데이터, 화자의 식별 데이터 및 화자의 감정 데이터를 포함하는 화자의 컨텍스트 데이터를 식별할 수 있다. 또한, 컨텍스트 데이터 획득 모듈은 화자의 컨텍스트 정보에 기초하여 화자의 특성 파라미터를 획득할 수 있다. 여기서, 화자의 특성 파라미터는, 음성 타입(voice type), 음성 억양(voice intonation), 음 성 피치(voice pitch), 음성 발화 속도(voice speech speed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 포함할 수 있다. 그 외, 음성의 주파수 대역, 주파수 스펙트럼, 주파수 파형 등의 정보를 더 포함할 수 있다. TTS(Text-To-Speech) 모듈은 자막 텍스트 데이터 및 화자의 특성 파라미터에 기초하여 화자 적응형 오디오 트랙 데이터를 생성할 수 있다. 일 예에 따라, TTS(Text-To-Speech) 모듈은 타이밍 데이터, 화자의 식별 데이터 및 화자의 감정 데이터에 기초하여 화자 특성 파라미터를 조정하고(143-1), 조정된 파라미터에 기초하여 자막 텍스트 데이터에 대한 TTS를 수행하여 자막 텍스트에 대응되는 스피치 데이터를 생성할 수 있다. 일 예에 따라 TTS 모듈은 타이밍 데이터, 화자의 식별 데이터 및 화자의 감정 데이터에 기초하여 음성 타 입(voice type), 음성 억양(voice intonation), 음성 피치(voice pitch), 음성 발화 속도(voice speech speed) 또는 음성 볼륨(voice volume) 중 적어도 하나를 조정할 수 있다. 이 후, 오디오 믹서는 배경 오디오 데이터 및 TTS 모듈로부터 출력된 스피치 데이터를 믹싱하여 새로 운 오디오 데이터, 즉 오디오 트랙 데이터를 출력할 수 있다. 도 8은 일 실시 예에 따른 TTS 모듈의 구현 형태를 설명하기 위한 도면이다. 일 예에 따르면, TTS 모듈은 저장된 기정의된 동작 규칙 또는 인공 지능 모델로 구현되어 입력 데이터를 처리할 수 있다. 기정의된 동작 규칙 또는 인공 지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습 을 통해 만들어진다는 것은, 다수의 학습 데이터들에 학습 알고리즘을 적용함으로써, 원하는 특성의 기정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공 지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버/시스템을 통해 이루어 질 수도 있다. 인공 지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 적어도 하나의 레이어는 적어도 하나의 가중치 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 적어도 하나의 정의된 연산을 통해 레이 어의 연산을 수행한다. 신경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트워크 (Deep Q-Networks), Transformer가 있으며, 본 개시에서의 신경망은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기(예컨대, 로봇)을 훈련시켜 소정의 대상 기기 스스로 결정을 내리거 나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 일 예에 따르면, TTS 모듈은 학습된 인공 지능 모델로 구현될 수 있다. 이 경우, 프로세서는 자막 텍스트 데이터 및 화자의 컨텍스트 정보를 입력하여 학습된 인공 지능 모델에 입력하여 오디오 트랙 데이터를 획득할 수 있다. 여기서, 학습된 인공 지능 모델은, 화자의 컨텍스트 정보에 기초하여 텍스트 데이터를 화자 적응형 (speaker adaptive) 오디오 데이터로 변환하여 출력하도록 학습된 Text-to-Speech(TTS) 인공 지능 모델일 수 있다. 일 예에 따르면, 학습된 인공 지능 모델은, 화자의 컨텍스트 정보에 기초하여 화자의 특성 파라미터를 획 득하고, 화자의 특성 파라미터에 기초하여 자막 텍스트 데이터를 화자 적응형 오디오 데이터로 변환한 오디오 트랙 데이터를 출력하도록 학습될 수 있다. 도 9는 일 실시 예에 따른 화자의 식별 데이터 획득 방법을 설명하기 위한 도면이다. 일 예에 따르면, 프로세서는 feature extractor, frame feature aggregation module, multi-modal fusion module 및 classifier를 이용하여 비디오 클립(video clip)으로부터 Person ID를 획득할 수 있다. 예를 들어, Face embeddings은 AttentionVLAD에 의해 모아진 후, multi-modal fusion module에 의해 다른 modal embeddings과 융합될 수 있다. Feature Extractor는 영상 전처리 및/또는 특징 추출을 수행할 수 있다. 페이스, 오디오, 바디, 텍스트 얼굴, 오디오, 몸, 텍스트 등과 같은 다중 모달 정보가 입력 비디오에서 획득될 수 있다. 이 후, Feature Extractor는 기 훈련된 모델을 사용하여 각 임베딩을 추출할 수 있다. Frame Aggregation Module은 비디오를 구성하는 프레임들의 특징을 집계할 수 있다. 예를 들어, 비디오는 지속 적으로 변화하는 프레임으로 구성될 수 있다. 즉, 특정 프레임에서 페이스가 감지되면 각도, 표정 또는 선명도 가 다를 수 있지만, 그럼에도 인접한 프레임에 동일한 레이블이 있는 페이스가 포함될 가능성이 있다. 페이스 퀄리티를 측정하고 서로 다른 클립(clip)에서 서로 다른 페이스 수로 인한 복잡성을 제거하기 위해 NetVLAD를 기반으로 프레임 특징을 집계할 수 있다. Frame Aggregation Module은 임의의 개수의 특징이 입력되면, 고정 길 이 임베딩을 비디오 레벨 페이스 특징(video-level face feature)으로 생성할 수 있다. Multi-modal Fusion Module은 다중 모달 특징의 가중치를 학습하고 가중치에 따라 다중 모달 특징을 재조정할 수 있다. 이는 비디오 클립의 다중 모달 특징은 보완적이고 중복될 수 있기 때문이다. Classifier는 Person ID(identity)를 예측할 수 있다. 비디오에서 supervised person recognition에 초점을 맞 춘다는 전제에 기초하여 MLP(Multi-Layer Perceptron)를 분류기로 사용하여 Person ID를 예측할 수 있다. 도 10은 일 실시 예에 따른 화자의 감정 데이터 획득 방법을 설명하기 위한 도면이다. 일 예에 따르면, Facial emotion recognition(FER)은 페이스 및 페이스 컴포넌트 검출, 특징 추출, 표정 분류 순으로 수행될 수 있다. 예를 들어, 도 10에 도시된 바와 같이 입력 영상(a)에서 페이스 영역 및 페이스 랜드 마크를 검출하고(b), 페이스 컴포넌트((예: 눈, 코) 및 랜드 마크에서 공간적, 시간적 특징을 추출하고(c), SVM(Support Vector Machine), AdaBoost, Random Forest와 같은 기 훈련된 패턴 분류기를 사용하여 식별된 페 이스 카테고리 중 하나에 기초하여 얼굴 표정을 결정할 수 있다(d). 다만, 이는 일 예일 뿐이며 화자의 감정 데 이터는 그 외 다양한 방법으로 획득가능하다. 도 11a 내지 도 11d는 일 실시 예에 따른 화자의 컨텍스트 정보의 획득 예시를 설명하기 위한 도면들이다. 도 11a에 따르면, 제1 비디오 데이터 및/또는 제1 오디오 데이터에 기초하여 제1 컨텍스트 정보 가 획득될 수 있다. 예를 들어, 제1 비디오 데이터에 포함된 복수의 화자(speaker 1, speaker 2) 각각의 성별, 나이, 감정, 발화 여부, 발화 볼륨 등이 획득될 수 있다. 다만, 해당 예시에서 제1 오디오 데이터 는 검출되지 않았으므로 발화여부를 제외한 제1 컨텍스트 정보는 제1 비디오 데이터에 기초 하여 획득될 수 있다. 도 11b에 따르면, 제2 비디오 데이터 및/또는 제2 오디오 데이터에 기초하여 제2 컨텍스트 정보 가 획득될 수 있다. 예를 들어, 제1 비디오 데이터에 포함된 복수의 화자(speaker 1, speaker 2) 각각의 성별, 나이, 감정, 발화 여부, 발화 볼륨 등이 획득될 수 있다. 해당 예시에서 speaker 2 만이 발화한 것으로 판단되었으므로 speaker 1의 컨텍스트 정보는 제2 비디오 데이터 만을 기초로 식별되고, speaker 2의 컨텍스트 정보는 제2 비디오 데이터 및 제2 오디오 데이터를 기초로 식별될 수 있다. 도 11c에 따르면, 제3 비디오 데이터 및/또는 제3 오디오 데이터에 기초하여 제3 컨텍스트 정보 가 획득될 수 있다. 해당 예시에서 speaker 1 만이 발화한 것으로 판단되었으므로 speaker 2의 컨텍스트 정보는 제3 컨텍스트 정보 만을 기초로 식별되고, speaker 1의 컨텍스트 정보는 제3 비디오 데이터및 제3 오디오 데이터를 기초로 식별될 수 있다. 도 11d에 따르면, 제4 비디오 데이터 및/또는 제4 오디오 데이터에 기초하여 제4 컨텍스트 정보 가 획득될 수 있다. 해당 예시에서 speaker 2 만이 발화한 것으로 판단되었으므로 speaker 1의 컨텍스트 정보는 제4 비디오 데이터 만을 기초로 식별되고, speaker 2의 컨텍스트 정보는 제4 비디오 데이터 및 제4 오디오 데이터를 기초로 식별될 수 있다. 상술한 실시 예에서는 speaker 1, speaker 2 가 동시 발화하는 경우는 도시하지 않았으나 speaker 1, speaker 2 가 동시 발화하는 경우에도 음성의 특성 파라미터를 기초로 오디오 데이터에서 speaker 1의 음성 및 speaker 2의 음성을 각각 식별하여 각 화자의 컨텍스트 정보를 식별할 수 있음은 물론이다. 도 12 내지 도 14는 일 실시 예에 따른 유즈 케이스를 설명하기 위한 도면들이다. 도 12에 도시된 바와 같은 일 예에 따르면, 화자 식별을 통해 화자의 음성 특징 및 감정이 반영된 오디오 트랙 을 생성할 수 있다. 이에 따라 자연스러운 오디오 트랙 생성이 가능하게 된다. 또한, 사용자는 화자의 특징 및/ 또는 배우의 음성을 정의하여 합성 오디오 트랙에서 특정 캐릭터 음성을 정의된 음성으로 발화되도록 구현할 수 있다. 도 13a에 도시된 바와 같은 일 예에 따르면, 시각 장애인은 실시간으로 화면에 표시되는 특정 세부 텍스트를 보 지 못할 수 있지만 오디오 트랙을 이용하면 이러한 특정 세부 텍스트 또한 인식할 수 있게 된다. 일 예에 따라 도 13a에 도시된 바와 같이 오리지널 오디오가 억제되거나 일시 중지된 상태에서 TTS를 사용하여 음성이 출력되 는 영역을 윤곽 표시하거나, 하이라이트 표시하는 모드를 제공할 수 있다. 도 13b에 도시된 바와 같은 일 예에 따르면, 시각 장애인은 TV 화면에 실시간으로 표시되지만 발음되지 않는 데 이터에 액세스할 수 없다. 하지만, 일 예에 따라 TTS를 이용하여 자막을 제외한 모든 텍스트를 인식하여 음성으 로 출력해주는 모드를 제공할 수 있다. 예를 들어, 원본 오디오를 억제하거나 일시 중지한 다음 일반 모드로 복 귀하여 TTS를 이용하여 자막을 제외한 모든 텍스트를 인식하여 음성으로 출력할 수 있다. 도 14에 도시된 바와 같은 일 예에 따르면, 컨텐츠에 부적절한 단어나 문구가 포함된 경우 이를 제거해야 할 수 있다(예를 들어, 자녀 보호 모드). 이 경우, 오디오 트랙 합성을 이용하여 불쾌한 단어나 구를 마스킹(필터링) 할 수 있다. 도 15는 일 실시 예에 따른 침해 적발 방법을 설명하기 위한 도면이다. 도 15에 따르면, 자막을 포함하는 비디오 컨텐츠가 디바이스로 스트리밍되는 경우(S1510), 자막 타입이 비쥬얼 자막인지 스트리밍 자막인지 식별할 수 있다(S1520). 여기서, 자막 언어는 기존의 오디오 트랙을 통해 제공되는 언어와 상이한 언어일 수 있다. 비쥬얼 자막이란 비디오 컨텐츠의 프레임에 고정되어 있는 자막이고 스트리밍 자막은 비디오 데이터와 별개로 스트리밍되는 자막일 수 있다. 자막 타입이 비쥬얼 자막인 경우, 자막의 검출 및 인식이 가능한지 식별할 수 있다(S1530). 자막의 검출 및 인식이 가능한 경우(S1530:True), 자막이 TTS에 의해 음성화 가능한지 식별할 수 있다(S1540). 한편, 자막 타입이 스트리밍 자막인 경우, 자막이 TTS에 의해 음성화 가능한지 식별할 수 있다(S1540). 자막이 TTS에 의해 음성화 가능한 경우(S1540:True), 음성의 특성 파라미터가 비디오 컨텐츠에 의존적인지 식별 할 수 있다(S1550). 음성의 특성 파라미터가 비디오 컨텐츠에 의존적인 경우(S1550:True), 침해가 성립된 것으로 볼 수 있다 (S1560). 이는 본 개시의 일 실시 예에 따라 비디오 컨텐츠의 컨텍스트를 반영하여 자막을 음성 처리한 것으로 볼 수 있기 때문이다. 한편, 비쥬얼 자막의 검출 및 인식이 불가능한 경우(S1530:False), 자막이 TTS에 의해 음성화 불가능한 경우 (S1540:False), 음성의 특성 파라미터가 비디오 컨텐츠에 비의존적인 경우(S1550:False) 중 적어도 하나의 경우 침해가 성립되지 않은 것으로 볼 수 있다. 이는 본 개시의 일 실시 예에 따라 비디오 컨텐츠의 컨텍스트를 반영 하여 자막을 음성 처리한 것으로 볼 수 없기 때문이다. 상술한 다양한 실시 예들에 따르면, 화자 식별을 통해 화자의 음성 특징 및 감정이 반영된 오디오 트랙을 생성 할 수 있다. 이에 따라 자연스러운 오디오 트랙 생성이 가능하므로 사용자의 시청 경험이 향상될 수 있다. 또한, 사용자는 화자의 특징 및/또는 배우의 음성을 정의하여 합성 오디오 트랙에서 특정 캐릭터 음성을 정의된음성으로 발화되도록 구현할 수 있다는 점에서 사용자의 시청 경험이 향상될 수 있다. 한편, 상술한 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 설치 가능한 어플리케이션 형태로 구현될 수 있다. 또는 상술한 본 개시의 다양한 실시 예들에 따른 방법들 중 적어도 일부는 딥 러닝 기반의 인공 지능 모델 즉, 학습 네트워크 모델을 이용하여 수행될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어 업그레이드, 또 는 하드웨어 업그레이드 만으로도 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들은 전자 장치에 구비된 임베디드 서버, 또는 전자 장치의 외부 서버를 통해 수행되는 것도 가능하다. 한편, 본 개시의 일시 예에 따르면, 이상에서 설명된 다양한 실시 예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine-readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실 시 예들에 따른 전자 장치(예: 전자 장치(A))를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세 서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 기기로 읽을 수 있는 저 장 매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적'은 저장매체가 신 호(signal)를 포함하지 않으며 실재(tangible)한다는 것을 의미할 뿐 데이터가 저장매체에 반영구적 또는 임시 적으로 저장됨을 구분하지 않는다. 또한, 본 개시의 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품 (computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있 다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2022-0119320", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2022-0119320", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시 예에 따른 전자 장치의 구현 예를 설명하기 위한 도면이다. 도 2a는 일 실시 예에 따른 전자 장치의 구성을 나타내는 블럭도이다. 도 2b는 본 개시의 일 실시 예에 따른 전자 장치의 일 구현 예의 세부 구성를 나타내는 도면이다. 도 3은 일 실시 예에 따른 오디오 트랙 데이터 획득 방법을 설명하기 위한 도면이다. 도 4는 일 실시 예에 따른 오디오 트랙 데이터 획득 방법을 설명하기 위한 도면들이다. 도 5는 도 4에 도시된 오디오 트랙 데이터 획득 방법을 자세히 설명하기 위한 도면이다. 도 6a 및 도 6b는 일 실시 예에 따라 화면을 캡쳐하여 자막 텍스트 데이터를 획득하는 방법을 설명하기 위한 도 면들이다. 도 7은 일 실시 예에 따른 오디오 트랙 획득 방법을 설명하기 위한 도면이다. 도 8은 일 실시 예에 따른 TTS 모듈의 구현 형태를 설명하기 위한 도면이다. 도 9는 일 실시 예에 따른 화자의 식별 데이터 획득 방법을 설명하기 위한 도면이다. 도 10은 일 실시 예에 따른 화자의 감정 데이터 획득 방법을 설명하기 위한 도면이다. 도 11a 내지 도 11d는 일 실시 예에 따른 화자의 컨텍스트 정보의 획득 예시를 설명하기 위한 도면들이다. 도 12 내지 도 14는 일 실시 예에 따른 유즈 케이스를 설명하기 위한 도면들이다. 도 15는 일 실시 예에 따른 침해 적발 방법을 설명하기 위한 도면이다."}
