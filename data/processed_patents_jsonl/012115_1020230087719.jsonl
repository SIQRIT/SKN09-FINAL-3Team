{"patent_id": "10-2023-0087719", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0007812", "출원번호": "10-2023-0087719", "발명의 명칭": "재배열 가능한 희소 다차원 데이터의 손실 압축 방법 및 장치", "출원인": "한국과학기술원", "발명자": "신기정"}}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 구동되는 손실 압축 장치로서, 입력 행렬이 자기-유사성을 갖도록 재배열하는 재배열부,재배열된 행렬을 재귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩하는 위치 인코딩부, 그리고일정 수의 매개변수들을 통해, 상기 타겟 성분의 위치 수열로부터 상기 타겟 성분의 값을 추론하도록 훈련된 근사 모델을 포함하는 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 재배열부는상기 입력 행렬을 구성하는 행들의 유사도 또는 열들의 유사도를 기초로 비슷한 두 행들 또는 두 열들을 선택하고, 선택한 한 쌍의 행들 또는 한 쌍의 열들을 가까이 위치시킴으로써 상기 재배열된 행렬을 생성하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,상기 위치 인코딩부는상기 재배열된 행렬을 부분 행렬들인 파티션들로 분해하고, 상기 타겟 성분이 위치한 파티션을 선택하고, 선택한 파티션을 분해하는 과정을 반복하면서, 분해 단계마다의 상기 타겟 성분의 위치를 순차적으로 인코딩하여 상기 위치 수열을 생성하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에서,상기 위치 인코딩부는상기 재배열된 행렬이 정사각형 행렬이라면, 상기 재배열된 행렬을 2x2 파티션으로 재귀적으로 분해하고, 상기 재배열된 행렬이 비정사각형 행렬이라면, 상기 재배열된 행렬을 2x2 파티션으로 재귀적으로 분해하다가1x2 파티션 또는 2x1 파티션으로 분해하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서,상기 근사 모델의 매개변수들은임의 성분의 위치 수열로부터 근사된 값과 실제 값의 근사 오차를 최소화하는 훈련을 통해 업데이트되는, 손실압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에서,상기 근사 모델은 공개특허 10-2025-0007812-3-상기 일정 수의 매개변수들을 통해, 상기 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들의 크로네커 곱을통해 상기 위치 수열에 해당하는 성분값을 추론하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에서,상기 근사 모델은순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델인, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에서,상기 입력 행렬은 희소 행렬인, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "적어도 하나의 프로세서에 의해 구동되는 손실 압축 장치의 동작 방법으로서,일정 수의 매개변수들을 통해, 입력 행렬에서 타겟 성분의 위치를 인코딩한 수열로부터 상기 타겟 성분의 값을추론하도록 근사 모델을 훈련시키는 단계, 그리고상기 일정 수의 매개변수들로 구성된 상기 근사 모델을 획득하는 단계를 포함하는, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에서,상기 근사 모델을 훈련시키는 단계는상기 입력 행렬이 자기-유사성을 갖도록 재배열하고, 재배열된 행렬을 재귀적으로 분해하여 타겟 성분의 위치를수열로 인코딩한 후, 상기 타겟 성분의 위치 수열로부터 근사된 값과 실제 값의 근사 오차가 최소화되도록 상기매개변수들을 업데이트하는 과정을 반복하는, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에서,상기 근사 모델을 훈련시키는 단계는상기 재배열된 행렬을 부분 행렬들인 파티션들로 분해하고, 상기 타겟 성분이 위치한 파티션을 선택하고, 선택한 파티션을 분해하는 과정을 반복하면서, 분해 단계마다의 상기 타겟 성분의 위치를 순차적으로 인코딩하여 상기 위치 수열을 생성하는, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에서,상기 재배열된 행렬이 정사각형 행렬이라면, 상기 재배열된 행렬은 2x2 파티션으로 재귀적으로 분해되고, 상기 재배열된 행렬이 비정사각형 행렬이라면, 상기 재배열된 행렬은 2x2 파티션으로 재귀적으로 분해되다가1x2 파티션 또는 2x1 파티션으로 분해되는, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에서,상기 근사 모델은 상기 일정 수의 매개변수들을 통해, 상기 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들의 크로네커 곱을공개특허 10-2025-0007812-4-통해 상기 위치 수열에 해당하는 성분값을 추론하는, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에서,상기 근사 모델은순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델인, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에서,상기 입력 행렬은 희소 행렬인, 동작 방법."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "명령어들을 실행함으로써, 입력 행렬 또는 입력 텐서가 자기-유사성을 갖도록 재배열하고, 재배열된 행렬 또는텐서를 재귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩하며, 상기 타겟 성분의 위치 수열로부터 근사된값과 실제 값의 근사 오차가 최소화되도록 근사 모델의 매개변수들을 업데이트하는 훈련을 반복하는 프로세서를 포함하는 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에서,상기 프로세서는 훈련된 상기 근사 모델 또는 상기 근사 모델을 구성하는 일정 수의 매개변수들을 상기 입력 행렬 또는 상기 입력 텐서의 압축 결과로 제공하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에서,상기 프로세서는 상기 입력 행렬 또는 상기 입력 텐서를 슬라이싱하고, 슬라이스들의 순서를 바꿔가면서 비슷한 슬라이스들이 가까워지도록 재배열하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에서,상기 근사 모델은 일정 수의 매개변수들을 통해, 입력 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들의 크로네커 곱을 통해상기 입력 위치 수열에 해당하는 성분값을 추론하는, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에서,상기 근사 모델은순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델인, 손실 압축 장치."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "적어도 하나의 프로세서에 의해 구동되는 손실 압축 장치로서, 입력 행렬이 자기-유사성을 갖도록 재배열하는 재 배열부, 재배열된 행렬을 재귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩하는 위치 인코딩부, 그리고 일 정 수의 매개변수들을 통해, 상기 타겟 성분의 위치 수열로부터 상기 타겟 성분의 값을 추론하도록 훈련된 근사 모델을 포함한다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 손실 압축 기술에 관한 것이다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "희소 행렬(sparse matrix)이란 0이 아닌 성분들(non-zero entries)의 수가 전체 성분들보다 훨씬 적은 행렬을 의미한다. 이러한 희소 행렬은 0이 아닌 성분이 희소하지만, 수십억 개의 행과 열로 이루어진 대규모 크기를 갖 기 때문에, 상당한 저장 공간이 필요하고, 대량의 트래픽을 발생시킨다. 따라서, 실세계 데이터로부터 얻어진 대규모 희소 행렬을 효율적으로 다루기 위해서는, 압축 기술이 필수적이다. 인공지능 모델의 훈련 및 응용에서 희소 행렬, 그리고 이를 일반화한 희소 다차원 데이터(텐서)를 다루기 위해 서 효율적인 압축 기술이 요구되고 있다. 최근에는 잠재적인 개인 정보 위험으로 인해 많은 양의 데이터를 클라 우드나 서버로 보내지 않고 모바일 장치에서 처리하도록 요구되는데, 메모리 제한적인 스마트폰과 IoT 장치에서 희소 행렬을 다루기 위해서도 압축 기술이 중요해지고 있다 지금까지도 다양한 손실 압축 기술이 연구되고 있지만, 기존의 손실 압축 기술은 0이 아닌 성분 수와 관계없이, 입력 행렬의 크기에 선형인 크기의 출력을 생성하기 때문에, 데이터 희소성을 충분히 활용하지 못하는 한계가 있다. 따라서, 기존의 손실 압축 기술보다 압축률을 높이면서도 근사 오차가 적은 새로운 손실 압축 기술이 요 구된다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 재배열 가능한 희소 다차원 데이터의 손실 압축 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 실시예에 따라 적어도 하나의 프로세서에 의해 구동되는 손실 압축 장치로서, 입력 행렬이 자기-유사성을 갖 도록 재배열하는 재배열부, 재배열된 행렬을 재귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩하는 위치 인코딩부, 그리고 일정 수의 매개변수들을 통해, 상기 타겟 성분의 위치 수열로부터 상기 타겟 성분의 값을 추 론하도록 훈련된 근사 모델을 포함한다. 상기 재배열부는 상기 입력 행렬을 구성하는 행들의 유사도 또는 열들의 유사도를 기초로 비슷한 두 행들 또는 두 열들을 선택하고, 선택한 한 쌍의 행들 또는 한 쌍의 열들을 가까이 위치시킴으로써 상기 재배열된 행렬을 생성할 수 있다. 상기 위치 인코딩부는 상기 재배열된 행렬을 부분 행렬들인 파티션들로 분해하고, 상기 타겟 성분이 위치한 파 티션을 선택하고, 선택한 파티션을 분해하는 과정을 반복하면서, 분해 단계마다의 상기 타겟 성분의 위치를 순 차적으로 인코딩하여 상기 위치 수열을 생성할 수 있다. 상기 위치 인코딩부는 상기 재배열된 행렬이 정사각형 행렬이라면, 상기 재배열된 행렬을 2x2 파티션으로 재귀 적으로 분해하고, 상기 재배열된 행렬이 비정사각형 행렬이라면, 상기 재배열된 행렬을 2x2 파티션으로 재귀적 으로 분해하다가 1x2 파티션 또는 2x1 파티션으로 분해할 수 있다. 상기 근사 모델의 매개변수들은 임의 성분의 위치 수열로부터 근사된 값과 실제 값의 근사 오차를 최소화하는 훈련을 통해 업데이트될 수 있다. 상기 근사 모델은 상기 일정 수의 매개변수들을 통해, 상기 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들 의 크로네커 곱을 통해 상기 위치 수열에 해당하는 성분값을 추론할 수 있다. 상기 근사 모델은 순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델일 수 있다. 상기 입력 행렬은 희소 행렬일 수 있다. 한 실시예에 따라 적어도 하나의 프로세서에 의해 구동되는 손실 압축 장치의 동작 방법으로서, 일정 수의 매개 변수들을 통해, 입력 행렬에서 타겟 성분의 위치를 인코딩한 수열로부터 상기 타겟 성분의 값을 추론하도록 근 사 모델을 훈련시키는 단계, 그리고 상기 일정 수의 매개변수들로 구성된 상기 근사 모델을 획득하는 단계를 포 함한다.상기 근사 모델을 훈련시키는 단계는 상기 입력 행렬이 자기-유사성을 갖도록 재배열하고, 재배열된 행렬을 재 귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩한 후, 상기 타겟 성분의 위치 수열로부터 근사된 값과 실 제 값의 근사 오차가 최소화되도록 상기 매개변수들을 업데이트하는 과정을 반복할 수 있다. 상기 근사 모델을 훈련시키는 단계는 상기 재배열된 행렬을 부분 행렬들인 파티션들로 분해하고, 상기 타겟 성 분이 위치한 파티션을 선택하고, 선택한 파티션을 분해하는 과정을 반복하면서, 분해 단계마다의 상기 타겟 성 분의 위치를 순차적으로 인코딩하여 상기 위치 수열을 생성할 수 있다. 상기 재배열된 행렬이 정사각형 행렬이라면, 상기 재배열된 행렬은 2x2 파티션으로 재귀적으로 분해되고, 상기 재배열된 행렬이 비정사각형 행렬이라면, 상기 재배열된 행렬은 2x2 파티션으로 재귀적으로 분해되다가 1x2 파 티션 또는 2x1 파티션으로 분해될 수 있다. 상기 근사 모델은 상기 일정 수의 매개변수들을 통해, 상기 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들 의 크로네커 곱을 통해 상기 위치 수열에 해당하는 성분값을 추론할 수 있다. 상기 근사 모델은 순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델일 수 있다. 상기 입력 행렬은 희소 행렬일 수 있다. 한 실시예에 따른 손실 압축 장치로서, 명령어들을 실행함으로써, 입력 행렬 또는 입력 텐서가 자기-유사성을 갖도록 재배열하고, 재배열된 행렬 또는 텐서를 재귀적으로 분해하여 타겟 성분의 위치를 수열로 인코딩하며, 상기 타겟 성분의 위치 수열로부터 근사된 값과 실제 값의 근사 오차가 최소화되도록 근사 모델의 매개변수들을 업데이트하는 훈련을 반복하는 프로세서를 포함한다. 상기 프로세서는 훈련된 상기 근사 모델 또는 상기 근사 모델을 구성하는 일정 수의 매개변수들을 상기 입력 행 렬 또는 상기 입력 텐서의 압축 결과로 제공할 수 있다. 상기 프로세서는 상기 입력 행렬 또는 상기 입력 텐서를 슬라이싱하고, 슬라이스들의 순서를 바꿔가면서 비슷한 슬라이스들이 가까워지도록 재배열할 수 있다. 상기 근사 모델은 일정 수의 매개변수들을 통해, 입력 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들의 크 로네커 곱을 통해 상기 입력 위치 수열에 해당하는 성분값을 추론할 수 있다. 상기 근사 모델은 순환신경망(recurrent neural network) 계열의 LSTM(Long Short Term Memory) 셀들로 구현된 모델일 수 있다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 다양한 크기의 희소한 행렬이나 텐서를 상수 크기(constant-size)로 압축할 수 있고, 결과적 으로 높은 압축률을 제공할 수 있으며, 압축 결과로부터 원본 데이터를 낮은 오차로 복원할 수 있다. 본 개시에 따르면, 0이 아닌 타겟 성분에만 접근하여 입력 행렬이나 텐서의 근사 오차를 빠르게 구할 수 있어서, 희소성을 활용하여 효율적인 계산을 할 수 있고, 선형 시간 훈련(linear-time training)을 할 수 있다. 본 개시에 따르면, 훈련된 근사 모델을 사용하여, 준선형 시간(sublinear time)에 입력 행렬이나 텐서의 각 성 분을 근사할 수 있다. 본 개시에 따르면, 재배열 불가능한 데이터라서 행과 열의 원래 위치를 저장하더라도, 기존 기술 대비 우수한 압축 성능을 제공할 수 있다."}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 개시의 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지 식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현 될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위 해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 설명에서, 도면 부호 및 이름은 설명의 편의를 위해 붙인 것으로서, 장치들이 반드시 도면 부호나 이름으로 한 정되는 것은 아니다. 설명에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재된 \"…부\", \"…기\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 설명에서, 단수로 기재된 표현은 \"하나\" 또는 \"단일\" 등의 명시적인 표현을 사용하지 않은 이상, 단수 또는 복 수로 해석될 수 있다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소를 설명하는데 사용될 수 있 지만, 구성요소는 이러한 용어에 의해 한정되지는 않는다. 이들 용어는 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로 사용될 수 있다. 도면을 참고하여 설명한 흐름도에서, 동작 순서는 변경될 수 있고, 여러 동작들이 병합되거나, 어느 동작이 분 해될 수 있고, 특정 동작은 수행되지 않을 수 있다. 도 1은 한 실시예에 따른 희소 데이터의 손실 압축 장치를 개념적으로 설명하는 도면이다. 도 1을 참고하면, 희소 행렬(sparse matrix)이란 0이 아닌 성분(non-zero entries)의 수가 전체 성분들보 다 훨씬 적은 행렬을 의미하는데, 전자상거래에서 사용자와 아이템의 관계, 소셜 네트워크에서의 친구 관계, 검 색 엔진에서 사용자와 클릭한 광고의 관계 등 다양한 도메인의 데이터가 희소 행렬로 저장되고 활용될 수 있다. 희소 행렬 압축은 많은 응용문제에 활용되고 있는데, 예를 들어, 전자상거래 시스템은 희소 행렬을 손실 압축한 뒤, 복원하는 과정을 통해, 각 구매자의 구매 이력을 확인할 수 있다. 실세계 데이터로 만들어진 희소 행렬은 수십억 개의 행과 열로 이루어지는 대규모 크기를 가지므로, 대규모 희 소 행렬을 효율적으로 다루기 위해서는, 압축 기술이 필수적이다. 예를 들어, 1억 명의 구매자와 1억 개의 상품 으로 구성된 전자상거래 구매 내역의 경우, 전체 구매자 수와 전체 상품 수의 곱에 해당하는 1경 개의 원소를 갖는 행렬이 만들어지기 때문이다. 기존의 손실 압축(lossy-compression) 기술에 따르면, 희소 행렬 압축이라고 하더라도, 입력 행렬의 크기에 선 형인 크기의 공간이 필요하기 때문에, 데이터 희소성을 충분히 활용하지 못한 한계가 있다. 최근에는 추천 시스 템이나 그래프 신경망 등 희소 행렬을 사용하는 인공지능 모델이 널리 연구되고 있는데, 인공지능 모델의 훈련 에서 주로 사용하는 GPU는 상대적으로 제한된 크기의 메모리를 갖는다. 이러한 메모리 제약 아래서 인공지능 모 델을 훈련시키기 위해서는 훈련 데이터의 압축이 필요하다. 또한, 최근에는 잠재적인 개인 정보 위험으로 인해 많은 양의 데이터를 클라우드나 서버로 보내지 않고 모바일 장치에서 처리하도록 요구되는데, 메모리 제한적인 스마트폰과 IoT 장치에서 희소 행렬을 다루기 위해서도 희소 행렬의 압축이 중요해지고 있다. 본 개시의 손실 압축 장치는 재배열 가능한(reorderable) 임의 크기의 희소 데이터를 상수 크기(constant- size)로 압축할 수 있다. 여기서, 희소 데이터는 2차원의 희소 행렬, 다층 그래프(multi-layer graphs)와 같은 텐서(Tensor)를 포함하고, 이외에도 재배열 가능한 고차원 데이터를 포함할 수 있다. 다음에서는 희소 데이터의 예로서 주로 희소 행렬을 사용하여 본 개시의 손실 압축 방법을 설명할 수 있는데, 이는 텐서를 비롯 한 고차원 데이터에도 적용될 수 있다. 실제로 희소 행렬은 행과 열이 임의로 재배열되어도 무방하다. 예를 들어, 전자상거래 데이터를 기반으로 구축 된 사용자-아이템 행렬의 경우, 어떤 사용자/아이템 옆에 어떤 사용자/아이템이 오는지는 중요하지 않다. 이렇 게 재배열 가능한 희소 행렬의 특징에 기반하여, 손실 압축 장치는 임의 크기의 희소 행렬이 자기-유사성 (self-similarity)을 가질 수 있도록 행과 열을 재배열할 수 있다. 여기서, 자기-유사성이란 대상의 일부분을 확대해 볼 때, 대상의 전체와 닮은 패턴이 나타나는 성질을 의미한다. 자기-유사성은 방대한 양의 데이터 속에 숨어 있기 때문에 찾아내기가 쉽지 않은데, 손실 압축 장치는 희 소 행렬의 행과 열의 순서를 재배열하여 숨어 있는 자기-유사성을 찾아낼 수 있다. 손실 압축 장치는 주어 진 행렬을 네 개의 부분 행렬로 분해하는 것을 재귀적(recursively)으로 반복함으로써, 타겟 성분이 각 분해 단 계에서 어느 부분 행렬에 놓이는지를 기반으로, 행렬의 각 성분을 수열(sequence)의 형태로 표현할 수 있다. 만 약 자기-유사성이 존재한다면, 비슷한 수열로 표현되는 성분끼리 유사한 값을 갖게 된다. 이렇게 성분마다 얻어 진 수열을 입력으로 하여 실제 성분의 값을 추론하는 근사 모델이 훈련된다. 근사 모델은 행렬의 자기-유사성을 기반으로 정확한 추론을 수행하고, 압축 결과로부터 원본 데이터를 낮은 오차로 복원(reconstruct)할 수 있다. 손실 압축 장치는 자기-유사성을 갖도록 재배열된 행렬을 재귀적으로 분해하는 과정을 통해, 행렬 성분의 위치를 수열로 인코딩할 수 있다. 여기서, 0이 아닌 성분을 타겟 성분이라고 부를 수 있고, 타겟 성분의 위치가 재귀적 분해를 통해 인코딩될 수 있다. 손실 압축 장치는 크로네커 곱(Kronecker product)을 일반화한 근사 모델 Θ를 포함하고, 일정 수의 매개 변수들(a constant number of parameters)로 구성된 근사 모델을 통해, 위치 수열에 해당하는 성분값을 복원할 수 있다. 도 2는 한 실시예에 따른 손실 압축 장치의 구성도이고, 도 3과 도 4 각각은 한 실시예에 따른 재배열을 설명하 는 도면이고, 도 5와 도 6 각각은 한 실시예에 따른 행렬 분해를 설명하는 도면이며, 도 7은 한 실시예에 따른 근사 모델의 동작을 설명하는 도면이다. 도 2를 참고하면, 손실 압축 장치는 적어도 하나의 프로세서에 의해 구동되는 컴퓨팅 장치로서, 프로세서 에 의해 실행되는 명령어들을 포함하는 컴퓨터 프로그램을 통해 본 개시의 손실 압축 방법을 제공할 수 있다. 손실 압축 장치는 재배열 가능한(reorderable) 임의 크기의 희소 데이터를 상수 크기(constant-size)로 압 축하는데, 희소 행렬을 예로 들어 설명한다. 다음에서 설명하는 방법은 텐서를 비롯한 재배열 가능한 고차원 데 이터의 압축에도 활용될 수 있다. 손실 압축 장치는 원본 행렬 A의 행 또는 열을 재배열하는 재배열부, 재배열된 행렬을 재귀적으로 분 해해서 성분의 위치를 수열로 인코딩하는 위치 인코딩부, 그리고 위치 수열로부터 성분값을 추론하는 근사 모델을 포함할 수 있다. 여기서, 재배열부, 위치 인코딩부, 그리고 근사 모델은 손실 압축 장치의 동작을 구분해서 설명하기 위해 나눈 것으로서, 이들이 반드시 구분되어 구현될 필요는 없고, 또한 이들이 물리적으로 구분된 단일 장치에 함께 구현될 필요는 없다. 근사 모델 또는 이를 포함하는 손실 압 축 장치를 뉴크론(NeuKron)이라고 부를 수 있다. 근사 모델은 원본 행렬을 압축하기 위해 크로네커 곱(Kronecker product)을 일반화한 모델로서, 일정 수 (최대 k개)의 매개변수들을 통해, 위치 수열로부터 행렬들을 생성하고, 생성된 행렬들의 크로네커 곱을 통해 위 치 수열에 해당하는 성분값을 추론할 수 있다. 여기서, 0이 아닌 성분을 타겟 성분이라고 부를 수 있고, 타겟 성분의 위치가 근사 모델로 입력되어, 타겟 성분의 값이 추론될 수 있다. 당연히, 0인 성분의 위치가 근사 모델로 입력되면 0에 가까운 값으로 추론될 수 있다. 근사 모델은 자동 회귀 시퀀스 모델(auto-regressive sequence model)로 구현되고, 예를 들면, 순환신경 망(recurrent neural network, RNN) 계열의 모델인 LSTM(Long Short Term Memory) 기반으로 구현될 수 있다. 근사 모델은 예를 들면, 일정 수의 매개변수들을 가지고 입력된 위치 수열을 개별적으로 처리하는 LSTM 셀 들로 구현될 수 있다. 행렬 A는, 근사 모델에서 생성된 행렬들의 크로네커 곱으로 근사될 수 있다. 행렬 A의 성분인 aij은 근사 모델에서 생성된 행렬들의 크로네커 곱의 (i,j)번째 성분으로 근사될 수 있다. 0이 아닌 원소 성분들의 위 치 수열로부터 해당 원소들의 근사값을 구하여 전체 성분에 대한 손실 함수가 계산될 수 있다. 여기서, 손실 함 수는 주어진 행렬 A와 근사 행렬(approximated matrix) 의 차이이고, 손실 함수로 계산한 주어진 행렬의 근 사 오차(approximation error)가 최소화되도록 행렬을 재배열하고 근사 모델의 매개변수들을 업데이트하는과정을 반복할 수 있다. 근사 모델은 데이터의 값이 모두 0일 때의 손실 함수를 하나의 변수 T로 표현할 수 있 다. 손실 함수는 T에서 타겟 성분들의 위치와 연관된 부분을 제외한 후, 타겟 성분들에 대해서만 계산된 실제 값과 근사값 사이의 오차를 더함으로써 계산될 수 있다. 위의 과정은 타겟 성분에만 접근하여 수행할 수 있으므 로, 근사 모델의 업데이트는 입력 행렬의 크기가 아니라, 0이 아닌 성분의 수에 비례하는 시간이 걸린다. 또한, 각 성분의 근사값은 대수 시간으로 검색될 수 있다. 이렇게 근사 모델에 의해 추론되는 근사 행렬은, 재귀적 분해로 인코딩된 위치 수열로부터 근사되므로, 원 본 행렬과 근사 행렬의 근사 오차는 원본 행렬의 자기-유사성이 높을수록 최소화된다. 따라서, 손실 압축 장치 는 행렬을 재배열하고 근사 오차를 계산하는 과정을 반복해서 일정 수의 매개변수들을 최적화할 수 있다. 먼저, 재배열부는 임의 크기의 희소 행렬이 자기-유사성을 가질 수 있도록 행과 열을 재배열한다. 재배열 방법은 다양할 수 있다. 예를 들면, 재배열부는 행들의 유사도와 열들의 유사도를 추정해서 유사한 행들/열들이 근처에 위치하도록 재배열한다. 행/열의 유사도는 0이 아닌 열/행 인덱스들의 Jaccard 유사도를 기초로 계산될 수 있고, 유사도를 빠르게 추정하기 위해 예를 들면, min-hashing 방법이 사용될 수 있다. 위치 재배열 방법은 다양할 수 있다. 예를 들면, 재배열부는 추정한 유사도에 따라 매칭된 한 쌍의 행들 또는 한 쌍의 열들을 샘플링하고, 샘플링된 한 쌍 중 하나의 행/열을 다른 행/열의 주변 행/열과 위치를 교환 (swap)하면서, 매칭된 한 쌍의 행들 또는 한 쌍의 열들이 가까워지도록 배치할 수 있다. 한편, 재배열부는 매칭되지 않고 남은 행들/열들을 무작위로 매칭해서 위에서 설명한 대로 쌍을 샘플링할 수 있다. 재배열부는 근사 오차가 최적화될 때까지 재배열 가능한 행렬의 행과 열을 반복적으로 정렬할 수 있다. 이 때, 재배열부는 행렬을 행과 열 단위로 슬라이싱하고, 슬라이스들(행 또는 열)의 순서를 바꿔가면서 비슷 한 슬라이스들이 가까워지도록 재배열함으로써, 자기-유사성을 가진 행렬을 만들 수 있다. 텐서의 경우에도, 재 배열부는 텐서를 차원마다 슬라이싱하고, 슬라이스들의 순서를 바꿔가면서 비슷한 슬라이스들이 가까워지 도록 재배열할 수 있다. 만약, 재배열을 통해 근사 오차가 늘어난다면, 재배열부는 샘플링된 쌍을 변경하 거나 추가적인 재배열을 진행하지 않을 수 있다. 즉, 재배열부는 근사 오차가 수렴할 때까지, 반복적으로 행렬을 재배열하고, 결과적으로 근사 모델의 복원 오류를 감소시킬 수 있다. 도 3을 참고하면, 재배열부는 주어진 행렬의 열들에서 행 인덱스들의 값이 비슷한 한 쌍(예를 들면, C3, C11)을 찾을 수 있다. 재배열부는 매칭된 C3과 C11을 근처에 배치하기 위해, C11 근처의 C10과 C3의 위치 를 교환함으로써, 0이 아닌 성분들이 근처에 배치되는 패턴으로 행렬을 정렬할 수 있다. 도 4를 참고하면, 행렬 A1은 사용자 행과 항목 열로 구성된 희소 행렬인데, 비슷한 행과 열을 근처에 배열하는 과정을 반복해서 행렬 A2가 만들어질 수 있다. 이때, 행렬 A2의 전체 패턴과 비슷한 패턴인 이 내부에서 반복되는데, 이를 자기-유사성이라고 부를 수 있다. 이렇게 자기-유사성을 가진 행렬 A2는 행렬 A3과 행렬 A4의 크로네커 곱(Kronecker product)으로 표현될 수 있다. 참고로, 크로네커 곱 는 수학식 1과 같이, 행렬 A 의 각 원소에 행렬 B를 곱해서 만들어지는 거대 행렬이다. 수학식 1"}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "위치 인코딩부는 입력 행렬을 재귀적으로 분해해서 성분의 위치를 수열로 인코딩할 수 있다. 위치 인코딩 부는 재배열된 행렬을 입력받을 수 있는데, 반드시 재배열 행렬만이 입력될 필요는 없다. 위치 인코딩을 설명하기 위해, 간단히, 로 표현되는 입력 행렬 A에서 N=M=2l이라고 가정한다. 만약, N=M이 2l을 만족하지 못하는 경우, 0으로 채워진 행과 열을 추가하는 제로 패딩(zero-padding)을 할 수 있다.위치 인코딩부는 행렬 A의 성분 aij에 대해 하향식으로 행렬 A를 재귀적으로 분해하여, 위치(i,j)를 길이 l= log2M의 수열로 인코딩한다. 즉, 성분의 위치(i,j)는 l개의 튜플(tuple)로 인코딩될 수 있다. 도 5를 참고하면, 위치 인코딩부는 행렬 A5를 부분 행렬들, 즉 2x2 파티션으로 분해하고,aij가 있는 파티션 을 선택한다. 2x2 파티션은 상단 왼쪽(Top Left, TL), 상단 오른쪽(Top Right, TR), 하단 왼쪽(Bottom Left, BL), 그리고 하단 오른쪽(Bottom Right, BR)의 4개의 파티션들로 구성될 수 있다. 상단 왼쪽은 (1, 1), 상단 오 른쪽은 (1, 2), 하단 왼쪽은 (2, 1), 그리고 하단 오른쪽은 (2, 2)로 인코딩될 수 있다. 수학식 2를 참고하면, 정사각형 행렬 , N=M=2l의 경우, 위치(i,j)는 행렬 분해를 통해 순차적으로 인코딩되는데, 위치 수 열의 k번째 튜플 (t(i,k), t(j,k))은 수학식 2와 같이 인코딩될 수 있다. 수학식 2"}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "위치 인코딩부는 aij만 남을 때까지, aij가 있는 파티션을 2x2 파티션으로 분해하고, aij가 있는 파티션을 선택하고 분해하는 과정을 반복한다. aij가 있는 파티션으로 선택된 위치들이 aij의 위치 수열로 인코딩된다. 예를 들어,l=3인 행렬 A5의 a3,4의 위치(3,4)를 인코딩한다면, a3,4는 행렬 A5의 상단 왼쪽(Top Left)에 있고, 상 단 왼쪽 파티션을 분해한 파티션들 중에서 하단 왼쪽(Bottom Left)에 있고, 하단 왼쪽 파티션을 분해한 파티션 들 중에서 상단 오른쪽(Top Right)에 위치한다. 따라서, a3,4는 3개의 튜플인 (1, 1), (2, 2), (1, 2)의 위치 수열로 인코딩될 수 있다. 마찬가지로, a5,1은 행렬 A5의 하단 왼쪽(Bottom Left)에 있고, 하단 왼쪽 파티션을 분해한 파티션들 중에서 상단 왼쪽(Top Left)에 있고, 상단 왼쪽 파티션을 분해한 파티션들 중에서 상단 왼쪽 (Top Left)에 위치한다. 따라서, a5,1의 위치(5,1)은 (2, 1), (1, 1), (1, 1)의 위치 수열로 인코딩될 수 있다. a8,5는 행렬 A5의 하단 오른쪽(Bottom Right)에 있고, 하단 오른쪽 파티션을 분해한 파티션들 중에서 하단 왼쪽 (Bottom Left)에 있고, 하단 왼쪽 파티션을 분해할 파티션들 중에서 하단 왼쪽(Bottom Left)에 위치한다. 따라 서, a8,5의 위치(8,5)는 (2, 2), (2, 1), (2, 1)의 위치 수열로 인코딩될 수 있다. 도 6을 참고하면, 행렬 분해는 도 5와 같은 정사각형 행렬이 아니어도 가능한다. 위치 인코딩부는 임의 크 기의 비정사각형 행렬 A6을 2x2 파티션으로 분해하고, aij가 있는 파티션을 선택하고 분해하는 과정을 반복한다. 이때, 비정사각 행렬이므로 마지막 파티션은 1x2 또는 2x1 파티션일 수 있다. 비정사각형 행렬 , 을 예로 들어 설명한다. 먼저, 비정사각형 행렬을 2x2 파티션으로 분해하기 위해, 0으로 채워진 행 또는 열을 추가해서 주어진 행렬을 로 확장한다. 확장 된 행렬의 행과 열의 크기가 다른 경우(예를 들면,lrow<lcol), 위치 인코딩부는 aij의 위치를 인코딩하기 위 해, 주어진 행렬을 2x2 파티션으로 lrow번 재귀적으로 분해하고, 의 크기를 가지는 파티션을 획득 한다. 그리고 위치 인코딩부는 의 크기를 같은 크기, 예를 들면, 1x2 파티션으로 lrow - lcol번 재귀적으로 분해할 수 있다. 1x2 파티션은 왼쪽(Left)과 오른쪽(Right)으로 분해될 수 있다. 위치(i,j)는 행렬 분해를 통해 순차적으로 인코딩되는데, k번째 튜플 (trow(i,k), (tcol(j,k))은 수학식 3과 같이 인코딩될 수 있다.수학식 3"}
{"patent_id": "10-2023-0087719", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "예를 들어, 비정사각형 행렬 A6의 a2,3의 위치(2,3)를 인코딩한다면, a2,3은 행렬 A6의 상단 왼쪽(Top Left)에 있 고, 상단 왼쪽 파티션을 분해한 파티션들 중에서 하단 오른쪽(Bottom Right)에 있고, 하단 오른쪽을 분해한 1x2 파티션 중에서 왼쪽(Left)에 위치한다. 따라서, a2,3은 (1,1), (2,2), (0,1)의 위치 수열로 인코딩될 수 있다. 여기서, 1x2 파티션 중에서 왼쪽(Left)의 위치는 (0,1)로 정의될 수 있다. 마찬가지로, 비정사각형 행렬 A6의 a4,6의 위치(4,6)은 행렬 A6의 하단 오른쪽(Bottom Right)에 있고, 하단 오른쪽 파티션을 분해한 파티션들 중에 서 하단 왼쪽(Bottom Left)에 있고, 하단 왼쪽을 분해한 1x2 파티션 중에서 오른쪽(Right)에 위치한다. 따라서, a4,6은 (2,2), (2,1), (1,0)의 위치 수열로 인코딩될 수 있다. 여기서, 1x2 파티션 중에서 오른쪽(Right)의 위치 는 (1,0)으로 정의될 수 있다. 도 7을 참고하면, 근사 모델은 타겟 성분(target entry)의 위치 수열을 입력받고, 일정 수의 매개변수들을 통해, 타겟 성분의 값을 추론하는데 사용되는 행렬들(K1, K2, …)을 생성한다. 여기서, 주어진 행렬의 근사 행렬 은 근사 모델에 의해 생성된 행렬들의 일반화된 크로네커 곱으로 간주될 수 있다. 예를 들어, 타겟 성분 a3,4의 위치를 인코딩한 수열 (1, 1), (2, 2)이 근사 모델로 입력되면, 일정 수의 매개변수들을 통해 생성 된 행렬들 K1, K2, K3이 출력된다. K1, K2, K3의 크로네커 곱으로 만들어진 근사 행렬 의 (3,4) 성분 값이 타겟 성분 a3,4의 근사값으로 복원될 수 있다. 여기서, 근사값을 얻기 위해 전체 크로네커 곱이 계산될 필요는 없고, 각 LSTM 셀에 의해 생성된 행렬에서 입력 튜플에 대응하는 성분들, 즉, K1(1,1), K2(2,2), K3(1,2)의 곱으로 a3,4 의 값이 근사될 수 있다. 한편, a3,4의 위치를 인코딩한 수열 중 마지막 튜플 (1, 2)는 근사 모델에 입력되지 않 고, 크로네커 곱을 통해 값을 복원할 때 사용될 수 있다. 다만, 도 7의 근사 모델은 하나의 예시이고, 근 사 모델은 일정 수의 매개변수들을 통해, 타겟 성분의 위치 수열로부터 성분값을 복원할 수 있는 다양한 형태의 인공지능 모델로 구현될 수 있다. 이렇게 근사 모델에 의해 추정된 근사값이 원래 성분값에 가까워지도록, 행렬을 재배열하고 근사 오차를 계산하여 경사 하강법을 사용해 이를 최소화하는 과정을 반복해서 일정 수의 매개변수들이 업데이트된다. 예를 들어, , N=M=2l인 행렬을 압축한다고 가정한다. 근사 모델의 각 LSTM 셀은 타겟 성분 aij 의 위치(i,j)가 주어지면, , …, 를 생성할 수 있다. 근사 모델의 내부 구성은 다 양하게 설계될 수 있다. 예를 들면, k번째 LSTM 셀은 위치 수열 중 k번째 튜플을 입력받고, k번째 LSTM 셀의 숨 겨진 상태(hidden state)는 선형 레이어(linear layer)와 활성화(예를 들면, Softplus 활성화)를 거쳐 Kk+1을 생성하는 데 기여할 수 있다. 여기서, K1의 각 성분들은 별도의 학습 가능한 매개변수일 수 있다. 타겟 성분 의 근사값 은 수학식 4와 같이 출력 행렬들의 크로네커 곱의 (i,j)번째 성분으로부터 계산될 수 있다. 수학식 4 수학식 4에서, 는 크로네커 곱의 (i,j)번째 성분이고, q는 학습 가능한 매개변수일 수 있다. 한편, 비정사각형 행렬의 경우, 일부 LMTM 셀에서 크기가 1x2인 행렬을 생성하도록 근사 모델이 변 형될 수 있다. 도 8은 한 실시예에 따른 텐서 압축을 설명하는 도면이다. 도 8을 참고하면, 손실 압축 장치(100A)는 희소 행렬과 비슷한 방법으로, 희소하고 재배열 가능한 텐서를 압축할 수 있다. D-차원의 텐서, 를 가정하면, 위치 인코딩부(130A)는 먼저 제로 패딩을 통해 텐서를 크 기로 확장한다. 위치 인코딩부(130A)는 위치 인코딩을 위해, 확장된 텐서를 2D 파티션들로 l1번 재귀적으로 분해 하여, 크기를 가지는 파티션을 획득한다. 위치 인코딩부(130A)는 (D-1)-차원의 텐서를 재귀적으로 분해하는 것을 반복해서, 결과적으로 D-차원의 위치 (i1,...,iD)를 인코딩한 위치 수열을 획득할 수 있다. 일정 수의 매개변수들을 가진 근사 모델(150A)은 위치 수열을 입력받고, 각 LSTM 셀의 매개변수들에 의해 생성 된 행렬들을 생성할 수 있다. 생성된 행렬들(K1, K2, …)의 크로네커 곱으로 타겟 성분의 값이 근사화될 수 있고, 이를 위해 일정 크기의 매개변수들이 최적화될 수 있다. 손실 압축 장치(100A) 역시, 근사 오차가 최소가 되도록 D-차원의 텐서를 재배열하고, 위치를 인코딩하며, 타겟 성분을 근사하는 과정을 반복할 수 있다. 텐서 압축을 위한 손실 압축 장치(100A)의 훈련은 행렬 압축과 유사한데, 슁글(shingle) 계산에 차이가 있을 수 있다. D개의 임의 전단사 함수들(bijective functions)이 사용되므로, 각 모드 인덱스는 동일한 모드의 함수를 제외하고 해당 함수에서 D-1개의 슁글을 갖는다. i1의 D-1 슁글과 인덱스 j의 슁글이 모두 같은 경우에만 위치 i1과 i2 쌍이 서로 가까이 위치하도록 한다. 텐서의 초기 인덱스는 무작위로 재배열한다. 도 9는 한 실시예에 따른 손실 압축 방법의 흐름도이다. 도 9를 참고하면, 손실 압축 장치는 주어진 행렬을 초기화한다(S110). 손실 압축 장치는 공동 클러스 터링 알고리즘(co-clustering algorithm)을 사용하여 주어진 행렬을 초기화할 수 있다. 손실 압축 장치는 행들의 유사도 또는 열들의 유사도를 추정해서 비슷한 두 행들 또는 두 열들을 선택한다 (S120). 손실 압축 장치는 행렬 내 위치 교환을 통해, 선택한 한 쌍의 행들 또는 한 쌍의 열들을 가까이 위치시킴 으로써 행렬을 재배열한다(S130). 손실 압축 장치는 행과 열을 재배열함으로써 주어진 행렬이 자기-유사성 을 갖도록 만든다. 손실 압축 장치는 재배열된 행렬을 재귀적으로 분해하고, 각 분해 단계에서 0이 아닌 성분들의 위치를 인 코딩한다(S140). 손실 압축 장치는 입력 행렬을 제로-패딩해서 확장한 후, 재귀적으로 분해할 수 있다. 손실 압축 장치는 0이 아닌 성분들의 위치 수열로부터 해당 성분들의 근사값을 구하여 전체 성분들에 대한 손실 함수(근사 오차)가 최소화되도록 근사 모델의 매개변수들을 업데이트하면서 근사 모델을 훈련시킨다 (S150). 손실 압축 장치는 해당 원소들의 근사값과 실제 값의 근사 오차를 계산하고, 근사 오차가 최소화 되도록 근사 모델의 매개변수들을 경사하강법으로 훈련시킬 수 있다. 손실 압축 장치는 근사 오차가 수렴하는지 판단하고(S160), 근사 오차가 수렴할 때까지 현재 행렬을 재배 열하는 과정을 반복한다. 손실 압축 장치는 매개변수들의 훈련과 행렬 재배열을 번갈아 수행할 수 있다. 손실 압축 장치는 근사 오차가 수렴하면 근사 모델의 훈련을 종료하고, 일정 수의 매개변수들로 구성 된 근사 모델을 획득한다(S170). 훈련된 근사 모델 또는 이를 구성하는 일정 수의 매개변수들이 희소 행렬 또는 희소 텐서의 압축 결과로 제공될 수 있다. 주어진 행렬의 성분값은 근사 모델을 통해, 복원될 수 있다. 근사 모델의 훈련 동안에는 타겟 성분들의 위치를 인코딩한 수열들과 타겟 성분들의 실제값이필요해서 희소 행렬의 0이 아닌 성분 수에 비례하는 공간이 필요하지만, 결과적으로 임의 크기의 희소 행렬은 매개변수들의 수(최대 k개)에 해당하는 상수 크기로 압축될 수 있다. 도 10은 한 실시예에 따른 컴퓨팅 장치의 구성도이다. 도 10을 참고하면, 손실 압축 장치는 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치로 구현될 수 있다. 컴퓨팅 장치는 하나 이상의 프로세서, 프로세서에 의하여 수행되는 컴퓨터 프로그램을 로드하는 메모리, 컴퓨터 프로그램 및 각종 데이터를 저장하는 저장 장치, 통신 인터페이스, 그리고 이들 을 연결하는 버스를 포함할 수 있다. 이외에도, 컴퓨팅 장치는 다양한 구성 요소가 더 포함될 수 있 다. 프로그램은 메모리에 로드될 때 프로세서로 하여금 본 개시의 다양한 실시예에 따른 방법/동작을 수행하도록 하는 명령어들(instruction)을 포함할 수 있다. 즉, 프로세서는 명령어들을 실행함으로써, 본 개시의 다양한 실시예에 따른 방법/동작들을 수행할 수 있다. 명령어는 기능을 기준으로 묶인 일련의 컴퓨터 판 독가능 명령어들로서 컴퓨터 프로그램의 구성 요소이자 프로세서에 의해 실행되는 것을 가리킨다. 프로세서는 컴퓨팅 장치의 각 구성의 전반적인 동작을 제어한다. 프로세서는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 또는 본 개시의 기술 분야에 잘 알려진 임의의 형태의 프로세서 중 적어도 하나를 포함하여 구성될 수 있다. 또 한, 프로세서는 본 개시의 다양한 실시예들에 따른 방법/동작을 실행하기 위한 적어도 하나의 컴퓨터 프로 그램에 대한 연산을 수행할 수 있다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 개시의 다양한 실시예들에 따른 방법/동작을 실행하기 위하여 저장 장치로부터 하나 이상의 프로그램을 로드할 수 있다. 메모리는 RAM과 같은 휘발성 메모리로 구현될 수 있을 것이나, 본 개시의 기술적 범위는 이에 한정되지 않는다. 저장 장치는 프로그램을 비임시적으로 저장할 수 있다. 저장 장치는 ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리 등과 같은 비휘발성 메모리, 하드 디스크, 착탈형 디스크, 또는 본 개시가 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터로 읽을 수 있는 기록 매체를 포함하여 구성될 수 있다. 통신 인터페이스는 컴퓨팅 장치의 유무선 통신을 지원한다. 이를 위해, 통신 인터페이스는 본 개시의 기술 분야에 잘 알려진 통신 모듈을 포함하여 구성될 수 있다. 버스는 컴퓨팅 장치의 구성 요소 간 통신 기능을 제공한 다. 버스는 주소 버스(Address Bus), 데이터 버스(Data Bus) 및 제어 버스(Control Bus) 등 다양한 형태의 버스로 구현될 수 있다. 컴퓨터 프로그램은 프로세서에 의해 실행되는 명령어들을 포함하고, 프로세서는 명령어들을 실행함으 로써, 주어진 행렬에서 행들의 유사도 또는 열들의 유사도를 추정해서 비슷한 두 행들 또는 두 열들을 선택하고, 행렬 내 위치 교환을 통해, 선택한 한 쌍의 행들 또는 한 쌍의 열들을 가까이 위치시킴으로써 행렬을 재배열한 후, 재배열된 행렬을 재귀적으로 분해하여 각 타겟 성분의 위치를 인코딩할 수 있다. 또한, 프로세서 는 명령어들을 실행함으로써, 타겟 성분의 위치 수열로부터 근사된 행렬의 근사 오차가 최소화되록 근사 모델의 매개변수들을 업데이트하고, 근사 오차가 수렴할 때까지 현재 행렬을 재배열하고 근사 모델을 업데이트 하는 과정을 반복할 수 있다. 프로세서는 최종적으로 업데이트된 매개변수들로 구성된 근사 모델을 획득하 고, 근사 모델을 통해 주어진 행렬의 성분값을 복원할 수 있다. 이와 같이, 손실 압축 장치는 다양한 크기의 행렬이나 텐서를 상수 크기(constant-size)로 압축을 할 수 있고, 결과적으로 비슷한 근사 오차를 가진 기존 기술보다 50배 이상 우수한 압축률을 제공하고, 비슷한 크기의 출력을 제공하는 기존 기술보다 10배 이상 작은 근사 오차를 제공할 수 있다. 본 개시에 따르면, 희소 행렬에 해당하는 2억 건의 비디오 시청 내역을 10KB 크기로 성공적으로 압축할 수 있고, 기존 기술을 이용해 1GB로 압 축한 것보다도 압축으로 인한 정보 손실이 적다. 손실 압축 장치는 0이 아닌 타겟 성분에만 접근하여 입력 행렬에 대한 손실함수를 계산할 수 있어서, 입력 행렬의 희소성을 활용하여 효율적인 계산을 할 수 있고, 선형 시간 훈련(linear-time training)을 할 수 있다. 손실 압축 장치의 훈련 동안에는 입력 행렬의 0이 아닌 성분 수에 비례하는 공간이 필요하지만, 결과적으 로 상수 크기의 압축을 제공할 수 있다.손실 압축 장치는 훈련된 근사 모델을 사용하여, 준선형 시간(sublinear time)에 입력 행렬의 각 성 분을 근사할 수 있다. 한편, 손실 압축 장치는 입력을 재배열해서 근사 오차를 줄일 수 있는데, 재배열 불가능한 데이터라서 행 과 열의 원래 위치를 저장하더라도 기존 기술 대비 우수한 압축 성능을 제공할 수 있다. 이상에서 설명한 본 개시의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 개시의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 개시의 실시예에 대하여 상세하게 설명하였지만 본 개시의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 개시의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 개시의 권리범위에 속하는 것이다."}
{"patent_id": "10-2023-0087719", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 희소 데이터의 손실 압축 장치를 개념적으로 설명하는 도면이다. 도 2는 한 실시예에 따른 손실 압축 장치의 구성도이다. 도 3과 도 4 각각은 한 실시예에 따른 재배열을 설명하는 도면이다. 도 5와 도 6 각각은 한 실시예에 따른 행렬 분해를 설명하는 도면이다. 도 7은 한 실시예에 따른 근사 모델의 동작을 설명하는 도면이다.도 8은 한 실시예에 따른 텐서 압축을 설명하는 도면이다. 도 9는 한 실시예에 따른 손실 압축 방법의 흐름도이다. 도 10은 한 실시예에 따른 컴퓨팅 장치의 구성도이다."}
