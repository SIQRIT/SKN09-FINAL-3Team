{"patent_id": "10-2021-0074262", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0165526", "출원번호": "10-2021-0074262", "발명의 명칭": "종단간 음성 합성 장치, 프로그램 및 그것의 제어 방법", "출원인": "주식회사 카카오엔터프라이즈", "발명자": "김재현"}}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 학습 데이터의 특징을 추출하여 제 1 표현형을 생성하는 후방 인코더(posterior encoder);상기 생성된 제 1 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성하는 디코더; 및상기 생성된 음성 파형과 상기 음성 학습 데이터 간에 손실인 제 1 손실(loss)을 계산하고, 상기 계산된 제 1손실을 최소화하는 방식으로 상기 후방 인코더 및 상기 디코더 중 적어도 하나를 학습시키는 학습부를포함하되,상기 음성 학습 데이터는 텍스트 학습 데이터와 매칭되도록 저장되어 지도 학습 데이터(supervised trainingdata)를 형성하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 텍스트 학습 데이터를 인코딩하는 텍스트 인코더;상기 인코딩된 텍스트로부터 제 2 표현형을 생성하는 프로젝션부; 및상기 제 1 및 제 2 표현형에 기초하여, 상기 텍스트 학습 데이터에 포함되어 있는 각 음소별 길이 정렬 데이터를 획득하는 정렬 데이터 획득부를 포함하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,텍스트에 포함되어 있는 각 음소별 길이를 예측하는 음소 길이 예측부를 더 포함하고,상기 학습부는, 상기 획득한 길이 정렬 데이터에 기초하여 상기 음소 길이 예측부를 학습시키는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 제 1 표현형의 사전 분포를 변환시키는 분포 변환부를 더 포함하고,상기 정렬 데이터 획득부는, 상기 변환된 제 1 표현형에 기초하여 상기 길이 정렬 데이터를 획득하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서, 상기 분포 변환부는,공개특허 10-2022-0165526-3-상기 제 1 표현형에 흐름 정규화(Normalizing Flow) 함수 를 적용하여 상기 사전 분포를 변환하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,음성 합성하고자 하는 텍스트인 타겟 텍스트가 입력되면, 상기 텍스트 인코더는 상기 타겟 텍스트를인코딩하고,상기 음소 길이 예측부는 상기 인코딩한 타겟 텍스트에 포함되어 있는 각 음소별 길이 를 예측하며,상기 프로젝션부는, 상기 인코딩한 타겟 텍스트로부터 제 2 표현형을 생성하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 생성된 제 2 표현형에 상기 예측된 음소별 길이 를 적용하는 음소 길이 적용부를 더 포함하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서, 상기 분포 변환부는,상기 예측된 음소별 길이가 적용된 제 2 표현형의 사전 분포에 함수 를 적용하여 분포를 변환하되,상기 함수 는 상기 함수 의 역함수인,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 디코더는, 상기 변환된 제 2 표현형의 사전 분포에 기초하여, 상기 타겟 텍스트에 대한 음성 파형을 생성하는,음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 2 항에 있어서, 상기 프로젝션부는,상기 인코딩된 텍스트로부터 제 2 표현형의 평균값 및 표준편차값을 산출하고, 상기 산출된 평균값 및 표준편차값에 기초하여 상기 제 2 표현형을 생성하는,공개특허 10-2022-0165526-4-음성 합성 장치."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "후방 인코더가 음성 학습 데이터의 특징을 추출하여 제 1 표현형을 생성하는 단계;디코더가 상기 생성된 제 1 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성하는 단계;및학습부가 상기 생성된 음성 파형과 상기 음성 학습 데이터 간에 손실인 제 1 손실(loss)을 계산하고, 상기 계산된 제 1 손실을 최소화하는 방식으로 상기 후방 인코더 및 상기 디코더 중 적어도 하나를 학습시키는 단계를 포함하되,상기 음성 학습 데이터는 텍스트 학습 데이터와 매칭되도록 저장되어 지도 학습 데이터(supervised trainingdata)를 형성하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,텍스트 인코더가 상기 텍스트 학습 데이터를 인코딩하는 단계;프로젝션부가 상기 인코딩된 텍스트로부터 제 2 표현형을 생성하는 단계; 및정렬 데이터 획득부가 상기 제 1 및 제 2 표현형에 기초하여, 상기 텍스트 학습 데이터에 포함되어 있는 각 음소별 길이 정렬 데이터를 획득하는 단계를 포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,음소 길이 예측부가 텍스트에 포함되어 있는 각 음소별 길이를 예측하는 단계; 및상기 학습부가 상기 획득한 길이 정렬 데이터에 기초하여 상기 음소 길이 예측부를 학습시키는 단계를 더 포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,분포 변환부가 상기 제 1 표현형의 사전 분포를 변환시키는 단계; 및상기 정렬 데이터 획득부가 상기 변환된 제 1 표현형에 기초하여 상기 길이 정렬 데이터를 획득하는 단계를 더포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 사전 분포를 변환시키는 단계는,공개특허 10-2022-0165526-5-상기 분포 변환부가 상기 제 1 표현형에 흐름 정규화(Normalizing Flow) 함수 를 적용하여 상기 사전 분포를 변환하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 텍스트 인코더가 음성 합성하고자 하는 텍스트인 타겟 텍스트가 입력되면, 상기 타겟 텍스트를 인코딩하는단계;상기 음소 길이 예측부가 상기 인코딩한 타겟 텍스트에 포함되어 있는 각 음소별 길이 를 예측하는 단계; 및상기 프로젝션부가 상기 인코딩한 타겟 텍스트에 대한 제 2 표현형을 생성하는 단계를 더 포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서,음소 길이 적용부가 상기 생성된 제 2 표현형의 타겟 텍스트에 상기 예측된 음소별 길이 를 적용하는 단계를더 포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 분포 변환부가 상기 예측된 음소별 길이 가 적용된 타겟 텍스트의 사전 분포에 함수 를 적용하여 분포를 변환하는 단계를 더 포함하되,상기 함수 는 상기 함수 의 역함수인,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서,상기 디코더가 상기 변환된 제 3 표현형의 사전 분포에 기초하여, 상기 타겟 텍스트에 대한 음성 파형을 생성하는 단계를 더 포함하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 12 항에 있어서, 상기 제 2 표현형을 생성하는 단계는,상기 프로젝션부가 상기 인코딩된 텍스트로부터 제 2 표현형의 평균값 및 표준편차값을 산출하고, 상기 산출된공개특허 10-2022-0165526-6-평균값 및 표준편차값에 기초하여 상기 제 2 표현형을 생성하는,음성 합성 장치의 제어 방법."}
{"patent_id": "10-2021-0074262", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "하드웨어와 결합되어 제 11 항 내지 제 20 항 중 어느 하나의 항의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터 프로그램."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 텍스트를 음성으로 변환하는 종단간 음성 합성 장치 및 그것의 제어 방법에 관한 것이다. 보다 구체 적으로 본 발명은, 음성 학습 데이터의 특징을 추출하여 제 1 표현형을 생성하는 후방 인코더; 상기 생성된 제 1 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성하는 디코더; 및 상기 생성된 음성 파형과 상기 음성 학습 데이터 간에 손실인 제 1 손실(loss)을 계산하고, 상기 계산된 제 1 손실을 최소화하는 방식으로 상기 후방 인코더 및 상기 디코더 중 적어도 하나를 학습시키는 학습부를 포함하되, 상기 음성 학습 데이터는 텍 스트 학습 데이터와 매칭되도록 저장되어 지도 학습 데이터(supervised training data)를 형성하는, 음성 합성 장치에 관한 것이다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음성 합성 기술에 관한 것으로, 보다 구체적으로는 인공지능 기술에 기초하여 사람의 목소리와 유사 도가 높으면서도 다양한 리듬이나 특징을 반영하여 음성을 합성할 수 있는 음성 합성 기술에 관한 것이다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 음성 합성 기술의 발전됨에 따라, 각종 음성 안내, 교육 분야 등에 널리 사용되고 있다. 음성 합성은 사람 이 말하는 소리와 유사한 소리를 생성해내는 기술로 흔히 TTS(Text To Speech) 시스템으로도 알려져 있다. 음성 합성 기술은 사용자에게 정보를 텍스트나 그림이 아닌 음성 신호로 전달함으로써 운전 중이거나, 맹인인 경우처 럼 사용자가 작동하는 기계의 화면을 볼 수 없는 경우에 매우 유용하다. 근래에 들어, 스마트폰, 전자 책 리더, 차량 네비게이션 등 개인 휴대용 장치와 더불어 인공지능 스피커, 스마트 TV, 스마트 냉장고 등과 같이 스마트 홈에서 스마트 가정용 장치의 개발과 보급이 활발하게 이루어짐으로써 음성 출력을 위한 음성 합성 기술 및 장 치의 필요성도 급속도로 증가하였다. 도 1은 기존의 음성 합성 기술을 적용한 기존 음성 합성 장치의 음파 생성 개념도를 도시한다. 기존 음성 합성 장치는, 스펙트로그램 변환부 및 음파 생성부를 포함하도록 구성된다. 스펙트로그램 변환부 는 입력으로 주어진 텍스트를 스펙트로그램(spectrogram)으로 변환한다. 스펙트로그램이란 소리나 파동을 시각화하여 파악하기 위한 도구로, 파형(waveform)과 스펙트럼(spectrum)의 특징이 조합되어 있다. 음파 생성부는 텍스트가 변환된 스펙트로그램에 기초하여 음파(waveform)을 생성한다. 기존 음성 합성 기술에서는 이와 같이 입력되는 텍스트에 기초하여 스펙트로그램을 생성하는 1단계 및 스펙트로 그램에 기초하여 음파를 생성하는 2단계로 구분된다. 각 단계는 서로 다른 연구로 진행될 만큼 효율적이지 못하 다는 문제점이 존재한다. 따라서, 주어진 텍스트 데이터를 효과적으로 음파로 생성할 수 있는 음성 합성 기술 및 이를 이용하는 장치에 대한 연구가 요구되는 실정이다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는 인공지능 기술에 기초한 종단간 음성 합성 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 다른 과제는 중간 데이터인 스펙트로그램을 거치지 않고 입력 텍스트를 음성으로 변 환할 수 있는 종단간 음성 합성 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 또 다른 과제는 다양한 리듬과 다양한 목소리의 특징을 반영할 수 있는 음성 합성 장치를 제공하는 것이다. 본 발명에서 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급하지 않은"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "또 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하 게 이해될 수 있을 것이다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 또는 다른 과제를 해결하기 위해 본 발명의 일 측면에 따르면, 음성 학습 데이터의 특징을 추출하여 제 1 표현형을 생성하는 후방 인코더(posterior encoder); 상기 생성된 제 1 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성하는 디코더; 및 상기 생성된 음성 파형과 상기 음성 학습 데이터 간에 손실인 제 1 손실(loss)을 계산하고, 상기 계산된 제 1 손실을 최소화하는 방식으로 상기 후방 인코더 및 상기 디코더 중 적어도 하나를 학습시키는 학습부를 포함하되, 상기 음성 학습 데이터는 텍스트 학습 데이터와 매칭되도록 저장되어 지도 학습 데이터(supervised training data)를 형성하는, 음성 합성 장치를 제공한다. 상기 텍스트 학습 데이터를 인코딩하는 텍스트 인코더; 상기 인코딩된 텍스트로부터 제 2 표현형을 생성하는 프 로젝션부; 및 상기 제 1 및 제 2 표현형에 기초하여, 상기 텍스트 학습 데이터에 포함되어 있는 각 음소별 길이 정렬 데이터를 획득하는 정렬 데이터 획득부를 포함할 수 있다. 텍스트에 포함되어 있는 각 음소별 길이를 예측하는 음소 길이 예측부를 더 포함하고, 상기 학습부는, 상기 획 득한 길이 정렬 데이터에 기초하여 상기 음소 길이 예측부를 학습시킬 수 있다. 상기 제 1 표현형의 사전 분포를 변환시키는 분포 변환부를 더 포함하고, 상기 정렬 데이터 획득부는, 상기 변 환된 제 1 표현형에 기초하여 상기 길이 정렬 데이터를 획득할 수 있다. 상기 분포 변환부는, 상기 제 1 표현형에 흐름 정규화(Normalizing Flow) 함수 를 적용하여 상기 사전 분 포를 변환할 수 있다. 음성 합성하고자 하는 텍스트인 타겟 텍스트가 입력되면, 상기 텍스트 인코더는 상기 타겟 텍스트를 인코딩하고, 상기 음소 길이 예측부는 상기 인코딩한 타겟 텍스트에 포함되어 있는 각 음소별 길이 를 예측하 며, 상기 프로젝션부는, 상기 인코딩한 타겟 텍스트로부터 제 2 표현형을 생성할 수 있다. 상기 생성된 제 2 표현형에 상기 예측된 음소별 길이 를 적용하는 음소 길이 적용부를 더 포함할 수 있다. 상기 분포 변환부는, 상기 예측된 음소별 길이가 적용된 제 2 표현형의 사전 분포에 함수 를 적용하여 분포를 변환하되, 상기 함수 는 상기 함수 의 역함수일 수 있다. 상기 디코더는, 상기 변환된 제 2 표현형의 사전 분포에 기초하여, 상기 타겟 텍스트에 대한 음성 파형을 생성 할 수 있다. 상기 프로젝션부는, 상기 인코딩된 텍스트로부터 제 2 표현형의 평균값 및 표준편차값을 산출하고, 상기 산출된 평균값 및 표준편차값에 기초하여 상기 제 2 표현형을 생성할 수 있다. 상기 또는 다른 과제를 해결하기 위해 본 발명의 다른 측면에 따르면, 후방 인코더가 음성 학습 데이터의 특징 을 추출하여 제 1 표현형을 생성하는 단계; 디코더가 상기 생성된 제 1 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성하는 단계; 및 학습부가 상기 생성된 음성 파형과 상기 음성 학습 데이터 간에 손실인 제 1 손실(loss)을 계산하고, 상기 계산된 제 1 손실을 최소화하는 방식으로 상기 후방 인 코더 및 상기 디코더 중 적어도 하나를 학습시키는 단계를 포함하되, 상기 음성 학습 데이터는 텍스트 학습 데 이터와 매칭되도록 저장되어 지도 학습 데이터(supervised training data)를 형성하는, 음성 합성 장치의 제어 방법을 제공한다. 텍스트 인코더가 상기 텍스트 학습 데이터를 인코딩하는 단계; 프로젝션부가 상기 인코딩된 텍스트로부터 제 2 표현형을 생성하는 단계; 및 정렬 데이터 획득부가 상기 제 1 및 제 2 표현형에 기초하여, 상기 텍스트 학습 데 이터에 포함되어 있는 각 음소별 길이 정렬 데이터를 획득하는 단계를 포함할 수 있다. 음소 길이 예측부가 텍스트에 포함되어 있는 각 음소별 길이를 예측하는 단계; 및 상기 학습부가 상기 획득한 길이 정렬 데이터에 기초하여 상기 음소 길이 예측부를 학습시키는 단계를 더 포함할 수 있다. 분포 변환부가 상기 제 1 표현형의 사전 분포를 변환시키는 단계; 및 상기 정렬 데이터 획득부가 상기 변환된 제 1 표현형에 기초하여 상기 길이 정렬 데이터를 획득하는 단계를 더 포함할 수 있다. 상기 사전 분포를 변환시키는 단계는, 상기 분포 변환부가 상기 제 1 표현형에 흐름 정규화(Normalizing Flow) 함수 를 적용하여 상기 사전 분포를 변환할 수 있다. 상기 텍스트 인코더가 음성 합성하고자 하는 텍스트인 타겟 텍스트가 입력되면, 상기 타겟 텍스트를 인코딩하는 단계; 상기 음소 길이 예측부가 상기 인코딩한 타겟 텍스트에 포함되어 있는 각 음소별 길이 를 예측하는 단계; 및 상기 프로젝션부가 상기 인코딩한 타겟 텍스트로부터 제 2 표현형을 생성하는 단계를 더 포함할 수 있 다. 음소 길이 적용부가 상기 생성된 제 2 표현형에 상기 예측된 음소별 길이 를 적용하는 단계를 더 포함할 수 있 다. 상기 분포 변환부가 상기 예측된 음소별 길이가 적용된 제 2 표현형의 사전 분포에 함수 를 적용하여 분 포를 변환하는 단계를 더 포함하되, 상기 함수 는 상기 함수 의 역함수일 수 있다. 상기 디코더가 상기 변환된 제 2 표현형의 사전 분포에 기초하여, 상기 타겟 텍스트에 대한 음성 파형을 생성하 는 단계를 더 포함할 수 있다. 상기 제 2 표현형을 생성하는 단계는, 상기 프로젝션부가 상기 인코딩된 텍스트로부터 제 2 표현형의 평균값 및 표준편차값을 산출하고, 상기 산출된 평균값 및 표준편차값에 기초하여 상기 제 2 표현형을 생성할 수 있다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 음성 합성 장치 및 그것의 제어 방법의 효과에 대해 설명하면 다음과 같다. 본 발명의 실시 예들 중 적어도 하나에 의하면, 기존 음성 합성 기술 보다 더 효율적인 음성 합성 기술을 제공 할 수 있다는 장점이 있다. 또한, 본 발명의 실시 예들 중 적어도 하나에 의하면, 텍스트에 기초하여 음성 파형을 생성하는데 있어서, 다양 한 음높이와 리듬을 갖는 여러 방식의 음성을 합성할 수 있다는 장점이 있다. 본 발명의 적용 가능성의 추가적인 범위는 이하의 상세한 설명으로부터 명백해질 것이다. 그러나 본 발명의 사 상 및 범위 내에서 다양한 변경 및 수정은 당업자에게 명확하게 이해될 수 있으므로, 상세한 설명 및 본 발명의 바람직한 실시 예와 같은 특정 실시 예는 단지 예시로 주어진 것으로 이해되어야 한다."}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 기존의 음성 합성(Text-to-Speech) 기술은 여러 구성 요소를 통해 주어진 텍스트에서 음성 파형을 합성한다. 심 층 신경망의 급속한 발전으로 음성 합성 기술의 파이프라인은 텍스트 정규화 및 음소화와 같은 텍스트 전처리와 별도로 2단계 생성 모델링으로 단순화된다. 첫 번째 단계는 사전 처리된 텍스트에서 멜 스펙트로그램 또는 언어 적 특징과 같은 중간 음성 표현을 생성하는 것이고, 두 번째 단계는 중간 표현을 조건으로 하는 파형을 생성하 는 것이다. 기존의 신경망 기반 자동회귀 음성 합성 기술은 사실적인 음성 합성 기능을 보여주었지만 순차적 생성 프로세스 로 인해 최신 병렬 프로세서를 완전히 활용하기 어렵다는 문제점이 존재한다. 즉, 순차적인 프로세스로 인하여 어느 하나의 프로세스가 끝나기 전에는 이후 프로세스가 대기할 수 밖에 없기 때문에, 프로세스 활용 면에서 효 율적이지 못하다는 것이다. 이에 따라 본 발명에서는 현재의 2단계 모델보다 더 자연스러운 사운드를 생성하는 병렬 종단 간 음성 합성 방 법을 제시한다. 본 발명의 일실시예에서는 VAE(variational autoencoder)를 사용하여 기존 음성 합성 기술에서 의 두 모듈을 잠재 변수(latent variable)로 구성되는 중간 표현형(representation)을 통해 연결하여 효율적인 종단 간 학습이 가능하도록 제안한다. 또한 본 발명의 일실시예에서는 고품질 음성 파형을 합성할 수 있도록 방법의 표현력을 향상시키기 위해 조건부 사전 분포(conditional prior distribution)에 흐름 정규화(Normalizing flow)를 적용하고, 파형 도메인에 적 대적 훈련을 적용하도록 제안한다. 실제와 가까운 오디오를 생성하는 것 외에도 음성 합성 기술은 텍스트 입력이 다양한 변형(예: 음높이 및 지속 시간)으로 여러 방식으로 말해질 수 있는 일대다 관계를 표현하는 것이 중요하다. 본 발명의 일실시예에서는, 일대다 관계를 표현하기 위해 입력 텍스트에서 다양한 리듬으로 음성을 합성하는 음소 길이 예측부도 제안한다. 잠재 변수에 대한 불확실성 모델링과 음소 길이 예측부를 사용하여 본 발명의 일실시예에서는 텍스트로는 표현 할 수 없는 음성 변화를 반영하여 음성 파형을 생성할 수 있다. 이하 도면을 참조하여, 보다 구체적으로 설명한 다.도 2는 본 발명의 일실시예에 따른 음성 합성 장치의 블록도를 도시한다. 본 발명의 일실시예에 따른 음성 합성 장치는, 후방 인코더, 디코더, 변환기, 텍스트 인코 더, 학습부, 프로젝션부, 분포 변환부, 정렬 데이터 획득부, 음소 길이 예측부 및 음소 길이 적용부를 포함하도록 구성될 수 있다. 도 2에 도시된 구성요소들은 음성 합성 장치를 구현하는데 있어서 필수적인 것은 아니어서, 본 명세서 상에서 설명되는 음성 합성 장치는 위에서 열거된 구성요소들 보다 많거나, 또는 적은 구성요소들을 가질 수 있다. 후방 인코더(201, posterior encoder)는 음성 데이터가 입력되면, 입력되는 오디오 신호를 인코딩한다. 인코딩 이란, 기존 데이터에서 특징을 추출하고, 기존 데이터 보다 데이터의 양 또는 차원을 감소시킨 데이터로 변환하 는 것을 의미한다. 즉, 인코딩을 통하여 출력된 결과는, 입력된 데이터를 압축한 결과일 수 있다. 본 발명의 일실시예에 따른 음성 합성 장치는 VAE(variational autoencoder)을 사용할 수 있다. 이때 후 방 인코더는 VAE의 인코더일 수 있다. 특히, 본 발명의 일실시예에 따른 후방 인코더는, wav, mp3 등의 오디오 파일 포맷 형태로 입력될 수도 있 지만, 멜 스펙트로그램(mel-spectrogram)이나 선형 스케일 스펙트로그램(linear-scale spectrogram) 형태로 오 디오 데이터가 입력될 수 있다. 본 발명의 일실시예에 따른 후방 인코더는 심층 신경망으로 구성될 수 있다. 후방 인코더를 통하여 변환된 데이터는 잠재 변수(latent variable)들로 구성된다. 본 발명의 일실시예에 따른 후방 인코더의 출력 결과인 잠재 변수(들)의 구성을 제 1 표현형(representation)이라 부른다. 즉, 음성 데이터를 변환시킨 표현형을 제 1 표현형이라 부른다. 본 발명의 일실시예에 따른 제 1 표현형을 구성하는 잠재 변수(들)는, 입력된 오디오 데이터에서 다소 중요도가 낮은 특징들은 제거되고, 중요한 특징들만을 담은 데이터를 주로 포함할 것이다. 본 발명의 일실시예에 따른 표현형(들)은 확률 분포(probability distribution) 형태를 가질 수 있다. 본 발명의 일실시예에 따른 후방 인코더는, 'WaveGlow' 및 'Glow-TTS'에서 사용된 'non-causal WaveNet residual block'을 사용한다. 'non-causal WaveNet residual block'은 게이트 활성화 장치와 스킵 연결이 있는 확장 컨볼루션 레이어로 구성된다. 블록 위의 선형 투영 레이어는 정규 사후 분포의 평균과 분산을 생성한다. 다중 화자의 경우에 본 발명의 일실시예에서는 화자 임베딩을 추가하기 위해 'residual block'에서 글로벌 조건 화(global conditioning)를 사용할 수 있다. 디코더는 표현형에 기초하여 음성 파형을 생성한다. 본 발명의 일실시예에 따른 디코더는 표현형의 사전 분포(prior distribution)에 기초하여 음성 파형을 생성할 수 있다. 본 발명의 일실시예에 따른 디코더 는 심층 신경망으로 구성될 수 있다. 본 발명의 일실시예에 따라 음성 합성 장치가 VAE를 이용할 경우 디코더는 VAE의 디코더일 수 있다. 본 발명의 일실시예에 따른 디코더는 'HiFi-GAN V1 생성기'를 사용하도록 제안한다. 이것은 전치된 컨볼루 션 스택(stack of transposed convolutions)으로 구성되며, 각 컨볼루션 뒤에는 다중 수신 필드 융합 모듈 (MRF, multi-receptive field fusion module)이 올 수 있다. MRF의 출력은 수용 필드 크기가 다른 'residual block'의 출력 합이다. 다중 화자 설정의 경우 화자 임베딩을 변환하는 선형 레이어를 추가하고 입력 잠재 변수 z에 추가한다. 변환기는 음성 파형을 멜 스펙트로그램으로 변환한다. 예를 들어, 변환기는 상기 디코더를 통하 여 생성된 음성 파형을, 멜 스펙트로그램으로 변환할 수 있다.텍스트 인코더는, 입력된 텍스트 Ctext를 인코딩한다. 예를 들어 텍스트 인코더는 텍스트 학습 데이터 또는 타겟 텍스트(음성 합성하고자 하는 텍스트)가 입력되면, 입력된 텍스트를 인코딩하여 텍스트 표현형을 출 력한다. 텍스트 표현형이란, 입력된 텍스트의 주요한 특징을 포함하는 숨겨진 형태의 데이터 표현형(hidden representation)을 말한다. 본 발명의 일실시예에 따른 텍스트 인코더는, 음소(phoneme) 단위로 인코딩을 수행할 수 있다. 이를 위하 여 텍스트 인코더는, 텍스트 데이터에 발음 데이터(발음 기호 등)를 더 포함하는 입력을 수신할 수 있다. 본 발명의 일실시예에 따른 텍스트 인코더는 절대 위치 인코딩(absolute positional encoding) 대신 상대 위치 표현(relative positional representation)을 사용하는 변환기 인코더를 사용한다. 학습부는 학습 데이터(training data)에 기초하여 상기 인코더 및 디코더를 학습시킨다. 본 발 명의 일실시예에 따른 학습 데이터는, 음성 학습 데이터와 텍스트 학습 데이터가 매칭되어 저장되는 형태로 구 성될 수 있다. VAE를 이용하는 본 발명의 일실시예에 따르면, 학습부는 아래 수학식 1에 기초하여 학습을 수행한다. 수학 식 1은 의 한계 로그 가능도(intractable marginal log-likelihood)에 대한 ELBO(evidence lower bound)를 산출하기 위한 수식이다. 학습부는 ELBO를 최대화시키는 방향으로 학습을 수행할 수 있다. 수학식 1"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 는 주어진 컨디션 c에 대해 잠재 변수 z에 대한 사전 분포(prior distribution)를 의미 하며, 는 데이터 포인트 x에 대한 가능도 함수(likelyhood function)를 의미한다. 는 사후 분포(posterior distribution)에 근사한 분포를 의미한다. 이 경우 훈련 손실(training loss)은, ELBO에 음수를 취하여 재건 손실인 와 KL divergence인 두 개의 합으로 볼 수 있을 것이다( 일 때). 프로젝션부는 텍스트 표현형을 제 2 표현형으로 변환시킨다. 본 발명의 일실시예에 따른 프로젝션부 는 텍스트 표현형으로부터 제 2 표현형의 평균값 및 표준편차값 를 산출하고, 상기 산출된 평균값 및 표준편차값 에 기초하여 상기 제 2 표현형을 생성할 수 있다. 즉, 텍스트 데이터를 변환시킨 표현 형을 제 2 표현형으로 부른다. 분포 변환부는, 제 1 표현형의 사전 분포를 다른 분포로 변환시킨다. 사전 분포의 표현력을 향상시키기 위 함이다. 표현력의 향상은 실제적인 음성 합성에 있어서 중요하다. 본 발명의 일실시예에 따른 분포 변환부는 제 1 표현형에 흐름 정규화(Normalizing Flow) 함수 를 적용하여 상기 사전 분포를 제 3 표현형으 로 변환할 수 있다. 즉, 이하에서 제 3 표현형은 제 1 표현형이 분포 변환부에 의해서 변환된 결과를 의미 할 수 있다. 단순한 형태의 사전 분포는 분포 변환부를 통하여 복잡한 형태의 분포로 변환된다. 이러한 변 환은 가역적이기 때문에, 분포 변환부는 이하 함수 에 대한 역함수 를 더 활용할 수 있다. 정렬 데이터 획득부는, 텍스트를 변환시킨 제 2 표현형와 음성 데이터를 변환 시킨 제 3 표현형을 비교하 여, 각 음소별 길이를 정렬하기 위한 길이 정렬 데이터를 획득한다. 획득된 길이 정렬 데이터는 음소 길이 예측 부를 학습시키는데 사용된다. 음소 길이 예측부는 소정 텍스트가 입력되면, 해당 텍스트에 포함되어 있는 각 음소별 길이를 예측한다. 본 발명의 일실시예에 따른 음소 길이 예측부는 최대 가능성 추정(maximum likelihood estimation)을 통 해 훈련되는 흐름 기반 생성 모델이다. 음소 길이 적용부는 제 2 표현형에 상기 예측된 음소별 길이를 적용한다. 즉, 음소 길이 적용부는 예 측된 음소별 길이에 맞도록 상기 제 2 표현형에 포함되어 있는 각 음소별 길이를 조절한다. 도 3은 본 발명의 일실시예에 따른 음성 합성 장치의 학습 개념도를 도시하는 도면이다. 도 4는 본 발명의 일실시예에 따른 음성 합성 장치의 학습 순서도를 도시하는 도면이다. 이하, 도 3 및 도 4를 함께 참조하 여 설명한다. 상술한 바와 같이, 음성 합성 장치의 구성 중 후방 인코더, 디코더 및 음소 길이 예측부 중 적어도 하나는 심층 신경망으로 구현될 수 있다. 심층 신경망으로 구현되는 구성은, 학습부에 의해서 학습이 수행될 수 있다. 이하에서는, 학습부에 의해서 심층 신경망으로 구현되는 구성을 학습하기 위한 절 차에 대해서 설명한다. S401단계에서 후방 인코더는, 음성 학습 데이터를 인코딩한다. 인코딩된 음성 학습 데이터는 잠재 변수 z 로 구성되는 제 1 표현형이라 부른다. 본 발명의 일실시예에 따른 후방 인코더로 입력되는 음성 학습 데이 터는, 오디오 파일 포맷이나 멜 스펙트로그램이 아닌, 선형 스케일 스펙트로그램인 이 입력되도록 제안한 다. 왜냐하면, 후방 인코더에 더 높은 해상도의 음성 학습 데이터가 제공되기를 원하기 때문이다. 이와 같 이 선형 스케일의 스펙트로그램이 입력으로 들어가더라도, VAE 동작의 제한 요건(properties of variational inference)을 위반하지 않을 것이다. S402 단계에서 디코더는, 잠재 변수 z로 구성되는 제 1 표현형에 기초하여 음성 파형 를 생성한다. 즉, 음성 데이터의 특징이 추출되어 제 1 표현형 z로 생성되고, 추출된 특징에 기초하여 음성 파형을 다시 생성하는 것이다. 본 발명의 일실시예에 따른 재건 손실(reconstruction loss)의 산출은, 멜 스펙트로그램 간의 차이에 기초하여 산출하도록 제안한다. 인간 청각 시스템의 응답을 근사하는 멜 스케일을 사용하여 지각 품질을 향상시키기 위함 이다. 파형의 멜 스펙트로그램 추정(변환)에는 STFT 및 멜 스케일에 대한 선형 투영만 사용하므로 학습 가능한 파라미터가 불필요하다. 이를 위해서 변환기는 S403 단계에서 생성된 음성 파형 을 멜 스펙트로그램 형태의 출력인 로 변 환한다.S404 단계에서 학습부는, 제 1 내지 제 3 손실을 계산한다. 먼저 제 1 손실은, 재건 손실로서, 아래 수학식으로 표현될 수 있다. 수학식 2"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "은 후방 인코더로 입력된 음성 학습 데이터의 멜 스펙트로그램이다. 학습부가 제 1 손실을 고려하여 학습을 수행한다면, 후방 인코더와 디코더를 순차적으로 거친 결과인 과 원래 음성 학습 데이터에 대한 가 최대한 비슷해 지는 방향으로 학습이 이루어질 수 있을 것이다. 제 1 손실은 데이터 분포에 대한 라플라스 분포를 가정하고 상수 항을 무시하는 최대 우도 추정(MLE, maximum likelihood estimation)으로 볼 수 있다. 또한 최대 우도 추정은 추론이 아닌 훈련 중에만 사용된다. 본 발명의 일실시예에서는, 전체 잠재 변수 z를 업샘플링하지 않고, 잠재 변수 z의 일부인 부분 시퀀스를 디코 더의 입력으로 사용하도록 제안한다. 효율적인 종단 간 훈련에 사용되는 창 생성기 훈련(windowed generator training)을 위해서이다. 본 발명의 일실시예에 따른 순서도는 S401 단계 이후, S411 단계로도 진행된다. S411 단계에서 분포 변환부는, 잠재 변수 z로 구성되는 제 1 표현형의 사전 분포를 다른 분포(예를 들면 더 복잡한 분포)로 변환시킨다. 본 발명의 일실시예에 따른 분포 변환부는 제 1 표현형에 흐름 정규화 (Normalizing Flow) 함수 를 적용하여 상기 사전 분포를 변환할 수 있다. 아래 수학식 3은 흐름 정규화 (Normalizing Flow) 함수 를 적용한 확률 분포 함수 를 나타내는 수식이다. 수학식 3"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "본 발명의 일실시예에 따른 순서도는 S401 단계와 함께, S421 단계를 수행할 수 있다. S421 단계에서 텍스트 인코더는, 입력된 텍스트 Ctext를 인코딩하여 텍스트 표현형 htext를 생성한다. 이때 음소 단위로 인코딩 수행이 가능하다는 것은 상술한 바와 동일하다. 이어서 S422 단계에서 프로젝션부는 텍스트 표현형 htext를 제 2 표현형으로 변환시킨다. 본 발명의 일실시 예에 따른 프로젝션부는 텍스트 표현형으로부터 제 2 표현형의 평균값 및 표준편차값 를 산출하고, 상기 산출된 평균값 및 표준편차값 에 기초하여 상기 제 2 표현형을 생성할 수 있다. 본 발명의 일실시예에서는, 입력 텍스트에 포함되어 있는 음소별 간의 정렬 데이터인 행렬 A를 추정하기 위해 흐름 정규화 함수 에 의해 매개변수화된 데이터의 가능성을 최대화하는 정렬을 검색하는 방법인 MAS(Monotonic Alignment Search)를 적용하도록 제안한다. MAS는 아래 수학식 4로 표현된다. S413 단계에서 정렬 데이터 획득부는 후술하는 MAS 방법에 기초하여, 정렬 데이터인 행렬 A를 획득한다. 여기서 행렬 A는 차원을 가지는 행렬이며, 각 음소의 길이를 조정하기 위한 정렬 정보로, 각 음소가 얼마나 짧거나 긴지에 대한 정보일 수 있다. 정렬에 대해서는 정답 레이블이 존재하지 않기 때문에, 반복적으로 각 음소의 길이를 바꿔가면서 가장 최적의 길이를 추정해 나가야 한다. 수학식 4"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이때 후보 정렬은 사람의 음성이 단어를 건너뛰지 않고 순서대로 텍스트를 읽는다는 사실에 기초하여, 단조롭고 건너뛰지 않는 것으로 제한될 것이다. 최적의 정렬을 찾기 위해 동적 프로그래밍을 사용할 수 있다. 한편, 학습의 최종 목적은 정확한 로그 가능성이 아니라 ELBO를 최대화 시키는 것이기 때문에 MAS를 직접 적용 하는 것은 어렵다. 따라서 본 발명의 일실시예에서는 ELBO를 최대화하는 정렬을 찾기 위해 MAS를 아래 수학식 5 에서와 같이 재정의(redefine) 하도록 제안한다. 수학식 5에서는 잠재 변수 z의 로그 가능성을 최대화하는 정렬 을 찾는 것으로 축소할 수 있다. 수학식 5"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "상술한 수학식 4 및 5를 조합할 경우, 기존 MAS에 대한 수정이 필요 없이 바로 적용하여 정렬 정보인 행렬 A를 획득할 수 있을 것이다.이하 S405 단계에서 학습부는 제 2 내지 제 4 손실을 계산한다. 학습 데이터 중 음성 학습 데이터를 변환한 제 1 표현형과 텍스트 학습 데이터를 변환한 제 2 표현형은 서로 가 까워지는 방향으로 학습되어야 할 것이다. 이를 반영하기 위하여 본 발명의 일실시예에 따른 학습부는 제 2 손실인 Lkl을 산출할 수 있다. 본 발명의 일실시예에 따른 제 2 손실인 Lkl은 KL-발산(Divergence)를 이용하여 제 1 및 제 2 표현형과의 차이 를 반영 할 수 있다. 텍스트에서 추출한 음소 ctext와 음소와 잠재 변수 간의 정렬 정보인 행렬 A에 대해서 KL- 발산은 아래 수학식 6으로 표현될 수 있다. 수학식 6"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "학습부는 S406 단계에서 상기 제 2 손실인 Lkl이 최소화되는 방향으로 학습을 수행할 수 있다. 이하 제 3 손실인 Ldur에 대해서 설명한다. 정렬 데이터 획득부에 의해서 추정된 행렬 A의 의 각 행에 있는 모든 열을 합산하여 각 입력 토큰 di의 지속 시간을 계산할 수 있을 것이다. 각 토큰의 지속 시간은 이전 작업에서 제안된 바와 같이 결정론적 지 속 시간 예측자(deterministic duration predictor)를 훈련하는 데 사용할 수 있지만 매번 다른 말하기 속도로 사람이 말하는 방식을 표현할 수는 없다. 본 발명의 일실시예에서는 인간과 유사한 음성 리듬을 생성하기 위해 샘플이 주어진 음소의 지속 시간 분포 (duration distribution)를 따르도록 음소 길이 예측부를 제안한다. 음소 길이 예측부는 일반적으로 최대 우도 추정을 통해 훈련되는 흐름 기반 생성 모델(flow-based generative model)이다. 그러나 각 입력 음소의 지속 시간이 1) 연속적인 흐름 정규화를 사용하기 위해 역양자화되어야 하는 이산 정수 이어야 하고, 2) 가역성(invertibility)으로 인하여 고차원 변환을 방지하는 스칼라이어야 하므로 최대 우도 추 정을 직접 적용하는 것은 어렵다. 본 발명의 일실시예에서는, 상술한 문제를 해결하기 위해 'Variational Dequantization' 및 'Variational Data Augmentation'을 적용하도록 제안한다. 구체적으로, 'Variational Dequatization' 및 'Variational Data Augmentation'에 대해 각각 지속시간 시퀀스 d와 동일한 시간 분해능 및 차원을 갖는 두 개의 랜덤 변수 u 및 ν를 도입한다. 그리고, 차이 d-u가 양의 실수 시퀀스가 되도록 u의 지원을 [0,1)으로 제한하고 ν와 d를 채널 별로 연결하여 더 높은 차원의 잠재 표현을 만든다. 근사 사후 분포 를 통해 두 변수를 샘 플링한다. 결과 목적은 음소 기간의 로그 가능도(log-likelihood)의 변동 하한(variational lower bound)이다. 아래 수학식 7은 시퀀스 d에 대한 로그 가능도에 대한 변동 하한을 산출하기 위한 수식이다. 수학식 7"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "제 3 손실인 Ldur는 시퀀스 d의 로그 가능도에 대한 변동 하한이다. 본 발명의 일실시예에서는, 입력 기울기의 역전파(back-propagating)를 방지하는 정지 그래디언트 연산자(stop gradient operator)를 입력 조건에 적용하 여 기간 예측자의 훈련이 다른 모듈의 훈련에 영향을 미치지 않도록 설정한다. 샘플링 절차는 비교적 간단하게 수행될 수 있다. 음소 지속 시간은 확률적 음소 길이 예측부의 역변환을 통해 무작위 노이즈에서 샘플링된 다음 정수로 변환될 수 있다. 제 4 손실인 와 제 5 손실인 는, 적대적 훈련(Adversarial Training)에 기초한 손실이다. 학습부는 적대적 훈련을 채택하기 위해 디코더(G)에서 생성된 출력인 음성 파형 와 정답 파형"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "를 구별하는 판별자(D, discriminator)를 포함한다. 적대적 훈련에서는 음성 합성에 성공적으로 적용된 두 가지 유형의 손실인 제 4 및 제 5 손실을 사용하도록 제안한다. 두 가지 유형의 손실은 적대적 훈련을 위한 최소 제 곱 손실 함수(least-squares loss function) 및 디코더(202, 적대적 훈련에서의 Generator)의 훈련을 위한 특 징 매칭 손실(feature-matching loss)을 포함한다. 제 4 손실인 와 제 5 손실인 는 아래 수학식 8에 기초하여 산출될 수 있다. 수학식 8"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서 T는 판별자의 총 레이어 수를 나타내고 은 Nl개의 피쳐를 갖는 판별자의 l번째 레이어의 피쳐 맵을 출력한다. 특히, 특징 매칭 손실은 VAE의 요소별 재구성 손실에 대한 대안으로 제안된 판별기의 은닉층에서 측 정된 재구성 손실로 볼 수 있을 것이다. 본 발명의 일실시예에 따른 판별자는, HiFi-GAN(Kong et al., 2020)에서 제안한 다주기 판별자의 판별자 아키텍 처를 따르도록 제안한다. 다중 주기 판별기는 Markovian 윈도우 기반 하위 판별기(Kumar et al., 2019)의 혼합 이며, 각각은 입력 파형의 서로 다른 주기 패턴에서 작동할 수 있다. 한편, 상술한 제 1 내지 제 5 손실에 기초하여 수행되는 학습은, 서로 독립적으로 수행될 수도 있지만, 하나의 손실 함수에 함께 반영되어 동시에 학습에 반영될 수 있을 것이다. 상술한 제 1 내지 제 5 손실을 모두 반영한 전체 손실은 아래 수학식 9로 표현될 수 있다. 수학식 9"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "따라서 S406 단계에서 학습부는 상기 전체 손실인 를 최소화시키는 방향으로 학습을 수행할 수 있다. 이하 도 5 및 도 6을 참조하여, 추론(inference) 과정에 대해서 설명한다. 도 5는 본 발명의 일실시예에 따른 음성 합성 장치의 추론 개념도를 도시하는 도면이다. 도 6은 본 발명의 일실시예에 따른 음성 합성 장치의 추론 순서도를 도시하는 도면이다. 이하, 도 5 및 도 6을 함께 참조하 여 설명한다. 텍스트 인코더는 음성을 합성하고자 하는 텍스트인 타겟 텍스트를 입력 받는다. 텍스트 인코더는 입 력된 타겟 텍스트를 인코딩(S601 단계)하여 텍스트 표현형 htext를 생성하고, 프로젝션부 및 음소 길이 예 측부로 전달한다. S602 단계에서 프로젝션부는, 텍스트 표현형인 htext를 프로젝션한다. 보다 구체적으로 프로젝션부는, 텍스트 표현형으로부터 제 2 표현형의 평균값 및 표준편차값 를 산출하고, 상기 산출된 평균값 및 표준편차값 에 기초하여 상기 제 2 표현형을 생성한다. S611 단계에서 음소 길이 예측부는, 텍스트 표현형인 htext에 기초하여 음소별 길이 를 예측한다. 그리고 예측된 음소별 길이 를 음소 길이 적용부에 전달한다. S603 단계에서 음소 길이 적용부는, 예측된 음소별 길이 를 적용한다. 보다 구체적으로 음소 길이 적용 부는 예측된 음소별 길이 에 기초하여 상기 생성된 제 2 표현형에 포함된 각 음소별 길이를 조절한다. S604 단계에서 분포 변환부는, 음소별 길이가 조절된 제 2 표현형에 대해서 분포를 변환시킨다. 본 발명의 일실시예에 따른 분포 변환부는, S604 단계에서 흐름 정규화(Normalizing Flow) 함수 의 역함수인 함수 를 적용하여 분포를 변환한 결과 z를 출력한다. S605 단계에서 디코더는, z(분포가 변환된 제 2 표현형)를 디코딩하여, 최종 음성 파형(Raw waveform)을 생성한다. 상술한 본 발명의 과정에 의해서 생성되는 최종 음성 파형은 사람의 실제 음성과 유사도가 높으면서"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "동시에 최종 음성 파형의 생성 처리 시간이 상당히 단축될 수 있는 효과가 존재한다.이하에서는, 본 발명의 효과를 증명하기 위하여 수행된 실험에 대해서 설명하고, 상술한 효과를 증명한다. * 실험 세팅 본 발명의 실시예의 효과를 증명하기 위하여 두 개의 서로 다른 데이터 세트에 대해 실험을 수행했다. 공개된 다른 음성 합성 모델과의 비교를 위해 'LJ Speech 데이터 세트'를 사용하고 다양한 음성 특성을 학습하 고 표현할 수 있는 모델인지 확인하기 위해 'VCTK 데이터 세트'를 사용했다. 'LJ Speech 데이터셋'은 총 길이가 약 24시간인 단일 화자의 짧은 오디오 클립 13,100개로 구성된다. 오디오 포 맷은 16비트 PCM으로 샘플레이트는 22kHz로 본 실험에서는 별도의 조작 없이 이를 그대로 사용했다. 'LJ Speech 데이터셋'를 훈련 세트(샘플 12,500개), 검증 세트(샘플 100개) 및 테스트 세트(샘플 500개)로 무작위로 분할했 다. 'VCTK 데이터 세트'는 다양한 억양을 가진 109명의 영어 원어민이 말한 약 44,000개의 짧은 오디오 클립으로 구 성되며, 오디오 클립의 총 길이는 약 44시간이다. 'VCTK 데이터 세트'의 오디오 형식은 16비트 PCM이며 샘플 속 도는 44kHz인데, 본 실험에서는 샘플 속도를 22kHz로 줄여서 사용했다. 'VCTK 데이터 세트'를 훈련 세트(샘플 43,470개), 검증 세트(샘플 100개) 및 테스트 세트(샘플 500개)로 무작위로 분할했다. STFT(Short-time Fourier transform)를 통해 파형에서 얻을 수 있는 선형 스펙트로그램을 후방 인코더의 입력으로 사용한다. 변환의 FFT 크기, 창 크기 및 홉 크기는 각각 1024, 1024 및 256으로 설정하였다. 그리고, 선형 스펙트로그램에 멜 필터뱅크를 적용하여 얻은 재구성 손실을 위해 80밴드 멜 스케일 스펙트로그램을 사용 한다. IPA(International Phonetic Alphabet) 시퀀스를 텍스트 인코더에 대한 입력으로 사용한다. 오픈 소스 소 프트웨어를 사용하여 텍스트 시퀀스를 IPA 음소 시퀀스로 변환하고 변환된 시퀀스는 'Glow-TTS' 구현에 따라 빈 토큰(들)이 사이사이 배치된다. 네트워크는 β1 = 0.8, β2 = 0.99 및 가중치 감소(weight decay) λ = 0.01인 AdamW 옵티마이저를 사용하여 훈 련되었다. 학습률 감쇠(learning rate decay)는 초기 학습률이 2 × 104인 모든 에포크(epoch)에서 0.9991/8 인 수로 스케쥴된다. 훈련 중 훈련 시간과 메모리 사용량을 줄이기 위해 파형의 일부만 생성하는 창 생성기 훈련 방법(windowed generator training)을 사용하였다. 전체 제 1 표현형을 디코더에 공급하는 대신 창 크기가 32인 제 1 표 현형의 세그먼트를 무작위로 추출하고 실제 파형에서 해당 오디오 세그먼트를 훈련 대상으로 추출한다. 4개의 NVIDIA V100 GPU에서 혼합 정밀 교육을 사용하였다. 배치 크기는 GPU당 64로 설정되고 모델은 최대 800,000단계 까지 학습된다. 본 발명의 실험 결과는 현재 공개되어 있는 모델 중 가장 성능이 좋은 모델과 비교한 결과이다. 'autoregressive 모델'인 'Tacotron 2'와 흐름 기반 'non-autoregressive 모델'인 'Glow-TTS'를 1단계 모델로 사용하고 HiFi-GAN을 2단계 모델로 사용하였다. 이때 공개되어 있는 설정과 사전 훈련된 가중치를 사용하였다. 2단계 TTS 시스템은 이론적으로 순차적 훈련을 통해 더 높은 합성 품질을 달성할 수 있으므로, 첫 번째 단계 모 델에서 예측된 출력과 함께 최대 100k 단계까지 미세 조정된 HiFi-GAN을 포함하였다. 교사 강제 모드(teacher- forcing mode)에서 미세 조정(fine-tuned)된 HiFi-GAN을 Tacotron 2에서 생성된 멜 스펙트로그램과 함께 사용 할 경우, Tacotron 2 and Glow-TTS 두 경우 모두에서 Glow-TTS에서 생성된 멜 스펙트로그램을 사용하여 미세 조 정된 경우 보다 더 좋은 품질을 보인다는 것을 확인하였다. 그래서 본 발명에서는 'Tacotron 2'와 'Glow-TTS'에 미세 조정(fine-tuned)된 HiFi-GAN을 추가하여 실험하였다.각 모델은 샘플링 중에 어느 정도 랜덤 속성을 가지므로 실험 전반에 걸쳐 각 모델의 랜덤 속성을 제어하기 위 한 하이퍼 매개변수를 고정하였다. 'Tacotron 2'의 pre-net에서 탈락 확률은 0.5로 설정하였다. 'Glow-TTS'의 경우 사전 분포의 표준 편차는 0.333으로 설정되었다. 본 발명의 실시예에서는 음소 길이 예측부의 입력 노이즈의 표준 편차는 0.8로 설정되었고 사전 분포의 표준 편차에 0.667의 스케일 팩터를 곱하였다. * 실험 결과 생성된 음질의 품질을 평가하기 위해 크라우드 소싱 MOS 테스트를 수행했다. 평가자는 무작위로 선택된 오디오 샘플을 듣고 1에서 5까지의 5점 척도로 자연스러움을 평가했다. 평가자는 각 오디오 샘플을 한 번 평가할 수 있 었고 모든 오디오 클립을 정규화하여 점수에 대한 진폭 차이의 영향을 방지했다. 이 작업의 모든 품질 평가는 이러한 방식으로 수행되었다. 평가 결과는 표 1과 같이 정리되었다. 본 발명에 따른 음성 학습 장치(표 1에서 VITS)는 다른 음성 합성 시스템 보다 성능이 우수하고 실제 음성과 유사한 MOS를 달성한 것을 확인할 수 있다. 표 1 Model MOS (CI) Ground Truth 4.46 (±0.06) Tacotron 2 + HiFi-GAN 3.77 (±0.08) Tacotron 2 + HiFi-GAN (Fine-tuned)4.25 (±0.07) Glow-TTS + HiFi-GAN 4.14 (±0.07) Glow-TTS + HiFi-GAN (Fine-tuned)4.32 (±0.07) VITS (DDP) (본 발명) 4.39 (±0.06) VITS (본 발명) 4.43 (±0.06) 본 발명의 일실시예에 따른 음성 합성 장치의 다른 구성은 모두 동일하고, 음소 길이 예측부를 사용 하는 대신 'Glow-TTS'에서 사용된 것과 동일한 결정적 지속 시간 예측 아키텍처를 사용하는 VITS(DDP)는 MOS 평 가에서 TTS 시스템 중 두 번째로 높은 점수를 받았다. 두 개의 결과를 비교하였을 때 1) 확률적으로 음소별 지속 시간 예측(stochastic duration predictor)을 수행하 는 음소 길이 예측부가 결정론적 지속 시간 예측자(deterministic duration predictor architecture)보다 더 현실적인 음소 지속 시간을 생성하고 2) 우리의 종단간(end-to-end) 학습 방법이 유사한 지속 시간 예측자의 구조를 그대로 사용하더라도 다른 TTS 모델보다 우수한 효능을 가진다는 것을 확인할 수 있다. 추가적으로 본 실험에서 사전 인코더(prior encoder)의 정규화된 흐름과 선형 스케일 스펙트로그램 사후 입력 (linear-scale spectrogram posterior input)을 포함하여 방법의 효율성을 입증하기 위해 절제 연구(ablation study)를 수행했다. 절제 연구의 모든 모델은 최대 300,000단계까지 훈련되었다. 결과는 아래 표 2에 나와 있다. 표 2 Model MOS (CI) Ground Truth 4.50 (±0.06) Baseline 4.50 (±0.06) without Normalizing Flow 2.98 (±0.08) with Mel-spectrogram 4.31 (±0.08) 표 2에서는 사전 인코더에서 흐름 정규화(Normalizing flow)를 제거하면 기준선에서 1.52 MOS가 감소하여 이전 분포의 유연성이 합성 품질에 상당한 영향을 미친다는 것을 보여준다. 또한, 표 2를 참조하면, 사후 입력에 대 한 선형 스케일 스펙트로그램을 멜 스펙트로그램으로 대체하면 품질 저하(-0.19 MOS)가 발생하여 고해상도 정보 가 본 발명의 실시예에 따른 음성 합성 장치가 합성 품질을 개선하는 데 효과적임을 나타낸다. 다양한 음성 특성을 학습하고 표현할 수 있음을 검증하기 위해 본 발명의 실시예에 따른 음성 합성 장치를 'Tacotron 2', 'Glow-TTS' 및 'HiFi-GAN'과 비교하여 다중 화자 음성 합성으로 확장할 수 있는 능력을 증명하 였다. 본 실험에서는 'VCTK 데이터 세트'에서 모델을 훈련했다. 앞서 설명한 대로 다중 화자를 고려하기 위하여, 모델에 스피커 임베딩을 추가하였다. 'Tacotron 2'의 경우 스피커 임베딩을 방송하여 인코더 출력과 연 결했으며 'Glow-TTS'의 경우 이전 작업에 이어 글로벌 컨디셔닝(global conditioning)을 적용하였다. 표 3에서 보는 바와 같이 본 발명의 실시예(VITS)에 따른 모델이 다른 모델보다 더 높은 MOS를 달성한 것을 확인할 수 있 다. 이것은 본 발명의 실시예(VITS)에 따른 모델이 종단간 방식으로 다양한 음성 특성을 학습하고 표현한다는 것을 보여준다. 표 3 Model MOS (CI) Ground Truth 4.38 (±0.07) Tacotron 2 + HiFi-GAN 3.14 (±0.09) Tacotron 2 + HiFi-GAN (Fine-tuned) 3.19 (±0.09) Glow-TTS + HiFi-GAN 3.76 (±0.07) Glow-TTS + HiFi-GAN (Fine-tuned) 3.82 (±0.07) VITS (본 발명) 4.38 (±0.06) 추가적으로 본 실험에서는 확률적 지속 시간 예측자를 사용하는 음소 길이 예측부가 생성하는 음성의 길이 가 얼마나 다른지, 합성된 샘플이 갖는 음성 특성이 얼마나 다른지 확인했다. 여기의 모든 샘플은 \"How much variation is there(얼마나 많은 변화가 있습니까)?\"라는 문장에서 생성되었다. 도 7은 각 모델에서 생성된 100개의 발화 길이에 대한 히스토그램을 도시한다. Glow-TTS는 결정적 지속 시간 예측기로 인해 고정된 길이의 발화만 생성하지만, 본 발명의 실시예에 따른 모델 (VITS)의 샘플은 'Tacotron 2'의 길이 분포와 유사한 길이 분포를 따르는 것이 확인된다. 이러한 결과는 다중 화자 설정에서 모델이 화자 종속 음소 지속 시간을 학습함을 의미한다. 도 8은 다중 화자 설정에서 본 발명의 실시예에 따른 모델(VITS)의 5개 화자 ID 각각으로 생성된 100발언의 길 이를 도시한다. 도 8은모델이 화자에 종속되는 음소 기간을 학습함을 의미한다. 도 9는 'YIN 알고리즘'으로 추출한 10개 발화의 F0 윤곽은 본 발명의 실시예에 따른 모델(VITS)이 다양한 음높 이와 리듬으로 음성을 생성함을 보여준다. 도 9 (d)의 서로 다른 화자 ID로 생성된 5개 샘플은 우리 모델을 보여준다. 각 화자 개개인에 따라 매우 다른 길이와 음조를 표현한다. 'Glow-TTS'는 사전 분포의 표준편차를 증가시켜 음정의 다양성을 증가시킬 수 있지만, 반대로 합성 품질을 저하시킬 수 있는 문제점이 존재한다. 또한 본 실험에서는, 본 발명의 실시예에 따른 모델(VITS)의 합성 속도를 병렬 2단계 TTS 시스템인 'Glow-TTS' 및 'HiFi-GAN'과 비교했다. 'LJ Speech 데이터 세트'의 테스트 세트에서 무작위로 선택된 100개의 문장으로, 음 소 시퀀스에서 파형을 생성하기 위해 전체 프로세스에서 동기화된 경과 시간을 측정하여 비교하였다. 본 속도 비교 실험에서는 배치 크기가 1인 단일 NVIDIA V100 GPU를 사용하였고, 결과를 표 4와 같이 정리하였다. 본 발명의 실시예에 따른 모델(VITS)에는 미리 정의된 중간 표현을 생성하기 위한 모듈이 필요하지 않기 때문에 샘플링 효율성과 속도가 크게 향상된 것을 확인할 수 있었다.표 4 Model Speed (kHz) Real-time Glow-TTS + HiFi-GAN 606.05 ×27.48 VITS (본 발명) 1480.15 ×67.12 VITS (DDP) (본 발명) 2005.03 ×90.93 도 10은 일 실시예에 따른 음성 합성 장치의 구성을 도시한 도면이다. 도 10을 참조하면, 음성 합성 장치는 프로세서 및 메모리를 포함한다. 메모리는 프로세 서에 의해 실행 가능한 하나 이상의 명령어를 저장한다. 프로세서는 메모리에 저장된 하나 이상의 명령어를 실행한다. 프로세서는 명령어를 실행하는 것에 의해 도 1 내지 도 6과 관련하여 위에서 설명된 하나 이상의 동작을 실행할 수 있다. 또한 도 2과 함께 상술한 본 발명의 구성은 프로세서에 의해 서 실행되는 명령어에 의해서 구현되는 구성일 수 있을 것이다. 이상으로 본 발명에 따른 음성 합성 장치 및 그것의 학습/제어 방법의 실시예를 설시하였으나 이는 적어도 하나 의 실시예로서 설명되는 것이며, 이에 의하여 본 발명의 기술적 사상과 그 구성 및 작용이 제한되지는 아니하는 것으로, 본 발명의 기술적 사상의 범위가 도면 또는 도면을 참조한 설명에 의해 한정／제한되지는 아니하는 것 이다. 또한 본 발명에서 제시된 발명의 개념과 실시예가 본 발명의 동일 목적을 수행하기 위하여 다른 구조로"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "수정하거나 설계하기 위한 기초로써 본 발명이 속하는 기술분야의 통상의 지식을 가진 자에 의해 사용되어질 수"}
{"patent_id": "10-2021-0074262", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "있을 것인데, 본 발명이 속하는 기술분야의 통상의 지식을 가진 자에 의한 수정 또는 변경된 등가 구조는 청구 범위에서 기술되는 본 발명의 기술적 범위에 구속되는 것으로서, 청구범위에서 기술한 발명의 사상이나 범위를 벗어나지 않는 한도 내에서 다양한 변화, 치환 및 변경이 가능한 것이다."}
{"patent_id": "10-2021-0074262", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 기존 음성 합성 기술의 음파 생성 개념도를 도시한다. 도 2는 본 발명의 일실시예에 따른 음성 합성 장치의 블록도를 도시한다. 도 3은 본 발명의 일실시예에 따른 음성 합성 장치의 학습 개념도를 도시하는 도면이다. 도 4는 본 발명의 일실시예에 따른 음성 합성 장치의 학습 순서도를 도시하는 도면이다. 도 5는 본 발명의 일실시예에 따른 음성 합성 장치의 추론 개념도를 도시하는 도면이다. 도 6은 본 발명의 일실시예에 따른 음성 합성 장치의 추론 순서도를 도시하는 도면이다. 도 7은 각 모델에서 생성된 100개의 발화 길이에 대한 히스토그램을 도시한다. 도 8은 다중 화자 설정에서 본 발명의 실시예에 따른 모델(VITS)의 5개 화자 ID 각각으로 생성된 100발언의 길 이를 도시한다. 도 9는 'YIN 알고리즘'으로 추출한 10개 발화의 F0 윤곽은 본 발명의 실시예에 따른 모델(VITS)이 다양한 음높 이와 리듬으로 음성을 생성함을 보여준다."}
