{"patent_id": "10-2022-0150085", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0068992", "출원번호": "10-2022-0150085", "발명의 명칭": "사용자의 표정과 감정 상태를 적용한 3D 모델 얼굴 움직임 생성 방법 및 시스템", "출원인": "한국전자기술연구원", "발명자": "김용화"}}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "에 있어서,사용자 얼굴 영상에 동기화된 사용자 음성을 입력받는 단계;입력된 음성을 텍스트로 변환하는 단계;를 더 포함하고,얼굴 움직임 제어정보 생성단계는,추출된 얼굴 움직임 외에 텍스트를 더 참조하여, 얼굴 움직임 제어정보를 생성하는 것을 특징으로 하는 사용자모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,얼굴 움직임 제어정보 생성단계는,사용자 얼굴 움직임으로부터 사용자의 표정을 파악하고, 얼굴 움직임 제어정보에 파악된 표정을 적용하는 것을특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 2에 있어서,얼굴 움직임 제어정보 생성단계는,미리 수집한 사용자의 얼굴 표정에 대한 특징 정보를 활용하여, 사용자 얼굴 움직임으로부터 사용자의 표정을파악하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 1에 있어서,감정 상태를 추출단계는,추출된 얼굴 움직임으로부터 사용자의 감정 상태를 추출하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 1에 있어서,적용 단계는,추출된 감정 상태에 따라 얼굴 움직임이 갖는 특성들을 정량화한 정보를 얼굴 움직임 제어정보에 적용하는 것을특징으로 하는 사용자 모사 캐릭터 영상 생성 방법.공개특허 10-2024-0068992-3-청구항 6"}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 6에 있어서,텍스트는,입모양 움직임에 대한 제어정보를 생성하기 위해 참조되는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 6에 있어서,감정 상태를 추출단계는,추출된 얼굴 움직임 및 입력된 사용자 음성으로부터 사용자의 감정 상태를 추출하는 것을 특징으로 하는 사용자모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서,캐릭터 영상 생성 단계는,얼굴 움직임 제어정보를 분석하여 해당 제어정보에 따라 얼굴이 움직이는 캐릭터 영상을 생성하도록 학습된 인공지능 모델을 활용하여, 캐릭터 영상을 생성하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "사용자 얼굴 영상을 입력받는 입력부;입력된 영상에서 얼굴 움직임을 추출하는 움직임 추출부;추출된 얼굴 움직임으로부터 얼굴 움직임 제어정보를 생성하는 제어정보 생성부;사용자 감정 상태를 추출하는 감정 추출부;추출된 감정 상태를 얼굴 움직임 제어정보에 적용하는 적용부;얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 캐릭터 영상 생성부;를 포함하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2024-0068992-4-사용자 얼굴 영상에서 얼굴 움직임을 추출하는 단계;추출된 얼굴 움직임으로부터 얼굴 움직임 제어정보를 생성하는 단계;사용자 감정 상태를 얼굴 움직임 제어정보에 적용하는 단계;얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 단계;를 포함하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "사용자 얼굴 영상에서 얼굴 움직임을 추출하는 추출부;추출된 얼굴 움직임으로부터 얼굴 움직임 제어정보를 생성하는 제어정보 생성부;사용자 감정 상태를 얼굴 움직임 제어정보에 적용하는 적용부;얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 캐릭터 영상 생성부;를 포함하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "사용자의 표정과 감정 상태를 적용한 3D 모델 얼굴 움직임 생성 방법 및 시스템이 제공된다. 본 발명의 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법은 사용자 얼굴 영상에서 얼굴 움직임을 추출하고, 추출된 얼굴 움직임 으로부터 얼굴 움직임 제어정보를 생성하며, 사용자 감정 상태를 얼굴 움직임 제어정보에 적용하고, 얼굴 움직임 제어정보를 기초로 얼굴이 움직이는 캐릭터 영상을 생성한다. 이에 의해, 사용자의 표정 특징과 감정 상태를 적 용하여 보다 자연스럽고 풍부한 표정을 갖는 캐릭터 영상을 생성할 수 있게 되어, 디지털 캐릭터의 자연스러운 움직임을 만들어 내어 이를 이용한 영화, 게임, 애니메이션 등 디지털 콘텐츠에서 활용할 수 있는 기틀을 마련할 수 있다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 얼굴 모션 캡쳐에 관한 것으로, 더욱 상세하게는 사용자의 얼굴을 실시간으로 추적하면서 모사하는 가상의 캐릭터 영상을 자동으로 생성하는 방법에 관한 것이다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "배경기술", "item": 1, "content": "도 1은 종래의 얼굴 모션 캡쳐 시스템이다. 얼굴 모션 캡쳐 시스템은 카메라로 사용자를 촬영하고, 촬영된 사용 자의 얼굴 모션을 파악하여 가상의 캐릭터가 이를 그대로 따라하도록 하는 시스템이다. 사용자의 얼굴 움직임을 실시간으로 추적하면서 이를 가상의 캐릭터에 그대로 반영한다는 점에서, 얼핏 보기에 는 유사해 보일 수 있지만, 자연스러운 움직임을 만들어내는 데에는 한계가 있다. 또한 캐릭터의 표정이 실제 사람 같지 않고 부자연스러우며, 풍부하지 못하다는 문제가 있다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기와 같은 문제점을 해결하기 위하여 안출된 것으로서, 본 발명의 목적은, 사용자를 모사하는 가상 의 캐릭터 영상을 생성함에 있어, 사용자의 표정 특징과 감정 상태를 적용하여 보다 자연스럽고 풍부한 표정을 갖는 캐릭터 영상을 생성하는 방법 및 시스템을 제공함에 있다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법은 사용자 얼굴 영 상을 입력받는 단계; 입력된 영상에서 얼굴 움직임을 추출하는 단계; 추출된 얼굴 움직임으로부터 얼굴 움직임 제어정보를 생성하는 단계; 사용자 감정 상태를 추출하는 단계; 추출된 감정 상태를 얼굴 움직임 제어정보에 적 용하는 단계; 얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 단계;를 포함한다. 얼굴 움직임 제어정보 생성단계는, 사용자 얼굴 움직임으로부터 사용자의 표정을 파악하고, 얼굴 움직임 제어정 보에 파악된 표정을 적용하는 것일 수 있다. 얼굴 움직임 제어정보 생성단계는, 미리 수집한 사용자의 얼굴 표정에 대한 특징 정보를 활용하여, 사용자 얼굴 움직임으로부터 사용자의 표정을 파악하는 것일 수 있다. 감정 상태를 추출단계는, 추출된 얼굴 움직임으로부터 사용자의 감정 상태를 추출하는 것일 수 있다. 적용 단계는, 추출된 감정 상태에 따라 얼굴 움직임이 갖는 특성들을 정량화한 정보를 얼굴 움직임 제어정보에 적용하는 것일 수 있다. 본 발명에 따른 사용자 모사 캐릭터 영상 생성 방법은 사용자 얼굴 영상에 동기화된 사용자 음성을 입력받는 단 계; 입력된 음성을 텍스트로 변환하는 단계;를 더 포함하고, 얼굴 움직임 제어정보 생성단계는, 추출된 얼굴 움 직임 외에 텍스트를 더 참조하여, 얼굴 움직임 제어정보를 생성하는 것일 수 있다. 텍스트는, 입모양 움직임에 대한 제어정보를 생성하기 위해 참조될 수 있다. 감정 상태를 추출단계는, 추출된 얼굴 움직임 및 입력된 사용자 음성으로부터 사용자의 감정 상태를 추출하는 것일 수 있다. 캐릭터 영상 생성 단계는, 얼굴 움직임 제어정보를 분석하여 해당 제어정보에 따라 얼굴이 움직이는 캐릭터 영 상을 생성하도록 학습된 인공지능 모델을 활용하여, 캐릭터 영상을 생성하는 것일 수 있다. 본 발명의 다른 측면에 따르면, 사용자 얼굴 영상을 입력받는 입력부; 입력된 영상에서 얼굴 움직임을 추출하는 움직임 추출부; 추출된 얼굴 움직임으로부터 얼굴 움직임 제어정보를 생성하는 제어정보 생성부; 사용자 감정 상태를 추출하는 감정 추출부; 추출된 감정 상태를 얼굴 움직임 제어정보에 적용하는 적용부; 얼굴 움직임 제어 정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 캐릭터 영상 생성부;를 포함하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법이 제공된다. 본 발명의 또다른 측면에 따르면, 사용자 얼굴 영상에서 얼굴 움직임을 추출하는 단계; 추출된 얼굴 움직임으로 부터 얼굴 움직임 제어정보를 생성하는 단계; 사용자 감정 상태를 얼굴 움직임 제어정보에 적용하는 단계; 얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 단계;를 포함하는 것을 특징으로 하는 사 용자 모사 캐릭터 영상 생성 방법이 제공된다. 본 발명의 또다른 측면에 따르면, 사용자 얼굴 영상에서 얼굴 움직임을 추출하는 추출부; 추출된 얼굴 움직임으 로부터 얼굴 움직임 제어정보를 생성하는 제어정보 생성부; 사용자 감정 상태를 얼굴 움직임 제어정보에 적용하 는 적용부; 얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하는 캐릭터 영상 생성부;를 포함하는 것을 특징으로 하는 사용자 모사 캐릭터 영상 생성 방법이 제공된다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상 설명한 바와 같이, 본 발명의 실시예들에 따르면, 사용자를 모사하는 가상의 캐릭터 영상을 생성함에 있어, 사용자의 표정 특징과 감정 상태를 적용하여 보다 자연스럽고 풍부한 표정을 갖는 캐릭터 영상을 생성할 수 있게 된다. 또한 본 발명의 실시예들에 따르면, 실제 사용자의 특징을 충분하게 반영한 가상의 캐릭터를 만듦으로써, 비대 면 환경 등의 디지털 세상에서 나만의 디지털 트윈 캐릭터를 이용할 수 있는 기반을 마련하며, 디지털 캐릭터의 자연스러운 움직임을 만들어 내어 이를 이용한 영화, 게임, 애니메이션 등 디지털 콘텐츠에서 활용할 수 있는 기틀을 마련할 수 있다."}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명을 보다 상세하게 설명한다. 본 발명의 실시예에서는 개인의 표정 특징 및 감정 상태를 적용한 3D 모델 얼굴 움직임 생성 방법 및 시스템을 제시한다. 구체적으로 개인의 얼굴 표정이 갖는 특징들을 추출하여 디지털 캐릭터의 얼굴 움직임을 재현하는데 적용함으로 써, 디지털 트윈 캐릭터로써 보다 사용자와 비슷하게 움직이는 얼굴 움직임 생성 시스템을 구현하며 임의의 감 정 상태도 함께 적용할 수 있도록 하여 디지털 캐릭터가 풍부한 얼굴 표정을 가질 수 있도록 하기 위한 기술이 다. 도 2는 본 발명의 일 실시예에 따른 사용자 모사 캐릭터 영상 생성 시스템의 구성을 도시한 도면이다. 본 발명 의 실시예에 따른 '사용자 모사 캐릭터 영상 생성 시스템'(이하, '캐릭터 생성 시스템'으로 약칭)은 도시된 바 와 같이, 영상 입력부, 얼굴 움직임 추출부, 제어정보 생성부, 감정 추출부, 감정 적용부 및 캐릭터 영상 생성부를 포함하여 구성된다. 영상 입력부는 카메라를 통해 촬영되는 사용자 얼굴 영상을 연속하는 프레임 단위로 입력받는다. 얼굴 움직임 추출부는 영상 입력부를 통해 입력되는 사용자 얼굴 영상에서 특징점들을 추출하고, 추 출된 특징점들의 움직임을 파악하여 사용자 얼굴 움직임을 추출한다. 제어정보 생성부는 얼굴 움직임 추출부에 의해 추출된 얼굴 움직임을 이용하여 얼굴 움직임 제어정보 를 생성한다. 이를 위해, 제어정보 생성부는 미리 수집한 사용자의 얼굴 표정에 대한 특징 정보를 활용한 다. 구체적으로 제어정보 생성부는 사용자 얼굴 움직임으로부터 사용자의 표정을 파악하고, 얼굴 움직임 제어 정보에 파악된 표정을 적용하는 것이다. 얼굴 표정은 사람 마다 다르기 때문에, 얼굴 움직임을 제어정보 생성에 그대로 적용하는 것이 아닌, 얼굴 표정 을 해석하고 해석된 얼굴 표정을 얼굴 움직임 제어정보에 반영한 것이다. 이는 사용자의 디지털 트윈에 해당하는 캐릭터가 사용자의 얼굴 움직임을 그대로 따라한다기 보다 사용자의 표 정을 캐릭터의 표정으로 소화하여 표현하게 된다는 점에서, 보다 자연스러운 표정을 갖는 캐릭터를 생성하도록 하여 준다. 감정 추출부는 얼굴 움직임 추출부에 의해 추출된 사용자 얼굴 움직임으로부터 사용자 감정 상태를 추출한다. 이를 위해 사용자 얼굴 움직임으로부터 감정 상태를 추정하는 기지의 알고리즘을 활용한다. 감정 적용부는 감정 추출부에 의해 추출된 감정 상태를 제어정보 생성부에 의해 생성된 얼굴 움 직임 제어정보에 반영한다. 이를 위해 감정 상태에 따라 얼굴 움직임이 갖는 특성들을 정량화한 정보를 사전에 마련하여 이용한다. 감정 상태는 얼굴 표정에 나타나기는 하지만 감정 상태를 얼굴 표정으로 정의할 수는 없다. 이에 따라 사용자의 감정 상태를 해석하고 해석된 감정 상태 까지 얼굴 움직임 제어정보에 반영하여, 캐릭터가 해당하는 감정 상태 에 따라 미묘한 변화를 갖도록 하여 표정이 보다 풍부해질 수 있도록 하여 준다. 캐릭터 영상 생성부는 제어정보 생성부에 의해 생성된 후 감정 적용부에 의해 감정 상태가 반영 된 얼굴 움직임 제어정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하여 출력한다. 캐릭터 영상 생성부는 얼굴 움직임 제어정보를 분석하여 해당 제어정보에 따라 얼굴이 움직이는 캐릭터 영 상을 생성하도록 학습된 인공지능 모델을 활용하여, 캐릭터 영상을 생성할 수 있다. 도 3은 본 발명의 다른 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법의 설명에 제공되는 흐름도이다. 캐릭터 영상을 생성하기 위해, 먼저 영상 입력부는 카메라를 통해 촬영되는 사용자 얼굴 영상을 입력받고 (S210), 얼굴 움직임 추출부는 S210단계에서 입력되는 사용자 얼굴 영상에서 사용자 얼굴 움직임을 추출한다(S220). 그러면 제어정보 생성부는 S220단계에서 추출된 얼굴 움직임을 이용하여 얼굴 움직임 제어정보를 생성하되, 사용자 얼굴 움직임으로부터 파악되는 사용자의 표정을 적용한다(S230). 이후 감정 추출부는 S220단계에서 추출된 사용자 얼굴 움직임으로부터 사용자 감정 상태를 추출하고 (S240), 감정 적용부는 S240단계에서 추출된 감정 상태를 S230단계에서 생성된 얼굴 움직임 제어정보에 반 영한다(S250). 그리고 캐릭터 영상 생성부는 S230단계에서 생성된 후 S250단계에서 감정 상태가 반영된 얼굴 움직임 제어 정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하여 출력한다(S260). 도 4는 본 발명의 또 다른 실시예에 따른 캐릭터 생성 시스템의 구성을 도시한 도면이다. 본 발명의 실시예에 따른 캐릭터 생성 시스템은 도 2에 도시된 시스템에서 음성 입력부 및 텍스트 변환부가 더 추가된 것 이다. 음성 입력부는 영상 입력부에 입력되는 사용자 얼굴 영상에 동기화된 사용자 음성을 입력받는다. 텍 스트 변환부는 음성 입력부를 통해 입력되는 사용자 음성을 텍스트로 변환하는 STT(Speech To Text) 모듈이다. 텍스트 변환부에 의해 생성된 텍스트는 제어정보 생성부로 전달되어 얼굴 움직임 제어정보 생성에 이 용된다. 구체적으로 제어정보 생성부는 얼굴 움직임 제어정보를 생성함에 있어, 입모양의 움직임에 대한 제어정보에 대해서는 사용자가 발음한 텍스트를 반영한다. 입모양은 발음한 텍스트에 영향을 받기 때문에, 입모양의 움직임을 생성함에 있어 사용자의 얼굴 움직임 외에 사용자가 발음한 텍스트를 더 반영하는 것이며, 이로 인해 입모양이 보다 정확하고 자연스러워 질 수 있다. 한편 음성 입력부에 입력된 사용자 음성은 사용자의 감정 상태 추출에 참조된다. 구체적으로 감정 추출부 는 얼굴 움직임 추출부에 의해 추출된 사용자 얼굴 움직임 외에 음성 입력부에 의해 입력된 사 용자 음성을 더 참조하여 사용자 감정 상태를 추출한다. 설명한 기능들을 제외한 도 4의 시스템 구성들의 기능들은 도 2의 시스템 구성들의 기능들과 동등하므로, 중복 되는 설명은 생략한다. 도 5는 본 발명의 또 다른 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법의 설명에 제공되는 흐름도이다. 캐릭터 영상을 생성하기 위해, 먼저 영상 입력부는 카메라를 통해 촬영되는 사용자 얼굴 영상을 입력받고 (S310), 얼굴 움직임 추출부는 S310단계에서 입력되는 사용자 얼굴 영상에서 사용자 얼굴 움직임을 추출한 다(S320). 그리고 음성 입력부는 S310단계에서 입력되는 사용자 얼굴 영상에 동기화된 사용자 음성을 입력받고 (S330), 텍스트 변환부는 S330단계에서 입력되는 사용자 음성을 텍스트로 변환한다(S340). 그러면 제어정보 생성부는 S320단계에서 추출된 얼굴 움직임과 S30단계에서 변환된 텍스트를 이용하여 얼 굴 움직임 제어정보를 생성한다(S350). 이후 감정 추출부는 S320단계에서 추출된 사용자 얼굴 움직임과 S310단계에서 입력된 사용자 음성으로부터 사용자 감정 상태를 추출하고(S360), 감정 적용부는 S360단계에서 추출된 감정 상태를 S350단계에서 생성 된 얼굴 움직임 제어정보에 반영한다(S370). 그리고 캐릭터 영상 생성부는 S350단계에서 생성된 후 S370단계에서 감정 상태가 반영된 얼굴 움직임 제어 정보를 기초로, 얼굴이 움직이는 캐릭터 영상을 생성하여 출력한다(S380). 지금까지 사용자를 모사하는 가상의 캐릭터 영상을 생성함에 있어, 사용자의 표정 특징과 감정 상태를 적용하여 보다 자연스럽고 풍부한 표정을 갖는 캐릭터 영상을 생성하는 방법에 대해 바람직한 실시예들을 들어 상세히 설 명하였다. 위 실시예들에서는 실제 사용자의 특징을 충분하게 반영한 가상의 캐릭터를 만듦으로써, 비대면 환경 등의 디지 털 세상에서 나만의 디지털 트윈 캐릭터를 이용할 수 있는 기반을 마련하며, 디지털 캐릭터의 자연스러운 움직 임을 만들어 냄으로써 이를 이용한 영화, 게임, 애니메이션 등 디지털 콘텐츠에서 활용할 수 있도록 하였다.한편, 본 실시예에 따른 장치와 방법의 기능을 수행하게 하는 컴퓨터 프로그램을 수록한 컴퓨터로 읽을 수 있는 기록매체에도 본 발명의 기술적 사상이 적용될 수 있음은 물론이다. 또한, 본 발명의 다양한 실시예에 따른 기 술적 사상은 컴퓨터로 읽을 수 있는 기록매체에 기록된 컴퓨터로 읽을 수 있는 코드 형태로 구현될 수도 있다. 컴퓨터로 읽을 수 있는 기록매체는 컴퓨터에 의해 읽을 수 있고 데이터를 저장할 수 있는 어떤 데이터 저장 장 치이더라도 가능하다. 예를 들어, 컴퓨터로 읽을 수 있는 기록매체는 ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광디스크, 하드 디스크 드라이브, 등이 될 수 있음은 물론이다. 또한, 컴퓨터로 읽을 수 있는 기록매체 에 저장된 컴퓨터로 읽을 수 있는 코드 또는 프로그램은 컴퓨터간에 연결된 네트워크를 통해 전송될 수도 있다. 또한, 이상에서는 본 발명의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2022-0150085", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2022-0150085", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 얼굴 모션 캡쳐 시스템, 도 2는 본 발명의 일 실시예에 따른 사용자 모사 캐릭터 영상 생성 시스템의 구성을 도시한 도면, 도 3은 본 발명의 다른 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법의 설명에 제공되는 흐름도,도 4는 본 발명의 또 다른 실시예에 따른 캐릭터 생성 시스템의 구성을 도시한 도면, 도 5는 본 발명의 또 다른 실시예에 따른 사용자 모사 캐릭터 영상 생성 방법의 설명에 제공되는 흐름도이다."}
