{"patent_id": "10-2014-0025967", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2014-0109337", "출원번호": "10-2014-0025967", "발명의 명칭": "기본 가치 신호를 이용한 강화 학습 방법 및 그 장치", "출원인": "한국과학기술원", "발명자": "크리스토퍼 피오릴로"}}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "정보 처리 소자(information processing element)를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법으로서,(a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 복수의 기본 가치 신호(elementary value signals)를 수신하는 단계;(b) 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을이용하여 강화 신호(reinforcement signal)를 계산하는 단계; 및(c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 단계를 포함하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서,상기 기본 가치 신호의 상기 활동들(activities)은,보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence forpunishment) 및 반 처벌 증거(evidence against punishment)인 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서,상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "에 있어서,상기 입력들의 하나의 부분집합(subset)은, 상기 정보 처리 소자의 이전 출력의 합에 상응하는 활동들(activities)을 가지며,이로부터 상기 정보 처리 소자는 가치(value)를 예측하는 시간적 패턴을 생성하는 것을 학습하는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "청구항 4에 있어서,상기 Hebbian-type 학습 법칙에서 사용되는 3개의 인수들(terms)은,동시에 발생하지 않고, 시간적으로 분리되며,상기 정보 처리 소자 내에서 일리저빌리티 트레이스(eligibility traces)에 의해 연결되고,이로부터 상기 정보 처리 소자는, 인과적으로 관련된 이벤트 시퀀스를 학습함을 통하여 가치(value)를 예측하는것을 학습하는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "청구항 1에 있어서,상기 기본 가치 신호의 상기 활동들(activities)은, 예측 오차(prediction errors)에 상응하는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 2에 있어서,복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고,상기 정보 처리 소자 각각은 8개의 타입 중 하나이며,상기 8개의 타입은,상기 각 기본 가치 신호에 대한 특정 가중치 값(weight values) 세트(set)를 가지고,상기 가중치 값은,보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence forpunishment) 및 반 처벌 증거(evidence against punishment) 각각에 대하여 양수(+) 또는 음수(-) 값을가지며,상기 8개 타입의 각각은,++++, ++--, --++, +---, -+--, --+-, ---+ 또는 +--+에 해당하고,상기 정보 처리 소자 각각은 특정한 가치(particular aspect of value)를 표현하는 것을 학습하는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 1에 있어서,복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고,상기 기본 가치 신호들은 수퍼바이저리 티칭 신호(supervisory teaching signal)의 방식으로 상기 망 외부에서공개특허 10-2014-0109337-3-생성되거나, 또는 상기 망 내에서 정보 처리 소자들 중 일부에 의해서 언수퍼바이즈드 방식으로(in anunsupervised manner) 생성되는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 1에 있어서,상기 기본 가치 신호의 상기 가중치는 양수 또는 음수 또는 0의 값을 가지며, 상기 기본 가치 신호의 적어도 두개의 가중치는 0이 아닌 값을 가지는 것을 특징으로 하는 정보 처리 소자를 이용하여 컴퓨터에 의해 수행되는 정보 처리 방법."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "정보 처리 소자를 이용하여 정보를 처리하는 장치로서,상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 복수의 기본 가치 신호(elementaryvalue signals)를 수신하는 수신부;상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 계산부; 및상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 학습부를 포함하는, 정보 처리 소자를 이용하여 정보를 처리하는 장치."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서,상기 기본 가치 신호의 상기 활동들(activities)은,보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence forpunishment) 및 반 처벌 증거(evidence against punishment)인 것을 특징으로 하는, 정보 처리 소자를 이용하여 정보를 처리하는 장치."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "청구항 11에 있어서,상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있는 것을 특징으로 하는, 정보 처리 소자를 이용하여 정보를 처리하는 장치."}
{"patent_id": "10-2014-0025967", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "정보 처리 소자를 이용하여 정보를 처리하는 하나 또는 복수의 명령어 시퀀스를 저장하는, 컴퓨터로 읽을 수 있는 기록매체로서,상기 명령어는, 하나 또는 복수의 프로세서에 의해 수행될 때,(a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 4개의 기본 가치 신호(elementary value signals)를 수신하는 단계;(b) 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을이용하여 강화 신호(reinforcement signal)를 계산하는 단계; 및(c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 단계를 포함하고,상기 4개의 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거공개특허 10-2014-0109337-4-(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence againstpunishment)이며,상기 4개의 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있는 것을 특징으로 하는, 정보 처리 소자를 이용하여 정보를 처리하는 하나 또는 복수의 명령어 시퀀스를 저장하는,컴퓨터로 읽을 수 있는 기록매체.명 세 서기 술 분 야본 발명은 신경망 모델 및 인공지능 분야와 관련된 것으로서, 특히 강화학습(reinforcement learning)과 관련이 [0001]있다.배 경 기 술강화학습(reinforcement learning, RL)이란, 기계 또는 동물 등과 같은 에이전트(agent)로 하여금 성공적으로 [0002]보상을 찾고 처벌을 피하게 하는 학습과 관련이 있다. 강화학습은, 에이전트에게 가치(value)를 가르치는 '가치신호(value signal)'(또는 강화 신호(reinforcement signal))에 의존한다. 어떤 모델에서는, 이러한 신호는 '보상 예측 오차(reward prediction error)'라고도 한다. 보상과 처벌은 일반적으로 '좋은 것과 나쁜 것'과 같은선상에서 서로 반대되는 것으로 인식되어져 왔고, 따라서 보상을 최대화한다는 것은 처벌을 최소화한다는 것과동의어로 인식되어져 왔다. 강화학습 모델은 보상과 처벌을 동일한 척도로 나타내는 단일 가치 신호(singlevalue signal)를 이용해왔는데, 이는 보상에 대하여는 양의 값을, 처벌에 대하여는 음의 값을 갖도록 한다. 따라서 만약 가치 신호(value signal)가 예측 오차(prediction error)이면, 신호값은 예기치 못한 혐오 자극(aversive stimulus)이 발생했거나 기대한 보상 자극(reward stimulus)이 발생하지 않은 순간에 음의 값을 갖는다. 또한 신호값은 예기치 못한 보상이 발생했거나 또는 예상되었던 처벌이 발생하지 않은 경우 양의 값을 갖는다. 여기서 제시하는 바는, 이러한 종류의 단일 가치 표현(single value representation)은 발생시키기 어렵고, 많은 종류의 강화학습에 대해서 부적합하며, 뇌(brain)에 의해 사용되지 않는 방법이라는 것이다. 여기서제안하고자 하는 것은, 만약 유효 가치 신호(effective value signal)가 네 개의 기본 가치 신호(elementaryvalue signals)의 합으로 구성된다면 강화학습은 더욱 유연하고 실제적인 방법으로 수행될 수 있다는 것이다.강화학습을 수행하기 위하여 가치(value)의 단일 표현(single representation)을 사용하는 종래의 접근법은 적 [0003]어도 네 가지 문제점이 있다. 첫째, 뇌는 가치(value)와 관련하여 엄청나게 다양한 정보를 갖고 있고, 그것이어떻게 하나의 신호로 통합될 수 있는지는 분명하지 않다. 그 문제는, 다수의 형태의 가치 신호(value signal)들이 충분하다면, 적어도 감소시킬 수는 있다. 둘째, 단일 뉴런의 활동(activity)은 어떤 자극에 대하여 우호적증거(evidence for stimulus) 또는 적대적 증거(evidence against stimulus)를 나타내지만, 우호적 증거와 적대적 증거를 동시에 표현할 수는 없다. 고도의 정확한 표현을 하기 위해서, 어떤 뉴런들은 빛의 세기와 같은 자극에 대한 우호적 증거에 의해 활성화되지만, 반면에 다른 뉴런들은 동일한 자극에 대한 적대적 증거에 의해 활성화된다. 이것은 적어도 두 개의 '대립되는(opponent)' 강화 신호(reinforcement signal)를 갖는 것이 유리하다는 것을 암시한다. 세번째이면서 가장 중요한 것은, 뇌는 다양한 가치 표현(value representation)을 필요로한다는 것이다. 예를 들어, 운동기관 영역(in motor regions)에서, 시냅스의 활동이 보상(reward)을예측한다면, 그 시냅스는 강화되어야 하며, 처벌(punishment)을 예측한다면 약화되어야 한다. 그러나 어떤 초기감각기관 영역에서는, 시냅스의 활동이 보상에 관련되어 있든지 또는 처벌에 관련되어 있든지 간에 그 시냅스는강화되어야 하는데, 그 이유는 '좋은 소식(good news)'과 '나쁜 소식(bad news)'은 모두 중요하므로 무시되어서는 안되기 때문이다. 따라서 감각 뉴런(sensory neuron)은 '현저성(salience)'을 표현하고, 운동 뉴런(motorneuron)은 '보상(reward)'을 표현한다. 그러므로 뇌는 강화학습을 위해 여러 타입의 가치 신호(value signal)를필요로 한다. 여기서 '현저성(salience)'이란, 예를 들어 자극(stimulus) 등이, 보상 증거(evidence forreward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 또는 반 처벌 증거(evidence against punishment) 등과 연관될 때, 그 자극 등이 현저성을 갖는다고 하며, 이하에서도 이를 이와같이 '현저성(salience)'이라는 용어로 표현하기로 한다. 넷째, 실험적 증거는, 실제로 뇌는 강화학습을 위한공개특허 10-2014-0109337-5-여러가지 가치 신호(multiple value signals)를 사용한다는 것이다.도파민(dopamine)은 '조절하는(modulatory)' 신경전달물질로서 중뇌(ventral midbrain)에서 뉴런들의 세포체에 [0004]서 방출되고, 양성 강화(positive reinforcement)를 중재하는 것으로 알려져 있다. 종래의 증거는, 도파민 뉴런은 기대했던 것보다 좋은 '어떤 것에 의해서' 활성화되며, 예상보다 나쁜 '어떤 것에 의해서' 억제된다는 제안을 지지한다. 이러한 근거에서, 도파민 뉴런은 강화학습을 구동하는 '보상 예측 오차(reward prediction error,RPE) 신호를 보내어, 뇌에 무엇이 좋고 무엇이 나쁜지를 가르친다는 것이 제안된 바 있다. 그러나, 곧 발표될새로운 실험은, 도파민 뉴런은 보상 증거(evidence for reward)에 의해 활성화되고, 반 보상 증거(evidenceagainst reward)에 의해 억제되나, 혐오감(처벌)에는 민감하지 않다는 것을 보여준다. 따라서 보상과 처벌은 뇌에 의해서 두 개의 별개 카테고리로 표현되는 것으로 보인다.혐오감에 대한 도파민 뉴런의 둔감성(insensitivity)은, 다른 뉴런이 그러한 혐오감을 표현해야 한다는 것을 강 [0005]력하게 시사한다. 그러나, 반 보상 증거(evidence against reward)에 의해 활성화되고 보상 증거(evidence forreward)에 의해 억제되는, 도파민 뉴런과는 반대 극성으로 보상을 표현하는 뉴런들이 있다는 강력한 증거가 있다. 따라서, 여기서 reward-ON(RON), reward-OFF(ROFF), aversive-ON(AON), aversive-OFF(AOFF) 등으로 표기되는네 가지 타입의 가치표현(value representation)이 있다는 것을 추론할 수 있다. 네 가지 유사한 타입의 학습은행위적 수준(behavioral level)에서 구별되어져 왔는데, 양성적 강화(positive reinforcement)(RON), 양성적 처벌(positive punishment)(AON), 음성적 강화(negative reinforcement)(AOFF), 음성적 처벌(negativepunishment)(ROFF) 등으로 표현되어져 왔다. 따라서 뇌에는 도파민과 유사하나 이와 같이 다른 종류의 가치(value)를 표현하는 적어도 세 개의 또 다른 신호들이 존재한다는 것을 제안한다. 이러한 네 가지 타입의 가치들은 기본 가치(elementary values)라 표현한다. 본 발명은, 이러한 네 개의 기본 가치(elementary values)로써 어떻게 다양한 강화 신호가 생성될 수 있는지를 나타낸다.선행기술문헌특허문헌(특허문헌 0001) KR 2010-0112742 A [0006]비특허문헌(비특허문헌 0001) Bear, M. F., Cooper, L. N. & Ebner, F. F. A physiological basis for a theory of [0007]synapse modification. Science 237, 42-48 (1987). (비특허문헌 0002) Bell, A.J. & Sejnowski, T.J. The 'independent components' of natural scenes areedge filters. Vision Res 37, 3327-3338 (1997). 발명의 내용해결하려는 과제본 발명은 에이전트에게 다양한 가치 표현을 가르치기 위하여 다양한 강화 신호를 발생시키는 문제를 해결하기 [0008]위하여 고안되었다. 뇌 또는 인공지능은, 위에서 설명한 바와 같이 다양한 가치 표현을 가지는 것이 필요하다.종래기술은 이러한 문제를 적절히 해결하지 못하였다.- 본 발명의 요약 [0009]본 발명은 강화학습(reinforcement learning, RL)이 뇌에서 어떻게 일어나느냐에 대한 증거에 기반한 정보 처리 [0010]방법을 설명한다. 강화학습(reinforcement learning)의 목적은, 에이전트(agent)로 하여금 보상을 최대화하고처벌을 최소화하는 것이다. 이러한 두 가지는 전통적으로, 서로 같은 것으로서 결국 하나라고 인식되어져 왔고,공개특허 10-2014-0109337-6-따라서 강화학습 모델에서의 학습은, '가치(value)'의 단일 척도에 의해 보상과 처벌을 표현하는(represent) 단일 강화 신호에 의해 구동되어져 왔다. 새로운 증거는, 뇌는 보상과 처벌을, 적어도 2개의 타입의 뉴런들에 의해 표현되는 두 개의 별개의 카테고리로 다룬다는 것을 암시한다. 또한 뇌는 '대립' 표현들('opponent'representations)을 사용한다는 증거가 있다. 예를 들어, 도파민 뉴런은 보상 증거(evidence for reward)에 의해 활성화되고 반 보상 증거(evidence against reward)에 의해 억제되는 반면, 다른 뉴런들은 보상 증거 및 반보상 증거에 대해 반대 패턴의 반응을 보인다. 따라서 reward-ON(RON: 도파민(dopamine)), reward-OFF(ROFF),aversive-ON(AON), 및 aversive-OFF(AOFF) 등의 강화(reinforcement)를 구동하는 4 타입의 '기본 가치 신호(elementary value signals)'를 제안한다.본 발명은 이러한 네 개의 기본 가치 신호가 다양한 유효 강화 신호(effective reinforcement signals)를 만드 [0011]는 다양한 조합에서 어떻게 합해질 수 있는지를 설명한다. 예를 들어, 운동 뉴런의 활동은 RON 및 AOFF를 예측하고 나타내야 하기 때문에, 운동 뉴런은 RON+AOFF-ROFF-AON에 상응하는 유효 강화 신호(effective reinforcementsignal)를 수신해야 한다. 이와는 대조적으로, 엄격히 감각적인(strictly sensory) 뉴런은, 그것이 좋은 소식(good news)을 주는 것으로 현저하든지 또는 나쁜 소식(bad news)을 주는 것으로 현저하든지에 관계없이, 외부세계의 '현저한(salient)' 부분에 반응해야 한다. 따라서 그러한 뉴런에 대한 유효 강화 신호는 RON+ROFF+AON+AOFF가 되어야 한다. 여기서 '현저(salient)하다'는 것은, 예를 들어 자극(stimulus) 등이, 보상 증거(evidencefor reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 또는 반 처벌 증거(evidence against punishment) 중 하나 이상과 관련이 있을 때, 그 자극(stimulus)이 '현저(salient)하다'라고 표현하며, 이하에서도 그와 같은 경우에, '현저(salient)하다'라는 용어로 사용하기로 한다.뇌의 각 뉴런은 8개의 서로 다른 타입 중 하나에 해당하고, 각 타입은 네 개의 기본 가치 신호(elementary [0012]value signal)의 서로 구별되는 합에 의해 정의된다는 것을 제안한다.과제의 해결 수단이와 같은 목적을 달성하기 위하여 본 발명에 따른, 정보 처리 소자(information processing element)를 이용하 [0013]여 컴퓨터에 의해 수행되는 정보 처리 방법은, (a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 복수의 기본 가치 신호(elementary value signals)를 수신하는 단계; (b) 상기 정보 처리소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 단계; 및 (c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 단계를 포함한다.상기 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거(evidence [0014]against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)일 수있다.상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소 [0015]화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있다.상기 정보 처리 소자는, 인공 뉴런에 해당하고, 상기 단계(a)는, 상기 정보 처리 소자에 의해, 각각 가중치와 [0016]활동을 갖는 상기 복수의 입력을 수신하는 단계를 포함하며, 상기 단계(b)는, (b1) 상기 정보 처리 소자에의해, 상기 가중치들을 이용하여 구한, 상기 입력들의 상기 활동들(activities)의 가중합(weighted sum)의 함수(function)를 이용하여 상기 정보 처리 소자의 출력을 계산하는 단계; 및 (b2) 상기 3개의 인수(3-terms)를 이용하는 Hebbian-type 학습 법칙(learning rule)의 사용을 통하여 상기 가중치들을 수정하는(modifying) 단계를포함하고, 상기 3개의 인수는, 입력의 활동(activity), 상기 입력들의 활동들의 가중합 및, 상기 강화 신호에해당하는 것을 특징으로 한다.상기 입력들의 하나의 부분집합(subset)은, 상기 정보 처리 소자의 이전 출력의 합에 상응하는 활동들 [0017](activities)을 가지며, 이로부터 상기 정보 처리 소자는 가치(value)를 예측하는 시간적 패턴을 생성하는 것을학습할 수 있다.상기 Hebbian-type 학습 법칙에서 사용되는 3개의 인수들(terms)은, 동시에 발생하지 않고, 시간적으로 분리되 [0018]며, 상기 정보 처리 소자 내에서 일리저빌리티 트레이스(eligibility traces)에 의해 연결되고, 이로부터 상기공개특허 10-2014-0109337-7-정보 처리 소자는, 인과적으로 관련된 이벤트 시퀀스를 학습함을 통하여 가치(value)를 예측하는 것을 학습할수 있다.상기 기본 가치 신호의 상기 활동들(activities)은, 예측 오차(prediction errors)에 상응할 수 있다. [0019]복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고, 상기 정보 처리 소자 각각은 8개의 타입 중 [0020]하나이며, 상기 8개의 타입은, 상기 각 기본 가치 신호에 대한 특정 가중치 값(weight values) 세트(set)를 가지고, 상기 가중치 값은, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment) 각각에 대하여 양수(+) 또는 음수(-) 값을 가지며, 상기 8개 타입의 각각은, ++++, ++--, --++, +---, -+--, --+-, ---+ 또는 +--+에 해당하고, 상기 정보 처리 소자 각각은 특정한 가치(particular aspect of value)를 표현하는 것을 학습할 수 있다.복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고, 상기 기본 가치 신호들은 수퍼바이저리 티칭 [0021]신호(supervisory teaching signal)의 방식으로 상기 망 외부에서 생성되거나, 또는 상기 망 내에서 정보 처리소자들 중 일부에 의해서 언수퍼바이즈드(unsupervised) 방식으로 생성될 수 있다.상기 기본 가치 신호의 상기 가중치는 양수 또는 음수 또는 0의 값을 가지며, 상기 기본 가치 신호의 적어도 두 [0022]개의 가중치는 0이 아닌 값을 가질 수 있다.본 발명의 다른 측면에 따르면, 정보 처리 소자를 이용하여 정보를 처리하는 장치, 상기 정보 처리 소자에 [0023]의해, 각각 가중치(weight)와 활동(activity)을 갖는 복수개의 기본 가치 신호(elementary value signals)를수신하는 수신부; 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 계산부; 및 상기 정보 처리 소자에의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 학습부를 포함한다. 상기 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거(evidence [0024]against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)일 수있다.상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소 [0025]화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있다.본 발명의 또 다른 측면에 따르면, 정보 처리 소자를 이용하여 정보를 처리하는 하나 또는 복수의 명령어 시퀀 [0026]스를 저장하는, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는, 하나 또는 복수의 프로세서에 의해 수행될 때, (a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 4개의 기본 가치 신호(elementary value signals)를 수신하는 단계; (b) 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 단계;및 (c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 단계를 포함하고, 상기 4개의 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidenceagainst punishment)이며, 상기 4개의 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는,보상을 최대화하거나 처벌을 최소화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있는것을 특징으로 한다.발명의 효과본 발명은, 다양한 강화 신호를 생성함에 의해 인공 지능을 만드는 것을 용이하게 하는 효과가 있다. [0027]도면의 간단한 설명도 1은 감각기관의 입력으로부터 운동기관의 출력까지의 4단계 처리 레벨(four levels of processing)에 분포되 [0028]어 있는 8개의 서로 다른 가치 표현의 형태로써, 본 발명에서 제안하는 뇌에서의 가치 조직(organization ofvalue)을 도시한 도면이다.도 2는 보상(reward)을 일례로서 사용하여, 본 발명에서 제안하는 대립 표현(opponent representation)에 대한뉴런 회로를 도시한 도면이다.도 3은 인공 뉴런에 있어서의 본 발명의 사용에 대한 일 실시예를 예시하는 블럭 다이어그램이다.공개특허 10-2014-0109337-8-도 4는 본 발명의 일 실시예에 따른, 컴퓨터에서 읽을 수 있는 매체를 예시하는 블럭 다이어그램이다.발명을 실시하기 위한 구체적인 내용본 발명(가치 학습, 'Learning value')을 설명하기 전에, 뇌에서의 가치의 조직(organization of value)을 먼 [0029]저 설명한다. 이는 본 발명이 도출되는 이론적 근거를 설명하는데 도움이 된다. 즉, 본 발명이 왜 유용하며, 어떻게 응용될 수 있는가를 이해할 수 있게 해 준다.- 뇌에서의 가치의 조직(organization of value) [0030]뇌의 기능은 보상을 최대화하고 처벌을 최소화하는 것이다. 이것은 앞서 언급한 기본 가치(elementary values) [0031]를 이용하여, 뇌의 기능은 RON과 AOFF를 예측하고 야기시키는 것이라는 말로써 설명된다. 각 뉴런은 외부의 어떤면(그 수용장(receptive field, 이하 'RF'라 한다)에 의해 결정되는 외부의 자극)에 대한 정보를 수신하고, 이는 미래의 가치(value) 예측에 관한 정보를 제공하도록 '선택'된다. 각 뉴런은 각각 자신에게 독특한(unique)RF와 자극을 갖지만, 각 뉴런은 도 1에 나타나 있는 8개의 '가치 표현형(value phenotypes)' 중의 하나로 분류된다는 것을 제안한다. 이러한 8개의 각 표현형은 4개의 기본 가치(elementary values)의 합에 해당한다. 예를들어, 척수 운동 뉴런(spinal motoneuron)은 RON 및 AOFF와 관련이 있지만, ROFF나 AON와는 관련이 없으며, 따라서RON+AOFF-ROFF-AON으로 나타내어질 수 있다. 이러한 네 개의 항은 뉴런의 RF를 결정하고 그 막 전압(membranevoltage)을 구동하는 흥분 시냅스 입력(excitatory synaptic input)과 관련있는 미래 가치(future value)의 예측을 설명한다. 운동 뉴런은 접근(RON)과 회피(AOFF)와 관련있는 행위(action)를 발생시키는데, 이는 운동 뉴런은RON을 예측하는 흥분 시냅스와 AOFF를 예측하는 다른 시냅스들을 수신하기 때문이다. ROFF 또는 AON을 예측하는 어떠한 시냅스라도 제거될 수 있다.대부분의 초기 감각기관 뉴런(즉, 청각, 시각, 촉각)들은 보상 또는 처벌에 대한 선호하는 관련성을 가지고 있 [0032]지 않다. 그럼에도 불구하고, 그 뉴런들은 생물학적으로 중요한, 즉, 현저한(salient) 정보를 제공하는데 그것은 RON+ROFF+AON+AOFF에 해당한다. 다운스트림 뉴런(downstream neuron)은 현저성(salience)을 보상과 처벌의 카테고리로 구별하며, 보상과 처벌은 이후 ON과 OFF로 더 세분된다고 제안한다. AON과 ROFF과 관련있는 신호는 필터링아웃(filtered out)되지만, RON 및 AOFF는 운동기관 영역(motor areas)으로 수렴된다. 이에 따라 감각기관으로부터 운동기관까지는 총 4 레벨의 가치 표현이 있게 된다(도 1).이러한 4 레벨(four levels)을 레벨 1의 운동(motor, action)으로부터 레벨 4의 현저성(salience)으로 표기하 [0033]는 것이 유용하다. 운동 조직이 가장 먼저 발달하는데, 운동 조직은 대부분의 감각 영역 이전에 진화된 것으로생각되며, 따라서 필수불가결한 조직이다. 가치(value) 레벨 2, 3 및 4의 뉴런들은 그들이 RON+AOFF를 예측하고야기시키는 목적에 기여할 때에만 유용하다. 그러나 감각 뉴런은 어떤 레벨에서도 발견될 수 있다. 어떤 주요감각 뉴런들(예를 들면, nociceptors)은, 특히 RON 또는 AON 등과 관련하여서는 비교적 명확한 정보를 가지고 있고, 따라서 레벨 2에 있게 된다.척수 운동 뉴런(spinal motoneurons)의 흥분을 직접적으로 일으키는 자기감수체(proprioceptors)는, 먹이 [0034](prey)를 향하여 달리든지(RON) 또는 약탈자(predators)로부터 도망가든지(AOFF) 간에 동일한 방식으로 활성화되며, 따라서 상위의 감각 레벨로 매핑되지만 운동 뉴런들과 같이 가치(value) 레벨 1으로 분류된다.도 1은 뇌에서의 가치(value) 조직을 도시하고 있다. 뇌 전체에서 각 뉴런은 8개의 가치 표현형(value [0035]phenotypes) 중의 하나인 것으로 제안된다. 하나의 뉴런의 표현형(phenotype)은 그 뉴런의 흥분 수용장(RF)에의해 결정되는데, 이는 네 개의 기본 가치(elementary value)인 reward-ON(RON), reward-OFF(ROFF), aversive-ON(AON), aversive-OFF(AOFF)의 합에 의해 나타내어진다. 운동 출력에서 가장 먼 레벨 4에서는, 뉴런들이 중립적이지만 중요한(\"현저한(salient)\") 감각 자극을 표현한다. 이들은 레벨 3에서, 보상(reward)과 처벌공개특허 10-2014-0109337-9-(punishment)이라는 대체적으로 독립적이고 병행적인 카테고리로 분화된다. 이들은 레벨 2에서 ON과 OFF로 더욱나누어지고, 이후 레벨 1의 운동 뉴런에서 '행위(action)'를 발생시키기 위해 RON과 AOFF가 모이게 된다. 하나의뉴런의 가치 표현형은 수학식 2의 3-인수(3-terms) Hebbian rule에 의해 형성되고, 수학식 2에서 유효한 순(net) 가치(value) 신호는 네 개의 기본 가치 신호(elementary value signals)의 합(수학식 3)으로 나타내어지는데, 8 개의 표현형 각각마다 하나의 합이 해당한다. (여기서 나타난 각 합은, 수학식 3의 가중치가 '+1' 또는'-1'인 경우에 대한 것이다) 각 기본 가치 신호는 적어도 하나의 뉴런 조절기(neuronmodulator)에 의해 나타내어지며, 도파민(dopamine)은 RON을 나타낸다. 뉴런의 '가치 유전자형(value genotype)'은 각각의 뉴런 조절기가어떻게 자기의 RF를 형성하는지를 결정하는 적어도 네 개의 GPCR들의 표현과 상응한다. 예를 들어, 도파민 D1수용체(dopamine D1 receptors)는 +RON 표현형을 발생시키고, 반면에 D2 수용체는 -RON 표현형을 발생시킨다. 가치(value)의 네 레벨은 감각기관 대 운동기관의 관계로 단순화되지 않는다. 대부분의 운동 뉴런은 레벨 1에 있는 반면, 레벨 2에서와 같이 특히 접근(RON) 또는 회피(AOFF)에 특정적으로 관계하는 운동 뉴런과 근육과 같은 특별한 경우가 있다. 감각 뉴런은 네 개의 레벨 중 어디에도 있을 수 있고, 낮은 레벨 뿐만 아니라 높은 레벨에도매핑될 수 있다.- 왜 보상과 처벌에 대하여 독립적인 표현(representation)이 있게 되는가? [0036]보상과 처벌은 생물학적 적합성의 관점에서 서로 역(opposites)에 해당하며, 긴 시간척도(timescales)에서 서로 [0037]반 상관관계(anti-correlated)에 있다. 그러나 RF 형성과 관련된 초 단위 이하의 짧은 시간척도에서, 그들의 감각 성분은 빛과 어둠의 의미에서와 같이 역(opposites)이 아니다. 뇌는, 시각 조직에서 분리된 물체를 인식하는것을 배우는 것과 동일한 방식으로 좋은 것과 나쁜 것(good and bad)을 두 개의 별개의 카테고리로 인식하는 것을 배운다고 제안된다. 자극들이 외부 세계에서 함께 일어나는 경향이 있을 때, (뇌 안의 어디에선가) 하나의뉴런은 그러한 자극들을 동일한 '대상'으로 인식하고 반응한다. 그러나 자극들이 상관관계 없이, 통계적으로 독립적인 방식으로 발생한다면, 그러한 자극들은 서로 다른 뉴런들에 의해 독립적으로 표현된다. Hebbian 학습을통해서 하나의 뉴런은 그 자신의 활성화를 예측하고 일으키는 흥분성 시냅스 입력들을 선택한다. 이것은 서로관련된 시냅스 전부(presynaptic)의 입력을 선택하고, 관련되어 있지 않은 입력들은 제거해 버리는 효과를 갖는다. 잘 알려진 예는 시각피질(visual cortex)의 뉴런에 의한 두 눈의(binocular) RF의 형성이다. 앞을 보는 두눈을 가진 동물에 있어서 자연적인 환경 하에서는, 각 눈은 동일한 '사물'을 거의 동시에 보며, 따라서 한쪽 눈으로부터의 활성화된 시냅스 입력은 다른 눈으로부터의 활성화된 입력을 예측하고, 두 눈은 주요 시각피질의시냅스 후부(postsynaptic)의 뉴런을 동시에 흥분시킨다. 이러한 예측에 있어서의 관련성은 각 눈으로부터의 시냅스 입력을 더 강하게 하며, 그러한 관련성이 없다면 두 눈의(binocular) RF는 형성되지 않는다. 만약 두 눈이독립적으로 움직인다면, 각 눈으로부터의 정보는 두 개의 외눈 뉴런들(two monocular neurons)에 의해 독립적으로 표현되게 될 것이다. 동일한 원리가 수많은 다른 사물 또는 카테고리-선택적(category-selective) 뉴런들의형성을 설명하는 것으로 생각된다.이것은, 보상과 처벌이 일관되게 상호관련된 것이 아니고 서로 예측할 수 있는 것이 아니라면(즉, 통계적으로 [0038]독립적이라면), 보상과 처벌은 적어도 두 가지 형태의 뉴런에 의해 독립적으로 표현된다는 것을 의미한다. 여기서 관심있는 상호관련성(correlation)은, RF 형성에서 중요한 초 이하 시간척도(sub-second timescale)에 관한것인데, 이것은 진화적(evolutionary) 또는 성장적(developmental) 시간척도에 있어서도 지속적으로 적용된다(즉 위에서 나타내었던 연구실 실험에서 만들어진 것과 같은 일시적인 종류의 상호관련성이 아니다). 우리는 단지 관련있는 통계적 관계성에 대하여 추측할 뿐이지만, 보상과 처벌 간의 상호관련성은 약한 경향이 있는 것으로 보이고, 음성적(negative)이라기 보다는 양성적(positive)이라는 것은 더욱 신빙성이 있어보인다. 예를들어, 외상수용기(nociceptor)의 활성화와 음식, 물 또는 섹스와 관련된 자극 간에는, 적어도 작은 시간척도에있어서는 강한 상호관련성이 없는 것으로 보인다. 천연음식에서 설탕과 쓴 화학약품의 양은 일반적으로 강한 상호관련성이 없는 것으로 보인다. 달콤씁쓸한 것은 일반적인 것(rule)이 아니나, 그렇다고 예외적인 것은 확실히아니다. 그러나 보상 자극(reward stimuli) 간(예를 들어, 음식을 보는 것과 냄새 맡는 것)에나, 혐오 자극(aversive stimuli) 간(예를 들어, 충돌이 몸의 여러 부분에 동시에 해를 가하는 것)에는 강한 양성적(positive) 상호관련성이 있는 경향이 있다고 보인다. 이러한 양성적 상호관련성은 어떤 뉴런들에서의 보상 RF와 다른 뉴런들에서의 처벌 RF의 Hebbian development를 증진시킨다(도 1).공개특허 10-2014-0109337-10-어떤 보상 자극과 혐오 자극 간에는 강한 양성적 상호관련성이 있다고 할지라도, 뇌는 보상과 처벌을 별개로 표 [0039]현하기 위해 일한다는 것이 제안된다. 단지 예시를 위해서, 우리는 좋은 것과 나쁜 것(good and bad)에 대한 정보는 일관적으로 각각 왼쪽 눈과 오른쪽 눈을 통하여 온다고 가정해 볼 수 있다. 비록 왼쪽 눈과 오른쪽 눈으로부터의 입력의 활동은 매우 상관 관계가 있지만, 두 눈에 의한(binocular) 수용장(receptive field, RF)은 형성되지 않는다. 이것이 아래에서 더욱 자세히 설명할, 3개의 인수(terms)에 기반한 Hebbian 학습 법칙(learningrules)이 의미하는 바이며, 별개의 '보상' 뉴런과 '처벌' 뉴런을 생산하도록 작동하는 것이다.레벨 2에서의 ON과 OFF로의 세분화는 필연적으로 레벨 3에서의 보상과 처벌로의 세분화를 일으킨다. 보상과 처 [0040]벌의 카테고리는, 보상 자극과 혐오 자극의 존재 및 부존재 모두와 자연스럽게 관련된다. 예를 들어, 돈에 대한일반적인 개념은 '보상' 카테고리 내에 있으나, 그것은 이득(RON)과 손실(ROFF)에 거의 동등하게 관련되어 있다.레벨 3의 다른 임의의 '보상 뉴런' 처럼, '돈(money) 뉴런'은 RON+ROFF로 나타내어질 수 있는데, 레벨 2는 두 개의 형태의 돈 뉴런을 포함하며, 하나는 이득(RON)에 해당하고 다른 하나는 손실(ROFF)에 해당한다.- 가치의 대립 표현(Opponent representation of value) [0041]앞에서 설명한 원리들로부터, 보상과 처벌은, 그것들이 일반적으로는 서로 반 상관관계(anti-correlated)에 있 [0042]지 않기 때문에 대립자(opposites)로 다루어지지 않는다는 것을 도출할 수 있다. 그러나, 일단 보상과 처벌이두 개의 카테고리로 구분되었다면, 그 각각에 대한 우호적(for) 및 대항적(against) 증거들(evidence) 간의 대립 관계(opponent relationship)가 있게 된다. 대립관계의 중요성은 시각 계통에서 비교적 잘 이해되고 있는데,여기서 빛과 어둠은 자연적 대립자(opposites)이고, 두 개의 구별되는 형태의 뉴런, 즉 ON과 OFF에 의해서 표현된다.빛의 세기는 단일 ON 타입 뉴런에 의해 표현되지만, 대립 표현(opponent representation)이 환경으로부터 더욱 [0043]많은 정보를 추출해낸다. 빛에 대한 우호적(for) 및 대항적(against) 증거들(evidence)(또는 좋은 것과 나쁜 것(good and bad)) 간에 반 상관관계(anti-correlation)가 있다는 것은 명백하다. 그러나 반 상관관계는 완전한것이 아니라는 것을 인식하는 것이 중요하다. 이는 시각 계통에서, 공간 수용장(spatial RFs)과 시냅스 흥분의타이밍이 ON 및 OFF 뉴런에서 완전하게 매치되지 않는다는 것 때문이다. 보상의 경우 반 상관관계는, 보통 특정시간에 RON 또는 ROFF의 증거(evidence)를 공급하는 하나의 중요한 자극이 존재하기 때문에 발생한다. 그러나 드문 경우에 동시에 RON과 ROFF 뉴런을 활성화시키는, 모순된 증거(evidence)를 공급하는 둘 또는 그 이상의 자극이있을 수 있다. 반 상관관계 때문에, OFF의 증거는 또한 반 ON(against ON)의 증거이기도 하며, 그 반대의 경우도 마찬가지인데, 이것은 두 개의 세포 타입 간에 상대방을 억제하는(opponent inhibition) 것의 토대를 형성한다. 도 2는 보상과 혐오 모두의 대립 표현에 적용하도록 제안된 회로를 보여주는데, 특히 도파민과 다른 기본가치(elementary value) 뉴런을 제어하는 '비판(critic)' 회로에 적용된다. 이것은 시상(thalamus)의 측면 슬상핵(lateral geniculate nucleus)에 상응하는 것으로, 뉴런간의 모든 연결은 예측을 중재한다는 원리에 따르도록 의도되었다. 여기서 가치 표현형(value phenotype)은 그것의 흥분(excitatory) RF 들에 의해 정의되었고, 어떤 대립 억제(opponent inhibition)도 수용하지 않는 뉴런이 존재할 수 있다는 점을 주목해야 한다.도 2는 대립(opponent) 프로세스를 성립시키는 제안된 연결성(connectivity)을 예시한 도면이다. 이러한 연결성 [0044]은 LGN(lateral geniculate neucleus)에서 보여주었고, 어떠한 대립 표현(opponent representation)에도 적용할 수 있다.여기서는 그것이 RON 및 ROFF에 적용된다. RON 또는 ROFF에 더하여, 뉴런들은 흥분을 일으키거나(excitatory)(e, [0045]화살표), 억제하거나(inhibitory)(i, 연결선 끝부분에 짧은 수직선분을 가지는 연결선) 또는 조절하는(modulatory)(m) 출력을 가진다. 억제(inhibitory) 시냅스는 굵은 선으로 표시된 'opponent'와 얇은 선으로 표시된 'homeostatic'으로 나누어질 수 있다. 뉴런은 동일한 가치 표현형(유사한 RF)을 갖는 다른 뉴런들로부터공개특허 10-2014-0109337-11-흥분(excitation) 및, 항상성을 유지하는 억제(homeostatic inhibition)를 수용하며(receive), 대립 표현형(opponent phenotype)을 갖는 뉴런으로부터 대립 억제(opponent inhibition)를 수용한다(receive). 따라서 RON뉴런은 보상에 대한(for) 증거(RONe)와 보상에 반한(against) 증거(ROFFi)를 합한다. 대립 억제(opponentinhibition)는 보통 시냅스 흥분 없이 발생하고 과분극(hyperpolarization)을 구동하는 반면, 항상성을 유지하는 억제(homeostatic inhibition)는 보통 흥분과 동시에 발생하고 그에 대항하여 행동하여 흥분의 항상성 수준을 유지시키는 것을 돕는다. 항상성을 유지하는 억제(homeostatic inhibition)는 '예측 오차(predictionerror)'에서 '예측'을 중재한다(mediate). 비록 여기서의 방법이 보상에 관련된 해부학이나 생리학의 직접적 지식에 의해 영감을 받은 것은 아니나, 측면 고삐핵(lateral habenula)(R-OFFe), rostromedial tegmentalnucleus(R-OFFi), ventral tegmental area의 GABAergic(R-ONi) 뉴런들과 같은 어떤 뉴런들의 특정한 분류를 뒷받침하는 제한된 증거가 있다.- 가치 학습(Learning value) [0046]본 발명에서 설명한 강화 신호는 강화학습(RL)의 어떤 응용에도 유용하다. 여기서 설명하는 실시예는 인공 뉴런 [0047]네트워크를 포함한다. 인공 뉴런은, 어떤 주어진 순간에서도 그 입력(ui)들의 가중합의 함수인, 아날로그 또는디지털의 출력(υ)을 갖는다.수학식 1[0048]학습은 가중치(wi)의 변화를 통해 일어난다. 가장 잘 알려진, 생물학적으로 설득력있는 관련 학습 법칙은 2개의 [0049]인수(terms)를 포함하는 Hebbian rule인데, 그 2개의 인수는 시냅스 전부 활동(presynaptic activity(u))와 시냅스 후부 활동(postsynaptic activity(v))이다. 기존의 2-인수(2-terms) Hebbian rule에 포함된 원리는, 시냅스 전부(presynaptic) 활동이 시냅스 후부(postsynaptic) 활동을 예측하고 야기시키면 흥분 시냅스(excitatorysynapse)가 강화 되어야 한다(가중치가 높아진다)는 것이다. 가치를 학습하기 위해, 만약 시냅스 전부(presynaptic) 활동(ui)이 시냅스 후부(postsynaptic) 활동(v)을 예측하고, 시냅스 전부 및 후부 활동의 동시성이 함께 수치 (V)를 예측한다면, 흥분 시냅스가 강화 되어야 한다. 가장 간단한 3-인수(3-terms) Hebbian rule은 수학식 2를 따라 시냅스 세기(strength of synapse)(w)를 수정한다.수학식 2[0050]이러한 학습 법칙(learning rule)을 통해, 뉴런은 가치(value)를 예측하는 외부세계의 일부와 상응하는 RF를 발 [0051]전시킨다. 수학식 2의 형태는 그것이 Hebbian rule의 가장 선호하는 형태를 대표하기 때문에 선택된 것이 아니라, 많은 Hebbian rule 중에서 가장 간단한 것으로서 선택된 것임에 주의해야 한다.강화 신호(V)가 '보상 예측 오차(reward prediction error)'에 상응하는 모델들이 개발되었는데, 이는 뇌 안에 [0052]서 도파민에 의해 중재된다. 본 발명에서 제시되는 새로운 기여는, 유효한(effective, 순(net)) 강화 신호(V)는단일 신호가 아니라, 네 개의 기본 가치 신호(elementary value signals)의 합이라는 것이다.공개특허 10-2014-0109337-12-수학식 3[0053]수학식 3의 가중치(w)는 순(net) 강화 신호(V)에 기여함에 있어서의 각 기본 가치 신호의 세기(strength)를 나 [0054]타낸다. 가장 간단한 실시예에서, 그러한 가중치들은 -1 또는 +1, 또는 어떤 경우에는 0이다. (이러한 가중치들은 수학식 1과 2에서의 가중치들과 혼동해서는 안되는데, 이는 뉴런으로의 흥분성 또는 억제성 입력의 세기(strength)를 나타내는 수정 가능한 가중치이다) 기본 가치 신호(elementary value signals) 그 자체는 양수(positive number)로 표현되고, 뇌에서의 신경전달물질의 농도(concentration)에 상응한다. 어떤 특정 순간에서의 유효 강화 신호(effective reinforcement signal)(V)는 양수 또는 음수일 수 있다. 하나의 기본 가치 신호(elementary value signal)와 그 가중치의 곱은, 시냅스 가까이에 위치하는 G-단백질 연결 수용체(G-proteincoupled receptor, GPCR)의 활성화에 상응한다. GPCR의 특정한 형태와 그 관련된 신호전달체계(signaltransduction cascade)는, 그것의 활성화가, 이하에서 도파민에 대해 설명하는 바와 같이, 활성화된(co-activated) 시냅스의 강화(w>0) 또는 약화(w<0)에 기여하는지 여부를 결정한다.도파민의 효과는 선조체(striatum)의 중간 가시 뉴런(medium spiny neurons, MSNs)에서 가장 면밀하게 연구되 [0055]어져 왔다. 어떤 MSN들은 D1 수용체를 표현하는 반면, 다른 뉴런은 D2 수용체를 표현하며, 이들 두 GPCR들은 결합되어 독특한 신호 전달 통로(transduction pathway)가 된다. D1과 D2 MSN은 아래와 같이 가치(value)를 표현한다.D1 MSNs V=RON-ROFF-AON-AOFF [0056]D2 MSNs V=AOFF-AON-RON-ROFF [0057]D1 MSN(pre+post)에 대한 흥분 시냅스의 활성화에 이어 로컬 D1 수용체의 활성화가 뒤따를 때, D1 수용체는 양 [0058]의 순 가치(positive net value) V에 기여한다. 그 동일한 시냅스의 활성화 뒤에 다른 세 개의 기본 가치 신호들 중에 어느 것이라도 뒤따를 때에는, 그것들의 GPCR은 순 가치(net value)에 대해 음의 항(negative terms)을부가할 것이다. 그러나, D2 MSN에 대한 D2 수용체의 활성화는 순 가치(net value)에 음의 값으로 기여할것이다. 따라서 D1 및 D2 수용체는 각각 함께 활성화되는 시냅스들을 강화하고 약화시키게 될 것이다.이러한 법칙들의 결과로, D1 MSN은 그 입력(또는 자극)을 결정하는 RON RF를 가지며, 그 출력(또는 행 [0059]동,action)은 접근 행위(approach behavior)에 상응한다. D2 MSN은 AOFF RF를 가지고, 회피 행위(avoidancebehavior)를 중재하는데 기여한다. 이러한 제안은 D2 MNS가 'no go'라는 억제 행위(suppressing behavior)를이루는데 반해 D1 MSN은 'go' 반응을 이루도록 하는 종래의 모델과 대조를 이룬다. 매우 다양한 MSN들이 매우다양한 접근 및 회피 행위를 성립시키지만, 회피 행위의 다수는 탈출(escape) 또는 방어(defense) 등의 능동적인(active) 회피보다는 물러남(withdrawal)과 '동작 멈춤(freezing)'과 같은 '수동적(passive)'인 회피에 해당한다. 수동적 회피는 운동능력(locomotion)을 억제하지만, 그럼에도 불구하고, 혐오 자극과 보상 자극 모두에대한 낮은 기대를 동반하는 동기부여 없는 'no go' 상태와는 반대되는, 혐오 자극에 대한 기대(expectation)를자연스럽게 동반하는 동기부여와 운동 활성화를 포함한다. D2 MSN이 회피를 중재한다는 가설은, D2 MSN의 선택적 활성화가 동작 멈춤(freezing)과 때로는 탈출(escape)을 끌어낸다는 최근의 실증에 의해 뒷받침된다. 어떤AOFF 뉴런들의 활성화가 긍정적인 정서(positive affect)(예를 들어 '안전')를 증진시키며 ('음성적(negative)')강화에 기여한다고 기대될 수 있지만, AOFF 신호는 혐오적 상황 내에서만 자연스럽게 발생하며, AON의 위험과 관련이 있을 수도 있다는 것을 주의해야 한다.GPCR을 통한 네 개의 가치 신호(value signals)의 통합은, 막 전압을 통한 전기신호의 합이 아니라 신호 전달 [0060]공개특허 10-2014-0109337-13-통로(signal transduction pathways)에 의존하는 통합의 새로운 메커니즘일 수 있다. GPCR은 다양한 시냅스에있어서 Hebbian 시냅스 유연성(synaptic plasticity)를 제어한다는 것이 증명되었고, 적어도 두 타입의 GPCR 간의 상호작용은, 선조체(striatal) MSN에서의 도파민 수용체를 포함하는 일부 단일 시냅스에서의 시냅스 유연성(synaptic plasticity)을 조절하는 것으로 증명되었다.제안된 방법은 뇌 전체에 걸쳐서 최소 네 개의 신경전달물질에 의해서 수행되며, 적어도 2개의 상이하게 결합된 [0061]GPCR들을 가지고, 적어도 총 8개의 GPCR을 가진다. 네 개의 GPCR들의 특정한 조합의 표현은 뉴런의 '가치 유전자형(value genotype)'을 결정할 수 있는데, 이것은 수학식 2와 3의 3-인수 Hebbian rules를 통하여 뉴런의 표현형(phenotype)에 영향을 미칠 수 있다(표현형을 결정하는 것이 아니라). 뇌의 특정 영역의 가치 표현형은 부분적으로 특정 가치 유전자형, 예를 들어 운동영역에서 RON+AOFF 등을 강제함에 의해 만들어진다. 그러나 특정 유전자형을 초기에 강제함이 없이도 유사한 결과를 잠재적으로 성취할 수 있다. 발달과정 동안, 독특한 가치 유전자형을 가지는 다양한 뉴런들이 동일한 뇌 영역에서 만들어질 수 있다. 예를 들어, 만약 그 영역으로의 시냅스입력이 확실하게 RON을 예측하지만, 다른 기본 가치 신호에 대해서는 그렇지 않은 경우라면, RON 유전자형의 시냅스 후부 뉴런들은 일군의 강한 흥분성 시냅스를 형성하고 살아남을 것이며, 반면에 다른 가치 유전자형의 뉴런들은 불충분한 일군의 시냅스를 형성하고 따라서 세포 자멸(apoptosis)을 통해 제거될 것이다.위에서 설명한, 그리고 도 1의 가치 표현형(value phenotypes)은 뉴런의 해부학적으로 정의된 RF의 특성이며, [0062]시냅스가 생성되고 파괴될 때만 천천히 변화한다. 뉴런의 기능적 RF는 그것의 시냅스 입력의 가중치에 의해 결정되는데, 무엇이 중요한지에 대한 현재의 기대에 의존하여 1초 이내에 변화할 수 있다. 이것은 빈약하게 이해되고 있는 뇌 피질에서의 탑-다운 방식의 선택적 주의(selective attention) 메커니즘을 통해서 뿐만 아니라,위에서 설명한 3-인수(term) Hebbian rule의 빠른 효과를 통해서도 일어날 수 있다. 그러므로 주어진 순간에서의 뉴런의 기능적 표현형은 그것의 해부학적으로(또는 유전학적으로) 정의된 가치 표현형에서 표현되는 기본 가치(elementary values)들의 부분집합으로 구성된다. 예를 들어, 현저성 뉴런(salience neuron)은 일시적으로네 개의 기본 가치의 어떠한 부분집합을 선택할 수도 있고, 일련의 접근(approach) 행위의 상황 속에서 운동 뉴런은 RON의 기능적 표현형을 갖기 위해 그것의 AOFF 흥분성 시냅스를 억제할 수도 있다.위에서 설명한 강화 신호는 (다른 뉴런들로부터) 뉴런으로의 시냅스 입력의 세기를 수정하도록 작용할 수 있는 [0063]한편, 동일한 타입의 3-인수 Hebbian 학습 법칙(rule)은, 본 발명자가 앞에서 설명하였듯이, 동일한 뉴런 내에서 전압에 의해 조정되는(voltage-regulated) 이온 채널의 세기를 수정하는 기능을 또한 수행할 수 있다. 뉴런은, 그 전압과 시간 의존성에 있어서 서로 다른 전압에 의해 조정되는(voltage-regulated) 다양한 이온 채널들을 표현한다. 각 짧은 순간에, 그러한 이온 채널의 활동은 '메모리'(즉 동일한 뉴런의 과거 출력의 합)에 의존하며, 여기서 과거의 특정 기간은 이온 채널의 특정한 종류에 따라 달라지게 된다. 따라서 이러한 이온 채널로의 입력은 그 뉴런의 출력이지만, 이 이온 채널들은 또한 뉴런에 입력을 공급한다. 예를 들어, 간단한 실시예에서, 인공 뉴런의 출력은 막 전압에 해당하고, 그것은 시간 t-1에서의 출력에 대응하는 하나의 입력(voltage-gated 이온 채널의 한 subtype), 시간 t-2 에서의 출력에 대응하는 또 하나의 입력 등을 가질 수 있고, 이들 각각은 자기의 수정 가능한 가중치를 갖는다. 그것은, 이러한 타입의 이온 채널에 대하여 관련된 Hebbian 학습(learning)을 적용함으로써 뉴런이 어떻게 시간적 패턴을 생성하는 것을 학습할 수 있는지를 이미 설명하였다.또한 뉴런이 외부 세계(world)의 '중요한' 면(aspect)을 예측하기를 배우기 위해 Hebbian 학습 법칙(rule)이 어떻게 강화 항(reinforcement term)을 포함해야 하는지를 설명했다. 본 발명은 강화 신호가 어떻게 만들어지는지를 제안한다.수학식 2에 의해 표현된대로, 입력의 가중치를 변화시키기 위하여, 세 개의 활동(three activities)이 적어도 [0064]근사적으로라도 시간적으로 일치해야 한다. 본 발명은 어떤 특정한 시간적인 관계도 명시하지 않는다. 그러나인과율의 법칙을 수행하기 위해서, 시냅스 전부의 활동(presynaptic activity) (u)는 시냅스 후부의 활동(postsynaptic activity) (v)이 뒤따라야 하는데(또한 시냅스 후부의 활동을 유발시키는 것을 도움), 이는 강화신호(reinforcement signal) (V)가 뒤따르게 된다(또한 강화신호를 유발시키는 것을 돕게 된다). 이를 성취하기위해서, 적어도 두 개의 'eligibility traces'를 사용하는 것이 유용하다. 첫번째 eligibility trace는 시냅스공개특허 10-2014-0109337-14-전부 활동(presynaptic activity)에 관한 점차적으로 감쇠하는 메모리에 해당한다. 시냅스 후부 활동(postsynaptic activity)과 첫번째 eligibility trace의 곱은 두번째 eligibility trace를 발생시키는데, 이것은 시냅스 전부와 후부 활동의 동시성에 관한 점차적으로 감쇠하는 메모리에 해당한다. 이로부터, 가중치의 변화는 두번째 eligibility trace와 강화 신호(reinforcement signal) (V)의 곱(product)에 의존한다.인공 뉴런의 출력은 아날로그(예를 들어 전압) 또는 디지털(예를 들어 뇌파에서 스파이크, spike) 변수가 될 수 [0065]있다. 생물학적 신경계에서, 디지털 출력은 먼거리 간의 통신에 유리한데, 이는 디지털 출력이 전송될 때 세기가 약해지지 않기 때문이다. 그러나 어떤 뉴런들은 원거리 통신이 필요하지 않고, 인공 뉴런 망에서는 원거리통신이라고 하여 반드시 신뢰성이 문제되는 것도 아니다. 다른 조건이 동일하다면, 아날로그 출력은 훨씬 많은정보를 운반한다.본 발명은 기본 가치 신호의 정교화를 명시하거나 한정하고 있지 않다. 강화 신호(reinforcement signal)가 예 [0066]측 오차(prediction error)에 상응하는 것이 바람직한 것으로 알려져 있고, 정확한 예측이 당연히 부정확한 예측보다 바람직하다. 도파민 뉴런들은 매우 일반적이고 정교한 방법으로 RON 신호를 보낸다. 그것들은 예를 들어음식과 관련된 자극 또는 특정한 감각 모드에 특화되어 있기보다는 보상(reward)의 모든 면에 민감한 것으로 보인다. 게다가, 도파민 뉴런들(또는 그것들의 구심 뉴런, afferent neurons)은 매우 정교한 예측을 수신하고, 이러한 예측은 도파민 뉴런들이 정보 이론 원리에 따라 매우 효율적이고 정확한 방식으로 보상 가치(rewardvalue)를 표현하도록 해준다. 도파민 뉴런들은 일반적인 방식으로 보상을 표현할 수 있고, 정교하고 정확한 예측을 하는데, 이는 그들이 대뇌의 매우 큰 부분으로부터 고도하게 처리된 정보를 수신하기 때문이다. 그러므로영장류의 뇌에서 도파민 뉴런들은 RON 강화 신호에 대한 이상형에 가까운 어떤 것을 표현한다. 그러나, 본 발명에 있어서, RON 강화 신호는 보상에 우호적인 증거에, AOFF 강화 신호는 혐오 자극에 적대하는 증거에 각각 상응할 수 있으며, 동일한 원리가 다른 두 개의 기본 강화 신호에도 적용된다.- 도 3에 도시된 실시예의 요약 [0067]인공 뉴런 31은 아날로그 또는 디지털의 출력(v)를 가지는데, 이것은 어떤 주어진 순간에도 그 입력(u)의 가중 [0068]치 적용 합(weighted sum)(32)의 함수이다(수학식 1). 학습은 3-인수 Hebbian-type 학습 법칙에 따라 가중치33을 수정함에 의해 진행되며, Hebbian-type 학습 법칙의 가장 간단한 형태는 수학식 2에 나타나 있다. 유효 강화 신호 34(V)는 네 개의 기본 가치 신호의 가중 합이며, 인공 뉴런 31 내의 기본 가치 신호로부터 계산된다.네 개의 기본 가치 신호는 보상 증거 35, 반 보상 증거 36, 처벌 증거 37 및, 반 처벌 증거 38이다. 네 개의 기본 가치 신호(elementary value signal)는 인공 뉴런 31의 외부에서 생성되는데, 관리 신호(supervisorysignal) 방식으로 생성되거나, 또는 다른 인공 뉴런들에 의해 관리되지 않는 방식으로(in an unsupervisedmanner) 만들어진다(도 2). 이와 같은 언수퍼바이즈드 방식으로 생성될 경우는 네 개의 기본 가치 신호가 인공뉴런 망 내부에서 생성될 수도 있다. 인공 뉴런 31은 가장 선호되는 실시예를 나타낸다. 그러나 본 발명은 신경망을 포함하지 않는 다른 형태의 강화학습(reinforcement learning, RL)과 함께 사용될 수 있다. 본 발명의 핵심이 되는 특징은, '강화 발생기(reinforcement generator)' 모듈 39에 의한 유효 강화 신호(effectivereinforcement signal)(V)의 발생시키는 것이다. 네 개의 기본 가치 신호의 합은 가중치에 의해 결정되는데, 이가중치는 각 네 개의 신호에 대하여 양수 또는 음수가 될 수 있다. 이러한 가중치는 뉴런이 어떤 면(aspect)의가치를 예측하는 것을 학습할지를 결정할 수 있다. 도 1은 뇌에서 가치(value)의 일반적인 조직을 예시하고 있는데, 각 뉴런은 양수와 음수의 가중치들의 조합으로 서로 다른 8개 타입 중의 하나라는 것을 제시하고 있다.- 컴퓨터로 읽을 수 있는 매체 [0069]일 실시예에서, 정보 처리 소자(information processing element)는 클라이언트 40에 저장된 소프트웨어 프로그 [0070]램을 이용하여 생성된다. 도 4는 하나 또는 복수의 정보 처리 소자의 생성을 위한 예시적 구조를 도시하는 단순화된 다이어그램이다. 이 예시적 구조는 클라이언트 40(예를 들어 컴퓨터와 같이 클라이언트 장치로 작동하도록구성된 컴퓨팅 플랫폼)인데, 메모리 RAM 또는 자기 또는 광학 매체와 같이 컴퓨터로 읽을 수 있는 매체 41 및이와 연결된 전자 프로세서 42 등을 포함하도록 구성된 디지털 미디어 플레이어, 개인용 디지털 보조장치, 또는공개특허 10-2014-0109337-15-휴대폰 등이 가능하다. 프로세서 42는 컴퓨터로 읽을 수 있는 매체 41에 저장되어 있는 프로그램 명령어를 수행한다.부호의 설명31: 인공 뉴런 32: 입력의 가중치 적용 합 [0071]33: 3-terms Hebbian-type 학습 법칙 가중치34: 유효 강화 신호 35: 보상(reward)36: 보상(reward) 37: 처벌(punishment)38: 처벌(punishment)39: 강화 발생기(reinforcement genrerator) 모듈40: 클라이언트41: 컴퓨터로 읽을 수 있는 매체42: 프로세서공개특허 10-2014-0109337-16-도면도면1공개특허 10-2014-0109337-17-도면2공개특허 10-2014-0109337-18-도면3공개특허 10-2014-0109337-19-도면4공개특허 10-2014-0109337-20-"}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "강화학습(reinforcement learning)의 목적은, 에이전트(agent)로 하여금 보상을 최대화하고 처벌을 최소화하는 것이다. 이러한 두 가지는 전통적으로, 서로 같은 것으로서 결국 하나라고 인식되어져 왔고, 따라서 강화학습 모 델에서의 학습은, '가치(value)'의 단일 척도에 의해 보상과 처벌을 표현하는(represent) 단일 강화 신호에 의해 구동되어져 왔다. 본 발명은 보상과 처벌을 두 개의 분리된 카테고리로 표현하고, 각각에 대해 두 개의 대립되는 표현을 갖도록 하는 것이 유리하다는 것을 제안한다. 따라서 보상 증거(evidence for reward), 반 보상 증거 (evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)의 4개의 '기본 가치 신호(elementary value signal)'를 제안한다. 본 발명은 이러한 네 개의 기본 가치 신호가 다양한 유효 강화 신호(effective reinforcement signal)를 만드는 다양한 조합으로 어떻게 합해질 수 있는지를 설명한다. 뇌는 수치 표현의 총 8가지 타입을 학습하는 방법을 사용한다는 것을 제안한다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 신경망 모델 및 인공지능 분야와 관련된 것으로서, 특히 강화학습(reinforcement learning)과 관련이 있다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "강화학습(reinforcement learning, RL)이란, 기계 또는 동물 등과 같은 에이전트(agent)로 하여금 성공적으로 보상을 찾고 처벌을 피하게 하는 학습과 관련이 있다. 강화학습은, 에이전트에게 가치(value)를 가르치는 '가치 신호(value signal)'(또는 강화 신호(reinforcement signal))에 의존한다. 어떤 모델에서는, 이러한 신호는 ' 보상 예측 오차(reward prediction error)'라고도 한다. 보상과 처벌은 일반적으로 '좋은 것과 나쁜 것'과 같은 선상에서 서로 반대되는 것으로 인식되어져 왔고, 따라서 보상을 최대화한다는 것은 처벌을 최소화한다는 것과 동의어로 인식되어져 왔다. 강화학습 모델은 보상과 처벌을 동일한 척도로 나타내는 단일 가치 신호(single value signal)를 이용해왔는데, 이는 보상에 대하여는 양의 값을, 처벌에 대하여는 음의 값을 갖도록 한다. 따 라서 만약 가치 신호(value signal)가 예측 오차(prediction error)이면, 신호값은 예기치 못한 혐오 자극 (aversive stimulus)이 발생했거나 기대한 보상 자극(reward stimulus)이 발생하지 않은 순간에 음의 값을 갖 는다. 또한 신호값은 예기치 못한 보상이 발생했거나 또는 예상되었던 처벌이 발생하지 않은 경우 양의 값을 갖 는다. 여기서 제시하는 바는, 이러한 종류의 단일 가치 표현(single value representation)은 발생시키기 어렵 고, 많은 종류의 강화학습에 대해서 부적합하며, 뇌(brain)에 의해 사용되지 않는 방법이라는 것이다. 여기서 제안하고자 하는 것은, 만약 유효 가치 신호(effective value signal)가 네 개의 기본 가치 신호(elementary value signals)의 합으로 구성된다면 강화학습은 더욱 유연하고 실제적인 방법으로 수행될 수 있다는 것이다. 강화학습을 수행하기 위하여 가치(value)의 단일 표현(single representation)을 사용하는 종래의 접근법은 적 어도 네 가지 문제점이 있다. 첫째, 뇌는 가치(value)와 관련하여 엄청나게 다양한 정보를 갖고 있고, 그것이 어떻게 하나의 신호로 통합될 수 있는지는 분명하지 않다. 그 문제는, 다수의 형태의 가치 신호(value signal) 들이 충분하다면, 적어도 감소시킬 수는 있다. 둘째, 단일 뉴런의 활동(activity)은 어떤 자극에 대하여 우호적 증거(evidence for stimulus) 또는 적대적 증거(evidence against stimulus)를 나타내지만, 우호적 증거와 적 대적 증거를 동시에 표현할 수는 없다. 고도의 정확한 표현을 하기 위해서, 어떤 뉴런들은 빛의 세기와 같은 자 극에 대한 우호적 증거에 의해 활성화되지만, 반면에 다른 뉴런들은 동일한 자극에 대한 적대적 증거에 의해 활 성화된다. 이것은 적어도 두 개의 '대립되는(opponent)' 강화 신호(reinforcement signal)를 갖는 것이 유리하 다는 것을 암시한다. 세번째이면서 가장 중요한 것은, 뇌는 다양한 가치 표현(value representation)을 필요로 한다는 것이다. 예를 들어, 운동기관 영역(in motor regions)에서, 시냅스의 활동이 보상(reward)을 예측한다면, 그 시냅스는 강화되어야 하며, 처벌(punishment)을 예측한다면 약화되어야 한다. 그러나 어떤 초기 감각기관 영역에서는, 시냅스의 활동이 보상에 관련되어 있든지 또는 처벌에 관련되어 있든지 간에 그 시냅스는 강화되어야 하는데, 그 이유는 '좋은 소식(good news)'과 '나쁜 소식(bad news)'은 모두 중요하므로 무시되어서 는 안되기 때문이다. 따라서 감각 뉴런(sensory neuron)은 '현저성(salience)'을 표현하고, 운동 뉴런(motor neuron)은 '보상(reward)'을 표현한다. 그러므로 뇌는 강화학습을 위해 여러 타입의 가치 신호(value signal)를 필요로 한다. 여기서 '현저성(salience)'이란, 예를 들어 자극(stimulus) 등이, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 또는 반 처벌 증거 (evidence against punishment) 등과 연관될 때, 그 자극 등이 현저성을 갖는다고 하며, 이하에서도 이를 이와 같이 '현저성(salience)'이라는 용어로 표현하기로 한다. 넷째, 실험적 증거는, 실제로 뇌는 강화학습을 위한여러가지 가치 신호(multiple value signals)를 사용한다는 것이다. 도파민(dopamine)은 '조절하는(modulatory)' 신경전달물질로서 중뇌(ventral midbrain)에서 뉴런들의 세포체에 서 방출되고, 양성 강화(positive reinforcement)를 중재하는 것으로 알려져 있다. 종래의 증거는, 도파민 뉴런 은 기대했던 것보다 좋은 '어떤 것에 의해서' 활성화되며, 예상보다 나쁜 '어떤 것에 의해서' 억제된다는 제안 을 지지한다. 이러한 근거에서, 도파민 뉴런은 강화학습을 구동하는 '보상 예측 오차(reward prediction error, RPE) 신호를 보내어, 뇌에 무엇이 좋고 무엇이 나쁜지를 가르친다는 것이 제안된 바 있다. 그러나, 곧 발표될 새로운 실험은, 도파민 뉴런은 보상 증거(evidence for reward)에 의해 활성화되고, 반 보상 증거(evidence against reward)에 의해 억제되나, 혐오감(처벌)에는 민감하지 않다는 것을 보여준다. 따라서 보상과 처벌은 뇌 에 의해서 두 개의 별개 카테고리로 표현되는 것으로 보인다. 혐오감에 대한 도파민 뉴런의 둔감성(insensitivity)은, 다른 뉴런이 그러한 혐오감을 표현해야 한다는 것을 강 력하게 시사한다. 그러나, 반 보상 증거(evidence against reward)에 의해 활성화되고 보상 증거(evidence for reward)에 의해 억제되는, 도파민 뉴런과는 반대 극성으로 보상을 표현하는 뉴런들이 있다는 강력한 증거가 있 다. 따라서, 여기서 reward-ON(RON), reward-OFF(ROFF), aversive-ON(AON), aversive-OFF(AOFF) 등으로 표기되는 네 가지 타입의 가치표현(value representation)이 있다는 것을 추론할 수 있다. 네 가지 유사한 타입의 학습은 행위적 수준(behavioral level)에서 구별되어져 왔는데, 양성적 강화(positive reinforcement)(RON), 양성적 처 벌(positive punishment)(AON), 음성적 강화(negative reinforcement)(AOFF), 음성적 처벌(negative punishment)(ROFF) 등으로 표현되어져 왔다. 따라서 뇌에는 도파민과 유사하나 이와 같이 다른 종류의 가치 (value)를 표현하는 적어도 세 개의 또 다른 신호들이 존재한다는 것을 제안한다. 이러한 네 가지 타입의 가치 들은 기본 가치(elementary values)라 표현한다. 본 발명은, 이러한 네 개의 기본 가치(elementary values)로 써 어떻게 다양한 강화 신호가 생성될 수 있는지를 나타낸다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 2010-0112742 A 비특허문헌 (비특허문헌 0001) Bear, M. F., Cooper, L. N. & Ebner, F. F. A physiological basis for a theory of synapse modification. Science 237, 42-48 . (비특허문헌 0002) Bell, A.J. & Sejnowski, T.J. The 'independent components' of natural scenes are edge filters. Vision Res 37, 3327-3338 ."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 에이전트에게 다양한 가치 표현을 가르치기 위하여 다양한 강화 신호를 발생시키는 문제를 해결하기 위하여 고안되었다. 뇌 또는 인공지능은, 위에서 설명한 바와 같이 다양한 가치 표현을 가지는 것이 필요하다. 종래기술은 이러한 문제를 적절히 해결하지 못하였다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "- 본 발명의 요약 본 발명은 강화학습(reinforcement learning, RL)이 뇌에서 어떻게 일어나느냐에 대한 증거에 기반한 정보 처리 방법을 설명한다. 강화학습(reinforcement learning)의 목적은, 에이전트(agent)로 하여금 보상을 최대화하고 처벌을 최소화하는 것이다. 이러한 두 가지는 전통적으로, 서로 같은 것으로서 결국 하나라고 인식되어져 왔고,따라서 강화학습 모델에서의 학습은, '가치(value)'의 단일 척도에 의해 보상과 처벌을 표현하는(represent) 단 일 강화 신호에 의해 구동되어져 왔다. 새로운 증거는, 뇌는 보상과 처벌을, 적어도 2개의 타입의 뉴런들에 의 해 표현되는 두 개의 별개의 카테고리로 다룬다는 것을 암시한다. 또한 뇌는 '대립' 표현들('opponent' representations)을 사용한다는 증거가 있다. 예를 들어, 도파민 뉴런은 보상 증거(evidence for reward)에 의 해 활성화되고 반 보상 증거(evidence against reward)에 의해 억제되는 반면, 다른 뉴런들은 보상 증거 및 반 보상 증거에 대해 반대 패턴의 반응을 보인다. 따라서 reward-ON(RON: 도파민(dopamine)), reward-OFF(ROFF), aversive-ON(AON), 및 aversive-OFF(AOFF) 등의 강화(reinforcement)를 구동하는 4 타입의 '기본 가치 신호 (elementary value signals)'를 제안한다. 본 발명은 이러한 네 개의 기본 가치 신호가 다양한 유효 강화 신호(effective reinforcement signals)를 만드 는 다양한 조합에서 어떻게 합해질 수 있는지를 설명한다. 예를 들어, 운동 뉴런의 활동은 RON 및 AOFF를 예측하 고 나타내야 하기 때문에, 운동 뉴런은 RON+AOFF-ROFF-AON에 상응하는 유효 강화 신호(effective reinforcement signal)를 수신해야 한다. 이와는 대조적으로, 엄격히 감각적인(strictly sensory) 뉴런은, 그것이 좋은 소식 (good news)을 주는 것으로 현저하든지 또는 나쁜 소식(bad news)을 주는 것으로 현저하든지에 관계없이, 외부 세계의 '현저한(salient)' 부분에 반응해야 한다. 따라서 그러한 뉴런에 대한 유효 강화 신호는 RON+ROFF+AON+AOFF 가 되어야 한다. 여기서 '현저(salient)하다'는 것은, 예를 들어 자극(stimulus) 등이, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 또는 반 처벌 증 거(evidence against punishment) 중 하나 이상과 관련이 있을 때, 그 자극(stimulus)이 '현저(salient)하다' 라고 표현하며, 이하에서도 그와 같은 경우에, '현저(salient)하다'라는 용어로 사용하기로 한다. 뇌의 각 뉴런은 8개의 서로 다른 타입 중 하나에 해당하고, 각 타입은 네 개의 기본 가치 신호(elementary value signal)의 서로 구별되는 합에 의해 정의된다는 것을 제안한다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이와 같은 목적을 달성하기 위하여 본 발명에 따른, 정보 처리 소자(information processing element)를 이용하 여 컴퓨터에 의해 수행되는 정보 처리 방법은, (a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동 (activity)을 갖는 복수의 기본 가치 신호(elementary value signals)를 수신하는 단계; (b) 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호 (reinforcement signal)를 계산하는 단계; 및 (c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학 습 과정(learning process)을 수행하는 단계를 포함한다. 상기 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)일 수 있다. 상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소 화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있다. 상기 정보 처리 소자는, 인공 뉴런에 해당하고, 상기 단계(a)는, 상기 정보 처리 소자에 의해, 각각 가중치와 활동을 갖는 상기 복수의 입력을 수신하는 단계를 포함하며, 상기 단계(b)는, (b1) 상기 정보 처리 소자에 의해, 상기 가중치들을 이용하여 구한, 상기 입력들의 상기 활동들(activities)의 가중합(weighted sum)의 함수 (function)를 이용하여 상기 정보 처리 소자의 출력을 계산하는 단계; 및 (b2) 상기 3개의 인수(3-terms)를 이 용하는 Hebbian-type 학습 법칙(learning rule)의 사용을 통하여 상기 가중치들을 수정하는(modifying) 단계를 포함하고, 상기 3개의 인수는, 입력의 활동(activity), 상기 입력들의 활동들의 가중합 및, 상기 강화 신호에 해당하는 것을 특징으로 한다. 상기 입력들의 하나의 부분집합(subset)은, 상기 정보 처리 소자의 이전 출력의 합에 상응하는 활동들 (activities)을 가지며, 이로부터 상기 정보 처리 소자는 가치(value)를 예측하는 시간적 패턴을 생성하는 것을 학습할 수 있다. 상기 Hebbian-type 학습 법칙에서 사용되는 3개의 인수들(terms)은, 동시에 발생하지 않고, 시간적으로 분리되 며, 상기 정보 처리 소자 내에서 일리저빌리티 트레이스(eligibility traces)에 의해 연결되고, 이로부터 상기정보 처리 소자는, 인과적으로 관련된 이벤트 시퀀스를 학습함을 통하여 가치(value)를 예측하는 것을 학습할 수 있다. 상기 기본 가치 신호의 상기 활동들(activities)은, 예측 오차(prediction errors)에 상응할 수 있다. 복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고, 상기 정보 처리 소자 각각은 8개의 타입 중 하나이며, 상기 8개의 타입은, 상기 각 기본 가치 신호에 대한 특정 가중치 값(weight values) 세트(set)를 가 지고, 상기 가중치 값은, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증 거(evidence for punishment) 및 반 처벌 증거(evidence against punishment) 각각에 대하여 양수(+) 또는 음 수(-) 값을 가지며, 상기 8개 타입의 각각은, ++++, ++--, --++, +---, -+--, --+-, ---+ 또는 +--+에 해당하 고, 상기 정보 처리 소자 각각은 특정한 가치(particular aspect of value)를 표현하는 것을 학습할 수 있다. 복수의 상기 정보 처리 소자를 포함하는 망(network)이 존재하고, 상기 기본 가치 신호들은 수퍼바이저리 티칭 신호(supervisory teaching signal)의 방식으로 상기 망 외부에서 생성되거나, 또는 상기 망 내에서 정보 처리 소자들 중 일부에 의해서 언수퍼바이즈드(unsupervised) 방식으로 생성될 수 있다. 상기 기본 가치 신호의 상기 가중치는 양수 또는 음수 또는 0의 값을 가지며, 상기 기본 가치 신호의 적어도 두 개의 가중치는 0이 아닌 값을 가질 수 있다. 본 발명의 다른 측면에 따르면, 정보 처리 소자를 이용하여 정보를 처리하는 장치, 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 복수개의 기본 가치 신호(elementary value signals)를 수신하는 수신부; 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합 (weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 계산부; 및 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 학습부를 포함한다. 상기 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)일 수 있다. 상기 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소 화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있다. 본 발명의 또 다른 측면에 따르면, 정보 처리 소자를 이용하여 정보를 처리하는 하나 또는 복수의 명령어 시퀀 스를 저장하는, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는, 하나 또는 복수의 프로세서에 의해 수행 될 때, (a) 상기 정보 처리 소자에 의해, 각각 가중치(weight)와 활동(activity)을 갖는 4개의 기본 가치 신호 (elementary value signals)를 수신하는 단계; (b) 상기 정보 처리 소자에 의해, 상기 기본 가치 신호의 상기 활동들(activities)의 가중합(weighted sum)을 이용하여 강화 신호(reinforcement signal)를 계산하는 단계; 및 (c) 상기 정보 처리 소자에 의해, 상기 강화 신호를 이용하여 학습 과정(learning process)을 수행하는 단계 를 포함하고, 상기 4개의 기본 가치 신호의 상기 활동들(activities)은, 보상 증거(evidence for reward), 반 보상 증거(evidence against reward), 처벌 증거(evidence for punishment) 및 반 처벌 증거(evidence against punishment)이며, 상기 4개의 기본 가치 신호의 상기 가중치를 선택함에 의해 상기 정보 처리 소자는, 보상을 최대화하거나 처벌을 최소화하는데 유용한(useful) 가치 표현(value representation)을 학습할 수 있는 것을 특징으로 한다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은, 다양한 강화 신호를 생성함에 의해 인공 지능을 만드는 것을 용이하게 하는 효과가 있다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명(가치 학습, 'Learning value')을 설명하기 전에, 뇌에서의 가치의 조직(organization of value)을 먼 저 설명한다. 이는 본 발명이 도출되는 이론적 근거를 설명하는데 도움이 된다. 즉, 본 발명이 왜 유용하며, 어 떻게 응용될 수 있는가를 이해할 수 있게 해 준다. - 뇌에서의 가치의 조직(organization of value) 뇌의 기능은 보상을 최대화하고 처벌을 최소화하는 것이다. 이것은 앞서 언급한 기본 가치(elementary values) 를 이용하여, 뇌의 기능은 RON과 AOFF를 예측하고 야기시키는 것이라는 말로써 설명된다. 각 뉴런은 외부의 어떤 면(그 수용장(receptive field, 이하 'RF'라 한다)에 의해 결정되는 외부의 자극)에 대한 정보를 수신하고, 이 는 미래의 가치(value) 예측에 관한 정보를 제공하도록 '선택'된다. 각 뉴런은 각각 자신에게 독특한(unique) RF와 자극을 갖지만, 각 뉴런은 도 1에 나타나 있는 8개의 '가치 표현형(value phenotypes)' 중의 하나로 분류 된다는 것을 제안한다. 이러한 8개의 각 표현형은 4개의 기본 가치(elementary values)의 합에 해당한다. 예를 들어, 척수 운동 뉴런(spinal motoneuron)은 RON 및 AOFF와 관련이 있지만, ROFF나 AON와는 관련이 없으며, 따라서 RON+AOFF-ROFF-AON으로 나타내어질 수 있다. 이러한 네 개의 항은 뉴런의 RF를 결정하고 그 막 전압(membrane voltage)을 구동하는 흥분 시냅스 입력(excitatory synaptic input)과 관련있는 미래 가치(future value)의 예 측을 설명한다. 운동 뉴런은 접근(RON)과 회피(AOFF)와 관련있는 행위(action)를 발생시키는데, 이는 운동 뉴런은 RON을 예측하는 흥분 시냅스와 AOFF를 예측하는 다른 시냅스들을 수신하기 때문이다. ROFF 또는 AON을 예측하는 어 떠한 시냅스라도 제거될 수 있다. 대부분의 초기 감각기관 뉴런(즉, 청각, 시각, 촉각)들은 보상 또는 처벌에 대한 선호하는 관련성을 가지고 있 지 않다. 그럼에도 불구하고, 그 뉴런들은 생물학적으로 중요한, 즉, 현저한(salient) 정보를 제공하는데 그것 은 RON+ROFF+AON+AOFF에 해당한다. 다운스트림 뉴런(downstream neuron)은 현저성(salience)을 보상과 처벌의 카테 고리로 구별하며, 보상과 처벌은 이후 ON과 OFF로 더 세분된다고 제안한다. AON과 ROFF과 관련있는 신호는 필터링 아웃(filtered out)되지만, RON 및 AOFF는 운동기관 영역(motor areas)으로 수렴된다. 이에 따라 감각기관으로부 터 운동기관까지는 총 4 레벨의 가치 표현이 있게 된다(도 1). 이러한 4 레벨(four levels)을 레벨 1의 운동(motor, action)으로부터 레벨 4의 현저성(salience)으로 표기하 는 것이 유용하다. 운동 조직이 가장 먼저 발달하는데, 운동 조직은 대부분의 감각 영역 이전에 진화된 것으로 생각되며, 따라서 필수불가결한 조직이다. 가치(value) 레벨 2, 3 및 4의 뉴런들은 그들이 RON+AOFF를 예측하고 야기시키는 목적에 기여할 때에만 유용하다. 그러나 감각 뉴런은 어떤 레벨에서도 발견될 수 있다. 어떤 주요 감각 뉴런들(예를 들면, nociceptors)은, 특히 RON 또는 AON 등과 관련하여서는 비교적 명확한 정보를 가지고 있 고, 따라서 레벨 2에 있게 된다. 척수 운동 뉴런(spinal motoneurons)의 흥분을 직접적으로 일으키는 자기감수체(proprioceptors)는, 먹이 (prey)를 향하여 달리든지(RON) 또는 약탈자(predators)로부터 도망가든지(AOFF) 간에 동일한 방식으로 활성화되 며, 따라서 상위의 감각 레벨로 매핑되지만 운동 뉴런들과 같이 가치(value) 레벨 1으로 분류된다. 도 1은 뇌에서의 가치(value) 조직을 도시하고 있다. 뇌 전체에서 각 뉴런은 8개의 가치 표현형(value phenotypes) 중의 하나인 것으로 제안된다. 하나의 뉴런의 표현형(phenotype)은 그 뉴런의 흥분 수용장(RF)에 의해 결정되는데, 이는 네 개의 기본 가치(elementary value)인 reward-ON(RON), reward-OFF(ROFF), aversive- ON(AON), aversive-OFF(AOFF)의 합에 의해 나타내어진다. 운동 출력에서 가장 먼 레벨 4에서는, 뉴런들이 중립적 이지만 중요한(\"현저한(salient)\") 감각 자극을 표현한다. 이들은 레벨 3에서, 보상(reward)과 처벌(punishment)이라는 대체적으로 독립적이고 병행적인 카테고리로 분화된다. 이들은 레벨 2에서 ON과 OFF로 더욱 나누어지고, 이후 레벨 1의 운동 뉴런에서 '행위(action)'를 발생시키기 위해 RON과 AOFF가 모이게 된다. 하나의 뉴런의 가치 표현형은 수학식 2의 3-인수(3-terms) Hebbian rule에 의해 형성되고, 수학식 2에서 유효한 순 (net) 가치(value) 신호는 네 개의 기본 가치 신호(elementary value signals)의 합(수학식 3)으로 나타내어지 는데, 8 개의 표현형 각각마다 하나의 합이 해당한다. (여기서 나타난 각 합은, 수학식 3의 가중치가 '+1' 또는 '-1'인 경우에 대한 것이다) 각 기본 가치 신호는 적어도 하나의 뉴런 조절기(neuronmodulator)에 의해 나타내 어지며, 도파민(dopamine)은 RON을 나타낸다. 뉴런의 '가치 유전자형(value genotype)'은 각각의 뉴런 조절기가 어떻게 자기의 RF를 형성하는지를 결정하는 적어도 네 개의 GPCR들의 표현과 상응한다. 예를 들어, 도파민 D1 수용체(dopamine D1 receptors)는 +RON 표현형을 발생시키고, 반면에 D2 수용체는 -RON 표현형을 발생시킨다. 가 치(value)의 네 레벨은 감각기관 대 운동기관의 관계로 단순화되지 않는다. 대부분의 운동 뉴런은 레벨 1에 있 는 반면, 레벨 2에서와 같이 특히 접근(RON) 또는 회피(AOFF)에 특정적으로 관계하는 운동 뉴런과 근육과 같은 특 별한 경우가 있다. 감각 뉴런은 네 개의 레벨 중 어디에도 있을 수 있고, 낮은 레벨 뿐만 아니라 높은 레벨에도 매핑될 수 있다. - 왜 보상과 처벌에 대하여 독립적인 표현(representation)이 있게 되는가? 보상과 처벌은 생물학적 적합성의 관점에서 서로 역(opposites)에 해당하며, 긴 시간척도(timescales)에서 서로 반 상관관계(anti-correlated)에 있다. 그러나 RF 형성과 관련된 초 단위 이하의 짧은 시간척도에서, 그들의 감 각 성분은 빛과 어둠의 의미에서와 같이 역(opposites)이 아니다. 뇌는, 시각 조직에서 분리된 물체를 인식하는 것을 배우는 것과 동일한 방식으로 좋은 것과 나쁜 것(good and bad)을 두 개의 별개의 카테고리로 인식하는 것 을 배운다고 제안된다. 자극들이 외부 세계에서 함께 일어나는 경향이 있을 때, (뇌 안의 어디에선가) 하나의 뉴런은 그러한 자극들을 동일한 '대상'으로 인식하고 반응한다. 그러나 자극들이 상관관계 없이, 통계적으로 독 립적인 방식으로 발생한다면, 그러한 자극들은 서로 다른 뉴런들에 의해 독립적으로 표현된다. Hebbian 학습을 통해서 하나의 뉴런은 그 자신의 활성화를 예측하고 일으키는 흥분성 시냅스 입력들을 선택한다. 이것은 서로 관련된 시냅스 전부(presynaptic)의 입력을 선택하고, 관련되어 있지 않은 입력들은 제거해 버리는 효과를 갖는 다. 잘 알려진 예는 시각피질(visual cortex)의 뉴런에 의한 두 눈의(binocular) RF의 형성이다. 앞을 보는 두 눈을 가진 동물에 있어서 자연적인 환경 하에서는, 각 눈은 동일한 '사물'을 거의 동시에 보며, 따라서 한쪽 눈 으로부터의 활성화된 시냅스 입력은 다른 눈으로부터의 활성화된 입력을 예측하고, 두 눈은 주요 시각피질의 시냅스 후부(postsynaptic)의 뉴런을 동시에 흥분시킨다. 이러한 예측에 있어서의 관련성은 각 눈으로부터의 시 냅스 입력을 더 강하게 하며, 그러한 관련성이 없다면 두 눈의(binocular) RF는 형성되지 않는다. 만약 두 눈이 독립적으로 움직인다면, 각 눈으로부터의 정보는 두 개의 외눈 뉴런들(two monocular neurons)에 의해 독립적으 로 표현되게 될 것이다. 동일한 원리가 수많은 다른 사물 또는 카테고리-선택적(category-selective) 뉴런들의 형성을 설명하는 것으로 생각된다. 이것은, 보상과 처벌이 일관되게 상호관련된 것이 아니고 서로 예측할 수 있는 것이 아니라면(즉, 통계적으로 독립적이라면), 보상과 처벌은 적어도 두 가지 형태의 뉴런에 의해 독립적으로 표현된다는 것을 의미한다. 여기 서 관심있는 상호관련성(correlation)은, RF 형성에서 중요한 초 이하 시간척도(sub-second timescale)에 관한 것인데, 이것은 진화적(evolutionary) 또는 성장적(developmental) 시간척도에 있어서도 지속적으로 적용된다 (즉 위에서 나타내었던 연구실 실험에서 만들어진 것과 같은 일시적인 종류의 상호관련성이 아니다). 우리는 단 지 관련있는 통계적 관계성에 대하여 추측할 뿐이지만, 보상과 처벌 간의 상호관련성은 약한 경향이 있는 것으 로 보이고, 음성적(negative)이라기 보다는 양성적(positive)이라는 것은 더욱 신빙성이 있어보인다. 예를 들어, 외상수용기(nociceptor)의 활성화와 음식, 물 또는 섹스와 관련된 자극 간에는, 적어도 작은 시간척도에 있어서는 강한 상호관련성이 없는 것으로 보인다. 천연음식에서 설탕과 쓴 화학약품의 양은 일반적으로 강한 상 호관련성이 없는 것으로 보인다. 달콤씁쓸한 것은 일반적인 것(rule)이 아니나, 그렇다고 예외적인 것은 확실히 아니다. 그러나 보상 자극(reward stimuli) 간(예를 들어, 음식을 보는 것과 냄새 맡는 것)에나, 혐오 자극 (aversive stimuli) 간(예를 들어, 충돌이 몸의 여러 부분에 동시에 해를 가하는 것)에는 강한 양성적 (positive) 상호관련성이 있는 경향이 있다고 보인다. 이러한 양성적 상호관련성은 어떤 뉴런들에서의 보상 RF 와 다른 뉴런들에서의 처벌 RF의 Hebbian development를 증진시킨다(도 1).어떤 보상 자극과 혐오 자극 간에는 강한 양성적 상호관련성이 있다고 할지라도, 뇌는 보상과 처벌을 별개로 표 현하기 위해 일한다는 것이 제안된다. 단지 예시를 위해서, 우리는 좋은 것과 나쁜 것(good and bad)에 대한 정 보는 일관적으로 각각 왼쪽 눈과 오른쪽 눈을 통하여 온다고 가정해 볼 수 있다. 비록 왼쪽 눈과 오른쪽 눈으로 부터의 입력의 활동은 매우 상관 관계가 있지만, 두 눈에 의한(binocular) 수용장(receptive field, RF)은 형성 되지 않는다. 이것이 아래에서 더욱 자세히 설명할, 3개의 인수(terms)에 기반한 Hebbian 학습 법칙(learning rules)이 의미하는 바이며, 별개의 '보상' 뉴런과 '처벌' 뉴런을 생산하도록 작동하는 것이다. 레벨 2에서의 ON과 OFF로의 세분화는 필연적으로 레벨 3에서의 보상과 처벌로의 세분화를 일으킨다. 보상과 처 벌의 카테고리는, 보상 자극과 혐오 자극의 존재 및 부존재 모두와 자연스럽게 관련된다. 예를 들어, 돈에 대한 일반적인 개념은 '보상' 카테고리 내에 있으나, 그것은 이득(RON)과 손실(ROFF)에 거의 동등하게 관련되어 있다. 레벨 3의 다른 임의의 '보상 뉴런' 처럼, '돈(money) 뉴런'은 RON+ROFF로 나타내어질 수 있는데, 레벨 2는 두 개 의 형태의 돈 뉴런을 포함하며, 하나는 이득(RON)에 해당하고 다른 하나는 손실(ROFF)에 해당한다. - 가치의 대립 표현(Opponent representation of value) 앞에서 설명한 원리들로부터, 보상과 처벌은, 그것들이 일반적으로는 서로 반 상관관계(anti-correlated)에 있 지 않기 때문에 대립자(opposites)로 다루어지지 않는다는 것을 도출할 수 있다. 그러나, 일단 보상과 처벌이 두 개의 카테고리로 구분되었다면, 그 각각에 대한 우호적(for) 및 대항적(against) 증거들(evidence) 간의 대 립 관계(opponent relationship)가 있게 된다. 대립관계의 중요성은 시각 계통에서 비교적 잘 이해되고 있는데, 여기서 빛과 어둠은 자연적 대립자(opposites)이고, 두 개의 구별되는 형태의 뉴런, 즉 ON과 OFF에 의해서 표현 된다. 빛의 세기는 단일 ON 타입 뉴런에 의해 표현되지만, 대립 표현(opponent representation)이 환경으로부터 더욱 많은 정보를 추출해낸다. 빛에 대한 우호적(for) 및 대항적(against) 증거들(evidence)(또는 좋은 것과 나쁜 것 (good and bad)) 간에 반 상관관계(anti-correlation)가 있다는 것은 명백하다. 그러나 반 상관관계는 완전한 것이 아니라는 것을 인식하는 것이 중요하다. 이는 시각 계통에서, 공간 수용장(spatial RFs)과 시냅스 흥분의 타이밍이 ON 및 OFF 뉴런에서 완전하게 매치되지 않는다는 것 때문이다. 보상의 경우 반 상관관계는, 보통 특정 시간에 RON 또는 ROFF의 증거(evidence)를 공급하는 하나의 중요한 자극이 존재하기 때문에 발생한다. 그러나 드 문 경우에 동시에 RON과 ROFF 뉴런을 활성화시키는, 모순된 증거(evidence)를 공급하는 둘 또는 그 이상의 자극이 있을 수 있다. 반 상관관계 때문에, OFF의 증거는 또한 반 ON(against ON)의 증거이기도 하며, 그 반대의 경우 도 마찬가지인데, 이것은 두 개의 세포 타입 간에 상대방을 억제하는(opponent inhibition) 것의 토대를 형성한 다. 도 2는 보상과 혐오 모두의 대립 표현에 적용하도록 제안된 회로를 보여주는데, 특히 도파민과 다른 기본 가치(elementary value) 뉴런을 제어하는 '비판(critic)' 회로에 적용된다. 이것은 시상(thalamus)의 측면 슬 상핵(lateral geniculate nucleus)에 상응하는 것으로, 뉴런간의 모든 연결은 예측을 중재한다는 원리에 따르도 록 의도되었다. 여기서 가치 표현형(value phenotype)은 그것의 흥분(excitatory) RF 들에 의해 정의되었고, 어 떤 대립 억제(opponent inhibition)도 수용하지 않는 뉴런이 존재할 수 있다는 점을 주목해야 한다. 도 2는 대립(opponent) 프로세스를 성립시키는 제안된 연결성(connectivity)을 예시한 도면이다. 이러한 연결성 은 LGN(lateral geniculate neucleus)에서 보여주었고, 어떠한 대립 표현(opponent representation)에도 적용 할 수 있다. 여기서는 그것이 RON 및 ROFF에 적용된다. RON 또는 ROFF에 더하여, 뉴런들은 흥분을 일으키거나(excitatory)(e, 화살표), 억제하거나(inhibitory)(i, 연결선 끝부분에 짧은 수직선분을 가지는 연결선) 또는 조절하는 (modulatory)(m) 출력을 가진다. 억제(inhibitory) 시냅스는 굵은 선으로 표시된 'opponent'와 얇은 선으로 표 시된 'homeostatic'으로 나누어질 수 있다. 뉴런은 동일한 가치 표현형(유사한 RF)을 갖는 다른 뉴런들로부터흥분(excitation) 및, 항상성을 유지하는 억제(homeostatic inhibition)를 수용하며(receive), 대립 표현형 (opponent phenotype)을 갖는 뉴런으로부터 대립 억제(opponent inhibition)를 수용한다(receive). 따라서 RON 뉴런은 보상에 대한(for) 증거(RONe)와 보상에 반한(against) 증거(ROFFi)를 합한다. 대립 억제(opponent inhibition)는 보통 시냅스 흥분 없이 발생하고 과분극(hyperpolarization)을 구동하는 반면, 항상성을 유지하 는 억제(homeostatic inhibition)는 보통 흥분과 동시에 발생하고 그에 대항하여 행동하여 흥분의 항상성 수준 을 유지시키는 것을 돕는다. 항상성을 유지하는 억제(homeostatic inhibition)는 '예측 오차(prediction error)'에서 '예측'을 중재한다(mediate). 비록 여기서의 방법이 보상에 관련된 해부학이나 생리학의 직접적 지 식에 의해 영감을 받은 것은 아니나, 측면 고삐핵(lateral habenula)(R-OFFe), rostromedial tegmental nucleus(R-OFFi), ventral tegmental area의 GABAergic(R-ONi) 뉴런들과 같은 어떤 뉴런들의 특정한 분류를 뒷 받침하는 제한된 증거가 있다. - 가치 학습(Learning value) 본 발명에서 설명한 강화 신호는 강화학습(RL)의 어떤 응용에도 유용하다. 여기서 설명하는 실시예는 인공 뉴런 네트워크를 포함한다. 인공 뉴런은, 어떤 주어진 순간에서도 그 입력(ui)들의 가중합의 함수인, 아날로그 또는 디지털의 출력(υ)을 갖는다. 수학식 1"}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "학습은 가중치(wi)의 변화를 통해 일어난다. 가장 잘 알려진, 생물학적으로 설득력있는 관련 학습 법칙은 2개의 인수(terms)를 포함하는 Hebbian rule인데, 그 2개의 인수는 시냅스 전부 활동(presynaptic activity(u))와 시 냅스 후부 활동(postsynaptic activity(v))이다. 기존의 2-인수(2-terms) Hebbian rule에 포함된 원리는, 시냅 스 전부(presynaptic) 활동이 시냅스 후부(postsynaptic) 활동을 예측하고 야기시키면 흥분 시냅스(excitatory synapse)가 강화 되어야 한다(가중치가 높아진다)는 것이다. 가치를 학습하기 위해, 만약 시냅스 전부 (presynaptic) 활동(ui)이 시냅스 후부(postsynaptic) 활동(v)을 예측하고, 시냅스 전부 및 후부 활동의 동시성 이 함께 수치 (V)를 예측한다면, 흥분 시냅스가 강화 되어야 한다. 가장 간단한 3-인수(3-terms) Hebbian rule 은 수학식 2를 따라 시냅스 세기(strength of synapse)(w)를 수정한다. 수학식 2"}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이러한 학습 법칙(learning rule)을 통해, 뉴런은 가치(value)를 예측하는 외부세계의 일부와 상응하는 RF를 발 전시킨다. 수학식 2의 형태는 그것이 Hebbian rule의 가장 선호하는 형태를 대표하기 때문에 선택된 것이 아니 라, 많은 Hebbian rule 중에서 가장 간단한 것으로서 선택된 것임에 주의해야 한다. 강화 신호(V)가 '보상 예측 오차(reward prediction error)'에 상응하는 모델들이 개발되었는데, 이는 뇌 안에 서 도파민에 의해 중재된다. 본 발명에서 제시되는 새로운 기여는, 유효한(effective, 순(net)) 강화 신호(V)는 단일 신호가 아니라, 네 개의 기본 가치 신호(elementary value signals)의 합이라는 것이다.수학식 3"}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3의 가중치(w)는 순(net) 강화 신호(V)에 기여함에 있어서의 각 기본 가치 신호의 세기(strength)를 나 타낸다. 가장 간단한 실시예에서, 그러한 가중치들은 -1 또는 +1, 또는 어떤 경우에는 0이다. (이러한 가중치들 은 수학식 1과 2에서의 가중치들과 혼동해서는 안되는데, 이는 뉴런으로의 흥분성 또는 억제성 입력의 세기 (strength)를 나타내는 수정 가능한 가중치이다) 기본 가치 신호(elementary value signals) 그 자체는 양수 (positive number)로 표현되고, 뇌에서의 신경전달물질의 농도(concentration)에 상응한다. 어떤 특정 순간에서 의 유효 강화 신호(effective reinforcement signal)(V)는 양수 또는 음수일 수 있다. 하나의 기본 가치 신호 (elementary value signal)와 그 가중치의 곱은, 시냅스 가까이에 위치하는 G-단백질 연결 수용체(G-protein coupled receptor, GPCR)의 활성화에 상응한다. GPCR의 특정한 형태와 그 관련된 신호전달체계(signal transduction cascade)는, 그것의 활성화가, 이하에서 도파민에 대해 설명하는 바와 같이, 활성화된(co- activated) 시냅스의 강화(w>0) 또는 약화(w<0)에 기여하는지 여부를 결정한다. 도파민의 효과는 선조체(striatum)의 중간 가시 뉴런(medium spiny neurons, MSNs)에서 가장 면밀하게 연구되 어져 왔다. 어떤 MSN들은 D1 수용체를 표현하는 반면, 다른 뉴런은 D2 수용체를 표현하며, 이들 두 GPCR들은 결 합되어 독특한 신호 전달 통로(transduction pathway)가 된다. D1과 D2 MSN은 아래와 같이 가치(value)를 표현 한다. D1 MSNs V=RON-ROFF-AON-AOFF D2 MSNs V=AOFF-AON-RON-ROFF D1 MSN(pre+post)에 대한 흥분 시냅스의 활성화에 이어 로컬 D1 수용체의 활성화가 뒤따를 때, D1 수용체는 양 의 순 가치(positive net value) V에 기여한다. 그 동일한 시냅스의 활성화 뒤에 다른 세 개의 기본 가치 신호 들 중에 어느 것이라도 뒤따를 때에는, 그것들의 GPCR은 순 가치(net value)에 대해 음의 항(negative terms)을 부가할 것이다. 그러나, D2 MSN에 대한 D2 수용체의 활성화는 순 가치(net value)에 음의 값으로 기여할 것이다. 따라서 D1 및 D2 수용체는 각각 함께 활성화되는 시냅스들을 강화하고 약화시키게 될 것이다. 이러한 법칙들의 결과로, D1 MSN은 그 입력(또는 자극)을 결정하는 RON RF를 가지며, 그 출력(또는 행 동,action)은 접근 행위(approach behavior)에 상응한다. D2 MSN은 AOFF RF를 가지고, 회피 행위(avoidance behavior)를 중재하는데 기여한다. 이러한 제안은 D2 MNS가 'no go'라는 억제 행위(suppressing behavior)를 이루는데 반해 D1 MSN은 'go' 반응을 이루도록 하는 종래의 모델과 대조를 이룬다. 매우 다양한 MSN들이 매우 다양한 접근 및 회피 행위를 성립시키지만, 회피 행위의 다수는 탈출(escape) 또는 방어(defense) 등의 능동적 인(active) 회피보다는 물러남(withdrawal)과 '동작 멈춤(freezing)'과 같은 '수동적(passive)'인 회피에 해당 한다. 수동적 회피는 운동능력(locomotion)을 억제하지만, 그럼에도 불구하고, 혐오 자극과 보상 자극 모두에 대한 낮은 기대를 동반하는 동기부여 없는 'no go' 상태와는 반대되는, 혐오 자극에 대한 기대(expectation)를 자연스럽게 동반하는 동기부여와 운동 활성화를 포함한다. D2 MSN이 회피를 중재한다는 가설은, D2 MSN의 선택 적 활성화가 동작 멈춤(freezing)과 때로는 탈출(escape)을 끌어낸다는 최근의 실증에 의해 뒷받침된다. 어떤 AOFF 뉴런들의 활성화가 긍정적인 정서(positive affect)(예를 들어 '안전')를 증진시키며 ('음성적(negative)') 강화에 기여한다고 기대될 수 있지만, AOFF 신호는 혐오적 상황 내에서만 자연스럽게 발생하며, AON의 위험과 관 련이 있을 수도 있다는 것을 주의해야 한다. GPCR을 통한 네 개의 가치 신호(value signals)의 통합은, 막 전압을 통한 전기신호의 합이 아니라 신호 전달 통로(signal transduction pathways)에 의존하는 통합의 새로운 메커니즘일 수 있다. GPCR은 다양한 시냅스에 있어서 Hebbian 시냅스 유연성(synaptic plasticity)를 제어한다는 것이 증명되었고, 적어도 두 타입의 GPCR 간 의 상호작용은, 선조체(striatal) MSN에서의 도파민 수용체를 포함하는 일부 단일 시냅스에서의 시냅스 유연성 (synaptic plasticity)을 조절하는 것으로 증명되었다. 제안된 방법은 뇌 전체에 걸쳐서 최소 네 개의 신경전달물질에 의해서 수행되며, 적어도 2개의 상이하게 결합된 GPCR들을 가지고, 적어도 총 8개의 GPCR을 가진다. 네 개의 GPCR들의 특정한 조합의 표현은 뉴런의 '가치 유전 자형(value genotype)'을 결정할 수 있는데, 이것은 수학식 2와 3의 3-인수 Hebbian rules를 통하여 뉴런의 표 현형(phenotype)에 영향을 미칠 수 있다(표현형을 결정하는 것이 아니라). 뇌의 특정 영역의 가치 표현형은 부 분적으로 특정 가치 유전자형, 예를 들어 운동영역에서 RON+AOFF 등을 강제함에 의해 만들어진다. 그러나 특정 유 전자형을 초기에 강제함이 없이도 유사한 결과를 잠재적으로 성취할 수 있다. 발달과정 동안, 독특한 가치 유전 자형을 가지는 다양한 뉴런들이 동일한 뇌 영역에서 만들어질 수 있다. 예를 들어, 만약 그 영역으로의 시냅스 입력이 확실하게 RON을 예측하지만, 다른 기본 가치 신호에 대해서는 그렇지 않은 경우라면, RON 유전자형의 시냅 스 후부 뉴런들은 일군의 강한 흥분성 시냅스를 형성하고 살아남을 것이며, 반면에 다른 가치 유전자형의 뉴런 들은 불충분한 일군의 시냅스를 형성하고 따라서 세포 자멸(apoptosis)을 통해 제거될 것이다. 위에서 설명한, 그리고 도 1의 가치 표현형(value phenotypes)은 뉴런의 해부학적으로 정의된 RF의 특성이며, 시냅스가 생성되고 파괴될 때만 천천히 변화한다. 뉴런의 기능적 RF는 그것의 시냅스 입력의 가중치에 의해 결 정되는데, 무엇이 중요한지에 대한 현재의 기대에 의존하여 1초 이내에 변화할 수 있다. 이것은 빈약하게 이해 되고 있는 뇌 피질에서의 탑-다운 방식의 선택적 주의(selective attention) 메커니즘을 통해서 뿐만 아니라, 위에서 설명한 3-인수(term) Hebbian rule의 빠른 효과를 통해서도 일어날 수 있다. 그러므로 주어진 순간에서 의 뉴런의 기능적 표현형은 그것의 해부학적으로(또는 유전학적으로) 정의된 가치 표현형에서 표현되는 기본 가 치(elementary values)들의 부분집합으로 구성된다. 예를 들어, 현저성 뉴런(salience neuron)은 일시적으로 네 개의 기본 가치의 어떠한 부분집합을 선택할 수도 있고, 일련의 접근(approach) 행위의 상황 속에서 운동 뉴 런은 RON의 기능적 표현형을 갖기 위해 그것의 AOFF 흥분성 시냅스를 억제할 수도 있다. 위에서 설명한 강화 신호는 (다른 뉴런들로부터) 뉴런으로의 시냅스 입력의 세기를 수정하도록 작용할 수 있는 한편, 동일한 타입의 3-인수 Hebbian 학습 법칙(rule)은, 본 발명자가 앞에서 설명하였듯이, 동일한 뉴런 내에 서 전압에 의해 조정되는(voltage-regulated) 이온 채널의 세기를 수정하는 기능을 또한 수행할 수 있다. 뉴런 은, 그 전압과 시간 의존성에 있어서 서로 다른 전압에 의해 조정되는(voltage-regulated) 다양한 이온 채널들 을 표현한다. 각 짧은 순간에, 그러한 이온 채널의 활동은 '메모리'(즉 동일한 뉴런의 과거 출력의 합)에 의존 하며, 여기서 과거의 특정 기간은 이온 채널의 특정한 종류에 따라 달라지게 된다. 따라서 이러한 이온 채널로 의 입력은 그 뉴런의 출력이지만, 이 이온 채널들은 또한 뉴런에 입력을 공급한다. 예를 들어, 간단한 실시예에 서, 인공 뉴런의 출력은 막 전압에 해당하고, 그것은 시간 t-1에서의 출력에 대응하는 하나의 입력(voltage- gated 이온 채널의 한 subtype), 시간 t-2 에서의 출력에 대응하는 또 하나의 입력 등을 가질 수 있고, 이들 각 각은 자기의 수정 가능한 가중치를 갖는다. 그것은, 이러한 타입의 이온 채널에 대하여 관련된 Hebbian 학습 (learning)을 적용함으로써 뉴런이 어떻게 시간적 패턴을 생성하는 것을 학습할 수 있는지를 이미 설명하였다. 또한 뉴런이 외부 세계(world)의 '중요한' 면(aspect)을 예측하기를 배우기 위해 Hebbian 학습 법칙(rule)이 어 떻게 강화 항(reinforcement term)을 포함해야 하는지를 설명했다. 본 발명은 강화 신호가 어떻게 만들어지는지 를 제안한다. 수학식 2에 의해 표현된대로, 입력의 가중치를 변화시키기 위하여, 세 개의 활동(three activities)이 적어도 근사적으로라도 시간적으로 일치해야 한다. 본 발명은 어떤 특정한 시간적인 관계도 명시하지 않는다. 그러나 인과율의 법칙을 수행하기 위해서, 시냅스 전부의 활동(presynaptic activity) (u)는 시냅스 후부의 활동 (postsynaptic activity) (v)이 뒤따라야 하는데(또한 시냅스 후부의 활동을 유발시키는 것을 도움), 이는 강화 신호(reinforcement signal) (V)가 뒤따르게 된다(또한 강화신호를 유발시키는 것을 돕게 된다). 이를 성취하기 위해서, 적어도 두 개의 'eligibility traces'를 사용하는 것이 유용하다. 첫번째 eligibility trace는 시냅스전부 활동(presynaptic activity)에 관한 점차적으로 감쇠하는 메모리에 해당한다. 시냅스 후부 활동 (postsynaptic activity)과 첫번째 eligibility trace의 곱은 두번째 eligibility trace를 발생시키는데, 이것 은 시냅스 전부와 후부 활동의 동시성에 관한 점차적으로 감쇠하는 메모리에 해당한다. 이로부터, 가중치의 변 화는 두번째 eligibility trace와 강화 신호(reinforcement signal) (V)의 곱(product)에 의존한다. 인공 뉴런의 출력은 아날로그(예를 들어 전압) 또는 디지털(예를 들어 뇌파에서 스파이크, spike) 변수가 될 수 있다. 생물학적 신경계에서, 디지털 출력은 먼거리 간의 통신에 유리한데, 이는 디지털 출력이 전송될 때 세기 가 약해지지 않기 때문이다. 그러나 어떤 뉴런들은 원거리 통신이 필요하지 않고, 인공 뉴런 망에서는 원거리 통신이라고 하여 반드시 신뢰성이 문제되는 것도 아니다. 다른 조건이 동일하다면, 아날로그 출력은 훨씬 많은 정보를 운반한다. 본 발명은 기본 가치 신호의 정교화를 명시하거나 한정하고 있지 않다. 강화 신호(reinforcement signal)가 예 측 오차(prediction error)에 상응하는 것이 바람직한 것으로 알려져 있고, 정확한 예측이 당연히 부정확한 예 측보다 바람직하다. 도파민 뉴런들은 매우 일반적이고 정교한 방법으로 RON 신호를 보낸다. 그것들은 예를 들어 음식과 관련된 자극 또는 특정한 감각 모드에 특화되어 있기보다는 보상(reward)의 모든 면에 민감한 것으로 보 인다. 게다가, 도파민 뉴런들(또는 그것들의 구심 뉴런, afferent neurons)은 매우 정교한 예측을 수신하고, 이 러한 예측은 도파민 뉴런들이 정보 이론 원리에 따라 매우 효율적이고 정확한 방식으로 보상 가치(reward value)를 표현하도록 해준다. 도파민 뉴런들은 일반적인 방식으로 보상을 표현할 수 있고, 정교하고 정확한 예 측을 하는데, 이는 그들이 대뇌의 매우 큰 부분으로부터 고도하게 처리된 정보를 수신하기 때문이다. 그러므로 영장류의 뇌에서 도파민 뉴런들은 RON 강화 신호에 대한 이상형에 가까운 어떤 것을 표현한다. 그러나, 본 발명 에 있어서, RON 강화 신호는 보상에 우호적인 증거에, AOFF 강화 신호는 혐오 자극에 적대하는 증거에 각각 상응 할 수 있으며, 동일한 원리가 다른 두 개의 기본 강화 신호에도 적용된다."}
{"patent_id": "10-2014-0025967", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "- 도 3에 도시된 실시예의 요약 인공 뉴런 31은 아날로그 또는 디지털의 출력(v)를 가지는데, 이것은 어떤 주어진 순간에도 그 입력(u)의 가중 치 적용 합(weighted sum)의 함수이다(수학식 1). 학습은 3-인수 Hebbian-type 학습 법칙에 따라 가중치 33을 수정함에 의해 진행되며, Hebbian-type 학습 법칙의 가장 간단한 형태는 수학식 2에 나타나 있다. 유효 강 화 신호 34(V)는 네 개의 기본 가치 신호의 가중 합이며, 인공 뉴런 31 내의 기본 가치 신호로부터 계산된다. 네 개의 기본 가치 신호는 보상 증거 35, 반 보상 증거 36, 처벌 증거 37 및, 반 처벌 증거 38이다. 네 개의 기 본 가치 신호(elementary value signal)는 인공 뉴런 31의 외부에서 생성되는데, 관리 신호(supervisory signal) 방식으로 생성되거나, 또는 다른 인공 뉴런들에 의해 관리되지 않는 방식으로(in an unsupervised manner) 만들어진다(도 2). 이와 같은 언수퍼바이즈드 방식으로 생성될 경우는 네 개의 기본 가치 신호가 인공 뉴런 망 내부에서 생성될 수도 있다. 인공 뉴런 31은 가장 선호되는 실시예를 나타낸다. 그러나 본 발명은 신경 망을 포함하지 않는 다른 형태의 강화학습(reinforcement learning, RL)과 함께 사용될 수 있다. 본 발명의 핵 심이 되는 특징은, '강화 발생기(reinforcement generator)' 모듈 39에 의한 유효 강화 신호(effective reinforcement signal)(V)의 발생시키는 것이다. 네 개의 기본 가치 신호의 합은 가중치에 의해 결정되는데, 이 가중치는 각 네 개의 신호에 대하여 양수 또는 음수가 될 수 있다. 이러한 가중치는 뉴런이 어떤 면(aspect)의 가치를 예측하는 것을 학습할지를 결정할 수 있다. 도 1은 뇌에서 가치(value)의 일반적인 조직을 예시하고 있 는데, 각 뉴런은 양수와 음수의 가중치들의 조합으로 서로 다른 8개 타입 중의 하나라는 것을 제시하고 있다. - 컴퓨터로 읽을 수 있는 매체 일 실시예에서, 정보 처리 소자(information processing element)는 클라이언트 40에 저장된 소프트웨어 프로그 램을 이용하여 생성된다. 도 4는 하나 또는 복수의 정보 처리 소자의 생성을 위한 예시적 구조를 도시하는 단순 화된 다이어그램이다. 이 예시적 구조는 클라이언트 40(예를 들어 컴퓨터와 같이 클라이언트 장치로 작동하도록 구성된 컴퓨팅 플랫폼)인데, 메모리 RAM 또는 자기 또는 광학 매체와 같이 컴퓨터로 읽을 수 있는 매체 41 및 이와 연결된 전자 프로세서 42 등을 포함하도록 구성된 디지털 미디어 플레이어, 개인용 디지털 보조장치, 또는휴대폰 등이 가능하다. 프로세서 42는 컴퓨터로 읽을 수 있는 매체 41에 저장되어 있는 프로그램 명령어를 수행 한다."}
{"patent_id": "10-2014-0025967", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 감각기관의 입력으로부터 운동기관의 출력까지의 4단계 처리 레벨(four levels of processing)에 분포되 어 있는 8개의 서로 다른 가치 표현의 형태로써, 본 발명에서 제안하는 뇌에서의 가치 조직(organization of value)을 도시한 도면이다. 도 2는 보상(reward)을 일례로서 사용하여, 본 발명에서 제안하는 대립 표현(opponent representation)에 대한 뉴런 회로를 도시한 도면이다. 도 3은 인공 뉴런에 있어서의 본 발명의 사용에 대한 일 실시예를 예시하는 블럭 다이어그램이다.도 4는 본 발명의 일 실시예에 따른, 컴퓨터에서 읽을 수 있는 매체를 예시하는 블럭 다이어그램이다."}
