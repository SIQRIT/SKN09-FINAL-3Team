{"patent_id": "10-2022-0082369", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0008424", "출원번호": "10-2022-0082369", "발명의 명칭": "딥페이크 음성의 설명가능한 분류 모델과 그 설명에 대한 해석 기법", "출원인": "한양대학교 산학협력단", "발명자": "채동규"}}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 분류 시스템에 의해 수행되는 음성 분류 방법에 있어서, 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하는 단계; 및 상기 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공하는 단계를 포함하고, 상기 분류 모델은, 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것인, 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 음성은, 소셜 네트워크 서비스(SNS)에 업로드된 음성을 포함하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 분류 모델은, CNN, CNN-LSTM 또는 CNN-LSTM-perm 중 어느 하나의 네트워크를 기반으로 구성된 것을 특징으로 하는 음성 분류방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 분류 모델에 적용된 설명가능한 인공지능 기법을 통해 음성의 특징 정보를 사용하여 획득한 기여도 점수가시각적 방법 또는 청각적 방법을 기반으로 해석되는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 분류 모델의 파라미터들이 학습됨에 따라 상기 분류 모델의 예측에 대한 입력 데이터의 기여도 분석을 통해 스펙트로그램 상의 히트맵의 형식으로 기여도 스코어(attribution score)가 도출되는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 설명가능한 인공지능 기법은,계층별 관련성 전파(Layer-wise Relevance Propagation), 통합 기울기(Integrated Gradients), 딥 테일러(DeepTaylor) 분해 중 어느 하나를 포함하는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 분류 모델을 통해 판단된 딥페이크 음성의 결과에 대한 절대값을 중심으로 상기 판단된 딥페이크 음성의공개특허 10-2024-0008424-3-결과에 대한 판단 근거가 해석되는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제4항에 있어서,상기 기여도 점수를 스펙트로그램으로 간주하고, 상기 간주된 스펙트로그램을 음성으로 재구성 재구성하여 딥페이크 음성으로 분류된 특성들이 해석되는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,Griffin-Lim 알고리즘을 이용하여 음성이 재구성되는 것을 특징으로 하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 판단하는 단계는,상기 분류 모델을 통해 상기 음성을 사용자의 음성 또는 딥페이크 음성으로 분류하는 단계를 포함하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 제공하는 단계는,상기 판단된 딥페이크 음성의 결과에 기초하여 소셜 네트워크 서비스에 업로드된 음성을 경고하는 단계 를 포함하는 음성 분류 방법."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항 내지 제11항 중 어느 한 항의 음성 분류 방법을 상기 음성 분류 시스템에 실행시키기 위해 비-일시적인컴퓨터 판독가능한 기록 매체에 저장되는 컴퓨터 프로그램."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "음성 분류 시스템에 있어서, 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하는 딥페이크 음성 판단부; 및 상기 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공하는 음성 판단 결과 제공부를 포함하고, 상기 분류 모델은, 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것인, 음성 분류 시스템."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 분류 모델에 적용된 설명가능한 인공지능 기법을 통해 음성의 특징 정보를 사용하여 획득한 기여도 점수가시각적 방법 또는 청각적 방법을 기반으로 해석되는 것을 특징으로 하는 음성 분류 시스템."}
{"patent_id": "10-2022-0082369", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13항에 있어서,상기 분류 모델을 통해 판단된 딥페이크 음성의 결과에 대한 절대값을 중심으로 상기 판단된 딥페이크 음성의결과에 대한 판단 근거가 해석되는 것을 특징으로 하는 음성 분류 시스템.공개특허 10-2024-0008424-4-"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "딥페이크 음성의 설명가능한 분류 모델과 그 설명에 대한 해석 기법이 개시된다. 일 실시예에 따른 음성 분류 시스템에 의해 수행되는 음성 분류 방법은, 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하는 단계; 및 상기 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공하는 단계를 포함하고, 상기 분류 모델은, 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것일 수 있다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 설명은 딥페이크 음성을 분류하는 기술에 관한 것이다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 합성 기술이 발달하고 일반 개인을 대상으로 한 데이터 수집이 용이해짐에 따라 딥페이크(DeepFake) 공격에 대해 일반 대중도 더 이상 안전하지 못하게 되었다. 시각 데이터를 대상으로는 페이스 스왑(Faceswap) 과 같이 원하는 이미지에 대상의 얼굴을 합성하는 방법이 있으며, 음성 데이터를 대상으로는 학습시킨 대상의 목소리로 원하는 구문을 TTS(Text-To-Speech) 방식으로 합성하거나 기존의 음성을 컨버팅하는 VoiceConversion 등의 방법이 전반적인 인공지능의 발달과 함께 일반적으로 식별할 수 없는 수준으로까지 발달하였다. 기존 딥페이크 감지 모델들은 주로 시각 딥페이크 감지에 초점을 맞춰져 있었으며, 음성 딥페이크 감지에는 상 대적으로 다뤄지지 않거나, 시각 딥페이크와 함께, 혹은 보조하는 수단으로만 분석이 되어왔다. 또한, 그 딥페 이크 분석의 설명가능성을 확보하는 시도 역시도 시각 딥페이크를 중심적으로 진행되어 왔다. 그러나, 시각 딥 페이크와 달리 데이터의 형태가 시계열이며 직관적으로 확인하기 어려운 만큼, 음성 딥페이크 역시도 시각 딥페 이크 이상으로 도메인 및 인공지능 전문가, 비전문가 모두 이해할 수 있는 설명가능성을 확보하는 것이 바람직 하다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하고, 판단된 딥페이크 음성의 결과를 판단 근거 와 함께 제공하는 방법 및 시스템을 제공할 수 있다. 분류 모델을 통해 판단된 딥페이크 음성의 결과에 대한 절대값을 중심으로 딥페이크 음성의 결과에 대한 판단 근거가 해석되는 방법 및 시스템을 제공할 수 있다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "음성 분류 시스템에 의해 수행되는 음성 분류 방법은, 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하는 단계; 및 상기 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공하는 단계를 포함하고, 상기 분 류 모델은, 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것일 수 있다. 상기 음성은, 소셜 네트워크 서비스(SNS)에 업로드된 음성을 포함할 수 있다. 상기 분류 모델은, CNN, CNN-LSTM 또는 CNN-LSTM-perm 중 어느 하나의 네트워크를 기반으로 구성된 것일 수 있 다. 상기 음성 분류 방법은, 상기 분류 모델에 적용된 설명가능한 인공지능 기법을 통해 음성의 특징 정보를 사용하 여 획득한 기여도 점수가 시각적 방법 또는 청각적 방법을 기반으로 해석될 수 있다. 상기 음성 분류 방법은, 상기 분류 모델의 파라미터들이 학습됨에 따라 상기 분류 모델의 예측에 대한 입력 데 이터의 기여도 분석을 통해 스펙트로그램 상의 히트맵의 형식으로 기여도 스코어(attribution score)가 도출되 는 것일 수 있다. 상기 설명가능한 인공지능 기법은, 계층별 관련성 전파(Layer-wise Relevance Propagation), 통합 기울기 (Integrated Gradients), 딥 테일러(Deep Taylor) 분해 중 어느 하나를 포함할 수 있다. 상기 음성 분류 방법은, 상기 분류 모델을 통해 판단된 딥페이크 음성의 결과에 대한 절대값을 중심으로 상기 판단된 딥페이크 음성의 결과에 대한 판단 근거가 해석될 수 있다. 상기 음성 분류 방법은, 상기 기여도 점수를 스펙트로그램으로 간주하고, 상기 간주된 스펙트로그램을 음성으로 재구성하여 딥페이크 음성으로 분류된 특성들이 해석될 수 있다. 상기 음성 분류 방법은, Griffin-Lim 알고리즘을 이용하여 음성이 재구성될 수 있다. 상기 판단하는 단계는, 상기 분류 모델을 통해 상기 음성을 사용자의 음성 또는 딥페이크 음성으로 분류하는 단 계를 포함할 수 있다. 상기 제공하는 단계는, 상기 판단된 딥페이크 음성의 결과에 기초하여 소셜 네트워크 서비스에 업로드된 음성을 경고하는 단계를 포함할 수 있다. 음성 분류 방법을 상기 음성 분류 시스템에 실행시키기 위해 비-일시적인 컴퓨터 판독가능한 기록 매체에 저장 되는 컴퓨터 프로그램을 포함할 수 있다. 음성 분류 시스템은, 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하는 딥페이크 음성 판단부; 및 상기 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공하는 음성 판단 결과 제공부를 포함하고, 상기 분류 모델은, 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것 일 수 있다. 상기 음성 분류 시스템은, 상기 분류 모델에 적용된 설명가능한 인공지능 기법을 통해 음성의 특징 정보를 사용 하여 획득한 기여도 점수가 시각적 방법 또는 청각적 방법을 기반으로 해석될 수 있다. 상기 음성 분류 시스템은, 상기 분류 모델을 통해 판단된 딥페이크 음성의 결과에 대한 절대값을 중심으로 상기 판단된 딥페이크 음성의 결과에 대한 판단 근거가 해석될 수 있다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "소셜 네트워크 서비스 또는 보안 분야 등 도메인 비전문가에게 딥페이크 음성 분류 결과에 대한 설명을 제공해 줄 수 있다. 설명가능한 인공지능 기법을 이용하여 분류 모델의 학습 양상을 사용자의 음성 분석 기법의 시각으로 해석할 수 있도록 지원할 수 있다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 실시예를 첨부한 도면을 참조하여 상세히 설명한다. 도 1은 일 실시예에 있어서, 음성 분류 시스템의 음성 모델을 이용하여 딥페이크 음성을 분류하는 동작을 설명 하기 위한 도면이다. 음성 분류 시스템은 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단하고, 판단된 딥페 이크 음성의 결과를 판단 근거(XAI 이용)와 함께 제공할 수 있다. 이때, 분류 모델은 음성 학습 데 이터 셋으로부터 추출된 스펙트로그램 기반의 특징 정보를 입력 데이터로 입력받아 딥페이크 음성을 분류하도록 학습된 것일 수 있다. 음성은 유성음(voiced), 무성음(unvoiced), 묵음(silence)을 포함하는 세 가지의 유형으 로 분류될 수 있다. 유성음은 주기적인 임펄스 시퀀스로 에너지를 제한하는 반면, 무성음은 비교적 무작위의 비주기적 노이즈와 유사한 시쿼스이다. 묵음은 의미있는 신호가 없는 구간을 가리킨다. 분류된 음성의 경우, 포먼트(formant)를 포함한 여러 가지 언어적 특징이 분석에 사용될 수 있다. 포먼트는 제한된 에너지를 가진 주파수이며, 스펙트럼에서 극대값을 나타낸다. 일반적인 사용자의 언어의 경우, 3-5개의 포먼트가 있으며, 저 주파 포먼트에서 오름차순으로 명명될 수 있다. 유성음 및 무성음에서 처음 3개의 포먼트는 메인프레임을 형성 하며, 대표적인 특징 중 하나로 간주된다. 유성음에는 피치(음높이)와 진폭도 주요 특징으로 간주된다. 이러 한 두 가지의 유성음의 특징은 강세 표현과 음성 억양을 나타낸다. 무성음의 경우, 국부 중심 주파수와 고주파 수에서의 진폭이 주요 특징으로 간주된다. 이러한 음향 정보를 기반으로 사용자 또는 알고리즘 분석을 통해 음 성 인식 또는 화자 인식이 수행될 수 있다. 예를 들면, 분류 모델의 일반적인 해석을 위해, 배타적 특성을 가진 두 개의 데이터 셋이 사용될 수 있다. 첫 번째 실험 셋은 ASVspoof 2021 논리적 액세스(Logical Access) 데이터 셋에 대해 수행될 수 있다. ASVspoof는 107개의 화자로부터 수집된 2580개의 진정한 사용자의 음성 데이터와 19개의 합성기를 사용하여 생성된 22,800 개의 합성 음성 데이터로 구성될 수 있다. 음성 분류 시스템은 진정한 사용자의 음성 데이터와 합성 음성 데이 터를 각각 '사용자의 음성'과 '딥페이크 음성'으로 분류할 수 있다. 딥페이크 음성 생성의 텍스트 입력을 위해 다양한 화자와 무작위로 샘플링된 트랜스크립트(transcript)를 사용하여 실험 환경은 거의 실제 행동을 나타낼 수 있다. 제한된 환경의 경우 사용자의 음성와 딥페이크 음성 모두에 대한 쌍(paired) 데이터가 필요하다. 13,100개의 트랜스크립트와 음성으로 구성된 LJSpeech 데이터 셋을 사용하고 해당 딥페이크 음성을 생성할 수 있다. LJSpeech 데이터 셋과 함께, 어텐션 기반 시퀀스 투 시퀀스 TTS 생성기인 Tacotron을 사용하여 생성 모 델이 학습될 수 있다. 전체 데이터 셋에서 8076개의 음성을 선택하고 해당 합성 음성을 준비할 수 있다. LJSpeech 음성과 합성 음성은 각각 '사용자의 음성'과 '딥페이크 음성'으로 분류되어 학습될 수 있다. 음성 분류 시스템은 보다 높은 수준의 해석을 위해 음성의 특징 정보를 사용하여 분류 모델에 적용된 설명가능 한 인공지능을 통해 획득한 기여도 점수를 해석할 수 있다. 음성 분류 시스템은 설명가능성과 해석을 용이하게 하기 위해 복수의 단순화된 모델의 구조를 사용하고, 완성된 분류 모델에 포스트-혹(post-hoc) 방식의 설명가능 한 인공지능(XAI) 기법을 적용할 수 있다. 분류 모델은 CNN, CNN-LSTM, CNN-LSTM-Permuted 기반으로 구 성된 것일 수 있다. 이외에도, 분류 모델은 다양한 네트워크를 기반으로 구성될 수 있다. 음성 분류 시스템은 푸리에 변환을 통해 복잡한 신호를 다양한 주파수에서 여러 스펙트럼으로 분류할 수 있다. 이러한 변환된 형태는 오디오 또는 비디오 분석에 사용될 수 있다. 음성 분류 시스템은 분석을 위해 특정 윈도 우 크기를 사용하여 오디오 파형의 각 주파수의 진폭을 분석할 수 있다. 일련의 스펙트럼은 시각화된 형태로 변환될 수 있다. 음성 분류 시스템은 분석을 위해 스펙트로그램을 사용하여 일반 파형 및 유성음-무성음 (voiced-unvoiced-silenced) 프레임과 같은 특성을 사용자가 육안으로 검사할 수 있다. 단기 푸리에 변환은 오디오 신호를 시간과 주파수의 2차원의 함수로 변환하는데 사용된다. 다음과 같이 단기 푸리에 변환은 표현될 수 있다. 수학식 1:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, 는 음성 신호이고, 는 윈도우 함수이다. 윈도우 함수에는 Hann 윈도우가 사용될 수 있다. 특히, 음성 분석에는 멜(mel) 스펙트로그램이 사용된다. 사용자는 선형 스케일의 오디오를 인식하지 못한다. 대신, 낮은 주파수에 초점을 맞춘다. 사용자 지각의 특성을 고려할 때 음성 분석은 전통적으로 멜(mel) 필터 뱅크를 사용하여 스펙트로그램을 멜(mel) 스케일로 변환한다. 도 4를 참고하면, 음성 분류 모델의 예를 나타낸 것이다. CNN은 이미지 데이터를 분석하는데 사용되는 신경망 중 하나이다. 맥락 정보를 보존할 수 있기 때문에 특징 추출에 사용된다. CNN은 컨볼루션 계층과 풀링 계층으 로 구성되며, 각각 활성화 함수를 사용하여 특징 벡터를 학습하고 데이터 크기를 줄이며, 상대적으로 대표성을 갖도록 한다. LSTM은 오디오가 스펙트로그램과 같은 시각화된 데이터로 변환되었지만, 다른 시각화된 데이터와는 달리 여전히 시계열적 특성을 가지고 있다. LSTM은 음성 또는 자연어 처리에 종종 사용되는 순차 모델이 사용된다. LSTM 이전의 순차 모델인 반복 신경망은 기울기(gradient) 소실 문제가 있다. 은닉 상태 LSTM에 셀 상태를 추가함으 로써 기울기 소실 문제가 해결될 수 있다. 전반적인 순차 데이터 및 작업에 대해 LSTM은 좋은 결과를 보여준다. 널리 사용되는 두 개의 간단한 네트워크를 사용하여, 표 1에 나열된 것과 같이, 분류 모델로 세 가 지의 모델이 설정될 수 있다. 표 1:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "대표적으로 CNN-LSTM 기반의 분류 모델의 구조 처리 워크플로우에 대하여 도 4를 참고하여 설명하기로 한다. CNN-LSTM 기반의 분류 모델은 모든 컨볼루션 계층에 대해 ReLU를 활성화 함수로 사용될 수 있으며, 커널 크기는 스트라이드 1로 3Х3으로 설정되고, 커널 크기는 스트라이드 1로 2Х2로 최대 풀링될 수 있다. 또한, CNN-LSTM-perm 구조의 경우 기본의 시간의 변화에 따른 특징을 중점적으로 잡아내는 것에 추가로 주파수 변화에 따른 특징을 확인할 수 있도록 LSTM 인풋 행렬을 치환할 수 있다. 모든, 분류 모델에는 완전히 연결된 밀집 계층인 FC-256이 구성될 수 있다. 실제 사람(사용자)의 음성과 딥페 이크 음성의 구분을 위한 마지막 FC 계층에는 소프트맥스(softmax)가 사용될 수 있다. ASVspoof 데이터 셋의 경우 딥페이크 음성의 생성 방법으로 구성된 4개의 하위 셋이 실험에 사용될 수 있다. LJSpeech의 경우 Tacotron으로 생성된 딥페이크 음성이 포함된 사용자 음성이 사용될 수 있다. 각 하위 집합은 무작위로 훈련 데이터의 80%, 검증 데이터의 10%, 테스트 데이터의 10%로 분할될 수 있다. 분류 모델은 50개의 음성 배치 크기로 100epoch 동안 아담 옵티마이저(Adam optimizer)로 학습될 수 있다. 학습률은 0.001로 설정 될 수 있다. 음성 분류 시스템은 학습이 완료된 분류 모델의 분류 정확도가 충분히 확보되었을 경우, 포스트-혹(post-hoc) 방식의 설명가능한 인공지능 기법들인 계층별 관련성 전파(Layer-wise Relevance Propagation), 통합 기울기 (Integrated Gradients), 딥 테일러(Deep Taylor) 분해를 포함하는 세 가지의 기법을 적용하여 분류 모델의 파 라미터들을 학습하고, 분류 모델의 예측에 대한 각 입력의 기여도를 분석하여 스펙트로그램 상의 히트맵의 형식 으로 기여도 스코어(attribution score)를 도출할 수 있다. 이때, 보통의 설명가능한 인공지능의 산출물에 대 한 해석은 부호를 중요시 하였으나, 실시예에서는 각각의 입력의 수치적인 기여도보다는 맥락적 해석을 보기 위 해 전반적인 절대값을 중심으로 해석할 수 있다. 보다 상세하게는, 통합 기울기 기반의 설명가능한 인공지능에 대하여 설명하기로 한다. 기여도 방법의 성능은 일반적으로 기여도 스코어가 높은 데이터 섭동(perturbation)에 의한 정확도 저하 측면에서 평가된다. 그러나 정확도 하락은 외부 요인에 의해 촉발될 수 있는 인과관계보다 상관관계를 평가한다. 적절하게 평가하기 위해 통합 기울기는 민감도와 불변성의 구현이라는 두 가지 원리를 고려한다. 경로 적분은 비선형 경로를 고려하고 여러 기여도 방법을 통합하여 적절한 기여도 평가를 수행할 수 있다. 경로 적분 공식을 사용하면, 통합 기울기 가 원리를 만족시키고 기여도 점수를 적절하게 계산할 수 있다. 신경망에서 입력과 기준 입력으로 각각 와 를 갖는 함수가 있다고 가정하기로 하 자. 모든 입력에 대한 기울기를 누적함으로써 통합 기울기를 획득할 수 있다. 입력 와 기준 입력 사이의 기울기의 경로 적분을 다음과 같이 계산할 수 있다. 수학식 2: 여기서, 는 차원 i에 대한 함수 의 기울기이고, 는 특징의 섭동에 의한 보간 상수이다. 다음으로, 테일러 분해 기반 설명가능한 인공지능에 대하여 설명하기로 한다. 여러 설명가능한 인공지능 방법 은 이미지 분류 또는 자연어 처리를 위한 모델 예측을 설명하기 위해 시각적 접근 방식을 사용한다. 이러한 방 법은 입력 데이터에 대한 히트맵을 생성하여 예측에 대한 입력의 연관성을 강조 표시한다. 세 가지의 주요 아이디어를 사용하여, 연관성 기반 설명가능한 인공지능 방법은 다음과 같이 점수가 계산될 수 있다. 각 계층의 모든 노드는 일정량의 연관성을 가지고 있으며, 연관성은 노드의 출력에서 입력으로 하 향식으로 재분포되며, 각 계층의 연관성 점수의 총량은 유지된다. 일련의 재분포를 통해 각 입력 데이터의 연관성 점수가 계산될 수 있다. 주어진 함수 를 분해하고 입력 의 각 연관성 점수 계산에는 테일러 급수 가 사용될 수 있다. 1차 테일러 급수에서 는 다음과 같이 정의할 수 있다. 수학식 3:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 는 임의 상수이고, 는 오차항이다. 각각 룰(rule)과 ReLU의 속성에 의해 와 는 0에 근사 할 수 있다. 근사를 통해 는 다음과 같이 정의할 수 있다. 수학식 4, 수학식 5:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, 는 함수 에 대한 의 연관성을 나타낸다. 출력을 모든 노드에 대한 입력의 연관성 점수의 합으로 반복적으로 분해함으로써 예측의 히트맵이 획득될 수 있다. 음성 분류 시스템은 Griffin-Lim 알고리즘을 이용하여 음성을 재구성할 수 있다. Griffin-Lim 알고리즘은 신경 보코더 이전에 위상 재구성 규칙 기반 보코더로 일반적으로 사용되는 알고리즘이다. 스펙트로그램의 두 가지 투영으로 단시간 푸리에 변환의 중복성을 사용한다. Griffin-Lim 알고리즘은 단시간 푸리에 변환의 일관성만 활용하고 사전 정보는 제외한다. Griffin-Lim 알고리즘은 다음과 같은 프로세스를 통해 오디오 신호에 대해 주 어진 진폭 A의 스펙트로그램을 재구성한다. 수학식 6:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, 는 반복 m 에서의 스펙트로그램이고, 와 는 특정한 진폭을 가지는 일관된 스펙트로그램 및 스 펙트로그램 샛의 메트릭 투영이다. 메트릭 투영은 다음과 같이 제공된다. 수학식 7:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 8:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, 와 는 각각 단시간 푸리에 변환 및 역 단시간 푸리에 변환을 나타낸다. Griffin-Lim 알고리즘의 최적화 문제는 다음과 같이 도출될 수 있다. 수학식 9:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, 는 프로베니우스 규범(Frobenius norm)이다. 반복에 의한 최적화로, 스펙트로그램은 수렴된다. 다양한 다른 재구성 알고리즘이나 신경 보코더를 사용할 수 있지만, 단순성 때문에 Griffin-Lim 알고리즘이 대 신 채택된다. 음성 분류 시스템은 기여도 점수를 스펙트로그램으로 간주하고, 간주된 스펙트로그램을 다시 오 디오로 재구성 재구성하여 딥페이크 음성으로 분류된 특징들을 직관적으로 확인할 수 있다. 에너지에 대한 기여도 점수의 일반적인 행동을 고려할 때, 기여도 점수는 원래 오디오와 스펙트로그램의 일반적 인 형태를 반영할 것으로 예상될 수 있다. 이러한 탐욕은 이미지 분류에서도 발견되었는데, 객체 배경 분할에 관계없이 주로 고대비 윤곽에서 무작위로 보이는 양-음의 기여도 점수가 강조될 수 있다. 완벽하지 않은 분할 에도 불구하고, 사용자의 시각적 인식의 간섭으로 인해 설명가능한 인공지능 방법이 해석 가능성을 제공할 수 있게 한다. 이에, 음성 분류 시스템은 오디오 작업뿐만 아니라 시각적 작업에도 사용되어 온 사용자의 인식을 지원하는 설명가능한 인공지능 기법을 적용할 수 있다. 도 2는 일 실시예에 있어서, 음성 분류 시스템의 구성을 설명하기 위한 블록도이고, 도 3은 일 실시예에 있어서, 음성 분류 방법을 설명하기 위한 흐름도이다. 음성 분류 시스템의 프로세서는 딥페이크 음성 판단부 및 음성 판단 결과 제공부를 포함할 수 있다. 이러한 프로세서의 구성요소들은 음성 분류 시스템에 저장된 프로그램 코드가 제공하는 제어 명령에 따 라 프로세서에 의해 수행되는 서로 다른 기능들(different functions)의 표현들일 수 있다. 프로세서 및 프로 세서의 구성요소들은 도 3의 음성 분류 방법이 포함하는 단계들(310 내지 320)을 수행하도록 음성 분류 시스템 을 제어할 수 있다. 이때, 프로세서 및 프로세서의 구성요소들은 메모리가 포함하는 운영체제의 코드와 적어도 하나의 프로그램의 코드에 따른 명령(instruction)을 실행하도록 구현될 수 있다. 프로세서는 음성 분류 방법을 위한 프로그램의 파일에 저장된 프로그램 코드를 메모리에 로딩할 수 있다. 예를 들면, 음성 분류 시스템에서 프로그램이 실행되면, 프로세서는 운영체제의 제어에 따라 프로그램의 파일로부터 프로그램 코드를 메모리에 로딩하도록 음성 분류 시스템을 제어할 수 있다. 이때, 딥페이크 음성 판단부 및 음성 판단 결과 제공부 각각은 메모리에 로딩된 프로그램 코드 중 대응하는 부분의 명령을 실행하여 이 후 단계들(310 내지 320)을 실행하기 위한 프로세서의 서로 다른 기능적 표현들일 수 있다. 단계에서 딥페이크 음성 판단부는 음성에 대해 분류 모델을 통해 딥페이크 음성인지 여부를 판단할 수 있다. 딥페이크 음성 판단부는 분류 모델을 통해 음성을 사용자의 음성 또는 딥페이크 음성으로 분류 할 수 있다. 딥페이크 음성 판단부는 소셜 네트워크 서비스(SNS)에 업로드된 음성을 분류 모델에 입력받 을 수 있다. 딥페이크 음성 판단부는 분류 모델을 이용하여 소셜 네트워크 서비스에 업로드된 음성으로부 터 딥페이크 음성인지 여부를 판단할 수 있다. 예를 들면, 메시징 서비스에서 송수신되는 메시지에 음성이 업 로드될 수 있다. 딥페이크 음성 판단부는 메시징 서비스에서 송수신되는 메시지를 통해 업로드된 음성에 대하여 분류 모델을 이용하여 딥페이크 음성 여부를 판단할 수 있다. 또는, 적어도 하나 이상의 사용자와 의견 을 공유하는 환경에서 게시글에 음성이 업로드될 수 있다. 딥페이크 음성 판단부는 적어도 하나 이상의 사용자와 의견을 공유하는 환경에서 게시글에 업로드된 음성에 대하여 분류 모델을 통해 딥페이크 음성 여부를 판단할 수 있다. 이외에도 소셜 네트워크 서비스는 다양한 형태로 존재할 수 있으며, 음성이 업로드되는 환경 이면 무방하다. 이때, 분류 모델은 음성 학습 데이터로부터 추출된 스펙트로그램을 이용하여 딥페이크 음성을 분류하도록 학습된 것일 수 있다. 단계에서 음성 판단 결과 제공부는 판단된 딥페이크 음성의 결과를 판단 근거와 함께 제공할 수 있다. 음성 판단 결과 제공부는 판단된 딥페이크 음성의 결과에 기초하여 소셜 네트워크 서비스에 업로드 된 음성을 경고할 수 있다. 예를 들면, 음성 판단 결과 제공부는 소셜 네트워크 서비스에 업로드된 음성 이 딥페이크 음성으로 판단될 경우, 공개된 게시글을 비공개로 전환시킬 수 있다. 또는, 음성 판단 결과 제공 부는 푸시 메시지와 함께 진동을 발생시켜 업로드된 음성에 대한 적절한 조치를 취할 수 있도록 할 수 있 다. 또한, 음성 판단 결과 제공부는 분류 모델을 통해 업로드된 음성이 딥페이크 음성으로 분류될 경우, 시청자에게 딥페이크일 수 있음을 경고하고, 그에 대한 근거로 설명가능한 인공지능 결과물을 제시하여 가짜 뉴스가 전파되는 것을 방지하며, 미디어에 대한 판단의 신뢰를 확보할 수 있다. 도 5 내지 도 8은 일 실시예에 있어서, 음성 모델을 사용한 기여도 점수를 설명하기 위한 그래프이다. 음성 분류 시스템은 세 가지의 설명가능한 인공지능 기법의 출력을 해석할 수 있다. 각 설명가능한 인공지능 기법은 두 가지 눈에 띄는 차이점과 함께 일관된 결과를 보여준다. 음성 분류 시스템은 멜 스펙트로그램과 멜 스펙트로그램에 대응되는 기여도 점수를 시각화 데이터로 검토할 수 있다. 그런 다음, 음성 분류 시스템은 기 여도 점수를 사용하여 오디오로 재구성하고, 음향적 접근 방식을 사용하여 검사할 수 있다. 데이터와 해석 형 식을 일치시킴으로써 시각적 해석만으로는 공식적으로 인식되지 않는 특성이 드러났다. 샘플 데이터는 각 실험 환경의 테스트 셋에서 선택될 수 있다. ASVspoof 데이터 셋의 경우, 단일의 TTS 모델에 서 생성된 딥페이크 음성, 복수 개의 TTS 모델에서 생성된 딥페이크 음성, 단일의 음성 변환 모델에 의해 생성 된 딥페이크 음성, 복수 개의 음성 변환 모델에 의해 생성된 딥페이크 음성을 포함하는 4개의 하위 셋이 사용될 수 있다. 음성 분류 시스템은 실시예에서 제안된 방법의 적용 가능성을 확인하기 위해 ASVspoof LA와 LJSpeech 및 이를 기반으로 합성된 음성 데이터를 이용할 수 있다. 음성 분류 시스템은 각 데이터에서 임의로 선정한 80%를 분류 모델 학습을 위한 학습 데이터, 10%를 검증 데이터, 그리고 나머지 10%를 테스트 데이터로 사용할 수 있다. ASVspoof LA 데이터의 경우 다양한 음성 합성기를 통해 딥페이크 음성이 형성되어 있는데, 이를 합성기 종류에"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "따라 환경을 세분화하여 학습할 수 있다. 각 환경별 분류 정확도는 표 2와 같이 요약될 수 있다. 설명가능한 인공지능 기법의 산출물에 대한 정확도가 확보될 수 있다. 표 2:"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 11, "content": "도 5 내지 도 7을 참고하면, LJSpeech 데이터를 대상으로 CNN, CNN-LSTM, CNN-LSTM-perm 기반 분류 모델에 각 설명가능한 인공지능 기법을 적용한 결과물들의 예시이다. 도 5 내지 도 7에서 각각의 (a), (b), (c)은 딥페이 크 음성에 대한 결과물이며, 도 5내지 도 7에서 각각의 (d), (e), (f)는 사용자의 음성에 대한 결과물이다. CNN 기반의 분류 모델의 경우 전반적으로 유사한 양상을 보이며, F1, F2, F3 과 같은 구역에 음성의 포먼트 (Formant)를 강조하며, 무성음 구간을 강조하는 현상이 모든 설명가능한 인공지능 기법들 상에서 공통적으로 확 인이 가능하다. CNN-LSTM 및 CNN-LSTM-perm기반의 분류 모델에 시계열 및 주파수 특징이 고려되면 설명가능한 인공지능의 결과물의 경향성이 변화되는 것이 확인된다. 통합 기울기의 경우 F1, F2, F3와 같이 포먼트를 중점 적으로 강조되는 한편, LRP의 경우 U의 무성음 구간의 저주파 구역이 강조된다. 특히 딥 테일러의 경우 C지점 인 2kHz 이상의 주파수 대역을 중점으로 강조되는 것을 확인할 수 있다. 기여도 점수의 도움을 기반으로 도 8 에서 구간 D의 스펙트로그램을 검사한 결과, 딥페이크 음성은 2kHz 부근에서 평탄한 파형을 보였던 반면, 사용 자의 음성은 더 많은 변화를 보이는 것을 확인할 수 있다. 딥페이크 음성의 구간이 상대적으로 단조로운 것이 스펙트로그램 상에서 확인되며, 스펙트로그램은 직관적으로 딥페이크 음성의 분류를 도울 수 있는 정보로 활용할 수 있다. 이와 같이, 음성 분류 시스템은 분류 모델의 학 습 양상을 설명가능한 인공지능 기법으로 모델의 학습 양상을 기존 음성 분석 기법의 시각으로 해석할 수 있도 록 지원한다. 보다 상세하게는, 음성 분류 시스템은 분류 모델의 결과를 해석하기 위해 시각적 방법, 즉, 스펙트로그램의 히 트맵을 사용하는 것에 초점을 맞추고 있다. 음성 분류 시스템은 Griffin-Lim 알고리즘을 사용하여 각 모델의 기여도 점수와 설명가능한 인공지능 방법을 다시 오디오로 재구성할 수 있다. CNN 전용 모델의 경우 설명가능 한 인공지능 방법이 에너지 유사 포먼트에 의존하는 경향이 있기 때문에 기여도 점수에서 변환하는 동안 원래 음성의 대부분이 복구될 수 있다. 원음의 트랜스크립트는 재구성된 음성에서도 인식할 수 있다. CNN-LSTM 기 반의 분류 모델에서 딥 테일러와 LRP의 기여도 점수는 에너지에 대한 민감도가 낮아 포먼트에서 하이라이트가 손실될 수 있다. 결과적으로 녹취록은 재구성된 음성에서 인식할 수 없다. CNN-LSTM기반의 분류 모델에서 딥테일러와 LRP의 재구성된 음성의 경우 포먼트 1, 2, 3의 감도가 부족하여 음성의 트랜스크립트 인식할 수 없다. 그러나 재구성된 음성을 통해 CNN-LSTM 모델의 기여도 점수에 대한 포먼트 의존성이 제거되면서 시각적으로 인 식할 수 없는 특성, 일반적인 음높이(피치) 변화 및 음성의 리듬이 드러났다. 따라서, 재구성된 음성에 대한 LRP 기여도 점수는 유성음 구간과 무성음 구간에서 상대적으로 뚜렷한 차이를 보인다. 딥 테일러 기반 재구성 음성의 리듬 및 음높이의 차이는 사용자의 음성과 딥페이크 음성을 구별하는 데 도움이 될 수 있다. 이러한 특 징은 생성 모델이 학습하지 않은 특징인 억양으로 추정되는 딥페이크에 비해 사용자의 음성의 편차가 상당히 큰 것으로 나타난다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2022-0082369", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2022-0082369", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 있어서, 음성 분류 시스템의 음성 모델을 이용하여 딥페이크 음성을 분류하는 동작을 설명 하기 위한 도면이다. 도 2는 일 실시예에 있어서, 음성 분류 시스템의 구성을 설명하기 위한 블록도이다. 도 3은 일 실시예에 있어서, 음성 분류 방법을 설명하기 위한 흐름도이다. 도 4는 일 실시예에 있어서, 음성 모델을 설명하기 위한 예이다. 도 5 내지 도 8은 일 실시예에 있어서, 음성 모델을 사용한 기여도 점수를 설명하기 위한 그래프이다."}
