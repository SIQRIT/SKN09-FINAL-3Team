{"patent_id": "10-2022-0004125", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0128935", "출원번호": "10-2022-0004125", "발명의 명칭": "사전 트레이닝 모델의 획득 방법 및 장치", "출원인": "베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니", "발명자": "니우, 구오청"}}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사전 트레이닝 모델의 획득 방법(method for obtaining pre-trained model)에 있어서, 트레이닝 데이터를 획득하는 단계 - 상기 트레이닝 데이터는 단일 모달 코퍼스(single-modal corpus)와 멀티 모달 코퍼스(multi-modal corpus)를 포함하고, 상기 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함함 -; 및상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하는 단계 - 상기 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크(cross-modal contrastive learning task)와 적어도하나의 단일 모달 학습 태스크(single-modal learning task)를 포함함 -;를 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 상기 멀티 모달 코퍼스 내의 상기 제1 모달코퍼스에 대한 벡터 표현과 상기 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 상기 단일 모달 코퍼스 내의 제1 부분 내용에 대한벡터 표현을 사용하여, 상기 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는예측에 의해 획득된 제2 부분 내용과 상기 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는 것인, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 단일 모달 코퍼스는 이미지와 텍스트를 포함하고, 상기 멀티 모달 코퍼스는 이미지-텍스트 쌍을 포함하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 트레이닝 데이터 내의 멀티 모달 코퍼스에 대해 고쳐 쓰기 확장과 검색 확장 중의 적어도 하나를수행하고, 확장에 의해 획득된 멀티 모달 코퍼스를 상기 트레이닝 데이터에 추가하는 단계를 더 포함하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 고쳐 쓰기 확장은, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번역 모델을 사용하여 상기제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하거나, 공개특허 10-2022-0128935-3-또는, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 상기 장면 그래프의 엔티티,속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획득된 장면 그래프를 제1 모달 코퍼스로 변환하고, 변환 후에 획득한 제1 모달 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는 단계;를 포함하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 검색 확장은, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터베이스에서 검색하고, 검색된코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티모달 코퍼스를 구축하는 단계를 포함하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항, 제4항 또는 제5항에 있어서,상기 크로스 모달 비교 학습 태스크에서, 상기 검색 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 상기 사전 트레이닝 모델이 제1 모달 코퍼스에서 획득된 벡터 표현과 상기 사전 트레이닝 모델이 제2 모달 코퍼스에서 획득된 벡터 표현에 대해 유사도 계산을 수행하는 방식을 사용하여 결정하고, 상기 고쳐 쓰기 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 제1 모달 코퍼스와 제2 모달 코퍼스를 스플라이싱하고, 상기 사전 트레이닝 모델이 스플라이싱 후의 코퍼스로부터 획득된 벡터 표현을 유사도의 값에 매핑하는 방식을 사용하여 결정하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 단일 모달 학습 태스크는, 상기 사전 트레이닝 모델이 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현및 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 제1 모달 코퍼스 내의 제2 부분 내용을 예측하는 단계를 더 포함하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 상기 제1 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 멀티 태스크 트레이닝 시, 구축된 총 손실 함수를 사용하여 상기 사전 트레이닝 모델의 파라미터를 업데이트하고, 상기 총 손실 함수는 상기 적어도 하나의 크로스 모달 비교 학습 태스크의 손실 함수와 상기 적어도 하나의 단일 모달 학습 태스크의 손실 함수의 합에 의해 획득되는, 사전 트레이닝 모델의 획득 방법."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "공개특허 10-2022-0128935-4-사전 트레이닝 모델의 획득 장치(device for obtaining pre-trained model)에 있어서, 트레이닝 데이터를 획득하기 위한 획득 유닛 - 상기 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를포함하고, 상기 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함함 -; 및상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하기 위한 트레이닝유닛 - 상기 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함함 -;을 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대한 벡터 표현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스와제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터표현을 사용하여, 상기 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 상기 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는 것인, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 단일 모달 코퍼스는 이미지와 텍스트를 포함하고, 상기 멀티 모달 코퍼스는 이미지-텍스트 쌍을 포함하는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 장치는, 상기 트레이닝 데이터 내의 멀티 모달 코퍼스에 대해 고쳐 쓰기 확장과 검색 확장 중의 적어도 하나를수행하고, 확장에 의해 획득된 멀티 모달 코퍼스를 상기 트레이닝 데이터에 추가하기 위한 확장 유닛을 더 포함하는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 확장 유닛은 상기 고쳐 쓰기 확장을 수행할 경우, 구체적으로, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번역 모델을 사용하여 상기제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하거나, 또는, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 상기 장면 그래프의 엔티티,속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획득된 장면 그래프를 제1 모달 코퍼스로 변환하고, 변환 후에 획득한 제1 모달 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는데 사용되는, 사전 트레이닝 모델의 획득 장치. 공개특허 10-2022-0128935-5-청구항 13 제11항에 있어서,상기 확장 유닛은 상기 검색 확장을 수행할 경우, 구체적으로, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터베이스에서 검색하고, 검색된코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티모달 코퍼스를 구축하는데 사용되는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항, 제12항 또는 제13항에 있어서,상기 트레이닝 유닛이 상기 크로스 모달 비교 학습 태스크를 수행하는 중에, 상기 검색 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 상기 사전 트레이닝 모델이 제1 모달 코퍼스에서 획득된 벡터 표현과 상기 사전 트레이닝 모델이 제2 모달 코퍼스에서 획득된 벡터 표현에 대해 유사도 계산을 수행하는 방식을 사용하여 결정하고, 상기 고쳐 쓰기 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 제1 모달 코퍼스와 제2 모달 코퍼스를 스플라이싱하고, 상기 사전 트레이닝 모델이 스플라이싱 후의 코퍼스로부터 획득된 벡터 표현을 유사도의 값에 매핑하는 방식을 사용하여 결정하는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,상기 트레이닝 유닛은 또한, 상기 단일 모달 학습 태스크를 수행할 경우, 상기 사전 트레이닝 모델이 상기 멀티모달 코퍼스 내의 제1 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현 및 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 제1 모달 코퍼스 내의 제2 부분 내용을 예측하고, 트레이닝 목표는 예측에 의해 획득된 제2부분 내용과 상기 제1 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항에 있어서,상기 트레이닝 유닛은 구체적으로, 상기 멀티 태스크 트레이닝을 할 때, 구축된 총 손실 함수를 사용하여 상기사전 트레이닝 모델의 파라미터를 업데이트하고, 상기 총 손실 함수는 상기 적어도 하나의 크로스 모달 비교 학습 태스크의 손실 함수와 상기 적어도 하나의 단일 모달 학습 태스크의 손실 함수의 합에 의해 획득되는, 사전 트레이닝 모델의 획득 장치."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 통신 연결되는 메모리;를 포함하고, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서에 의해 제1항 내지 제8항 중 어느 한 항의 방법이 수행되도록 하는, 공개특허 10-2022-0128935-6-전자 기기."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제8항 중 어느 한 항의 방법을 수행하도록 하는, 비일시적 컴퓨터 판독 가능 기록 매체."}
{"patent_id": "10-2022-0004125", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "비일시적 컴퓨터 판독 가능 기록 매체에 저장되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 상기 컴퓨터가 제1항 내지 제8항 중 어느 한 항의 방법을 수행하도록 하는, 비일시적 컴퓨터 판독 가능 기록 매체에 저장되어 있는 컴퓨터 프로그램."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공 지능"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에서의 자연 언어 처리와 딥 러닝 기술에 관한 사전 트레이닝 모델의 획득 방법 및 장치를 개시한다. 구체적인 구현 방안은 트레이닝 데이터를 획득하고, 상기 트레이닝 데이터는 단일 모달 코 퍼스와 멀티 모달 코퍼스를 포함하고, 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼 스 쌍을 포함하고, 상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하 고, 상기 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함한다. 본 발명에서 획득된 사전 트레이닝 언어 모델은 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하는 서 로 다른 형식의 코퍼스로부터 학습하여, 사전 트레이닝 언어 모델이 다양한 서로 다른 모달의 정보를 효과적으로 처리할 수 있다. 대 표 도 - 도1"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2022-0128935 CPC특허분류 G06F 40/279 (2020.01) G06F 40/58 (2020.01) G06N 3/04 (2013.01) 발명자 가오, 찬 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스 2층 씨아오, 씬옌 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스 2층우, 화 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스 2층명 세 서 청구범위 청구항 1 사전 트레이닝 모델의 획득 방법(method for obtaining pre-trained model)에 있어서, 트레이닝 데이터를 획득하는 단계 - 상기 트레이닝 데이터는 단일 모달 코퍼스(single-modal corpus)와 멀티 모 달 코퍼스(multi-modal corpus)를 포함하고, 상기 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구 성되는 코퍼스 쌍을 포함함 -; 및 상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하는 단계 - 상기 멀 티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크(cross-modal contrastive learning task)와 적어도 하나의 단일 모달 학습 태스크(single-modal learning task)를 포함함 -; 를 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 상기 멀티 모달 코퍼스 내의 상기 제1 모달 코퍼스에 대한 벡터 표현과 상기 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 상기 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용하여, 상기 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 상기 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는 것인, 사전 트레이닝 모델의 획득 방법. 청구항 2 제1항에 있어서, 상기 단일 모달 코퍼스는 이미지와 텍스트를 포함하고, 상기 멀티 모달 코퍼스는 이미지-텍스트 쌍을 포함하는, 사전 트레이닝 모델의 획득 방법. 청구항 3 제1항에 있어서, 상기 트레이닝 데이터 내의 멀티 모달 코퍼스에 대해 고쳐 쓰기 확장과 검색 확장 중의 적어도 하나를 수행하고, 확장에 의해 획득된 멀티 모달 코퍼스를 상기 트레이닝 데이터에 추가하는 단계를 더 포함하는, 사전 트레이닝 모델의 획득 방법. 청구항 4 제3항에 있어서, 상기 고쳐 쓰기 확장은, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번역 모델을 사용하여 상기 제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트 와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코 퍼스를 구축하거나, 또는, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 상기 장면 그래프의 엔티티, 속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획득된 장면 그래프를 제1 모달 코퍼스로 변환 하고, 변환 후에 획득한 제1 모달 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용 하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는 단계;를 포함하는, 사전 트레이닝 모델의 획득 방법. 청구항 5 제3항에 있어서, 상기 검색 확장은, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터베이스에서 검색하고, 검색된 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하는 단계를 포함하는, 사전 트레이닝 모델의 획득 방법. 청구항 6 제3항, 제4항 또는 제5항에 있어서, 상기 크로스 모달 비교 학습 태스크에서, 상기 검색 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 상 기 사전 트레이닝 모델이 제1 모달 코퍼스에서 획득된 벡터 표현과 상기 사전 트레이닝 모델이 제2 모달 코퍼스 에서 획득된 벡터 표현에 대해 유사도 계산을 수행하는 방식을 사용하여 결정하고, 상기 고쳐 쓰기 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도 는, 제1 모달 코퍼스와 제2 모달 코퍼스를 스플라이싱하고, 상기 사전 트레이닝 모델이 스플라이싱 후의 코퍼스 로부터 획득된 벡터 표현을 유사도의 값에 매핑하는 방식을 사용하여 결정하는, 사전 트레이닝 모델의 획득 방법. 청구항 7 제1항에 있어서, 상기 단일 모달 학습 태스크는, 상기 사전 트레이닝 모델이 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현 및 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 제1 모달 코퍼스 내의 제2 부분 내용을 예측하는 단계 를 더 포함하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 상기 제1 모달 코퍼스 내의 제2 부분 내 용의 차이를 최소화하는, 사전 트레이닝 모델의 획득 방법. 청구항 8 제1항에 있어서, 상기 멀티 태스크 트레이닝 시, 구축된 총 손실 함수를 사용하여 상기 사전 트레이닝 모델의 파라미터를 업데이 트하고, 상기 총 손실 함수는 상기 적어도 하나의 크로스 모달 비교 학습 태스크의 손실 함수와 상기 적어도 하나의 단 일 모달 학습 태스크의 손실 함수의 합에 의해 획득되는, 사전 트레이닝 모델의 획득 방법. 청구항 9 사전 트레이닝 모델의 획득 장치(device for obtaining pre-trained model)에 있어서, 트레이닝 데이터를 획득하기 위한 획득 유닛 - 상기 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하고, 상기 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함함 -; 및 상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하기 위한 트레이닝 유닛 - 상기 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스 크를 포함함 -; 을 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대 한 벡터 표현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용하여, 상기 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측 에 의해 획득된 제2 부분 내용과 상기 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는 것인, 사전 트레이닝 모델의 획득 장치. 청구항 10 제9항에 있어서, 상기 단일 모달 코퍼스는 이미지와 텍스트를 포함하고, 상기 멀티 모달 코퍼스는 이미지-텍스트 쌍을 포함하는, 사전 트레이닝 모델의 획득 장치. 청구항 11 제9항에 있어서, 상기 장치는, 상기 트레이닝 데이터 내의 멀티 모달 코퍼스에 대해 고쳐 쓰기 확장과 검색 확장 중의 적어도 하나를 수행하고, 확장에 의해 획득된 멀티 모달 코퍼스를 상기 트레이닝 데이터에 추가하기 위한 확장 유닛을 더 포함 하는, 사전 트레이닝 모델의 획득 장치. 청구항 12 제11항에 있어서, 상기 확장 유닛은 상기 고쳐 쓰기 확장을 수행할 경우, 구체적으로, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번역 모델을 사용하여 상기 제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트 와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코 퍼스를 구축하거나, 또는, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 상기 장면 그래프의 엔티티, 속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획득된 장면 그래프를 제1 모달 코퍼스로 변환 하고, 변환 후에 획득한 제1 모달 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용 하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는데 사용되는, 사전 트레이닝 모델의 획득 장치. 청구항 13 제11항에 있어서, 상기 확장 유닛은 상기 검색 확장을 수행할 경우, 구체적으로, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터베이스에서 검색하고, 검색된 코퍼스와 상기 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하는데 사용되는, 사전 트레이닝 모델의 획득 장치. 청구항 14 제11항, 제12항 또는 제13항에 있어서, 상기 트레이닝 유닛이 상기 크로스 모달 비교 학습 태스크를 수행하는 중에, 상기 검색 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 상 기 사전 트레이닝 모델이 제1 모달 코퍼스에서 획득된 벡터 표현과 상기 사전 트레이닝 모델이 제2 모달 코퍼스 에서 획득된 벡터 표현에 대해 유사도 계산을 수행하는 방식을 사용하여 결정하고, 상기 고쳐 쓰기 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도 는, 제1 모달 코퍼스와 제2 모달 코퍼스를 스플라이싱하고, 상기 사전 트레이닝 모델이 스플라이싱 후의 코퍼스 로부터 획득된 벡터 표현을 유사도의 값에 매핑하는 방식을 사용하여 결정하는, 사전 트레이닝 모델의 획득 장치. 청구항 15 제9항에 있어서, 상기 트레이닝 유닛은 또한, 상기 단일 모달 학습 태스크를 수행할 경우, 상기 사전 트레이닝 모델이 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현 및 제2 모달 코퍼스에 대한 벡터 표현 을 사용하여, 상기 제1 모달 코퍼스 내의 제2 부분 내용을 예측하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 상기 제1 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화하는, 사전 트레이닝 모델의 획득 장치. 청구항 16 제9항에 있어서, 상기 트레이닝 유닛은 구체적으로, 상기 멀티 태스크 트레이닝을 할 때, 구축된 총 손실 함수를 사용하여 상기 사전 트레이닝 모델의 파라미터를 업데이트하고, 상기 총 손실 함수는 상기 적어도 하나의 크로스 모달 비교 학습 태스크의 손실 함수와 상기 적어도 하나의 단 일 모달 학습 태스크의 손실 함수의 합에 의해 획득되는, 사전 트레이닝 모델의 획득 장치. 청구항 17 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 통신 연결되는 메모리;를 포함하고, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서에 의해 제1항 내지 제8항 중 어느 한 항 의 방법이 수행되도록 하는, 전자 기기. 청구항 18 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제8항 중 어느 한 항의 방법을 수행하도록 하는, 비일시적 컴퓨터 판독 가능 기록 매체. 청구항 19 비일시적 컴퓨터 판독 가능 기록 매체에 저장되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 상기 컴퓨터가 제1항 내지 제8항 중 어느 한 항의 방법을 수행하도록 하는, 비일시적 컴퓨터 판독 가능 기록 매체에 저장되어 있는 컴퓨터 프로그램. 발명의 설명 기 술 분 야 본 발명은 컴퓨터 응용 기술의 분야에 관한 것이고, 특히, 인공 지능 기술 분야에서의 자연 언어 처리(natural language processing) 및 딥 러닝(deep learning) 기술에 관한 것이다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "대규모 사전 트레이닝 모델은 강력한 일반화 능력과 대규모 데이터의 효율적인 이용으로 인해 널리 주목받고 있 다. 기존의 사전 트레이닝 방법은 대부분이 단일 모달 장면(uni-modal scenes)에만 사용된다. 예를 들어, 이미 지 또는 텍스트에만 사용된다, 그러나, 인간은 시각, 언어, 소리 등과 같은, 여러 가지 방식으로 세계를 인식한다. 여러 가지 모달(multiple modalities)의 정보를 결합하여 정보를 더 잘 이해할 수 있으므로, 뛰어난 인공 지능 시스템은 다양한 서로 다 른 모달(different modalities)의 정보를 효과적으로 처리할 수 있어야 한다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 크로스 모달의 사전 트레이닝 모델의 획득 방법, 장치, 기기, 컴퓨터 기록 매체 및 프로그램 제품을 제공한다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 제1 측면에 따르면, 사전 트레이닝 모델의 획득 방법을 제공하고, 트레이닝 데이터를 획득하는 단계 - 상기 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하고, 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함함 -; 상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하는 단계 - 상기 멀 티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함함 -;를 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대 한 벡터 표현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용하여, 당해 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내용과 당해 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화한다. 본 발명의 제2 측면에 따르면, 사전 트레이닝 모델의 획득 장치를 제공하고, 트레이닝 데이터를 획득하기 위한 획득 유닛 - 상기 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하고, 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함함 -; 상기 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하기 위한 트레이닝 유닛 - 상기 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스 크를 포함함 -;을 포함하고, 상기 크로스 모달 비교 학습 태스크는 상기 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대 한 벡터 표현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 상기 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 상기 단일 모달 학습 태스크는 상기 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용하여, 당해 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측 에 의해 획득된 제2 부분 내용과 당해 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화한다. 본 발명의 제3의 측면에 따르면, 본 발명은 전자 기기를 제공하고, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 통신 연결되는 메모리;를 포함하고, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령이 상기 적 어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 상기 방법을 수행하도록 한다. 본 발명의 제4의 측면에 따르면, 본 발명은 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 기록 매체 를 제공하고, 상기 컴퓨터 명령은 상기 컴퓨터에 상기 방법을 수행하도록 한다. 본 발명의 제5의 측면에 따르면, 본 발명은 컴퓨터 프로그램 제품을 제공하고, 컴퓨터 프로그램을 포함하고, 상 기 컴퓨터 프로그램이 프로세서에 의해 수행될 때, 상기 방법을 구현한다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 기술 방안으로부터 알 수 있는 것은, 본 발명에서 획득된 사전 트레이닝 언어 모델은 단일 모달 코퍼스 와 멀티 모달 코퍼스를 포함하는 서로 다른 형식의 코퍼스로부터 학습하여, 사전 트레이닝 언어 모델이 다양한 서로 다른 모달의 정보를 효과적으로 처리할 수 있다. 또한, 학습 프로세스에서 서로 다른 모달 코퍼스 사이는 서로 강화되어, 획득된 사전 트레이닝 언어 모델이 더 좋은 시멘틱 이해 능력과 일반화 가능한 표현을 갖추도록 한다. 본 명세서에서 설명된 내용은 본 발명의 실시예의 키 또는 중요한 특징을 식별하려는 것이 아니고, 또한 본 발 명의 범위를 제한하려는 것도 아닌 것을 이해하여야 한다. 본 발명의 다른 특징은 하기의 명세서를 통해 용이하 게 이해할 수 있다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "하기는 첨부된 도면을 결부하여 본 발명의 예시적 실시예를 설명하되, 여기에는 이해를 돕기 위한 본 발명의 실"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "시예의 다양한 세부 사항이 포함되며, 이는 단지 예시적인 것으로 간주되어야 한다. 따라서, 본 기술분야의 통 상의 기술자는 본 발명의 범위와 사상을 벗어나지 않으면서, 여기서 설명되는 실시예에 대한 다양한 변경과 수 정이 이루어질 수 있음을 이해해야 한다. 마찬가지로, 명확성 및 간결성을 위해, 아래의 설명에서 공지된 기능 과 구조에 대한 설명을 생략한다. 기존의 사전 트레이닝 모델에서, 대부분은 단일 모달 데이터만 처리할 수 있고, 예를 들어, BERT(Bidirectional Encoder Representation from Transformers, 트랜스포머로부터의 쌍방향 인코더 표현) 모델은 텍스트 데이터만 을 학습하고 처리할 수 있다. SimCLR(Simple Framework for Contrastive Learning of Visual Representations, 시각 표현 비교 학습의 심플 프레임워크) 모델은 이미지 데이터만을 학습하고 처리할 수 있다. VilBERT(Vision-and-Language BERT, 시각-언어BERT) 모델은 이미지 및 텍스트 쌍(즉, 이미지와 텍스트로 구성된 코퍼스 쌍)을 학습하고 처리할 수 있지만, 순수한 이미지 또는 텍스트 데이터에 대한 처리 능력이 매우 나쁘다. 본 발명은 크로스 모달 데이터 학습을 구현하는 사전 트레이닝 모델의 획득 방법을 제공하고, 여러 가 지 모달의 정보를 충분히 사용하여, 사전 트레이닝 모델은 여러 가지 모달의 데이터를 동일한 시멘틱 표현 공간 에 매핑할 수 있다. 하기는 실시예를 결합하여 본 발명에서 제공되는 방법을 상세히 설명한다. 도 1은 본 발명의 실시예에서 제공되는 주요 방법의 흐름도이다. 당해 방법의 실행 주체는 서버측의 장치일 수 있고, 컴퓨터 단말에서의 장치일 수도 있다. 당해 장치는 애플리케이션으로 구현할 수 있고, 애플리케이션에서 의 플러그인 또는 소프트웨어 개발 키트(Software Development Kit, SDK) 등의 기능 유닛으로 구현할 수도 있고, 본 발명의 실시예는 이에 대해 특히 한정되지 않는다. 도 1에 도시된 바와 같이, 당해 방법은 하기와 같 은 단계를 포함할 수 있다. 101에서, 트레이닝 데이터를 획득하고, 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하고, 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함한다. 본 발명에 관한 단일 모달 코퍼스는 단일 모달의 코퍼스를 가리키고, 예를 들어, 이미지, 텍스트, 비디오 또는 오디오 등의 모달의 코퍼스이다. 본 발명에서, 단일 모달 코퍼스만을 포함할 수 있고, 복수의 단일 모달 코퍼스 를 포함할 수도 있다. 멀티 모달 코퍼스는 예를 들어, 이미지와 텍스트로 구성된 코퍼스 쌍, 이미지와 오디오로 구성된 코퍼스 쌍, 텍 스트와 비디오로 구성된 코퍼스 쌍 등과 같은, 두가지의 모달 코퍼스로 구성된 코퍼스 쌍을 가리킨다. 바람직한 실시 형태로서, 멀티 모달 코퍼스는 긍정적인 예의 멀티 모달 코퍼스와 부정적인 예의 멀티 모달 코퍼 스를 포함할 수 있고, 긍정적인 예의 멀티 모달 코퍼스에 포함되는 제1 모달 코퍼스와 제2 모달 코퍼스 사이는 같은 시멘틱을 나타내고, 부정적인 예의 멀티 모달 코퍼스에 포함되는 제1 모달 코퍼스와 제2 모달 코퍼스 사이 는 서로 다른 시멘틱을 나타낸다. 102에서, 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하고, 멀티 태스 크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함한다. 크로스 모달 비교 학습 태스크는 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대한 벡터 표 현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼 스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스 와 제2 모달 코퍼스 사이의 유사도를 최소화하고, 단일 모달 학습 태스크는 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용 하여, 당해 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측에 의해 획 득된 제2 부분 내용과 당해 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화한다. 본 발명에서의 사전 트레이닝 언어 모델은 다층 변환층(multi-layer Transformer)을 메인 모델로 사용할 수 있 고, 도 2에 도시된 바와 같이, 트레이닝 데이터를 사전 트레이닝 언어 모델의 입력으로 하고, 사전 트레이닝 언 어 모델로 트레이닝 데이터 내의 코퍼스를 모두 통일된 벡터 공간에 매핑하고, 즉 각 코퍼스(단일 모달 코퍼스 의 각 코퍼스 및 멀티 모달 코퍼스에서의 각 제1 모달 코퍼스와 제2 모달 코퍼스를 포함한다)의 벡터 표현을 출력한다. 보다 구체적으로, 하나의 코퍼스에 대해, 당해 코퍼스에 포함되는 각Token(시멘틱 요소)을 사전 트레이닝 언어 모델에 입력하고, 사전 트레이닝 언어 모델에 의해 각 시멘틱 요소(Token)의 벡터 표현을 출력한다. 텍스트류의 코퍼스에 대해, 각 시멘틱 요소(Token)는 텍스트에 포함되는 각 문자일 수 있고, 각 단어 등일 수도 있다. 이미 지류의 코퍼스에 대해, 각 시멘틱 요소(Token)는 이미지에 포함되는 각 타깃 영역 또는 각 픽셀 등일 수도 있다. 비디오류의 코퍼스에 대해, 각 프레임 이미지로 분해할 수 있고, 각 시멘틱 요소(Token)는 프레임 이미지 또는 키 프레임 이미지 등일 수도 있다. 오디오류의 코퍼스에 대해, 각 시멘틱 요소(Token)는 각 오디오 프레임, 오디오 세그먼트일 수 있고, 오디오의 주파수, 강도 등에 따라 획득된 각 프레임의 스펙트로그램 (spectrogram)일 수도 있다. 사전 트레이닝 언어 모델의 트레이닝 프로세스에서, 멀티 태스크 트레이닝을 사용한다. 크로스 모달 비교 학습 태스크는 주로 멀티 모달 코퍼스를 사용하여 트레이닝하고, 단일 모달 학습 태스크는 단일 모달 코퍼스를 사용 하여 트레이닝한다. 멀티 태스크는 연합 트레이닝할 수 있고, 교대 트레이닝할 수도 있고 또는 각각 순서대로 트레이닝할 수도 있다. 본 발명을 더 잘 이해하기 위해, 하기는, 단일 모달 코퍼스를 이미지, 텍스트로 하고, 멀티 모달 코퍼스를 이미 지-텍스트 쌍으로 예를 들어, 상기 방법의 각 단계를 상세히 설명한다. 먼저, 실시예를 결합하여 상술한 단계 101, 즉 \"트레이닝 데이터를 획득한다\"는 것에 대해 상세히 설명한다. 네트워크에는 많은 서로 다른 모달의 데이터가 존재하고, 주로 텍스트 정보와 시각 정보이며, 텍스트 지식과 시 각 지식은 일반적으로 서로 보완된다. 인간의 대뇌에 시각을 담당하는 부분은 또한, 촉각과 소리를 포함하는 서 로 다른 모달의 정보를 학습하고 처리할 수 있다. 서로 다른 모달의 데이터를 통일하는 가장 큰 도전은 이를 동"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "일한 시멘틱 공간에 통일하는 것이며, 이러한 시멘틱 공간은 서로 다른 데이터 패턴으로 요약할 수 있다. 따라 서, 본 단계에서 획득된 트레이닝 데이터는 멀티 모달 코퍼스 및 단일 모달 코퍼스로 구현되는 여러 가지 모달 의 데이터를 포함한다. 멀티 모달 코퍼스는 네트워크에서의 시멘틱에서 쌍(pair)으로 출현되는 데이터이다. 이미지-텍스트 쌍을 예로 한다. 네트워크에서 이미지-텍스트 쌍을 미리 크롤링(crawling)하여 이미지-텍스트 쌍 데이터베이스를 구성할 수 있고, 본 단계에서, 직접 당해 이미지-텍스트 쌍 데이터베이스로부터 멀티 모달 코퍼스를 획득할 수 있다. 어떻게 네트워크에서 이미지-텍스트 쌍을 크롤링하는 것은 기존의 기술을 사용하여 구현할 수 있다. 예를 들어, 네트워크에서 이미지를 크롤링하고, 그 다음에, 이미지의 컨텍스트로부터 당해 이미지의 설명 텍스트를 추출하 고, 그 다음에, 당해 이미지 및 설명 텍스트로부터 이미지-텍스트 쌍을 구성할 수 있다. 그 다음에, 예를 들어, 네트워크에서 이미지를 크롤링하고, 그 다음에, 수동으로 설명 텍스트에 주석을 붙일 수 있고, 당해 이미지 및 설명 텍스트로부터 이미지-텍스트 쌍을 구성하는 등일 수 있다. 쌍(pair) 형태의 멀티 모달 코퍼스 이외에, 네트워크에는 예를 들어, 순수한 텍스트 코퍼스, 이미지 코퍼스 등 과 같은, 보다 대규모적인 비쌍(non-pair) 형태의 단일 모달 코퍼스가 존재한다. 네트워크에서 이러한 단일 모 달 코퍼스를 크롤링한 후, 선별하여 텍스트 데이터베이스, 화상 데이터베이스 등의 단일 모달 코퍼스 데이터베 이스를 구성할 수 있다. 본 단계에서, 텍스트 코퍼스와 이미지 코퍼스를 획득하는 것과 같은, 단일 모달 코퍼스 데이터베이스로부터 단일 모달 코퍼스를 직접 획득할 수 있다. 또한, 트레이닝 데이터에 있어서, 멀티 모달 코퍼스 데이터베이스로부터 획득된 멀티 모달 코퍼스는 긍정적인 예 및 부정적인 예 중의 적어도 하나의 확장을 수행할 수 있고, 사용되는 확장 방식은 고쳐 쓰기 확장과 검색 확장 중의 적어도 하나를 포함할 수 있고, 그 다음에, 획득된 멀티 모달 코퍼스를 트레이닝 데이터에 추가할 수 있다. 후속의 크로스 모달 비교 학습 프로세스에 있어서, 긍정적인 예의 멀티 모달 코퍼스와 부정적인 예의 멀 티 모달 코퍼스의 품질이 사전 트레이닝 모델의 최종 효과에 중요한 영향을 미치므로, 고품질의 긍정적인 예의 멀티 모달 코퍼스와 부정적인 예의 멀티 모달 코퍼스를 획득하고, 더 많은 단일 모달 코퍼스를 융합하여 학습을 지원하기 위해, 하기의 확장 방식 중의 적어도 하나를 사용할 수 있다. 제1 확장 방식: 고쳐 쓰기 확장. 긍정적인 예의 멀티 모달 코퍼스의 확장에 대해, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번역 모델을 사용하여 당해 제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트와 당해 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하는 단계를 포함할 수 있다. 상술한 긍정적인 예의 멀티 모달 코퍼스의 확장에 대해, 주로 역 번역(back translation)을 기반으로 한다. 예 를 들어, 어느 긍정적인 예의 멀티 모달 코퍼스는 이미지 1과 텍스트 1로 구성된 이미지-텍스트 쌍이다. 텍스트 1이 중국어라고 가정하면, 기계 번역 모델을 사용하여 중국어를 영어와 프랑스어 등으로 번역하고, 그 다음에, 획득된 영어와 프랑스어 등의 텍스트를 중국어로 번역하여, 텍스트 2, 텍스트 3 등을 획득할 수 있다. 그러면, 이미지 1과 텍스트 2로 구성된 이미지-텍스트 쌍과, 이미지 1과 텍스트 3으로 구성된 이미지-텍스트 쌍을 새로 운 긍정적인 예의 멀티 모달 코퍼스로 트레이닝 데이터에 각각 추가할 수 있고, 긍정적인 예의 멀티 모달 코퍼 스의 확장을 구현할 수 있다. 이 확장은 통상, 문장 수준의 텍스트 고쳐 쓰기 확장을 근거로 한다. 부정적인 예의 멀티 모달 코퍼스의 확장에 대해, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 장면 그래프의 엔티티, 속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획 득된 장면 그래프를 제1 모달 코퍼스로 변환하고, 변환 후에 획득한 제1 모달 코퍼스와 당해 긍정적인 예의 멀 티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는 단계를 포함할 수 있다. 장면 그래프는 엔티티 노드, 속성 노드 및 관계 노드의 3종류의 노드를 포함하는 데이터 구조이다. 엔티티 노드 는 엔티티 워드에 대응하고, 속성 노드는 속성 워드에 대응하고, 관계 노드는 관계 워드에 대응한다. 예를 들면, 긍정적인 예의 이미지-텍스트 쌍에서의 이미지 1과 텍스트 1로 구성되고, 텍스트 1이 \"여성은 파란색 스 커트를 입었다.(女人穿着靑色的裙子)\"이다. 여기서, \"여성(女人)\"과 \"스커트(裙子)\"는 엔티티 노드이다. \"파란 색(靑色)\"은 속성 노드이며, 엔티티 노드 \"스커트\"의 속성을 구현하고, \"입었다(穿着)\"는 관계 노드이며, 엔티 티 노드 \"여성\"과 \"스커트\" 사이의 관계를 구현한다. 생성되는 장면 그래프는 도 3에 도시된 바와 같이, 도면의 원형 노드는 엔티티 노드를 나타내고, 4각형 노드는 관계 노드를 나타내고, 평행사변형 노드는 속성 노드를 나 타낼 수 있다. 장면 그래프의 엔티티, 속성 및 관계 노드 중의 적어도 하나를 교체하고, 예를 들어, 텍스트 2의 \"여성은 빨간 색 스커트를 입었다.\", 텍스트 3의 “여성은 파란색 바지를 입었다.” 및 텍스트 4의 \"여성은 빨간색 바지를 입 었다.\" 등을 형성한다. 그 다음에, 각각 이미지 1과 텍스트 2로 하나의 새로운 이미지-텍스트 쌍을 구성하고, 이미지 1과 텍스트 3으로 하나의 새로운 이미지-텍스트 쌍을 구성하고, 이미지 1과 텍스트 4로 하나의 새로운 이미지-텍스트 쌍을 구성하고, 이러한 새로운 텍스트 쌍을 부정적인 예의 멀티 모달 코퍼스로 트레이닝 데이터 에 추가할 수 있고, 부정적인 예의 멀티 모달 코퍼스에 대한 확장을 구현할 수 있다. 이 확장은 주로 구문 수준 및 어휘 수준에 따라 고쳐 쓰기 확장을 수행한다. 제2 확장 방식: 검색 확장. 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터베이스에서 검색하고, 검색된 코퍼스와 당해 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축한다. 예를 들어, 이미지 1과 텍스트 1로 구성된 긍정적인 예의 이미지-텍스트 쌍이다. 하나의 구현 방식으로서, 텍스트 1을 사용하여 텍스트 데이터베이스에서 검색하고, 텍스트 1 사이의 유사도가 미리 설정된 유사도 임계값 이상인 텍스트를 획득하고, 검색된 텍스트를 각각 이미지 1로 결합하여 새로운 긍정 적인 예의 이미지-텍스트 쌍을 획득할 수 있다. 여기서, 검색된 텍스트는 이미지 1과 텍스트 1로 구성된 긍정적 인 예의 이미지-텍스트 쌍의 텍스트 수준의 배경 지식으로 간주하고, 트레이닝 샘플의 보충으로 할 수 있다. 검색 중에 텍스트 사이의 유사도를 결정할 때, 여러 가지 유사도 결정 방식을 사용할 수 있다. 예를 들어, BERT 가 강력한 시멘틱 표현 능력에 기반하여, 각 텍스트의 벡터 표현을 결정하고, 벡터 표현 사이의 유사도를 계산 하여 텍스트 사이의 유사도로 한다. 그 다음에, 예를 들어, TF-IDF 통계 수단에 기반하여, 각 텍스트 내의 키워 드 (즉, TF-IDF 미리 설정된 통계 임계값 이상인 단어)를 각각 결정하여 각 텍스트의 단어 세트를 획득할 수 있 고, 각 텍스트에서의 단어 세트 내의 각 단어의 단어 빈도에 기반하여, 각 텍스트에 대응하는 단어 빈도 벡터를 획득하고, 단어 빈도 벡터 사이의 유사도를 계산하여 대응하는 텍스트 사이의 유사도로 할 수 있다. 별도의 구현 방식으로서, 이미지 1을 사용하여 이미지 텍스트 라이브러리에서 검색을 수행하고, 이미지 1 사이 의 유사도가 미리 설정된 유사도 임계값 이상인 이미지를 획득하고, 검색된 이미지를 각각 텍스트 1로 결합하여 새로운 긍정적인 예의 이미지-텍스트 쌍을 획득할 수 있다. 여기서, 검색된 이미지는 이미지 1과 텍스트 1로 구 성된 긍정적인 예의 이미지-텍스트 쌍의 이미지 수준의 배경 지식으로 간주하고, 트레이닝 샘플의 보충으로 할수 있다. 검색 중에 이미지 간의 유사도를 결정할 때, 여러 가지 유사도 결정 방식을 사용할 수 있다. 여기에서, 이하에 하나 방식을 제공한다. 목표 검출 툴을 사용하여 각 이미지를 각각 검출하고, 검출된 각 이미지에 포함되는 대 상의 중첩률을 결정하여 대응하는 이미지 간의 유사도의 구현으로 할 수 있다. 예를 들어, 이미지 1이 대상 \"여 성\", \"꽃\", \"푸른 풀\" 및 \"태양\"을 포함하고, 이미지 2가 \"여성\", \"꽃\", \"무지개\" 및 \"가방\"을 포함하고, 이미 지 3이 \"여성\", \"꽃\" 및 \"푸른 풀\"을 포함할 경우, 그러면, 이미지 1과 이미지 3 사이의 대상 중첩률은 이미지 1과 이미지 2 사이의 대상 중첩률보다 높고, 다시 말하면, 이미지 3은 이미지 2보다 이미지 1과 더 유사하다. 실시예를 결합하여 상술한 단계 102, 즉 \"트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하고, 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함한다\"는 것을 상세히 설명한다. 트레이닝 데이터는 이미지, 텍스트 및 이미지-텍스트 쌍을 포함한다고 가정한다. W로 표시되는 텍스트에 대해, 각 시멘틱 요소(Token) 구성 시퀀스로 분할하여 사전 트레이닝 언어 모델의 입력 으로 하고, {[CLS], ,[SEP]}로 표시한다. 여기서, 특수 기호 [CLS]와 [SEP]는 시퀀스의 시 작 위치와 종료 위치를 각각 표시하고, n는 양의 정수이다. 사전 트레이닝 언어 모델의 다층 주의력 메커니즘을 거친 후, 각 시멘틱 요소(Token)에 대응하는 시멘틱 표현을 획득하고, 벡터 표현의 시퀀스 { }로 구현한다. V로 표시되는 이미지에 대해, 목표 검출 툴로 이미지에 포함되는 대상 영역을 검출하고, 각 대상 영역의 특징을 각 시멘틱 요소(Token)와 이미지 전체의 특징 구성 시퀀스를 사전 트레이닝 언어 모델의 입력으로 하고, { }로 표시하고, t는 양의 정수이다. 여기서, [Image]는 이미지 전체의 특징을 표 시한다. 상술한 특징은 대응하는 이미지 또는 영역이 선형 변환 또는 뉴럴 네트워크 변환을 거쳐 획득된 특징일 수도 있다. 사전 트레이닝 언어 모델의 다층 주의력 메커니즘을 거친 후, 각 시멘틱 요소(Token)에 대응하는 시 멘틱 표현을 획득하고, 벡터 표현의 시퀀스 { }로 구현한다. (V, W)로 표시되는 이미지-텍스트 쌍에 대해，V와 W의 각 시멘틱 요소(Token)를 스플라이싱하여 시퀀스를 구성 하여 사전 트레이닝 언어 모델의 입력으로 하고 이하로 표시한다. { } 사전 트레이닝 언어 모델의 다층 주의력 메커니즘을 거친 후, 각 시멘틱 요소(Token)에 대응하는 시멘틱 표현을 획득하고, 벡터 표현의 시퀀스로 구현하고, { } (V, W)로 표시되는 이미지-텍스트 쌍에 대해，V와 W의 시퀀스를 각각 입력으로 하고, 사전 트레이닝 언어 모델 에서 V의 벡터 표현과 W의 벡터 표현을 각각 획득할 수도 있다. 크로스 모달 비교 학습 태스크에서, 주요 아이디어는 의미가 같은 이미지-텍스트 쌍, 즉 긍정적인 예의 이미지- 텍스트 쌍을 시멘틱 공간에서 더 가깝도록 하고, 의미가 다른 이미지-텍스트 쌍, 즉 부정적인 예의 이미지-텍스 트 쌍을 시멘틱 공간에서의 거리를 더 멀게 한다. 즉, 트레이닝 목표는 긍정적인 예의 이미지-텍스트 쌍에서의 이미지와 텍스트 사이의 유사도를 최대화하고, 부정적인 예의 이미지-텍스트 쌍에서의 이미지와 텍스트 사이의 유사도를 최소화한다. 바람직한 실시 형태로서, 고쳐 쓰기 확장 방식과 검색 확장 방식에 의해 획득된 이미지-텍스트 쌍에 대해 유사 도를 계산하는 방식으로 구별할 수 있다. 여기서, 고쳐 쓰기 확장에 의해 획득된 이미지-텍스트 쌍에서의 이미지와 텍스트 사이의 유사도에 대해, 하기와 같은 방식을 사용하여 결정한다. 이미지와 텍스트를 스플라이싱하고, 사전 트레이닝 모델이 스플라이싱한 후의코퍼스로부터 획득된 벡터 표현을 유사도의 값에 매핑한다. 이러한 유사도 계산 방식은 \"단일 스트림식(單流式, single stream)\"이라고 부른다. 이러한 방식에서, 이미지 시퀀스와 텍스트의 시퀀스는 스플라이싱을 수행한 후 에 사전 트레이닝 모델에 입력한다. 사전 트레이닝 모델은 스플라이싱한 후에 획득된 시퀀스에 대해 전체 벡터 표현을 획득하고, 당해 벡터 표현을 매핑(예를 들어, Softmax)한 후, 유사도의 값을 획득한다. 검색 확장에 의해 획득된 이미지-텍스트 쌍에서의 이미지와 텍스트 사이의 유사도에 대해, 하기와 같은 방식을 사용하여 결정하는 사전 트레이닝 모델이 이미지에 대해 획득된 벡터 표현과 사전 트레이닝 모델이 텍스트에 대 해 획득된 벡터 표현을 유사도 계산하고, 예를 들어, 2개의 벡터 표현 사이의 코사인 유사도를 계산한다. 이러 한 유사도 계산 방식은 \"더블 타워(雙塔式, double-tower)\"이라고 부른다. 이러한 방식에서, 이미지-텍스트 쌍 에서의 이미지 시퀀스(이미지에 포함되는 각 시멘틱 요소(Token)로 구성된 시퀀스)와 텍스트의 시퀀스(텍스트에 포함되는 각 시멘틱 요소(Token)로 구성된 시퀀스)는 사전 트레이닝 모델에 각각 입력하여, 사전 트레이닝 모델 에 의해 이미지의 벡터 표현과 텍스트의 벡터 표현을 각각 획득한다. 고쳐 쓰기 확장 방식과 검색 확장 방식에 의해 획득된 이미지-텍스트 쌍에 대해 유사도를 계산하는 방식으로 구 별하는 이유는 \"단일 스트림식\"의 유사도 계산 방식은 이미지와 텍스트를 스플라이싱한 후에 강한 관련 매핑을 수행하기 때문에, 이러한 방식은 \"구별\"을 수행 하는 것, 즉 다시 쓰는 방식으로 획득된 이미지-텍스트 쌍과 원 래의 긍정적인 예의 이미지-텍스트 쌍을 구별하는데 적합하다. 검색 확장에 의해 획득된 이미지 또는 텍스트는 원래의 이미지-텍스트 쌍의 \"배경 지식\"으로 존재하는 긍정적인 예의 이미지-텍스트 쌍이다. \"더블 타워(雙塔式)\"의 유사도 계산 방식은 이미지와 텍스트를 각각 인코딩한 후에 거리를 통해 약한 관련 계산을 수 행하고, 이러한 방식은 이미지 또는 텍스트의 내용을 더 잘 이해하는데 적합하므로, 검색 확장 방식은 \"더블 타 워(雙塔式)\"의 유사도 계산 방식에 적합하며, 이는 사전 트레이닝 모델이 이미지 또는 텍스트의 내용을 더 잘 이해할 수 있도록 한다. 트레이닝 데이터에서의 원래의 이미지-텍스트 쌍은 상술한 유사도 계산 방식 중의 임의의 하나를 사용할 수 있 다. 도 2에 도시된 바와 같이, 크로스 모달 비교 학습 태스크에서, 상술한 두 가지의 유사도 결정 방식에 따라 손실 함수를 구성할 수 있다. 예를 들어, 하기와 같은 손실 함수 Losscmcl를 사용할 수 있고,"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, E는 기대치를 취하는 연산 함수를 가리키고, A+와 A-는 고쳐 쓰기 확장에 의해 획득된 긍정적인 예의 이 미지-텍스트 쌍과 부정적인 예의 이미지-텍스트 쌍을 표시하고, B+와 B-는 검색 확장에 의해 획득된 긍정적인 예 의 이미지-텍스트 쌍과 부정적인 예의 이미지-텍스트 쌍을 표시한다. d()는 유사도 함수를 표시한다. D는 트레 이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 이미지-텍스트 쌍 코퍼스의 데이터 세트를 가리 킨다. V+와 W+는 긍정적인 예의 이미지-텍스트 쌍에서의 텍스트와 이미지를 각각 표시한다. V'와 W'는 긍정적인 예의 이미지-텍스트 쌍에서의 텍스트와 이미지를 획득할 수 있고, 부정적인 예의 이미지-텍스트 쌍에서의 텍스 트와 이미지를 획득할 수 있는 것도 각각 표시한다. 도 2에 도시된 바와 같이, 단일 모달 학습 태스크에는, 시각 학습 태스크와 텍스트 학습 태스크가 포함된다. 시각 학습 태스크는 이미지에 대해 수행하는 학습이다. 시각 학습 태스크에서, 마스크 언어 모델과 유사한 방법 을 사용한다. 이미지로부터 일부 영역을 랜덤으로 선택하여 마스크(Mask)를 수행하고, 사전 트레이닝 모델이 마 스크(Mask) 되지 않은 영역에 대한 벡터 표현을 사용하여 마스크(Mask) 된 영역을 재구성하고, 트레이닝 목표는 재구성된 영역과 이미지 내의 마스크(Mask) 된 영역의 차이를 최소화한다. 예를 들어, 시각 학습 태스크의 손실 함수 Lossv를 구축할 수 있고,"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서, vm는 이미지 내의 마스크(Mask) 된 영역이며, v/m는 이미지 V 내의 마스크(Mask) 되지 않은 영역이며, f θ()는 KL 거리(상대 엔트로피) 함수이며, D는 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서,구체적으로, 이미지 코퍼스의 데이터 세트를 가리킬 수 있고, 이미지-텍스트 쌍 코퍼스의 데이터 세트 내의 이 미지를 더 포함할 수도 있다. 유사하게, 이미지-텍스트 쌍에 대해, 이미지-텍스트 쌍에서의 이미지에 대해 일부 영역 마스크(Mask)를 수행하 고, 사전 트레이닝 모델이 텍스트에 대한 벡터 표현과 이미지 내의 마스크(Mask) 되지 않은 영역에 대한 벡터 표현을 사용하여 이미지 내의 마스크(Mask) 된 영역을 재구성할 수 있고, 트레이닝 목표는 재구성된 영역과 이 미지 내의 마스크(Mask) 되지 않은 영역의 차이를 최소화할 수 있다. 이때, 시각 학습 태스크를 구축하는 손실 함수는 하기의 식일 수도 있다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, 이미지 W와 텍스트 V는 이미지-텍스트 쌍을 구성하고, D는 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 이미지-텍스트 쌍 코퍼스의 데이터 세트를 가리킨다. 텍스트 학습 태스크는 텍스트에 대해 수행하는 학습이다. 텍스트 학습 태스크에서, 여전히 마스크 언어 모델과 유사한 방법을 사용할 수 있다. 단일 모달 코퍼스의 텍스트로부터 일부의 시멘틱 요소(Token)를 랜덤으로 선택 하여 마스크(Mask)를 수행하고, 사전 트레이닝 모델이 마스크(Mask) 되지 않은 시멘틱 요소(Token)에 대한 벡터 표현을 사용하여 마스크(Mask) 된 시멘틱 요소(Token)를 복원한다. 트레이닝 목표의 목적은 복원된 시멘틱 요소 (Token)와 텍스트 내의 마스크(Mask) 된 시멘틱 요소(Token)의 차이를 최소화한다. 상술한 복원은 예측 모델을 사용하여 구현할 수 있고, 생성 모델을 사용하여 구현할 수도 있다. 예측 모델을 사 용하여 구현할 경우, 쌍방향 예측 모델일 수 있고, 사용되는 손실 함수 LossBidir는 하기와 같다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, wm은 텍스트 W 내의 마스크(Mask) 된 시멘틱 요소(Token)이며, w/m는 텍스트 W 내의 마스크(Mask) 되지 않은 시멘틱 요소(Token)이며, Pθ()는 우도 함수이며, D는 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 텍스트 코퍼스의 데이터 세트를 가리킬 수 있고, 이미지-텍스트 쌍 코퍼스의 데이터 세 트 내의 텍스트를 더 포함할 수도 있다. 생성 모델을 사용하여 구현할 경우, 시퀀스 생성 모델을 사용할 수 있다. 시퀀스 생성 모델을 사용할 경우, 텍 스트 W로부터 일부 연속적인 시멘틱 요소(Token)를 랜덤으로 선택하고, T로 표시하고, T={Wi,..., Wj}이며, 나 머지 시멘틱 요소(Token)는 S로 표시한다. 이 경우, 사용되는 손실 함수 LossSeq2seq는 하기와 같다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서, D은 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 텍스트 코퍼스의 데이터 세트를 가리키고, 또한, 이미지-텍스트 쌍 코퍼스의 데이터 세트 내의 텍스트를 포함할 수도 있다. 유사하게, 텍스트 학습 태스크에서, 이미지-텍스트 쌍(V, W) 내의 텍스트에 대해 학습할 수도 있고, 텍스트로부 터 일부의 시멘틱 요소(Token)를 랜덤으로 선택하여 마스크(Mask)를 수행하고, 사전 트레이닝 모델이 마스크 (Mask) 되지 않은 시멘틱 요소(Token)에 대한 벡터 표현 및 이미지-텍스트 쌍에서의 이미지 W에 대한 벡터 표현 을 사용하여 마스크(Mask) 된 시멘틱 요소(Token)를 복원한다. 트레이닝 목표의 목적은 복원된 시멘틱 요소 (Token)와 텍스트 내의 마스크(Mask) 된 시멘틱 요소(Token)의 차이를 최소화한다. 마찬가지로, 상술한 복원은 예측 모델을 사용하여 구현할 수 있고, 생성 모델을 사용하여 구현할 수도 있다. 예 측 모델을 사용하여 구현할 경우, 쌍방향 예측 모델일 수 있고, 사용되는 손실 함수 LossBidir는 하기와 같다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, D는 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 이미지-텍스트 쌍 코퍼스의 데 이터 세트를 가리킨다.생성 모델을 사용하여 구현할 경우, 시퀀스 생성 모델을 사용할 수 있다. 사용되는 손실 함수 LossSeq2seq는 하기 와 같다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "여기서, D는 트레이닝 데이터 세트를 표시하고, 본 손실 함수에서, 구체적으로, 이미지-텍스트 쌍 코퍼스의 데 이터 세트를 가리킨다. 상술한 멀티 태스크 트레이닝을 할 경우, 도 2에 도시된 바와 같이, 연합 트레이닝의 방식을 사용할 수 있다. 바람직한 실시 형태로서, 각 태스크의 손실 함수를 사용하여 총 손실 함수를 구축하고, 구축된 총 손실 함수를 사용하여 사전 트레이닝 모델의 파라미터를 업데이트할 수 있다. 예를 들어, 하기의 총 손실 함수 Loss를 구축 할 수 있다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "상술한 연합 트레이닝의 방식 이외에, 교대 트레이닝의 방식을 사용할 수도 있다. 예를 들어, 각 트레이닝 태스 크를 순서대로 교대 트레이닝하고, 각 트레이닝 태스크가 트레이닝할 때에 각자의 손실 함수를 사용한다. 연합 트레이닝과 교대 트레이닝의 방식 이외에, 순서대로 트레이닝하는 방식을 사용할 수도 있다. 예를 들어, 먼저, 크로스 모달 비교 학습 태스크를 사용하여 트레이닝한다. 트레이닝 종료 후, 트레이닝에 의해 획득된 사 전 트레이닝 모델에 따라 시각 학습 태스크를 다시 수행한다. 트레이닝 종료 후, 트레이닝에 의해 획득된 사전 트레이닝 모델에 따라 텍스트 학습 태스크를 다시 수행하여, 최종적으로 사전 트레이닝 모델을 획득한다. 트레 이닝 프로세스 중의 각 트레이닝 태스크는 각자의 손실 함수를 사용한다. 사전 트레이닝 모델의 트레이닝을 완성한 후, 하류 태스크에 연결할 수도 있고, 당해 하류 태스크에 대응하는 트레이닝 데이터에 따라 사전 트레이닝 모델을 미세 조정할 수도 있다. 하류 태스크는 예를 들어, 단일 모달 데 이터의 분류 태스크, 멀티 모달의 이해 및 생성 태스크, 단일 모달 데이터의 이해 및 생성 태스크 등일 수도 있 다. 예를 들어, 하류 태스크는 텍스트 분류 태스크, 이미지 분류 태스크, 이미지에 대해 질문과 대답을 생성하 는 태스크, 텍스트에 대해 이미지를 생성하는 태스크 등일 수도 있다. 하기의 설명으로부터 알 수 있는 것은 상기 방법의 실시예는 하기의 이점을 가질 수 있다. 1) 본 발명의 실시예에 의해 획득된 사전 트레이닝 언어 모델은 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하 는 서로 다른 형식의 코퍼스로부터 학습하여, 사전 트레이닝 언어 모델이 다양한 서로 다른 모달의 정보를 효과 적으로 처리할 수 있다. 2) 학습 프로세스에서 서로 다른 모달 코퍼스 사이는 서로 강화되어, 획득된 사전 트레이닝 언어 모델이 더 좋 은 시멘틱 이해 능력과 일반화 가능한 표현을 가지게 된다. 이는 인공 지능 기술의 사고방식 혁신에 있어서 매 우 큰 혁신적인 의미가 있다. 3) 네트워크에서의 많은 짝을 이루지 않은 비 짝형 텍스트 코퍼스 데이터베이스와 이미지 집합을 충분히 이용하"}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "여, 더 많은 요약 가능한 텍스트와 시각 표현을 학습할 수 있어, 시각, 언어 이해 및 생성 능력을 향상시킬 수 있다. 이상은 본 발명에서 제공되는 방법의 상세에 설명이지만, 하기는 실시예를 결합하여 본 발명에서 제공되는 장치 에 대해 상세히 설명한다. 도 4는 본 발명의 실시예에서 제공되는 사전 트레이닝 모델의 획득 장치의 개략적인 구조도이다. 도 4에 도시된 바와 같이, 당해 장치는 획득 유닛과 트레이닝 유닛을 포함하고, 또한, 확장 유닛을 포함 할 수도 있다. 각 구성 유닛의 주요 기능은 하기와 같다. 획득 유닛은 트레이닝 데이터를 획득하는데 사용되고, 트레이닝 데이터는 단일 모달 코퍼스와 멀티 모달 코퍼스를 포함하고, 멀티 모달 코퍼스는 제1 모달 코퍼스와 제2 모달 코퍼스로 구성되는 코퍼스 쌍을 포함한다. 트레이닝 유닛은 트레이닝 데이터를 사용하여 사전 트레이닝 모델에 대해 멀티 태스크 트레이닝을 수행하 는데 사용되고, 멀티 태스크는 적어도 하나의 크로스 모달 비교 학습 태스크와 적어도 하나의 단일 모달 학습 태스크를 포함한다. 여기서, 크로스 모달 비교 학습 태스크는 사전 트레이닝 모델이 멀티 모달 코퍼스 내의 제1 모달 코퍼스에 대한 벡터 표현과 제2 모달 코퍼스에 대한 벡터 표현을 사용하여, 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모 달 코퍼스 사이의 유사도를 결정하는 것을 포함하고, 트레이닝 목표는 긍정적인 예의 멀티 모달 코퍼스 내의 제 1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최대화하고, 부정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도를 최소화한다. 단일 모달 학습 태스크는 사전 트레이닝 모델이 단일 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현을 사용 하여, 당해 단일 모달 코퍼스 내의 제2 부분 내용을 예측하는 것을 포함하고, 트레이닝 목표는 예측에 의해 획 득된 제2 부분 내용과 당해 단일 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화한다. 확장 유닛은 트레이닝 데이터 내의 멀티 모달 코퍼스에 대해 고쳐 쓰기 확장과 검색 확장 중의 적어도 하 나를 수행하고, 확장에 의해 획득된 멀티 모달 코퍼스를 트레이닝 데이터에 추가하는데 사용된다. 여기서, 상술한 검색 확장은 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 제1 모달 코퍼스 데이터 베이스에서 검색하고, 검색된 코퍼스와 당해 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정적인 예의 멀티 모달 코퍼스를 구축하는 것을 포함한다. 상술한 고쳐 쓰기 확장은 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스가 제1 언어 텍스트일 경우, 번 역 모델을 사용하여 당해 제1 언어 텍스트를 제2 언어 텍스트로 번역한 후에 제1 언어로 재번역하고, 번역 후에 획득된 제1 언어 텍스트와 당해 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 긍정 적인 예의 멀티 모달 코퍼스를 구축하거나, 또는, 긍정적인 예의 멀티 모달 코퍼스 내의 제1 모달 코퍼스를 장면 그래프로 해석하고, 장면 그래프의 엔티티, 속성 및 관계 중의 적어도 하나를 랜덤으로 교체하고, 교체 후에 획득된 장면 그래프를 제1 모달 코퍼스로 변환하고, 변환 후에 획득한 제1 모달 코퍼스와 당해 긍정적인 예의 멀티 모달 코퍼스 내의 제2 모달 코퍼스를 사용하여 새로운 부정적인 예의 멀티 모달 코퍼스를 구축하는 것을 포함한다. 구현 가능한 방식으로서, 트레이닝 유닛이 크로스 모달 비교 학습 태스크를 수행하는 중에, 검색 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 사전 트레이닝 모델 이 제1 모달 코퍼스에서 획득된 벡터 표현과 사전 트레이닝 모델이 제2 모달 코퍼스에서 획득된 벡터 표현에 대 해 유사도 계산을 수행하는 방식을 사용하여 결정하고, 고쳐 쓰기 확장에 의해 획득된 멀티 모달 코퍼스 내의 제1 모달 코퍼스와 제2 모달 코퍼스 사이의 유사도는, 제1 모달 코퍼스와 제2 모달 코퍼스를 스플라이싱하고, 사전 트레이닝 모델이 스플라이싱한 후의 코퍼스로부터 획득된 벡터 표현을 유사도의 값에 매핑하는 방식을 사 용하여 결정한다. 트레이닝 유닛은 또한, 단일 모달 학습 태스크를 수행할 경우, 또한, 사전 트레이닝 모델이 멀티 모달 코 퍼스 내의 제1 모달 코퍼스 내의 제1 부분 내용에 대한 벡터 표현 및 제2 모달 코퍼스에 대한 벡터 표현을 사용 하여, 당해 제1 모달 코퍼스 내의 제2 부분 내용을 예측하고, 트레이닝 목표는 예측에 의해 획득된 제2 부분 내 용과 당해 제1 모달 코퍼스 내의 제2 부분 내용의 차이를 최소화한다. 구체적으로, 트레이닝 유닛은 멀티 태스크 트레이닝을 할 때, 구축된 총 손실 함수를 사용하여 사전 트레 이닝 모델의 파라미터를 업데이트할 수 있다. 여기서, 총 손실 함수는 적어도 하나의 크로스 모달 비교 학습 태 스크의 손실 함수와 적어도 하나의 단일 모달 학습 태스크의 손실 함수의 합에 의해 획득된다. 상술한 장치의 각 유닛의 구체적인 처리 방식은 전술의 방법의 실시예의 관련 설명을 참조할 수 있고, 여기서 상세히 설명하지 않는다. 본 발명의 실시예에 따르면, 본 발명은 전자 기기, 판독 가능 기록 매체 및 컴퓨터 프로그램 제품을 더 제공한 다. 도 5에 도시된 바와 같이, 본 발명의 실시예의 전자 기기의 블록도이다. 전자 기기는 랩톱 컴퓨터, 데스크톱 컴 퓨터, 운영 플랫폼, 개인 디지털 처리, 서버, 블레이드 서버, 대형 컴퓨터, 및 다른 적합한 컴퓨터와 같은 다양 한 형태의 디지털 컴퓨터를 의미한다. 전자 기기는 개인 디지털 처리, 셀룰러폰, 스마트폰, 웨어러블 기기 및 다른 유사한 계산 장치와 같은 다양한 형태의 이동 장치를 의미할 수도 있다. 본문에서 나타낸 부재, 이들의 연 결과 관계, 및 이들의 기능은 단지 예시적인 것으로, 본문에서 설명 및/또는 요구된 본 발명의 구현을 한정하지 않는다. 도 5에 도시된 바와 같이, 기기는 컴퓨팅 유닛을 포함하고, 컴퓨팅 유닛은 판독 전용 메모리 (ROM)에 저장되어 있는 컴퓨터 프로그램 또는 저장 유닛으로부터 랜덤 액세스 메모리(RAM)에 로 드된 컴퓨터 프로그램에 따라, 다양한 적절한 동작과 처리를 실행할 수 있다. RAM에는 기기가 동작하 는데 필요한 여러 가지 프로그램과 데이터도 저장할 수 있다. 컴퓨팅 유닛, ROM 및 RAM는 버스 를 통해 서로 연결된다. 입력/출력 (I/O)인터페이스도 버스에 연결된다. 기기 중의 복수 컴포넌트는 I/O 인터페이스에 연결되고, 키보드, 마우스 등과 같은 입력 유닛; 여러 가지 타입의 디스플레이, 스피커 등과 같은 출력 유닛; 디스크, 광디스크 등과 같은 저장 유닛 및 네트워크 카드, 모뎀, 무선 통신 트랜시버 등과 같은 통신 유닛을 포함한다. 통신 유닛은 전자 기 기가 인터넷 등과 같은 컴퓨터 네트워크 및 여러 가지 통신 네트워크 중의 적어도 하나를 통해 다른 기기 와 정보/데이터를 교환할 수 있다. 컴퓨팅 유닛은 여러 가지 처리와 계산 능력을 갖춘 범용 처리 컴포넌트 및 전용 처리 컴포넌트 중의 적어 도 하나일 수 있다. 컴퓨팅 유닛의 일부 예는, 중앙 처리 유닛(CPU), 그래픽스 처리 유닛(GPU), 다양한 전 용 인공지능(AI) 계산 팁, 다양한 기계 학습 모델 알고리즘을 실행하는 컴퓨팅 유닛, 디지털 신호 프로세서 (DSP) 및 임의의 적절한 프로세서, 컨트롤러, 마이크로 컨트롤러 등을 포함하지만, 이에 한정되지 않는다. 컴퓨 팅 유닛은 사전 트레이닝 모델의 획득 방법 등과 같은 상기의 다양한 방법과 처리를 실행한다. 예를 들면, 일부 실시예에서, 사전 트레이닝 모델의 획득 방법은 저장 유닛 등과 같은 기계 판독 가능 매체에 유형적 으로 포함되는 컴퓨터 소프트웨어 프로그램으로 구현할 수 있다. 일부 실시예에서, 컴퓨터 프로그램의 일부 또는 전부는 ROM 및 통신 유닛 중의 적어도 하나를 통해 전자 기기에 로드 및/또는 인스톨될 수 있다. 컴퓨터 프로그램이 RAM에 로드되어 컴퓨팅 유닛에 의해 실행될 경우, 사전 트레이닝 모델의 획득 방법의 하나 또는 복수의 단계를 실행할 수 있다. 대안적으로, 다른 실시예에서, 컴퓨팅 유닛은 다른 임의의 적절한 방식(예를 들면, 펌웨어에 의해)을 통해 사전 트레이 닝 모델의 획득 방법을 실행하도록 구성될 수 있다. 여기서 설명된 시스템 및 기술의 다양한 실시형태는 디지털 전자 회로 시스템, 집적 회로 시스템, 필드 프로그 래밍 가능한 게이트 어레이(FPGA), 특정 용도 대상 집적 회로(ASIC), 특정 용도 대상 표준제품(ASSP), 시스템 온 칩 시스템(SOC), 부하 프로그래밍 가능 논리 장치(CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어, 및/또는 이 들의 결합에서 구현될 수 있다. 이러한 다양한 실시형태는 하나 또는 다수의 컴퓨터 프로그램에서의 구현을 포 함할 수 있고, 상기 하나 또는 다수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프 로그램 가능 시스템에서 실행 및/또는 해석될 수 있으며, 상기 프로그램 가능 프로세서는 전용 또는 범용 프로 그램 가능 프로세서일 수 있고, 저장 시스템, 적어도 하나의 입력 장치, 및 적어도 하나의 출력 장치로부터 데 이터 및 명령을 수신할 수 있으며, 데이터 및 명령을 상기 저장 시스템, 상기 적어도 하나의 입력 장치, 및 상 기 적어도 하나의 출력 장치에 전송할 수 있다. 본 발명의 방법을 실시하기 위한 프로그램 코드는 하나 또는 복수의 프로그래밍 언어의 임의의 결합을 사용하여 작성할 수 있다. 이러한 프로그램 코드는 프로그램 코드가 프로세서 또는 컨트롤러에 의해 실행될 때 흐름도 및 블록도 중의 적어도 하나에 규정된 기능/동작이 실행되도록, 대형 기계(슈퍼 컴퓨터), 전용 컴퓨터 또는 다른 프로그램 가능한 데이터 처리 장치의 프로세서 또는 컨트롤러에 제공할 수 있다. 프로그램 코드는 완전히 기계 에서 실행되거나, 부분적으로 기계에서 실행되거나, 독립된 소프트웨어 패키지로서 부분적으로 기계에서 실행되 고, 부분적으로 리모트 기계에서 실행되거나 또는 완전히 리모트 기계 또는 서버에서 실행될 수 있다. 본 발명의 문맥에서, 기계 판독 가능 매체는 명령 실행 시스템, 장치 또는 기기의 사용, 또는 명령 실행 시스템, 장치 또는 기기와 결합하여 사용되는 프로그램을 포함하거나 저장할 수 있는 유형적인 매체일 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호 매체 또는 기계 판독 가능 기록 매체일 수 있다. 기계 판독 가능 매체는 전자, 자기, 광학, 전자기, 적외선, 또는 반도체 시스템, 장치 또는 기기, 또는 상술한 내용의 임의의 적절한 결합을 포함하지만, 이에 한정되지 않는다. 기계 판독 가능 기록 매체의 더 구체적인 예는 하나 또는 복 수의 와이어에 기반한 전기 연결, 휴대용 컴퓨터 디스크, 하드 디스크, 랜덤 액세스 메모리(RAM), 판독 전용 메 모리(ROM), 소거 가능 프로그래머블 판독 전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 포터블 컴팩트 디스 크 판독 전용 메모리(CD-ROM), 광학 저장 장치, 자기 저장 장치 또는 상술한 내용의 임의의 적절한 결합을 포함 한다. 사용자와의 인터랙션을 제공하기 위하여, 컴퓨터에서 여기서 설명된 시스템 및 기술을 실시할 수 있고, 상기 컴 퓨터는 사용자에게 정보를 표시하기 위한 표시 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 표시 장치) 모니 터); 및 키보드 및 지향 장치(예를 들어, 마우스 또는 트랙 볼)를 구비하며, 사용자는 상기 키보드 및 상기 지향 장치를 통해 컴퓨터에 입력을 제공한다. 다른 타입의 장치는 또한 사용자와의 인터랙션을 제공할 수 있는데, 예를 들어, 사용자에게 제공된 피드백은 임의의 형태의 감지 피드백(예를 들어, 시각 피드백, 청각 피드백, 또 는 촉각 피드백)일 수 있고; 임의의 형태(소리 입력, 음성 입력, 또는 촉각 입력)로 사용자로부터의 입력을 수 신할 수 있다. 여기서 설명된 시스템 및 기술은 백엔드 부재를 포함하는 계산 시스템(예를 들어, 데이터 서버로 사용됨), 또는 미들웨어 부재를 포함하는 계산 시스템(예를 들어, 애플리케이션 서버), 또는 프론트 엔드 부재를 포함하는 계 산 시스템(예를 들어, 그래픽 사용자 인터페이스 또는 네트워크 브라우저를 구비하는 사용자 컴퓨터인 바, 사용 자는 상기 그래픽 사용자 인터페이스 또는 상기 네트워크 브라우저를 통해 여기서 설명된 시스템 및 기술의 실 시형태와 인터랙션할 수 있음), 또는 이러한 백엔드 부재, 미들웨어 부재, 또는 프론트 엔드 부재의 임의의 결 합을 포함하는 계산 시스템에서 구현될 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워크)을 통해 시스템의 부재를 서로 연결할 수 있다. 통신 네트워크의 예는, 근거리 통신망(LAN), 광역망 (WAN), 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고 일반적으로 통신 네트워크를 통해 서로 인터랙션한다. 대응되는 컴퓨터에서 실행되고 또한 서로 클라이언트- 서버 관계를 가지는 컴퓨터 프로그램을 통해 클라이언트 및 서버의 관계를 생성한다. 위에서 설명된 다양한 형태의 프로세스를 사용하여 단계를 재배열, 추가 또는 삭제할 수 있음을 이해해야 한다. 예를 들어, 본 발명에 기재된 각 단계는 동시에, 순차적으로, 또는 상이한 순서로 수행될 수 있으며, 본 발명에 개시된 기술적 해결수단이 이루고자 하는 결과를 구현할 수 있는 한, 본문은 여기서 한정되지 않는다."}
{"patent_id": "10-2022-0004125", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "상기 구체적인 실시형태는 본 발명의 보호 범위를 한정하지 않는다. 본 기술분야의 통상의 기술자는, 설계 요구 및 다른 요소에 따라 다양한 수정, 결합, 서브 결합 및 대체를 진행할 수 있음을 이해해야 한다. 본 발명의 정 신 및 원칙 내에서 이루어진 임의의 수정, 등가 교체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다."}
{"patent_id": "10-2022-0004125", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부 도면은 본 해결수단을 더 잘 이해하기 위한 것으로, 본 발명에 대해 한정하는 것으로 구성되지 않는다. 도 1은 본 발명의 실시예에서 제공되는 주요 방법의 흐름도이다. 도 2는 본 발명의 실시예에서 제공되는 사전 트레이닝 언어 모델을 트레이닝하는 개략도이다. 도 3은 본 발명의 실시예에서 제공되는 장면 그래프의 개략도이다. 도 4는 본 발명의 실시예에서 제공되는 사전 트레이닝 모델의 획득 장치의 개략적인 구조도이다. 도 5는 본 발명의 실시예를 구현하기 위한 전자 기기의 블록도이다."}
