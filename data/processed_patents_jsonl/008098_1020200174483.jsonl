{"patent_id": "10-2020-0174483", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0144521", "출원번호": "10-2020-0174483", "발명의 명칭": "다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법 및 시스템", "출원인": "네오사피엔스 주식회사", "발명자": "김태수"}}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법으로서,제1 언어의 입력 음성 데이터 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계;상기 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환하는 단계;상기 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계; 및상기 제2 언어의 텍스트 및 상기 제1 언어에 대한 화자의 발성 특징을 단일 인공 신경망 다중 언어 텍스트-음성합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함하고,상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델은,제1 언어의 학습 텍스트, 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터 및 상기 제1언어의 학습 음성 데이터와 연관된 제1 화자 정보와, 제2 언어의 학습 텍스트, 상기 제2 언어의 학습 텍스트에대응되는 제2 언어의 학습 음성 데이터 및 상기 제2 언어의 학습 음성 데이터와 연관된 제2 화자 정보에 기초하여 학습된 모델인, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 언어에 대한 화자의 발성 특징은, 상기 제1 언어의 입력 음성 데이터로부터 특징 벡터를 추출하여 생성되는, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 언어의 입력 음성 데이터로부터 상기 제1 언어에 대한 화자의 감정 특징(emotion feature)을 생성하는단계를 더 포함하고,상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 상기 제2 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징 및 상기 감정 특징을 상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함하는, 방법"}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 감정 특징은, 상기 화자의 발화 내용에 내재된 감정에 대한 정보를 포함하는, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2020-0144521-3-제1항에 있어서,상기 제1 언어의 입력 음성 데이터로부터 상기 제1 언어에 대한 화자의 운율 특징(prosody feature)을 생성하는단계를 더 포함하고,상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 상기 제2 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징 및 상기 운율 특징을 상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함하는, 방법"}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 운율 특징은, 발화 속도에 대한 정보, 발음 강세에 대한 정보, 음 높이에 대한 정보 및 휴지 구간에 대한정보 중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "다중 언어 텍스트-음성 합성 모델을 이용한 비디오 번역 방법으로서,제1 언어의 입력 음성 데이터를 포함하는 비디오 데이터, 상기 제1 언어의 입력 음성 데이터에 대응하는 제1 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계;상기 비디오 데이터로부터 상기 제1 언어의 입력 음성 데이터를 삭제하는 단계;상기 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계; 상기 제2 언어의 텍스트 및 상기 제1 언어에 대한 화자의 발성 특징을 단일 인공 신경망 다중 언어 텍스트-음성합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계; 및상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 상기 비디오 데이터에 결합하는 단계를 포함하고,상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델은,제1 언어의 학습 텍스트, 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터 및 상기 제1언어의 학습 음성 데이터와 연관된 제1 화자 정보와, 제2 언어의 학습 텍스트, 상기 제2 언어의 학습 텍스트에대응되는 제2 언어의 학습 음성 데이터 및 상기 제2 언어의 학습 음성 데이터와 연관된 제2 화자 정보에 기초하여 학습된 모델인, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제1 언어의 입력 음성 데이터로부터 상기 제1 언어에 대한 화자의 감정 특징(emotion feature)을 생성하는단계를 더 포함하고,상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 상기 제2 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징 및 상기 감정 특징을 상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함하는, 방법.공개특허 10-2020-0144521-4-청구항 9 제7항에 있어서,상기 제1 언어의 입력 음성 데이터로부터 상기 제1 언어에 대한 화자의 운율 특징(prosody feature)을 생성하는단계를 더 포함하고,상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 상기 제2 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징 및 상기 운율 특징을 상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 제1 언어에 대한 화자의 발성 특징은 화자 ID 또는 화자 임베딩 벡터를 포함하는, 방법"}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제10항 중 어느 한 항의 방법에 따른 각각의 단계들을 수행하는 명령어를 포함하는 프로그램이 기록된, 컴퓨터 판독가능 저장매체."}
{"patent_id": "10-2020-0174483", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 시스템으로서,메모리; 및상기 메모리와 연결되고, 상기 메모리에 포함된 컴퓨터 판독 가능한 적어도 하나의 프로그램을 실행하도록 구성된 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로그램은,제1 언어의 입력 음성 데이터 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하고, 상기 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환하고,상기 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하고,상기 제2 언어의 텍스트 및 상기 제1 언어에 대한 화자의 발성 특징을 단일 인공 신경망 다중 언어 텍스트-음성합성 모델에 입력하여, 상기 제1 언어에 대한 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하기 위한 명령어들을 포함하고,상기 단일 인공 신경망 다중 언어 텍스트-음성 합성 모델은,제1 언어의 학습 텍스트, 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터 및 상기 제1언어의 학습 음성 데이터와 연관된 제1 화자 정보와, 제2 언어의 학습 텍스트, 상기 제2 언어의 학습 텍스트에대응되는 제2 언어의 학습 음성 데이터 및 상기 제2 언어의 학습 음성 데이터와 연관된 제2 화자 정보에 기초하여 학습된 모델인, 음성 번역 시스템."}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이용한 음성 번역 방 법, 비디오 번역 방법 또는 시스템에 관한 것이다. 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법은 제1 언어의 학습 텍스트 및 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학 (뒷면에 계속)"}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이용한 음성 번역 방 법 또는 시스템에 관한 것이다. 또한 본 개시는 다중 언어 텍스트-음성 합성 모델을 이용한 비디오 번역 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 텍스트-음성 합성(TTS; Text-To-Speech)라고도 불리는 음성 합성 기술은, 안내방송, 네비게이션, 인 공지능 비서 등과 같이 사람의 음성이 필요한 어플리케이션에서 실제 사람의 음성을 사전에 녹음해 두지 않고 필요한 음성을 재생하기 위해 사용되는 기술이다. 음성 합성의 전형적인 방법은, 음성을 음소 등 아주 짧은 단 위로 미리 잘라서 저장해두고, 합성할 문장을 구성하는 음소들을 결합하여 음성을 합성하는 연결 음성 합성 (concatenative TTS)과, 음성의 특징을 파라미터(parameter)로 표현하고 합성할 문장을 구성하는 음성 특징들을 나타내는 파라미터(parameter)들을 보코더(vocoder)를 이용해 문장에 대응하는 음성으로 합성하는 파라미터 음 성 합성(parametric TTS)이 있다. 한편, 최근에는 인공 신경망(artificial neural networks) 기반의 음성 합성 방법이 활발히 연구되고 있으며, 이 음성 합성 방법에 따라 합성된 음성은, 기존의 방법에 비해 훨씬 자연스러운 음성 특징을 보여주고 있다. 하 지만, 인공 신경망 기반의 음성 합성 방법으로 새로운 목소리의 음성 합성부를 구현하기 위해서는 그 목소리에 해당하는 많은 데이터가 필요하고, 이 데이터를 이용한 신경망 모델의 재학습이 요구된다. 또한, 특정 언어의 텍스트를 다른 언어의 텍스트로 번역하여, 번역된 언어의 음성으로 합성하는 연구도 진행되 고 있다. 여기서, 번역된 언어의 음성 합성에는 그 언어의 대표적인 특징을 갖는 화자의 음성 데이터가 사용될 수 있다."}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이용한 음성 번역 방 법 또는 시스템을 제공한다. 또한 본 개시는 다중 언어 텍스트-음성 합성 모델을 이용한 비디오 번역 방법 및 시스템을 제공한다."}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이 용한 음성 번역 방법은, 제1 언어의 학습 텍스트 및 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학습 텍스트 및 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초 하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계, 제1 언어의 입력 음성 데이터 및 제1 언어에 대한 화자의 발성 특징을 수신하는 단계, 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환하는 단계, 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계, 및 제2 언어의 텍스 트 및 화자의 발성 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법에 있어서, 제1 언어에 대한 화자의 발성 특징은, 화자가 제1 언어로 발화한 음성 데이터로부터 특징 벡터를 추출하여 생성된다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법은, 제1 언어의 입력 음 성 데이터로부터 제1 언어에 대한 화자의 감정 특징(emotion feature)을 생성하는 단계를 더 포함하고, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 제2 언어의 텍스트, 및 제1 언어에 대한 화자의 발성 특징 및 감정 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음 성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법에 있어서, 감정 특징은, 화자의 발화 내용에 내재된 감정에 대한 정보를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법은, 제1 언어의 입력 음 성 데이터로부터 제1 언어에 대한 화자의 운율 특징(prosody feature)을 생성하는 단계를 더 포함하고, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 제2 언어의 텍스트, 및 제1 언어에 대한 화자의 발성 특징 및 운율 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 방법에 있어서, 운율 특징은, 발화 속도에 대한 정보, 발음 강세에 대한 정보, 음 높이에 대한 정보 및 휴지 구간에 대한 정보 중 적 어도 하나를 포함한다. 본 개시의 일 실시예에 따른 다중 언어(multilingual) 텍스트-음성 합성(text-to-speech synthesis) 모델을 이 용한 비디오 번역 방법은, 제1 언어의 학습 텍스트 및 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음 성 데이터와, 제2 언어의 학습 텍스트 및 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계, 제1 언 어의 입력 음성 데이터를 포함하는 비디오 데이터, 제1 언어의 입력 음성 데이터에 대응하는 제1 언어의 텍스트, 및 제1 언어에 대한 화자의 발성 특징을 수신하는 단계, 비디오 데이터로부터 제1 언어의 입력 음성 데 이터를 삭제하는 단계, 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계, 제2 언어의 텍스트 및 제1 언 어에 대한 화자의 발성 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계, 및 출력 음성 데이터를 비디오 데이터에 결합하 는 단계를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 비디오 번역 방법은, 제1 언어의 입력 음성 데이터로부터 제1 언어에 대한 화자의 감정 특징(emotion feature)을 생성하는 단계를 더 포함하고, 화자 의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 제2 언어의 텍스트, 및 제 1 언어에 대한 화자의 발성 특징 및 감정 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함한다. 본 개시의 일 실시예에 따른 다중 언어 텍스트-음성 합성 모델을 이용한 비디오 번역 방법은, 제1 언어의 입력 음성 데이터로부터 제1 언어에 대한 화자의 운율 특징(prosody feature)을 생성하는 단계를 더 포함하고, 화자 의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계는, 제2 언어의 텍스트, 및 제 1 언어에 대한 화자의 발성 특징 및 운율 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 포함한다. 또한, 상술한 바와 같은 다중 언어 텍스트-음성 합성 모델을 이용한 음성 번역 및 비디오 번역 방법을 구현하기 위한 프로그램은, 컴퓨터로 판독 가능한 기록 매체에 기록될 수 있다."}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "개시된 실시예의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예 들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 개시가 완전하도록 하고, 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 개시된 실시예에 대해 구체적으로 설명하기로 한다. 본 명세서에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들 을 선택하였으나, 이는 관련 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서의 단수의 표현은 문맥상 명백하게 단수인 것으로 특정하지 않는 한, 복수의 표현을 포함한다. 또 한 복수의 표현은 문맥상 명백하게 복수인 것으로 특정하지 않는 한, 단수의 표현을 포함한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에서 사용되는 \"부\"라는 용어는 소프트웨어 또는 하드웨어 구성요소를 의미하며, \"부\"는 어떤 역할 들을 수행한다. 그렇지만 \"부\"는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. \"부\"는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 \"부\"는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태 스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트개시된 실시예의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 개시가 완전하도록 하고, 본 개시가 속하는"}
{"patent_id": "10-2020-0174483", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 개시된 실시예에 대해 구체적으로 설명하기로 한다. 본 명세서에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들 을 선택하였으나, 이는 관련 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서의 단수의 표현은 문맥상 명백하게 단수인 것으로 특정하지 않는 한, 복수의 표현을 포함한다. 또 한 복수의 표현은 문맥상 명백하게 복수인 것으로 특정하지 않는 한, 단수의 표현을 포함한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에서 사용되는 \"부\"라는 용어는 소프트웨어 또는 하드웨어 구성요소를 의미하며, \"부\"는 어떤 역할 들을 수행한다. 그렇지만 \"부\"는 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. \"부\"는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 \"부\"는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태 스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레 이들 및 변수들을 포함한다. 구성요소들과 \"부\"들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 \"부\"들 로 결합되거나 추가적인 구성요소들과 \"부\"들로 더 분리될 수 있다. 본 개시의 일 실시예에 따르면 \"부\"는 프로세서 및 메모리로 구현될 수 있다. 용어 \"프로세서\" 는 범용 프로세 서, 중앙 처리 장치 (CPU), 마이크로프로세서, 디지털 신호 프로세서 (DSP), 애플리케이션 프로세서 (AP), 제어 기, 마이크로제어기, 상태 머신 등을 포함하도록 넓게 해석되어야 한다. 몇몇 환경에서는, \"프로세서\" 는 주문 형 반도체 (ASIC), 프로그램가능 로직 디바이스 (PLD), 필드 프로그램가능 게이트 어레이 (FPGA) 등을 지칭할 수도 있다. 용어 \"프로세서\" 는, 예를 들어, DSP 와 마이크로프로세서의 조합, 복수의 마이크로프로세서들의 조 합, DSP 코어와 결합한 하나 이상의 마이크로프로세서들의 조합, 또는 임의의 다른 그러한 구성들의 조합과 같 은 처리 디바이스들의 조합을 지칭할 수도 있다. 용어 \"메모리\" 는, 전자 정보를 저장 가능한 임의의 전자 컴포넌트를 포함하도록 넓게 해석되어야 한다. 용어 메모리는 임의 액세스 메모리 (RAM), 판독-전용 메모리 (ROM), 비-휘발성 임의 액세스 메모리 (NVRAM), 프로그 램가능 판독-전용 메모리 (PROM), 소거-프로그램가능 판독 전용 메모리 (EPROM), 전기적으로 소거가능 PROM (EEPROM), 플래쉬 메모리, 자기 또는 광학 데이터 저장장치, 레지스터들 등과 같은 프로세서-판독가능 매체의 다양한 유형들을 지칭할 수도 있다. 프로세서가 메모리로부터 정보를 판독하고/하거나 메모리에 정보를 기록할 수 있다면 메모리는 프로세서와 전자 통신 상태에 있다고 불린다. 프로세서에 집적된 메모리는 프로세서와 전자 통신 상태에 있다. 아래에서는 첨부한 도면을 참고하여 실시예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략한다. 도 1 은 본 개시의 일 실시예에 따른 음성 번역 시스템을 나타내는 블록도이다. 도시된 바와 같이, 음성 번역 시스템은, 입력부, 음성 번역부 및 출력부를 포함할 수 있다. 입력부는 음성을 수신하여 음성 번역부로 제1 언어의 음성 데이터를 송신할 수 있다. 입력부 는 음성을 수신할 수 있는 다양한 수단을 포함할 수 있다. 예를 들어, 입력부는 사용자로부터 직접 음성을 수신할 수 있는 마이크로폰(microphone), 보이스 레코더(voice recorder), 헤드셋(headset) 등과 같은 다양한 음성 입력 장치 또는 수단 중의 하나를 포함할 수 있다. 다른 예에서, 입력부는, 메모리 또는 데이 터베이스에 저장되어 있는 음성 데이터를 수신할 수 있는 입력 장치일 수 있다. 또 다른 예에서, 입력부는 블루투스(Blutooth), WiFi, 인터넷 등의 무선 또는 유선 네트워크를 통해 음성 데이터를 수신할 수 있는 무선 또는 유선 입력장치일 수 있다. 또 다른 예에서, 입력부는 메모리에 저장된 비디오 데이터 또는 영상 파일, 온라인 상에서 다운로드 가능한 비디오 데이터 또는 영상 파일로부터 음성 데이터를 추출할 수 있는 프로 세서 또는 음성 처리 장치일 수 있다. 음성 번역부는, 제1 언어의 음성 데이터를 제2 언어의 음성 데이터로 변환할 수 있다. 본 개시에 있어서 \"제1 언어\"는 한국어, 일본어, 중국어, 영어 등 다양한 국가 또는 민족이 사용하는 다양한 언어 중의 하나를 지칭할 수 있고, \"제2 언어\"는 제1 언어와 다른 국가 또는 민족이 사용하는 언어 중의 하나를 지칭할 수 있다. 음성 번역부는, 이하 상세히 설명하는 바와 같이, 제1 언어의 음성을 제2 언어의 음성으로 번역하기 위해 인공신경망 또는 기계학습 모델을 사용할 수 있다. 음성 번역부에 사용되는 인공신경망 또는 기계학습 모 델은, RNN(recurrent neural network), LSTM(long short-term memory model), DNN(deep neural network), CNN(convolution neural network) 등을 포함하는 다양한 인공신경망 모델 중의 어느 하나 또는 이들의 조합으로 구성될 수 있다. 일 실시예에서, 음성 번역부는, 제1 언어의 음성을 제2 언어의 음성으로 번역하는데 있어서, 제1 언어를 사용하는 화자의 발성 특징, 운율 특징 또는 감정 특징 중 적어도 하나를 반영하여 제2 언어의 음성 데이터를생성할 수 있다. 다른 실시예에서, 음성 번역부는, 제1 언어의 음성을 제2 언어의 음성으로 번역하는데 있 어서, 제1 언어의 음성의 화자가 아닌 다른 화자의 발성 특징, 운율 특징 또는 감정 특징 중 적어도 하나를 반 영하여 제2 언어의 음성 데이터를 생성할 수도 있다. 출력부는, 음성 번역부에 의해 생성된 제2 언어의 음성 데이터를 출력할 수 있다. 출력부는 음 성을 출력할 수 있는 수단을 포함할 수 있다. 예를 들어, 출력부는, 제2 언어의 음성 데이터를 소리로 변 환할 수 있는 스피커, 헤드셋, 헤드폰, 이어폰 등과 같은 다양한 음성 입력 장치 또는 수단 중의 하나를 포함할 수 있다. 다른 예에서, 출력부는, 메모리 또는 데이터베이스로 음성 데이터를 전송하여 저장할 수 있는 출 력 장치일 수 있다. 또 다른 예에서, 출력부는, 블루투스, WiFi, 인터넷 등의 무선 또는 유선 네트워크를 통해 음성 데이터를 송신할 수 있는 무선 또는 유선 출력장치일 수 있다. 또 다른 예에서, 출력부는 비디 오 데이터 또는 영상 파일에 음성 데이터를 결합하여, 메모리로 전송 및 저장하거나 온라인 상에서 다운로드 가 능하도록 변환할 수 있는 프로세서 또는 음성 처리 장치일 수 있다. 도 2 는 본 개시의 일 실시예에 따른 음성 번역부의 상세 구성를 나타내는 블록도이다. 음성 번역부는, 음성 인식부, 기계 번역부, 음성 합성부, 발성 특징 추출부, 감정 특 징 추출부 및 운율 특징 추출부 또는 운율 번역부를 포함할 수 있다. 음성 인식부는, 제1 언어의 음성 데이터를 제1 언어의 텍스트로 변환할 수 있다. 음성 인식부는, 제1 언어의 음성 데이터를 제1 언어로 텍스트로 변환하기 위해, 본 발명의 기술 분야에서 알려진 다양한 음성 인식 알고리즘 중의 하나를 사용할 수 있다. 예를 들어, 음성 인식부가 사용하는 음성인식 알고리즘은, HMM(hidden markov model), GMM(Gaussian mixuture model), SVM(support vector machine)과 같은 통계적 패턴 인식 방법, 또는 RNN, LSTM, DNN, CNN과 같은 인공신경망 모델 중의 어느 하나 또는 이들의 조합을 포함할 수 있으나, 이에 한정되는 것은 아니다. 기계 번역부는, 음성 인식부로부터 제1 언어의 텍스트를 수신하여 제2 언어의 텍스트를 생성할 수 있 다. 즉, 기계 번역부는 제1 언어의 텍스트를 이와 동일 또는 유사한 의미를 갖는 제2 언어의 텍스트로 변 환한다. 예를 들어, 기계 번역부가 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하기 위해, 통계적 기계 번역(statistical machine translation), 인공신경망 기계번역(neural network machine translation) 등 다양 한 기계번역 알고리즘 중의 하나를 사용할 수 있으나, 이에 한정되는 것은 아니다. 기계 번역부가, 기계번 역 알고리즘을 사용하는 경우, 복수의 제1 언어의 텍스트와, 이에 대응되는 복수의 제2 언어의 텍스트에 기초하 여 기계 학습을 실행함으로써, 기계번역 모델을 생성할 수 있다. 기계 번역부는, 이와 같이 미리 학습된 기계번역 모델에 제1 언어의 텍스트를 입력하여 제2 언어의 텍스트를 생성할 수 있다. 음성 합성부는, 기계번역부로부터 제2 언어의 텍스트를 수신하고 이에 대응하는 제2 언어의 음성 데 이터를 출력할 수 있다. 일 실시예에서, 음성 합성부는, 제2 언어의 텍스트에 더하여 제1 언어에 대한 화자의 발성 특징을 수신하 여, 이 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성할 수 있다. 여기서, 제1 언 어에 대한 화자의 발성 특징은, 해당 화자의 발성의 음색 또는 화자의 발성의 음 높이 중 적어도 하나를 포함할 수 있다. 다른 실시예에서, 음성 합성부는, 제2 언어의 텍스트에 더하여 제1 언어에 대한 화자의 발성 특징 및 제1 언어에 대한 화자의 감정 특징(emotion feature)를 수신하여, 이 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성할 수 있다. 여기서, 제1 언어에 대한 화자의 감정 특징은, 이 화자의 발화 내용 에 내재된 감정(예를 들어, 화자의 기쁨, 슬픔, 분노, 공포, 신뢰, 혐오, 놀람 또는 기대와 같은 감정)에 대한 정보를 포함할 수 있다. 또 다른 실시예에서, 음성 합성부는, 제2 언어의 텍스트에 더하여 제1 언어에 대한 화자의 발성 특징 및 제1 언어에 대한 화자의 운율 특징(prosody feature)를 수신하여, 이 화자의 음성을 모사하는 제2 언어의 텍스 트에 대한 출력 음성 데이터를 생성할 수 있다. 여기서, 제1 언어에 대한 화자의 운율 특징은, 발화 속도에 대 한 정보, 발음 강세에 대한 정보, 음 높이에 대한 정보 및 휴지 구간에 대한 정보 중 적어도 하나를 포함할 수 있다. 음성 합성부에서, 제2 언어의 텍스트에 더하여 수신되는 제1 언어에 대한 화자의 발성 특징, 감정 특징 및 운율 특징은, 제1 언어에 대한 임의의 화자가 발화한 음성 데이터로부터 추출될 수 있다. 일 실시예에서, 제1 언어에 대한 화자의 발성 특징, 감정 특징 및 운율 특징은, 제1 언어를 사용하는 서로 다른 화자가 발화한 음성데이터로부터 추출될 수 있다. 예를 들어, 제1 언어에 대한 감정 특징과 운율 특징은, 제1 언어의 음성 데이터 로부터 추출하고, 제1 언어에 대한 발성 특징은 제1 언어의 음성데이터의 화자가 아닌 제3의 화자(예를 들어, 목소리가 알려진 연예인, 정치인 등 유명인)가 발화한 음성 데이터로부터 추출될 수도 있다. 이와 같이, 제1 언 어에 대한 화자가 발화한 음성 데이터로부터 추출되는, 제1 언어에 대한 화자의 발성 특징, 감정 특징 및 운율 특징은, 음성 번역부의 외부로부터 수신되거나, 음성 번역부로 입력되는 제1 언어의 음성 데이터로부 터 추출될 수도 있다. 또한, 제1 언어에 대한 화자의 발성 특징, 감정 특징 또는 운율 특징은, 이 화자가 제1 언어로 발화한 음성 데 이터로부터 특징 벡터(feature vector)를 추출하여 생성될 수 있다. 예를 들어, 제1 언어로 발화한 음성데이터 로부터 추출되는 특징 벡터는, MFCC(mel frequency cepstral coefficient), LPC(linear predictive coefficients), PLP(perceptual linear prediction) 등과 같은 다양한 음성 특징 벡터들 중의 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 음성 번역부는, 입력되는 제1 언어의 음성 데이터로부터 제1 언어에 대한 화자의 발성 특징, 감정 특징 또 는 운율 특징을 추출하기 위해, 발성 특징 추출부, 감정 특징 추출부 또는 운율 특징 추출부 중 적어도 하나를 포함할 수 있다. 발성 특징 추출부는 제1 언어의 음성 데이터로부터 제1 언어에 대한 화자 의 발성 특징을 생성할 수 있다. 감정 특징 추출부는 제1 언어의 음성 데이터로부터 제1 언어에 대한 화자 의 감정 특징을 생성할 수 있다. 또한, 운율 특징 추출부는 제1 언어의 음성 데이터로부터 제1 언어에 대 한 화자의 운율 특징을 생성할 수 있다. 음성 합성부는, 기계 번역부로부터 수신한 제2 언어의 텍스 트에 더하여, 발성 특징 추출부, 감정 특징 추출부 또는 운율 특징 추출부 중 적어도 하나로부 터 수신된 정보를 수신하여, 제1 언어에 대한 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데 이터를 생성할 수 있다. 음성 합성부는, 제2 언어의 텍스트를 제2 언어의 음성 데이터로 변환하기 위해, 단일 인공 신경망 텍스트- 음성 합성(text-to-speech synthesis) 모델을 사용할 수 있다. 단일 인공 신경망 텍스트-음성 합성 모델은, 이 하 상세히 설명하는 인공신경망을 이용한 다양한 텍스트-음성 합성 모델 중의 하나로서, 복수의 제1 언어의 학 습 텍스트 및 복수의 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 복수의 제2 언어의 학습 텍스트 및 복수의 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초하여 학습된 텍 스트-음성 합성 모델일 수 있다. 이 경우, 음성 합성부는, 제1 언어에 대한 화자의 발성 특징과, 감정 특징 또는 운율 특징 중의 적어도 하 나를 다중 언어 텍스트-음성 합성 모델에 입력하여, 제1 언어에 대한 화자의 음성을 모사하는 제2 언어의 텍스 트에 대한 출력 음성 데이터를 생성할 수 있다. 이와 같이, 음성 번역부가 제1 언어에 대한 화자가 발화한 음성 데이터로부터 화자의 특징을 추출하여, 제 1 언어의 음성 데이터로부터 제2 언어의 음성데이터를 번역 및 합성하는데 이용하는 경우, 음성 합성부의 텍스트-음성 합성 모델이 해당 화자의 목소리를 사전에 학습하지 않은 경우에도 해당 화자의 음성을 모사하여 제2 언어의 출력 음성 데이터를 생성할 수 있다. 또한, 음성 번역부가, 제1 언어의 음성 데이터로부터 제2 언어의 음성 데이터를 번역 및 합성하는데 있어서, 제1 언어에 대한 화자의 감정 특징 또는 운율 특징을 반영하 는 경우, 그 화자가 발화한 음성에 내재된 감정이나 운율을 모사하여 제2 언어의 출력 음성 데이터를 생성할 수 있다. 음성 번역부는, 운율 특징 추출부로부터 출력되는 제1 언어에 대한 운율 특징을 수신하여 제2 언어에 대한 운율 특징을 생성하는 운율 번역부를 더 포함할 수 있다. 도시된 바와 같이, 운율 번역부는, 운 율 특징 추출부에 의해 제1 언어의 음성 데이터에서 추출된 운율 특징을 수신할 수 있다. 운율 번역부 는, 제1 언어의 음성 데이터에서 추출된 운율 특징을 제2 언어의 출력 음성의 운율 특징으로 변환할 수 있 다. 이와 같이 운율 번역부가 제1 언어의 입력 음성에서 화자의 운율 특징을 추출하여 제2 언어의 운율 특 징으로 번역하는 경우, 제1 언어의 음성의 말하는 속도, 끊어 읽기, 강조 등의 특징들이 제2 언어의 출력 음성 에 반영될 수 있다. 도 3은 본 개시의 일 실시예에 따른 음성 합성부의 상세 구성을 나타내는 블록도이다. 도시된 바와 같이, 음성 합성부는, 인코더(encoder), 어텐션(attention), 및 디코더(decoder)를 포함할 수 있다. 음성 합성부에 포함된 인코더는, 제2 언어의 텍스트를 수신하여, 합성해야할 음성에 대응되는 입력 텍스트가 어떤 것인지를 나타내는 정보를 생성한다. 또한, 어텐션은, 음성을 합성해야할 입력 텍스트의 위 치 정보를 생성한다. 디코더는, 어텐션으로부터 수신한 입력 텍스트의 위치 정보에 기초하여, 시간경과에 따라 해당 입력 텍스트에 대응되는 음성 데이터를 생성한다. 또한, 음성 합성부에 포함된 인코더 및 디코더는 화자의 발성 특징, 감정 특징 및 운율 특징을 수신할 수 있다. 여기서 발성특징, 감정 특징, 및 운율 특징 각각은, 화자 임베딩 벡터 (speaker embedding), 감정 임베딩 벡터(emotion embedding), 운율 임베딩 벡터(prosody embedding)일 수 있다. 음성 합성부에 포함된 인코더, 어텐션, 및 디코더는, 이하 도 5 내지 도 7을 참조하여 상세히 설명하는 바와 같이, 제2 언어의 입력 텍스트를 이와 대응되는 제2 언어의 음성 데이터로 변환하는데 있어서, 목적 화자의 발 성 특징, 감정 특징, 및/또는 운율 특징을 반영하여 목적 화자의 음성을 모사하는 단일 인공 신경망 텍스트-음 성 합성 모델을 구성할 수 있다. 본 개시의 일 실시예에서, 음성 합성부가 구성하는 단일 인공 신경망 텍 스트-음성 합성 모델은, 시퀀스-투-시퀀스 학습 모델(seq2seq; sequence-to-sequence model)을 이용하여 학습 된 것일 수 있다. 예를 들어, 시퀀스-투-시퀀스 학습 모델은, RNN에 기반한 인코더-디코더 구조(encoder- decoder architecture)(\"Sequence to Sequence Learning with Neural Networks,\" Ilya Sutskever, et al., 2014 참조)에 어텐션 구조(attention mechanism)(\"Neural Machine Translation by Jointly Learning to Align and Translate,\" Dzmitry Bahdanau, at al., 2015 및 \"Effective Approaches to Attention-based Neural Machine Translation,\" Minh-Thang Luong, at al., 2015 참조)를 결합하여 구현될 수 있다. 도 4는 본 개시의 일 실시예에 따른 운율 번역부(prosody translation)의 상세 구성을 나타내는 블록도이다. *도시된 바와 같이, 운율 번역부는 운율 인코더, 어텐션 및 운율 디코더를 포함할 수 있다. 운율 번역부에 포함된 인코더는, 제1 언어의 운율 특징을 수신하여, 번역해야할 운율 특징이 어떤 것인지를 나타내는 정보를 생성한다. 또한, 어텐션은, 번역해야할 운율 특징의 위치 정보를 생성한다. 디코더는, 어텐션으로부터 수신한 운율 특징의 위치 정보에 기초하여, 시간 경과에 따라 해당 운율 특징에 대응되는 제2 언어의 운율 특징을 생성한다. 운율 번역부에 포함된 인코더, 어텐션, 및 디코더는, 시퀀스-투-시퀀스 학습 모델 (seq2seq; sequence-to-sequence model)을 이용하여 학습된 것일 수 있다. 예를 들어, 시퀀스-투-시퀀스 학습 모델은, RNN에 기반한 인코더-디코더 구조에 어텐션 구조(attention mechanism)를 결합하여 구현될 수 있다. 도 5는 일 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타내는 도면이다. 인공 신경망 기반의 음성 합성부는, 다국어(언어 1, 언어 2, …, 언어 N)의 학습 텍스트와 이에 대응하는 다국어의 학습 음성 데이터 쌍으로 구성되는 데이터베이스를 이용하여 학습할 수 있다. 음성 합성부는, 텍 스트를 수신하고, 텍스트를 기계 학습 모델에 적용하여 출력된 음성 데이터를 정답 음성 데이터와 비교하여 손 실 함수(loss function)을 정의할 수 있다. 정답 음성 데이터는, 목적 화자에 의하여 직접 녹음된 음성 데이터 일 수 있다. 음성 합성부는, 손실 함수를 오차 역전파 (error back propagation) 알고리즘을 통해 학습하 여, 최종적으로 임의의 텍스트를 입력했을 때 원하는 출력 음성 데이터가 생성되는 인공 신경망을 얻을 수 있다. 음성 합성부는, 인코더, 디코더 및 보코더(vocoder)를 포함할 수 있다. 도 5의 인 코더 및 디코더는 각각 도 3의 인코더 및 디코더에 대응될 수 있다. 또한, 디코더에 포함된 어텐션(monotonic attention)은 도 3의 어텐션에 대응될 수 있다. 음성 합성부에 포함된 인코더는 적어도 하나의 글자 임베딩(text embedding)(ex: 언어1, 언어2, …, 언어N)을 수신할 수 있다. 여기서 글자 임베딩은 자모단위, 글자단위, 또는 음소(phoneme)단위로 분리된 텍스트 에 대응되는 임베딩 벡터일 수 있다. 인코더는 텍스트 임베딩 벡터를 획득하기 위하여 이미 학습된 기계 학습 모델을 사용할 수 있다. 인코더는 기계 학습을 수행하면서 기계 학습 모델을 갱신할 수 있는데, 이 경우, 분리된 텍스트에 대한 텍스트 임베딩 벡터도 변경될 수 있다. 음성 합성부에 포함된 인코더는, 텍스트 임베딩 벡터를 완전연결층(fully-connected layer)으로 구성 된 선행-네트(pre-net) 모듈에 통과시킬 수 있다. Pre-net은 일반적인 피드포워드 레이어(feedforward layer) 또는 선형 레이어(linear layer)일 수 있다. 인코더는, 선행-네트(pre-net) 모듈의 출력을 CBHG 모듈에 통과시킬 수 있다. CBHG 모듈은 ID 컨볼루션 뱅 크(ID convolution bank), 맥스 풀링(max pooling), 하이웨이 네트워크(highway network), 양방향 GRU(Bidirectional Gated Recurrent Unit)중 적어도 하나를 포함할 수 있다. 인코더는 CBHG 모듈로부터 출력되는 숨겨진 상태들(hidden states)(h)을 디코더로 출력할 수 있다. 인코더로부터 출력되는 숨겨진 상태들은, 음성을 합성해야 할 입력 텍스트가 어떨 것인지를 나타내는 정보일 수 있다. 디코더의 어텐션(Monotonic Attention)은 인코더로부터 수신한 숨겨진 상태들(h)을 수신할 수 있다. 또한 디코더의 어텐션은 어텐션 RNN으로부터 정보를 수신할 수 있다. 어텐션 RNN으로부터 수신 한 정보는 디코더가 이전 시간-단계(time-step)까지 어떤 음성을 생성했는지에 대한 정보일 수 있다. 또한 디코더의 어텐션은 어텐션 RNN으로부터 수신한 정보 및 인코더로부터의 수신한 숨겨진 상태들에 기초하여 컨텍스트 벡터(ct)를 출력할 수 있다. 컨텍스트 벡터(ct)는 현재 시간-단계(time-step)에서 입력 텍스 트 중 어떤 부분으로부터 음성을 생성할지 결정하기 위한 정보일 수 있다. 예를 들어, 디코더의 어텐션 은 음성 생성의 초반에는 텍스트 입력의 앞부분에 기초하여 음성을 생성하고, 음성이 생성되어 감에 따라 점점 텍스트 입력의 뒷부분에 기초하여 음성을 생성하도록 하는 정보를 출력할 수 있다. 디코더는, 완전연결층으로 구성된 선행-네트(pre-net) 모듈, GRU로 구성된 어텐션(attention) RNN 및 레지 듀얼(residual) GRU로 구성된 디코더 RNN의 세트를 적어도 하나 이상 포함할 수 있다. 디코더의 디코더 RNN은 r 프레임들을 생성할 수 있다. 디코더는 생성된 r 프레임들을 시간순으로 나열하여 멜-스케일 스펙 트로그램으로 표현된 음성 데이터를 출력할 수 있다. 음성을 합성하기 위해, 디코더는 화자의 발성 특징에 대응되는 정보인 원-핫 화자 ID(one-hot speaker i d)를 수신할 수 있다. 디코더는 룩업 테이블에서 원-핫 화자 ID 를 검색하여, 원-핫 화자 ID에 대응되는 화자의 임베딩 벡터(s)를 획득할 수 있다. 화자의 임베딩 벡터(s)는 인코더의 CBHG 모듈, 디코더의 디코더 RNN 또는 어텐션 RNN에 출력될 수 있다. 디코더 RNN 또는 어텐션 RNN은 화자마다 다르게 디코딩을 하도 록 인공 신경망의 구조를 구성할 수 있다. 또한, 음성 합성부는, 인공 신경망을 학습하기 위해서, 텍스트, 화자 인덱스, 음성 신호를 포함하는 데이 터베이스를 이용할 수 있다. 입력된 텍스트는 문자 단위의 원-핫 벡터(one-hot vector)로 구성될 수 있다. 다국 어 텍스트를 입력으로 하기 위해서는 해당 다국어 문자를 모두 표현할 수 있는 원-핫 벡터(one-hot vector)로 구성될 수 있다. 여기에 선택적으로 특정 언어를 나타내는 정보를 추가로 입력으로 할 수 있다. 특정 언어를 나 타내는 정보를 통해 언어를 바꿈으로써 특정 언어 스타일로 특정 언어와 다른 언어를 발음하는 음성을 합성할 수도 있다. 또한 화자 정보를 추가로 입력하여 화자별로 다른 음성을 합성하거나 해당 화자가 다른 언어를 말하 는 것처럼 음성을 합성할 수도 있다. 이렇게, 텍스트, 화자 정보, 언어 정보 등을 각각 인공 신경망의 입력으로 하고 해당 음성 신호를 정답으로 하여 앞서 언급한 방법과 같이 학습함으로써, 텍스트와 화자 정보를 입력으로 주었을 때 해당 화자의 음성을 출력할 수 있는 음성 합성부를 얻는다. 보코더는 디코더의 출력을 수신할 수 있다. 보코더에 포함된 CBHG 모듈은 디코더의 출력을 선형-스케일(linear-scale) 스펙트로그램으로 변환할 수 있다. 보코더에 포함된 CBHG 모듈의 출력은 크기 스펙트로그램(magnitude spectrogram)일 수 있다. 보코더는 스펙트로그램의 위상(phase)을 Griffin-Lim 알고리즘을 통해 예측할 수 있다. 보코더는 역 단기 푸리에 변환(Inverse Short-Time Fourier Transfor m)을 이용하여 시간 도메인(time domain)의 음성 신호를 출력할 수 있다. 도 6는 다른 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타내 는 도면이다. 도시된 바와 같이, 음성 합성부는, 인코더, 디코더 및 보코더(vocoder)를 포함할 수 있다. 도 6의 인코더 및 디코더는 각각 도 3의 인코더 및 디코더에 대응될 수 있다. 또한, 디코 더에 포함된 어텐션(monotonic attention)은 도 3의 어텐션에 대응될 수 있다. 이하에서, 도 6의 인코더, 디코더 및 보코더의 구성과 기능 중에서, 도 5의 인코더, 디코더 및 보 코더와 동일 또는 유사한 구성과 기능에 대해서는 설명을 생략한다. 도 6의 디코더는, 도 5의 디코더가 화자의 정보를 원-핫 화자ID로 수신한 것과 달리, 화자의 음성을 수신한다. 또한, 디코더는 화자의 음성에서 특징 정보를 추출하기 위하여 화자 식별 네트(speaker identification net)를 사용할 수 있다. 화자 식별 네트워크는 화자를 구분할 수 있는 특징을 추출할 수 있는 다양한 형태의 기계 학습 모델들 중의 하나 또는 이들 조합으로 구현될 수 있다. 일 실시예에서, 화자 식별 네트는, 기계 학습 모델에 기초하여 화자의 음성을 화자 임베딩 벡터로 변환할 수 있 다. 화자 임베딩 벡터는 화자의 발성 특징, 감정 특징 및 운율 특징 중 적어도 하나에 대한 임베딩 벡터일 수 있다. 화자 식별 네트의 기계 학습 모델은, 복수의 상이한 언어에 대한 복수의 화자의 음성 데이터를 수신하여화자의 발성 특징, 운율 특징, 감정 특징 등을 기계 학습할 수 있다. 화자 식별 네트는, 변환된 화자 임베딩 벡터(s)를 인코더의 CBHG, 디코더의 디코더 RNN 및 어텐션 RNN에 출력할 수 있다. 디코더는 화자 임베딩 벡터(s), 입력 텍스트에 기초하여 복수의 r 프레임들을 생성 할 수 있다. 디코더는 복수의 r 프레임들을 시간순으로 나열하여 멜 스펙트로그램으로 표현된 출력 음성 데이터를 생성할 수 있다. 보코더는 멜 스펙트로그램으로 표현된 출력 음성 데이터를 시간도메인의 음성으 로 변환할 수 있다. 도 7는 또 다른 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타 내는 도면이다. 도시된 바와 같이, 음성 합성부는, 인코더, 디코더 및 보코더를 포함할 수 있다. 도 7의 인코더 및 디코더는 각각 도 3의 인코더 및 디코더에 대응될 수 있다. 또한, 도 7의 디코 더에 포함된 어텐션은 도 3의 어텐션에 대응될 수 있다. 이하에서, 도 7의 인코더, 디코더 및 보코더의 구성과 기능 중에서, 도 5 및 도 6의 인코더(510, 610), 디코더(520, 620) 및 보코더 (530, 630)와 동일 또는 유사한 구성과 기능에 대해서는 설명을 생략한다. 도 7에 있어서, 인코더는 입력 텍스트를 수신할 수 있다. 인코더가 수신한 입력 텍스트는 복수의 언 어에 대한 텍스트를 포함할 수 있다. 예를 들어서, 입력 텍스트는 \"안녕하세요\" 또는 \"How are you?\" 와 같은 문장을 포함할 수 있다. 인코더는 수신된 입력 텍스트를 자모단위, 글자단위, 음소(phoneme)단위로 분리할 수 있다. 또는 인코더는 자모단위, 글자단위, 음소(phoneme)단위로 분리된 입력 텍스트를 수신할 수 있다. 인코더는 적어도 하나의 임베딩 레이어(ex: EL 언어1, EL 언어2, …, EL 언어N)를 포함할 수 있다. 인코더 의 적어도 하나의 임베딩 레이어는, 자모단위, 글자단위, 음소(phoneme)단위로 분리된 입력 텍스트를 텍스 트 임베딩 벡터로 변환할 수 있다. 인코더는 분리된 입력 텍스트를 텍스트 임베딩 벡터로 변환하기 위하여 이미 학습된 기계 학습 모델을 사용할 수 있다. 인코더는 기계 학습을 수행하면서 기계 학습 모델을 갱신 할 수 있는데, 이 경우, 분리된 입력 텍스트에 대한 텍스트 임베딩 벡터도 변경될 수 있다. 인코더는 텍스트 임베딩 벡터를 완전연결층(fully-connected layer)으로 구성된 DNN 모듈에 통과시킬 수 있다. DNN은 일반적인 피드포워드 레이어(feedforward layer) 또는 선형 레이어(linear layer)일 수 있다. 인코더는 DNN의 출력을 CNN과 RNN의 조합이 포함된 모듈에 통과시킬 수 있다. CNN은 컨볼루션 커널 (convolution kernel) 사이즈에 따른 지역적 특성을 포착할 수 있으며, RNN은 장기 의존성(long term dependency)을 포착할 수 있다. 인코더는 인코더의 숨겨진 상태들(h)을 출력할 수 있다. 인코더로부 터 출력되는 숨겨진 상태들은, 음성을 합성해야 할 입력 텍스트가 어떨 것인지를 나타내는 정보일 수 있다. 디코더의 임베딩 레이어는 인코더의 임베딩 레이어와 유사한 연산을 수행할 수 있다. 임베딩 레이어 는 화자 ID를 수신할 수 있다. 화자 ID는 원-핫 화자 ID(one-hot speaker ID)일 수 있으며, 이 경우 원-핫 화 자 ID에 의하여 화자에 따라 번호가 매겨질 수 있다. 예를 들어, \"제1 화자\"의 화자 ID는 \"1\"로, \"제2 화자\"의 화자 ID \"2\"로, \"제3 화자\"의 화자 ID \"3\"으로 매겨질 수 있다. 임베딩 레이어는 화자 ID를 화자 임베딩 벡터 (s)로 변환할 수 있다. 디코더는 화자 ID를 화자 임베딩 벡터(s)로 변환하기 위하여 이미 학습된 기계 학 습 모델을 사용할 수 있다. 디코더는 기계 학습을 수행하면서 기계 학습 모델을 갱신할 수 있는데, 이 경 우, 화자 ID에 대한 화자 임베딩 벡터(s)도 변경될 수 있다. 디코더의 어텐션(Attention)은 인코더로부터 수신한 숨겨진 상태들(h)을 수신할 수 있다. 또한 디코 더의 어텐션은 어텐션 RNN으로부터 정보를 수신할 수 있다. 어텐션 RNN으로부터 수신한 정보는 디코 더가 이전 시간-단계(time-step)까지 어떤 음성을 생성했는지에 대한 정보일 수 있다. 또한 디코더의 어텐션은 어텐션 RNN으로부터 수신한 정보 및 인코더로부터의 수신한 숨겨진 상태들에 기초하여 컨텍 스트 벡터(ct)를 출력할 수 있다. 컨텍스트 벡터(ct)는 현재 시간-단계(time-step)에서 입력 텍스트 중 어떤 부 분으로부터 음성을 생성할지 결정하기 위한 정보일 수 있다. 예를 들어, 디코더의 어텐션은 음성 생 성의 초반에는 텍스트 입력의 앞부분에 기초하여 음성을 생성하고, 음성이 생성되어 감에 따라 점점 텍스트 입 력의 뒷부분에 기초하여 음성을 생성하도록 하는 정보를 출력할 수 있다. 디코더는 화자 임베딩 벡터(s)를 어텐션 RNN 및 디코더 RNN에 입력하여, 화자마다 다르게 디코딩을 하도록 인공 신경망의 구조를 구성할 수 있다. 또한, 음성 합성부는 인공 신경망(단일 인공 신경망 텍스트-음성 합성 모델)을 학습하기 위해, 텍스트, 화자 인덱스, 음성 데이터의 쌍으로 존재하는 데이터 베이스를 이용할 수있다. 텍스트 입력은 문자 단위의 원-핫 벡터(one-hot vector)로 구성될 수 있다. 음성 합성부는 다국어 텍스트를 입력으로 하기 위해서 해당 다국어 문자를 모두 표현할 수 있는 원-핫 벡터(one-hot vector)로 구성할 수 있다. 또한, 음성 합성부는 해당 언어를 나타내는 정보를 추가로 입력으로 할 수 있다. 더미 프레임들은 이전 시간-단계(time-step)가 존재하지 않는 경우 디코더에 입력되는 프레임이다. RNN은 자동회귀적(autoregressive)으로 기계학습을 할 수 있다. 즉, 직전 시간-단계에서 출력된 r 프레임 은 현재 시간-단계의 입력이 될 수 있다. 최초 시간-단계에서는 직전 시간-단계가 있을 수 없으므로, 디코더는 최초 시간-단계의 기계 학습에 더미 프레임들을 입력할 수 있다. 디코더는 완전연결층으로 구성된 DNN, GRU로 구성된 어텐션(attention) RNN 및 레지듀얼(residual) GRU로 구성된 디코더 RNN의 세트(721, 722, 723)를 적어도 하나 이상 포함할 수 있다. 여기서, DNN은 일반적인 피드포 워드 레이어(feedforward layer) 또는 선형 레이어(linear layer)일 수 있다. 또한, 어텐션 RNN은 어텐션에서 사용될 정보를 출력하는 레이어이다. 어텐션에 대해서는 위에서 이미 설명하였으므로 자세한 설명은 생략한다. 디코더 RNN은 어텐션으로부터 입력 텍스트의 위치 정보를 수신할 수 있다. 즉, 위치 정보는 디코더가 입력 텍스트의 어떤 위치를 음성으로 변환하고 있는지에 관한 정보일 수 있다. 디코더 RNN은 어텐션 RNN으로부터 정 보를 수신할 수 있다. 어텐션 RNN으로부터 수신한 정보는 디코더가 이전 시간-단계(time-step)까지 어떤 음성을 생성했는지에 대한 정보일 수 있다. 디코더 RNN은 지금까지 생성한 음성에 이어질 다음 출력 음성을 생 성할 수 있다. 출력 음성은 멜 스펙트로그램 형태인 r개의 프레임을 포함할 수 있다. 텍스트-음성 합성을 위하여 DNN, 어텐션 RNN 및 디코더 RNN의 동작은 반복적으로 수행될 수 있다. 예를 들어, 최초 시간-단계에서 획득된 r개의 프레임은 다음 시간-단계의 입력이 될 수 있다. 또한 시간-단계 에서 출력된 r개의 프레임은 다음 시간-단계의 입력이 될 수 있다. 상술한 바와 같은 과정을 통하여 텍스트의 모든 단위에 대한 음성이 생성될 수 있다. 음성 합성부는 각각 의 시간-단계마다 나온 멜 스펙트로그램을 시간순으로 연결(concatenate)하여 전체 텍스트에 대한 멜 스펙트로 그램의 음성을 획득할 수 있다. 전체 텍스트에 대한 멜 스펙트로그램의 음성은 보코더로 출력될 수 있다. 본 개시의 일 실시예에 따른 보코더의 CNN 또는 RNN은 인코더의 CNN 또는 RNN과 유사한 동작을 할 수 있다. 즉, 보코더의 CNN 또는 RNN은 지역적 특성과 장기 의존성을 포착할 수 있다. 보코더의 CNN 또 는 RNN은 선형-스케일 스펙트로그램(linear-scale spectrogram)을 출력할 수 있다. 선형-스케일 스펙트로그램은 크기 스펙트로그램(magnitude spectrogram)일 수 있다. 보코더는 스펙트로그램의 위상(phase)을 Griffin- Lim 알고리즘을 통해 예측할 수 있다. 보코더는 역 단기 푸리에 변환(Inverse Short-Time Fourier Transform)을 이용하여 시간 도메인(time domain)의 음성 신호를 출력할 수 있다. 본 개시의 다른 실시예에 따른 보코더는 기계학습모델에 기초하여 멜 스펙트로그램으로부터 음성 신호를 획득할 수 있다. 기계학습모델은 멜 스펙트로그램과 음성 신호 사이의 상관 관계를 기계학습한 모델일 수 있다. 예를 들어 WaveNet 또는 WaveGlow와 같은 모델이 사용될 수 있다. 인공 신경망 기반의 음성 합성부는, 다국어의 텍스트와 음성 데이터의 쌍으로 존재하는 대용량의 데이터 베이스를 이용하여 학습된다. 음성 합성부는 텍스트를 수신하고, 출력된 음성 데이터를 정답 음성 신호와 비교하여 손실 함수(loss function)을 정의할 수 있다. 음성 합성부는 손실 함수를 오차 역전파(error back propagation) 알고리즘을 통해 학습하여, 최종적으로 임의의 텍스트를 입력했을 때 원하는 음성 출력이 나 오는 인공 신경망을 얻을 수 있다. 음성 합성부는 위와 같은 방법으로 생성된 단일 인공 신경망 텍스트-음성 합성 모델을 이용하여, 언어를 바꿈으로써 제1 언어 스타일로 제2 언어를 발음하는 음성을 합성할 수 있다. 또한 음성 합성부는 화자의 발성 특징을 수신하여, 해당 화자의 스타일에 따른 음성을 합성할 수 있다. 또한 음성 합성부는 제1 언어 를 구사하는 화자가 제2 언어를 말하는 것처럼 음성을 합성할 수도 있다. 음성 합성부는 텍스트, 화자의 발성 특징, 언어 정보 등을 각각 인공 신경망의 입력으로 하여 음성 데이터 를 출력할 수 있다. 음성 합성부는 출력된 음성 데이터와 정답 음성 데이터를 비교하여 학습함으로써, 텍 스트와 화자의 발성 특징을 수신할 때 해당 화자의 음성을 모사하여 텍스트를 읽은 것 같은 출력 음성 데이터를 생성할 수 있다. 도 8은 본 개시의 일 실시예에 따라 다국어의 음성을 합성하기 위한 단일 인공 신경망 텍스트-음성 합성 모델을 학습하는 방법을 나타내는 도면이다.도시된 바와 같이, 복수의 언어에 대한 텍스트와 이에 대응하는 음성데이터에 기초하여 단일 인공 신경망 텍스 트-음성 합성 모델을 생성하는 학습 단계(training stage)는 한국어 텍스트와 음성 데이터의 쌍에 기초하여 실 행될 수 있다. 예를 들어 한국어 텍스트인 \"안녕하세요?\"를 인코더에 입력하고, 한국어 화자 벡터를 인코 더와 디코더에 입력한다. 인코더와 디코더는, 도 3 및 도 5 내지 도 7에 도시된 인코더 (310, 510, 610, 710)와 디코더(330, 520, 620, 720)에 대응될 수 있다. 한국어 화자 벡터는 도 5 내지 도 7의 화자 임베딩 벡터(s)에 대응될 수 있다. 한국어 화자 벡터는 화자의 발성 특징, 운율 특징, 또는 감정 특 징 중 적어도 하나에 대응될 수 있다. 도 3 및 도 5 내지 7을 참조하여 설명한 바와 같이, 디코더는 인코더로부터 한국어 음성 출력에 대응 하는 텍스트가 무엇인지를 나타내는 정보(즉, 인코더의 숨겨진 상태)를 수신할 수 있다. 디코더는 인코더 의 숨겨진 상태 및 한국어 화자 벡터에 기초하여 한국어 음성 출력을 생성할 수 있다. 음성 합성부는 생성 된 음성 출력과 실측 음성의 차이가 있는 경우 기계 학습하여 단일 인공 신경망 텍스트-음성 합성 모델을 갱신 할 수 있다. 음성 합성부는 생성된 음성 출력과 실측 음성의 차이가 최소가 되도록 단일 인공 신경망 텍스 트-음성 합성 모델을 반복적으로 갱신할 수 있다. 도 9는 본 개시의 일 실시예에 따라 다국어의 음성을 합성하기 위한 단일 인공 신경망 텍스트-음성 합성 모델을 학습하는 방법을 나타내는 도면이다. 도시된 바와 같이, 복수의 언어에 대한 텍스트와 이에 대응하는 음성데이터에 기초하여 단일 인공 신경망 텍스 트-음성 합성 모델을 생성하는 학습 단계(training stage)는, 도 8에 도시된 학습 단계에 이어서, 영어 텍스트 와 음성 데이터의 쌍에 기초하여 실행될 수 있다. 예를 들어 영어 텍스트인 \"Hello?\"를 인코더에 입력하고, 영어 화자 벡터를 인코더와 디코더에 입력한다. 도 3 및 도 5 내지 7을 참조하여 설명한 바와 같이, 디코더는 인코더로부터 영어 음성 출력에 대응하 는 텍스트가 무엇인지를 나타내는 정보(즉, 인코더의 숨겨진 상태)를 수신할 수 있다. 디코더는 인코더의 숨겨진 상태 및 영어 화자 벡터에 기초하여 영어 음성 출력을 생성할 수 있다. 음성 합성부는 생성된 음성 출력과 실측 음성의 차이가 있는 경우 기계 학습하여 단일 인공 신경망 텍스트-음성 합성 모델을 갱신할 수 있 다. 음성 합성부는 생성된 음성 출력과 실측 음성의 차이가 최소가 되도록 단일 인공 신경망 텍스트-음성 합성 모델을 반복적으로 갱신할 수 있다. 도 8의 음성 합성부에 의한 학습단계 및 도 10의 음성 합성부에 의한 학습단계가 별도로 도시되어 있 으나, 이들 학습단계는 동일한 단일 인공 신경망 텍스트-음성 합성 모델을 생성하기 위한 전체 학습 과정의 일 부를 나타낸 것일 수 있다. 즉, 음성 합성부는 복수의 언어의 텍스트 및 복수의 언어의 텍스트에 대응되는 음성 데이터를 수신하여, 각 언어의 텍스트 및 그에 대응되는 음성 데이터에 대한 학습을 반복 수행함으로써, 단일 인공 신경망 텍스트-음성 합성 모델을 생성할 수 있다. 도 10는 본 개시의 일 실시예에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 이용하여 다국어 음성 을 합성하는 방법을 나타내는 도면이다. 도시된 바와 같이, 도 8 및 도 9에 도시된 방법에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 적용 하여 다국어 음성 출력을 생성하는 테스트 단계(test stage)는, 인코더에 영어 텍스트(예: \"Hello?\")를 입 력하고, 인코더와 디코더에 한국어 화자 벡터를 입력하여 실행될 수 있다. 한국어 화자 벡터는 화자의 발성 특징에 대한 임베딩 벡터, 운율 특징에 관한 임베딩 벡터, 또는 감정 특징에 관한 임베딩 벡터 중 적어도 하나를 포함할 수 있다. 이 경우, 디코더는 영어 텍스트 및 한국어 화자 벡터를 단일 인공 신경망 텍스트-음성 합성 모델에 적용하 여 한국어 스타일의 영어 음성(예: \"Hello?\")을 생성할 수 있다. 단일 인공 신경망 텍스트-음성 합성 모델은 도 8 및 도 9의 방법에 따라 생성한 단일 인공 신경망 텍스트-음성 합성 모델일 수 있다. 디코더가 생성한 음성은 한국어 화자의 음성을 모사하여 영어 \"Hello?\"를 발음한 효과를 내는 음성일 수 있다. 도 11은 본 개시의 일 실시예에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 이용하여 다국어 음성 을 합성하는 방법을 나타내는 도면이다. 도시된 바와 같이, 도 8 및 도 9에 도시된 방법에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 적용 하여 다국어 음성 출력을 생성하는 테스트 단계(test stage)는, 인코더에 한국어 텍스트(예: \"안녕하세 요?\")를 입력하고, 인코더와 디코더에 영어 화자 벡터를 입력하여 실행될 수 있다. 영어 화자벡터는 화자의 발성 특징에 대한 임베딩 벡터, 운율 특징에 관한 임베딩 벡터, 또는 감정 특징에 관한 임베딩 벡터 중 적어도 하나를 포함할 수 있다. 이 경우, 디코더는 한국어 텍스트 및 영어 화자 벡터를 단일 인공 신경망 텍스트-음성 합성 모델에 적용하 여 영어 스타일의 영어 음성(예: \"안녕하세요?\")을 생성할 수 있다. 단일 인공 신경망 텍스트-음성 합성 모델은 도 8 및 도 9의 방법에 따라 생성한 단일 인공 신경망 텍스트-음성 합성 모델일 수 있다. 디코더가 생성한 음성은 영어 화자의 음성을 모사하여 영어 \"안녕하세요?\"를 발음한 효과를 내는 음성일 수 있다. 도 12는 본 개시의 일 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도시된 바와 같이, 비디오 번역 시스템은, 음성 제거부 및 음성 합성부를 포함할 수 있다. 음성 합성부는 도 2 내지 도 11의 음성 합성부에 대응될 수 있다. 비디오 번역 시스템은, 제1 언어의 학습 텍스트 및 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학습 텍스트 및 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데 이터에 기초하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계 를 수행할 수 있다. 비디오 번역 시스템은 단일 인공 신경망 텍스트-음성 합성 모델을 메모리에 저장하고 있을 수 있다. 또는 비디오 번역 시스템은 단일 인공 신경망 텍스트-음성 합성 모델을 네트워크로부터 수 신할 수 있다. 음성 제거부는, 제1 언어의 비디오 데이터를 수신할 수 있다. 제1 언어의 비디오 데이터는 제1 언어의 입 력 음성 데이터를 포함할 수 있다. 음성 제거부는, 제1 언어의 비디오 데이터에서 제1 언어의 입력 음성 을 제거한 비디오 데이터를 출력할 수 있다. 음성 합성부는, 미리 번역된 텍스트를 수신할 수 있다. 미리 번역된 텍스트는, 제1 언어 입력 음성 데이 터를 동일 또는 유사한 의미의 제1 언어의 텍스트로 변환한 후, 제1 언어의 텍스트를 이에 대응하는 제2 언어의 텍스트로 번역한 텍스트일 수 있다. 예를 들어, 미리 번역된 텍스트는 제1 언어의 비디오 데이터에 포함된 제2 언어의 자막 데이터일 수 있다. 음성 합성부는, 화자 ID 및 타이밍 정보를 수신할 수 있다. 일 실시예에서, 화자 ID는, 제1 언어의 비디 오 데이터에 포함된 음성 데이터의 화자의 발성 특징을 나타낼 수 있거나, 제1 언어를 사용하는 다른 화자의 발 성 특징을 나타낼 수도 있다. 예를 들어, 화자 ID는, 제1 언어의 비디오 데이터에 포함된 음성데이터의 화자가 아닌 제3의 화자(예를 들어, 목소리가 알려진 연예인, 정치인 등 유명인)가 발화한 음성 데이터로부터 추출될 수도 있다. 여기서 화자 ID는 도 5 내지 도 7의 화자 임베딩 벡터(s)에 대응될 수 있다. 화자 ID는 특정 화자 (예를 들어, 제1 언어에 대한 화자)에 대응되는 임베딩 벡터를 나타낼 수 있다. 따라서, 음성 합성부는 화자 ID에 기초하여 특정 화자의 목소리를 모사하는 음성을 생성할 수 있다. 한편, 타이밍 정보는 미리 번역된 텍스트가 비디오 데이터와 함께 표시될 시점을 나타내는 정보일 수 있다. 또는 타이밍 정보는 미리 번역된 텍스 트에 대응되는 제2 언어의 음성이 비디오 데이터와 함께 표시될 시점을 나타내는 정보일 수 있다. 음성 합성부 는 미리 번역된 텍스트 및 화자 ID를 단일 인공 신경망 텍스트-음성 합성 모델에 적용하여 번역된 언어의 음성을 획득할 수 있다. 비디오 번역 시스템은 타이밍 정보에 기초하여 번역된 언어의 음성 및 음성이 제 거된 비디오 데이터를 결합할 수 있다. 이에 따라 번역된 언어의 음성이 결합된 비디오 데이터는 제2 언어의 음 성을 포함하고 있을 수 있다. 도 13는 본 개시의 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도시된 바와 같이, 비디오 번역 시스템은, 음성 제거부, 음성 인식부, 기계 번역부, 음성 합성부, 화자 인식부, 타이밍 동기화부 및 발성 특징 추출부를 포함할 수 있다. 도 13의 음성 제거부 및 음성 합성부는, 도 12의 음성 제거부 및 음성 합성부 에 각각 대응될 수 있다. 따라서, 도 13에 대한 설명 중 도 12와 중복되는 설명은 생략한다. 비디오 번역 시스템은, 제1 언어의 학습 텍스트 및 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학습 텍스트 및 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데 이터에 기초하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계 를 수행할 수 있다. 비디오 번역 시스템은, 단일 인공 신경망 텍스트-음성 합성 모델을 메모리에 저장하 고 있을 수 있다. 또는 비디오 번역 시스템은, 단일 인공 신경망 텍스트-음성 합성 모델을 네트워크로부 터 수신할 수 있다. 비디오 번역 시스템은, 제1 언어의 입력 음성 데이터를 포함하는 비디오 데이터, 제1 언어의 입력 음성 데이터에 대응하는 제1 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계를 수행할 수 있다. 도 13을 참조하면, 음성 제거부는 제1 언어의 비디오 데이터를 수신할 수 있다. 제1 언어의 비디오 데이 터는 제1 언어의 입력 음성 데이터를 포함하는 비디오 데이터일 수 있다. 음성 제거부는 비디오 데이터로 부터 제1 언어의 입력 음성 데이터를 삭제하는 단계를 수행할 수 있다. 비디오 번역 시스템은, 제1 언어의 비디오 데이터로부터 제1 언어의 입력 음성 데이터를 추출할 수 있다. 음성 인식부는, 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환할 수 있다. 기계 번역부(133 0)는 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계를 수행할 수 있다. 음성 인식부 및 기계 번역부는 도 2의 음성 인식부 및 기계 번역부에 각각 대응할 수 있다. 화자 인식부는, 제1 언어의 음성에 기초하여 화자 ID를 획득할 수 있다. 일 실시예에 따르면, 화자 인식 부는 미리 학습된 기계 학습 모델로부터 획득될 수 있다. 기계 학습 모델은 복수의 화자들의 음성에 기초 하여 기계 학습된 모델일 수 있다. 화자 인식부는 제1 언어의 음성을 기계 학습 모델에 적용하여 화자 ID 를 생성할 수 있다. 화자 ID는 특정한 화자에 대응되는 임베딩 벡터일 수 있다. 다른 실시예에 따르면, 화자 인 식부는 기계 학습 모델을 사용하지 않을 수도 있다. 화자 인식부는 제1 언어의 음성과 가장 유사한 화자의 음성에 대한 화자 ID를 데이터베이스로부터 선택할 수 있다. 타이밍 동기화부는 음성 인식부와 기계 번역부으로부터 출력되는 음성 및 텍스트의 위치 정 보에 기초하여 타이밍 정보를 생성하여 출력할 수 있다. 타이밍 정보는 제1 언어의 음성에 대응되는 제2 언어의 음성이 비디오 데이터와 함께 표시될 시점을 나타내는 정보일 수 있다. 발성 특징 추출부는 제1 언어의 음성으로부터 제1 언어에 대한 화자의 발성 특징을 획득할 수 있다. 이미 설명한 바와 같이 화자의 발성 특징은 화자의 발성의 음색 또는 화자의 발성의 음 높이 중 적어도 하나를 포함 할 수 있다. 음성 합성부는 제2 언어의 텍스트 및 제1 언어에 대한 화자의 발성 특징을 상기 단일 인공 신경망 텍스트 -음성 합성 모델에 입력하여, 상기 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 수행할 수 있다. 비디오 번역 시스템은, 출력 음성 데이터를 음성 제거부에 의하 여 음성이 제거된 비디오 데이터에 결합하는 단계를 수행할 수 있다. 음성 합성부는 제2 언어의 텍스트, 화자 ID 및 제1 언어에 대한 화자의 발성 특징을 상기 단일 인공 신경 망 텍스트-음성 합성 모델에 입력하여, 상기 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계를 수행할 수 있다. 비디오 번역 시스템은, 제1 언어에 대한 화자의 발성 특징을 모사하는 제2 언어의 음성을 포함하는 비디 오 데이터를 출력할 수 있다. 즉, 제2 언어의 음성은 마치 제1 언어에 대한 화자의 음성을 모사하여 제2 언어로 말하는 것과 같은 음성일 수 있다. 도 14는 본 개시의 또 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 14를 참조하면 비디오 번역 시스템은, 음성 제거부, 음성 인식부, 기계 번역부, 음 성 합성부, 화자 인식부, 타이밍 동기화부, 발성 특징 추출부 및 감정 특징 추출부 를 포함할 수 있다. 도 14의 음성 제거부, 음성 인식부, 기계 번역부, 음성 합성부 , 화자 인식부, 타이밍 동기화부 및 발성 특징 추출부는, 도 13 의 음성 제거부 , 음성 인식부, 기계 번역부, 음성 합성부, 화자 인식부, 타이밍 동기화부 및 발성 특징 추출부에 각각 대응될 수 있다. 따라서 도 14 설명 중 도 13과 중복되는 설명은 생 략한다. 비디오 번역 시스템은 감정 특징 추출부를 더 포함할 수 있다. 감정 특징 추출부는 제1 언어 의 입력 음성 데이터로부터 제1 언어에 대한 화자의 감정 특징(emotion feature)을 생성할 수 있다. 감정 특징 추출부는, 도 2의 감정 특징 추출부에 대응할 수 있다. 음성 합성부는 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성할 수 있다. 음성 합성부는 제2 언어의 텍스트, 및 제1 언어에 대한 화자의 발성 특징 및 감정 특징을 단일 인공 신경 망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를생성하는 단계를 수행할 수 있다. 도 15는 본 개시의 또 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 15를 참조하면 비디오 번역 시스템은 음성 제거부, 음성 인식부, 기계 번역부, 음 성 합성부, 화자 인식부, 타이밍 동기화부, 발성 특징 추출부, 감정 특징 추출부 , 운율 특징 추출부 및 운율 번역부를 포함할 수 있다. 도 15의 음성 번역부는 음성 제 거부, 음성 인식부, 기계 번역부, 음성 합성부, 화자 인식부, 타이밍 동기화부 , 발성 특징 추출부 및 감정 특징 추출부는, 도 14 의 음성 제거부, 음성 인식부 , 기계 번역부, 음성 합성부, 화자 인식부, 타이밍 동기화부, 발성 특징 추출 부 및 감정 특징 추출부에 각각 대응될 수 있다. 따라서 도 15 설명 중 도 14과 중복되는 설명은 생략한다. 비디오 번역 시스템은, 운율 특징 추출부를 더 포함할 수 있다. 운율 특징 추출부는 제1 언 어의 입력 음성 데이터로부터 제1 언어에 대한 화자의 운율 특징(prosody feature)을 생성하는 단계를 수행할 수 있다. 운율 특징 추출부는 도 2의 운율 특징 추출부에 대응될 수 있다. 또한, 비디오 번역 시스템은, 운율 특징 추출부로부터 출력되는 제1 언어에 대한 운율 특징을 수신 하여 제2 언어에 대한 운율 특징을 생성하는 운율 번역부를 더 포함할 수 있다. 도시된 바와 같이, 운율 번역부는, 운율 특징 추출부에 의해 제1 언어의 음성 데이터에서 추출된 운율 특징을 수신할 수 있 다. 운율 번역부는, 제1 언어의 음성 데이터에서 추출된 운율 특징을 제2 언어의 출력 음성의 운율 특징 으로 변환할 수 있다. 이와 같이 운율 번역부가 제1 언어의 입력 음성에서 화자의 운율 특징을 추출하여 제2 언어의 운율 특징으로 번역하는 경우, 제1 언어의 음성의 말하는 속도, 끊어 읽기, 강조 등의 특징들이 제2 언어의 출력 음성에 반영될 수 있다. 또한 음성 합성부는, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성할 수 있다. 또한 음성 합성부는 제2 언어의 텍스트, 및 제1 언어에 대한 화자의 발성 특징, 감정 특징 및 운율 특징을 상기 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 화자의 음성을 모사하는 상기 제2 언어의 텍 스트에 대한 출력 음성 데이터를 생성하는 단계를 수행할 수 있다. 도 16은 본 개시의 일 실시예에 따른 음성 번역 방법을 나타내는 흐름도이다. 음성 번역부는 다중 언어 텍스트-음성 합성 모델을 이용하여 음성을 번역할 수 있다. 음성 번역부는 제1 언어의 학습 텍스트 및 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학 습 텍스트 및 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계를 수행할 수 있다. 음성 번역 부는 제1 언어의 입력 음성 데이터 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계를 수행할 수 있다. 음성 번역부는 제1 언어의 입력 음성 데이터를 제1 언어의 텍스트로 변환하는 단계(163 0)를 수행할 수 있다. 음성 번역부는 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단계를 수 행할 수 있다. 음성 번역부는 제2 언어의 텍스트 및 화자의 발성 특징을 단일 인공 신경망 텍스트-음성 합 성 모델에 입력하여, 화자의 음성을 모사하는 제2 언어의 텍스트에 대한 출력 음성 데이터를 생성하는 단계 를 수행할 수 있다. 도 17은 본 개시의 일 실시예에 따른 비디오 번역 방법을 나타내는 흐름도이다. 음성 번역부는 다중 언어 텍스트-음성 합성 모델을 이용하여 비디오 데이터의 음성을 번역을 할 수 있다. 음성 번역부는 제1 언어의 학습 텍스트 및 상기 제1 언어의 학습 텍스트에 대응되는 제1 언어의 학습 음성 데이터와, 제2 언어의 학습 텍스트 및 상기 제2 언어의 학습 텍스트에 대응되는 제2 언어의 학습 음성 데이터에 기초하여 학습된 단일 인공 신경망 텍스트-음성 합성(text-to-speech synthesis) 모델을 획득하는 단계를 수행할 수 있다. 음성 번역부는 제1 언어의 입력 음성 데이터를 포함하는 비디오 데이터, 상기 제1 언어의 입력 음성 데이터에 대응하는 제1 언어의 텍스트, 및 상기 제1 언어에 대한 화자의 발성 특징을 수신하는 단계 를 수행할 수 있다. 음성 번역부는 비디오 데이터로부터 상기 제1 언어의 입력 음성 데이터를 삭제 하는 단계를 수행할 수 있다. 음성 번역부는 제1 언어의 텍스트를 제2 언어의 텍스트로 변환하는 단 계를 수행할 수 있다. 제2 언어의 텍스트 및 상기 제1 언어에 대한 화자의 발성 특징을 단일 인공 신경망 텍스트-음성 합성 모델에 입력하여, 상기 화자의 음성을 모사하는 상기 제2 언어의 텍스트에 대한 출력 음성 데 이터를 생성하는 단계를 수행할 수 있다. 음성 번역부는 출력 음성 데이터를 비디오 데이터에 결합하는 단계를 수행할 수 있다. 도 18은 본 개시의 일 실시예에 따른 텍스트-음성 합성 시스템의 블록도이다. 도 18을 참조하면, 일 실시예에 따른 텍스트-음성 합성 시스템은 데이터 학습부 및 데이터 인식부 를 포함할 수 있다. 데이터 학습부 및 데이터 인식부 각각은, 상술한 다양한 실시예에 따른 음성 번역 시스템, 비디오 번역 시스템(1200, 1300, 1400, 1500)의 구성요소들 중에서 인공신경망에 기반 한 음성 합성부에 대응될 수 있다. 또한, 텍스트-음성 합성 시스템은 프로세서 및 메모리를 포함할 수 있 다. 데이터 학습부는 텍스트에 대한 음성 학습할 수 있다. 데이터 학습부는 텍스트에 따라 어떤 음성을 출력할지에 관한 기준을 학습할 수 있다. 또한, 데이터 학습부는 어떤 음성의 특징을 이용하여 음성을 출 력할지에 관한 기준을 학습할 수 있다. 음성의 특징은 음소의 발음, 사용자의 어조, 억양, 강세 중 적어도 하나 를 포함할 수 있다. 데이터 학습부는 학습에 이용될 데이터를 획득하고, 획득된 데이터를 후술할 데이터 학습모델에 적용함으로써, 텍스트에 따른 음성을 학습할 수 있다. 데이터 인식부는 텍스트에 기초하여 텍스트에 대한 음성을 출력할 수 있다. 데이터 인식부는 학습 된 데이터 학습모델을 이용하여, 소정의 텍스트로부터 음성을 출력할 수 있다. 데이터 인식부는 학습에 의한 미리 설정된 기준에 따라 소정의 텍스트(데이터)를 획득할 수 있다. 또한, 데이터 인식부는 획득된 데이터를 입력 값으로 하여 데이터 학습모델을 이용함으로써, 소정의 데이터에 기초한 음성을 출력할 수 있다. 또한, 획득된 데이터를 입력 값으로 하여 데이터 학습모델에 의해 출력된 결과 값은, 데이터 학습모델을 갱신하 는데 이용될 수 있다. 데이터 학습부 및 데이터 인식부 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 데이터 학습부 및 데이터 인식부 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로 세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 이미 설명 한 각종 전자 장치에 탑재될 수도 있다. 또한 데이터 학습부 및 데이터 인식부는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들어, 데이터 학습부 및 데이터 인식부 중 하나는 전자 장치에 포함되고, 나머지 하나는 서버에 포함될 수 있다. 또한, 데이터 학습부 및 데이터 인식부는 유선 또는 무선으로 통하여, 데이터 학습부 가 구축한 모델 정보를 데이터 인식부로 제공할 수도 있고, 데이터 인식부로 입력된 데이터 가 추가 학습 데이터로써 데이터 학습부로 제공될 수도 있다. 한편, 데이터 학습부 및 데이터 인식부 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이 터 학습부 및 데이터 인식부 중 적어도 하나가 소프트웨어 모듈(또는, 인스트럭션(instruction)을 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 메모리 또는 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적 어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부 는 소정의 애플리케이션에 의해 제공될 수 있다. 본 개시의 일 실시예에 따른 데이터 학습부는 데이터 획득부, 전처리부, 학습 데이터 선택부 , 모델 학습부 및 모델 평가부를 포함할 수 있다. 데이터 획득부는 기계학습에 필요한 데이터를 획득할 수 있다. 학습을 위해서는 많은 데이터가 필요하므 로, 데이터 획득부는 복수의 텍스트 및 그에 대응되는 음성을 수신할 수 있다. 전처리부는 사용자의 심리 상태 판단을 위하여 획득된 데이터가 기계학습에 이용될 수 있도록, 획득된 데 이터를 전처리할 수 있다. 전처리부는 후술할 모델 학습부가 이용할 수 있도록, 획득된 데이터를 미리 설정된 포맷으로 가공할 수 있다. 예를 들어 전처리부는 텍스트 및 음성을 형태소 분석하여 형태소 임베딩을 획득할 수 있다. 학습 데이터 선택부는 전처리된 데이터 중에서 학습에 필요한 데이터를 선택할 수 있다. 선택된 데이터는 모델 학습부에 제공될 수 있다. 학습 데이터 선택부는 기 설정된 기준에 따라, 전처리된 데이터 중 에서 학습에 필요한 데이터를 선택할 수 있다. 또한, 학습 데이터 선택부는 후술할 모델 학습부에의한 학습에 의해 기 설정된 기준에 따라 데이터를 선택할 수도 있다. 모델 학습부는 학습 데이터에 기초하여 텍스트에 따라 어떤 음성을 출력할 지에 관한 기준을 학습할 수 있다. 또한, 모델 학습부는 텍스트에 따라 음성을 출력하는 학습모델을 학습 데이터로써 이용하여 학습시 킬 수 있다. 이 경우, 데이터 학습모델은 미리 구축된 모델일 수 있다. 예를 들어, 데이터 학습모델은 기본 학 습 데이터(예를 들어, 샘플 음성 등)을 입력 받아 미리 구축된 모델일 수 있다. 데이터 학습모델은, 학습모델의 적용 분야, 학습의 목적 또는 장치의 컴퓨터 성능 등을 고려하여 구축될 수 있 다. 데이터 학습모델은, 예를 들어, 신경망(Neural Network)을 기반으로 하는 모델일 수 있다. 예컨대, Deep Neural Network (DNN), Recurrent Neural Network (RNN), Long Short-Term Memory models (LSTM), BRDNN (Bidirectional Recurrent Deep Neural Network), Convolutional Neural Networks (CNN)과 같은 모델이 데이 터 학습모델로써 사용될 수 있으나, 이에 한정되지 않는다. 다양한 실시예에 따르면, 모델 학습부는 미리 구축된 데이터 학습모델이 복수 개가 존재하는 경우, 입력 된 학습 데이터와 기본 학습 데이터의 관련성이 큰 데이터 학습모델을 학습할 데이터 학습모델로 결정할 수 있 다. 이 경우, 기본 학습 데이터는 데이터의 타입 별로 기 분류되어 있을 수 있으며, 데이터 학습모델은 데이터 의 타입 별로 미리 구축되어 있을 수 있다. 예를 들어, 기본 학습 데이터는 학습 데이터가 생성된 지역, 학습 데이터가 생성된 시간, 학습 데이터의 크기, 학습 데이터의 장르, 학습 데이터의 생성자, 학습 데이터 내의 오 브젝트의 종류 등과 같은 다양한 기준으로 기 분류되어 있을 수 있다. 또한, 모델 학습부는, 예를 들어, 오류 역전파법(error back-propagation) 또는 경사 하강법(gradient descent)을 포함하는 학습 알고리즘 등을 이용하여 데이터 학습모델을 학습시킬 수 있다. 또한, 모델 학습부는, 예를 들어, 학습 데이터를 입력 값으로 하는 지도 학습(supervised learning)을 통 하여, 데이터 학습모델을 학습할 수 있다. 또한, 모델 학습부는, 예를 들어, 별다른 지도없이 상황 판단 을 위해 필요한 데이터의 종류를 스스로 학습함으로써, 상황 판단을 위한 기준을 발견하는 비지도 학습 (unsupervised learning)을 통하여, 데이터 학습모델을 학습할 수 있다. 또한, 모델 학습부는, 예를 들어, 학습에 따른 상황 판단의 결과가 올바른 지에 대한 피드백을 이용하는 강화 학습(reinforcement learning)을 통하여, 데이터 학습모델을 학습할 수 있다. 또한, 데이터 학습모델이 학습되면, 모델 학습부는 학습된 데이터 학습모델을 저장할 수 있다. 이 경우, 모델 학습부는 학습된 데이터 학습모델을 데이터 인식부를 포함하는 전자 장치의 메모리에 저장할 수 있다. 또는, 모델 학습부는 학습된 데이터 학습모델을 전자 장치와 유선 또는 무선 네트워크로 연결되 는 서버의 메모리에 저장할 수도 있다. 이 경우, 학습된 데이터 학습모델이 저장되는 메모리는, 예를 들면, 전자 장치의 적어도 하나의 다른 구성요소 에 관계된 명령 또는 데이터를 함께 저장할 수도 있다. 또한, 메모리는 소프트웨어 및/또는 프로그램을 저장할 수도 있다. 프로그램은, 예를 들면, 커널, 미들웨어, 어플리케이션 프로그래밍 인터페이스(API) 및/또는 어플리 케이션 프로그램(또는 \"어플리케이션\") 등을 포함할 수 있다. 모델 평가부는 데이터 학습모델에 평가 데이터를 입력하고, 평가 데이터로부터 출력되는 결과가 소정 기 준을 만족하지 못하는 경우, 모델 학습부로 하여금 다시 학습하도록 할 수 있다. 이 경우, 평가 데이터는 데이터 학습모델을 평가하기 위한 기 설정된 데이터일 수 있다. 예를 들어, 모델 평가부는 평가 데이터에 대한 학습된 데이터 학습모델의 결과 중에서, 인식 결과가 정확 하지 않은 평가 데이터의 개수 또는 비율이 미리 설정된 임계치를 초과하는 경우 소정 기준을 만족하지 못한 것으로 평가할 수 있다. 예컨대, 소정 기준이 비율 2%로 정의되는 경우, 학습된 데이터 학습모델이 총 1000개의 평가 데이터 중의 20개를 초과하는 평가 데이터에 대하여 잘못된 인식 결과를 출력하는 경우, 모델 평가부 는 학습된 데이터 학습모델이 적합하지 않은 것으로 평가할 수 있다. 한편, 학습된 데이터 학습모델이 복수 개가 존재하는 경우, 모델 평가부는 각각의 학습된 동영상 학습모 델에 대하여 소정 기준을 만족하는지를 평가하고, 소정 기준을 만족하는 모델을 최종 데이터 학습모델로써 결정 할 수 있다. 이 경우, 소정 기준을 만족하는 모델이 복수 개인 경우, 모델 평가부는 평가 점수가 높은 순 으로 미리 설정된 어느 하나 또는 소정 개수의 모델을 최종 데이터 학습모델로써 결정할 수 있다. 한편, 데이터 학습부 내의 데이터 획득부, 전처리부, 학습 데이터 선택부, 모델 학습 부 및 모델 평가부 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치에 탑재될 수 있다. 예를 들어, 데이터 획득부, 전처리부, 학습 데이터 선택부, 모델 학습부 및 모델 평가부 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨 어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전술한 각종 전자 장치에 탑재될 수도 있다. 또한, 데이터 획득부, 전처리부, 학습 데이터 선택부, 모델 학습부 및 모델 평가부 는 하나의 전자 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들 어, 데이터 획득부, 전처리부, 학습 데이터 선택부, 모델 학습부 및 모델 평가부 중 일부는 전자 장치에 포함되고, 나머지 일부는 서버에 포함될 수 있다. 또한, 데이터 획득부, 전처리부, 학습 데이터 선택부, 모델 학습부 및 모델 평가부 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이터 획득부, 전처리부, 학습 데이 터 선택부, 모델 학습부 및 모델 평가부 중 적어도 하나가 소프트웨어 모듈(또는, 인스트럭 션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가 능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경 우, 적어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머 지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 본 개시의 일 실시예에 따른 데이터 인식부는 데이터 획득부, 전처리부, 인식 데이터 선택부 , 인식 결과 제공부 및 모델 갱신부를 포함할 수 있다. 데이터 획득부는 음성을 출력하기 위해 필요한 텍스트를 획득할 수 있다. 반대로 데이터 획득부는 텍스트를 출력하기 위해 필요한 음성을 획득할 수 있다. 전처리부는 음성 또는 텍스트를 출력하기 위해 획득된 데이터가 이용될 수 있도록, 획득된 데이터를 전처리할 수 있다. 전처리부는 후술할 인식 결과 제 공부가 음성 또는 텍스트를 출력하기 위해 획득된 데이터를 이용할 수 있도록, 획득된 데이터를 기 설정 된 포맷으로 가공할 수 있다. 인식 데이터 선택부는 전처리된 데이터 중에서 음성 또는 텍스트를 출력하기 위해 필요한 데이터를 선택 할 수 있다. 선택된 데이터는 인식 결과 제공부에게 제공될 수 있다. 인식 데이터 선택부는 음성 또는 텍스트를 출력하기 위한 기 설정된 기준에 따라, 전처리된 데이터 중에서 일부 또는 전부를 선택할 수 있 다. 또한, 인식 데이터 선택부는 모델 학습부에 의한 학습에 의해 기 설정된 기준에 따라 데이터를 선택할 수도 있다. 인식 결과 제공부는 선택된 데이터를 데이터 학습모델에 적용하여 음성 또는 텍스트를 출력할 수 있다. 인식 결과 제공부는 인식 데이터 선택부에 의해 선택된 데이터를 입력 값으로 이용함으로써, 선택 된 데이터를 데이터 학습모델에 적용할 수 있다. 또한, 인식 결과는 데이터 학습모델에 의해 결정될 수 있다. 모델 갱신부는 인식 결과 제공부에 의해 제공되는 인식 결과에 대한 평가에 기초하여, 데이터 학습 모델이 갱신되도록 할 수 있다. 예를 들어, 모델 갱신부는 인식 결과 제공부에 의해 제공되는 인식 결과를 모델 학습부에게 제공함으로써, 모델 학습부가 데이터 학습모델을 갱신하도록 할 수 있다. 한편, 데이터 인식부 내의 데이터 획득부, 전처리부, 인식 데이터 선택부, 인식 결과 제공부 및 모델 갱신부 중 적어도 하나는, 적어도 하나의 하드웨어 칩 형태로 제작되어 전자 장치 에 탑재될 수 있다. 예를 들어, 데이터 획득부, 전처리부, 인식 데이터 선택부, 인식 결과 제공부 및 모델 갱신부 중 적어도 하나는 인공 지능(AI; artificial intelligence)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 또는 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전술한 각종 전자 장치에 탑재될 수도 있다. 또한, 데이터 획득부, 전처리부, 인식 데이터 선택부, 인식 결과 제공부 및 모델 갱신 부는 하나의 전자 장치에 탑재될 수도 있으며, 또는 별개의 전자 장치들에 각각 탑재될 수도 있다. 예를 들어, 데이터 획득부, 전처리부, 인식 데이터 선택부, 인식 결과 제공부 및 모델 갱신 부 중 일부는 전자 장치에 포함되고, 나머지 일부는 서버에 포함될 수 있다. 또한, 데이터 획득부, 전처리부, 인식 데이터 선택부, 인식 결과 제공부 및 모델 갱신 부 중 적어도 하나는 소프트웨어 모듈로 구현될 수 있다. 데이터 획득부, 전처리부, 인식 데 이터 선택부, 인식 결과 제공부 및 모델 갱신부 중 적어도 하나가 소프트웨어 모듈(또는, 인스트럭션(instruction) 포함하는 프로그램 모듈)로 구현되는 경우, 소프트웨어 모듈은 컴퓨터로 읽을 수 있는 판독 가능한 비일시적 판독 가능 기록매체(non-transitory computer readable media)에 저장될 수 있다. 또한, 이 경우, 적어도 하나의 소프트웨어 모듈은 OS(Operating System)에 의해 제공되거나, 소정의 애플리케이션에 의해 제공될 수 있다. 또는, 적어도 하나의 소프트웨어 모듈 중 일부는 OS(Operating System)에 의해 제공되고, 나머지 일부는 소정의 애플리케이션에 의해 제공될 수 있다. 이제까지 다양한 실시예들을 중심으로 살펴보았다. 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있 을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되어야 한다. 본 발명 의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다. 한편, 상술한 본 발명의 실시예들은 컴퓨터에서 실행될 수 있는 프로그램으로 작성가능하고, 컴퓨터로 읽을 수 있는 기록매체를 이용하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 상기 컴퓨터로 읽을 수 있는 기록매체는 마그네틱 저장매체(예를 들면, 롬, 플로피 디스크, 하드디스크 등), 광학적 판독 매체 (예를 들면, 시디롬, 디브이디 등)와 같은 저장매체를 포함한다."}
{"patent_id": "10-2020-0174483", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 음성 번역 시스템을 나타내는 블록도이다. 도 2는 일 실시예에 따른 음성 번역부의 상세 구성을 나타내는 블록도이다. 도 3은 일 실시예에 따른 음성 합성부의 상세 구성을 나타내는 블록도이다. 도 4는 일 실시예에 따른 운율 번역부(prosody translation)의 상세 구성을 나타내는 블록도이다. 도 5는 일 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타내는 도면이다. 도 6는 다른 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타내 는 도면이다. 도 7는 또 다른 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성부의 구성을 나타 내는 도면이다. 도 8은 일 실시예에 따라 다국어의 음성을 합성하기 위한 단일 인공 신경망 텍스트-음성 합성 모델을 학습하는 방법을 나타내는 도면이다. 도 9는 일 실시예에 따라 다국어의 음성을 합성하기 위한 단일 인공 신경망 텍스트-음성 합성 모델을 학습하는 방법을 나타내는 도면이다. 도 10는 일 실시예에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 이용하여 다국어의 음성을 합성하 는 방법을 나타내는 도면이다.도 11은 일 실시예에 따라 학습된 단일 인공 신경망 텍스트-음성 합성 모델을 이용하여 다국어의 음성을 합성하 는 방법을 나타내는 도면이다. 도 12는 일 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 13는 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 14는 또 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 15는 또 다른 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 블록도이다. 도 16은 일 실시예에 따른 음성 번역 방법을 나타내는 흐름도이다. 도 17은 일 실시예에 따른 비디오 번역 방법을 나타내는 흐름도이다. 도 18은 일 실시예에 따른 텍스트-음성 합성 시스템의 블록도이다."}
