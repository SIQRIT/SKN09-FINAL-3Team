{"patent_id": "10-2020-0185251", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0094008", "출원번호": "10-2020-0185251", "발명의 명칭": "인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치 및 방법", "출원인": "주식회사 슈퍼플럭스", "발명자": "윤성진"}}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법에 있어서,사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보를 결정하는 단계;미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 움직임 정보를 결정하는 단계;미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 감정표현 정보를 결정하는 단계; 및상기 외형 정보, 상기 움직임 정보 및 상기 감정 표현 정보에 기초하여 상기 디지털 휴먼을 시각화하는 단계, 를 포함하는, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 디지털 휴먼과의 상호 작용을 위한 제1사용자 입력을 사용자 단말을 통해 수신하는 단계; 및상기 제1사용자 입력에 대응하는 감정 분석 및 상황 분석을 포함하는 제1분석을 수행하는 단계,를 더 포함하고,상기 움직임 정보 및 상기 감정 표현 정보는, 상기 제1분석의 결과에 기초하여 결정되는 상기 제1사용자 입력에대한 상기 디지털 휴먼의 응답에 부합하도록 결정되는 것을 특징으로 하는, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,미리 학습된 인공지능 기반의 콘텐츠 연출 모델에 기초하여 상기 디지털 휴먼과 연계된 연출 요소 정보를 결정하는 단계,를 더 포함하는 것인, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 디지털 휴먼이 등장하는 콘텐츠의 스크립트를 포함하는 제2사용자 입력을 사용자 단말을 통해 수신하는 단계; 및상기 제2사용자 입력에 대한 콘텐츠 분석을 포함하는 제2분석을 수행하는 단계,를 더 포함하고,상기 움직임 정보, 상기 감정 표현 정보 및 상기 연출 요소 정보는, 상기 제2분석의 결과에 기초하여 파악되는상기 스크립트의 내용을 상기 디지털 휴먼이 표현하도록 결정되는 것을 특징으로 하는, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 외형 정보를 결정하는 단계 이전에,적어도 하나의 참조 이미지에 기초하여 상기 디지털 휴먼과 연계된 컨셉을 도출하는 단계,공개특허 10-2022-0094008-3-를 더 포함하는 것인, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 컨셉을 도출하는 단계는,사용자 단말로 인가된 컨셉 선택 입력에 기초하여 미리 설정된 복수의 컨셉 카테고리 중 어느 하나의 카테고리를 결정하는 단계; 및상기 결정된 카테고리에 기초하여 상기 디지털 휴먼이 착용하는 의상을 포함하는 외형 특성을 결정하는 단계,를 포함하는 것인, 콘텐츠 생성 방법."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치에 있어서,사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보를 결정하는 외형 결정부;미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 움직임 정보를 결정하는 모션 결정부;미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 감정표현 정보를 결정하는 감정 결정부; 및상기 외형 정보, 상기 움직임 정보 및 상기 감정 표현 정보에 기초하여 상기 디지털 휴먼을 시각화하는 시각화부,를 포함하는, 콘텐츠 생성 장치."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 디지털 휴먼과의 상호 작용을 위한 제1사용자 입력을 사용자 단말을 통해 수신하고, 상기 제1사용자 입력에 대응하는 감정 분석 및 상황 분석을 포함하는 제1분석을 수행하는 입력 분석부,를 더 포함하고,상기 움직임 정보 및 상기 감정 표현 정보는, 상기 제1분석의 결과에 기초하여 결정되는 상기 제1사용자 입력에대한 상기 디지털 휴먼의 응답에 부합하도록 결정되는 것을 특징으로 하는, 콘텐츠 생성 장치."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,미리 학습된 인공지능 기반의 콘텐츠 연출 모델에 기초하여 상기 디지털 휴먼과 연계된 연출 요소 정보를 결정하는 연출 적용부,를 더 포함하는 것인, 콘텐츠 생성 장치."}
{"patent_id": "10-2020-0185251", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 입력 분석부는,상기 디지털 휴먼이 등장하는 콘텐츠의 스크립트를 포함하는 제2사용자 입력을 사용자 단말을 통해 수신하고,상기 제2사용자 입력에 대한 콘텐츠 분석을 포함하는 제2분석을 수행하고,상기 움직임 정보, 상기 감정 표현 정보 및 상기 연출 요소 정보는, 상기 제2분석의 결과에 기초하여 파악되는상기 스크립트의 내용을 상기 디지털 휴먼이 표현하도록 결정되는 것을 특징으로 하는, 콘텐츠 생성 장치.공개특허 10-2022-0094008-4-청구항 11 제7항에 있어서,적어도 하나의 참조 이미지에 기초하여 상기 디지털 휴먼과 연계된 컨셉을 도출하는 컨셉 도출부,를 더 포함하는 것인, 콘텐츠 생성 장치."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치 및 방법이 개시되며, 본원의 일 실시예에 따른 인공지능 을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보 를 결정하는 단계, 미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디 지털 휴먼의 움직임 정보를 결정하는 단계, 미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 감정 표현 정보를 결정하는 단계 및 상기 외형 정보, 상기 움직임 정보 및 상기 감정 표현 정보에 기초하여 상기 디지털 휴먼을 시각화하는 단계를 포함할 수 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 들어 신종 코로나 바이러스 감염증의 확산으로 인해 비접촉(Untact)을 실현하기 위한 다양한 기술이 주목 받고 있다. 이와 관련하여, 디지털 휴먼 기술은 사람의 신체 구조 및 움직임을 데이터화하여 분석하고, 가상 공 간에서 마치 실재하는 사람처럼 움직임을 재현하는 기술이다. 한편, 인공지능 기반 챗봇(Chatter Robot)이 사용자의 질문에 적절한 답변을 자동으로 탐색하여 제공하는 기능 까지는 갖추고 있으나, 사용자와의 감정적 교류는 이루어지기 어려웠던 것과 달리, 디지털 휴먼은 실제 사람만 큼 생동적인 상호 작용이 가능한 가상 인간으로서 구현되고 있으며, 최근 들어서는 실제 사람과 생김새가 흡사 하고, 행동과 표정에 이질감이 느껴지지 않도록 하는 방향으로 개발되고 있다. 즉, 최근까지 개발된 개인비서 시스템, 챗봇 플랫폼, 인공지능 스피커 등에서 사용되는 인공지능 대화 시스템은 사람의 명령어에 대한 의도를 이해하고 그에 대응하는 답변 문구를 제공하는 방식으로 동작하여, 사람이 기능적 인 요구를 전달하고 이에 기계가 사람의 요구에 대한 해답을 제공하는 단편적인 형태로 발전되어 왔다. 달리 말해, 종래의 인공지능 대화 시스템은 사용자의 감정을 파악하거나 인공지능의 감정을 표현하는 데에는 한 계가 있어, 사용자의 기능적 요구를 해결하는 데에는 적합하나 인공지능이 제공하는 표현을 인간처럼 풍부하고 자연스럽게 느껴지도록 하여 사람과 기계 간의 정서적 교류를 불러일으킬 수 있는 인터랙션(interaction)을 구 현하기에는 부족하였다. 이에 따라, 자연스러운 감정 표현, 움직임 표현 등을 수행하고 사용자의 언어에 대한 맥락, 분위기 등을 면밀하 게 파악하여 사용자와의 정서적 교류를 제공할 수 있는 디지털 휴먼 기반의 콘텐츠의 고도화가 요구된다. 본원의 배경이 되는 기술은 한국등록특허공보 제10-2063389호에 개시되어 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 사용자와의 자연스러운 인터랙션을 위한 외형, 움직임, 감정 표현 등의 구현 요소를 포함하는 디지털 휴먼 기반의 콘텐츠를 제공하는 인공지능을 이용한 디지 털 휴먼 기반 콘텐츠 생성 장치 및 방법을 제공하려는 것을 목적으로 한다. 다만, 본원의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보를 결정하는 단계, 미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 움직 임 정보를 결정하는 단계, 미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 감정 표현 정보를 결정하는 단계 및 상기 외형 정보, 상기 움직임 정보 및 상기 감정 표현 정보에 기초하여 상기 디지털 휴먼을 시각화하는 단계를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 상기 디지털 휴먼과 의 상호 작용을 위한 제1사용자 입력을 사용자 단말을 통해 수신하는 단계 및 상기 제1사용자 입력에 대응하는 감정 분석 및 상황 분석을 포함하는 제1분석을 수행하는 단계를 포함할 수 있다. 또한, 상기 움직임 정보 및 상기 감정 표현 정보는, 상기 제1분석의 결과에 기초하여 결정되는 상기 제1사용자 입력에 대한 상기 디지털 휴먼의 응답에 부합하도록 결정될 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 미리 학습된 인공지 능 기반의 콘텐츠 연출 모델에 기초하여 상기 디지털 휴먼과 연계된 연출 요소 정보를 결정하는 단계를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 상기 디지털 휴먼이 등장하는 콘텐츠의 스크립트를 포함하는 제2사용자 입력을 사용자 단말을 통해 수신하는 단계 및 상기 제2사용 자 입력에 대한 콘텐츠 분석을 포함하는 제2분석을 수행하는 단계를 포함할 수 있다. 또한, 상기 움직임 정보, 상기 감정 표현 정보 및 상기 연출 요소 정보는, 상기 제2분석의 결과에 기초하여 파 악되는 상기 스크립트의 내용을 상기 디지털 휴먼이 표현하도록 결정될 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은, 상기 외형 정보를 결 정하는 단계 이전에, 적어도 하나의 참조 이미지에 기초하여 상기 디지털 휴먼과 연계된 컨셉을 도출할 수 있다. 또한, 상기 컨셉을 도출하는 단계는, 사용자 단말로 인가된 컨셉 선택 입력에 기초하여 미리 설정된 복수의 컨 셉 카테고리 중 어느 하나의 카테고리를 결정하는 단계 및 상기 결정된 카테고리에 기초하여 상기 디지털 휴먼 이 착용하는 의상을 포함하는 외형 특성을 결정하는 단계를 포함할 수 있다. 한편, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치는, 사용자와 상호 작용 하도록 구현되는 디지털 휴먼의 외형 정보를 결정하는 외형 결정부, 미리 학습된 인공지능 기반의 모션 구현 모 델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 움직임 정보를 결정하는 모션 결정부, 미리 학습 된 인공지능 기반의 감정 구현 모델에 기초하여 상기 외형 정보에 대응하는 상기 디지털 휴먼의 감정 표현 정보 를 결정하는 감정 결정부 및 상기 외형 정보, 상기 움직임 정보 및 상기 감정 표현 정보에 기초하여 상기 디지 털 휴먼을 시각화하는 시각화부를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치는, 상기 디지털 휴먼과 의 상호 작용을 위한 제1사용자 입력을 사용자 단말을 통해 수신하고, 상기 제1사용자 입력에 대응하는 감정 분 석 및 상황 분석을 포함하는 제1분석을 수행하는 입력 분석부를 포함할 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치는, 미리 학습된 인공지 능 기반의 콘텐츠 연출 모델에 기초하여 상기 디지털 휴먼과 연계된 연출 요소 정보를 결정하는 연출 적용부를 포함할 수 있다. 또한, 상기 입력 분석부는, 상기 디지털 휴먼이 등장하는 콘텐츠의 스크립트를 포함하는 제2사용자 입력을 사용 자 단말을 통해 수신할 수 있다. 또한, 상기 입력 분석부는, 상기 제2사용자 입력에 대한 콘텐츠 분석을 포함하는 제2분석을 수행할 수 있다. 또한, 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치는, 적어도 하나의 참조 이미지에 기초하여 상기 디지털 휴먼과 연계된 컨셉을 도출하는 컨셉 도출부를 포함할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, 사용자와의 자연스러운 인터랙션을 위한 외형, 움직임, 감정 표현 등 의 구현 요소를 포함하는 디지털 휴먼 기반의 콘텐츠를 제공하는 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치 및 방법을 제공할 수 있다. 전술한 본원의 과제 해결 수단에 의하면, 사용자가 구현하고자 하는 다양한 컨셉과 외형 특성에 부합하는 개성 있는 디지털 휴먼 기반 콘텐츠를 제공할 수 있다.전술한 본원의 과제 해결 수단에 의하면, 사용자 입력에 대한 내용 분석에 기초하여 배경 등의 디지털 휴먼과 연계된 연출 요소를 맞춤형으로 변화시킬 수 있는 고도화된 디지털 휴먼 기반 콘텐츠를 제공할 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본원은 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치 및 방법에 관한 것이다. 도 1은 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치를 포함하는 콘텐츠 제 공 시스템의 개략적인 구성도이다. 도 1을 참조하면, 본원의 일 실시예에 따른 콘텐츠 제공 시스템은, 본원의 일 실시예에 따른 인공지능을 이 용한 디지털 휴먼 기반 콘텐츠 생성 장치(이하, '콘텐츠 생성 장치'라 한다.), 라이브러리 DB 및 사용자 단말을 포함할 수 있다. 콘텐츠 생성 장치, 라이브러리 DB 및 사용자 단말 상호간은 네트워크를 통해 통신할 수 있 다. 네트워크는 단말들 및 서버들과 같은 각각의 노드 상호간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트워크의 일 예에는, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, 5G 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인 터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), wifi 네트워크, 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등이 포함되나 이에 한정되지는 않는다. 사용자 단말은 예를 들면, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC등과 PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말기 같은 모든 종류의 무선 통신 장치일 수 있다. 또한, 도 1을 참조하면, 사용자 단말은 본원에서 개시하는 디지털 휴먼 기반의 콘텐츠가 재생되어 사용자와 디지털 휴먼 간의 인터랙션을 수행하도록 구비되는 키오스크, 디지털 사이니지 등의 대화형 단말을 포함할 수 있다. 본원의 실시예에 관한 설명에서, 라이브러리 DB는 콘텐츠 생성 장치가 소정의 디지털 휴먼의 움직임, 감정 표현 등의 특성을 결정하기 위하여 참조할 수 있는 미리 구축된 학습 데이터베이스일 수 있다. 예를 들어, 라이브러리 DB는 모션 라이브러리(L1), 감정 라이브러리(L2) 등을 포함하며, 예를 들어, 모션 라이브러리에 는 영상 매체로부터 추출된 사람의 표정, 동작 등의 다양한 모션 데이터가 저장되고, 감정 라이브러리에는 다양 한 모션 데이터와 후술하는 NLP 엔진에 기반하여 추출된 감정 데이터가 매칭되어 저장될 수 있으나, 이에만 한 정되는 것은 아니다. 콘텐츠 생성 장치는 디지털 휴먼의 외형 정보, 움직임 정보, 감정 표현 정보, 연출 요소 정보 등 디지털 휴먼이 등장하도록 구현되는 콘텐츠와 관련한 각종 요소를 결정하고, 결정된 요소에 기반하여 디지털 휴먼을 시 각화하도록 동작할 수 있다. 이하에서는, 콘텐츠 생성 장치에 의해 구현되는 디지털 휴먼과 연계된 시각적 요소 및 디지털 휴먼이 등장하는 콘텐츠의 연출적 요소를 콘텐츠 생성 장치가 결정하는 구체적인 실시예에 관하여 설명하도록 한다. 도 2는 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치의 개략적인 구성도이다. 도 2를 참조하면, 콘텐츠 생성 장치는 컨셉 도출부, 외형 결정부, 모션 결정부, 감정 결정 부, 시각화부 및 입력 분석부를 포함할 수 있다. 컨셉 도출부는 적어도 하나의 참조 이미지에 기초하여 구현되는 디지털 휴먼과 연계된 컨셉(Concept)을 도 출할 수 있다. 도 3a 및 도 3b는 참조 이미지에 기초하여 디지털 휴먼과 연계된 컨셉을 도출하는 프로세스를 예시적으로 설명 하기 위한 도면이다. 구체적으로, 도 3a를 참조하면, 컨셉 도출부는 인공지능 기반의 3차원 얼굴 생성기(3D face generator)를 탑재하여, 입력된 2차원의 얼굴 이미지(도 3a의 2D Image)를 3차원으로 구현되는 디지털 휴먼의 얼굴 특성에 입 력된 이미지에 나타낸 얼굴 특성이 반영되도록 2차원 이미지를 3차원 화상 형태의 3차원 컨셉(3D Concept)으로 변환할 수 있다. 예를 들어, 2차원 이미지는 사용자의 얼굴을 촬영한 Selfie 등을 의미할 수 있다. 또한, 도 3b를 참조하면, 컨셉 도출부는 복수의 2차원 또는 3차원 이미지를 병합(Merge)하여 복수의 이미 지에 반영된 얼굴 특성의 조합을 통해 본 발명에 의해 구현될 디지털 휴먼의 얼굴 특성을 결정하도록 하는 병합 기반의 캐릭터 컨셉 빌더(Concept Builder)를 포함할 수 있다. 다른 예로, 컨셉 도출부는 미리 생성된 복 수의 컨셉 자체를 병합(Merge)하여 복수의 컨셉 각각에 반영된 얼굴 특성이 구현될 디지털 휴먼의 얼굴 특성에 함께 반영되도록 하는 컨셉 병합기(미도시)를 포함할 수 있다. 또한, 본원의 일 실시예에 따르면, 컨셉 도출부는 사용자 단말로 인가된 컨셉 선택 입력에 기초하여 미리 설정된 복수의 컨셉 카테고리 중 어느 하나의 카테고리를 결정할 수 있다. 도 4는 미리 설정된 복수의 컨셉 카테고리를 설명하기 위한 도면이다. 도 4의 (a) 내지 (c)를 참조하면, 컨셉 도출부는 다양한 개성의 개별 외형 특성에 대응되는 복수의 컨셉 카테고리를 포함할 수 있다. 본원의 일 실시예에 따르면, 컨셉 카테고리는 Manga 컨셉, Pixar 컨셉, Cute 컨셉 및 Realistic 컨셉을 포함하는 4가지 분류를 포함할 수 있으나, 이에만 한정되는 것은 아니다. 이와 관련하여, 종래의 디지털 휴먼 기반 콘텐츠가 주로 사람의 외형과 유사한 사실적인 디지털 휴먼을 설계하 는 데에만 집중하던 것과 달리, 본원에서 개시하는 콘텐츠 생성 장치는 사용자가 구현하고자 하는 디지털 휴먼의 외형 특성을 미리 설정된 복수의 컨셉에 기반하여 커스터마이징 할 수 있으므로, 자유롭고 다채로운 표현을 수행하는 디지털 휴먼을 생성할 수 있을 뿐만 아니라, 특정 브랜드의 운용 주체가 해당 브랜드의 스타일에 부합하는 디지털 휴먼을 생성하여 마케팅 등에 활용할 수 있도록 하는 확장성을 부여할 수 있다. 또한, 사실적으로 설계되는 디지털 휴먼의 구현 및 해당 디지털 휴먼이 등장하는 콘텐츠의 재생 과정에서 매우 높은 연산량과 리소스 요구량이 필요한 것과 달리, 본원에서 개시하는 디지털 휴먼 기반의 콘텐츠는 사용자 단 말 등의 자원 등을 고려하여 낮은 리소스로도 구현이 가능한 디지털 휴먼 기반의 콘텐츠를 제공할 수 있는 이점이 있다. 또한, 컨셉 도출부는 결정된 카테고리에 기초하여 디지털 휴먼이 착용하는 의상을 포함하는 외형 특성을 결정할 수 있다. 예를 들어, 컨셉 도출부는 결정된 카테고리에 기초하여, 디지털 휴먼이 착용하는 의상의 유형, 형상, 색상, 질감 등을 결정할 수 있다. 외형 결정부는 사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보를 결정할 수 있다. 구체적으 로, 외형 결정부는 앞서 결정된 디지털 휴먼의 컨셉에 기초하여, 결정된 컨셉에 부합하는 외형 특성을 갖 는 외형 정보를 결정할 수 있다. 예를 들어, 외형 정보는 구현될 디지털 휴먼의 체격, 신체 부위 각각의 형상 등을 포함할 수 있으나, 이에만 한정되는 것은 아니다. 모션 결정부는 미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 외형 결정부에 의해 결정된 외형 정보에 대응하는 디지털 휴먼의 움직임 정보를 결정할 수 있다. 본원의 일 실시예에 따르면, 모션 결정부는 라이브러리 DB 내에 기 구축된 모션 라이브러리(L1)를 참 조하여 구현하려는 디지털 휴먼의 외형 정보에 대응하는 움직임 정보를 결정할 수 있다. 참고로, 본원의 구현예 에 따라 모션 결정부는 'Motion Automation Studio'로 달리 지칭될 수 있다. 도 5는 모션 구현 모델을 학습시키기 위한 학습용 데이터를 구축하는 프로세스를 설명하기 위한 도면이다. 도 5를 참조하면, 구현하려는 디지털 휴먼의 움직임 정보를 결정하도록 학습되는 모션 구현 모델의 구축을 위하 여 라이브러리 DB에 저장되는 학습용 데이터는 예시적으로, 소정의 영상 콘텐츠에 등장하는 인물의 적어도 하나의 신체 부위의 위치 변화를 해당 영상 콘텐츠로부터 인공지능 기반의 움직임 추출 모델(도 5의 Motion A I)에 기초하여 추출하는 방식으로 구축(수집)될 수 있다. 또한, 도 5를 참조하면, 모션 라이브러리(L1)에는 영상 콘텐츠에서 등장하는 인물의 전신 움직임의 시간의 흐름 에 따른 변화에 대한 정보(도 5의 a) 및 해당 인물의 표정의 시간의 흐름에 따른 변화에 대한 정보(도 5의 b)가 구분되어 저장될 수 있다. 이와 관련하여, 후술하는 감정 라이브러리(L2)는 모션 라이브러리(L1)의 영상 콘텐츠 내 인물의 표정의 변화에 대한 정보(도 5의 b)를 활용하여 구축되는 것일 수 있다. 이렇듯, 본원에서 개시하는 콘텐츠 생성 장치는 모션 라이브러리(L1)를 참조하여 디지털 휴먼의 움직임 정 보를 생성함으로써, 시간의 흐름에 따른 표정 및 신체 움직임을 타임라인 단위로 분석하여 디지털 휴먼의 움직 임 정보를 미세하게 조정할 수 있어 사용자와 상호 작용하는 디지털 휴먼의 움직임이 자연스러울 뿐만 아니라, 특정 감정에 대응하는 표정과 동작이 자연스럽게 제공될 수 있다. 또한, 본원의 일 실시예에 따르면, 모션 라이브러리(L1)는 FBX, OBJ 등의 사용자 제작 데이터를 포함하여, 사용 자가 요구하는 동작 특성을 갖는 애니메이션의 구현이 손쉽게 이루어질 수 있도록 구축될 수 있다. 감정 결정부는 미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 앞서 결정된 디지털 휴먼의 외형 정보에 대응하는 디지털 휴먼의 감정 표현 정보를 결정할 수 있다. 도 6은 감정 구현 모델을 학습시키기 위한 학습용 데이터를 구축하는 프로세스를 설명하기 위한 도면이다. 도 6을 참조하면, 구현하려는 디지털 휴먼의 감정 정보를 결정하도록 학습되는 감정 구현 모델의 구축을 위하여 라이브러리 DB에 저장되는 학습용 데이터는 예시적으로, 소정의 감정을 표현하는 텍스트 데이터와 해당 감 정에 대응하는 디지털 휴먼의 표정 및 포즈를 매칭하여 저장하는 방식으로 구축(수집)될 수 있다. 또한, 본원의 일 실시예에 따르면, 감정 라이브러리(L2) 내의 특정 감정에 대응하는 표정 및 포즈는 전술한 모션 라이브러리 (L1) 내에 저장된 모션 데이터일 수 있다. 구체적으로, 도 6을 참조하면, 콘텐츠 생성 장치는 수집된 언어 데이터(텍스트 데이터, Language Date)로 부터 감정 정보(Emotion Information)를 도출하는 자연어 처리 엔진(NLP Engine)을 보유하고, 자연어 처리 엔진에 의해 도출된 감정 정보를 모션 데이터(Motion Dataset)와 매칭하여 저장하는 방식으로 감정 구현 모델의 학 습을 위한 학습용 데이터를 감정 라이브러리(L2)에 수집할 수 있다. 또한, 도 6을 참조하면, 감정 정보와 매칭 되는 모션 데이터는 예시적으로 모션 결정부의 모션 구현 모델에 의해 생성되는 데이터이거나 모션 라이브 러리(L1) 내에 저장된 모션 데이터를 포함할 수 있다. 시각화부는 결정된 외형 정보, 움직임 정보 및 감정 표현 정보에 기초하여 디지털 휴먼을 사용자 단말 을 통해 시각화(재생)할 수 있다. 예를 들어, 시각화부는 앞서 결정된 외형 정보, 움직임 정보, 감정 표현 정보 등을 반영하는 디지털 휴먼이 등장하는 콘텐츠(예를 들면, 콘텐츠 비디오 등)가 사용자 단말을 통해 재생되도록 할 수 있다. 이하에서는, 사용자가 인가한 사용자 입력에 기초하여 디지털 휴먼 기반의 콘텐츠의 구체적인 내용이 결정 또는 변경되는 실시예에 관해 구체적으로 설명한다. 참고로, 이하에서 설명하는 본원의 실시예에 관한 설명에서 '제1 사용자 입력'은 사용자 단말을 통해 입력되는 디지털 휴먼과의 상호 작용을 위한 사용자 입력을 의미하고, '제2사용자 입력'은 콘텐츠 제작자 등에 해당하는 사용자가 디지털 휴먼이 등장하는 콘텐츠의 내용을 구체적으 로 결정하기 위하여 인가하는 사용자 입력을 의미하는 것일 수 있다. 구체적으로, 입력 분석부는 디지털 휴먼과의 상호 작용을 위한 제1사용자 입력을 사용자 단말을 통해 수신할 수 있다. 예를 들면, 제1사용자 입력은 사용자 단말에 인가되는 음성 입력, 영상 입력, 텍스트 입 력 등의 형식을 포함할 수 있다. 또한, 입력 분석부는 인가된 제1사용자 입력에 대응하는 감정 분석 및 상황 분석을 포함하는 제1분석을 수 행할 수 있다. 예를 들어, 입력 분석부는 제1사용자 입력에 포함된 발화의 의도와 해당 발화에 담긴 사용 자의 감정을 도출하기 위한 제1분석을 수행할 수 있다. 다른 예로, 입력 분석부는 제1사용자 입력이 사용 자의 얼굴 영역을 촬영한 영상 입력인 경우, 해당 영상 입력에 반영된 사용자의 얼굴 표정, 움직임 등에 대한 분석을 통해 해당 사용자의 감정과 해당 사용자가 표현하고자 하는 의사 표시의 내용을 식별하기 위한 제1분석 을 수행할 수 있다. 이러한 제1분석이 완결되고 나면, 모션 결정부는 제1분석의 결과에 기초하여 결정되는 제1사용자 입력에 대한 디지털 휴먼의 응답에 부합하는 움직임 정보를 결정할 수 있다. 마찬가지로, 감정 결정부는 제1분석 의 결과에 기초하여 결정되는 제1사용자 입력에 대한 디지털 휴먼의 응답에 부합하는 감정 정보를 결정할 수 있 다. 달리 말해, 입력 분석부에 의해 도출된 사용자의 감정 분석 결과와 상황 분석 결과에 기초하여 콘텐츠 생성 장치는 디지털 휴먼의 제1사용자 입력에 대응하는 적절한 응답 패턴을 결정하고, 결정된 응답 패턴에 기초하여 디지털 휴먼의 움직임 정보 및 감정 정보를 결정함으로써, 사용자와 자연스럽게 인터랙션 할 수 있는 디지털 휴먼 기반의 콘텐츠를 제공할 수 있다. 또한, 입력 분석부는 디지털 휴먼이 등장하는 콘텐츠의 스크립트를 포함하는 제2사용자 입력을 사용자 단 말을 통해 수신할 수 있다. 또한, 입력 분석부는 제2사용자 입력에 대한 콘텐츠 분석을 포함하는 제2분석 을 수행할 수 있다. 구체적으로, 제2사용자 입력은 콘텐츠 제작자 등에 해당하는 사용자가 앞서 구축된 디지털 휴먼이 등장하는 제작 대상 콘텐츠의 구체적인 내용을 포함하는 것일 수 있다. 예를 들어, 제2사용자 입력은 디 지털 휴먼이 소정의 스크립트를 읽어주도록 생성되는 콘텐츠와 관련하여, 디지털 휴먼이 읽을 스크립트를 포함 할 수 있다. 보다 구체적으로, 이해를 돕기 위해 예시하면, 콘텐츠 생성 장치가 제2사용자 입력으로서 소정의 제품에 대한 사용 설명을 포함하는 설명서를 수신하면, 전술한 제2분석을 통해 수신한 제품 사용 설명서에 포함된 텍스 트 데이터 중에서 디지털 휴먼이 해당 콘텐츠 내에서 리딩할 내용이 결정되고, 결정된 내용에 기초하여 디지털 휴먼이 제품 사용 설명을 소개하는 영상 등의 제2사용자 입력에 기반한 콘텐츠가 생성될 수 있다. 다른 예로, 콘텐츠 생성 장치가 제2사용자 입력으로서 뉴스 기사를 수신하면, 전술한 제2분석을 통해 수신한 뉴스 기 사에 포함된 텍스트 데이터 중에서 디지털 휴먼이 해당 콘텐츠 내에서 리딩할 내용이 결정되고, 결정된 내용에 기초하여 디지털 휴먼이 해당 뉴스 기사를 소개하는 영상 등의 제2사용자 입력에 기반한 콘텐츠가 생성될 수 있 다. 한편, 본원의 일 실시예에 따른 콘텐츠 생성 장치는 제2사용자 입력에 기반한 콘텐츠와 관련하여, 디지털 휴먼이 리딩하도록 결정된 내용(스크립트)을 음성합성(Speech Synthesis) 기술 기반의 나레이션으로 변환하여 출력할 수 있다.또한, 제2분석 결과에 기반하여 움직임 정보, 감정 정보 등이 결정된다는 것은 구체적으로, 디지털 휴먼이 제2 분석 결과에 기초하여 파악되는 스크립트의 내용에 부합하는 움직임과 감정 표현을 할 수 있도록 하는 움직임 정보 및 감정 표현 정보가 결정될 수 있다는 것을 의미할 수 있다. 또한, 이와 관련하여, 입력 분석부는 수신된 제2사용자 입력을 문장 단위 또는 단락 단위로 분할하여 제2분석을 수행하고, 모션 결정부 및 감정 결정부는 분할된 문장 또는 단락마다의 제2분석 수행 결과에 기초하여 해당 문장 또는 단락의 내용에 대응 되는 움직임 정보 및 감정 표현 정보를 결정하도록 동작할 수 있다. 달리 말해, 제2사용자 입력에 대응하여 생성되는 디지털 휴먼 기반의 콘텐츠에 대한 디지털 휴먼의 움직임 정보 및 감정 표현 정보는 제2사용자 입력에 대한 제2분석의 결과에 기초하여 파악되는 스크립트의 내용을 디지털 휴 먼이 표현하도록 결정될 수 있다. 또한, 본원의 일 실시예에 따르면, 콘텐츠 생성 장치는 미리 학습된 인공지능 기반의 콘텐츠 연출 모델에 기초하여 디지털 휴먼과 연계된 연출 요소 정보를 결정하는 연출 적용부(미도시)를 포함할 수 있다. 본원의 일 실시예에 따르면, 디지털 휴먼과 연계된 연출 요소는 콘텐츠 내에서의 디지털 휴먼의 시각화 영역 외 의 배경 영역의 형상 정보, 디지털 휴먼 기반의 콘텐츠에 대한 플랫폼 정보 등을 포함할 수 있다. 또한, 플랫폼 정보는 예시적으로 디지털 휴먼이 등장하도록 생성되는 콘텐츠가 소정의 소셜 네트워크 서비스(SNS)에 기반하여 제공하는 서식(양식)에 부합하게 생성되도록 SNS 플랫폼의 유형별 서식 정보를 포함할 수 있다. 또 다른 예로, 연출 요소는 디지털 휴먼이 등장하는 이미지 필터 기반의 콘텐츠가 제공될 수 있도록 연출 요소 정보는 소정의 이미지 필터에 대한 형상 정보를 포함할 수 있다. 또한, 본원의 일 실시예에 따르면, 콘텐츠 생성 장치는 인가된 사용자 입력(전술한 제1사용자 입력, 제2사 용자 입력 등)으로부터 미리 설정된 유형의 내용, 맥락, 특정 문구, 조건 등 연출 요소에 영향을 미칠 수 있는 파라미터를 식별(추출)하고, 식별된 파라미터에 기초하여 디지털 휴먼의 감정, 움직임 등을 다양한 연출 요소에 기초하여 표현하는 콘텐츠를 생성할 수 있다. 예를 들어, 디지털 휴먼의 감정, 움직임 등을 표출하기 위한 구현 요소로서, 디지털 휴먼의 자세(포즈), 얼굴 표정, 입모양(립싱크) 등이 고려될 수 있으나, 디지털 휴먼의 감정, 움직임 등의 표출을 위한 연출 관련 요소는 전술한 예시로만 제한되지 않는다. 또한, 본원의 일 실시예에 따르면, 콘텐츠 생성 장치는 사용자 입력으로부터 식별(추출)된 파라미터(전술 한 내용, 맥락 등)에 기초하여 디지털 휴먼이 등장하는 콘텐츠 내의 배경 영역의 형상을 변화시키거나 식별(추 출)된 파라미터와 매칭되는 소정의 유형의 효과음 등 각종 특수 효과가 콘텐츠 내에 반영되도록 할 수 있다. 다른 예로, 콘텐츠 생성 장치는 사용자 입력으로부터 식별(추출)된 파라미터에 기초하여 디지털 휴먼의 표 출 방향(예를 들면, 디지털 휴먼을 바라보는 촬영 방향 등)을 변화시킴으로써 콘텐츠 내에서 디지털 휴먼이 등 장하는 구도를 적절히 변경할 수 있다. 또한, 이와 관련하여, 제2사용자 입력에 대응하여 생성되는 디지털 휴먼 기반의 콘텐츠에 대한 연출 요소 정보 는 제2사용자 입력에 대한 제2분석의 결과에 기초하여 파악되는 스크립트의 내용을 디지털 휴먼이 표현하도록 결정될 수 있다. 달리 말해, 연출 적용부(미도시)는 사용자 입력에 대한 분석(제2분석) 결과를 고려하여 해당 콘텐츠의 내용에 부합하는 연출 요소를 맞춤형으로 설정할 수 있다. 보다 구체적으로, 모션 결정부는 전술한 모션 구현 모델에 기초하여 콘텐츠 생성 장치로 인가된 제2 사용자 입력의 스크립트에 포함된 텍스트 데이터에 대응하는 움직임 정보를 변환할 수 있다(Convert Text to Motion). 본원의 일 실시예에 따르면, 모션 구현 모델, 감정 구현 모델, 콘텐츠 연출 모델 등 본원에서 디지털 휴먼 기반 의 콘텐츠를 생성하기 위한 인공지능 기반의 모델은, 딥 러닝, 지도 학습 기반의 머신 러닝 등 종래에 개발되었 거나 향후 개발 가능한 다양한 인공지능 학습 방법론에 기초하여 생성되는 것일 수 있다. 예를 들어, 모션 구현 모델, 감정 구현 모델 및 콘텐츠 연출 모델 중 적어도 하나는 라이브러리 DB를 참조하여 모션, 감정, 콘텐 츠 연출 등의 구현 요소를 생성하는 생성 모델 및 상기 생성 모델에 의해 구현된 요소의 위조 여부를 판별하는 판별 모델의 상호 대립 관계에 의해 학습을 수행하는 생성적 대립 신경망(Generative Adversarial Network, GAN)에 기반하여 학습될 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 7은 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법에 대한 동작 흐름도이 다. 도 7에 도시된 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은 앞서 설명된 콘텐츠 생성 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 콘텐츠 생성 장치에 대하여 설명된 내 용은 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 7을 참조하면, 단계 S11에서 컨셉 도출부는 적어도 하나의 참조 이미지에 기초하여 디지털 휴먼과 연계 된 컨셉을 도출할 수 있다. 다음으로, 단계 S12에서 외형 결정부는 사용자와 상호 작용하도록 구현되는 디지털 휴먼의 외형 정보를 결 정할 수 있다. 다음으로, 단계 S13에서 모션 결정부는 미리 학습된 인공지능 기반의 모션 구현 모델에 기초하여 결정된 외형 정보에 대응하는 디지털 휴먼의 움직임 정보를 결정할 수 있다. 다음으로, 단계 S14에서 감정 결정부는 미리 학습된 인공지능 기반의 감정 구현 모델에 기초하여 결정된 외형 정보에 대응하는 디지털 휴먼의 감정 표현 정보를 결정할 수 있다. 다음으로, 단계 S15에서 시각화부는 결정된 디지털 휴먼의 외형 정보, 움직임 정보 및 감정 표현 정보에 기초하여 디지털 휴먼을 시각화할 수 있다. 상술한 설명에서, 단계 S11 내지 S15는 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단 계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은 다양한 컴퓨터 수단을 통하 여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매 체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업 자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스 크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭 티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메 모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령 의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법은 기록 매체에 저장되는 컴퓨터에 의해 실 행되는 컴퓨터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다."}
{"patent_id": "10-2020-0185251", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해 석되어야 한다."}
{"patent_id": "10-2020-0185251", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치를 포함하는 콘텐츠 제 공 시스템의 개략적인 구성도이다. 도 2는 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 장치의 개략적인 구성도이다. 도 3a 및 도 3b는 참조 이미지에 기초하여 디지털 휴먼과 연계된 컨셉을 도출하는 프로세스를 예시적으로 설명 하기 위한 도면이다. 도 4는 미리 설정된 복수의 컨셉 카테고리를 설명하기 위한 도면이다. 도 5는 모션 구현 모델을 학습시키기 위한 학습용 데이터를 구축하는 프로세스를 설명하기 위한 도면이다. 도 6은 감정 구현 모델을 학습시키기 위한 학습용 데이터를 구축하는 프로세스를 설명하기 위한 도면이다. 도 7은 본원의 일 실시예에 따른 인공지능을 이용한 디지털 휴먼 기반 콘텐츠 생성 방법에 대한 동작 흐름도이 다."}
