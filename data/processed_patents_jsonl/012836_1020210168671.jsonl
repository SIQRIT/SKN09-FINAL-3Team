{"patent_id": "10-2021-0168671", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0081013", "출원번호": "10-2021-0168671", "발명의 명칭": "딥러닝 기반 사람 인식 방법 및 비대면 교육 관리 방법", "출원인": "주식회사 마블러스", "발명자": "이충건"}}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "모바일 환경을 위한 딥러닝 모듈의 구동 방법에 있어서,외부 전자장치로부터 이미지 데이터를 수신하는 단계;수신한 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계;얼굴 데이터가 검출되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계; 및검출된 얼굴 데이터 또는 사람 데이터를 바탕으로 비대면 화상 정보를 관리하는 단계;를 포함하는 딥러닝 모듈의 구동 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 검출된 얼굴 데이터 및 사람 데이터를 바탕으로 감성 지표를 도출하는 단계;를 더 포함하는 딥러닝 모듈의구동 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 비대면 화상 정보를 관리하는 단계는 상기 도출된 감성 지표를 바탕으로 관리하는 단계인 것을 특징으로하는 딥러닝 모듈의 구동 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,온라인 상황에서 수집한 데이터를 전이학습하여 딥러닝 모델을 피드백 업데이트 하는 단계를 더 포함하는 것을특징으로 하는 딥러닝 모듈의 구동 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "외부 전자장치로부터 이미지 데이터를 수신하는 단계;수신한 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계;얼굴 데이터가 검출되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계; 및검출된 얼굴 데이터 또는 사람 데이터를 실시간으로 트랙킹하며 비대면 교육 화상 정보를 관리하는 단계;를 포함하는 교육 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서,상기 비대면 교육 화상 정보를 관리하는 단계는,상기 검출된 얼굴 데이터 또는 사람 데이터가 기설정된 시간 이상으로 검출 상태가 변화하는 경우 자리 비움으로 인지하고 경고 알람을 송신하는 단계를 포함하는 것을 특징으로 하는 교육 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5 항에 있어서,공개특허 10-2023-0081013-3-상기 검출된 얼굴 데이터 및 사람 데이터를 바탕으로 감성 지표를 도출하는 단계;를 더 포함하는 교육 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 감성 지표는 감정 지표와 집중 지표를 포함하고,상기 감정 지표는 얼굴 데이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 긍정, 부정, 중립 중 어느하나의 감정 유형 또는 즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함 중 어느 하나의 감정 유형을 확률값으로 도출되고,상기 집중 지표는 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 rPPG를 통해 도출하고 보통, 집중,몰입 단계별로 집중 유형을 확률값으로 도출하는 것을 특징으로 하는 교육 방법."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "전자 장치에 있어서,메모리, 송수신기 및 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는, 제5 항 내지 제8 항 중 어느 한 항에 따른 교육 방법을 수행하도록 구성된,교육 장치."}
{"patent_id": "10-2021-0168671", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제5 항 내지 제8 항 중 어느 한 항에 따른 교육 방법을 전자 장치를 통해 수행하도록 구성되며, 컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 모바일 환경을 위한 딥러닝 모듈의 구동 방법으로, 외부 전자장치로부터 이미지 데이터를 수신하는 단 계, 수신한 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계, 얼굴 데이터가 검출되지 않는 경우 2차 적으로 사람 데이터를 검출하는 단계 및 검출된 얼굴 데이터 또는 사람 데이터를 바탕으로 비대면 화상 정보를 관리하는 단계를 포함한다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 기반 사람 인식 방법 및 비대면 교육 관리 방법에 관한 것이다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "IT기술의 발달로 ZOOM, Google MEET, MS Teams, Skype 등 다양한 온라인 화상회의 플랫폼을 통해 비대면 회의 나 강의를 진행하는 곳이 많다. 코로나19의 팬데믹 이후 일 뿐만 아니라 일상생활 환경이 오프라인에서 온라인 으로 옮겨가면서 이러한 현상은 더욱 두드러진다. 온라인 화상회의 플랫폼이 언제 어디서나 편리하게 이용할 수 있는 수단이나 대면 상황이 아닌 점으로 인한 수강생의 집중력 저하 및 자리 이탈의 빈번함 등이 문제로 지적된 다. 특히, 초,중,고 학교 수업의 경우 지속적인 집중과 자리 유지가 요구됨에도 비대면 강의를 로그인 상태로만 둔뒤 자리를 이탈 하는 문제가 발생하여 사회적 문제화되고 있다. 기존에도 사물 인식 영상 기술이 있었으나, 기존 프로그램은 산업용으로 개발되어 빌딩 관리, 공장 관리, 또는 자율 주행 등 특별한 상황하에서 사용되는 프로그램으로 프로그램의 최적화 및 간이성을 크게 고려할 필요가 없 었으나, 이러한 교육용으로 개발되는 프로그램은 현장에서 사용할 수 있도록 간편하고 모바일 환경에서도 사용 가능한 딥러닝 모델을 구축하는 것이 요구되나 기존의 기술이 솔루션을 충분하게 제공하지 못한 문제점이 있었 다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 문제점을 해결하기 위하여 딥러닝 기반 사람 인식 방법 및 비대면 교육 관리 방법을 제공하고 자 한다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 컴퓨팅 장치에 의해 수행되는 모바일 환경을 위한 딥러닝 모듈의 구동 방법에 있어 서, 외부 전자장치로부터 이미지 데이터를 수신하는 단계; 수신한 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계; 얼굴 데이터가 검출되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계; 및 검출된 얼굴 데 이터 또는 사람 데이터를 바탕으로 비대면 화상 정보를 관리하는 단계;를 포함한다. 상기 검출된 얼굴 데이터 및 사람 데이터를 바탕으로 감성 지표를 도출할 수 있다. 상기 비대면 화상 정보를 관리하는 단계는 상기 도출된 감성 지표를 바탕으로 관리하는 단계일 수 있다. 온라인 상황에서 수집한 데이터를 전이학습하여 딥러닝 모델을 피드백 업데이트 하는 단계를 더 포함할 수 있다. 본 발명의 일 실시예에 따른 컴퓨팅 장치에 의해 수행되는 교육 방법에 있어서, 외부 전자장치로부터 이미지 데 이터를 수신하는 단계; 수신한 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계; 얼굴 데이터가 검출 되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계; 및 검출된 얼굴 데이터 또는 사람 데이터를 실시간으 로 트랙킹하며 비대면 교육 화상 정보를 관리하는 단계;를 포함한다. 상기 비대면 교육 화상 정보를 관리하는 단계는, 상기 검출된 얼굴 데이터 또는 사람 데이터가 기설정된 시간 이상으로 검출 상태가 변화하는 경우 자리 비움으 로 인지하고 경고 알람을 송신하는 단계를 포함할 수 있다. 상기 검출된 얼굴 데이터 및 사람 데이터를 바탕으로 감성 지표를 도출하는 단계;를 더 포함할 수 있다. 상기 감성 지표는 감정 지표와 집중 지표를 포함하고, 상기 감정 지표는 얼굴 데이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 긍정, 부정, 중립 중 어느 하나의 감정 유형 또는 즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함 중 어느 하나의 감정 유형을 확률값 으로 도출되고, 상기 집중 지표는 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 rPPG를 통해 도출하고 보통, 집중, 몰입 단계별로 집중 유형을 확률값으로 도출할 수 있다. 본 발명의 다른 실시예에 따른, 전술한 내용에 따른 교육 방법을 전자 장치를 통해 수행하도록 구성되는 컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램을 기술한다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 AI 딥러닝 기술 기반의 사물 인식 기술을 사용하여 일반 RGB 카메라로 촬영된 이미지 데이터로도 실 시간 얼굴 인식 및 사람 인식이 가능하다. 모바일 환경에서도 구동가능한 빠르고 정확한 알고리즘의 구현이 가능하다. 비대면 교육 환경에서 접목가능하도록 감성 지표를 추출하여, 자리비움 여부 확인부터 감정 지표 및 집중 지표 를 확인을 통한 능동적 교육 환경 관리가 가능하다. 전이학습을 통해 AI 딥러닝 모듈의 능동적 업데이트가 가능하다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터"}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "당해 기술분야에 있어서의 통상의 지식을 가진 자가 명확하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0168671", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면을 참고로 하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 도 1은 본 발명의 다양한 실시 예들에 따른 통신 시스템을 도시한다. 도 1을 참고하면, 본 발명의 다양한 실시 예들에 따른 통신 시스템은 전자 장치, 유/무선 통신 네트워크 , 서버를 포함한다. 서버는 이미지 데이터를 유/무선통신 네트워크를 통해 사용자의 전자 장치로 부터 획득하고, 감성상태 및 집중상태를 도출한 뒤 해당 상태에 대응하는 챗봇 메시지 UI를 유/무 선통신 네트워크를 통해 사용자의 전자 장치에 다시 송신한다. 전자 장치는, 유/무선 통신 네트워크를 통하여 서버의 요청에 따라 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 촬영하여 송신한다. 전자 장치는 퍼스널 컴퓨터, 셀룰러 폰, 스마트 폰 및 태블릿 컴퓨터 등과 같이, 정보를 저장할 수 있는 메모리, 정보의 송수신을 수행할 수 있는 송수신부, 정보의 연산을 수행할 수 있는 적어도 하나의 프로세서를 포함하는 전자 장치일 수 있다. 전자 장치 의 종류는 한정되지 않는다. 유/무선 통신 네트워크는, 전자 장치 및 서버가 서로 신호 및 데이터를 송수신할 수 있는 통신 경로를 제공한다. 유/무선 통신 네트워크는 특정한 통신 프로토콜에 따른 통신 방식에 한정되지 않으며, 구현 예에 따라 적절한 통신 방식이 사용될 수 있다. 예를 들어, 인터넷 프로토콜(IP) 기초의 시스템으로 구성 되는 경우 유/무선 통신 네트워크는 유무선 인터넷망으로 구현될 수 있으며, 전자 장치 및 서버(13 0)가 이동 통신 단말로서 구현되는 경우 유/무선 통신 네트워크는 셀룰러 네트워크 또는 WLAN(wireless local area network) 네트워크와 같은 무선망으로 구현될 수 있다. 서버는, 유/무선 통신 네트워크를 통하여 전자 장치로부터 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 수신한다. 서버는 정보를 저장할 수 있는 메모리, 정보의 송수신을 수행할 수 있는 송수신부, 정보의 연산을 수행할 수 있는 적어도 하나의 프로세서를 포함하는 전자 장치일 수 있다. 도 2는 본 발명의 다양한 실시 예들에 따른 전자 장치의 구성에 대한 블록도를 도시한다. 도 2를 참고하면, 본 발명의 다양한 실시 예들에 따른 전자 장치는 메모리, 송수신부 및 프로세 서를 포함한다. 메모리는 휘발성 메모리, 비휘발성 메모리 또는 휘발성 메모리와 비휘발성 메모리의 조합으로 구성될 수 있다. 그리고, 메모리는 프로세서의 요청에 따라 저장된 데이터를 제공할 수 있다. 송수신부는, 프로세서와 연결되고 신호를 전송 및/또는 수신한다. 송수신부의 전부 또는 일부는 송신기(transmitter), 수신기(receiver), 또는 송수신기(transceiver)로 지칭될 수 있다. 송수신기는 유 선 접속 시스템 및 무선 접속 시스템들인 IEEE(institute of electrical and electronics engineers) 802.xx 시스템, IEEE Wi-Fi 시스템, 3GPP(3rd generation partnership project) 시스템, 3GPP LTE(long term evolution) 시스템, 3GPP 5G NR(new radio) 시스템, 3GPP2 시스템, 블루투스(bluetooth) 등 다양한 무선 통신 규격 중 적어도 하나를 지원할 수 있다. 프로세서는, 본 발명에서 제안한 절차 및/또는 방법들을 구현하도록 구성될 수 있다. 프로세서는 생 체 정보의 기계 학습 분석에 기반하여 컨텐츠를 제공하기 위한 전자 장치의 전반적인 동작들을 제어한다. 예를 들어, 프로세서는 송수신부를 통해 정보 등을 전송 또는 수신한다. 또한, 프로세서는 메모 리에 데이터를 기록하고, 읽는다. 프로세서는 적어도 하나의 프로세서(processor)를 포함할 수 있다.도 3은 본 발명의 다양한 실시 예들에 따른 서버의 구성에 대한 블록도를 도시한다. 도 3을 참고하면, 본 발명의 다양한 실시 예들에 따른 서버는 메모리, 송수신부 및 프로세서 를 포함한다. 서버는 전자 장치의 일종일 수 있다. 서버는 유/무선 통신 네트워크를 통하여 전자 장치로부터 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 수신한다. 서버는 수신한 이미지 데이터를 수신한 이미지 데이터를 Mat 데이터로 변환하고, Mat 데이터를 감성인식 SDK 모듈에 입력하여 감성 및 집중상태 값으로 변환하고, 감성 상태 및 집중상태 값을 제어 로직에 삽입하고, 상기 제어 로직에 연동되는 챗봇 메시지 UI를 호출한다. 메모리는, 송수신부와 연결되고 통신을 통해 수신한 정보 등을 저장할 수 있다. 또한, 메모리는, 프로세서와 연결되고 프로세서의 동작을 위한 기본 프로그램, 응용 프로그램, 설정 정보, 프로세서의 연산에 의하여 생성된 정보 등의 데이터를 저장할 수 있다. 메모리는 휘발성 메모 리, 비휘발성 메모리 또는 휘발성 메모리와 비휘발성 메모리의 조합으로 구성될 수 있다. 그리고, 메모리 는 프로세서의 요청에 따라 저장된 데이터를 제공할 수 있다. 송수신부는, 프로세서와 연결되고 신호를 전송 및/또는 수신한다. 송수신부의 전부 또는 일부는 송신기(transmitter), 수신기(receiver), 또는 송수신기(transceiver)로 지칭될 수 있다. 송수신기는 유 선 접속 시스템 및 무선 접속 시스템들인 IEEE(institute of electrical and electronics engineers) 802.xx 시스템, IEEE Wi-Fi 시스템, 3GPP(3rd generation partnership project) 시스템, 3GPP LTE(long term evolution) 시스템, 3GPP 5G NR(new radio) 시스템, 3GPP2 시스템, 블루투스(bluetooth) 등 다양한 무선 통신 규격 중 적어도 하나를 지원할 수 있다. 프로세서는, 본 발명에서 제안한 절차 및/또는 방법들을 구현하도록 구성될 수 있다. 프로세서는 이 미지 데이터로부터 감성인식 SDK 모듈에 입력하여 감성 및 집중상태 값으로 변환하고, 감성상태 및 집중상태 값 을 입력값으로 하는 제어 로직 등 서버의 전반적인 동작들을 제어한다. 예를 들어, 프로세서는 송수 신부를 통해 정보 등을 전송 또는 수신한다. 또한, 프로세서는 메모리에 데이터를 기록하고, 읽 는다. 프로세서는 적어도 하나의 프로세서(processor)를 포함할 수 있다. 도 4는 본 발명의 다양한 실시 예들에 따른 교육 방법을 도시한다. 도 4를 참조하면, 도 4를 참조하면, 외부 전 자장치로부터 이미지 데이터를 수신하는 단계(S100), 수신 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하 는 단계(S200), 수신 이미지 데이터에서 얼굴 데이터가 검출되는지 여부를 판별하는 단계(S300), 얼굴 데이터가 검출되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계(S400), 딥러닝 모듈을 통해 감성 지표를 도출하는 단계(S500), 감성 지표를 바탕으로 비대면 화상 정보를 관리하는 단계(S600), 온라인 상황에서 수집한 데이터를 전이학습하여 딥러닝 모델을 피드백 업데이트 하는 단계(S700)를 포함한다. 단계 S100은, 사용자의 전자장치으로부터 촬영되거나 실시간으로 전송되는 얼굴을 포함하는 이미지 데이터 를 수신하는 단계이다. 본 발명에 따른 교육 방법 또는 딥러닝 모듈 구동 방법은 사용자의 얼굴 표정 또는 사람 형상이 포함된 이미지 데이터를 수신할 수 있다. 단계 S200은, 수신 이미지 데이터에서 1차적으로 얼굴 데이터를 검출하는 단계이다. 단계 S300은, 수신 이미지 데이터에서 얼굴 데이터가 검출되는지 여부를 판별하는 단계이다. 단계 S400은, 얼굴 데이터가 검출되지 않는 경우 2차적으로 사람 데이터를 검출하는 단계이다. 단계 S500은, 딥러닝 모듈을 통해 감성 지표를 도출하는 단계이다. 본 단계는, OpenCV 모듈 등을 통하여 수신한 이미지 데이터를 가공할 수 있다. 예를 들어, 본 단계에서는 이미지 데이터를 OpenCV 영상처리 라이브러리에서 다루는 데이터 형식 Mat(matrix) 형태로 변환할 수 있다. 도출한 얼굴 데이터는 Mat 데이터로 변환하고, 상기 Mat 데이터를 감성인식 SDK 모듈에 입력하여 감성 및 집중상태 값으로 변환할 수 있다. 상기 감정상태는 합산이 1.0이 되는 복수의 감정 상태값을 포함하고, 상기 집중상태는 합산이 1.0이 되는 복수의 집중 상태값을 포함할 수 있다. 구체적으로, 감정 상태값 및 집중 상태값은 확률값을 나타내며, 해당 상태들의 합은 각각 항상 1.0을 만족해야 한다. 7개 감정 및 3개 집중상태는 확률값으로 산출되며, 7개 감정의 합은 항상 1.0, 3개 집중상태의 합 또한 항상 1.0의 확률값을 가질 수 있다. 즉, 추가 변수가 없는 닫혀진 변수 공간을 형성할 수 있다. 단계 S600은, 감성 지표를 바탕으로 비대면 화상 정보를 관리하는 단계일 수 있다. 구체적으로는 도 6에서 후술 한다. 단계 S700은, 온라인 상황에서 수집한 데이터를 전이학습하여 딥러닝 모델을 피드백 업데이트 하는 단계를 포함 할 수 있다. 도 5는 본 발명의 다양한 실시 예들에 따른 기계 학습 모델의 구조를 도시한다. 본 발명의 다양한 실시 예들에 따른 생체 정보의 기계 학습 분석에 기반하여 컨텐츠를 제공하기 위한 다층 인공 신경망(multi-layer perceptron, MLP)의 구조를 도시한다. 심층 학습(deep learning)은 최근 기계 학습 분야에서 대두되고 있는 기술 중 하나로써, 복수 개의 은닉 계층 (hidden layer)과 이들에 포함되는 복수 개의 유닛(hidden unit)으로 구성되는 신경망(neural network)이다. 심층 학습 모델에 기본 특성(low level feature)들을 입력하는 경우, 이러한 기본 특성들이 복수 개의 은닉 계 층을 통과하면서 예측하고자 하는 문제를 보다 잘 설명할 수 있는 상위 레벨 특성(high level feature)로 변형 된다. 이러한 과정에서 전문가의 사전 지식 또는 직관이 요구되지 않기 때문에 특성 추출에서의 주관적 요인을 제거할 수 있으며, 보다 높은 일반화 능력을 갖는 모델을 개발할 수 있게 된다. 나아가, 심층 학습의 경우 특징 추출과 모델 구축이 하나의 세트로 구성되어 있기 때문에 기존의 기계학습 이론들 대비 보다 단순한 과정을 통 하여 최종 모델을 형성할 수 있다는 장점이 있다. 다층 인공 신경망(multi-layer perceptron, MLP)은 심층 학습에 기반하여 여러 개의 노드가 있는 인공 신경망 (artificial neural network, ANN)의 한 종류이다. 각 노드는 동물의 연결 패턴과 유사한 뉴런으로 비선형 활성 화 기능을 사용한다. 이 비선형 성질은 분리할 수 없는 데이터를 선형적으로 구분할 수 있게 한다. 도 5를 참고하면, 본 발명의 다양한 실시 예들에 따른 MLP 모델의 인공 신경망은 하나 이상의 입력 계층 (input layer), 복수 개의 은닉 계층(hidden layer), 하나 이상의 출력 계층(output layer)으 로 구성된다. 입력 계층의 노드에는 단위 시간별 적어도 하나의 초음파 이미지 내 각각의 픽셀의 RGB 값과 같은 입력 데 이터가 입력된다. 여기서, 사용자의 생체 정보, 예를 들어, 심전도 정보, 집중도 수치, 행복 감정 강도의 정보, 및, 조정 컨텐츠의 정보, 예를 들어, 컨텐츠 장르, 컨텐츠 주제, 컨텐츠 채널의 정보 각각은 심층 학습 모 델의 기본 특성(low level feature)에 해당한다. 은닉 계층의 노드에서는 입력된 인자들에 기초한 계산이 이루어진다. 은닉 계층은 사용자의 생체 정 보 및 조정 컨텐츠의 정보를 규합시켜 형성된 복수 개의 노드로 정의되는 유닛들이 저장된 계층이다. 은닉 계층은 도 5에 도시된 바와 같이 복수 개의 은닉 계층으로 구성될 수 있다. 예를 들어, 은닉 계층이 제1 은닉 계층 및 제2 은닉 계층으로 구성될 경우, 제1 은닉 계층(53 1)은 사용자의 생체 정보 및 조정 컨텐츠의 정보를 규합시켜 형성된 복수 개의 노드로 정의되는 제1 유닛 들이 저장되는 계층으로서, 제1 유닛은 사용자의 생체 정보 및 조정 컨텐츠의 정보의 상위 특징 에 해당된다. 제2 은닉 계층은 제1 은닉 계층의 제1 유닛들을 규합시켜 형성된 복수 개의 노드로 정 의되는 제2 유닛들이 저장되는 계층으로, 제2 유닛은 제1 유닛의 상위 특징에 해당된다. 출력 계층의 노드에서는 계산된 예측 결과를 나타낸다. 출력 계층에는 복수 개의 예측 결과 유닛들 이 구비될 수 있다. 구체적으로 복수 개의 예측 결과 유닛들은 참(true) 유닛 및 거짓(false) 유닛의 두 개의 유닛들로 구성될 수 있다. 구체적으로, 참 유닛은 조정 컨텐츠로의 컨텐츠 조정 후 사용자의 얼굴 데이 터 중 감성 상태값과 집중 상태 값이 임계 수치보다 높을 가능성이 높다는 의미를 지닌 예측 결과 유닛이고, 거 짓 유닛은 조정 컨텐츠로의 컨텐츠 조정 후 사용자의 얼굴 데이터 중 감성 상태값과 집중 상태 값이 임계 수치 보다 높을 가능성이 낮다는 의미를 지닌 예측 결과 유닛이다. 은닉 계층 중 마지막 계층인 제2 은닉 계층에 포함된 제2 유닛들과 예측 결과 유닛들 간의 연결에 대하여 각각의 가중치들이 부여되게 된다. 이러한 가중치에 기초하여 조정 컨텐츠로의 컨텐츠 조정 후 사용자의 얼굴 데이터 중 감성 상태값과 집중 상태값이 임계 수치 이상일지 여부를 예측하게 된다. MLP 모델의 인공 신경망은 학습 파라미터들을 조정하여 학습한다. 일 실시 예에 따라서, 학습 파라미터들 은 가중치 및 편차 중 적어도 하나를 포함한다. 학습 파라미터들은 기울기 하강법(gradient descent)이라는 최 적화 알고리즘을 통해 반복적으로 조정된다. 주어진 데이터 샘플로부터 예측 결과가 계산될 때마다(순방향 전파, forward propagation), 예측 오류를 측정하는 손실 함수를 통해 네트워크의 성능이 평가된다. 인공 신경 망의 각 학습 파라미터는 손실 함수의 값을 최소화하는 방향으로 조금식 증가하여 조정되는데, 이 과정은 역 전파(back-propagation)라고 한다.도 6에 따른 제어로직을 살핀다. 사용자의 전자장치에서 제공된 이미지 데이터로부터 학습자의 얼굴을 인식한 뒤, 제1 뎁스로 감성(감정) 판단, 얼굴 감지, 집중 상태 판단으로 구분할 수 있다. 전술한 감성인식 SDK 모듈에 서 도출된 학습자 감성상태 값 및 집중상태 값은 각기 감성 판단부, 집중 상태 판단부로 할당되고, 기초적인 얼 굴 감지 데이터는 얼굴 감지부에 대응될 수 있다. 감성 판단부에 대응되는 제2 뎁스 에서는 감성 판단부에 입력된 결과를 바탕으로 7가지 종류의 학습자 감성상태 중 어느하나를 도출할 수 있다. 이러한 감성 판단부는 딥러닝, 인공지능 등의 머신러닝(기계학습) 모듈을 통해 해당 결과를 도출할 수 있으며 특정 기술에 한정되지 않는다. 감성 판단부에 대응되는 제3 뎁스에서는 제2 뎁스에서 판단된 7가지 감성들에 대응되는 감성 상태값을 바탕으로 적절한 제어 로직을 산출할 수 있다. 예를 들어, 긍정적인 감성상태값이 장기간 동안 0단계(0.5 미만)에 해당하 는 경우, 사용자의 관리를 위한 돌봄 챗봇 UI를 구동하도록 제어 로직이 활성화 될 수 있다. 얼굴 감지부에 대응되는 제2 뎁스에서는 추출한 얼굴 감지 데이터를 바탕으로, 자세 판단부와 자리 이탈 여부 판단부를 통해 자세의 적정성에 대한 결과와 자리 이탈 여부에 대한 판단을 도출할 수 있다. 얼굴 감지부에 대응되는 제3 뎁스에서는 자세 판단부에서 도출된 자세 판단값이 적정 영역에서 도과할 경우, 챗 봇 UI를 제어하여 대사 및 얼굴 위치 재설정 화면으로 가이드 구현하도록 사용자의 전자장치로 출력 신호를 송 신할 수 있다. 자리 이탈 여부 판단부에 대응되는 제3 뎁스에서는 도출된 자리 이탈 여부 판단값이 적정 영역에서 도과할 경우, 챗봇 UI를 제어하여, 장시간 자리 비움인 경우 학습 중 음성 인터페이스를 통한 가이드 구현하도록 사용 자의 전자장치로 출력 신호를 송신할 수 있다. 집중 상태 판단부에 대응되는 제2 뎁스에서는, 3가지 구분에 대응하는 보통 상태, 집중 상태, 몰입 상태로 구분 할 수 있다. 집중 상태 판단부에 대응되는 제3 뎁스에서는, 보통 상태가 기설정된 시간보다 장기간 연장될 경우, 챗봇 UI를 제어하여, 보통이 장시간 이어지는 경우 돌봄이 필요한 상태라고 판단하고, 학습 중 음성 인터페이스를 통한 가 이드 구현 하도록 사용자의 전자장치로 출력 신호를 송신할 수 있다. 전술한 바와 같이, 제어 로직의 3단계 뎁스 구조에서 제어로직이 사용자의 감성/집중상태 값을 바탕으로 학습 효율이 저하되거나, 또는 자리를 비우거나, 변화가 필요한 시점이라고 판단되는 경우, 제어 로직의 제어에 따라 챗봇 메시지 UI가 호출되어 사용자의 전자 장치에서 제어되도록 신호가 송출될 수 있다. 도 7을 참조하면, 본 발명에 따른 ai 딥러닝 기술 기반의 표정 인식 딥러닝 모듈은 한국 유초등생(5~13세)의 이 미지 데이터셋을 입력 데이터로 하여 학습된 모델이다. 도 7은 본 발명에 따른 딥러닝 모듈의 아키텍쳐 및 학습 설명을 위한 예시적 도면으로 전이학습 모듈을 기술한다. 도 7을 참조하면, 5천개~ 1만개 내지 한국 유초등생의 이미지 데이터 셋을 기설정된 해상도 및 크기로 전처리 가공을 수행할 수 있다. 이후, 전처리 가공된 이미지 데이터셋을 제1 컨벌루션 네트워크에 삽입하고, 이때 제1 컨벌루션 네트워크의 채 널 수는 n by n 으로 결정될 수 있으며, 예를 들어, n은 예시적으로 24일 수 있다. 이후, 제1 컨벌루션을 맥스 풀링(max-pooling)하여,오버 피팅(overfitting)을 방지하고, 데이터 셋중의 최대값 을 뽑아내어 네트워크의 크기를 감소시킬 수 있다. 예를 들어, 맥스 풀링된 이후의 네트워크는 n/2 by n/2 로 결정될 수 있으며, 예시적으로 12 by 12 네트워크 일 수 있으나 이에 한정되는 것은 아니다. 이후 맥스 풀링된 데이터를 제2 컨벌루션 네트워크에 삽입할 수 있다. 이때, 제2 컨벌루션 네트워크의 채널수는 m by m으로 결정될 수 있으며, 예를 들어, m은 예시적으로 8일 수 있으나 이에 한정되는 것은 아니다. 제2 컨벌루션을 재차 맥스 풀링할 수 있다. 이후 최종 네트워크를 정류한 선형 유닛(Rectified Linear Unit;ReLu)를 Active Function으로 하여 딥러닝 학 습을 위한 뉴럴 네트워크의 전단에 삽입할 수 이다. 이러한 학습을 통해 딥러닝은 입력된 이미지가 복수개의 감 성상태 또는 복수개의 집중상태 중 어느 하나의 결과값과 대응되는지를 도출하도록 학습될 수 있다. 본 발명은 이러한 한국 유초등생의 이미지를 전이학습 아키텍쳐를 통해 딥러닝 학습을 시킴으로써 한국인 학생 들의 감성 상태, 학습 상태, 집중 상태에 대한 정량적 지표를 도출할 수 있다.도 8을 참조하면, 본 발명에 따른 감성인식 도출 모델은 다음의 세가지 세부 모델로 구성될 수 있다. 제1 감정 인식 모델은 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수 행하고 3가지 감정 유형(긍정, 부정, 중립) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감정지수가 도출될 수 있다. 제2 감정 인식 모델은 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수 행하고 7가지 감정 유형(즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감정지수가 도출될 수 있다. 제1 감정 인식 모델과 제2 감정 인식 모델은 택일 적으로 적용되거나 양 모델이 상호보완적으로 적용되어 보다 정밀한 감정지수의 산출이 가능할 수 있다. 집중도 인식 모델은, 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 도출할 수 있다. 도출된 심박변이 도 데이터에 rPPG(원격 광혈류측정) 기술을 적용함으로써 심박수와 심박변이도를 측정하고 분석할 수 있다. 집 중도 인식 모델은 심박변이도의 분석 결과를 토대로 3단계 집중 상태(보통→집중 →몰입) 각각의 확률값이 산출 하여 이것을 집중 상태 지표를 도출할 수 있다. 집중 상태 지표를 바탕으로 집중 지수를 도출할 수 있다. 집중 상태 지표는 학습지수를 도출하기 위한 기초 수치로 활용될 수 있다. 최종적으로 감성인식 도출 모델은 전술한 제1 감정 인식 모델, 제2 감정 인식 모델, 집중도 인식 모델에서 도출 된 정량적 지표 값을 바탕으로 학습지수를 도출할 수 있다. 학습지수는 사용자의 학습 상태를 정량적으로 파악 하기 위한 지표를 의미하며, 학생 데이터 및 학습 상황에서 발생하는 데이터, 예를 들어, 학습 시간, 질문 수, 자리이탈 상태 등을 추가적으로 활용하여 보다 정밀한 산출이 가능할 수 있다. 도 9는 본 발명에 따른 감정 지표 및 집중 지표 획득에 대한 순서도를 도시한 도면이다. 도 9를 참조하면, 본 발명에 따른 교육 방법은, 외부 전자장치로부터 이미지 데이터를 수신하는 단계(S110), 수 신한 이미지 데이터에서 얼굴 데이터를 검출하는 단계(S210), 상기 얼굴 데이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 감정 지표를 획득하는 단계(S310) 및 상기 얼굴 데이터를 광혈류량 변화량 기반의 rPPG 모델에 입력하여 집중 지표를 획득하는 단계(S410) 및 집중 지표와 상기 감정 지표를 바탕으로 학습 지수를 산 출하는 단계(S510)을 포함할 수 있다. 단계(S110)은, 사용자의 전자장치으로부터 촬영되거나 실시간으로 전송되는 얼굴을 포함하는 이미지 데이 터를 수신하는 단계이다. 단계(S210)은, 수신한 이미지 데이터에서 사용자의 얼굴 표정을 추출하여 얼굴표정 기반 감성 및 집중상태를 도 출하기 위한 데이터 셋을 추출하는 단계이다. 단계(S310)은, 제1 감정 인시 모듈 및 제2 감정 인식 모듈을 바탕으로 3가지 감정 유형(긍정, 부정, 중립) 각각 의 확률값을 산출하거나 7가지 감정 유형(즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함) 각각의 확률값을 산출하는 단계일 수 있다. 단계(S410)은, 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 도출할 수 있다. 도출된 심박변이도 데 이터에 rPPG(원격 광혈류측정) 기술을 적용함으로써 심박수와 심박변이도를 측정하고 분석하는 단계이다. 단계(S510)은, 집중 지표와 상기 감정 지표를 바탕으로 학습 지수를 산출하는 단계이다. 감성 지수와 학습 지표 등은 데이터베이스로 구축될 수 있다. 데이터베이스는 감성 지수와 학습 지표를 시계열 형태로 저장하여 종합적인 분석이 가능할 수 있다. 하드웨어를 이용하여 본 발명의 실시 예를 구현하는 경우에는, 본 발명을 수행하도록 구성된 ASICs(application specific integrated circuits) 또는 DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays) 등이 본 발명의 프로 세서에 구비될 수 있다. 한편, 상술한 방법은, 컴퓨터에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터 판독 가능 매체를 이용 하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 방법에서 사용된 데이 터의 구조는 컴퓨터 판독 가능한 저장 매체에 여러 수단을 통하여 기록될 수 있다. 본 발명의 다양한 방법들을 수행하기 위한 실행 가능한 컴퓨터 코드를 포함하는 저장 디바이스를 설명하기 위해 사용될 수 있는 프로그램 저장 디바이스들은, 반송파(carrier waves)나 신호들과 같이 일시적인 대상들은 포함하는 것으로 이해되지는 않 아야 한다. 상기 컴퓨터 판독 가능한 저장 매체는 마그네틱 저장매체(예를 들면, 롬, 플로피 디스크, 하드 디스크 등), 광학적 판독 매체(예를 들면, 시디롬, DVD 등)와 같은 저장 매체를 포함한다. 이상에서 설명된 실시 예들은 본 발명의 구성요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성요소 또 는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성요소 또는 특징은 다른 구 성요소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명의 실시 예를 구성하는 것도 가능하다. 발명의 실시 예들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시 예의 일부 구성이나 특징은 다른 실시 예에 포함될 수 있고, 또는 다른 실시 예의 대응하는 구성 또 는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시 예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다. 본 발명이 본 발명의 기술적 사상 및 본질적인 특징을 벗어나지 않고 다른 형태로 구체화될 수 있음은 본 발명 이 속한 분야 통상의 기술자에게 명백할 것이다. 따라서, 상기 실시 예는 제한적인 것이 아니라 예시적인 모든 관점에서 고려되어야 한다. 본 발명의 권리범위는 첨부된 청구항의 합리적 해석 및 본 발명의 균등한 범위 내 가능한 모든 변화에 의하여 결정되어야 한다."}
{"patent_id": "10-2021-0168671", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 다양한 실시 예들에 따른 통신 시스템을 도시한다. 도 2는 본 발명의 다양한 실시 예들에 따른 감성 및 집중상태의 변화에 기반한 화상 전자 장치의 구성에 대한 블록도를 도시한다. 도 3은 본 발명의 다양한 실시 예들에 따른 서버의 구성에 대한 블록도를 도시한다. 도 4는 본 발명의 다양한 실시 예들에 따른 딥러닝 기반 사람 인식 방법 및 비대면 교육 관리 방법을 도시한다. 도 5는 본 발명의 다양한 실시 예들에 따른 기계 학습 모델의 구조를 도시한다. 도 6은 본 발명에 따른 인공지능 로직이 감성, 자세, 집중에 대한 결과값을 도출하는 일 예를 도시한다. 도 7은 본 발명에 따른 딥러닝 모듈의 아키텍쳐 및 학습에 대한 설명을 위한 예시적 도면이다. 도 8은 전이 학습을 바탕으로 한 감성인식 모델을 구성하는 세부 모델들의 프로세스를 구체적으로 나타낸 일 예 를 도시한다. 도 9는 본 발명에 따른 감정 지표 및 집중 지표 획득에 대한 순서도를 도시한 도면이다. 도 10은 본 발명에 따른 얼굴 및 사람 인식 모델을 데모 수행한 결과를 도시한다."}
