{"patent_id": "10-2021-7029446", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0126697", "출원번호": "10-2021-7029446", "발명의 명칭": "인공 지능에 기반한 애니메이션 이미지 구동 방법 및 장치", "출원인": "텐센트 테크놀로지", "발명자": "왕 성"}}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법으로서,구동 캐릭터(driving character)에 대응하는 표정 베이스(expression base) 및 피구동 캐릭터(drivencharacter)에 대응하는 표정 베이스를 획득하는 단계 ― 상기 구동 캐릭터는 대응하는 얼굴 맞춤화 베이스(facecustomization base)를 갖고, 상기 피구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않음 ―;상기 구동 캐릭터에 대응하는 표정 베이스 및 상기 피구동 캐릭터에 대응하는 표정 베이스에 따라, 상기 구동캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는단계; 및상기 구동 캐릭터에 대응하는 표정 파라미터 및 상기 매핑 관계에 따라, 상기 피구동 캐릭터를 구동하는 단계를 포함하는 애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖는 캐릭터이고, 상기 피구동 캐릭터는 상기 제1 캐릭터 및 상기 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖지 않는 다른 캐릭터이며; 상기 구동 캐릭터에 대응하는 표정 베이스 및 상기 피구동 캐릭터에 대응하는 표정 베이스에 따라, 상기 구동캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는단계는:상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시(target mesh)를 결정하는단계 ― 상기 제1 표정 파라미터는 제1 차원수(quantity of dimensions)를 갖는 표정 베이스를 구동하는 데 사용되며, 상기 제1 표정 베이스의 차원수는 제1 차원수이고, 상기 타깃 메시는 상기 제1 표정 파라미터에 대응하는 표정을 짓는 제1 캐릭터를 식별하기 위한 타깃 정점 토폴로지(target vertex topology)를 가짐 ―;상기 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스를 획득하는 단계 ― 상기 타깃 표정베이스의 차원수는 제2 차원수이고, 상기 타깃 표정 베이스는 상기 제2 캐릭터에 대응하는 제2 표정 베이스에따라 결정됨 ―;상기 제1 캐릭터에 대응하는 제2 표정 파라미터를 상기 타깃 메시 및 상기 타깃 표정 베이스에 따라 결정하는단계 ― 상기 제2 표정 파라미터는 상기 타깃 메시에 대한 상기 제1 캐릭터의 표정의 변화도(change degree)를반영하는 데 사용됨 ―; 및상기 구동 캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를상기 제1 표정 파라미터 및 상기 제2 표정 파라미터에 따라 결정하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 타깃 정점 토폴로지는 상기 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고, 상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시를 결정하는 단계는:상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 초기 메시를 결정하는 단계 ― 상기초기 메시는 상기 제1 표정 베이스에 대응하는 제1 정점 토폴로지를 가짐 ―; 및공개특허 10-2021-0126697-3-상기 제1 정점 토폴로지와 상기 제2 정점 토폴로지 간의 대응관계에 따라 상기 타깃 메시를 생성하는 단계를 포함하고; 상기 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스를 획득하는 단계는:상기 제1 표정 베이스로부터, 상기 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고,상기 제2 표정 베이스로부터, 상기 제2 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하는 단계;상기 제1 캐릭터에 대응하는 무표정 메시 및 상기 제2 캐릭터에 대응하는 무표정 메시에 따라 조정 메시를 결정하는 단계 ― 상기 조정 메시는 상기 제2 정점 토폴로지를 갖고, 상기 제1 캐릭터가 무표정일 때 상기 제1 캐릭터를 식별하는 데 사용됨 ―; 및상기 조정 메시 및 상기 제2 표정 베이스에서의 메시 변형 관계에 따라 상기 타깃 표정 베이스를 생성하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 타깃 정점 토폴로지는 상기 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고,상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시를 결정하는 단계는:상기 제2 캐릭터에 대응하며, 상기 제2 캐릭터에 대응하는 제2 표정 베이스로부터 어떠한 표정도 갖지 않는 무표정 메시를 결정하는 단계;상기 제2 캐릭터에 대응하는 무표정 메시 및 상기 제1 캐릭터에 대응하는 제1 표정 베이스에 따라, 상기 1 캐릭터에 대응하고 상기 제2 정점 토폴로지를 갖는 조정 표정 베이스를 결정하는 단계 ― 상기 조정 표정 베이스의차원수는 상기 제1 차원수임 ―; 및상기 제1 표정 파라미터 및 상기 조정 표정 베이스에 따라 상기 타깃 메시를 결정하는 단계를 포함하며; 상기 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스를 획득하는 단계는:상기 조정 표정 베이스로부터, 상기 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하는단계; 및상기 제1 캐릭터에 대응하는 무표정 메시 및 상기 제2 표정 베이스에서의 메시 변형 관계에 따라 상기 타깃 표정 베이스를 생성하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항 또는 제4항에 있어서,상기 제1 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않고, 상기 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 가지며, 상기 제1 표정 파라미터는 랜덤 표정 파라미터이거나; 또는상기 제1 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 상기 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2021-0126697-4-제2항 내지 제4항 중 어느 한 항에 있어서,상기 구동 캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를상기 제1 표정 파라미터 및 상기 제2 표정 파라미터에 따라 결정하는 단계는:복수의 제1 표정 파라미터들 및 복수의 제2 표정 파라미터들을 쌍들로 획득하는 단계; 및상기 복수의 제1 표정 파라미터들에 의해 형성된 제1 행렬 및 상기 복수의 제2 표정 파라미터들에 의해 형성된제2 행렬에 따라 상기 매핑 관계를 결정하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 구동 캐릭터에 대응하는 표정 베이스 및 상기 피구동 캐릭터에 대응하는 표정 베이스에 따라, 상기 구동캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는단계는:상기 구동 캐릭터에 대응하는 표정 베이스 및 표정 파라미터에 따라 상기 구동 캐릭터에 대응하는 제1 점군 데이터(point cloud data)를 결정하는 단계;상기 피구동 캐릭터에 대응하는 표정 베이스 및 표정 파라미터에 따라, 상기 피구동 캐릭터에 대응하는 제2 점군 데이터를 결정하는 단계;상기 제1 점군 데이터 및 상기 제2 점군 데이터의 변환 파라미터를 결정하는 단계 ― 상기 변환 파라미터는 상기 제2 점군 데이터와 상기 제1 점군 데이터 간의 변환 관계를 식별하는 데 사용됨 ―; 및상기 구동 캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를상기 제1 점군 데이터, 상기 제2 점군 데이터 및 상기 변환 파라미터에 따라 결정하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "처리 디바이스에 의해 수행되는 애니메이션 캐릭터 구동 방법으로서,구동 캐릭터에 대응하는 변형 베이스(deformation base) 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하는단계 ― 상기 구동 캐릭터는 대응하는 구조 베이스를 갖고, 상기 피구동 캐릭터는 대응하는 구조 베이스를 갖지않으며, 상기 구조 베이스는 대응하는 캐릭터의 구조적 특징을 식별하는 데 사용되고, 상기 변형 베이스는 대응하는 캐릭터의 변형 특징을 식별하는 데 사용됨 ―;상기 구동 캐릭터에 대응하는 변형 베이스 및 상기 피구동 캐릭터에 대응하는 변형 베이스에 따라, 상기 구동캐릭터에 대응하는 변형 파라미터와 상기 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계를 결정하는단계; 및상기 구동 캐릭터에 대응하는 변형 파라미터 및 상기 매핑 관계에 따라, 상기 피구동 캐릭터를 구동하는 단계를 포함하는,애니메이션 캐릭터 구동 방법."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "획득 유닛, 결정 유닛 및 구동 유닛을 포함하는 애니메이션 캐릭터 구동 장치로서,상기 획득 유닛은 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스를 획득하도록구성되며, 상기 구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 상기 피구동 캐릭터는 대응하는 얼굴 맞춤공개특허 10-2021-0126697-5-화 베이스를 갖지 않으며;상기 결정 유닛은 상기 구동 캐릭터에 대응하는 표정 베이스 및 상기 피구동 캐릭터에 대응하는 표정 베이스에따라, 상기 구동 캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑관계를 결정하도록 구성되고;상기 구동 유닛은 상기 구동 캐릭터에 대응하는 표정 파라미터 및 상기 매핑 관계에 따라, 상기 피구동 캐릭터를 구동하도록 구성되는,애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖는 캐릭터이고, 상기 피구동 캐릭터는 상기 제1 캐릭터 및 상기 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖지 않는 다른 캐릭터이며;상기 결정 유닛은:상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시를 결정하고 ― 상기 제1표정 파라미터는 제1 양의 차원들을 갖는 표정 베이스를 구동하는 데 사용되며, 상기 제1 표정 베이스의 차원수는 제1 차원수이고, 상기 타깃 메시는 상기 제1 표정 파라미터에 대응하는 표정을 짓는 제1 캐릭터를 식별하기위한 타깃 정점 토폴로지를 가짐 ―;상기 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스를 획득하고 ― 상기 타깃 표정 베이스의 차원수는 제2 차원수이고, 상기 타깃 표정 베이스는 상기 제2 캐릭터에 대응하는 제2 표정 베이스에 따라결정됨 ―;상기 제1 캐릭터에 대응하는 제2 표정 파라미터를 상기 타깃 메시 및 상기 타깃 표정 베이스에 따라 결정하고― 상기 제2 표정 파라미터는 상기 타깃 메시에 대한 상기 제1 캐릭터의 표정의 변화도를 반영하는 데 사용됨―; 그리고상기 구동 캐릭터에 대응하는 표정 파라미터와 상기 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를상기 제1 표정 파라미터 및 상기 제2 표정 파라미터에 따라 결정하도록 구성되는,애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 타깃 정점 토폴로지는 상기 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고,상기 결정 유닛은 추가로:상기 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 초기 메시를 결정하고 ― 상기 초기메시는 상기 제1 표정 베이스에 대응하는 제1 정점 토폴로지를 가짐 ―, 상기 제1 정점 토폴로지와 상기 제2 정점 토폴로지 간의 대응관계에 따라 상기 타깃 메시를 생성하도록 구성되고; 그리고, 상기 결정 유닛은,상기 제1 표정 베이스로부터, 상기 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고,상기 제2 표정 베이스로부터, 상기 제2 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고,상기 제1 캐릭터에 대응하는 무표정 메시 및 상기 제2 캐릭터에 대응하는 무표정 메시에 따라 조정 메시를 결정하고 ― 상기 조정 메시는 상기 제2 정점 토폴로지를 갖고, 상기 제1 캐릭터가 무표정일 때 상기 제1 캐릭터를식별하는 데 사용됨 ―, 상기 조정 메시 및 상기 제2 표정 베이스에서의 메시 변형 관계에 따라 상기 타깃 표정 베이스를 생성하도록 구공개특허 10-2021-0126697-6-성되는,애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 타깃 정점 토폴로지는 상기 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고,상기 결정 유닛은 추가로:상기 제2 캐릭터에 대응하며 상기 제2 캐릭터에 대응하는 제2 표정 베이스로부터 어떠한 표정도 갖지 않는 무표정 메시를 결정하고,상기 제2 캐릭터에 대응하는 무표정 메시 및 상기 제1 캐릭터에 대응하는 제1 표정 베이스에 따라, 상기 1 캐릭터에 대응하고 상기 제2 정점 토폴로지를 갖는 조정 표정 베이스를 결정하고 ― 상기 조정 표정 베이스의 차원수는 상기 제1 차원수임 ―, 상기 제1 표정 파라미터 및 상기 조정 표정 베이스에 따라 상기 타깃 메시를 결정하도록 구성되며; 그리고, 상기 결정 유닛은, 상기 조정 표정 베이스로부터, 상기 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고, 상기 제1 캐릭터에 대응하는 무표정 메시 및 상기 제2 표정 베이스에서의 메시 변형 관계에 따라 상기 타깃 표정 베이스를 생성하도록 구성되는,애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "획득 유닛, 결정 유닛 및 구동 유닛을 포함하는 애니메이션 캐릭터 구동 장치로서,상기 획득 유닛은 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하도록구성되고, 상기 구동 캐릭터는 대응하는 구조 베이스를 갖고, 상기 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으며, 상기 구조 베이스는 대응하는 캐릭터의 구조적 특징을 식별하는 데 사용되고, 상기 변형 베이스는대응하는 캐릭터의 변형 특징을 식별하는 데 사용되며;상기 결정 유닛은 상기 구동 캐릭터에 대응하는 변형 베이스 및 상기 피구동 캐릭터에 대응하는 변형 베이스에따라, 상기 구동 캐릭터에 대응하는 변형 파라미터와 상기 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑관계를 결정하도록 구성되고; 그리고상기 구동 유닛은 상기 구동 캐릭터에 대응하는 변형 파라미터 및 상기 매핑 관계에 따라, 상기 피구동 캐릭터를 구동하도록 구성되는,애니메이션 캐릭터 구동 장치."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "프로세서 및 메모리를 포함하는 디바이스로서,상기 메모리는, 프로그램 코드를 저장하고 상기 프로그램 코드를 상기 프로세서에 전송하도록 구성되며; 상기 프로세서는 상기 프로그램 코드의 명령들에 따라 제1항 내지 제8항 중 어느 한 항에 따른 방법을 수행하도록 구성되는,디바이스."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "프로그램 코드를 저장하도록 구성된 컴퓨터 판독 가능 저장 매체로서,상기 프로그램 코드는 제1항 내지 제8항 중 어느 한 항에 따른 방법을 수행하는 데 사용되는, 컴퓨터 판독 가능공개특허 10-2021-0126697-7-저장 매체."}
{"patent_id": "10-2021-7029446", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "명령들을 포함하는 컴퓨터 프로그램 제품으로서,상기 명령들은 컴퓨터 상에서 실행될 때 상기 컴퓨터로 하여금, 제1항 내지 제8항 중 어느 한 항에 따른 방법을수행하게 하는, 컴퓨터 프로그램 제품."}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 지능에 기반한 애니메이션 이미지 구동 방법이 개시된다. 애니메이션 이미지가 대응하는 얼굴 핀치 베이스 를 갖고 다른 애니메이션 이미지는 대응하는 얼굴 핀치 베이스를 갖지 않을 때, 얼굴 핀치 베이스를 갖는 애니메 이션 이미지는 구동 파티 이미지로서 사용될 수 있고, 얼굴 핀치 베이스가 없는 애니메이션 이미지는 구동되는 (뒷면에 계속)"}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 \"인공 지능에 기반한 애니메이션 이미지 구동 방법 및 장치\"라는 명칭으로 중국 국가지식산권국에 2019년 8월 30일자 출원된 중국 특허출원 제201910816780.X호에 대한 우선권을 주장하며, 이 출원은 그 전체가 인용에 의해 본 명세서에 포함된다. 본 개시내용은 데이터 처리 분야에 관한 것으로, 특히 인공 지능(artificial intelligence) 기반(AI 기반) 애니 메이션 캐릭터 구동 기술에 관한 것이다."}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현재, 인간-컴퓨터 상호 작용들이 보편화되었다. 사용자는 애니메이션 캐릭터와 상호 작용할 수 있고, 상호 작 용 중에 사용자는 스피치의 임의의 세그먼트에 진입할 수 있다. 대응하게, 구동 네트워크가 스피치의 세그먼트 에 대응하는 입 형상들을 만들기 위해 애니메이션 캐릭터를 구동시킬 수 있다. 이 시나리오에서, 애니메이션 캐 릭터의 존재는 현실감을 크게 증강시키고, 표현성을 개선하며, 사용자에게 보다 몰입감 있는 경험을 가져올 수 있다. 애니메이션 캐릭터는 다수의 방식들로 획득될 수 있는데, 예를 들어 설계자에 의해 수동으로 설계될 수 있다. 애니메이션 캐릭터가 구동 네트워크에 의해 구동될 때, 애니메이션 캐릭터는 대개, 대응하는 표정 베이스 및 대 응하는 얼굴 맞춤화 베이스를 가질 필요가 있다. 표정 베이스는 상이한 표정들을 표현하는 변형 가능한 메시들 에 의해 형성되고, 각각의 변형 가능한 메시는 상이한 표정들 하에서 애니메이션 캐릭터의 3차원(3D: three- dimensional) 모델을 변경함으로써 형성된다. 얼굴 맞춤화 베이스는 상이한 얼굴 형상들을 표현하는 변형 가능 한 메시들에 의해 형성되고, 각각의 변형 가능한 메시는 평균 얼굴 형상에 비해 크게 변화하는 얼굴이며, 얼굴 맞춤화 베이스의 얼굴들은 애니메이션 캐릭터와 관련될 필요가 있다. 새롭게 생성된 일부 애니메이션 캐릭터들의 경우, 설계자는 애니메이션 캐릭터들의 3D 메시들을 수동으로 설계 할 수 있다. 표정 베이스는 눈들을 감거나 입을 닫는 것과 같은 제어를 수행하기 위한 엄격한 의미 정보를 갖기 때문에, 표정 베이스는 설계자에 의한 수동 모델링을 통해 편리하게 획득될 수 있다. 그러나 얼굴 맞춤화 베이 스는 주성분 분석(PCA: principal component analysis)을 통해 상당량의 얼굴 데이터를 분해함으로써 획득되고, 명확한 의미 정보를 갖지 않으며, 모델의 메시 정점들과 부정확한 연관성을 갖는다. 따라서 수동 설계를 통해 얼굴 맞춤화 베이스를 획득하는 것은 어렵다. 그 결과, 대응하는 얼굴 맞춤화 베이스는 단지 복잡한 데이터 처 리를 통해 결정되어야 하며, 새로운 애니메이션 캐릭터의 론치(launch)가 지연된다. 앞서 말한 기술적 문제들을 해결하기 위해, 새로운 애니메이션 캐릭터에 대응하는 얼굴 맞춤화 베이스를 획득하 기 위한 처리를 수행하지 않고 다른 애니메이션 캐릭터의 알려진 표정 파라미터를 사용함으로써 새로운 애니메 이션 캐릭터를 직접 구동함으로써, 새로운 애니메이션 캐릭터를 론치하는 속도를 높이도록, 본 개시내용에 따라 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법 및 장치가 제공된다. 본 개시내용의 실시예들에서 다음의 기술적 솔루션들이 개시된다. 제1 양상에 따르면, 본 개시내용의 일 실시예에 따라 처리 디바이스에 의해 수행되는 AI 기반 애니메이션 캐릭 터 구동 방법이 제공되며, 이 방법은: 구동 캐릭터(driving character)에 대응하는 표정 베이스 및 피구동 캐릭터(driven character)에 대응하는 표 정 베이스를 획득하는 단계 ― 구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 피구동 캐릭터는 대응하는얼굴 맞춤화 베이스를 갖지 않음 ―; 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 구동 캐릭터에 대응하 는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는 단계; 및 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하는 단계를 포함한다. 제2 양상에 따르면, 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 장치가 제공된다. 이 장 치는 획득 유닛, 결정 유닛 및 구동 유닛을 포함한다. 획득 유닛은 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스를 획득하도록 구성 되며, 구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 피구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖 지 않으며; 결정 유닛은 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 구동 캐릭 터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하도록 구성되 고; 그리고 구동 유닛은 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하도록 구성된 다. 제3 양상에 따르면, 본 개시내용의 일 실시예에 따라 처리 디바이스에 의해 수행되는 AI 기반 애니메이션 캐릭 터 구동 방법이 제공되며, 이 방법은: 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하는 단계 ― 구동 캐릭 터는 대응하는 구조 베이스를 갖고, 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으며, 구조 베이스는 대응 하는 캐릭터의 구조적 특징을 식별하는 데 사용되고, 변형 베이스는 대응하는 캐릭터의 변형 특징을 식별하는 데 사용됨 ―; 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스에 따라, 구동 캐릭터에 대응하 는 변형 파라미터와 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계를 결정하는 단계; 및 구동 캐릭터에 대응하는 변형 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하는 단계를 포함한다. 제4 양상에 따르면, 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 장치가 제공된다. 이 장 치는 획득 유닛, 결정 유닛 및 구동 유닛을 포함하며: 획득 유닛은 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하도록 구성 되고, 구동 캐릭터는 대응하는 구조 베이스를 갖고, 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으며, 구 조 베이스는 대응하는 캐릭터의 구조적 특징들을 식별하는 데 사용되고, 변형 베이스는 대응하는 캐릭터의 변형 특징들을 식별하는 데 사용되며; 결정 유닛은 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스에 따라, 구동 캐릭 터에 대응하는 변형 파라미터와 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계를 결정하도록 구성되 고; 그리고 구동 유닛은 구동 캐릭터에 대응하는 변형 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하도록 구성된 다. 제5 양상에 따르면, 본 개시내용의 일 실시예에 따른 디바이스가 제공되며, 디바이스는 프로세서 및 메모리를 포함하고, 메모리는, 프로그램 코드를 저장하고 프로그램 코드를 프로세서에 전송하도록 구성되며; 그리고 프로세서는 프로그램 코드의 명령들에 따라 제1 양상 또는 제3 양상에 따른 방법을 수행하도록 구성된다. 제6 양상에 따르면, 본 개시내용의 일 실시예에 따른 컴퓨터 판독 가능 저장 매체가 제공된다. 컴퓨터 판독 가 능 저장 매체는 프로그램 코드를 저장하도록 구성되며, 프로그램 코드는 제1 양상 또는 제3 양상에 따른 방법을 수행하는 데 사용된다. 제7 양상에 따르면, 본 개시내용의 일 실시예에 따라, 명령들을 포함하는 컴퓨터 프로그램 제품이 제공되며, 명 령들은 컴퓨터 상에서 실행될 때, 컴퓨터로 하여금 제1 양상 또는 제3 양상에 따른 방법을 수행하게 한다.하나의 애니메이션 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 다른 애니메이션 캐릭터는 대응하는 얼굴 맞 춤화 베이스를 갖지 않을 때, 얼굴 맞춤화 베이스를 갖는 애니메이션 캐릭터는 구동 캐릭터로서 사용될 수 있고, 얼굴 맞춤화 베이스를 갖지 않는 애니메이션 캐릭터는 피구동 캐릭터로서 사용될 수 있는 것이 앞서 말한 기술적 솔루션들로부터 학습될 수 있다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표 정 파라미터 간에 매핑 관계가 존재한다. 따라서 매핑 관계가 결정된 후에, 피구동 캐릭터가 얼굴 맞춤화 베이 스를 갖지 않더라도 구동 캐릭터에 대응하는 표정 파라미터를 사용함으로써, 피구동 캐릭터가 직접 구동될 수 있다. 피구동 캐릭터는 구동 캐릭터에 대응하는 실제 표정 파라미터에 따라, 실제 표정을 만들도록 구동될 수 있고, 실제 표정 파라미터는 상이한 차원들 하에서 구동 캐릭터의 표정 베이스와 실제 표정 간의 상관도를 반영 할 수 있는데, 즉 피구동 캐릭터에 대응하는 실제 표정 파라미터는 또한, 상이한 차원들 하에서 피구동 캐릭터 의 표정 베이스와 피구동 캐릭터의 실제 표정 간의 상관도를 반영할 수 있다. 따라서 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라 앞서 말한 표정 파라미터와 표정 베이스 간의 연 관 관계에 기초하여 표정 파라미터들 간의 매핑 관계가 결정될 수 있고, 피구동 캐릭터는 구동 캐릭터의 알려진 표정 파라미터 및 앞서 말한 매핑 관계를 사용함으로써 매핑 관계에 따라 구동되어, 피구동 캐릭터가 알려진 표 정 파라미터에 의해 식별된 실제 표정을 만든다. 따라서 새로운 애니메이션 캐릭터에 대응하는 얼굴 맞춤화 베 이스를 획득하기 위한 처리를 수행하지 않고 다른 애니메이션 캐릭터의 알려진 표정 파라미터를 사용함으로써 새로운 애니메이션 캐릭터가 직접 구동될 수 있으며, 이로써 새로운 애니메이션 캐릭터를 론치하는 속도를 높일 수 있다."}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다음은 첨부 도면들을 참조하여 본 개시내용의 실시예들을 설명한다. 관련 기술에서, 새롭게 생성된 일부 애니메이션 캐릭터들의 경우, 일반적으로 설계자는 애니메이션 캐릭터들의 3차원(3D) 메시들을 수동으로 설계할 수 있다. 예를 들어, 설계자는 수동 모델링을 통해 표정 베이스를 획득할 수 있다. 그러나 얼굴 맞춤화 베이스는 명확한 의미 정보를 갖지 않고, 모델의 메시 정점들과 부정확한 연관성을 가지며, 따라서 수동 설계를 통해 얼굴 맞춤화 베이스를 획득하는 것이 어렵다. 추가로, (예를 들어, 정점들의 양 및 정점들 간의 삼각형 토폴로지를 포함하는) 3D 메시들의 정점 토폴로지들 및 상이한 애니메이션 캐릭터들의 표정 베이스들의 차원수들이 상이할 수 있기 때문에, 특정 표정 베이스 아래 의 얼굴 맞춤화 베이스를 갖는 애니메이션 캐릭터의 표정 파라미터를 사용함으로써 다른 애니메이션 캐릭터의 표정 베이스를 직접 구동하는 것은 어렵다. 따라서 다른 애니메이션 캐릭터의 표정 베이스를 구동하고 새로운 애니메이션 캐릭터를 론치하기 위해, 단지 복잡한 데이터 처리를 통해서만 대응하는 얼굴 맞춤화 베이스가 결정 되어야 하며, 이로써 새로운 애니메이션 캐릭터의 론치를 지연시킨다. 이 때문에, 본 개시내용의 일 실시예에 따라, 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법이 제공된 다. 이 방법은 상이한 애니메이션 캐릭터들의 표정 베이스들 사이의 표정 파라미터 마이그레이션을 구현할 수 있다. 즉, 하나의 애니메이션 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 하나의 애니메이션 캐릭터는 대응 하는 얼굴 맞춤화 베이스를 갖지 않을 때, 얼굴 맞춤화 베이스를 갖는 애니메이션 캐릭터는 구동 캐릭터로서 사 용되고, 얼굴 맞춤화 베이스를 갖지 않는 애니메이션 캐릭터는 피구동 캐릭터로서 사용된다. 피구동 캐릭터에 대응하는 얼굴 맞춤화 베이스를 획득하기 위한 처리를 수행하지 않고 구동 캐릭터의 알려진 표정 파라미터 및 앞서 말한 매핑 관계를 사용함으로써, 피구동 캐릭터를 구동하기 위해, 구동 캐릭터에 대응하는 표정 파라미터 와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계가 결정된다. 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은 애니메이션 캐릭터를 확립하 는 능력을 갖는 처리 디바이스에 적용 가능하다. 처리 디바이스는 단말 디바이스일 수 있거나, 서버일 수 있다. 처리 디바이스는 컴퓨터 비전 기술을 구현하는 능력을 가질 수 있다. 본 개시내용의 실시예들에서는, 앞서 말한 컴퓨터 비전 기술을 구현함으로써, 처리 디바이스는 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라 표정 파라미터들 간의 매핑 관계를 결정하여, 매핑 관계에 따라 구동 캐릭터의 알 려진 표정 파라미터 및 앞서 말한 매핑 관계를 사용함으로써, 피구동 캐릭터를 구동할 수 있으며, 이로써 이를 테면, 새로운 애니메이션 캐릭터를 신속하게 론치하는 기능을 구현할 수 있다. 처리 디바이스가 단말 디바이스라면, 단말 디바이스는 스마트 단말, 컴퓨터, 개인용 디지털 보조기기(PDA: personal digital assistant), 태블릿 컴퓨터 등일 수 있다. 처리 디바이스가 서버라면, 서버는 독립적인 서버일 수 있거나, 클러스터 서버일 수 있다. 서버가 AI 기반 애니 메이션 캐릭터 구동 방법을 구현할 때, 서버는 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하 는 표정 베이스에 따라 표정 파라미터들 간의 매핑 관계를 결정하고, 구동 캐릭터의 알려진 표정 파라미터 및 앞서 말한 매핑 관계를 사용함으로써, 피구동 캐릭터를 구동하여 새로운 애니메이션 캐릭터를 획득하고, 새로운 애니메이션 캐릭터를 단말 디바이스 상에 디스플레이 및 론칭할 수 있다. 본 개시내용의 이 실시예에서, 애니메이션 캐릭터들은 2차원(2D: two-dimensional) 애니메이션 캐릭터 및 3D 애 니메이션 캐릭터를 포함할 수 있고; 애니메이션 캐릭터는 구체적으로 얼굴 구역 또는 머리 구역만을 제시할 수 있거나, 전신 구역을 제시할 수 있고; 추가로, 애니메이션 캐릭터는 구체적으로 만화 캐릭터로서 표현될 수 있 거나, 실제 인간 또는 동물에 기초하여 구성된 가상 캐릭터로서 표현될 수 있다. 애니메이션 캐릭터의 표현 형 태는 본 개시내용에서 제한되지 않는다. 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은, 수동으로 설계된 애니메이션 캐릭터들이 구동될 필요가 있는 애플리케이션 시나리오들, 이를테면 뉴스 방송, 날씨 예보, 게임 해설, 및 가상 게임 캐릭터가 존재하는 게임 장면에 적용 가능하며, 애니메이션 캐 릭터들이 개인 서비스들, 예를 들어 심리학자 또는 가상 어시스턴트와 같은 개인들에게 향하는 일대일 서비스들 을 착수하는 애플리케이션 시나리오들에도 또한 적용 가능하다. 애플리케이션 시나리오들에서는, 본 개시내용의 실시예들에 따라 제공되는 방법을 사용함으로써, 애니메이션 캐릭터에 대응하는 얼굴 맞춤화 베이스를 획득하기 위한 처리를 수행하지 않고 애니메이션 캐릭터가 구동될 수 있다. 본 개시내용의 기술적 솔루션들의 이해의 편의상, 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은 실제 애플리케이션 시나리오를 참조하여 아래에 설명된다. 본 개시내용의 일 실시예에 따른, AI 기반 애니메이션 캐릭터 구동 방법의 애플리케이션 시나리오의 개략도인 도 1이 참조된다. 이 애플리케이션 시나리오는 처리 디바이스가 단말 디바이스인 예를 사용함으로써 설명된다. 애플리케이션 시나리오는 단말 디바이스를 포함한다. 단말 디바이스는 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스를 획득할 수 있다. 구동 캐릭터는 표정 베이스 및 얼굴 맞춤 화 베이스를 갖고, 피구동 캐릭터는 표정 베이스를 갖지만, 얼굴 맞춤화 베이스를 갖지 않는다.표정 베이스는 애니메이션 캐릭터의 얼굴의 표정 특징을 식별하는 데 사용되며, 상이한 표정들을 표현하는 변형 가능한 메시들에 의해 형성되고, 여기서 각각의 변형 가능한 메시는 상이한 표정들 하에서 애니메이션 캐릭터의 3D 모델을 변경함으로써 형성된다. 얼굴 맞춤화 베이스는 애니메이션 캐릭터의 얼굴의 기본 특징을 식별하는 데 사용되며, 상이한 얼굴 형상들을 표현하는 변형 가능한 메시들에 의해 형성되고, 여기서 각각의 변형 가능한 메 시는 평균 얼굴 형상에 비해 크게 변화하는 얼굴이며, 얼굴 맞춤화 베이스의 얼굴들은 애니메이션 캐릭터와 관 련될 필요가 있다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간에 매핑 관계가 존재한다. 따라서 매핑 관계가 결정된다면, 피구동 캐릭터가 얼굴 맞춤화 베이스를 갖지 않더라도 구동 캐릭터에 대응하는 표정 파라미터를 사용함으로써, 피구동 캐릭터가 직접 구동될 수 있다. 표정 파라미터는 계수의 형태일 수 있는 데, 예를 들어 특정 양의 차원들을 갖는 벡터일 수 있고; 매핑 관계는 선형 매핑 관계일 수 있거나, 비선형 매 핑 관계일 수 있다. 이는 이 실시예에서 제한되지 않는다. 피구동 캐릭터는 구동 캐릭터에 대응하는 실제 표정 파라미터에 따라, 실제 표정을 만들도록 구동될 수 있고, 실제 표정 파라미터는 상이한 차원들 하에서 구동 캐릭터의 표정 베이스와 실제 표정 간의 상관도를 반영할 수 있으며, 유사하게, 피구동 캐릭터에 대응하는 실제 표정 파라미터는 또한, 상이한 차원들 하에서 피구동 캐릭터 의 표정 베이스와 피구동 캐릭터의 실제 표정 간의 상관도를 반영할 수 있다. 따라서 단말 디바이스는 구 동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 앞서 말한 표정 파라미터 와 표정 베이스 간의 연관 관계에 기초하여, 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하 는 표정 파라미터 간의 매핑 관계를 결정할 수 있다. 단말 디바이스는 매핑 관계 및 구동 캐릭터에 대응하는 표정 파라미터에 따라, 피구동 캐릭터에 대응하는 표정 파라미터를 계산할 수 있고, 피구동 캐릭터의 계산된 표정 파라미터는 피구동 캐릭터의 표정 베이스와 동 일한 양의 차원들을 갖기 때문에, 단말 디바이스는 피구동 캐릭터의 계산된 표정 파라미터를 사용함으로써, 표 정을 만들도록, 피구동 캐릭터를 구동할 수 있다. 따라서 단말 디바이스는 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 직접 구동할 수 있다. 도 1에 도시된 애플리케이션 시나리오는 단지 일례일 뿐이라고 이해되어야 한다. 실제 애플리케이션에서, 본 개 시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법은 다른 애플리케이션 시나리오에 추 가로 적용 가능하고, 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법이 적용 가 능한 애플리케이션 시나리오는 본 명세서에서 제한되지 않는다. 다음으로, 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법이 첨부 도면들을 참 조하여 상세히 설명된다. 도 2를 참조하면, 이 방법은 다음의 단계들(S201 내지 S203)을 포함한다. 단계(S201)에서, 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스가 획득된다. 일부 애니메이션 캐릭터들에서, 애니메이션 캐릭터들의 일부는, 론칭되었고 표정 베이스들 및 얼굴 맞춤화 베이 스들을 갖는 애니메이션 캐릭터들인 한편, 애니메이션 캐릭터들의 일부는, 표정 베이스들만을 갖지만 얼굴 맞춤 화 베이스들을 갖지 않는 새로운 애니메이션 캐릭터들이다. 이 실시예에서, 구동 캐릭터는 대응하는 얼굴 맞춤 화 베이스를 갖고, 피구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다. 본 개시내용의 실시예들에서의 애니메이션 캐릭터는 모델 라이브러리 내의 모델일 수 있거나, 모델 라이브러리 내의 모델들의 선형 결합을 통해 획득될 수 있다. 모델 라이브러리는 얼굴 3D 모핑 가능 모델(3DMM: 3D morphable model) 라이브러리 또는 다른 모델 라이브러리일 수 있으며, 이는 이 실시예에서 제한되지 않는다. 구동 캐릭터 또는 피구동 캐릭터와 같은 애니메이션 캐릭터는 3D 메시일 수 있다. 3DMM 라이브러리가 일례로 사용된다. 3DMM 라이브러리는 주성분 분석(PCA) 방법을 사용함으로써 대량의 고정밀 얼굴 데이터로부터 획득되고, 평균 얼굴에 대한 고차원 얼굴 형상 및 표현의 주요 변화를 설명하고, 텍스처 정 보를 설명할 수 있다. 일반적으로, 3DMM 라이브러리가 무표정 얼굴 형상을 설명할 때, 3DMM 라이브러리는 를 통해 획득될 수 있으며, 여기서 mu는 중립 표정을 갖는 평균 얼굴이고, Pfacei는 i번째 얼굴 형상의 주성분이며, αi는 i번째 얼굴 형상의 주성분의 가중치, 즉 얼굴 맞춤화 파라미터이다. 3DMM 라이브러리 내의 애니메이션 캐릭터에 대응하는 메시가 M으로 표현될 수 있다고 가정하면, 즉 3DMM 라이브 러리 내의 얼굴 형상, 표정 및 정점 간의 관계가 M으로 표현되고, M은 3D 행렬 [m×n×d]라고 가정하면, 여기서 차원들은 메시의 정점 좌표(m), 얼굴 형상 주성분(n) 및 표정 주성분(d)이다. 3DMM 라이브러리에서의 M의 차원 들의 분포 및 의미는 도 3에 도시된다. m은 3개의 좌표들(x, y, z)의 값들을 나타내기 때문에, 메시의 정점들의 양은 m/3이며, 이는 v로 표시된다. 애니메이션 캐릭터의 얼굴 형상 또는 표정이 결정된다면, M은 2D 행렬일 수 있다. 본 개시내용의 일 실시예에서, 3DMM 라이브러리 내의 텍스처 차원에 관계없이, 애니메이션 캐릭터의 구동이 F라 고 가정하면, F는 다음의 식을 사용함으로써 결정될 수 있다. , 여기서 M은 애니메이션 캐릭터의 메시이고, α는 얼굴 맞춤화 파라미터이며, β는 표정 파라미터이고; d는 표정 베이스의 표정 메시들의 양이고, n은 얼굴 맞춤화 베이스의 얼굴 맞춤화 메시들의 양이고, Mk,j,i는 i번째 표정 메시 및 j번째 얼굴 맞춤화 메시를 포함하는 k번째 메시이고, αj는 한 세트의 얼굴 맞춤화 파라미터들에서 j번 째 차원의 j번째 얼굴 형상 주성분을 나타내는 가중치이고, βi는 한 세트의 표정 파라미터들에서 i번째 차원의 i번째 표정 주성분을 나타내는 가중치이다. 얼굴 맞춤화 파라미터를 결정하는 프로세스는 얼굴 맞춤화 알고리즘이고, 표정 파라미터를 결정하는 프로세스는 표정 맞춤화 알고리즘이다. 얼굴 맞춤화 파라미터는 얼굴 맞춤화 베이스와 선형적으로 조합하여 대응하는 얼굴 형상을 획득하는 데 사용된다. 예를 들어, 50개의 (변형 가능한 메시들, 예를 들어 블렌드 쉐입(blendshape)들 인) 얼굴 맞춤화 메시들을 포함하는 얼굴 맞춤화 베이스가 존재하고, 얼굴 맞춤화 베이스에 대응하는 얼굴 맞춤 화 파라미터는 50개의 차원들을 갖는 벡터이고, 각각의 차원은 얼굴 맞춤화 파라미터에 대응하는 얼굴 형상과 하나의 얼굴 맞춤화 메시 간의 상관도를 식별할 수 있다. 얼굴 맞춤화 베이스에 포함된 얼굴 맞춤화 메시들은 각각 상이한 얼굴 형상을 표현하고, 각각의 얼굴 맞춤화 메시는 평균 얼굴에 대해 크게 변화하는 얼굴 외양이고, PCA가 대량의 얼굴들을 분해함으로써 획득된 상이한 차원들의 얼굴 형상 주성분이며, 동일한 얼굴 맞 춤화 베이스의 상이한 얼굴 맞춤화 메시들에 대응하는 정점 시퀀스 번호들은 일관되게 유지된다. 표정 파라미터는 표정 베이스와 선형적으로 조합하여 대응하는 표정을 획득하는 데 사용된다. 예를 들어, (50인 차원수와 동등한) 50개의 (변형 가능한 메시들, 예를 들어 블렌드 쉐입들인) 표정 메시들을 포함하는 표정 베이 스가 존재하고, 표정 베이스에 대응하는 표정 파라미터는 50개의 차원들을 갖는 벡터이고, 각각의 차원은 표정 파라미터에 대응하는 표정과 하나의 표정 메시 간의 상관도를 식별할 수 있다. 표정 베이스에 포함된 표정 메시 들은 각각 상이한 표정을 표현하고, 각각의 표정 메시는 상이한 표정들 하에서 동일한 3D 모델을 변화시킴으로 써 형성되며, 동일한 표정 베이스의 상이한 표정 메시들에 대응하는 정점 시퀀스 번호들은 일관되게 유지된다. 앞서 말한 변형 가능한 메시들의 경우, 단일 메시가 미리 정의된 형상에 따라 변형되어, 임의의 양의 메시들을 획득할 수 있다. 단계(S202)에서, 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계가 결정된다. 이 실시예의 방법에 기초하여, 피구동 캐릭터가 구동 캐릭터의 표정 파라미터를 사용함으로써 표정 파라미터들 간의 매핑 관계에 따라 구동되는 시나리오에서는, 얼굴 맞춤화 베이스가 변경되지 않고 유지된다. 즉, 얼굴 형 상이 고정되고, 표정 베이스만이 조정될 필요가 있다. 따라서 앞서 말한 식에 기초한 애니메이션 캐릭터의 구동은 다음 식으로서 추가로 표현될 수 있으며: , 여기서 Mk는 고정된 얼굴 형상을 갖는 애니메이션 캐릭터의 구동이고, Mk,i는 i번째 표정 메시이고, βi는 i번째 표정 메시에 대응하는 표정 파라미터이고, n은 표정 베이스의 표정 메시들의 양이다. 애니메이션 캐릭터(a)가 표정 베이스 및 얼굴 맞춤화 베이스를 갖고, 애니메이션 캐릭터(b)는 표정 베이스를 갖 지만 얼굴 맞춤화 베이스를 갖지 않으며, 애니메이션 캐릭터(a)의 일부 표정 파라미터들이 표정 맞춤화 알고리 즘을 사용함으로써 획득되었다면, 애니메이션 캐릭터(a)는 구동 캐릭터로서 사용될 수 있고, 애니메이션 캐릭터 (b)는 피구동 캐릭터로서 사용될 수 있다. 표정 파라미터와 표정 베이스의 선형 조합을 통해, 다음 식에 기 초하여 애니메이션 캐릭터(a)의 구동이 획득될 수 있다., 여기서 은 애니메이션 캐릭터(a)의 구동이고, 은 애니메이션 캐릭터(a)의 i번째 표정 메시이고, 은 애니메이션 캐릭터(a)의 i번째 표정 메시에 대응하는 표정 파라미터이고, na은 애니메이션 캐릭터(a)의 표정 베 이스에 포함된 표정 메시들의 양이다. 유사하게, 애니메이션 캐릭터(b)의 구동은 다음 식에 기초하여 획득된다. , 여기서 은 애니메이션 캐릭터(b)의 구동이고, 은 애니메이션 캐릭터(b)의 i번째 표정 메시이고, 은 애니메이션 캐릭터(b)의 i번째 표정 메시에 대응하는 표정 파라미터이고, nb은 애니메이션 캐릭터(b)의 표정 베 이스 내의 표정 메시들의 양이다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계가 존재하며, 따라서 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계는 함 수 f()를 사용함으로써 표현될 수 있다. 이런 식으로, 구동 캐릭터에 대응하는 표정 파라미터에 따라, 피구동 캐릭터에 대응하는 표정 파라미터를 계산하기 위한 다음 식은 다음과 같으며: , 여기서 βb는 피구동 캐릭터에 대응하는 표정 파라미터이고, βa은 구동 캐릭터에 대응하는 표정 파라미터이고, f()는 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 나 타낸다. 따라서 매핑 관계가 결정된다면, 피구동 캐릭터(애니메이션 캐릭터(b))는 구동 캐릭터(애니메이션 캐릭터(a))에 대응하는 표정 파라미터를 사용함으로써 식과 조합하여 식에 따라 직접 구동될 수 있다. 피구동 캐릭터는 구동 캐릭터에 대응하는 실제 표정 파라미터에 따라, 실제 표정을 만들도록 구동될 수 있고, 실제 표정 파라미터는 상이한 차원들 하에서 구동 캐릭터의 표정 베이스와 실제 표정 간의 상관도를 반영할 수 있는데, 즉 피구동 캐릭터에 대응하는 실제 표정 파라미터는 또한, 상이한 차원들 하에서 피구동 캐릭터의 표정 베이스와 피구동 캐릭터의 실제 표정 간의 상관도를 반영할 수 있다. 따라서 표정 파라미터들 간의 매핑 관계는 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 앞서 말한 표정 파라미 터와 표정 베이스 간의 연관 관계에 기초하여 결정될 수 있다. 단계(S203)에서, 피구동 캐릭터가 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라 구동된다. 앞서 말한 애니메이션 캐릭터(a) 및 애니메이션 캐릭터(b)가 여전히 일례로 사용된다. 매핑 관계가 선형 관계라 면, 앞서 말한 식은 다음 식으로 표현될 수 있으며: , 여기서 βb는 피구동 캐릭터에 대응하는 표정 파라미터이고, βa은 구동 캐릭터에 대응하는 표정 파라미터이고, f는 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 나타 낸다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계(f)가 결정된 후에, 애니메이션 캐릭터(b)의 표정 파라미터(βb)가 식에 따라 계산될 수 있다. βa의 차원수는 애니메이션 캐릭터(b)의 표정 베이스의 차원수와 상이하지만, 매핑 관계를 통해 획득된 βb의 차원수는 애니메이션 캐릭터 (b)의 표정 베이스의 차원수와 동일하므로, 애니메이션 캐릭터(b)가 표정을 만들도록 구동될 수 있다. 따라서 애니메이션 캐릭터(b)는 βb를 사용함으로써 식에 기초하여 구동된다. 즉, 애니메이션 캐릭터(b)의 얼굴 맞 춤화 베이스를 결정하지 않고 애니메이션 캐릭터(a)에 대응하는 표정 파라미터(βa)를 사용함으로써 애니메이션캐릭터(b)가 직접적으로 구동될 수 있다. 본 개시내용의 실시예들에 따라 제공되는 애니메이션 캐릭터 구동 방법은 상이한 애니메이션 캐릭터들에 대응하 는 (얼굴들에 대한) 표정 베이스들 사이의 마이그레이션에 적용 가능한 것으로 제한되지 않는다. 즉, 피구동 캐 릭터는 구동 캐릭터에 대응하는 표정 파라미터를 사용함으로써, 표정, 예를 들어 입 모양, 미소 또는 울음을 만 들도록 직접 구동된다. 본 개시내용의 실시예들에 따라 제공되는 애니메이션 캐릭터 구동 방법은 신체 액션과 같은 다른 오브젝트들을 마이그레이션하는 데 추가로 적용 가능하다. 즉, 피구동 캐릭터는 구동 캐릭터에 대응 하는 액션 파라미터를 사용함으로써 액션을 하도록 직접 구동된다. 하나의 애니메이션 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 다른 애니메이션 캐릭터는 대응하는 얼굴 맞 춤화 베이스를 갖지 않을 때, 얼굴 맞춤화 베이스를 갖는 애니메이션 캐릭터는 구동 캐릭터로서 사용될 수 있고, 얼굴 맞춤화 베이스를 갖지 않는 애니메이션 캐릭터는 피구동 캐릭터로서 사용될 수 있는 것이 앞서 말한 기술적 솔루션들로부터 학습될 수 있다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표 정 파라미터 간에 매핑 관계가 존재한다. 따라서 매핑 관계가 결정된 후에, 피구동 캐릭터가 얼굴 맞춤화 베이 스를 갖지 않더라도 구동 캐릭터에 대응하는 표정 파라미터를 사용함으로써, 피구동 캐릭터는 매핑 관계에 기초 하여 직접 구동될 수 있다. 피구동 캐릭터는 구동 캐릭터에 대응하는 실제 표정 파라미터에 따라, 실제 표정을 만들도록 구동될 수 있고, 실제 표정 파라미터는 상이한 차원들 하에서 구동 캐릭터의 표정 베이스와 실제 표정 간의 상관도를 반영할 수 있는데, 즉 피구동 캐릭터에 대응하는 실제 표정 파라미터는 또한, 상이한 차원들 하 에서 피구동 캐릭터의 표정 베이스와 피구동 캐릭터의 실제 표정 간의 상관도를 반영할 수 있다. 따라서 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라 앞서 말한 표정 파라미터와 표정 베이스 간의 연관 관계에 기초하여 표정 파라미터들 간의 매핑 관계가 결정될 수 있고, 피구동 캐릭터는 구동 캐릭터의 알려진 표정 파라미터 및 매핑 관계를 사용함으로써 매핑 관계에 따라 구동되어, 피구동 캐릭터 가 알려진 표정 파라미터에 의해 식별된 실제 표정을 만든다. 따라서 새로운 애니메이션 캐릭터에 대응하는 얼 굴 맞춤화 베이스를 획득하기 위한 처리를 수행하지 않고 다른 애니메이션 캐릭터의 알려진 표정 파라미터를 사 용함으로써 새로운 애니메이션 캐릭터가 직접 구동될 수 있으며, 이로써 새로운 애니메이션 캐릭터를 론치하는 속도를 높일 수 있다. 다음으로, 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 어떻게 결정하는지가 단계(S202)에 대해 상세히 설명된다. 다음의 실시예들에서는 선형 매핑 관계인 매핑 관계 가 주로 설명된다. 이 실시예에 따라, 매핑 관계를 결정하기 위한 두 가지 타입들의 방법들이 주로 제공된다. 제1 타입의 방법의 아이디어는 제1 표정 파라미터와 같은 알려진 표정 파라미터에 따라 제2 표정 파라미터와 같은 다른 표정 파라 미터를 결정하는 것이다. 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계가 존재하기 때문에, 매핑 관 계는 제1 표정 파라미터 및 제2 표정 파라미터에 따라 해결된다. 제2 타입의 방법의 아이디어는 분석적인 식에 기초하여 매핑 관계를 결정하는 것이다. 구동 캐릭터의 표정 베이스와 피구동 캐릭터의 표정 베이스 간의 변환 을 통해 동일한 점군 데이터(point cloud data)가 획득될 수 있기 때문에, 매핑 관계는 점군 데이터 간의 식 관 계에 기초하여 해결된다. 제1 타입의 방법의 흐름도에 대해, 도 4가 참조될 수 있다. 제1 표정 파라미터가 제1 양의 차원들을 갖는 표정 베이스를 구동하는 데 사용되며, 제1 표정 베이스의 차원수는 제1 차원수이다. 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시를 결정하기 위해, 제1 표정 파라미터를 사용함으로써 제1 캐릭 터에 대응하는 표정 베이스가 구동된다. 타깃 메시는 제1 표정 파라미터에 대응하는 표정을 짓는 제1 캐릭터를 식별하는 데 사용되는 타깃 정점 토폴로지를 갖는다. 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스가 획득되고, 타깃 표정 베이스의 차원수는 제2 차원수이고, 타깃 표정 베이스는 제2 캐릭터에 대응 하는 제2 표정 베이스에 따라 결정된다. 이어서, 제1 캐릭터에 대응하는 제2 표정 파라미터가 타깃 메시 및 타 깃 표정 베이스에 따라 결정되며, 제2 표정 파라미터는 타깃 메시에 대한 제1 캐릭터의 표정의 변화도를 반영하 는 데 사용된다. 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계는 제1 표정 파라미터 및 제2 표정 파라미터에 따라 결정된다. 타깃 메시를 결정하는 프로세스는 도 4의 좌 측 점선 박스에 주석이 달려 있고, 타깃 표정 베이스를 결정하는 프로세스는 도 4의 우측 점선 박스에 주석이 달려 있다. 제1 캐릭터 및 제2 캐릭터 중 하나는 대응하는 얼굴 맞춤화 베이스를 갖고, 다른 하나는 대응하는 얼굴 맞춤화 베이스를 갖지 않으며, 구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖는 캐릭터이고, 피구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖지 않는 캐릭터이다. 타깃 메시를 결정하는 다수의 상이한 방식들이 도 4에 대응하는 실시예에 기초하여 본 개시내용의 일 실시예에 따라 제공될 수 있다. 타깃 정점 토폴로지가 제2 표정 베이스에 대응하는 제2 정점 토폴로지라면, 제1 표정 베 이스가 제1 정점 토폴로지에 대응하기 때문에, 일반적으로, 제1 표정 파라미터를 사용하여 제1 표정 베이스를 구동함으로써 획득된 메시가 제1 정점 토폴로지를 갖는다고 이해될 수 있다. 제1 정점 토폴로지가 제2 정점 토 폴로지와 상이할 때, 제2 정점 토폴로지를 갖는 타깃 메시를 획득하기 위해, 제1 정점 토폴로지를 갖는 메시가 제2 정점 토폴로지를 갖는 타깃 메시로 변환될 필요가 있다. 본 개시내용의 실시예들에서 언급된 제1 정점 토폴로지, 제2 정점 토폴로지 및 타깃 정점 토폴로지와 같은 정점 토폴로지들은 애니메이션 캐릭터의 구동에 수반되는 메시들 내의 정점 토폴로지들의 일부일 수 있다. 예를 들어, 애니메이션 캐릭터는 머리이고, 애니메이션 캐릭터가 얼굴 표정을 만들기 위해 구동될 때, 수반되는 정점 토폴로지들은 머리를 표현하는 메시들의 얼굴에 대응하는 정점 토폴로지들이다. 따라서 가능한 구현에서, 도 5를 참조하면, 타깃 메시를 결정하는 방식은: 제1 캐릭터에 대응하는 제1 표정 파 라미터 및 제1 표정 베이스에 따라 초기 메시를 결정하는 것을 포함할 수 있다. 초기 메시가 제1 표정 베이스에 대응하는 제1 정점 토폴로지를 갖기 때문에, 타깃 메시는 제1 정점 토폴로지와 제2 정점 토폴로지 간의 대응관 계에 따라 생성될 수 있다. 이 경우, 타깃 표정 베이스를 결정하는 방식은: 제1 표정 베이스로부터, 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하는 것, 그리고 제2 표정 베이스로부터, 제2 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하는 것; 그리고 그 다음에, 제1 캐릭터에 대응하는 무표정 메시 및 제2 캐릭터에 대응하는 무표정 메시에 따라 조정 메시를 결정하는 것을 포함할 수 있으며, 조정 메시는 제2 정점 토폴로지를 갖고, 제1 캐릭터가 무표정일 때 제1 캐릭터를 식별하는 데 사용된다. 조정 메시 및 제2 표정 베이스에서의 메 시 변형 관계가 알려져 있기 때문에, 타깃 표정 베이스는 조정 메시 및 제2 표정 베이스에서의 메시 변형 관계 에 따라 생성될 수 있다. 메시 변형 관계는 표정 베이스에서의 무표정 메시에 대한 표정 메시의 변형 관계를 반 영할 수 있다. 정점 토폴로지는 애니메이션 캐릭터와 특정 관계를 갖지만, 정점 토폴로지와 애니메이션 캐릭터 간의 관계는 강 하지 않다. 즉, 애니메이션 캐릭터들이 동일하다면, 정점 토폴로지들은 확실히 동일하지만, 정점 토폴로지들이 동일하다면, 애니메이션 캐릭터들은 상이할 수 있다(예를 들어, 정점 토폴로지들이 변형됨). 조정 메시를 결정 하는 특정 방식은: 얼굴 맞춤화 알고리즘을 사용함으로써 제2 캐릭터에 대응하는 무표정 메시를 제1 캐릭터에 대응하는 무표정 메시에 부착하는 것을 포함할 수 있다. 예를 들어, 새로운 메시, 즉, 조정 메시를 획득하기 위 해, nricp 알고리즘이 얼굴 맞춤화 알고리즘으로서 사용될 수 있다. 확실히, nricp 알고리즘에 추가로 다른 얼 굴 맞춤화 알고리즘들이 사용될 수 있다. 이는 이 실시예에서 제한되지 않는다. 제1 정점 토폴로지가 제2 정점 토폴로지와 동일할 때, 제1 표정 파라미터를 사용하여 제1 표정 베이스를 구동함 으로써 획득된 메시는 타깃 메시이지만 초기 메시는 아니며, 초기 메시를 타깃 메시로 변환하는 앞서 말한 단계 는 필요하지 않다. 제1 캐릭터에 대응하는 제2 표정 파라미터와 제1 캐릭터에 대응하는 대량의 기존 표정 파라미터들이 동일한 데 이터 분포를 가질 때, 타깃 메시 및 타깃 표정 베이스는 도 5에 대응하는 실시예에 따라 제공된 방법을 사용함 으로써 보다 정확하게 결정될 수 있고, 매핑 관계가 더 정확하게 결정되어, 피구동 캐릭터가 매핑 관계 및 구동 캐릭터의 표정 파라미터에 따라 더 양호하게 구동될 수 있다. 그러나 일부 경우들에는, 도 5에 대응하는 실시예를 통해 결정된 제1 캐릭터에 대응하는 제2 표정 파라미터 및 제1 캐릭터에 대응하는 대량의 기존 표정 파라미터들이 동일한 데이터 분포를 갖지 않을 수 있다. 제1 캐릭터에 대응하는 제2 표정 파라미터와 제1 캐릭터에 대응하는 대량의 기존 표정 파라미터들 간의 데이터 분포 차이로 인해 매핑 관계가 정확하게 결정될 수 없고, 피구동 캐릭터가 구동 캐릭터에 대응하는 표정 파라미터를 사용함 으로써 구동될 때 야기되는 부적절한 표정으로 인해 정확한 표현이 더는 매핑될 수 없다는 문제 및 다른 문제들 을 피하기 위해, 본 개시내용의 일 실시예에 따라 타깃 메시를 결정하는 다른 방식이 추가로 제공된다. 도 6을 참조하면, 이러한 방식은: 제2 캐릭터에 대응하고, 제2 캐릭터에 대응하는 제2 표정 베이스로부터 어떠 한 표정도 갖지 않는 무표정 메시를 결정하는 것; 제2 캐릭터에 대응하는 무표정 메시 및 제1 캐릭터에 대응하 는 제1 표정 베이스에 따라, 제1 캐릭터에 대응하고 제2 정점 토폴로지를 갖는 조정 표정 베이스를 결정하는 것 ― 조정 표정 베이스의 차원수는 제1 차원수임 ―; 그리고 제1 표정 파라미터 및 조정 표정 베이스에 따라 타깃메시를 결정하기 위해, 제1 표정 파라미터를 사용함으로써 조정 표정 베이스를 구동하는 것을 포함한다. 이 경 우, 타깃 표정 베이스를 결정하는 방식은: 조정 표정 베이스로부터, 제1 캐릭터에 대응하고 표정이 없는 무표정 메시를 결정하는 것; 그리고 제1 캐릭터에 대응하는 무표정 메시 및 제2 표정 베이스의 메시 변형 관계에 따라 타깃 표정 베이스를 생성하는 것을 포함할 수 있다. 조정 표정 베이스를 결정하는 방식은: 새로운 표정 베이스, 예를 들어 조정 표정 베이스를 획득하기 위해, 얼굴 맞춤화 알고리즘을 사용함으로써 제2 캐릭터에 대응하는 무표정 메시를 제1 표정 베이스의 각각의 표정 메시에 부착하는 것을 포함할 수 있다. 이는 비강성(non-rigid) 등록 방법이고, 얼굴 맞춤화 알고리즘은 예를 들어, nricp 알고리즘일 수 있다. 도 6에 대응하는 실시예에 따라 제공되는 방법을 사용함으로써, 상이한 데이터 분포들로 인해 매핑 관계가 정확 하게 결정될 수 없는 문제가 방지되어, 매핑 품질이 상당히 개선된다. 도 7은 도 5 및 도 6에 대응하는 실시예들에 따라 제공되는 방법에 기초하여 애니메이션 캐릭터를 구동하는 효 과의 도면이다. 좌측의 도면은 도 5에 대응하는 실시예에 따라 제공된 방법에 기초하여 애니메이션 캐릭터를 구 동하는 효과를 도시하고, 우측의 도면은 도 6에 대응하는 실시예에 따라 제공된 방법에 기초하여 애니메이션 캐 릭터를 구동하는 효과를 도시한다. 우측의 효과의 도면에서, 입술들 주위에 더 적은 주름들이 있는데, 이들은 정상적인 스피킹의 입 모양과 더 유사다고 학습될 수 있다. 도 5 및 도 6에서 타깃 메시를 결정하는 방식들은 서로 교환될 수 있다. 유사하게, 도 5 및 도 6에서 타깃 표정 베이스를 결정하는 방식들은 또한 서로 교환될 수 있다. 제1 타입의 방법을 사용하는 경우, 제1 표정 파라미터 및 제2 표정 파라미터에 따라 구동 캐릭터에 대응하는 표 정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는 방식은: 다수의 제1 표정 파라미터들 및 다수의 제2 표정 파라미터들을 쌍들로 획득하는 것; 그리고 그 다음, 다수의 제1 표정 파라미터 들에 의해 형성된 제1 행렬 및 다수의 제2 표정 파라미터들에 의해 형성된 제2 행렬에 따라 매핑 관계를 결정하 는 것을 포함할 수 있다. 도 5 및 도 6에 대응하는 실시예들 중 임의의 실시예에서, 제1 캐릭터 및 제2 캐릭터 중 하나는 대응하는 얼굴 맞춤화 베이스를 갖고, 다른 하나는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다고 이해될 수 있다. 제1 캐릭터 가 대응하는 얼굴 맞춤화 베이스를 갖지 않는다면, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 제1 표 정 파라미터는 랜덤 표정 파라미터이다. 대안으로, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖는다면, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다. 도 5 및 도 6에 대응하는 실시예들에 기초하여, 제1 캐 릭터가 얼굴 맞춤화 베이스를 갖는 경우와 제2 캐릭터가 얼굴 맞춤화 베이스를 갖는 경우 간의 차이에 대해, 애 니메이션 캐릭터 구동 방법이 설명된다. 도 5에 대응하는 실시예에서, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖지 않고, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 제1 표정 파라미터는 랜덤 표정 파라미터라면, 구동 캐릭터는 제2 캐릭터이고, 피 구동 캐릭터는 제1 캐릭터이다. 구동 캐릭터는 애니메이션 캐릭터(a)이고, 피구동 캐릭터는 애니메이션 캐릭터 (b)이고, 제1 표정 파라미터는 랜덤 표정 파라미터(Bb)이고, 형상은 Fb이고, 정점 토폴로지는 제1 정점 토폴로 지(Tb)이고, 차원수는 Nb이다. 즉, 제1 표정 베이스(Eb)는 랜덤 표정 파라미터(Bb)를 사용함으로써 직접 구동될 수 있고, 제1 표정 베이스(Eb)의 형상은 Fb이고, 정점 토폴로지는 제1 정점 토폴로지(Tb)이고, 차원수는 제1 차 원수(Nb)이다. 애니메이션 캐릭터(a)의 제2 표정 베이스(Ea)의 형상은 Fa이고, 정점 토폴로지는 제2 정점 토폴 로지(Ta)이고, 차원수는 제2 차원수(Na)이고, 제2 표정 베이스(Ea)를 구동하기 위한 대량의 표정 파라미터들 (Ba)이 있었다. Fa는 Fb와 동일하지 않고, Ta는 Tb와 동일하지 않으며, Na는 Nb와 동일하지 않다. 이 경우, 도 5에 대응하는 실시예의 구현을 위해, 도 8이 참조될 수 있다. 제1 표정 베이스(Eb)는 랜덤 표정 파 라미터(Bb)를 사용함으로써 Meshmid로 표시된 초기 메시를 획득하도록 직접 구동되며, 여기서 초기 메시의 형상 은 Fb이고, 초기 메시의 정점 토폴로지는 제1 정점 토폴로지(Tb)이다. 이어서, Meshmid는 제1 정점 토폴로지와 제2 정점 토폴로지 간의 대응관계에 따라 타깃 메시로 변환되고, 타깃 메시의 정점 토폴로지는 제2 정점 토폴로 지(타깃 정점 토폴로지)(Ta)이며, 타깃 메시의 형상은 Fb로서 유지되고, 타깃 메시는 Meshc로 표시된다. 제2 표 정 베이스(Ea)로부터 결정된 무표정 메시가 nricp 알고리즘을 사용함으로써, 제1 표정 베이스(Eb)로부터 결정된 무표정 메시에 부착되고, 공간에서 가장 가까운 점을 검색하기 위한 방법을 사용함으로써 점 대응 관계가 획득 되어, Newb로 표기된 조정 메시를 획득하고, 여기서 조정 메시의 형상은 Fb이고, 조정 메시의 정점 토폴로지는 제2 정점 토폴로지(Ta)이다. 또한, 새로운 표정 베이스, 예를 들어 타깃 표정 베이스(Ea')가 획득될 필요가 있고, 타깃 표정 베이스(Ea')의 형상은 Fb이고, 타깃 표정 베이스의 정점 토폴로지는 제2 정점 토폴로지(Ta)이고, 타깃 표정 베이스의 차원수는 Na이다. 조정 메시(Newb) 및 중립 표정(무표정)에 대한 제2 표정 베이스(Ea)의 각 각의 차원에서의 표정 간의 메시 변형 관계가 알려져 있기 때문에, 타깃 표정 베이스(Ea')는 조정 메시(Newb) 및 제2 표정 베이스(Ea)에서의 메시 변형 관계에 따라 조정 메시(Newb)로부터의 변형을 통해 획득될 수 있다. 따라서 타깃 메시(Meshc)는 표정 맞춤화 알고리즘을 사용함으로써 그리고 타깃 표정 베이스(Ea')를 사용함으로 써 맞춤화될 수 있고, Na인 차원수를 갖는 제2 표정 파라미터(Ba')가 또한 획득된다. 따라서 Bb와 Ba' 간의 매핑 관계가 설정된다. 대량의 제1 표정 파라미터들(Bb)이 랜덤하게 생성될 때, 대량의 대응하는 제2 표정 파라미터들(Ba')이 생성될 수 있다. L개의 제1 표정 파라미터들(Bb) 및 L개의 제2 표정 파라 미터들(Ba')이 있고, L개의 제1 표정 파라미터들(Bb)은 제1 행렬을 형성하고, L개의 제2 표정 파라미터들(Ba') 은 제2 행렬을 형성한다고 가정되며, 여기서 제1 행렬 및 제2 행렬은 다음 식으로 도시된 바와 같이, 각각 BB 및 BA'로 표기된다: , 그리고 . 이 솔루션에서, 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계가 예를 들어, 식으로 표현된 선형 매핑 관계라면, 매핑 관계는 다음 식에 따라 결정될 수 있으며: , 여기서 f는 매핑 관계이고, BB는 제1 행렬이고, BA'는 제2 행렬이고, inv는 역행렬 연산이다. 매핑 관계(f)가 획득된 후, 대량의 Ba가 존재했기 때문에, Ba 및 Ba' 각각에 대응하는 표정 베이스의 차원수는 Na이고, 각각의 차원수의 의미 정보는 동일하며, Ba와 Ba'는 동등할 수 있다. 따라서 임의의 세트의 Ba에 대해, 애니메이션 캐릭터(b)를 구동하기 위해, 대응하는 Bb = f *Ba가 획득되어, 표정 파라미터들(Ba)에 따라 표정 파 라미터들(Bb)을 획득할 수 있다. 도 5에 대응하는 실시예에서, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다면, 구동 캐릭터는 제1 캐릭터이고, 피구동 캐릭터는 제2 캐릭터이다. 구동 캐릭터 는 애니메이션 캐릭터(a)이고, 피구동 캐릭터는 애니메이션 캐릭터(b)이고, 제1 표정 파라미터는 표정 파라미터 (Bb)이고, 형상은 Fa이고, 정점 토폴로지는 제1 정점 토폴로지(Ta)이고, 차원수는 Na이다. 즉, 제1 표정 베이스 (Eb)는 표정 파라미터(Ba)를 사용함으로써 직접 구동될 수 있고, 제1 표정 베이스(Ea)의 형상은 Fa이고, 정점 토폴로지는 제1 정점 토폴로지(Ta)이고, 차원수는 제1 차원수(Na)이다. 애니메이션 캐릭터(b)의 제2 표정 베이 스(Eb)의 형상은 Fb이고, 정점 토폴로지는 제2 정점 토폴로지(Tb)이고, 차원수는 제2 차원수(Nb)이고, 제1 표정 베이스(Ea)를 구동하기 위한 대량의 표정 파라미터들(Ba)이 있었다. Fa는 Fb와 동일하지 않고, Ta는 Tb와 동일 하지 않으며, Na는 Nb와 동일하지 않다. 이 경우, 도 5에 대응하는 실시예의 구현을 위해, 도 9가 참조될 수 있다. 제1 표정 베이스(Ea)는 표정 파라미 터(Ba)를 사용함으로써 Meshmid로 표시된 초기 메시를 획득하도록 직접 구동되며, 여기서 초기 메시의 형상은 Fa 이고, 초기 메시의 정점 토폴로지는 제1 정점 토폴로지(Ta)이다. 이어서, Meshmid는 제1 정점 토폴로지와 제2 정 점 토폴로지 간의 대응관계에 따라 타깃 메시로 변환되고, 타깃 메시의 정점 토폴로지는 제2 정점 토폴로지(타 깃 정점 토폴로지)(Tb)이며, 타깃 메시의 형상은 Fa로서 유지되고, 타깃 메시는 Meshc로 표시된다. 제2 표정 베 이스(Eb)로부터 결정된 무표정 메시가 nricp 알고리즘을 사용함으로써, 제1 표정 베이스(Ea)로부터 결정된 무표 정 메시에 부착되고, 공간에서 가장 가까운 점을 검색하기 위한 방법을 사용함으로써 점 대응 관계가 획득되어, Newb로 표기된 조정 메시를 획득하고, 여기서 조정 메시의 형상은 Fa이고, 조정 메시의 정점 토폴로지는 제2 정 점 토폴로지(Tb)이다. 또한, 새로운 표정 베이스, 예를 들어 타깃 표정 베이스(Eb')가 획득될 필요가 있고, 타 깃 표정 베이스(Eb')의 형상은 Fa이고, 타깃 표정 베이스(Eb')의 정점 토폴로지는 제2 정점 토폴로지(Tb)이고, 타깃 표정 베이스(Eb')의 차원수는 Nb이다. 조정 메시(Newb) 및 제2 표정 베이스(Ea)에서의 메시 변형 관계가 알려져 있기 때문에, 타깃 표정 베이스(Eb')는 조정 메시(Newb) 및 제2 표정 베이스(Eb)에서의 메시 변형 관계 에 따라 조정 메시(Newb)로부터의 변형을 통해 획득될 수 있다. 따라서 타깃 메시(Meshc)는 표정 맞춤화 알고리 즘을 사용함으로써 그리고 타깃 표정 베이스(Eb')를 사용함으로써 맞춤화될 수 있고, Nb인 차원수를 갖는 제2 표정 파라미터(Bb)가 또한 획득된다. 대량의 대응하는 제2 표정 파라미터들(Bb)은 대량의 기존 제1 표정 파라미터들(Ba)을 사용함으로써 생성될 수 있다. 유사하게, 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계는 앞서 말한 식 및 식의 방법들 을 사용함으로써 결정될 수 있다. 따라서 임의의 세트의 Ba에 대해, 애니메이션 캐릭터(b)를 구동하기 위해, 대응하는 Bb = f *Ba가 획득되어, 표 정 파라미터들(Ba)에 따라 표정 파라미터들(Bb)을 획득할 수 있다. 도 6에 대응하는 실시예에서, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다면, 구동 캐릭터는 제1 캐릭터이고, 피구동 캐릭터는 제2 캐릭터이다. 구동 캐릭터 는 애니메이션 캐릭터(a)이고, 피구동 캐릭터는 애니메이션 캐릭터(b)이고, 제1 표정 파라미터는 표정 파라미터 (Ba)이고, 형상은 Fa이고, 정점 토폴로지는 제1 정점 토폴로지(Ta)이고, 차원수는 Na이다. 다시 말해서, 제1 표 정 베이스(Ea)는 표정 파라미터(Ba)를 사용함으로써 직접 구동될 수 있다. 애니메이션 캐릭터(b)의 제2 표정 베 이스(Eb)의 형상은 Fb이고, 정점 토폴로지는 제2 정점 토폴로지(Tb)이고, 차원수는 제2 차원수(Nb)이고, 제1 표 정 베이스(Ea)를 구동하기 위한 대량의 표정 파라미터들(Ba)이 있었다. Fa는 Fb와 동일하지 않고, Ta는 Tb와 동 일하지 않으며, Na는 Nb와 동일하지 않다. 이 경우, 도 6에 대응하는 실시예의 구현을 위해, 도 10이 참조될 수 있다. 타깃 정점 토폴로지가 제2 정점 토 폴로지(Tb)라면, 제2 정점 토폴로지(Tb)인 정점 토폴로지를 갖는 조정 표정 베이스(Ea')가 먼저 구성되고, 조정 표정 베이스(Ea')가 제1 표정 파라미터(Ba)에 의해 구동될 수 있다는 것이 보장된다. 조정 표정 베이스(Ea')를 구성하는 다수의 방식들이 존재할 수 있다. 제1 방식은: 조정 표정 베이스(Ea')를 획득하기 위해, nricp 알고리 즘과 같은 얼굴 맞춤화 알고리즘을 사용함으로써, 제2 표정 베이스(Eb)로부터 결정된 무표정 메시를 제1 표정 베이스(Ea)의 각각의 표정 메시에 부착하는 것을 포함할 수 있다. 제2 방식은: 제2 표정 베이스(Eb)에서의 메시 의 정점 토폴로지와 동일한 정점 토폴로지 및 Fa인 형상을 갖는 무표정 메시를 획득하기 위해, 얼굴 맞춤화 알 고리즘을 사용함으로써, Eb로부터 결정된 무표정 메시를 제1 표정 베이스(Ea)의 무표정 메시에 부착하는 것, 그 리고 이어서, 조정 표정 베이스(Ea')를 획득하기 위해, 무표정 메시에 대한 Ea에서의 각각의 표정 메시의 변형 에 따라, Fa인 형상 및 Tb인 정점 토폴로지를 갖는, 앞서 말한 획득된 무표정 메시를 변경함으로써, Ea 및 Eb의 한 쌍의 정점들 간의 대응이 매핑 동안 고유함을 보장하는 것을 포함할 수 있다. 조정 표정 베이스(Ea')의 형상 은 Fa이고, 조정 표정 베이스(Ea')의 정점 토폴로지는 제2 정점 토폴로지(Tb)이며, 조정 표정 베이스(Ea')의 차 원수는 제1 차원수(Na)이다. 제1 표정 베이스(Ea)는 제1 표정 파라미터(Ba)를 사용함으로써 직접 구동될 수 있고, 조정 표정 베이스(Ea')는 제1 표정 베이스(Ea)와 동일한 양의 차원들을 가지며, 각각의 차원의 의미 정보는 동일하기 때문에, 타깃 메시 를 획득하기 위해 제1 표정 파라미터(Ba)를 사용함으로써 조정 표정 베이스(Ea')가 직접 구동될 수 있다. 타깃 메시의 형상은 Fa이고, 정점 토폴로지는 제2 정점 토폴로지(Tb)이며, 타깃 메시는 Meshc로 표기된다. 타깃 메시 및 타깃 표정 베이스에 따라, 제2 차원수(Nb)인 차원수를 갖는 제2 표정 파라미터(Bb)를 결정하기 위 해, Fa인 형상, 제2 정점 토폴로지(Tb)인 정점 토폴로지, 및 Nb인 차원수를 갖는 타깃 표정 베이스가 구성될 필 요가 있다. 따라서 표정이 없는 제1 캐릭터에 대응하는 무표정 메시가 조정 표정 베이스(Ea')로부터 결정되고, 제1 캐릭터에 대응하는 무표정 메시 및 제2 표정 베이스에서의 메시 변형 관계에 따라 타깃 표정 베이스(Eb')가 생성되며, 타깃 표정 베이스(Eb')의 형상은 Fa이고, 타깃 표정 베이스(Eb')의 정점 토폴로지는 제2 정점 토폴로 지(Tb)이며, 타깃 표정 베이스(Eb')의 차원수는 Nb이다. 따라서 타깃 메시(Meshc)는 표정 맞춤화 알고리즘을 사 용함으로써 그리고 타깃 표정 베이스(Eb')를 사용함으로써 맞춤화될 수 있고, Nb인 차원수를 갖는 제2 표정 파 라미터(Bb)가 또한 획득된다. 대량의 대응하는 제2 표정 파라미터들(Bb)은 대량의 기존 제1 표정 파라미터들(Ba)을 사용함으로써 생성될 수 있다. 유사하게, 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계는 앞서 말한 식 및 식의 방법들 을 사용함으로써 결정될 수 있다. 따라서 임의의 세트의 Ba에 대해, 애니메이션 캐릭터(b)를 구동하기 위해, 대응하는 Bb = f *Ba가 획득되어, 표 정 파라미터들(Ba)에 따라 표정 파라미터들(Bb)을 획득할 수 있다. 도 6에 대응하는 실시예에서, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖지 않고, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 제1 표정 파라미터는 랜덤 표정 파라미터라면, 구동 캐릭터는 제2 캐릭터이고, 피 구동 캐릭터는 제1 캐릭터이다. 구동 캐릭터는 애니메이션 캐릭터(a)이고, 피구동 캐릭터는 애니메이션 캐릭터 (b)이고, 제1 표정 파라미터는 랜덤 표정 파라미터(Bb)이고, 형상은 Fb이고, 정점 토폴로지는 제1 정점 토폴로 지(Tb)이고, 차원수는 Nb이다. 즉, 제1 표정 베이스(Eb)는 랜덤 표정 파라미터(Bb)를 사용함으로써 직접 구동될수 있고, 제1 표정 베이스(Eb)의 형상은 Fb이고, 정점 토폴로지는 제1 정점 토폴로지(Tb)이고, 차원수는 제1 차 원수(Nb)이다. 애니메이션 캐릭터(a)의 제2 표정 베이스(Ea)의 형상은 Fa이고, 정점 토폴로지는 제2 정점 토폴 로지(Ta)이고, 차원수는 제2 차원수(Na)이고, 제2 표정 베이스(Ea)를 구동하기 위한 대량의 표정 파라미터들 (Ba)이 있었다. Fa는 Fb와 동일하지 않고, Ta는 Tb와 동일하지 않으며, Na는 Nb와 동일하지 않다. 이 경우, 도 6에 대응하는 실시예의 구현을 위해, 도 11이 참조될 수 있다. 타깃 정점 토폴로지가 제2 정점 토 폴로지(Ta)라면, 제2 정점 토폴로지(Ta)인 정점 토폴로지를 갖는 조정 표정 베이스(Eb')가 먼저 구성되고, 조정 표정 베이스(Eb')가 제1 표정 파라미터(Bb)에 의해 구동될 수 있다는 것이 보장된다. 조정 표정 베이스(Eb')를 구성하는 다수의 방식들이 존재할 수 있다. 제1 방식은: 조정 표정 베이스(Eb')를 획득하기 위해, nricp 알고리 즘과 같은 얼굴 맞춤화 알고리즘을 사용함으로써, 제2 표정 베이스(Ea)로부터 결정된 무표정 메시를 제1 표정 베이스(Eb)의 각각의 표정 메시에 부착하는 것을 포함할 수 있다. 제2 방식은: 제2 표정 베이스(Ea)에서의 메시 의 정점 토폴로지와 동일한 정점 토폴로지 및 Fb인 형상을 갖는 무표정 메시를 획득하기 위해, 얼굴 맞춤화 알 고리즘을 사용함으로써, Ea로부터 결정된 무표정 메시를 제1 표정 베이스(Eb)의 무표정 메시에 부착하는 것, 그 리고 이어서, 조정 표정 베이스(Eb')를 획득하기 위해, 무표정 메시에 대한 Eb에서의 각각의 표정 메시의 변형 에 따라, Fb인 형상 및 Ta인 정점 토폴로지를 갖는, 앞서 말한 획득된 무표정 메시를 변경함으로써, Ea 및 Eb의 한 쌍의 정점들 간의 대응이 매핑 동안 고유함을 보장하는 것을 포함할 수 있다. 조정 표정 베이스(Eb')의 형상 은 Fb이고, 조정 표정 베이스(Eb')의 정점 토폴로지는 제2 정점 토폴로지(Ta)이며, 조정 표정 베이스(Eb')의 차 원수는 제1 차원수(Nb)이다. 제1 표정 베이스(Eb)는 제1 표정 파라미터(Bb)를 사용함으로써 직접 구동될 수 있고, 조정 표정 베이스(Eb')는 제1 표정 베이스(Eb)와 동일한 양의 차원들을 가지며, 각각의 차원의 의미 정보는 동일하기 때문에, 타깃 메시 를 획득하기 위해 제1 표정 파라미터(Bb)를 사용함으로써 조정 표정 베이스(Eb')가 직접 구동될 수 있다. 타깃 메시의 형상은 Fb이고, 정점 토폴로지는 제2 정점 토폴로지(Ta)이며, 타깃 메시는 Meshc로 표기된다. 타깃 메시 및 타깃 표정 베이스에 따라, 제2 차원수(Na)인 차원수를 갖는 제2 표정 파라미터(Ba')를 결정하기 위해, Fb인 형상, 제2 정점 토폴로지(Ta)인 정점 토폴로지, 및 Na인 차원수를 갖는 타깃 표정 베이스가 구성될 필요가 있다. 따라서 표정이 없는 제1 캐릭터에 대응하는 무표정 메시가 조정 표정 베이스(Eb')로부터 결정되고, 제1 캐릭터에 대응하는 무표정 메시 및 제2 표정 베이스에서의 메시 변형 관계에 따라 타깃 표정 베 이스(Ea')가 생성되며, 타깃 표정 베이스(Ea')의 형상은 Fb이고, 타깃 표정 베이스(Ea')의 정점 토폴로지는 제2 정점 토폴로지(Ta)이며, 타깃 표정 베이스(Eb')의 차원수는 Na이다. 따라서 타깃 메시(Meshc)는 표정 맞춤화 알 고리즘을 사용함으로써 그리고 타깃 표정 베이스(Ea')를 사용함으로써 맞춤화될 수 있고, Na인 차원수를 갖는 제2 표정 파라미터(Ba')가 또한 획득된다. 대량의 제1 표정 파라미터들(Bb)이 랜덤하게 생성될 때, 대량의 대응하는 제2 표정 파라미터들(Ba')이 생성될 수 있다. 유사하게, 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계는 앞서 말한 식 및 식의 방 법들을 사용함으로써 결정될 수 있다. 매핑 관계(f)가 획득된 후, 대량의 Ba가 존재했기 때문에, 각각 Ba 및 Ba'에 대응하는 표정 베이스의 차원수는 Na이고, 각각의 차원수의 의미 정보는 동일하며, Ba와 Ba'는 동등할 수 있다. 따라서 임의의 세트의 Ba에 대해, 애니메이션 캐릭터(b)를 구동하기 위해, 대응하는 Bb = f *Ba가 획득되어, 표정 파라미터들(Ba)에 따라 표정 파 라미터들(Bb)을 획득할 수 있다. 위에서 설명된 제1 타입의 방법은 주로: 샘플링을 통해 획득된 기존의 제1 표정 파라미터에 기초하여 제2 표정 파라미터를 결정하는 단계, 및 그 다음, 제1 표정 파라미터와 제2 표정 파라미터 간의 매핑 관계를 결정하는 단 계를 포함한다. 불균등한 샘플링에 의해 야기된 제2 표정 파라미터들의 불완전한 분포의 문제를 피하기 위해, 본 개시내용의 일 실시예에 따라 제2 타입의 방법이 추가로 제공된다. 제2 타입의 방법의 구현은: 구동 캐릭터 에 대응하는 표정 베이스 및 표정 파라미터에 따라 구동 캐릭터에 대응하는 제1 점군 데이터를 결정하는 단계, 및 피구동 캐릭터에 대응하는 표정 베이스 및 표정 파라미터에 따라, 피구동 캐릭터에 대응하는 제2 점군 데이 터를 결정하는 단계를 포함할 수 있다. 제2 점군 데이터는 제1 점군 데이터를 변환함으로써 획득될 수 있거나, 제1 점군 데이터는 제2 점군 데이터를 변환함으로써 획득될 수 있다. 제1 점군 데이터 및 제2 점군 데이터의 변 환 파라미터가 결정된다면, 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계가 제1 점군 데이터, 제2 점군 데이터 및 변환 파라미터에 따라 결정될 수 있다. 변환은 예를 들 어, 회전, 병진 및 스케일링을 포함할 수 있다. 변환 파라미터는 제2 점군 데이터가 제1 점군 데이터로 변환되는 변환 관계를 식별하는 데 사용된다. 다음으로, 점군 데이터(예를 들어, 제1 점군 데이터 및 제2 점군 데이터)를 결정하는 원리가 설명된다. 표정이 없는 애니메이션 캐릭터가 mu이고, 표정 베이스가 E이며, 표정 베이스(E)의 차원수가 n이라면, B는 n*1 의 벡터이다. E가 표정 베이스 행렬이라면, 주어진 n차원 파라미터 B에 대해, 다음 식을 사용함으로써 점군 데이터(R)가 획득될 수 있다: . 구동 캐릭터의 표정 베이스가 Ea이고, 구동 캐릭터의 표정 파라미터는 Ba이고, 표정이 없는 구동 캐릭터는 mua이 고, 피구동 캐릭터의 표정 베이스는 Eb이고, 피구동 캐릭터의 표정 파라미터는 Bb이고, 표정이 없는 피구동 캐릭 터는 mub라면, 제1 점군 데이터는 Ra = mua + Ea * Ba이고, 제2 점군 데이터는 Rb = mub + Eb * Bb이다. 이 실시예에서, 변환 파라미터는 다수의 방법들을 사용함으로써 결정될 수 있다고 이해될 수 있다. 예를 들어, 변환 파라미터는 반복 최근점(ICP: iterative closest point) 알고리즘을 사용함으로써 계산될 수 있다. 변환 파라미터는 다음 식으로 표현될 수 있으며: , 여기서 trans는 변환 파라미터이고, s는 스케일링을 나타내고, R은 회전을 나타내고, T는 병진을 나타낸다. 변환 파라미터를 사용함으로써 제1 점군 데이터가 제2 점군 데이터로 변환되는 일례를 사용함으로써, 구동 캐릭 터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계가 제1 점군 데이터, 제2 점군 데이터 및 변환 파라미터에 따라 결정되는 경우가 설명된다. 변환 파라미터를 사용함으로써 처리되는 제1 점군 데이터와 제2 점군 데이터가 완전히 동일하다면, 변환 파라미 터를 사용함으로써 처리되는 제1 점군 데이터 및 제2 점군 데이터는 다음 식으로 표현되는 다음의 관계를 갖는다: . trans는 주로 표정 베이스에 대해 작용하기 때문에, 식은 다음 식으로 변환될 수 있으며: , 여기서 Ec는 trans를 사용하여 표정 베이스(Ea)를 처리함으로써 획득된 새로운 표정 베이스이다. trans는 주로 표정 베이스에 대해 작용하고, 표정 파라미터에는 영향을 미치지 않기 때문에, Bc = Ba이다. 또한, 식에 기 초하여, 2개의 표정 베이스들이 동일한 형상을 갖고, 표정이 없는 캐릭터들도 또한 동일하기 때문에, mub = mua 이다. 따라서 식은 다음 식으로 추가로 단순화될 수 있다: . 실시예의 목적은 구동 캐릭터에 대응하는 표정 파라미터(Ba)와 피구동 캐릭터에 대응하는 표정 파라미터(Bb) 간 의 매핑 관계(f)를 결정하는 것이므로 Bb = f *Ba이기 때문에, 식을 참조하여 f = Eb-1 * Ec가 획득될 수 있 다. 제2 타입의 방법은 더 작은 수학적 에러를 갖고, 획득된 f는 샘플링 솔루션이 아닌 분석 솔루션이며, 이로써 샘 플링에 의해 야기되는 불완전한 분포의 문제를 피한다. 추가로, 이 방법에서의 계산 결과(f)는 단지 표정 베이스에만 의존한다. 원래의 점군 데이터는 불균일하게 분포 될 수 있기 때문에, 균일한 포인트들의 인라이너에 의해 형성된 표정 베이스에 의해 달성될 수 있는 효과는 메 시를 다운샘플링함으로써 달성될 수 있다. 추가로, 사용되는 부분(이를테면, 단지 사용되는 입 또는 눈 위의 점 들)이 또한 제어될 수 있다. 이런 식으로, 애니메이션 캐릭터에서 구동될 필요가 있는 부분이 정확하게 제어될 수 있고, 이로써 중요하지 않은 부분(이를테면, 뺨)에 의해 야기되는 간섭을 피할 수 있다. 이 실시예에서는, 애니메이션 캐릭터의 얼굴에 추가하여 애니메이션 캐릭터의 다양한 구조들이 추가로 구동될 수 있다. 각각의 구동되는 구조는 애니메이션 캐릭터의 변형 가능한 컴포넌트라고 이해되어야 한다. 예를 들어, 애니메이션 캐릭터는 사람이다. 사람의 손, 발 등이 변형(예를 들어, 구부러짐)될 수 있기 때문에, 구동되는 구 조는 손, 발 등일 수 있다. 따라서 본 개시내용의 일 실시예는 애니메이션 캐릭터 구동 방법을 제공한다. 도 12a를 참조하면, 이 방법은 다 음의 단계들(S1201 내지 S1203)을 포함한다. 단계(S1201)에서, 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스가 획득된다. 구동 캐릭터는 대응하는 구조 베이스를 갖고, 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으며, 구조 베이 스는 대응하는 캐릭터의 구조적 특징들을 식별하는 데 사용되고, 변형 베이스는 대응하는 캐릭터의 변형 특징들 을 식별하는 데 사용된다. 예를 들어, 대응하는 오브젝트는 손이다. 구조 베이스는 구조적 특징들, 이를테면 손 가락 길이, 손가락 두께, 손바닥 폭 및 두께, 그리고 손가락 포지션을 반영할 수 있고; 변형 베이스는 변형 특 징들, 이를테면 손가락 휨 정도를 반영할 수 있다. 확실히, 대응하는 오브젝트가 얼굴이라면, 구조 베이스는 앞 서 말한 실시예들에서 언급된 얼굴 맞춤화 베이스이고, 변형 베이스는 표정 베이스이다. 단계(S1202)에서, 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스에 따라, 구동 캐릭터에 대응하는 변형 파라미터와 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계가 결정된다. 변형 파라미터는 대응하는 오브젝트의 형상의 변화도를 식별하는 데 사용된다. 예를 들어, 대응하는 오브젝트는 손이고, 변형 파라미터는 손가락 휨 정도를 반영한다. 확실히, 대응하는 오브젝트가 얼굴이라면, 변형 파라미터 는 앞서 말한 실시예들에서 언급된 표정 파라미터이다. 단계(S1203)에서, 피구동 캐릭터가 구동 캐릭터에 대응하는 변형 파라미터 및 매핑 관계에 따라 구동된다. 이 실시예의 단계들의 구현을 위해, 도 2에 대응하는 실시예의 구현이 참조될 수 있다. 세부사항들은 여기서 다 시 설명되지 않는다. 다음으로, 본 개시내용의 실시예들에 따라 제공되는 AI 기반 애니메이션 캐릭터 구동 방법이 실제 애플리케이션 시나리오를 참조하여 설명된다. 본 개시내용 시나리오에서, 애니메이션 캐릭터가 설 인사들(Chinese New Year's greetings)을 송신하는 데 사용 되는 것으로 가정된다. 애니메이션 캐릭터(a)가 존재했고, 애니메이션 캐릭터(a)가 얼굴 맞춤화 베이스를 갖는 다면, 애니메이션 캐릭터(a)의 표정 베이스(Ea)의 형상은 Fa이고, 표정 베이스(Ea)의 정점 토폴로지는 Ta이고, 표정 베이스(Ea)의 차원수는 Na이다. 설 인사들을 송신하기 위한 새로운 애니메이션 캐릭터(b)를 론치하는 것으 로 의도된다면, 애니메이션 캐릭터(b)의 표정 베이스(Eb)의 형상은 Fb이고, 표정 베이스(Eb)의 정점 토폴로지는 Tb이고, 표정 베이스(Eb)의 차원수는 Nb이다. Fa는 Fb와 동일하지 않고, Ta는 Tb와 동일하지 않으며, Na는 Nb와 동일하지 않다. 애니메이션 캐릭터(a)의 표정 파라미터(Ba)가 알려져 있다면, 애니메이션 캐릭터(b)를 론치하는 스피치를 증가 시키기 위해, 알려진 표정 파라미터(Ba)를 갖는 애니메이션 캐릭터(a)가 구동 캐릭터로서 사용될 수 있고, 애니 메이션 캐릭터(b)는 피구동 캐릭터로서 사용될 수 있으며, 구동 캐릭터에 대응하는 표정 베이스(Ea) 및 피구동 캐릭터에 대응하는 표정 베이스(Eb)에 따라, 구동 캐릭터에 대응하는 표정 파라미터(Ba)와 피구동 캐릭터에 대 응하는 표정 파라미터(Bb) 간의 매핑 관계가 결정된다. 이런 식으로, 구동 캐릭터의 표정 파라미터(Ba)가 알려 졌을 때, 피구동 캐릭터에 대응하는 표정 파라미터(Bb)는 새로운 애니메이션 캐릭터를 구동하기 위해, 애니메이 션 캐릭터(b)에 대응하는 얼굴 맞춤화 베이스를 획득하기 위한 처리를 수행하지 않고 알려진 표정 파라미터(Ba) 및 매핑 관계에 따라 결정될 수 있으며, 이로써 애니메이션 캐릭터(b)를 론치하는 스피치를 증가시킬 수 있다. 앞서 말한 실시예들에 따라 제공된 방법에 기초하여, 본 개시내용의 일 실시예에 따라 AI 기반 애니메이션 캐릭 터 구동 장치가 추가로 제공된다. 도 12b를 참조하면, 이 장치는 획득 유닛, 결정 유닛 및 구동 유 닛을 포함하며: 획득 유닛은 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스를 획득하도 록 구성되며, 구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖고, 피구동 캐릭터는 대응하는 얼굴 맞춤화 베이 스를 갖지 않으며; 결정 유닛은 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 구 동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하도록구성되고; 그리고 구동 유닛은 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하도록 구성된다. 가능한 구현에서, 구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖는 캐릭터이고, 피구동 캐릭터는 제1 캐릭터 및 제2 캐릭터 중 대응하는 얼굴 맞춤화 베이스를 갖지 않는 캐릭터이 다. 결정 유닛은: 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 타깃 메시를 결정하고 ― 제1 표정 파라 미터는 제1 양의 차원들을 갖는 표정 베이스를 구동하는 데 사용되며, 제1 표정 베이스의 차원수는 제1 차원수 이고, 타깃 메시는 제1 표정 파라미터에 대응하는 표정을 짓는 제1 캐릭터를 식별하기 위한 타깃 정점 토폴로지 를 가짐 ―; 타깃 정점 토폴로지를 갖는 제1 캐릭터에 대응하는 타깃 표정 베이스를 획득하고 ― 타깃 표정 베이스의 차원수 는 제2 차원수이고, 타깃 표정 베이스는 제2 캐릭터에 대응하는 제2 표정 베이스에 따라 결정됨 ―; 제1 캐릭터에 대응하는 제2 표정 파라미터를 타깃 메시 및 타깃 표정 베이스에 따라 결정하고 ― 제2 표정 파라 미터는 타깃 메시에 대한 제1 캐릭터의 표정의 변화도를 반영하는 데 사용됨 ―; 그리고 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 제1 표정 파라미터 및 제2 표정 파라미터에 따라 결정하도록 구성된다. 가능한 구현에서, 타깃 정점 토폴로지는 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고, 결정 유닛 은 추가로: 제1 캐릭터에 대응하는 제1 표정 파라미터 및 제1 표정 베이스에 따라 초기 메시를 결정하고 ― 초기 메시는 제 1 표정 베이스에 대응하는 제1 정점 토폴로지를 가짐 ―, 그리고 제1 정점 토폴로지와 제2 정점 토폴로지 간의 대응관계에 따라 타깃 메시를 생성하도록 구성되며, 그리고 제1 표정 베이스로부터, 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고, 제2 표정 베 이스로부터, 제2 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고, 제1 캐릭터에 대응하는 무표정 메시 및 제2 캐릭터에 대응하는 무표정 메시에 따라 조정 메시를 결정하고 ― 조 정 메시는 제2 정점 토폴로지를 갖고, 무표정인 제1 캐릭터를 식별하는 데 사용됨 ―, 그리고 조정 메시 및 제2 표정 베이스에서의 메시 변형 관계에 따라 타깃 표정 베이스를 생성하도록 구성된다. 가능한 구현에서, 타깃 정점 토폴로지는 제2 표정 베이스에 대응하는 제2 정점 토폴로지이고, 결정 유닛 은 추가로: 제2 캐릭터에 대응하며 제2 캐릭터에 대응하는 제2 표정 베이스로부터 어떠한 표정도 갖지 않는 무표정 메시를 결정하고, 제2 캐릭터에 대응하는 무표정 메시 및 제1 캐릭터에 대응하는 제1 표정 베이스에 따라, 제1 캐릭터에 대응하고 제2 정점 토폴로지를 갖는 조정 표정 베이스를 결정하고 ― 조정 표정 베이스의 차원수는 제1 차원수임 ―; 그 리고 제1 표정 파라미터 및 조정 표정 베이스에 따라 타깃 메시를 결정하도록 구성되며; 그리고 조정 표정 베이스로부터, 제1 캐릭터에 대응하고 어떠한 표정도 갖지 않는 무표정 메시를 결정하고, 그리고 제1 캐릭터에 대응하는 무표정 메시 및 제2 표정 베이스에서의 메시 변형 관계에 따라 타깃 표정 베이스를 생성 하도록 구성된다. 가능한 구현에서, 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖지 않는다면, 제2 캐릭터는 대응하는 얼굴 맞 춤화 베이스를 갖고, 제1 표정 파라미터는 랜덤 표정 파라미터이거나; 또는 제1 캐릭터가 대응하는 얼굴 맞춤화 베이스를 갖고, 제2 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않는다. 가능한 구현에서, 결정 유닛은 추가로: 다수의 제1 표정 파라미터들 및 다수의 제2 표정 파라미터들을 쌍들로 획득하고; 그리고 다수의 제1 표정 파라미터들에 의해 형성된 제1 행렬 및 다수의 제2 표정 파라미터들에 의해 형성된 제2 행렬에 따라 매핑 관계를 결정하도록 구성된다. 가능한 구현에서, 결정 유닛은: 구동 캐릭터에 대응하는 표정 베이스 및 표정 파라미터에 따라 구동 캐릭터에 대응하는 제1 점군 데이터를 결정 하고; 피구동 캐릭터에 대응하는 표정 베이스 및 표정 파라미터에 따라, 피구동 캐릭터에 대응하는 제2 점군 데이터를 결정하고; 제1 점군 데이터 및 제2 점군 데이터의 변환 파라미터를 결정하고 ― 변환 파라미터는 제2 점군 데이터와 제1 점군 데이터 간의 변환 관계를 식별하는 데 사용됨 ―; 그리고 구동 캐릭터에 대응하는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 제1 점군 데이터, 제2 점군 데이터 및 변환 파라미터에 따라 결정하도록 구성된다. 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 장치가 추가로 제공된다. 도 13a를 참조하면, 이 장치는 획득 유닛, 결정 유닛 및 구동 유닛을 포함하며: 획득 유닛은 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하도 록 구성되고, 구동 캐릭터는 대응하는 구조 베이스를 갖고, 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으 며, 구조 베이스는 대응하는 캐릭터의 구조적 특징을 식별하는 데 사용되고, 변형 베이스는 대응하는 캐릭터의 변형 특징을 식별하는 데 사용되며; 결정 유닛은 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스에 따라, 구 동 캐릭터에 대응하는 변형 파라미터와 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계를 결정하도록 구성되고; 그리고 구동 유닛은 구동 캐릭터에 대응하는 변형 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하도록 구성된다. 본 개시내용의 일 실시예에 따라 디바이스가 추가로 제공된다. 디바이스는 AI에 기반하여 애니메이션 캐릭터를 구동할 수 있다. 디바이스는 첨부 도면들을 참조하여 아래에서 설명된다. 도 13b를 참조하면, 본 개시내용의 일 실시예에 따라 디바이스가 제공되며, 여기서 디바이스는 대안으로 단말 디바이스일 수 있다. 단말 디바이스는 휴대 전화, 태블릿 컴퓨터, PDA, 판매 시점(POS: point of sales) 또는 탑재형 컴퓨터를 포함하는 임의의 스마트 단말일 수 있으며, 휴대 전화인 단말 디바이스가 일례로 사용된다. 도 13b는 본 개시내용의 일 실시예에 따른 단말 디바이스와 관련된 휴대 전화의 일부의 구조의 블록도이다. 도 13b를 참조하면, 휴대 전화는 무선 주파수(RF: radio frequency) 회로, 메모리, 입력 유닛, 디스플레이 유닛, 센서, 오디오 회로, 무선 충실도(Wi-Fi: wireless fidelity) 모듈,"}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "프로세서 및 전원 공급 장치와 같은 컴포넌트들을 포함한다. 당해 기술분야에서 통상의 지식을 가 진 자는, 도 13b에 도시된 휴대 전화의 구조가 휴대 전화에 대한 어떠한 제한도 구성하지 않으며, 휴대 전화가 도면에 도시된 것들보다 더 많은 또는 더 적은 컴포넌트들을 포함할 수 있거나, 일부 컴포넌트들이 조합될 수 있거나, 상이한 컴포넌트 배치가 사용될 수 있다고 이해할 수 있다. 메모리는 소프트웨어 프로그램 및 모듈을 저장하도록 구성될 수 있다. 프로세서는 휴대 전화의 다 양한 기능 애플리케이션들 및 데이터 처리를 수행하기 위해, 메모리에 저장되는 소프트웨어 프로그램 및 모듈을 실행한다. 메모리는 주로 프로그램 저장 영역 및 데이터 저장 영역을 포함할 수 있다. 프로그램 저장 영역은 운영 체제, 적어도 하나의 기능(이를테면, 사운드 재생 기능 및 이미지 디스플레이 기능)에 의해 요구되는 애플리케이션 프로그램 등을 저장할 수 있다. 데이터 저장 영역은 휴대 전화의 사용에 따라 생성된 데 이터(이를테면, 오디오 데이터 및 주소록) 등을 저장할 수 있다. 추가로, 메모리는 고속 랜덤 액세스 메 모리를 포함할 수 있고, 또한, 비휘발성 메모리, 예를 들어 적어도 하나의 자기 디스크 저장 디바이스, 플래시 메모리, 또는 다른 휘발성 솔리드 스테이트 저장 디바이스를 포함할 수 있다. 프로세서는 휴대 전화의 제어 센터이며, 다양한 인터페이스들 및 라인들을 사용함으로써 전체 휴대 전화 의 다양한 부분들에 연결된다. 메모리에 저장된 소프트웨어 프로그램 및/또는 모듈을 구동 또는실행하고, 메모리에 저장된 데이터를 호출함으로써, 프로세서는 휴대 전화의 다양한 기능들 및 데이터 처 리를 수행함으로써, 휴대 전화에 대한 전체 모니터링을 수행한다. 선택적으로, 프로세서는 하나 이상의 처리 유닛들을 포함할 수 있다. 바람직하게는, 프로세서는 애플리케이션 프로세서 및 모뎀을 통합할 수 있다. 애플리케이션 프로세서는 주로 운영 체제, 사용자 인터페이스, 애플리케이션 프로그램 등을 처리한다. 모 뎀은 주로 무선 통신을 처리한다. 모뎀은 프로세서에 통합되지 않을 수도 있다고 이해될 수 있다. 이 실시예에서, 단말에 포함된 프로세서는 추가로 다음의 기능들: 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스를 획득하는 기능 ― 구동 캐릭 터는 대응하는 얼굴 맞춤화 베이스를 갖고, 피구동 캐릭터는 대응하는 얼굴 맞춤화 베이스를 갖지 않음 ―; 구동 캐릭터에 대응하는 표정 베이스 및 피구동 캐릭터에 대응하는 표정 베이스에 따라, 구동 캐릭터에 대응하 는 표정 파라미터와 피구동 캐릭터에 대응하는 표정 파라미터 간의 매핑 관계를 결정하는 기능; 및 구동 캐릭터에 대응하는 표정 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하는 기능, 또는 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스를 획득하는 기능 ― 구동 캐릭 터는 대응하는 구조 베이스를 갖고, 피구동 캐릭터는 대응하는 구조 베이스를 갖지 않으며, 구조 베이스는 대응 하는 캐릭터의 구조적 특징을 식별하는 데 사용되고, 변형 베이스는 대응하는 캐릭터의 변형 특징을 식별하는 데 사용됨 ―; 구동 캐릭터에 대응하는 변형 베이스 및 피구동 캐릭터에 대응하는 변형 베이스에 따라, 구동 캐릭터에 대응하 는 변형 파라미터와 피구동 캐릭터에 대응하는 변형 파라미터 간의 매핑 관계를 결정하는 기능; 및 구동 캐릭터에 대응하는 변형 파라미터 및 매핑 관계에 따라, 피구동 캐릭터를 구동하는 기능을 갖는다. 본 개시내용의 일 실시예는 서버를 추가로 제공한다. 도 14는 본 개시내용의 일 실시예에 따른 서버의 구 조도이다. 서버는 상이한 구성들 또는 성능으로 인해 크게 변할 수 있고, 하나 이상의 중앙 처리 유닛 (CPU: central processing unit)들(예를 들어, 하나 이상의 프로세서들) 및 메모리, 그리고 애플 리케이션 프로그램들 또는 데이터를 저장하는 하나 이상의 저장 매체들(예를 들어, 하나 이 상의 대용량 저장 디바이스들)을 포함할 수 있다. 메모리 및 저장 매체는 일시적 저장 또는 영구 저장을 구현할 수 있다. 저장 매체에 저장된 프로그램은 (도면에 도시되지 않은) 하나 이상의 모듈들을 포함할 수 있고, 각각의 모듈은 서버에 대한 일련의 명령 동작들을 포함할 수 있다. 더욱이, CPU는 저장 매체와 통신하고, 서버 상에서, 저장 매체에서의 일련의 명령 동작들을 수행하도록 구성될 수 있다. 서버는 하나 이상의 전원 공급 장치들, 하나 이상의 유선 또는 무선 네트워크 인터페이스들, 하나 이상의 입력/출력 인터페이스들 및/또는 하나 이상의 운영 체제들, 이를테면 Windows Server ™, Mac OS X™, Unix™, Linux™ 및 FreeBSD™를 더 포함할 수 있다. 앞서 말한 실시예들에서 서버에 의해 수행되는 단계들은 도 14에 도시된 서버 구조에 기반하여 구현될 수 있다. 본 개시내용의 일 실시예에 따라 컴퓨터 판독 가능 저장 매체가 추가로 제공된다. 컴퓨터 판독 가능 저장 매체 는 프로그램 코드를 저장하도록 구성되며, 프로그램 코드는 앞서 말한 실시예들에 따른 AI 기반 애니메이션 캐 릭터 구동 방법을 수행하는 데 사용된다. 본 개시내용의 일 실시예에 따라, 명령들을 포함하는 컴퓨터 프로그램 제품이 추가로 제공되며, 명령들은 컴퓨 터 상에서 실행될 때 컴퓨터로 하여금, 앞서 말한 실시예들에 따른 AI 기반 애니메이션 캐릭터 구동 방법을 수 행하게 한다. 앞서 말한 실시예들은 본 개시내용을 제한하기 위한 것이 아니라, 단지 본 개시내용의 기술적 솔루션들을 설명"}
{"patent_id": "10-2021-7029446", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "하기 위해 의도된다. 본 개시내용은 앞서 말한 실시예들을 참조하여 상세히 설명되지만, 당해 기술분야에서 통 상의 지식을 가진 자들은 본 개시내용의 실시예들의 기술적 솔루션들의 사상 및 범위를 벗어나지 않으면서, 이 들이 여전히 앞서 말한 실시예들에서 설명된 기술적 솔루션들을 수정하거나 이들의 일부 기술적 특징들에 대해 동등한 대체들을 행할 수 있다고 이해한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12a 도면12b 도면13a 도면13b 도면14"}
{"patent_id": "10-2021-7029446", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시내용의 일 실시예에 따른, 인공 지능 기반(AI 기반) 애니메이션 캐릭터 구동 방법의 애플리케이 션 시나리오의 개략도이다. 도 2는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 3은 본 개시내용의 일 실시예에 따른 3DMM 라이브러리에서의 M의 차원들의 의미 및 분포의 예시적인 도면이 다. 도 4는 본 개시내용의 일 실시예에 따른, AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 5는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 6은 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 7은 본 개시내용의 일 실시예에 따라 애니메이션 캐릭터를 구동하는 효과의 도면이다. 도 8은 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 9는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 10은 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 11은 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 12a는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 방법의 흐름도이다. 도 12b는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 장치의 구조도이다. 도 13a는 본 개시내용의 일 실시예에 따른 AI 기반 애니메이션 캐릭터 구동 장치의 구조도이다. 도 13b는 본 개시내용의 일 실시예에 따른 단말 디바이스의 구조도이다. 도 14는 본 개시내용의 일 실시예에 따른 서버의 구조도이다."}
