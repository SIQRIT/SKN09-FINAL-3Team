{"patent_id": "10-2022-0016415", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0119984", "출원번호": "10-2022-0016415", "발명의 명칭": "다중인자 비선형 활성화 함수의 배타적 논리연산 학습 가능성 및 활용 방법", "출원인": "한양대학교 산학협력단", "발명자": "윤기중"}}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 장치에 의해 수행되는 활성화 함수의 학습 방법에 있어서,다중 인자 기반의 비선형 활성화 함수를 이용하여 내부 네트워크를 구성하는 단계; 및 상기 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습하는 단계를 포함하는 활성화 함수의 학습 방법."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 내부 네트워크를 구성하는 단계는,다중 인자 기반의 비선형 활성화 함수를 복수 개의 입력 인자와 적어도 하나 이상의 출력 단자를 갖는 멀티 레이어 퍼셉트론(Multilayer Perceptron)으로 모델링하여 내부 네트워크를 구성하는 단계 를 포함하는 활성화 함수의 학습 방법."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 내부 네트워크를 구성하는 단계는,기 설정된 사이즈의 컨볼루션을 사용하여 내부 네트워크를 구성하는 단계를 포함하는 활성화 함수의 학습 방법."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 학습하는 단계는,상기 구성된 내부 네트워크를 상기 외부 네트워크의 히든 레이어 사이에 위치시켜 상기 구성된 내부 네트워크와상기 외부 네트워크를 결합하는 단계를 포함하는 활성화 함수의 학습 방법."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 학습하는 단계는,상기 내부 네트워크의 깊이 차원에서 슬라이스 및 연결 작업을 통해 상기 구성된 내부 네트워크와 상기 외부 네트워크를 결합하는 단계를 포함하는 활성화 함수의 학습 방법"}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 학습하는 단계는, 상기 다중 인자 기반의 비선형 활성화 함수에 대하여 지도 학습을 사용하여 내부 네트워크를 사전 학습시키는공개특허 10-2023-0119984-3-단계 를 포함하는 활성화 함수의 학습 방법"}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 학습하는 단계는,파라미터 공유를 통해 상기 사전 학습된 내부 네트워크를 외부 네트워크와 결합하여 결합 모델이 생성되도록 내부 네트워크와 외부 네트워크를 동시에 학습시키는 단계를 포함하는 활성화 함수의 학습 방법"}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 학습하는 단계는,상기 학습된 내부 네트워크를 고정한 다음 상기 학습된 외부 네트워크를 초기화시키고, 상기 초기화된 외부 네트워크를 재학습시키는 단계를 포함하는 활성화 함수의 학습 방법"}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항의 활성화 함수의 학습 방법을 상기 컴퓨터 장치에 실행시키기 위해 비-일시적인 컴퓨터 판독가능한 기록 매체에 저장되는 컴퓨터 프로그램."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터 장치에 있어서,다중 인자 기반의 비선형 활성화 함수를 이용하여 내부 네트워크를 구성하는 내부 네트워크 구성부; 및 상기 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습하는 모델 학습부를 포함하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 내부 네트워크 구성부는,다중 인자 기반의 비선형 활성화 함수를 복수 개의 입력 인자와 적어도 하나 이상의 출력 단자를 갖는 멀티 레이어 퍼셉트론(Multilayer Perceptron)으로 모델링하여 내부 네트워크를 구성하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 내부 네트워크 구성부는,상기 구성된 내부 네트워크를 상기 외부 네트워크의 히든 레이어 사이에 위치시켜 상기 구성된 내부 네트워크와상기 외부 네트워크를 결합하는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에 있어서,공개특허 10-2023-0119984-4-상기 모델 학습부는,상기 다중 인자 기반의 비선형 활성화 함수에 대하여 지도 학습을 사용하여 내부 네트워크를 사전 학습시키는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 모델 학습부는,파라미터 공유를 통해 상기 사전 학습된 내부 네트워크를 외부 네트워크와 결합하여 결합 모델이 생성되도록 내부 네트워크와 외부 네트워크를 동시에 학습시키는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 모델 학습부는,상기 학습된 내부 네트워크를 고정한 다음 상기 학습된 외부 네트워크를 초기화시키고, 상기 초기화된 외부 네트워크를 재학습시키는 것을 특징으로 하는 컴퓨터 장치."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다중인자 비선형 활성화 함수의 배타적 논리연산 학습 가능성 및 활용 방법이 개시된다. 일 실시예에 따른 컴퓨 터 장치에 의해 수행되는 활성화 함수의 학습 방법은, 다중 인자 기반의 비선형 활성화 함수를 이용하여 내부 네 트워크를 구성하는 단계; 및 상기 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습하 는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "아래의 설명은 활성화 함수의 학습 기술에 관한 것이다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "뇌의 뉴런은 단순히 반파 정류를 따르는 선형 필터가 아니며, 분할 정규화, 우연 감지, 역사 의존성과 같은 특 성을 보인다. 시그모이드, tanh, ReLU와 같은 고정된 표준 비선형 활성화 함수 대신 다른 비선형 활성화 함수 들이 더 현실적이고 유용할 수 있다. 특히, 다중 인자 기반의 비선형 활성화 함수에 관심이 대두되고 있다. 여기서 인자는 예를 들어 피드포워드, 측면 또는 피드백 연결과 같은 여러 개별 경로 또는 다른 수상돌기 구획에서 발생하는 입력에 해당할 수 있다. 다중 인자 기반의 비선형 활성화 함수는 한 특징이 다른 특징의 처리를 변조할 수 있게 할 수 있다. 최근에는 단일 뉴런의 단일 수상돌기 구획이 XOR(exclusive-or) 연산을 계산할 수 있다는 것을 보여주었다. 인 공 뉴런이 이러한 기본적인 계산 연산을 계산할 수 없다는 사실은 수십 년 동안 뉴럴 네트워크를 신뢰하지 못하 게 했다. 비록 XOR은 뉴런 네트워크로 계산될 수 있지만, 단일 뉴런조차 머신러닝에서 종종 가정되는 것보다 훨씬 더 정교할 수 있다는 가능성을 강조한다. 많은 단일 인자 기반의 비선형 활성화 함수는 보편적인 계산을 허용하지만, 뇌와 인공 네트워크 모두에 대해 더 빠른 학습과 더 나은 일반화를 가능하게 하는 기술이 요구된다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "다중 인자 기반의 비선형 활성화 함수를 구현하고, 구현된 다중 인자 기반의 비선형 활성화 함수를 이용한 내부 네트워크를 구성하는 방법 및 장치를 제공할 수 있다. 다중 인자 기반의 비선형 활성화 함수를 이용한 내부 네트워크를 임의의 외부 네트워크와 결합한 결합 모델을 학습하는 방법 및 장치를 제공할 수 있다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "컴퓨터 장치에 의해 수행되는 활성화 함수의 학습 방법은, 다중 인자 기반의 비선형 활성화 함수를 이용하여 내 부 네트워크를 구성하는 단계; 및 상기 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습하는 단계를 포함할 수 있다. 상기 내부 네트워크를 구성하는 단계는, 다중 인자 기반의 비선형 활성화 함수를 복수 개의 입력 인자와 적어도 하나 이상의 출력 단자를 갖는 멀티 레이어 퍼셉트론(Multilayer Perceptron)으로 모델링하여 내부 네트워크를 구성하는 단계를 포함할 수 있다. 상기 내부 네트워크를 구성하는 단계는, 기 설정된 사이즈의 컨볼루션을 사용하여 내부 네트워크를 구성하는 단 계를 포함할 수 있다. 상기 학습하는 단계는, 상기 구성된 내부 네트워크를 상기 외부 네트워크의 히든 레이어 사이에 위치시켜 상기 구성된 내부 네트워크와 상기 외부 네트워크를 결합하는 단계를 포함할 수 있다. 상기 학습하는 단계는, 상기 내부 네트워크의 깊이 차원에서 슬라이스 및 연결 작업을 통해 상기 구성된 내부 네트워크와 상기 외부 네트워크를 결합하는 단계를 포함할 수 있다. 상기 학습하는 단계는, 상기 다중 인자 기반의 비선형 활성화 함수에 대하여 지도 학습을 사용하여 내부 네트워 크를 사전 학습시키는 단계를 포함할 수 있다. 상기 학습하는 단계는, 파라미터 공유를 통해 상기 사전 학습된 내부 네트워크를 외부 네트워크와 결합하여 결 합 모델이 생성되도록 내부 네트워크와 외부 네트워크를 동시에 학습시키는 단계를 포함할 수 있다. 상기 학습하는 단계는, 상기 학습된 내부 네트워크를 고정한 다음 상기 학습된 외부 네트워크를 초기화시키고, 상기 초기화된 외부 네트워크를 재학습시키는 단계를 포함할 수 있다. 활성화 함수를 이용한 학습 방법을 상기 컴퓨터 장치에 실행시키기 위해 비-일시적인 컴퓨터 판독가능한 기록 매체에 저장되는 컴퓨터 프로그램을 포함할 수 있다. 컴퓨터 장치는, 다중 인자 기반의 비선형 활성화 함수를 이용하여 내부 네트워크를 구성하는 내부 네트워크 구 성부; 및 상기 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습하는 모델 학습부를 포함할 수 있다. 상기 내부 네트워크 구성부는, 다중 인자 기반의 비선형 활성화 함수를 복수 개의 입력 인자와 적어도 하나 이 상의 출력 단자를 갖는 멀티 레이어 퍼셉트론(Multilayer Perceptron)으로 모델링하여 내부 네트워크를 구성할 수 있다. 상기 모델 학습부는, 상기 구성된 내부 네트워크를 상기 외부 네트워크의 히든 레이어 사이에 위치시켜 상기 구 성된 내부 네트워크와 상기 외부 네트워크를 결합할 수 있다. 상기 모델 학습부는, 상기 다중 인자 기반의 비선형 활성화 함수에 대하여 지도 학습을 사용하여 내부 네트워크 를 사전 학습시킬 수 있다. 상기 모델 학습부는, 파라미터 공유를 통해 상기 사전 학습된 내부 네트워크를 외부 네트워크와 결합하여 결합 모델이 생성되도록 내부 네트워크와 외부 네트워크를 동시에 학습시킬 수 있다. 상기 모델 학습부는, 상기 학습된 내부 네트워크를 고정한 다음 상기 학습된 외부 네트워크를 초기화시키고, 상 기 초기화된 외부 네트워크를 재학습시킬 수 있다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "내부 네트워크로부터 학습된 다중 인자 기반의 비선형 활성화 함수의 패턴을 분석한 결과를 통해 soft XOR 함수 의 패턴을 확인함에 따라 뇌 안의 단일 신경세포가 굉장히 복잡한 비선형적 함수로 동작할 수 있음을 추정할 수 있다. 다중 인자 기반의 비선형 활성화 함수로 구성된 내부 네트워크의 구조가 다양한 노이즈 및 적대적 공격에 더 강 인하다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 실시예를 첨부한 도면을 참조하여 상세히 설명한다. 도 1은 일 실시예에 있어서, 컴퓨터 장치의 구성을 도시하는 도면이다. 도 1을 참조하면, 컴퓨터 장치는 입력 모듈, 출력 모듈, 메모리, 또는 프로세서 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 컴퓨터 장치의 구성 요소들 중 적어도 하나가 생략될 수 있으며, 적어도 하나의 다른 구성 요소가 추가될 수 있다. 어떤 실시예에서, 컴퓨터 장치의 구성 요소 들 중 적어도 두 개가 하나의 통합된 회로로 구현될 수 있다. 입력 모듈은 컴퓨터 장치의 적어도 하나의 구성 요소에 사용될 신호를 입력할 수 있다. 입력 모듈 은, 사용자가 컴퓨터 장치에 직접적으로 신호를 입력하도록 구성되는 입력 장치, 주변의 변화를 감지 하여 신호를 발생하도록 구성되는 센서 장치, 또는 외부 기기로부터 신호를 수신하도록 구성되는 수신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 입력 장치는 마이크로폰(microphone), 마우스(mouse) 또는 키보드 (keyboard) 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 입력 장치는 터치를 감지하도록 설정된 터치 회로(touch circuitry) 또는 터치에 의해 발생되는 힘의 세기를 측정하도록 설정된 센서 회로 중 적어도 하나를 포함할 수 있다. 이때, 입력 모듈은 PPG 센서를 포함할 수 있다. 출력 모듈은 컴퓨터 장치의 외부로 정보를 출력할 수 있다. 출력 모듈은, 정보를 시각적으로 출력하도록 구성되는 표시 장치, 정보를 오디오 신호로 출력할 수 있는 오디오 출력 장치, 또는 정보를 무선으 로 송신할 수 있는 송신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 표시 장치는 디스플레이, 홀로그 램 장치 또는 프로젝터 중 적어도 하나를 포함할 수 있다. 일 예로, 표시 장치는 입력 모듈의 터치 회로또는 센서 회로 중 적어도 하나와 조립되어, 터치 스크린으로 구현될 수 있다. 예를 들면, 오디오 출력 장치는 스피커 또는 리시버 중 적어도 하나를 포함할 수 있다. 일부 실시예들에 따르면, 수신 장치와 송신 장치는 통신 모듈로 구현될 수 있다. 통신 모듈은 컴퓨터 장치(10 0)에서 외부 기기와 통신을 수행할 수 있다. 통신 모듈은 컴퓨터 장치와 외부 기기 간 통신 채널을 수립 하고, 통신 채널을 통해, 외부 기기와 통신을 수행할 수 있다. 여기서, 외부 기기는 차량, 위성, 기지국, 서버 또는 다른 컴퓨터 시스템 중 적어도 하나를 포함할 수 있다. 통신 모듈은 유선 통신 모듈 또는 무선 통신 모듈 중 적어도 하나를 포함할 수 있다. 유선 통신 모듈은 외부 기기와 유선으로 연결되어, 유선으로 통신할 수 있다. 무선 통신 모듈은 근거리 통신 모듈 또는 원거리 통신 모듈 중 적어도 하나를 포함할 수 있다. 근거리 통신 모듈은 외부 기기와 근거리 통신 방식으로 통신할 수 있다. 예를 들면, 근거리 통신 방식은, 블루투스 (Bluetooth), 와이파이 다이렉트(WiFi direct), 또는 적외선 통신(IrDA; infrared data association) 중 적어 도 하나를 포함할 수 있다. 원거리 통신 모듈은 외부 기기와 원거리 통신 방식으로 통신할 수 있다. 여기서, 원거리 통신 모듈은 네트워크를 통해 외부 기기와 통신할 수 있다. 예를 들면, 네트워크는 셀룰러 네트워크, 인터넷, 또는 LAN(local area network)이나 WAN(wide area network)과 같은 컴퓨터 네트워크 중 적어도 하나를 포함할 수 있다. 메모리는 컴퓨터 장치의 적어도 하나의 구성 요소에 의해 사용되는 다양한 데이터를 저장할 수 있다. 예를 들면, 메모리는 휘발성 메모리 또는 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 데이터는 적어도 하나의 프로그램 및 이와 관련된 입력 데이터 또는 출력 데이터를 포함할 수 있다. 프로그램은 메모리 에 적어도 하나의 명령을 포함하는 소프트웨어로서 저장될 수 있으며, 운영 체제, 미들 웨어 또는 어플리 케이션 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리의 프로그램을 실행하여, 컴퓨터 장치의 적어도 하나의 구성 요소를 제어할 수 있다. 이를 통해, 프로세서는 데이터 처리 또는 연산을 수행할 수 있다. 이 때, 프로세서는 메 모리에 저장된 명령을 실행할 수 있다. 다양한 실시예들에 따르면, 프로세서는 다중 인자 기반의 비선형 활성화 함수를 이용하여 구성된 내부 네 트워크를 임의의 외부 네트워크와 결합하여 생성된 결합 모델을 학습하도록 구성된 것일 수 있다. 이러한 프로 세서에 대한 설명은 도 2및 도 3에서 보다 상세하게 설명하기로 한다. 도 2는 일 실시예에 있어서, 프로세서의 구성을 설명하기 위한 블록도이고, 도 3은 일 실시예에 있어서, 비선형 활성화 함수의 학습 방법을 설명하기 위한 흐름도이다. 컴퓨터 장치의 프로세서는 내부 네트워크 구성부 및 모델 학습부를 포함할 수 있다. 이러 한 프로세서의 구성요소들은 컴퓨터 장치에 저장된 프로그램 코드가 제공하는 제어 명령에 따라 프로 세서에 의해 수행되는 서로 다른 기능들(different functions)의 표현들일 수 있다. 프로세서 및 프 로세서의 구성요소들은 도 3의 비선형 활성화 함수의 학습 방법이 포함하는 단계들(310 내지 320)을 수행 하도록 컴퓨터 장치를 제어할 수 있다. 이때, 프로세서 및 프로세서의 구성요소들은 메모리가 포함하는 운영체제의 코드와 적어도 하나의 프로그램의 코드에 따른 명령(instruction)을 실행하도록 구현될 수 있다. 프로세서는 비선형 활성화 함수의 학습 방법을 위한 프로그램의 파일에 저장된 프로그램 코드를 메모리에 로딩할 수 있다. 예를 들면, 컴퓨터 장치에서 프로그램이 실행되면, 프로세서는 운영체제의 제어에 따라 프로그램의 파일로부터 프로그램 코드를 메모리에 로딩하도록 컴퓨터 장치를 제어할 수 있다. 이때, 내부 네트워크 구성부 및 모델 학습부 각각은 메모리에 로딩된 프로그램 코드 중 대응하는 부분의 명 령을 실행하여 이후 단계들(310 내지 320)을 실행하기 위한 프로세서의 서로 다른 기능적 표현들일 수 있 다. 단계에서 내부 네트워크 구성부는 다중 인자 기반의 비선형 활성화 함수를 이용하여 내부 네트워크를 구성할 수 있다. 우선적으로, 도 4를 참고하여, 다중 인자 기반의 비선형 활성화 함수의 개념을 설명하기로 한 다. 도 4에는 뉴런들(검정색 동그라미)과 뉴런들을 연결하는 비선형 활성화 함수(보라색 삼각형)가 구성되어 있다. 내부 네트워크에 의해 유연하게 비선형 입력 출력 변환이 파라미터화될 수 있다. 이때, 내부 네트워크 는 주어진 셀 유형의 모든 레이어와 모든 노드에서 공유되는 파라미터가 있는 복잡한 뉴런으로 구성된 기존의 외부 네트워크에서 호출되는 서브루틴이 된다. 실시예에서는 복수 개의 입력과 적어도 하나의 출력을 갖는 다 중 인자 기반의 비선형 활성화 함수를 예를 들어 설명하기로 한다. 다중 인자 기반의 비선형 활성화 함수에서다중 인자는 특징의 서로 다른 선형 가중치 합이고, 정점 및 기저 수상돌기와 같은 고유한 입력에 해당될 수 있 다. 내부 네트워크 구성부는 다중 인자 기반의 비선형 활성화 함수를 복수 개의 입력 인자와 적어도 하나 이상의 출력 단자를 갖는 멀티 레이어 퍼셉트론(Multilayer Perceptron)으로 모델링하여 내부 네트워크를 구성 할 수 있다. 내부 네트워크 구성부는 기 설정된 사이즈의 컨볼루션을 사용하여 내부 네트워크를 구성할 수 있다. 단계에서 모델 학습부는 구성된 내부 네트워크를 외부 네트워크와 결합하여 생성된 결합 모델을 학습 할 수 있다. 모델 학습부는 구성된 내부 네트워크를 외부 네트워크의 히든 레이어 사이에 위치시켜 구성 된 내부 네트워크와 외부 네트워크를 결합할 수 있다. 예를 들면, 모델 학습부는 구성된 내부 네트워크를 리커런트(recurrent), 레지듀얼(residual) 네트워크와 같은 임의의 외부 네트워크(outer network)와 결합할 수 있다. 모델 학습부는 내부 네트워크의 전후의 깊이 차원에서 슬라이스 및 연결 작업을 통해 구성된 내부 네트워크와 외부 네트워크를 결합할 수 있다. 이러한, 내부 네트워크와 외부 네트워크가 결합되는 동작에 대하 여 도 5를 참고하기로 한다. 이하에서는, 내부 네트워크와 외부 네트워크가 결합된 전체 뉴럴 네트워크를 결합 모델이라고 기재하기로 한다. 또한, 모델 학습부는 다중 인자 기반의 활성화 함수에 대하여 지도 학습을 사용하여 내부 네트워크를 사전 학습시킬 수 있다. 모델 학습부는 파라미터 공유를 통해 사전 학습된 내부 네트워크를 외부 네트워크와 결합하여 결합 모델이 생성되도록 내부 네트워크와 외부 네트워크를 동시에 학습시킬 수 있다. 모델 학습부 는 학습된 내부 네트워크를 고정한 다음 학습된 외부 네트워크를 초기화시키고, 초기화된 외부 네트워크를 재학습시킬 수 있다. 이러한, 학습 동작에 대하여 도 6을 참고하기로 한다. 도 5를 참고하면, 내부 네트워크와 외부 네트워크를 결합하는 구조를 설명하기 위한 도면이다. 다중 인자 기반 의 비선형 활성화 함수를 정의하기 위해 내부 네트워크와 외부 네트워크에 대하여 설명하기로 한다. 내부 네트워크는 복수 개의 입력과 적어도 하나의 출력을 갖는 임의의 다중 인자 기반의 비선형 활성화 함수를 학습할 수 있다. 이러한 다중 인자 기반의 비선형 활성화 함수는 ReLU와 같은 일반적인 스칼라 활성화 함수를 대체할 수 있게 된다. 외부 네트워크는 비선형 활성화 함수 외에 모델 구조의 나머지를 참조한다. 두 개의 분 리된 네트워크로 구성된 프레임워크는 멀티 레이어 퍼셉트론(MLP), 컨볼루션 신경망(CNN), ResNets 등과 같은 다양한 뉴럴 아키텍처를 외부 네트워크로 사용할 수 있기 때문에 유연하고 일반적이다. 반면에, 내부 네트워크 의 경우, 64개의 유닛이 있는 2개의 히든 레이어를 갖는 멀티 레이어 퍼셉트론을 사용하고 ReLU 비선형성을 따 른다. 멀티 레이어 퍼셉트론은 피드포워드 딥 뉴럴 네트워크에 일반적으로 사용되는 고정 표준 비선형 활성화 함수와 유사하게 모든 레이어에 걸쳐 공유될 수 있다. CNN 기반 외부 네트워크를 테스트할 때, 결합 모델을 완 전 컨볼루션하게 만들기 위해 내부 네트워크에 대해 멀티 레이어 퍼셉트론 대신 1Х1 컨볼루션을 사용하지만 2 레이어 멀리 레이어 퍼셉트론과 동일하다. 이러한 프레임워크에서, 1Х1 컨볼루션은 내부 네트워크에 대한 입 력이 채널별 특징임을 암시한다. 도 5를 통해 다중 인자 기반의 비선형 활성화 함수가 외부 네트워크와 결합되는 방식이 비교 및 설명될 수 있다. 도 5에서 비선형 활성화 함수는 빨간색 상자로 색으로 구분되며 빨간색 상자를 제외한 나머지 검은색은 외부 네트워크의 요소를 나타낸다. 도 5(a)는 기존의 ReLU 함수를 모듈화한 것으로, 모듈화된 ReLU 함수가 비 선형 활성화 함수가 있는 멀티 레이어 퍼셉트론 기반 외부 네트워크와 결합된 것을 나타낸 것이다. 도 5(b)는 복수 개의 입력 인자를 갖는 멀티 레이어 퍼셉트론 기반 내부 네트워크(예를 들면, n = 2)와 멀티 레이어 퍼셉 트론 기반 외부 네트워크가 결합된 것을 나타낸 것이다. 이때, 기존의 ReLU 함수를 갖는 내부 네트워크는 복수 개의 입력 인자를 갖는 멀티 레이어 퍼셉트론 기반 내부 네트워크로 대체될 수 있다. 도 5(c)는 1 Х 1 컨볼루 션 기반 내부 네트워크가 컨볼루션 기반 외부 네트워크와 결합된 것을 나타낸 것이다. 내부 네트워크는 다양한 특징 맵에서 입력을 받는다. 컨볼루션 기반 외부 네트워크에는 내부 네트워크 전후의 깊이 차원에서 슬라이스 및 연결 작업이 필요하다. 도 6을 참고하면, 내부 네트워크와 외부 네트워크로 구성된 결합 모델을 학습하는 동작을 설명하기 위한 도면이 다. 사전 학습(세션 1), 내부 및 외부 네트워크 학습(세션 2) 및 고정된 내부 네트워크를 위한 외부 네트워크 학습 (세션 3)이 수행될 수 있다. 도 6(a)은 사전 학습에 관한 것으로, 평활화된 랜덤 초기 활성화 맵을 예측하도록 학습된 다중 인자 기반의 내부 네트워크(초록색)를 나타낸 것이고, 도 6(b)는 내부 네트워크(빨간색) 및 외부 네트워크(검정색)를 동시에 학습시키는 것을 나타낸 것이고, 도 6(c)는 내부 네트워크(회색)가 고정되고 외부 네트워크(검정색)만 재학습시키는 것을 나타낸 것이다. 사전 학습 동작(도 6(a))에 대하여 설명하기로 한다. 먼저, 랜덤 초기 비선형 활성화 함수를 생성한 다음 지도 학습을 사용하여 내부 네트워크가 사전 학습될 수 있다. 충분히 복잡한 랜덤 초기 비선형 활성화 함수로 시작 하기 위해, 입력 공간을 타일링하는 단위 정사각형 5Х5 그리드에 대해 [-1, 1]에서 균일하게 샘플링된 조각 별 상수 랜덤 출력이 생성될 수 있다. 랜덤 평활화된 활성화 맵을 정의하기 위해 2D 가우스 커널 ( = 3 units) 로 출력을 블러 처리할 수 있다. 비선형 활성화 함수는 일치하는 내부 네트워크의 타겟 역할을 한다(도 6(a)). 사전 학습된 다중 인자 기반의 비선형 활성화 함수의 예는 도 6(b)에 도시되어 있다. 파라미터가 다음 학습 단 계로 전달되는 초기화된 내부 네트워크를 생성한다. 내부 및 외부 네트워크 학습 동작(도 6(b))에 대하여 설명하기로 한다. 파라미터 공유를 통해 사전 학습된 내 부 네트워크가 외부 네트워크와 결합될 수 있다. 예를 들면, 내부 네트워크와 외부 네트워크가 결합된 결합 모 델이 이미지 분류 작업에 적용될 수 있다. 내부 네트워크와 외부 네트워크는 동시에 학습되어 결합 모델이 비 선형 셀 속성이 나타나는 진화적 시간 척도와 유사할 수 있는 것에 학습하도록 한다. 외부 네트워크로는 복수 개(예를 들면, 64개)의 유닛이 있는 세 개의 히든 레이어를 갖는 멀티 레이어 퍼셉트론, 또는 4개의 컨볼루션 레이어가 있는 CNN(스트라이드가 2인 2 × 2의 맥스 풀링을 사용하여, 크기가 3 Х 3이고 스트라이드가 1인 [60, 120, 120, 120] 커널을 포함하는)가 사용될 수 있다. 멀티 레이어 퍼셉트론 또는 컨볼루션 레이어 외에도, 외부 네트워크로 다른 표준 구조 구성요소, 즉, 레이어 정규화, 드롭아웃 등이 사용될 수도 있다. 이와 같이, 결합 모델은 검증 오류가 포화될 때까지 0.001의 학습률로 ADAM가 사용되어 MNIST 및 CIFAR-10 데이 터 셋에 대해 학습될 수 있다. 조기 중지는 20의 윈도우 크기일 때 사용될 수 있다. 학습된 다중 인자 기반의 비선형 활성화 함수 가 포화 시점 또는 최대 에포크에서 동결될 수 있다. 학습된 다중 인자 기반 의 비선형 활성화 함수의 예는 도 7(c)와 같다. 도 7을 참고하면, 학습된 다중 인자 기반의 비선형 활성화 함수의 결과를 나타낸 예이다. 도 7(a)는 입력 분포, 도 7(b)는 사전 학습된 랜덤 초기 활성화 함수, 도 7(c)는 두 개의 서로 다른 데이터 세트 CIFAR-10 및 MNIST에 의해 학습된 서로 다른 다중 인자 기반의 비선형 활성화 함수(컨볼루션 기반 내부 네트워크, 멀티 레이 어 퍼셉트론 기반 내부 네트워크)를 나타낸 예이다. 색상은 입력 분포의 가장 잘 학습된 부분, 즉, 가장 일반 적인 입력 값의 99%에 대해 마스크된 활성화 함수의 출력을 나타낸다. 도 7(d)를 통해 다중 인자 기반의 비선 형 활성화 함수를 갖는 내부 네트워크는 다른 네트워크보다 빠르게 학습함을 확인할 수 있다. 학습된 다중 인자 기반의 비선형 활성화 함수에 대한 직관을 얻기 위해, 추론 시 모든 테스트 데이터에 대해 내 부 네트워크의 비선형 활성화 함수에 대한 모든 입력 값이 수집될 수 있다. 표시를 위해 사전 비선형 활성화 함수의 입력 분포(도 7(a))가 계산되고 입력 분포의 99%가 둘러쌓인 영역에 대한 비선형 활성화 함수를 보여준 다(도 7(c)). 고정된 내부 네트워크를 위한 외부 네트워크 학습 동작(6(c))에 대하여 설명하기로 한다. 다중 인자 기반의 비 선형 활성화 함수가 학습됨에 따라 내부 네트워크를 고정하고 외부 네트워크를 새로운 작업 데이터에 사용하도 록 외부 네트워크가 재학습될 수 있다. 앞서 설명한, 세션 2에서 학습된 파라미터에서 을 차용하여 내부 네트워크를 고정시킨 다음 외부 네트워크를 다시 초기화할 수 있다. 이때, 표준 비선형 활성화 함수가 있 는 딥 뉴럴 네트워크의 일반적인 학습과 같이 외부 네트워크만 학습될 수 있다. 세션 3의 학습 곡선은 세션 2 (도 7(d))에서 관찰한 것과 다르지 않으며, 긴 시간 간격(epoch)에 걸친 학습의 대부분이 외부 네트워크의 파라 미터 변화에 기인한다는 것을 나타낸다. 즉, 다중 인자 기반의 비선형 활성화 함수의 학습은 조기에 종료될 수 있으며, 나머지 학습은 작업 해결에 전념할 수 있다. 세션 2에서 학습된 다중 인자 기반의 비선형 활성화 함수를 매 에포크마다 구성함으로써 내부 네트워크의 구조 적 안정성의 증거를 찾을 수 있다. 도 8을 참고하면, 다중 인자 기반의 비선형 활성화 함수가 일반적으로 1-5 에포크 내에 일반적인 2차원 공간 패턴으로 발달됨을 확인할 수 있다. 이는 활성화 함수의 전체 공간 구조가 학습 과정의 초기에 발생하는 압력에서 상당히 빠르게 나타난다는 것을 시사한다. 실시예에 따르면, 내부 네트워크 및 외부 네트워크는 주어진 인공지능 태스크를 수행함과 동시에 독립적으로 학 습될 수 있다. 실시예에 따른 다중인자 비선형 활성화 함수의 배타적 논리연산 학습 가능성 및 활용 방법에 대한 기술적 효과 를 설명하기 위하여 다른 비선형 활성화 함수와 비교하기로 한다.실시예에서 제안된 다중 인자 기반의 비선형 활성화 함수와 종래의 단일 인자 기반의 비선형 활성화 함수가 비 교될 수 있다. 공정한 비교를 위해 외부 네트워크를 학습시키는 것처럼 도 9에 설명되어 있는 베이스라인 모델 을 학습시킬 수 있다. 도 9(a)는 다중 인자 기반의 비선형 활성화(빨간색)와 함께 유닛이 있는 L 개의 히든 레이어로 구성된 멀티 레이어 퍼셉트론 기반 외부 네트워크이고, 도 9(b)는 각 레이어 에 유닛(파란색)이 있는 L 개의 히든 레이어로 구성된 ReLU가 있는 베이스라인 모델 구조를 나 타낸 것이다. 이때, 베이스라인 모델은 모두 동일한 멀티 레이어 퍼셉트론 또는 CNN 구조를 포함할 수 있다. 즉, 제안된 결합 모델과 동일한 유형과 외부 네트워크 레이어의 수를 사용한다. 여러 구조를 비교할 때 각 레 이어의 히든 유닛 또는 특징 맵의 수를 체계적으로 조정하여 분류 태스크에서 비슷한 수의 학습 가능한 파라미 터를 사용해야 한다. 구체적으로, 다중 인자 기반의 비선형 활성화 함수를 갖는 멀티 레이어 퍼셉트론 기반 외 부 네트워크(도 8(a))는 파라미터를 포함하며, 여기서 x, y 및 은 각각 입력, 출력 차원 및 히든 레이어 의 유닛 개수이다. 마지막 항은 내부 네트워크 파라 미터의 수를 나타낸다. 이는 히든 레이어 L의 수뿐만 아니라 입력 및 출력 차원과도 무관하므로 파라미터 공유 로 인한 결합 모델 복잡성을 증가시키지 않는다. 대조적으로, 두 번째 항 은 파라미터 카운트를 지배하므로, 베이스라인 모델(도 9(b))은 각각 히든 유닛으로 구성되며, 여기서 는 제안된 결합 모델( )의 파라미터 카운트를 근사화하기 위 한 상수이다. 멀티 레이어 퍼셉트론 기반 외부 네트워크에서 파라미터 카운트를 일치시키는 방법은 을 히 든 유닛 대신 컨볼루션 레이어 의 특징 맵의 수로 설정하여 CNN 기반 모델에도 적용될 수 있다. 실시예에 따르면, 내부 네트워크는 임의의 함수 패턴을 학습 가능하도록 함수 근사기로서의 역할을 하게 된다. 도 7(d)를 참고하면, 다중 인자 기반의 비선형 활성화 함수의 학습 성능을 ReLU 또는 단일 인자 기반의 비선형 활성화 함수를 사용하는 네트워크와 비교한 것이다. MNIST와 CIFAR-10에 대한 학습이 4번 반복되어 4개의 다른 모델 성능 샘플이 생성될 수 있다. 4개의 샘플 결과를 평균화하고 학습된 다중 인자 기반의 비선형 활성화 함 수가 있는 내부 네트워크가 전체적으로 강력한 성능을 달성한다는 것을 확인할 수 있다. 특히, 다중 인자 기반 의 비선형 활성화 함수가 있는 내부 네트워크가 ReLU 보다 더 빨리 학습하고 더 나은 점근 성능을 달성하여 학 습된 다중 인자 기반의 비선형 활성화 함수로 인해 내부 네트워크에서 더 나은 유도 편향을 위한 증거를 제공함 을 시사한다. 도 10은 일 실시예에 있어서, 다중 인자 기반의 비선형 활성화 함수에서의 게이팅 연산을 설명하기 위한 예이다. 학습 실험의 4개의 다른 시도가 반복되고 외부 네트워크(예를 들면, 멀티 레이어 퍼셉트론 기반 외부 네트워크 /CNN 기반 외부 네트워크) 내에서 MNIST와 CIFAR-10에 대해 학습된 다중 인자 기반의 비선형 활성화 함수의 샘 플이 수집될 수 있다. 도 10(a) 내지 도 10(d)의 왼쪽 열은 학습된 다중 인자 기반의 비선형 활성화 함수의 예 이고, 각 행은 학습 실험의 다른 반복을 나타낸다. 도 10(a) 내지 도 10(d)에서 학습된 다중 인자 기반의 비선 형 활성화 함수가 2차 함수처럼 신뢰성 있게 형성되며, 이동 및/또는 회전에 따라 달라진다는 것을 나타낸다. 모든 예는 두 입력 인자 간의 상호 작용을 반영하는 사소하지 않은 2차원 구조를 나타낸다. 대다수는 (잠재적 으로 회전된) 흰색 X 모양을 보여 입력 특징 간의 승법 상호 작용을 나타내며 게이팅 상호 작용 또는 소프트 XOR과 일치한다. 도 10(a) 내지 도 10(d)의 오른쪽 열은 왼쪽 열의 학습된 다중 인자 기반의 비선형 활성화 함수에 대응하는 가 장 적합한 2차 방정식이고, 도 10(e)는 Xavier 가중치 초기화에서 생성된 랜덤 활성화 함수이고, 도 10(f)는 비 선형성 곡률의 누적 분포 함수(CDF)이고, 도 10(g)는 음의(XOR 유사) 곡률이 있는 비선형성의 비율을 나타낸 것 이다. 랜덤 함수 셋이라도 우연히 평균 곡률이 0이 아닐 수 있다. 학습된 다중 인자 기반의 비선형 활성화 함수를 갖는 내부 네트워크에 대수적 2차 함수 형태인 을 맞추고 학습된 다중 인자 기반의 비선형 활성화 함수와 최적의 2차 방정식론이 매우 유사한 구조를 가지고 있음을 확인할 수 있다. 이는 공간 패턴의 회전이 다른 경 우에도 마찬가지이다. 다음으로 관찰된 내부 네트워크 출력 응답의 특수성이 검증될 수 있다. 학습된 다중 인자 기반의 비선형 활성 화 함수가 랜덤 함수에 의해 생성된 비선형 활성화 함수와는 상당히 다르다는 것을 눈으로 확인할 수 있다(도 7(b) 내지 도 7(c)). 그러나 학습된 다중 인자 기반의 비선형 활성화 함수의 이러한 규칙적인 패턴은 종래의 Xavier 가중치 초기화 같은 대중적인 네트워크 초기화 방법을 통해서도 획득될 수 있다. 따라서 이 두 가능성 사이를 구별하기 위해 학습된 다중 인자 기반의 비선형 활성화 함수가 Xavier 랜덤 초기화된 내부 네트워크와 비교될 수 있다(도 10(e)). Xavier 랜덤 초기 활성화가 실시예에서 설명한 직접 생성한 것처럼 \"랜덤\"하지는 않지만(도 7(b)), 학습된 다중 인자 기반의 비선형 활성화 함수에서 관찰된 규칙적인 2차 패턴과는 거리가 멀다 는 것이 확인될 수 있다(도 10(e)). 평활화된 2차 패턴을 표시하도록 진화하여(도 8(b)) 관찰되는 2차 구조는 표준 가중치 초기화 방식에 의해 포착되지 않고 대신 최적화 프로세스에 의해 선호된다는 것을 시사한다. 학습된 2차 함수가 통계적으로 유의미한 하위 구조(예를 들면, 쌍곡선 대 타원 또는 음수 대 양의 곡률)를 가지 는지 테스트하기 위해, 위의 2차 형태인 에 의해 암시되는 곡률이 계산될 수 있다(도 10(f) 내지 도 10(g)). 컨볼루션 기반 내부 네트워크는 두 작업 모두에 대해 음의 곡률을 갖는 다중 인자 기반의 비 선형 활성화 함수를 학습했는데, 이는 48번의 시행 중 총 78% (어느 곡률에서 반반의 확률을 가지는 이항 null 분포에 따라 p = 0.007)이다. 이것은 입력 특징 간의 곱셈 상호작용을 나타내며 게이팅 상호작용 또는 소프트 XOR과 일치한다. 대조적으로, 멀티 레이어 퍼셉트론 기반의 내부 네트워크 구조는 더 많은 양의 곡선을 만들어 냈지만, 이것들은 통계적으로 유의하지 않았다(동일한 테스트에 의해 p = 0.06). 도 11은 일 실시예에 있어서, 일반적인 이미지 손상에 대한 다중 인자 기반의 비선형 활성화 함수의 견고성을 설명하기 위한 예이다. CIFAR-10-C는 일반적인 이미지 손상에 대한 분류기의 견고성을 측정하도록 설계될 수 있으며, 5가지 다른 심각 도 수준에서 각 CIFAR-10 검증 이미지에 적용된 15가지 다른 손상 유형을 포함한다. 이때, CIFAR-10-C의 견고 성 성능은 손상 오류 CE에 의해 측정될 수 있다. 도 11에 CIFAR-10-C 및 컨볼루션 기반 외부 네트워크에서 다양한 손상의 손상 오류(CE, 막대), mCE(검은색 실선) 및 상대 mCE(검은색 점선)이 표시되어 있다. mCE는 노이즈, 흐림, 날씨 및 디지털 범주에서 손상의 평균 손상 오류를 나타낸다. 도 11에서 볼 수 있듯이 다중 인자 기반의 비선형 활성화 함수는 ReLU 기준 모델(mCE = 91.3%)에 비해 견고성을 크게 향상시킨 것을 확인할 수 있다. 100보다 낮은 mCE 점수는 참조 모델보다 손상된 분포를 일반화하는 데 더 성공적임을 나타낸다. 또한, 상대 mCE(= 99.5%, 100 미만)는 손상이 있는 경우 제안된 모델의 정확도 감소가 ReLU가 있는 네트워크의 정확도 감소보다 평균적으로 작음을 보여준다. 결과는 이러한 손상 견고성 개선이 깨끗한 이미지에 대한 단순 한 모델 정확도 개선뿐만 아니라 자연 손상에 대한 ReLU보다 학습 가능한 다중 인자 기반의 비선형 활성화 함수 의 더 강력한 표현에 기인한다는 것을 시사한다. 또한, 4가지 공격의 앙상블로 수행되어 모든 공격의 하이퍼 파라미터가 데이터 세트 및 모델에 대한 모든 실험에 대해 고정되어 있는 적대적 견고성이 안정적으로 평가될 수 있다. 4가지의 공격 중 적어도 하나가 적대적인 예를 찾을 경우, 공격 성공으로 간주하게 된다. 이에, 다 중 인자 기반의 비선형 활성화 함수가 있는 내부 네트워크와 ReLU 비선형 함수를 사용하는 베이스 모델 간의 견 고성 차이의 계산을 통해 다중 인자 기반의 비선형 활성화 함수가 있는 내부 네트워크에서 더 큰 견고성을 나타 냄을 도출할 수 있다. 실시예에 따른 다중인자 비선형 활성화 함수의 배타적 논리연산 학습 가능성 및 활용 방법에 대한 기술적 효과 는 다음과 같이 설명될 수 있다. 소프트 XOR이 입력의 한 입력 차원을 선택하고 다른 입력 차원에 의해 출력을 변조하거나 게이트하는 출력으로 해석될 수 있기 때문에 게이팅과 같은 함수가 학습된 다중 인자 기반의 비선형 활성화 함수에서 자동으로 나타나는 것을 확인할 수 있다. 이러한 학습된 다중 인자 기반의 비선형 활성화 함 수가 포함된 내부 네트워크는 더 빨리 학습하고 더 강력하게 된다. 다중 인자 기반의 비선형 활성화 함수는 내부 네트워크에 약간의 복잡성을 추가하지만 전체적으로 내부 네트워 크의 파라미터는 외부 네트워크의 모든 뉴런에서 공유되기 때문에 파라미터의 개수가 적다. 또한, 학습된 대수다항식 근사를 이용하여 다중 인자 기반의 비선형 활성화 함수는 실제 애플리케이션에서 내부 네트워크의 파라 미터의 수와 메모리 요구사항을 모두 줄일 수 있다. 이상에서 설명된 장치는 하드웨어 구성요소, 소프트웨어 구성요소, 및/또는 하드웨어 구성요소 및 소프트웨어 구성요소의 조합으로 구현될 수 있다. 예를 들어, 실시예들에서 설명된 장치 및 구성요소는, 예를 들어, 프로 세서, 콘트롤러, ALU(arithmetic logic unit), 디지털 신호 프로세서(digital signal processor), 마이크로컴 퓨터, FPGA(field programmable gate array), PLU(programmable logic unit), 마이크로프로세서, 또는 명령 (instruction)을 실행하고 응답할 수 있는 다른 어떠한 장치와 같이, 하나 이상의 범용 컴퓨터 또는 특수 목적 컴퓨터를 이용하여 구현될 수 있다. 처리 장치는 운영 체제(OS) 및 상기 운영 체제 상에서 수행되는 하나 이상 의 소프트웨어 애플리케이션을 수행할 수 있다. 또한, 처리 장치는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 처리 장치는 하나가 사용되는 것으로 설"}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "명된 경우도 있지만, 해당 기술분야에서 통상의 지식을 가진 자는, 처리 장치가 복수 개의 처리 요소 (processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 처리 장치 는 복수 개의 프로세서 또는 하나의 프로세서 및 하나의 콘트롤러를 포함할 수 있다. 또한, 병렬 프로세서 (parallel processor)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 소프트웨어는 컴퓨터 프로그램(computer program), 코드(code), 명령(instruction), 또는 이들 중 하나 이상의 조합을 포함할 수 있으며, 원하는 대로 동작하도록 처리 장치를 구성하거나 독립적으로 또는 결합적으로 (collectively) 처리 장치를 명령할 수 있다. 소프트웨어 및/또는 데이터는, 처리 장치에 의하여 해석되거나 처리 장치에 명령 또는 데이터를 제공하기 위하여, 어떤 유형의 기계, 구성요소(component), 물리적 장치, 가상 장치(virtual equipment), 컴퓨터 저장 매체 또는 장치에 구체화(embody)될 수 있다. 소프트웨어는 네트워크로 연결된 컴퓨터 시스템 상에 분산되어서, 분산된 방법으로 저장되거나 실행될 수도 있다. 소프트웨어 및 데이터 는 하나 이상의 컴퓨터 판독 가능 기록 매체에 저장될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다."}
{"patent_id": "10-2022-0016415", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2022-0016415", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 있어서, 컴퓨터 장치의 구성을 도시하는 도면이다. 도 2는 일 실시예에 있어서, 프로세서의 구성을 설명하기 위한 블록도이다. 도 3은 일 실시예에 있어서, 비선형 활성화 함수의 학습 방법을 설명하기 위한 흐름도이다. 도 4는 일 실시예에 있어서, 다중 인자 기반의 비선형 활성화 함수의 개념을 설명하기 위한 도면이다. 도 5는 일 실시예에 있어서, 내부 네트워크와 외부 네트워크를 결합하는 구조를 설명하기 위한 도면이다. 도 6은 일 실시예에 있어서, 내부 네트워크와 외부 네트워크로 구성된 결합 모델을 학습하는 동작을 설명하기 위한 도면이다. 도 7은 일 실시예에 있어서, 학습된 다중 인자 기반의 활성화 함수의 결과를 설명하기 위한 예이다. 도 8은 일 실시예에 있어서, 학습된 다중 인자 기반의 활성화 함수의 2차원 공간 패턴으로 발달하는 과정을 설 명하기 위한 예이다. 도 9는 일 실시예에 있어서, 파라미터 카운트를 위한 베이스라인 모델의 구조를 설명하기 위한 예이다. 도 10은 일 실시예에 있어서, 다중 인자 기반의 비선형 활성화 함수에서의 게이팅 연산을 설명하기 위한 예이다. 도 11은 일 실시예에 있어서, 일반적인 이미지 손상에 대한 다중 인자 기반의 비선형 활성화 함수의 견고성을 설명하기 위한 예이다."}
