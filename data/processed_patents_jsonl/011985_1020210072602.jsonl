{"patent_id": "10-2021-0072602", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0164177", "출원번호": "10-2021-0072602", "발명의 명칭": "발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템 및 방법", "출원인": "한국전력공사", "발명자": "황명하"}}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자의 검색어 입력에 따른 다수의 제 1 단어(Wordxi)를 추출하는 전처리 기능 수행부(110);미리 산출되는 학습 데이터 및 단어 임베딩 모델을 이용하여 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 단어 추천 기능 수행부(120); 및 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 이용하여 추천 문서를 선정하는 문서 추천기능 수행부(130);를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 전처리 기능 수행부(110)는,다수의 상기 제 1 단어(Wordxi)를 구분하는 단위의 각 토큰으로 분리하는 토큰화 모듈(111);상기 각 토큰에 대한 품사를 붙여주는 품사 태깅 모듈(112); 및상기 품사 중에서 다수의 상기 제 1 단어(Wordxi)에 대응하는 다수의 명사를 추출하는 명사 추출 모듈(130);을포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 단어 추천 기능 수행부(120)는,미리 수집한 다수의 발전 운전 전문가 지식 문서에 대한 학습 데이터를 저장하는 학습 데이터베이스(121); 및상기 학습 데이터 및 다수의 상기 제 1 단어(Wordxi)에 상기 단어 임베딩 모델 중 스킵-그램(Skip-gram) 모델을적용하여 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 워드 임베딩 모듈(122);을 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 스킵-그램(Skip-gram) 모델의 로그함수(L(θ))는 수학식(여기서, ,, P(+|t,c)는 다수의 상기 제 1 단어(Wordxi)가 긍정 샘플일 확률이고, P(-공개특허 10-2022-0164177-3-|t,c)는 다수의 상기 제 1 단어(Wordxi)가 부정 샘플일 확률이고,+는 긍정 샘플을 나타내고, t는 타겟 단어이고,c는 문맥 단어이며, vc는 입력층-은닉층을 잇는 가중치 행렬 W의 행벡터, ut는 은닉층-출력층을 잇는 가중치 행렬 W의 열벡터, cp는 중심 단어이다)으로 정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3 항에 있어서,상기 학습 데이터는 발전기 진단시 위치에 따라 보일러(Boiler), 발전기(Electric Generator), 성능(Performance), 가스터빈(Gas Turbine), 및 증기터빈(Steam Turbine)을 갖는 카테고리, 상기 카테고리를 고장진단, 정밀진단, 예방진단, 및 절연진단으로 분류하는 서브 카테고리, 및 상기 서브 카테고리에 해당하는 문서의 개수를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항에 있어서,상기 유사도는 코사인 유사도(Cos(θ))(여기서, θ는 상기 제 1 단어(Wordxi)와 상기 제 2 단어(Wordyi)간의 각도값이다)이고, 다수의 상기 제 2 단어(Wordyi)는 상기 코사인 유사도가 큰 순서대로 출력되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서, 다수의 상기 제 2 단어(Wordyi)중 중복되는 단어는 제외되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 6 항에 있어서,상기 코사인 유사도(Cos(θ)는 수학식 (여기서, WordAi는제 1 단어(Wordxi), WordBi는 제 2 단어(Wordyi)이다)으로 정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 4 항에 있어서,상기 문서 추천 기능 수행부(130)는, 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 기반으로 각 문서(d)의 가중치를 산출하는문서 가중치 모듈(131); 및상기 각 문서(d)로부터 가중치를 더하여 스코어를 나타내는 문서 점수화를 통해 상기 추천 문서를 선정하는 문공개특허 10-2022-0164177-4-서 점수화 모듈(132);을 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 가중치 (Gen2Vecweight)는 수학식 (여기서, d는각 문서이고, D는 전체 문서의 개수며, 이고, t는Wordxi 또는 Wordyi이고, f()는 총빈도이고, w는 각 문서가 포함하는 각 단어이다)으로 정의되는 것을 특징으로하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서,상기 문서 점수화는 수학식 (여기서, T는 Wordxi와 Wordyi의 총 개수이고, K는 양의 자연수이며, Top()은 조회 함수이다)으로 정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템"}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,다수의 상기 제 2 단어 (Wordyi)는 수학식 (여기서, N>K이다)으로정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 시스템."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "(a) 전처리 기능 수행부(110)가 사용자의 검색어 입력에 따른 다수의 제 1 단어(Wordxi)를 추출하는 단계;(b) 단어 추천 기능 수행부(120)가 미리 산출되는 학습 데이터 및 단어 임베딩 모델을 이용하여 다수의 상기 제1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 단계; 및 (c) 문서 추천 기능 수행부(130)가 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 이용하여 추천 문서를 선정하는 단계;공개특허 10-2022-0164177-5-를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 (a) 단계는, (a-1) 토큰화 모듈(111)이 다수의 상기제 1 단어(Wordxi)를 구분하는 단위의 각 토큰으로 분리하는 단계;(a-2) 품사 태깅 모듈(112)이 상기 각 토큰에 대한 품사를 붙여주는 단계; 및(a-3) 명사 추출 모듈(130)이 상기 품사 중에서 다수의 상기제 1 단어(Wordxi)에 대응하는 다수의 명사를 추출하는 단계;를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 13 항에 있어서,상기 (b) 단계는, (b-1) 미리 수집한 다수의 발전 운전 전문가 지식 문서에 대한 학습 데이터를 학습 데이터베이스(121)에 저장하는 단계;(b-2) 워드 임베딩 모듈(122)이 상기 학습 데이터 및 다수의 상기 제 1 단어(Wordxi)에 상기 단어 임베딩 모델중 스킵-그램(Skip-gram) 모델을 적용하는 단계;(b-3) 상기 워드 임베딩 모듈(122)이 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 단계;를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 스킵-그램(Skip-gram) 모델의 로그함수(L(θ))는 수학식(여기서, ,, P(+|t,c)는 다수의 상기 제 1 단어(Wordxi)가 긍정 샘플일 확률이고, P(-|t,c)는 다수의 상기 제 1 단어(Wordxi)가 부정 샘플일 확률이고, +는 긍정 샘플을 나타내고, t는 타겟단어이고, c는 문맥 단어이며, vc는 입력층-은닉층을 잇는 가중치 행렬 W의 행벡터, ut는 은닉층-출력층을 잇는가중치 행렬 W의 열벡터, cp는 중심 단어이다)으로 정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 15 항에 있어서,상기 유사도는 코사인 유사도(Cos(θ)(여기서, θ는 상기 제 1 단어(Wordxi)와 상기 제 2 단어(Wordyi) 간의 각공개특허 10-2022-0164177-6-도 값이다)이고, 다수의 상기 제 2 단어(Wordyi)는 상기 코사인 유사도가 큰 순서대로 출력되는 것을 특징으로하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 16 항에 있어서,상기 (c) 단계는, (c-1) 문서 가중치 모듈(131)이 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 기반으로각 문서(d)의 가중치를 산출하는 단계; 및(c-2) 문서 점수화 모듈(132)이 상기 각 문서(d)로부터 가중치를 더하여 스코어를 나타내는 문서 점수화를 통해상기 추천 문서를 선정하는 단계;를 포함하는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서,상기 가중치 (Gen2Vecweight)는 수학식 (여기서, d는 각 문서이고, D는전체 문서의 개수며, 이고, t는 Wordxi 또는 Wordyi이고, f()는 총빈도이고, w는 각 문서가 포함하는 각 단어이다)으로 정의되는 것을 특징으로 하는 발전 운전 지식서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 19 항에 있어서,다수의 상기 제 2 단어 (Wordyi)는 수학식 (여기서, N>K이다)으로정의되는 것을 특징으로 하는 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처리 방법."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "발전소 운전과 관련한 정보의 지식서비스를 위한 딥러닝 기반 자연어 처리 시스템이 개시된다. 상기 딥러닝 기반 자연어 처리 시스템은, 사용자의 검색어 입력에 따른 다수의 제 1 단어를 추출하는 전처리 기능 수행부, 미리 산 출되는 학습 데이터 및 단어 임베딩 모델을 이용하여 다수의 상기 제 1 단어와 유사도를 갖는 다수의 제 2 단어 를 추출하는 단어 추천 기능 수행부, 및 다수의 상기 제 1 단어 및 다수의 상기 제 2 단어를 이용하여 추천 문서 를 선정하는 문서 추천 기능 수행부를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 기반 지식 정보 처리 기술에 관한 것으로서, 더 상세하게는 발전 운전 지식 서비스를 위한 딥 러닝 기반 자연어 처리 시스템 및 방법에 대한 것이다."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현재 발전분야 기술지원 인력들의 고령화로 인해 전문원들의 지식 및 노하우가 전수되지 못하고 단절될 위험에 처해 있다. 따라서, 발전소 운전 데이터를 해석하는데 있어서 전문원 간 지식의 격차가 크며, 전문원의 지식 및 노하우는 데이터베이스화되지 않고 보고서 및/또는 문서 등에 분산 관리되고 있어 이를 효율적으로 전수하기 쉽 지 않은 환경이다. 또한, 한전이 관할하는 발전소는 주로 해외에 위치하고 있기 때문에 해외 발전소에서 장애가 발생하면 사내 전 문원이 기술 지원을 위해 현장의 발전소에 방문하여 문제점을 진단하고 해결책을 제시하여야 하기 때문에, 시간 과 비용뿐만 아니라 긴급 해결이 필요한 문제에 효과적으로 대응할 수 없다는 한계를 갖고 있다. 이를 위해 자연어 처리 기술이 도입되었다. 자연어 처리 분야 소프트웨어, 하드웨어, 서비스와 관련하여 세계적 인 시장 예상 예측으로써, \"Tractica\"는 2015년 277.2백만 달러로 평가했던 자연어 처리 시장을 2024년까지 2.1 억 달러로, 연평균 25% 성장할 것이라 전망하였다. 이처럼 자연어처리 분야의 높은 수요 덕분에 자연어처리 분 야 시장규모는 증가하고 있는 추세라고 할 수 있다. 자연어 처리 분야에서도 특히 단어간의 유사성을 벡터화하는 워드 임베딩 관련 딥러닝 기술이 각광 받고 있다. 이러한 워드 임베딩 기술 중에서도 2013년 구글에서 개발한 Word2Vec 기술이 주요 기술로 선정되어 다양한 산업 분야에 적용되고 있다. 그런데, 기술적 측면에서 볼 때, 기존 Word2Vec 임베딩 기술은 각 단어 1개에 대한 임베딩만 가능한 기술적 특 징을 가지고 있다. 따라서, Word2Vec 기술이 적용된 검색 시스템에서 문장 단위 검색 시 문장을 구성하는 단어 단위로 검색어를 추출하여 검색하기 때문에 사용자가 의도하는 검색 결과를 도출하기 미흡하다. 그러므로, 다수 의 단어로 구성된 문장 단위 검색을 위해서는 다수의 단어를 통합 반영할 수 있는 임베딩 기술 기반 검색 알고 리즘이 요구되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 1. 한국등록특허번호 제10-2185869호(등록일자: 2020.11.26)"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 위 배경기술에 따른 문제점을 해소하기 위해 제안된 것으로서, 발전소 운전과 관련한 정보의 지식서 비스를 위한 딥러닝 기반 자연어 처리 시스템 및 방법을 제공하는데 그 목적이 있다. 또한, 본 발명은 다수의 단어를 통합 반영할 수 있는 임베딩 기술 기반 검색 알고리즘을 구현할 수 있는 딥러닝 기반 자연어 처리 시스템 및 방법을 제공하는데 다른 목적이 있다."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명은 위에서 제시된 과제를 달성하기 위해, 발전소 운전과 관련한 정보의 지식서비스를 위한 딥러닝 기반 자연어 처리 시스템을 제공한다. 상기 딥러닝 기반 자연어 처리 시스템은, 사용자의 검색어 입력에 따른 다수의 제 1 단어(Wordxi)를 추출하는 전처리 기능 수행부; 미리 산출되는 학습 데이터 및 단어 임베딩 모델을 이용하여 다수의 상기 제 1 단어와 유사도를 갖는 다수의 제 2 단어(Wordyi)를 추출하는 단어 추천 기능 수행부; 및 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 이용하여 추천 문서를 선정하는 문서 추천 기능 수행부;를 포함하는 것을 특징으로 한다. 또한, 상기 전처리 기능 수행부는, 다수의 상기제 1 단어(Wordxi)를 구분하는 단위의 각 토큰으로 분리하는 토큰 화 모듈; 상기 각 토큰에 대한 품사를 붙여주는 품사 태깅 모듈; 및 상기 품사 중에서 다수의 상기 제 1 단어(Wordxi)에 대응하는 다수의 명사를 추출하는 명사 추출 모듈;을 포함하는 것을 특징으로 한다. 또한, 상기 단어 추천 기능 수행부는, 미리 수집한 다수의 발전 운전 전문가 지식 문서에 대한 학습 데이터를 저장하는 학습 데이터베이스; 및 상기 학습 데이터 및 다수의 상기 제 1 단어(Wordxi)에 상기 단어 임베딩 모델 중 스킵-그램(Skip-gram) 모델을 적용하여 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어 (Wordyi)를 추출하는 워드 임베딩 모듈;을 포함하는 것을 특징으로 한다. 또한, 상기 스킵-그램(Skip-gram) 모델의 로그함수(L(θ))는 수학식 (여기서, , , P(+|t,c)는 다수의 상기 제 1 단어(Wordxi)가 긍정 샘플일 확률이고, P(- |t,c)는 다수의 상기 제 1 단어(Wordxi)가 부정 샘플일 확률이고, +는 긍정 샘플을 나타내고, t는 타겟 단어이고, c는 문맥 단어이며, vc는 입력층-은닉층을 잇는 가중치 행렬 W의 행벡터, ut는 은닉층-출력층을 잇는 가중치 행렬 W의 열벡터, cp는 중심 단어이다)로 정의되는 것을 특징으로 한다. 또한, 상기 학습 데이터는 발전기 진단시 위치에 따라 보일러(Boiler), 발전기(Electric Generator), 성능 (Performance), 가스터빈(Gas Turbine), 및 증기터빈(Steam Turbine)를 갖는 카테고리, 상기 카테고리를 고장 진단, 정밀진단, 예방진단, 및 절연진단으로 분류하는 서브 카테고리, 및 상기 서브 카테고리에 해당하는 문서 의 개수를 포함하는 것을 특징으로 한다. 또한, 상기 유사도는 코사인 유사도(Cos(θ))(여기서, θ는 제 1 단어(Wordxi)와 제 2 단어(Wordyi) 간의 각도 값이다)이고, 다수의 상기 제 2 단어(Wordyi)는 상기 코사인 유사도가 큰 순서대로 출력되는 것을 특징으로 한다. 또한, 다수의 상기 제 2 단어(Wordyi)중 상기 코사인 유사도가 큰 단어만을 제외하고 중복되는 단어는 제외되는 것을 특징으로 한다. 또한, 상기 코사인 유사도(Cos(θ)는 수학식 (여기서, WordAi는 제 1 단어(Wordxi), WordBi는 제 2 단어(Wordyi)이다)으로 정의되는 것을 특징으로 한다. 또한, 상기 문서 추천 기능 수행부는, 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어(Wordyi)를 기반 으로 각 문서(d)의 가중치를 산출하는 문서 가중치 모듈; 및 상기 각 문서(d)로부터 가중치를 더하여 스코어를 나타내는 문서 점수화를 통해 상기 추천 문서를 선정하는 문서 점수화 모듈;을 포함하는 것을 특징으로 한다. 또한, 상기 가중치 (Gen2Vecweight)는 수학식 (여기서, d는 각 문서이고, D는 전체 문서의 개수며, 이고, t는 Wordxi 또는 Wordyi이고, f()는 총빈도이고, w는 각 문서가 포함하는 각 단어이다)으로 정의되는 것을 특징으로한다. 또한, 상기 문서 점수화는 수학식 (여기서, T는 Wordxi와 Wordyi의 총 개수이고, K는 양의 자연수이며, Top()은 조회 함수이다)으로 정의되는 것을 특징으로 한다. 또한, 다수의 상기 제 2 단어 (Wordyi)는 수학식 (여기서, N>K이 다)으로 정의되는 것을 특징으로 한다. 다른 한편으로, 본 발명의 다른 일실시예는, (a) 전처리 기능 수행부가 사용자의 검색어 입력에 따른 다수의 제 1 단어(Wordxi)로부터 다수의 명사를 추출하는 단계; (b) 단어 추천 기능 수행부가 미리 산출되는 학습 데이터 및 단어 임베딩 모델을 이용하여 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 단계; 및 (c) 문서 추천 기능 수행부가 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어 (Wordyi)를 이용하여 추천 문서를 선정하는 단계;를 포함하는 것을 특징으로 한다. 또한, 상기 (a) 단계는, (a-1) 토큰화 모듈이 다수의 상기제 1 단어(Wordxi)를 구분하는 단위의 각 토큰으로 분 리하는 단계; (a-2) 품사 태깅 모듈이 상기 각 토큰에 대한 품사를 붙여주는 단계; 및 (a-3) 명사 추출 모듈이 상기 품사 중에서 다수의 상기 제 1 단어(Wordxi)에 대응하는 다수의 명사를 추출하는 단계;를 포함하는 것을 특 징으로 한다. 또한, 상기 (b) 단계는, (b-1) 미리 수집한 다수의 발전 운전 전문가 지식 문서에 대한 학습 데이터를 학습 데 이터베이스에 저장하는 단계; (b-2) 워드 임베딩 모듈이 상기 학습 데이터 및 다수의 상기 제 1 단어(Wordxi)에 상기 단어 임베딩 모델 중 스킵-그램(Skip-gram) 모델을 적용하는 단계; (b-3) 상기 워드 임베딩 모듈이 다수의 상기 제 1 단어(Wordxi)와 유사도를 갖는 다수의 제 2 단어(Wordxi)를 추출하는 단계;를 포함하는 것을 특징으로 한다. 또한, 상기 (c) 단계는, (c-1) 문서 가중치 모듈이 다수의 상기 제 1 단어(Wordxi) 및 다수의 상기 제 2 단어 (Wordyi)를 기반으로 각 문서(d)의 가중치를 산출하는 단계; 및 (c-2) 문서 점수화 모듈이 상기 각 문서(d)로부 터 가중치를 더하여 스코어를 나타내는 문서 점수화를 통해 상기 추천 문서를 선정하는 단계;를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 발전 운전원 및 발전 분야 신입사원들이 발전소내 기기들을 진단할 시에 20년간 축적된 전문 가 문서들을 적은 시간 내에 추출할 수 있으며, 발전소 내에 전문가가 없어도 전문가들의 노하우를 배울 수 있 고 현장에서 쉽게 적용할 수 있다. 또한, 본 발명의 다른 효과로서는 단어 추천 기능 및 문서 추천 기능을 각 개인의 검색어와 관련한 최적화 기능 을 제안하여 개인 맞춤형 추천 기능으로 개선할 수 있다는 점을 들 수 있다. 또한, 본 발명의 또 다른 효과로서는 영어, 중국어 등의 다국어로 학습한 프레임워크 장치를 적용함으로써 다국 어 지원을 할 뿐만 아니라, 발전 운전 지식 서비스의 핵심 엔진으로 탑재하여 챗봇 서비스, 음성 기반 검색 서 비스 등의 사용자 친화적인 프레임워크로 적용될 수 있다는 점을 들 수 있다."}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는바, 특정 실시예들을 도면에 예시하고 상세한 설명에 구체적으로 설명하고자 한다. 그러나 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용한다. 제 1, 제 2등의 용어는 다양한 구 성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성요소는 제 2 구성요소로 명명될 수 있고, 유사 하게 제 2 구성요소도 제 1 구성요소로 명명될 수 있다. \"및/또는\" 이라는 용어는 복수의 관련된 기재된 항목들 의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미가 있다. 일반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않아야 한다. 이하 첨부된 도면을 참조하여 본 발명의 일실시예에 따른 발전 운전 지식 서비스를 위한 딥러닝 기반 자연어 처 리 시스템 및 방법을 상세하게 설명하기로 한다. 일반적으로 자연어 처리(Natural Langauge Processing)는 광범위한 인공지능 분야에서 가장 많이 활용되는 기술 중 하나로 부상하고 있다. 최근 자연어 처리 기술의 발전과 데이터의 가치 및 양이 지속적으로 상승함에 따라 인간이 말하고 쓰는 것으로 자연어를 인식할 수 있는 프로그램은 이미 임상 문서, 항공사 예약, 자동차 도로변 지원에 널리 사용되고 있다. 특히, 본 발명의 일실시예에서는 사용자가 검색하는 문장에서 명사 단어를 추출하는 전처리 기능(Preprocessing Function), 딥러닝 기반 워드 임베딩을 활용하여 사용자의 검색어내 여러 단어와 연관된 단어를 추천해주는 단 어 추천 기능(Word Recommendation Function), 그리고 검색어와 연관된 문서를 추천해주는 문서 추천 기능 (Document Recommendation Function)이 포함된 프레임워크를 구현한다. 도 1은 본 발명의 일실시예에 따른 딥러닝 기반 자연어 처리 시스템의 구성 블럭도이다. 도 1을 참조하면, 딥러닝 기반 자연어 처리 시스템은, 전처리 기능 수행부, 단어 추천 기능 수행부, 및 문서 추천 기능 수행부 등을 포함하여 구성될 수 있다. 전처리 기능 수행부는 사용자가 검색창에 검색어를 입력할 시, 입력된 단어를 구분하는 단위의 각 토큰으 로 분리할 수 있는 토큰화 모듈, 각 토큰에 대한 품사를 붙여주는 품사 태깅 모듈 태깅된 품사 중에 서 명사만 추출하는 명사 추출 모듈로 구성된다. 단어 추천 기능 수행부는 수집한 발전 운전 전문가 지식 문서들의 수 만건에 대한 학습 데이터를 저장하는 학습 데이터베이스와 학습 데이터를 명사 추출 모듈로부터 추출한 단어들에 임베딩하여 임베딩 단어 를 생성하는 워드 임베딩 모듈로 구성된다. 학습 데이터베이스는 별도의 데이터베이스 서버로 구성할 수도 있다. 문서 추천 기능 수행부는 명사 추출 모듈로부터 추출된 단어들과 워드 임베딩 모듈로부터 추출 된 단어들을 기반으로 문서의 가중치를 업데이트하는 문서 가중치 모듈과 이로부터 점수를 계산하여 추천 문서를 선정하기 위한 문서 점수화 모듈로 구성된다. 도면 기재된 \"…모듈\" 의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 소프트웨어 및/또는 하드웨어로 구현될 수 있다. 하드웨어 구현에 있어, 상술한 기능을 수행하기 위해 디자인된 ASIC(application specific integrated circuit), DSP(digital signal processing), PLD(programmable logic device), FPGA(field programmable gate array), 프로세서, 마이크로프로세서, 다른 전자 유닛 또는 이들의 조 합으로 구현될 수 있다. 소프트웨어 구현에 있어, 소프트웨어 구성 컴포넌트(요소), 객체 지향 소프트웨어 구성 컴포넌트, 클래스 구성 컴포넌트 및 작업 구성 컴포넌트, 프로세스, 기능, 속성, 절차, 서브 루틴, 프로그램 코 드의 세그먼트, 드라이버, 펌웨어, 마이크로 코드, 데이터, 데이터베이스, 데이터 구조, 테이블, 배열 및 변수 를 포함할 수 있다. 소프트웨어, 데이터 등은 메모리에 저장될 수 있고, 프로세서에 의해 실행된다. 메모리나 프로세서는 당업자에게 잘 알려진 다양한 수단을 채용할 수 있다. 도 2는 도 1에 도시된 전처리 기능 수행부의 처리 과정을 보여주는 흐름도이다. 도 2를 참조하면, 전처리 기능 수행부는 사용자가 검색창에 자신이 검색하기 원하는 단어 또는 문장을 입력한 후 실행된다. 먼저, 사용자가 \"가스터빈의 압축기 블레이드에 균열이 발생했다\"라는 검색어를 검색창에 입력한다(단계 S210). 이후, 토큰화 모듈은 해당 단어 또는 문장에 대해 각 단어를 토큰으로 분리한다(단계 S220). 부연하면, \" 가스터빈의 압축기 블레이드에 균열이 발생했다\"을 \"가스터빈/의/압축기/블레이드/에/균열/이/ 발생/했/다\"로 토큰화(Tokenization)된다. 토큰화는 명사, 조사, 부사 등 기준으로 토큰화를 진행한다. 이후, 품사 태깅 모듈이 각 토큰에 품사(Part Of Speech(POS))를 붙여준다(단계 S230). 부연하면, \"가스 터빈/의/압축기/블레이드/에/균열/이/ 발생/했/다\"를 \"가스터빈/(Noun)의/(Josa)압축기/(Noun)블레이드/(Noun) 에/(Josa)균열/(Noun)이/(Josa) 발생/(Noun)했/(Verb)다/(Eomi)\"로 POS(Part Of Speech) 태깅(tagging)한다. 이후, 명사 추출 모듈이 명사인 토큰만 추출한다(단계 S240). 이를 예시하면 다음과 같다. 표 1 가스터빈 압축기 블레이드 균열 발생 추출된 명사 단어들은 단어 추천 기능의 입력으로 결정된다. 도 3은 도 2에 도시된 전처리 과정을 더 상세하게 보여주는 흐름도이다. 도 3을 참조하면, 사용자가 검색창에 단어 또는 문장을 입력하면, 딥러닝 기반 자연어 처리 시스템은 입력된 단어 또는 문장에 토큰화를 적용하 여 명사들을 추출한다(단계 S210,S220). 이후, 딥러닝 기반 자연어 처리 시스템은 스킵 그램(Skip-gram) 모델을 활용하여 타겟 단어(t)와 문맥 단 어(c)에 대해 긍정 샘플과 부정샘플의 확률을 학습하여 각 토큰을 벡터화한다(단계 S330). 부연하면, 연관 단어 예측을 위해 위드 임베딩 모듈은 Word2Vec의 Skip-gram 모델에서 타겟 단어(t)와 문맥 단어(c)가 긍정 샘 플(Positive Sample)일 확률은 아래 수학식과 같다. 타겟 단어(t)는 학습의 주체가 되는 단어이며, 문맥 단어 (c)는 타겟 단어(t)와의 상관관계를 학습하기 위한 주변 단어이다. 수학식 1"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, +는 긍정 샘플을 나타내고, t는 타겟 단어이고, c는 문맥 단어이며, vc는 입력층-은닉층을 잇는 가중치 행렬 W의 행벡터, ut는 은닉층-출력층을 잇는 가중치 행렬 W의 열벡터이다. 한편, t와 c가 부정 샘플(Negative Sample)일 확률은 아래 수학식과 같다. 수학식 2"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "검색어를 활용하여 최적화된 임베딩 결과를 도출하기 위하여 활용하는 Skip-gram의 로그함수(Loglikelihood Function)는 아래 수학식과 같이 전개할 수 있다. 수학식 3"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, cp는 중심 단어이다.(정의하여 주시기 바랍니다) Skip-gram 모델은 긍정 샘플과 부정 샘플의 확률을 학습하며 각 토큰을 벡터화할 수 있다. 이후, 워드 임베딩 모듈은 위 수학식 3을 최적화하기 위해 학습한 후, 문서 군집 내 단어들을 벡터화한다 (단계 S340). 학습이 완료된 단어들을 대상으로 사용자의 검색어 내 각 명사에 대해 워드 임베딩 모듈을 실행하여 결과 를 추출하고, 사용자에게 결과로 출력하여 보여준다, 결과적으로 사용자 입장에서는 검색어 내 각 단어와 연관 된 단어들을 볼 수 있다(단계 S350). 즉 추천 문서를 사용자에게 보여준다. 도 4는 도 1에 도시된 학습 데이터베이스의 테이블을 보여주는 도면이다. 도 4를 참조하면, 본 발명의 일 실시예에 따른 알고리즘에 적용하기 위한 데이터베이스의 학습 데이터는 약 20년간 각 발전사에서 발전기 를 운영하며 전문가가 직접 발전기를 진단한 문서 수 만건을 대상으로 선정한다. 전문가들은 발전기 진단시 위치에 따라 보일러(Boiler), 발전기(Electric Generator), 성능(Performance), 가 스터빈(Gas Turbine), 증기터빈(Steam Turbine)으로 구분함과 동시에 고장진단(Fault Diagnosis), 정밀진단 (Precision Diagnosis) 등 상세한 분류를 나누어 문서를 작성한 것이다. 특히, 도 4에 도시된 테이블은 2000년도부터 2018년도까지 수집한 발전운전 전문가 진단 문서의 사례이다. 도 5는 도 1에 도시된 문서 추천 기능 수행부의 결과 예시이다. 도 5를 참조하면, 해당 실험은 도 5에 도 시된 표와 같이 전처리 기능 수행부의 결과로 추출된 명사 단어(Wordxi)들을 대상으로 워드 임베딩(word embedding) 모델인 Word2Vec 모델로 사전 학습하였다. 그리고, 이를 적용하여 TopN에 해당하는 임베딩 단어 (Wordyi)를 추출한다. 이때 임베딩 단어는 코사인 유사도가 가장 큰 단어부터 차례대로 출력이 되는데, 본 실험 에서는 N=5로 가정하였으며, 추출된 각 임베딩 단어(Wordyi)는 코사인 유사도값(Cosine Similarity)을 갖는다. 실험 결과로 각 Wordxi와 연관성이 높은 단어들이 Wordyi로 추출된 것을 볼 수 있다. 예로써 ‘가스터빈’ 단어 의 경우, ‘시운전’, ‘군산’, ‘저압’, ‘터빈’, ‘부품’ 단어들이 연관 단어로 추출되었으며, 연관 확률 은 각각 0.88, 0.85, 0.83, 0.81, 0.80인 것을 확인할 수 있다. 그리고, 추출된 단어들 중에 중복된 단어들이 있는데, 코사인 유사도가 높은 단어를 제외한 다른 중복 단어들은 문서 추천 기능에서 사용될 Wordyi에서 제외된다. 도 6은 도 1에 도시된 문서 추천 기능 수행부의 결과 예시이다. 도 6을 참조하면, 사용자가 검색창에 필요 한 단어 혹은 문장을 넣을 경우, 전처리 기능 수행부를 활용하여 명사인 단어(Wordxi)를 추출한다. 추출된 단어들을 학습된 워드 임베딩 모듈에 넣어 각 단어(Wordxi)에 대해 코사인 유사도(Cosine Similarity)가높은 TopN 단어(Wordyi)를 추출한다. 이때 코사인 유사도(Cos(θ))를 구하기 위한 식은 다음 수학식과 같다. 수학식 4"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서,θ는 제 1 단어(Wordxi)와 제 2 단어(Wordyi) 간의 각도 값이며, WordAi는 제 1 단어(Wordxi), WordBi는 제 2 단어(Wordyi)이다이다. 임베딩 단어(Wordyi)는 다음 수학식을 통해 산출된다. 수학식 5"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서, Top은 테이블 중 지정된 상위 갯수(N)를 조회하는 조회 함수이고, N은 양의 자연수이다. 한편, 문서들을 대상으로 각 단어(w), 각 타겟 단어(t), 각 문서(d), 전체 문서의 갯수(D), 총 빈도 (Frequency)(f())를 정의한 후, TF-IDF를 생성한다. TF-TDF는 다음 수학식과 같다. w는 각 문서 내 각 단어를 지칭하며, c는 타겟 단어와의 상관관계를 학습하기 위한 주변단어이다. 수학식 6"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "TF-IDF를 대상으로 데이터 프레임(Data Frame)을 추출하는데, Wordxi와 Wordyi로 이루어진 단어 리스트를 열 (Column) 값으로, 각 문서를 행(Row)값을 갖는다. 해당 테이블 추출 결과의 예시는 다음 표와 같다. 표 2 Wordx1 ... Wordxn Wordy1 ... Wordym Doc10.27 ... 0.32 0.41 ... 0.19 ...... ... ... ... ... ... DocZ0.13 ... 0.11 0.21 ... 0.02 위 데이터 프레임에는 사용자가 직접 검색창에 입력한 단어와 딥러닝으로 추출된 연관 단어(Wordxi)와 딥러닝으 로 추출된 연관 단어(Wordyi)가 포함되어 있다. 따라서, 도 7은 도 1에 도시된 문서 추천 기능 수행부의 성능 비 교표이다. 도 7을 참조하면, 가중치를 다르게 부여할 필요가 있기 때문에 문서 가중치 모듈은 문서 가중 치를 Gen2Vecweight으로 정의할 수 있으며, 이는 다음 수학식과 같다. 이를 활용하여 데이터 프레임내 TF-IDF값을 업데이트한다.수학식 7"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "그 다음 데이터 프레임 내 각 문서에 대한 Gen2Vecweight을 더한 후, TopK 문서를 추출한다. 이를 계산하기 위해 문서 점수화 모듈은 문서 점수화를 Gen2Vecscore로 정의할 수 있다. T를 Wordxi와 Wordyi의 총 개수로 정의하면, Gen2Vecscore은 다음 수학식과 같다. 수학식 8"}
{"patent_id": "10-2021-0072602", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서, K는 양의 자연수이고, N>K이다. 도 6은 도 1에 도시된 문서 추천 기능 수행부의 결과 예시이다. 도 6을 참조하면, 전처리 기능 수행부 의 결과로 추출된 명사 단어들(Wordxi)과 단어 추천 기능에 의해 추천된 단어들(Wordyi)을 대상으로 사전 학습 된 TF-IDF 값을 본 발명에서 제시하는 Gen2Vecweight로 업데이트한다. 그 다음 Gen2Vecscore를 통해 추출된 문 서들을 사용자에게 보여준다. 본 발명의 실험 예시에 활용한 검색어와 관련되어 TopK의 K를 10으로 정의할 때, 추출된 문서들이 도 6에 도시 된다. 도 6을 참조하면, 순위(Rank), 문서 이름(document name), 스코어가 나열된다. 스코어가 높은 순서대로 정렬된다. 도 6에 도시된 결과를 살펴보면, 천연가스 발전소 가스터빈 보고서, 블레이드 손상 보고서, 고온부품 손상 보고 서 등 사용자의 검색어로부터 추출한 명사들과 관련성이 높은 문서들이 추출된 것을 확인할 수 있다. 도 7은 도 1에 도시된 문서 추천 기능 수행부의 성능 비교표이다. 도 7을 참조하여, 결과를 살펴보면, 기 존 문서 추천 방법인 TF-IDF 대비 약 10.8%, Word2Vec 대비 약 3.9% 성능(Accuracy)이 높은 것을 증명할 수 있 다. 또한, 정밀도(precision) 및 리콜(Recall)도 높은 것을 확인할 수 있다. 여기서 정밀도란 정보 검색 분야에 서 검색된 문서들 중 관련 있는 문서들의 비율을 말하며, 리콜이란 정보 검색 분야에서 관련 있는 문서들 중 실 제로 검색된 문서들의 비율이다. 또한, 여기에 개시된 실시형태들과 관련하여 설명된 방법 또는 알고리즘의 단계들은, 마이크로프로세서, 프로세 서, CPU(Central Processing Unit) 등과 같은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태 로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 (명령) 코드, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 (명령) 코드는 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소 프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프 등과 같은 자기 매체(magnetic media), CD-ROM, DVD, 블루레이 등과 같 은 광기록 매체(optical media) 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 (명령) 코드를 저장하 고 수행하도록 특별히 구성된 반도체 기억 소자가 포함될 수 있다. 여기서, 프로그램 (명령) 코드의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프 리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2021-0072602", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 딥러닝 기반 자연어 처리 시스템의 구성 블럭도이다. 도 2는 도 1에 도시된 전처리 기능 수행부의 처리 과정을 보여주는 흐름도이다. 도 3은 도 2에 도시된 전처리 과정을 더 상세하게 보여주는 흐름도이다. 도 4는 도 1에 도시된 학습 데이터베이스의 테이블을 보여주는 도면이다.도 5는 도 1에 도시된 문서 추천 기능 수행부의 결과 예시이다. 도 6은 도 1에 도시된 문서 추천 기능 수행부의 결과 예시이다. 도 7은 도 1에 도시된 문서 추천 기능 수행부의 성능 비교표이다."}
