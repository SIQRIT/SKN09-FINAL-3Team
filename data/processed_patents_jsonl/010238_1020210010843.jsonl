{"patent_id": "10-2021-0010843", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0107772", "출원번호": "10-2021-0010843", "발명의 명칭": "캐릭터 감정 인식 및 태깅 장치, 캐릭터 감정 인식 및 태깅 방법 및 캐릭터 감정 인식 및 태", "출원인": "주식회사 플랫팜", "발명자": "천애리"}}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "송수신기(transceiver);적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서에 동작 가능하게 연결되어 상기 적어도 하나의 프로세서가 동작들을 수행하도록하는 적어도 하나의 명령어들(instructions)을 저장하는 적어도 하나의 메모리(memory)를 포함하고,상기 동작들은:상기 송수신기를 통하여 사용자 단말로부터 캐릭터 객체를 포함하는 입력 이미지 데이터를 수신하고,상기 입력 이미지 데이터의 특징(feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정카테고리 중 하나의 감정 카테고리로 분류하고,상기 하나의 감정 카테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성하고, 및상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전송하고,상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의통합 AU(action unit)를 상기 특징으로 하여 학습되는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 통합 AU는 인간적 AU 및 캐릭터적 AU를 포함하는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 인간적 AU는 상기 훈련 이미지 데이터에 포함된 캐릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함하는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 캐릭터적 AU는 상기 훈련 이미지 데이터에 포함된 배경 객체 AU 및 상기 훈련 이미지 데이터에 포함된 감정 객체 AU 중 적어도 하나를 포함하는,공개특허 10-2022-0107772-3-캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 감정 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체를 제외한 나머지 객체 중 상기 캐릭터 객체의 에지(edge)와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응되고, 및상기 배경 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체 및 상기 감정 객체 AU를 제외한 나머지 객체에 대응되는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 동작들은:상기 송수신기를 통하여 상기 사용자 단말로부터 분류 옵션 데이터 - 상기 분류 옵션 데이터는 상기 배경 객체AU 및 상기 감정 객체 AU 중 적어도 하나를 선택하기 위한 선택 정보를 포함 -; 를 수신하고,상기 배경 객체 AU 및 상기 감정 객체 AU 중 상기 선택 정보에 대응되는 AU를 선택하고, 및상기 인간적 AU, 상기 선택 정보에 대응되는 AU 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를상기 하나의 감정 카테고리로 분류하는 것을 더 포함하는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제4항에 있어서,상기 동작들은:상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터에 대한 평가 데이터를 수신하고, 및상기 평가 데이터가 기 설정된 임계 값 미만이면, 상기 인간적 AU, 상기 배경 객체 AU 및 상기 감정 객체 AU 중적어도 하나 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카테고리로 분류하는 것을 더 포함하는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2항에 있어서,상기 통합 AU 각각은 상기 복수의 감정 카테고리 중 적어도 하나에 라벨링(labeling)되는,캐릭터 감정 인식 및 태깅 장치."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "캐릭터 감정 인식 및 태깅 장치에 의해 수행되는 캐릭터 감정 인식 및 태깅 방법으로서,공개특허 10-2022-0107772-4-사용자 단말로부터 캐릭터 객체를 포함하는 입력 이미지 데이터를 수신하고;상기 입력 이미지 데이터의 특징(feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정카테고리 중 하나의 감정 카테고리로 분류하고;상기 하나의 감정 카테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성하고; 및상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전송하는 것을 포함하고,상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의통합 AU(action unit)를 상기 특징으로 하여 학습되는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 통합 AU는 인간적 AU 및 캐릭터적 AU를 포함하고,상기 인간적 AU는 상기 훈련 이미지 데이터에 포함된 캐릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함하고, 및상기 캐릭터적 AU는 상기 훈련 이미지 데이터에 포함된 배경 객체 AU 및 상기 훈련 이미지 데이터에 포함된 감정 객체 AU 중 적어도 하나를 포함하는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 감정 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체를 제외한 나머지 객체 중 상기 캐릭터 객체의 에지(edge)와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응되고, 및상기 배경 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체 및 상기 감정 객체 AU를 제외한 나머지 객체에 대응되는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 사용자 단말로부터 분류 옵션 데이터 - 상기 분류 옵션 데이터는 상기 배경 객체 AU 및 상기 감정 객체 AU중 적어도 하나를 선택하기 위한 선택 정보를 포함 -; 를 수신하고;상기 배경 객체 AU 및 상기 감정 객체 AU 중 상기 선택 정보에 대응되는 AU를 선택하고; 및상기 인간적 AU, 상기 선택 정보에 대응되는 AU 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를상기 하나의 감정 카테고리로 분류하는 것을 더 포함하는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2022-0107772-5-제10항에 있어서,상기 태깅된 입력 이미지 데이터에 대한 평가 데이터를 수신하고; 및상기 평가 데이터가 기 설정된 임계 값 미만이면, 상기 인간적 AU, 상기 배경 객체 AU 및 상기 감정 객체 AU 중적어도 하나 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카테고리로 분류하는 것을 더 포함하는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에 있어서,상기 통합 AU 각각은 상기 복수의 감정 카테고리 중 적어도 하나에 라벨링(labeling)되는,캐릭터 감정 인식 및 태깅 방법."}
{"patent_id": "10-2021-0010843", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "사용자 단말; 및상기 사용자 단말로부터 수신한 입력 이미지 데이터에 포함된 캐릭터 객체의 감정을 분류하는 캐릭터 감정 인식및 태깅 장치를 포함하고,상기 캐릭터 감정 인식 및 태깅 장치는:송수신기(transceiver);적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서에 동작 가능하게 연결되어 상기 적어도 하나의 프로세서가 동작들을 수행하도록하는 적어도 하나의 명령어들(instructions)을 저장하는 적어도 하나의 메모리(memory)를 포함하고,상기 동작들은:상기 송수신기를 통하여 상기 사용자 단말로부터 상기 입력 이미지 데이터를 수신하고,상기 입력 이미지 데이터의 특징(feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정카테고리 중 하나의 감정 카테고리로 분류하고,상기 하나의 감정 카테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성하고, 및상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전송하고,상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의통합 AU(action unit)를 상기 특징으로 하여 학습되는,캐릭터 감정 인식 및 태깅 시스템."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 양상으로, 송수신기(transceiver); 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 동 작 가능하게 연결되어 상기 적어도 하나의 프로세서가 동작들을 수행하도록 하는 적어도 하나의 명령어들 (instructions)을 저장하는 적어도 하나의 메모리(memory)를 포함하고, 상기 동작들은: 상기 송수신기를 통하여 (뒷면에 계속)"}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시 (present disclosure)는 인공지능 시각(상황)처리 분야에서의 캐릭터 감정 인식 및 태깅 장치, 캐릭터 감정 인식 및 태깅 방법 및 캐릭터 감정 인식 및 태깅 장치를 포함하는 캐릭터 감정 인식 및 태깅 시스템에 관 한 것이다."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "콘텐츠 분석, 검색 및 제공은 딥러닝 기반의 인공지능 기술 발전으로 급격한 발전이 이루어지고 있으며, 다양한 플랫폼 분야에 적용되어 활용될 것으로 전망된다. 예를 들어, 경험재 콘텐츠의 질과 취향에 대한 불확실성을 줄 이기 위하여, 디지털 콘텐츠의 소비 의사결정과정에 있어서 사용자 맞춤형 콘텐츠가 인공지능 기술에 기반하여 제공될 수 있다. 사용자 맞춤형 콘텐츠는 인공지능 기술, 특히 콘텐츠 관련 기술에서 중요한 키워드로 부각되고 있다. 이와 같이 규모의 경제가 중요한 콘텐츠 시장에 인공지능 기술은 필수적인 요소이다. 또한, 최근에는 콘텐츠 유통/서비스 모바일을 통한 비대면 커뮤니케이션이 증가하고 있는 추세이다. 모바일 콘 텐츠 산업의 발달로 급증하고 있는 콘텐츠 업로드의 속도와 수량에 맞춰 업로드되는 콘텐츠의 분류 및 분석이 필요하다. 다만, 현재 기술은 다양한 콘텐츠들 중 텍스트나 스피치 콘텐츠 쪽에 중점을 두고 있으며, 다양한 멀티미디어에 대한 기술적 발전이 부족한 실정이다. 특히, 캐릭터나 이모티콘 등과 같은 창작물 콘텐츠의 경우, 업로드부터 배포 과정에서 창작물에 포함된 캐릭터의 감정을 자동으로 인식, 분류 및 이를 태깅해줄 수 있는 사용자 맞춤형 콘텐츠 제공 서비스가 전무한 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 10-2018-0111467 (특허문헌 0002) 대한민국 등록특허 10-2110393"}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 다양한 예들은 콘텐츠의 업로드 및 배포 과정에서 콘텐츠에 포함된 캐릭터의 감정을 자동으로 인식 및 태깅할 수 있는 캐릭터 감정 인식 및 태깅 장치, 캐릭터 감정 인식 및 태깅 방법 및 캐릭터 감정 인식 및 태 깅 장치를 포함하는 캐릭터 감정 인식 및 태깅 시스템을 제공하기 위함이다. 본 개시의 다양한 예들에서 이루고자 하는 기술적 과제들은 이상에서 언급한 사항들로 제한되지 않으며, 언급하"}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "지 않은 또 다른 기술적 과제들은 이하 설명할 본 개시의 다양한 예들로부터 당해 기술분야에서 통상의 지식을 가진 자에 의해 고려될 수 있다."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 양상으로, 송수신기(transceiver); 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 동작 가능하게 연결되어 상기 적어도 하나의 프로세서가 동작들을 수행하도록 하는 적어도 하나의 명령어들 (instructions)을 저장하는 적어도 하나의 메모리(memory)를 포함하고, 상기 동작들은: 상기 송수신기를 통하여 사용자 단말로부터 캐릭터 객체를 포함하는 입력 이미지 데이터를 수신하고, 상기 입력 이미지 데이터의 특징 (feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정 카테고리 중 하나의 감정 카테 고리로 분류하고, 상기 하나의 감정 카테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이 미지 데이터를 생성하고, 및 상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전 송하고, 상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의 통합 AU(action unit)를 상기 특징으로 하여 학습되는, 캐릭터 감정 인식 및 태깅 장치이다. 상기 통합 AU는 인간적 AU 및 캐릭터적 AU 를 포함할 수 있다. 상기 인간적 AU는 상기 훈련 이미지 데이터에 포함된 캐릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복 수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수 의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함할 수 있다. 상기 캐릭터적 AU는 상기 훈련 이미지 데이터에 포함된 배경 객체 AU 및 상기 훈련 이미지 데이터에 포함된 감 정 객체 AU 중 적어도 하나를 포함할 수 있다. 상기 감정 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체를 제외한 나머지 객체 중 상기 캐릭터 객 체의 에지(edge)와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응되고, 및 상기 배경 객체 AU는 상기 훈 련 이미지 데이터에서 상기 캐릭터 객체 및 상기 감정 객체 AU를 제외한 나머지 객체에 대응될 수 있다. 상기 동작들은: 상기 송수신기를 통하여 상기 사용자 단말로부터 분류 옵션 데이터 - 상기 분류 옵션 데이터는 상기 배경 객체 AU 및 상기 감정 객체 AU 중 적어도 하나를 선택하기 위한 선택 정보를 포함 -; 를 수신하고, 상기 배경 객체 AU 및 상기 감정 객체 AU 중 상기 선택 정보에 대응되는 AU를 선택하고, 및 상기 인간적 AU, 상 기 선택 정보에 대응되는 AU 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카 테고리로 분류하는 것을 더 포함할 수 있다. 상기 동작들은: 상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터에 대한 평가 데이터를 수신하고, 및 상기 평가 데이터가 기 설정된 임계 값 미만이면, 상기 인간적 AU, 상기 배경 객체 AU 및 상기 감정 객체 AU 중 적어도 하나 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카테고리로 분류하 는 것을 더 포함할 수 있다. 상기 통합 AU 각각은 상기 복수의 감정 카테고리 중 적어도 하나에 라벨링(labeling)될 수 있다. 본 개시의 다른 일 양상으로, 캐릭터 감정 인식 및 태깅 장치에 의해 수행되는 캐릭터 감정 인식 및 태깅 방법 으로서, 사용자 단말로부터 캐릭터 객체를 포함하는 입력 이미지 데이터를 수신하고; 상기 입력 이미지 데이터 의 특징(feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정 카테고리 중 하나의 감 정 카테고리로 분류하고; 상기 하나의 감정 카테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성하고; 및 상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전송하는 것을 포함 하고, 상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의 통합 AU(action unit)를 상기 특징으로 하여 학습될 수 있다. 상기 통합 AU는 인간적 AU 및 캐릭터적 AU를 포함하고, 상기 인간적 AU는 상기 훈련 이미지 데이터에 포함된 캐 릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖 는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함하고, 및 상기 캐릭터적 AU는 상기 훈련 이미지 데이터에 포함된 배경 객체 AU 및 상기 훈련 이미지 데이터에 포함된 감정 객체 AU 중 적어도 하나를 포함할 수 있다. 상기 감정 객체 AU는 상기 훈련 이미지 데이터에서 상기 캐릭터 객체를 제외한 나머지 객체 중 상기 캐릭터 객 체의 에지(edge)와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응되고, 및 상기 배경 객체 AU는 상기 훈 련 이미지 데이터에서 상기 캐릭터 객체 및 상기 감정 객체 AU를 제외한 나머지 객체에 대응될 수 있다. 상기 사용자 단말로부터 분류 옵션 데이터 - 상기 분류 옵션 데이터는 상기 배경 객체 AU 및 상기 감정 객체 AU 중 적어도 하나를 선택하기 위한 선택 정보를 포함 -; 를 수신하고; 상기 배경 객체 AU 및 상기 감정 객체 AU 중 상기 선택 정보에 대응되는 AU를 선택하고; 및 상기 인간적 AU, 상기 선택 정보에 대응되는 AU 및 상기 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카테고리로 분류하는 것을 더 포함할 수 있 다. 상기 태깅된 입력 이미지 데이터에 대한 평가 데이터를 수신하고; 및 상기 평가 데이터가 기 설정된 임계 값 미 만이면, 상기 인간적 AU, 상기 배경 객체 AU 및 상기 감정 객체 AU 중 적어도 하나 및 상기 기 학습된 분류 모 델에 기초하여 상기 캐릭터 객체를 상기 하나의 감정 카테고리로 분류하는 것을 더 포함할 수 있다. 상기 통합 AU 각각은 상기 복수의 감정 카테고리 중 적어도 하나에 라벨링(labeling)될 수 있다. 본 개시의 또 다른 일 양상으로, 사용자 단말; 및 상기 사용자 단말로부터 수신한 입력 이미지 데이터에 포함된 캐릭터 객체의 감정을 분류하는 캐릭터 감정 인식 및 태깅 장치를 포함하고, 상기 캐릭터 감정 인식 및 태깅 장 치는: 송수신기(transceiver); 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서에 동작 가능하게 연결 되어 상기 적어도 하나의 프로세서가 동작들을 수행하도록 하는 적어도 하나의 명령어들(instructions)을 저장 하는 적어도 하나의 메모리(memory)를 포함하고, 상기 동작들은: 상기 송수신기를 통하여 상기 사용자 단말로부 터 상기 입력 이미지 데이터를 수신하고, 상기 입력 이미지 데이터의 특징(feature) 및 기 학습된 분류 모델에 기초하여 상기 캐릭터 객체를 복수의 감정 카테고리 중 하나의 감정 카테고리로 분류하고, 상기 하나의 감정 카 테고리를 상기 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성하고, 및 상기 송수신기를 통하여 상기 태깅된 입력 이미지 데이터를 상기 사용자 단말에 전송하고, 상기 기 학습된 분류 모델은 상기 적어도 하나의 메모리에 저장된 훈련 이미지 데이터에 포함된 캐릭터 객체의 통합 AU(action unit)를 상기 특징으로 하여 학습되는, 캐릭터 감정 인식 및 태깅 시스템이다. 상술한 본 개시의 다양한 예들은 본 개시의 바람직한 예들 중 일부에 불과하며, 본 개시의 다양한 예들의 기술"}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "적 특징들이 반영된 여러 가지 예들이 당해 기술분야의 통상적인 지식을 가진 자에 의해 이하 상술할 상세한 설 명을 기반으로 도출되고 이해될 수 있다."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 다양한 예들에 따르면 다음과 같은 효과가 있다. 본 개시의 다양한 예들에 따르면, 콘텐츠의 업로드 및 배포 과정에서 콘텐츠에 포함된 캐릭터의 감정을 자동으 로 인식 및 태깅할 수 있는 캐릭터 감정 인식 및 태깅 장치, 캐릭터 감정 인식 및 태깅 방법 및 캐릭터 감정 인 식 및 태깅 장치를 포함하는 캐릭터 감정 인식 및 태깅 시스템이 제공될 수 있다. 또한, 사용자(크리에이터) 정보를 토대로 사용자 맞춤 정보를 파악하고 캐릭터 이미지를 분석하여 최적의 감정 인식 태그의 추천이 가능하다. 또한, 캐릭터 이미지 플랫폼에 업로드 된 콘텐츠에 대해 자동으로 태그 정보를 추천하고 감정 카테고리 분류의 정확도가 증대될 수 있다. 또한, 콘텐츠 관리의 효율성이 향상될 수 있다. 본 개시의 다양한 예들로부터 얻을 수 있는 효과들은 이상에서 언급된 효과들로 제한되지 않으며, 언급되지 않"}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "은 또 다른 효과들은 이하의 상세한 설명을 기반으로 당해 기술분야에서 통상의 지식을 가진 자에게 명확하게 도출되고 이해될 수 있다."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 구현들을 첨부된 도면을 참조하여 상세하게 설명한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 발명의 예시적인 구현을 설명하고자 하는 것이며, 본 발명이 실시될 수 있는 유일한 구현 형 태를 나타내고자 하는 것이 아니다. 이하의 상세한 설명은 본 발명의 완전한 이해를 제공하기 위해서 구체적 세 부사항을 포함한다. 그러나 당업자는 본 개시가 이러한 구체적 세부사항 없이도 실시될 수 있음을 안다. 몇몇 경우, 본 개시의 개념이 모호해지는 것을 피하기 위하여 공지의 구조 및 장치는 생략되거나, 각 구조 및 장치의 핵심기능을 중심으로 한 블록도 형식으로 도시될 수 있다. 또한, 본 개시 전체에서 동일한 구성요소에 대해서는 동일한 도면 부호를 사용하여 설명한다. 본 발명의 개념에 따른 다양한 예들은 다양한 변경들을 가할 수 있고 여러 가지 형태들을 가질 수 있으므로 다 양한 예들을 도면에 예시하고 본 개시에 상세하게 설명하고자 한다. 그러나 이는 본 발명의 개념에 따른 다양한 예들을 특정한 개시 형태들에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 변경, 균 등물, 또는 대체물을 포함한다.제1 또는 제2 등의 용어를 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들 에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만, 예를 들어 본 발명의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관 계를 설명하는 표현들, 예를 들어 \"~사이에\"와 \"바로~사이에\" 또는 \"~에 직접 이웃하는\" 등도 마찬가지로 해석 되어야 한다. 본 개시의 다양한 예에서, “/” 및 “,”는 “및/또는”을 나타내는 것으로 해석되어야 한다. 예를 들어, “ A/B”는 “A 및/또는 B”를 의미할 수 있다. 나아가, “A, B”는 “A 및/또는 B”를 의미할 수 있다. 나아가, “A/B/C”는 “A, B 및/또는 C 중 적어도 어느 하나”를 의미할 수 있다. 나아가, “A, B, C”는 “A, B 및/또 는 C 중 적어도 어느 하나”를 의미할 수 있다. 본 개시의 다양한 예에서, “또는”은 “및/또는”을 나타내는 것으로 해석되어야 한다. 예를 들어, “A 또는 B ”는 “오직 A”, “오직 B”, 및/또는 “A 및 B 모두”를 포함할 수 있다. 다시 말해, “또는”은 “부가적으 로 또는 대안적으로”를 나타내는 것으로 해석되어야 한다. 본 개시에서 사용한 용어는 단지 특정한 다양한 예들을 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의 도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 설시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함으로 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적 으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미를 갖는 것으로 해석되어야 하며, 본 개시에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미 로 해석되지 않는다. 이하, 본 개시의 다양한 예들을 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 장치의 블록도이다. 도 1을 참조하면, 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 장치는 송수신기, 적어도 하나 의 프로세서 및 적어도 하나의 메모리를 포함한다. 송수신기는 프로세서와 연결될 수 있고, 유/무선 신호를 송신 및/또는 수신할 수 있다. 예를 들어, 송수신기는 무선 통신망을 통해 사용자 단말과 연결될 수 있다. 여기서, 무선 통신망은 이동 통신망, 무선 LAN, 근거리 무선 통신망 등을 포함할 수 있다. 예를 들어, 무선 통신망은 LTE, LTE-A(LTE Advance), CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), 또는 GSM(Global System for Mobile Communications) 등 중 적어도 하 나를 사용하는 셀룰러 통신을 포함할 수 있다. 예를 들어, 무선 통신망은 WiFi(wireless fidelity), 블루투스, 블루투스 저전력(BLE), 지그비 (Zigbee), NFC(near field communication), 또는 라디오 프리퀀시(RF) 중 적어 도 하나를 포함할 수 있다. 송수신기는 송신기 및 수신기를 포함할 수 있다. 송수신기는 RF(radio frequency) 유닛과 혼용될 수 있다. 송수신기는 프로세서의 제어를 통해 사용자 단말과 다양한 신호를 송수신할 수 있다. 프로세서는 메모리 및/또는 송수신기를 제어하며, 본 개시의 설명, 기능, 절차, 제안, 방법 및/ 또는 동작 순서도들을 구현하도록 구성될 수 있다. 예를 들어, 프로세서는 송수신기를 통해 무선 신 호를 수신하고, 무선 신호에 포함된 정보를 메모리에 저장할 수 있다. 또한, 프로세서는 메모리(13 0)에 저장된 정보를 처리하여 무선 신호를 생성한 뒤, 생성한 무선 신호를 송수신기를 통해 전송할 수 있 다. 메모리는 프로세서와 연결될 수 있고, 프로세서의 동작과 관련한 다양한 정보를 저장할 수 있다. 예를 들어, 메모리는 프로세서에 의해 제어되는 프로세스들 중 일부 또는 전부를 수행하거나, 본 개시의 설명, 기능, 절차, 제안, 방법 및/또는 동작 순서도들을 수행하기 위한 명령들을 포함하는 소프트웨 어 코드를 저장할 수 있다. 이하에서는, 캐릭터 감정 인식 및 태깅 장치의 다양한 동작 예들에 대하여 설명한다. 하기 다양한 동작 예 들은 상술한 적어도 하나의 프로세서의 동작에 포함되는 것일 수 있다. 프로세서는, 송수신기를 통하여 사용자 단말로부터 캐릭터 객체를 포함하는 입력 이미지 데이터 를 수신한다. 본 개시에서, 캐릭터 객체는 실사가 아닌 창작자에 의해 창작된 캐릭터나 이모티콘 등에 해당하는 객체일 수 있다. 입력 이미지 데이터는 캐릭터 객체를 포함하고, 여기에 해당 캐릭터의 감정 상태를 나타내기 위한 감정 객체 및/또는 배경 이미지에 해당하는 배경 객체를 포함할 수도 있다. 프로세서는, 사용자 단말로부터 수신한 입력 이미지 데이터의 특징(feature) 및 기 학습된 분류 모델 에 기초하여 입력 이미지 데이터에 포함된 캐릭터 객체를 복수의 감정 카테고리 중 하나의 감정 카테고리로 분 류할 수 있다. 다시 말해서, 프로세서는 입력 이미지 데이터의 특징을 기 학습된 분류 모델의 입력 데이터 로 하여 캐릭터 객체의 감정을 인식 및 분류할 수 있다. 캐릭터 객체의 감정 인식 및 분류를 위한 복수의 감정 카테고리는 다양하게 설정되거나 정의될 수 있다. 예를 들어, 감정 카테고리는 기쁨, 간절함, 신남, 놀람, 화남, 두려움, 슬픔, 지루함, 피곤함, 실망스러움, 부끄러움, 삐짐, 편안함, 걱정스러움, 아낌, 음흉함, 배고픔, 아픔, 당황함 및 장난스러움 등으로 설정되거나 정의될 수 있다. 상술한 예와 같은 감정 카테고리들 중 어느 하나의 감정 카테고리로 분류될지 여부는, 다양한 AI(artificial intelligence) 알고리즘에 기반하여 수행될 수 있다. 다시 말해서, 상술한 기 학습된 분류 모델의 학습을 위한 학습 알고리즘은 지도 학습(supervised learning), 비지도 학습(unsupervised learning), 강화 학습 (reinforcement learning) 및 전이 학습(transfer learning) 등과 같이 다양한 기계 학습(machine learning) 알고리즘을 포함할 수 있다. 분류 모델의 학습을 위한 특징은 캐릭터 객체의 통합 AU(action unit)일 수 있다. 즉, 프로세서는, 캐릭터 객체의 통합 AU를 특징으로 하여 분류 모델을 학습하고, 학습 결과로 기 학습된 분류 모델을 생성할 수 있다. 본 개시에서, AU는 캐릭터 객체로부터 추출되는 에지(edge) 및 에지에 포함되는 특징점(feature point) 중 적어 도 하나가 이루는 하나의 요소를 의미할 수 있다. 이하에서는, 본 개시에 정의되는 통합 AU에 대하여 보다 구체적으로 설명한다. 도 2 내지 도 5는 본 개시의 다양한 예들에 따라 정의되는 AU(action unit)를 설명하기 위한 것이다. 통합 AU는 훈련 이미지 데이터나 입력 이미지 데이터로부터 추출되는 것으로써, 인간적 AU 및 캐릭터적 AU를 포 함할 수 있다. 여기서, 훈련 이미지 데이터는 분류 모델의 학습을 위해 사용되는 이미지 데이터로써, 메모리 에 기 저장되어 있거나 또는 사용자 단말 외 다른 사용자 기기로부터 수신하는 것일 수 있다. 인간적 AU는 얼굴의 이목구비 등과 같이 표정을 나타내는 요소에 대응되는 것일 수 있고, 캐릭터적 AU는 얼굴의 표정과는 관련 없으나 직간접적으로 캐릭터의 감정을 나타내는 요소에 대응되는 것이며, 편의상 각각 인간적 AU 및 캐릭터적 AU로 칭해질 수 있다. 도 2를 참조하면, 인간적 AU는 분류 모델의 학습을 위한 훈련 이미지 데이터나 입력 이미지 데이터에 포함된 캐 릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖 는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함할 수 있다. 눈썹 AU, 눈 AU, 코 AU, 입 AU 및 볼 AU는 캐릭터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 및 에지 중 적어도 하나를 포함하는 것일 수 있다. 눈썹 AU, 눈 AU, 코 AU, 입 AU 및 볼 AU는 감정에 따라 다양 하게 설정되거나 정의될 수 있다. 도 3을 참조하면, 예를 들어, 슬픔 감정 카테고리에 해당하는 캐릭터 객체는 찡그러진 눈썹 AU, 감은 눈 AU, 커진 코 AU 및/또는 M자형 입 AU 등을 포함할 수 있다. 예를 들어, 화남 감정 카테고리에 해당하는 캐릭터 객체는 화난 눈 AU, 커진 코 AU 및/또는 벌린 입 AU 등을 포함할 수 있다.예를 들어, 기쁨 감정 카테고리에 해당하는 캐릭터 객체는 웃는 눈 AU 및/또는 U자형 입 AU 등을 포 함할 수 있다. 도 4 및 도 5를 참조하면, 캐릭터적 AU는 분류 모델의 학습을 위한 훈련 이미지 데이터나 입력 이미지 데이터에 포함된 배경 객체 AU 및 감정 객체 AU 중 적어도 하나를 포함할 수 있다. 배경 객체 AU 및 감정 객체 AU는 각각 상술한 배경 객체 및 감정 객체 각각에 포함된 복수의 특징점 및 에지 중 적어도 하나를 포함하는 것일 수 있다. 배경 객체 AU 및 감정 객체 AU는 감정에 따라 다양하게 설정되거나 정의될 수 있다. 보다 구체적으로, 감정 객체 AU는 훈련 이미지나 입력 이미지 데이터에서 캐릭터 객체를 제외한 나머지 객체 중 캐릭터 객체의 에지와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응되는 것일 수 있다. 또한, 배경 객 체 AU는 훈련 이미지 데이터 나 입력 이미지 데이터에서 캐릭터 객체 및 감정 객체 AU를 제외한 나머지 객체에 대응되는 것일 수 있다. 예를 들어, 감정 객체 AU는 눈물 AU, 화남 표시 AU, 불남 AU 및 땀방울 AU 등을 포함할 수 있다. 예를 들어, 배경 객체 AU는 꽃 문양 AU 및 텍스트 AU 등을 포함할 수 있다. 여기서, 텍스트 AU 는 예를 들어 기쁨 감정 카테고리에 해당하는 경우 도시된 바와 같이 'I`m so happy'와 같이 기쁨을 나타내는 텍스트를 포함할 수 있다. 상술한 통합 AU들은 예시적인 것에 불과하고, 그 밖에 각 감정 카테고리 별로 해당 감정을 효과적으로 나타낼 수 있는 다양한 요소들이 통합 AU로써 설정되거나 정의될 수 있다. 도 6은 감정 카테고리 라벨링(labeling)을 설명하기 위한 것이다. 도 6을 참조하면, 상술한 통합 AU 각각은 복수의 감정 카테고리 중 적어도 하나에 라벨링(labeling)될 수도 있 다. 이러한 경우, 훈련 이미지 데이터는 예를 들어 (감정 카테고리, 통합 AU) 쌍이 될 수 있다. 즉, 분류 모델 의 학습 시 감정 카테고리에 라벨링되어 있는 통합 AU를 훈련 이미지 데이터로 하는 지도 학습이 사용될 수 있 다. 프로세서는, 분류된 하나의 감정 카테고리를 입력 이미지 데이터에 태깅(tagging)하여 태깅된 입력 이미지 데이터를 생성할 수 있다. 여기서, 태깅된 입력 이미지 데이터는 입력 이미지 데이터와 분류된 하나의 감정 카 테고리 정보를 함께 포함하는 것일 수 있다. 프로세서는, 송수신기를 통하여 생성된 태깅된 입력 이미지 데이터를 사용자 단말에 전송할 수 있다. 상술한 캐릭터 감정 인식 및 태깅 장치에 따르면, 실사가 아닌 특히 캐릭터나 이모티콘과 같은 창작물에 포함된 객체의 감정이 효과적으로 인식 및 분류될 수 있다. 구체적으로, 캐릭터 감정 인식 및 태깅 장치는 기계 학습과 같은 AI 알고리즘 시에 사용되는 특징에 캐릭터적 요소를 고려함으로써 캐릭터 객체의 감정을 보다 정밀하게 인식 및 분류할 수 있다. 또는, 본 개시의 다른 일 예에 따르면 캐릭터 감정 인식 및 태깅 장치는 입력 이미지 데이터의 감정 분류 시 감정 분류에 사용할 특징을 선택할 수도 있다. 여기서, 선택되는 특징은 캐릭터적 AU 중 어느 하나일 수 있 다. 구체적으로, 프로세서는, 송수신기를 통하여 사용자 단말로부터 분류 옵션 데이터를 수신할 수 있다. 여기서, 분류 옵션 데이터는 배경 객체 AU 및 감정 객체 AU 중 적어도 하나를 선택하기 위한 선택 정보일 수 있다. 프로세서는, 배경 객체 AU 및 상기 감정 객체 AU 중 수신한 선택 정보에 대응되는 AU를 선택할 수 있다. 프로세서는, 인간적 AU, 수신한 선택 정보에 대응되는 AU 및 기 학습된 분류 모델에 기초하여 캐릭터 객체 를 하나의 감정 카테고리로 분류할 수 있다. 이에 따라, 본 개시의 다른 일 예에 따른 캐릭터 감정 인식 및 태깅 장치는 창작자에게 감정 인식 및 분류 를 위한 특징을 선택하게 할 수 있고, 선택된 특징에 기반하여 감정 인식 및 분류를 수행함으로써 기계 학습의 만족도를 올릴 수 있다. 여기에, 추가적으로 프로세서는 콘텐츠의 시장 조사를 위하여, 분류 및 태깅된 감정 카테고리에 해당하는 유사 이미지를 검색하고, 검색된 유사 이미지를 송수신기를 통해 사용자 단말에 전송할 수도 있다. 또한, 프로세서는 검색된 유사 이미지와 업로드된 이미지 간 유사도 검사를 통해 콘텐츠 저작권의 위배 여 부를 판단할 수도 있다. 이에 따라, 만약 유사도 검사를 통해 콘텐츠 저작권이 위배되는 것으로 판단되면, 프로 세서는 송수신기를 통해 사용자 단말에 별도의 경고 메시지를 전송할 수도 있다. 또는, 본 개시의 또 다른 일 예에 따르면 캐릭터 감정 인식 및 태깅 장치는 분류된 감정의 평가도에 따라 특징을 추가하여 재분류를 수행할 수도 있다. 구체적으로, 프로세서는, 송수신기를 통하여 태깅된 입력 이미지 데이터에 대한 평가 데이터를 수신 할 수 있다. 여기서, 평가 데이터는 1차적으로 분류된 감정에 대한 사용자의 만족도를 수치화한 데이터일 수 있 다. 프로세서는, 수신한 평가 데이터가 기 설정된 임계 값 미만이면, 인간적 AU, 배경 객체 AU 및 감정 객체 AU 중 적어도 하나 및 기 학습된 분류 모델에 기초하여 캐릭터 객체를 하나의 감정 카테고리로 분류할 수 있다. 예를 들어, 평가 데이터에 대응되는 평가 수치의 최대값이 10점이고, 기 설정된 임계 값이 5점이고, 수신한 평 가 데이터에 대응되는 평가 수치의 값이 4점인 경우, 프로세서는 인간적 AU, 배경 객체 AU 및 감정 객체 AU 중 적어도 하나를 특징으로 하여 감정 분류를 다시 수행할 수 있다. 상술한 본 개시의 또 다른 일 예에 따르면 캐릭터 감정 인식 및 태깅 장치는 특히 1차적으로 수행되는 감 정 분류가 오직 인간적 AU만을 특징으로 한 경우에 유용할 수 있다. 즉, 캐릭터 감정 인식 및 태깅 장치가 1차적으로는 인간적 AU를 특징으로 하여 감정 분류를 수행하였다가, 분류된 감정에 대한 사용자의 만족도가 낮 은 경우에는 캐릭터적 AU를 함께 특징으로 하여 감정 분류를 재수행함으로써 사용자의 만족도를 올릴 수 있다. 도 7은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 시스템의 개요도이다. 이하에서는, 앞서 설명한 부분과 중복되는 부분에 대한 상세한 설명은 생략한다. 도 7을 참조하면, 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 시스템은 캐릭터 감정 인식 및 태깅 장치 및 사용자 단말을 포함한다. 캐릭터 감정 인식 및 태깅 장치는 사용자 단말로부터 수신한 입력 이미지 데이터에 포함된 캐릭터 객 체의 감정을 분류한다. 사용자 단말은 캐릭터 감정 인식 및 태깅 장치와 데이터를 주고받을 수 있다. 예를 들어, 사용자 단 말은 감정 분류를 위한 입력 이미지 데이터를 전송할 수 있다. 이때, 사용자 단말은 캐릭터의 이미지 스타일, 색상, 형태 및 그림체 등과 같은 디자인 정보를 입력하고, 해당 디자인 정보도 함께 전송할 수 있다. 예를 들어, 이미지 스타일은 정적 이미지(still image)나 동적 이미지(moving image)를 포함할 수 있다. 캐릭터 감정 인식 및 태깅 장치는 사용자 단말로부터 디자인 정보를 수신하면, 수신한 디자인 정보를 입력 이미지 데이터에 매칭시켜 메모리에 저장할 수 있다. 도 8은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 방법의 흐름도이다. 도 8을 참조하면, S110에서, 캐릭터 감정 인식 및 태깅 장치는 사용자 단말로부터 캐릭터 객체를 포 함하는 입력 이미지 데이터를 수신할 수 있다. S120에서, 캐릭터 감정 인식 및 태깅 장치는 입력 이미지 데이터의 특징 및 기 학습된 분류 모델에 기초하 여 캐릭터 객체를 복수의 감정 카테고리 중 하나의 감정 카테고리로 분류할 수 있다. 여기서, 기 학습된 분류 모델은 훈련 이미지 데이터에 포함된 캐릭터 객체의 통합 AU를 특징으로 하여 학습되는 것일 수 있다. 통합 AU는 인간적 AU 및 캐릭터적 AU를 포함할 수 있다. 이때, 인간적 AU는 훈련 이미지 데이터에 포함된 캐릭 터 객체의 눈썹, 눈, 코, 입 및 볼 각각에 포함된 복수의 특징점 간 거리 및 비율 중 적어도 하나를 달리 갖는 복수의 눈썹 AU, 복수의 눈 AU, 복수의 코 AU, 복수의 입 AU 및 복수의 볼 AU 중 적어도 하나를 포함할 수 있다. 캐릭터적 AU는 훈련 이미지 데이터에 포함된 배경 객체 AU 및 훈련 이미지 데이터에 포함된 감정 객체 AU 중 적 어도 하나를 포함할 수 있다. 이때, 감정 객체 AU는 훈련 이미지 데이터에서 캐릭터 객체를 제외한 나머지 객체 중 캐릭터 객체의 에지와 적어도 일부가 중첩되는 에지를 포함하는 객체에 대응될 수 있다. 배경 객체 AU는 훈 련 이미지 데이터에서 캐릭터 객체 및 감정 객체 AU를 제외한 나머지 객체에 대응될 수 있다.S130에서, 캐릭터 감정 인식 및 태깅 장치는 하나의 감정 카테고리를 입력 이미지 데이터에 태깅하여 태깅 된 입력 이미지 데이터를 생성할 수 있다. S140에서, 캐릭터 감정 인식 및 태깅 장치는 태깅된 입력 이미지 데이터를 사용자 단말에 전송할 수 있다. 여기에, 본 개시의 다른 일 예에 따른 캐릭터 감정 인식 및 태깅 방법은 감정 분류에 사용할 특징을 선택하기 위한 단계를 더 포함할 수도 있다. 예를 들어, 캐릭터 감정 인식 및 태깅 방법은 캐릭터 감정 인식 및 태깅 장치가 사용자 단말로부터 분류 옵션 데이터를 수신하는 단계, 배경 객체 AU 및 감정 객체 AU 중 선택 정보에 대응되는 AU를 선택하는 단 계 및 인간적 AU, 선택 정보에 대응되는 AU 및 기 학습된 분류 모델에 기초하여 캐릭터 객체를 하나의 감정 카 테고리로 분류하는 단계를 더 포함할 수 있다. 이때, 분류 옵션 데이터는 배경 객체 AU 및 감정 객체 AU 중 적어도 하나를 선택하기 위한 선택 정보를 포함할 수 있다. 여기에, 본 개시의 또 다른 일 예에 따른 캐릭터 감정 인식 및 태깅 방법은 감정 분류에 사용할 특징을 선택하 기 위한 단계를 더 포함할 수도 있다. 예를 들어, 캐릭터 감정 인식 및 태깅 방법은 캐릭터 감정 인식 및 태깅 장치가 태깅된 입력 이미지 데이 터에 대한 평가 데이터를 수신하는 단계, 평가 데이터가 기 설정된 임계 값 미만이면, 인간적 AU, 배경 객체 AU 및 감정 객체 AU 중 적어도 하나 및 기 학습된 분류 모델에 기초하여 캐릭터 객체를 하나의 감정 카테고리로 분 류하는 단계를 더 포함할 수 있다. 상술한 설명에서 제안 방식에 대한 일례들 또한 본 개시의 구현 방법들 중 하나로 포함될 수 있으므로, 일종의 제안 방식들로 간주될 수 있음은 명백한 사실이다. 또한, 상기 설명한 제안 방식들은 독립적으로 구현될 수 도 있지만, 일부 제안 방식들의 조합 (혹은 병합) 형태로 구현될 수 도 있다."}
{"patent_id": "10-2021-0010843", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "상술한 바와 같이 개시된 본 개시의 예들은 본 개시와 관련된 기술분야의 통상의 기술자가 본 개시를 구현하고 실시할 수 있도록 제공되었다. 상기에서는 본 개시의 예들을 참조하여 설명하였지만, 해당 기술 분야의 통상의 기술자는 본 개시의 예들을 다양하게 수정 및 변경시킬 수 있다. 따라서, 본 개시는 여기에 기재된 예들에 제한 되려는 것이 아니라, 여기서 개시된 원리들 및 신규한 특징들과 일치하는 최광의 범위를 부여하려는 것이다."}
{"patent_id": "10-2021-0010843", "section": "도면", "subsection": "도면설명", "item": 1, "content": "이하에 첨부되는 도면들은 본 개시의 다양한 예들에 관한 이해를 돕기 위한 것으로, 상세한 설명과 함께 본 개 시의 다양한 예들을 제공한다. 다만, 본 개시의 다양한 예들의 기술적 특징이 특정 도면에 한정되는 것은 아니 며, 각 도면에서 개시하는 특징들은 서로 조합되어 새로운 실시예로 구성될 수 있다. 각 도면에서의 참조 번호 (reference numerals) 들은 구조적 구성요소 (structural elements) 를 의미한다. 도 1은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 장치의 블록도이다. 도 2 내지 도 5는 본 개시의 다양한 예들에 따라 정의되는 AU(action unit)를 설명하기 위한 것이다. 도 6은 감정 카테고리 라벨링(labeling)을 설명하기 위한 것이다. 도 7은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 시스템의 개요도이다. 도 8은 본 개시의 일 예에 따른 캐릭터 감정 인식 및 태깅 방법의 흐름도이다."}
