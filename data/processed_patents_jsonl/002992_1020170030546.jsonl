{"patent_id": "10-2017-0030546", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2018-0084580", "출원번호": "10-2017-0030546", "발명의 명칭": "복수 문단 텍스트의 추상적 요약문 생성 장치 및 방법, 그 방법을 수행하기 위한 기록 매체", "출원인": "경북대학교 산학협력단", "발명자": "이민호"}}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "문서를 문단들로 자동 구분하여 전달하는 입력부;상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하고, 다중 시상수를 갖는 GRU(Multiple TimescalesGated Recurrent Unit, 이하 MTGRU)를 포함하는 회귀신경망을 통해 변환된 벡터를 내부표현(representation)으로 생성하여 전달하는 부호화 처리부;MTGRU를 포함하는 회귀신경망을 통해 상기 부호화 처리부로부터 전달받은 내부표현을 복호화하고, 언어 모델링을 이용하여 문장들을 생성하는 복호화 처리부; 및각 문단의 요약 출력을 수집하여 최종 추상적 요약을 출력하는 출력부를 포함하는, 복수 문단 텍스트의 추상적요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 부호화 처리부는,상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하는 내재 벡터화부; 및MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 딥러닝 기술을 기초로 상기 변환된 벡터를 내부표현(representation)으로 생성하여 상기 복호화 처리부에 전달하는 부호화 신경망을 포함하는, 복수 문단 텍스트의추상적 요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 복호화 처리부는,MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 상기 부호화 처리부로부터 전달받은 상기 내부표현을 복호화하는 복호화 신경망; 및언어 모델링을 이용한 학습 내용에 따라 상기 복호화 신경망으로부터 전달받은 복호화된 내부표현으로부터 자연어로 표현되는 문장들을 생성하는 자연어 생성부를 포함하는, 복수 문단 텍스트의 추상적 요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 회귀신경망은 MTGRU을 기본 유닛으로 하는 시간적 계층 모델을 사용한 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 이용하는, 복수 문단 텍스트의 추상적 요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 MTGRU는 각 계층마다 설정된 시상수를 가지며, 상기 시상수가 클수록 느린 컨텍스트 단위를 구성하는, 복수 문단 텍스트의 추상적 요약문 생성 장치.공개특허 10-2018-0084580-3-청구항 6 제5항에 있어서,계층이 올라갈수록 상기 시상수가 커지는, 복수 문단 텍스트의 추상적 요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 회귀신경망은 다중 시상수 기반의 조합가능성(compositionality)을 구비하는, 복수 문단 텍스트의 추상적요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 입력부는 LaTeX 소스 파일로부터 소개(Introductions)를 추출하는, 복수 문단 텍스트의 추상적 요약문 생성 장치."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "문서를 문단들로 자동 구분하는 단계;상기 문단을 내부표현 벡터로 변환하는 단계;다중 시상수를 갖는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)의 다중층으로 이루어진 회귀신경망을 통해, 딥러닝 기술을 기초로 상기 변환된 벡터를 내부표현(representation)으로 생성하는 단계;MTGRU의 다중층으로 이루어진 회귀신경망을 통해 상기 내부표현을 복호화하는 단계;언어 모델링을 이용한 학습 내용에 따라 복호화된 내부표현으로부터 자연어로 표현되는 문장들을 생성하는단계; 및각 문단의 요약 출력을 수집하여 최종 추상적 요약을 출력하는 단계를 포함하는, 복수 문단 텍스트의 추상적 요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 회귀신경망은 MTGRU을 기본 유닛으로 하는 시간적 계층 모델을 사용한 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 이용하는, 복수 문단 텍스트의 추상적 요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 MTGRU는 각 계층마다 설정된 시상수를 가지며, 상기 시상수가 클수록 느린 컨텍스트 단위를 구성하는, 복수 문단 텍스트의 추상적 요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2018-0084580-4-제11항에 있어서,계층이 올라갈수록 상기 시상수가 커지는, 복수 문단 텍스트의 추상적 요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 회귀신경망은 다중 시상수 기반의 조합가능성(compositionality)을 구비하는, 복수 문단 텍스트의 추상적요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서,LaTeX 소스 파일로부터 소개(Introductions)를 추출하는 단계를 더 포함하는, 복수 문단 텍스트의 추상적 요약문 생성 방법."}
{"patent_id": "10-2017-0030546", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항 내지 제14항 중 어느 하나의 항에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법을 수행하기 위한,컴퓨터 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "문 생성 장치 및 방법, 그 방법을 수행하기 위한 기록 매체 요 약"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "요약", "paragraph": 2, "content": "복수 문단 텍스트의 추상적 요약문 생성 장치는, 문서를 문단들로 자동 구분하여 전달하는 입력부; 상기 입력부 로부터 전달된 문단을 내부표현 벡터로 변환하고, 다중 시상수를 갖는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)를 포함하는 회귀신경망을 통해 변환된 벡터를 내부표현(representation)으로 생성 하여 전달하는 부호화 처리부; MTGRU를 포함하는 회귀신경망을 통해 상기 부호화 처리부로부터 전달받은 내부표"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "요약", "paragraph": 3, "content": "현을 복호화하고, 언어 모델링을 이용하여 문장들을 생성하는 복호화 처리부; 및 각 문단의 요약 출력을 수집하"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "요약", "paragraph": 4, "content": "여 최종 추상적 요약을 출력하는 출력부를 포함한다. 이에 따라, 추상적 표현을 생성함으로써 보다 사람이 작성"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "요약", "paragraph": 5, "content": "한 요약에 가까운 요약문을 생성할 수 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 복수 문단 텍스트의 추상적 요약문 생성 장치 및 방법, 그 방법을 수행하기 위한 기록 매체에 관한"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "것으로서, 더욱 상세하게는 복수 개의 문단을 가지는 텍스트의 문단 별 자동 요약 시스템으로써, 심화 의미의"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 3, "content": "이해가 가능하고, 단순 단어 추출이 아닌 사람이 작성한 것과 같은 추상적 요약이 가능한 기술에 관한 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "요약(Summarization)은 지난 수십 년 동안 광범위하게 연구되어 왔다. 포괄적으로, 요약 방법은 계산 작업의 유"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "형에 따라 추출 접근법과 추상 접근법으로 분류할 수 있다. 추출(extractive) 요약은 선택 문제인 반면, 추상적"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "(abstractive) 요약은 텍스트의 의미 및 담론에 대한 더 깊은 이해와 새로운 텍스트 생성 과정을 필요로 한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "과거에는 추출 요약에 초점이 맞추어져 있고, 추상적 요약은 아직 해결해야 할 과제가 많이 있다. 최근에, 시퀀스-투-시퀀스(sequence-to-sequence, 이하 seq2seq) 반복적 신경망(RNNs)은 많은 작업에서 폭넓게 응용되고 있다. 이러한 RNN 인코더-디코더(Cho et al., 2014; Bahdanau et al., 2014)는 표현 학습 인코더와 언어 모델링 디코더를 결합하여 두 시퀀스 간의 매핑을 수행한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "유사하게, 최근의 연구는 입력 시퀀스와 요약 시퀀스 간의 매핑 문제로 요약을 캐스팅할 것을 제안했다. Rush"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "et al. ; Nallapati et al. 은 RNN 인코더-디코더가 짧은 텍스트를 요약하는데 매우 뛰어나다는 것을 보여 주었다. 이러한 seq2seq 접근 방식은 의미 및 담화 이해와 텍스트 생성 모두에 대해 완벽한 데이터 중심 솔루션을 제공한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 7, "content": "seq2seq는 추상적 요약에 대한 유망한 방법을 제공하지만, 과학 기술 문서 요약과 같은 다른 작업으로 방법론을 외삽(extrapolating)하는 것은 쉽지 않으며, 실질적이고 이론적인 아래와 같은 여러 가지 문제가 발생한다. 1) 전체 기사에 대해 RNN 인코더-디코더를 단순히 학습하기는 어렵다. 현재 GPU의 메모리 용량에 비해, 과학 기 사가 너무 길어 RNN을 통해 전체를 처리할 수 없기 때문이다. 2) 한 두 문장에서 여러 문장 또는 여러 단락으로 이동하면, 조합가능성 및 풍부한 담화 구조의 추가 레벨(level)이 도입된다. 이 경우, 어떻게 RNN 인코더-디코더를 더 잘 캡처할 수 있도록 개선할 수 있을까 하는 문제가 발생한다. 3) 딥 러닝(Deep learning)은 좋은"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 8, "content": "품질, 큰 규모의 데이터 세트에 매우 의존적으로 접근한다. 소스 요약 데이터 쌍을 수집하는 것은 어렵고, 데이 터 세트는 뉴스 와이어 도메인 외부에서 부족하다. 정리하면, 최근 딥러닝 기반 인공지능 기술의 발달로 다수의 딥러닝 모델들이 제안되었으나 여전히 몇몇 어려운"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 9, "content": "문제들이 남아있다. 자연어 처리 분야에서 추상적(abstractive) 요약은 그 중 하나로, 기존의 요약 시스템은 원"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 10, "content": "문에 존재하는 단어들의 선택적 추출(extraction)을 통해 요약하는 방법을 사용하여 그 표현력에 한계가 명백하 였다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 10-2013-0116908 A (특허문헌 0002) KR 10-0785927 B1 비특허문헌 (비특허문헌 0001) Minsoo Kim, Moirangthem Dennis Singh, and Minho Lee, \"Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization\", Association for Computational Linguistics ."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이에, 본 발명의 기술적 과제는 이러한 점에서 착안된 것으로 본 발명의 목적은 복수 문단 텍스트의 추상적 요 약문 생성 장치를 제공하는 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "본 발명의 다른 목적은 복수 문단 텍스트의 추상적 요약문 생성 방법을 제공하는 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 3, "content": "본 발명의 또 다른 목적은 상기 복수 문단 텍스트의 추상적 요약문 생성 방법을 수행하기 위한 컴퓨터 프로그램 이 기록된 기록 매체를 제공하는 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 장치는, 문 서를 문단들로 자동 구분하여 전달하는 입력부; 상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하고, 다중 시상수를 갖는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)를 포함하는 회귀신경망을 통 해 변환된 벡터를 내부표현(representation)으로 생성하여 전달하는 부호화 처리부; MTGRU를 포함하는 회귀신경 망을 통해 상기 부호화 처리부로부터 전달받은 내부표현을 복호화하고, 언어 모델링을 이용하여 문장들을 생성"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "하는 복호화 처리부; 및 각 문단의 요약 출력을 수집하여 최종 추상적 요약을 출력하는 출력부를 포함한다. 본 발명의 실시예에서, 상기 부호화 처리부는, 상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하는 내 재 벡터화부; 및 MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 딥러닝 기술을 기초로 상기 변환된 벡터 를 내부표현(representation)으로 생성하여 상기 복호화 처리부에 전달하는 부호화 신경망을 포함할 수 있다. 본 발명의 실시예에서, 상기 복호화 처리부는, MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 상기 부호 화 처리부로부터 전달받은 상기 내부표현을 복호화하는 복호화 신경망; 및 언어 모델링을 이용한 학습 내용에 따라 상기 복호화 신경망으로부터 전달받은 복호화된 내부표현으로부터 자연어로 표현되는 문장들을 생성하는 자연어 생성부를 포함할 수 있다. 본 발명의 실시예에서, 상기 회귀신경망은 MTGRU을 기본 유닛으로 하는 시간적 계층 모델을 사용한 시퀀스-투- 시퀀스(sequence-to-sequence) 모델을 이용할 수 있다.본 발명의 실시예에서, 상기 MTGRU는 각 계층마다 설정된 시상수를 가지며, 상기 시상수가 클수록 느린 컨텍스 트 단위를 구성할 수 있다. 본 발명의 실시예에서, 계층이 올라갈수록 상기 시상수가 커질 수 있다. 본 발명의 실시예에서, 상기 회귀신경망은 다중 시상수 기반의 조합가능성(compositionality)을 구비할 수 있다. 본 발명의 실시예에서, 상기 입력부는 LaTeX 소스 파일로부터 소개(Introductions)를 추출할 수 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "상기한 본 발명의 다른 목적을 실현하기 위한 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법은, 문서를 문단들로 자동 구분하는 단계; 상기 문단을 내부표현 벡터로 변환하는 단계; 다중 시상수를 갖 는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)의 다중층으로 이루어진 회귀신경망을 통해, 딥러닝 기술을 기초로 상기 변환된 벡터를 내부표현(representation)으로 생성하는 단계; MTGRU의 다중층으로 이루어진 회귀신경망을 통해 상기 내부표현을 복호화하는 단계; 언어 모델링을 이용한 학습 내용에 따라 복호화"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "된 내부표현으로부터 자연어로 표현되는 문장들을 생성하는 단계; 및 각 문단의 요약 출력을 수집하여 최종 추"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "상적 요약을 출력하는 단계를 포함한다. 본 발명의 실시예에서, 상기 회귀신경망은 MTGRU을 기본 유닛으로 하는 시간적 계층 모델을 사용한 시퀀스-투- 시퀀스(sequence-to-sequence) 모델을 이용할 수 있다. 본 발명의 실시예에서, 상기 MTGRU는 각 계층마다 설정된 시상수를 가지며, 상기 시상수가 클수록 느린 컨텍스 트 단위를 구성할 수 있다. 본 발명의 실시예에서, 계층이 올라갈수록 상기 시상수가 커질 수 있다. 본 발명의 실시예에서, 상기 회귀신경망은 다중 시상수 기반의 조합가능성(compositionality)을 구비할 수 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "본 발명의 실시예에서, 상기 복수 문단 텍스트의 추상적 요약문 생성 방법은, LaTeX 소스 파일로부터 소개 (Introductions)를 추출하는 단계를 더 포함할 수 있다. 상기한 본 발명의 또 다른 목적을 실현하기 위한 일 실시예에 따른 컴퓨터로 판독 가능한 저장 매체에는, 복수"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 7, "content": "문단 텍스트의 추상적 요약문 생성 방법을 수행하기 위한 컴퓨터 프로그램이 기록되어 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이와 같은 복수 문단 텍스트의 추상적 요약문 생성 장치 및 방법에 따르면, 문단에 존재하는 단어를 조합하는"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "것이 아니라 요약에 필요한 단어를 기존에 학습한 내용으로부터 생성함으로써 추상적 요약이 가능하다. 기존 시"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "스템은 추출 및 압축과 같은 효과만 가져오기 때문에 문장이 어색하고 사람이 생성한 요약과는 거리가 있다. 본 발명은 딥러닝 기술 기반의 내부표현 과정을 도입함으로써 긴 텍스트를 이해하고 이해한 내용을 기반으로 추상"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 4, "content": "적 요약문을 생성해낼 수 있다. 본 발명은 인간 두뇌에 대해 널리 연구된 내용을 딥러닝 기반 인공지능에 적용하여 별도의 기억장치나 구조적"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 5, "content": "복합성을 추가하지 않고도 추상적 요약이 가능하도록 한다. 또한, 본 발명은 단순 추출이 아닌 추상적 요약으로"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 6, "content": "써 복수개의 문단으로 이루어진 장문의 텍스트 또한 요약 가능하다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "후술하는 본 발명에 대한 상세한 설명은, 본 발명이 실시될 수 있는 특정 실시예를 예시로서 도시하는 첨부 도 면을 참조한다. 이들 실시예는 당업자가 본 발명을 실시할 수 있기에 충분하도록 상세히 설명된다. 본 발명의 다양한 실시예는 서로 다르지만 상호 배타적일 필요는 없음이 이해되어야 한다. 예를 들어, 여기에 기재되어 있 는 특정 형상, 구조 및 특성은 일 실시예에 관련하여 본 발명의 정신 및 범위를 벗어나지 않으면서 다른 실시예 로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 본 발명의 정신 및 범 위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미로 서 취하려는 것이 아니며, 본 발명의 범위는, 적절하게 설명된다면, 그 청구항들이 주장하는 것과 균등한 모든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하거 나 유사한 기능을 지칭한다. 이하, 도면들을 참조하여 본 발명의 바람직한 실시예들을 보다 상세하게 설명하기로 한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 1은 본 발명의 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 장치의 블록도이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본 발명에 따른 복수 문단 텍스트의 추상적 요약문 생성 장치(10, 이하 장치)는 복수개 문단을 가지는 텍스트의"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "문단별 자동 요약 시스템으로써, 심화 의미의 이해가 가능하고, 단순 단어 추출이 아닌 사람과 같은 추상적 요 약이 가능하다. 이러한 기능을 구현하기 위해서 새로 제안한 MTGRU(Multiple Timescales Gated Recurrent Unit)를 사용한 회귀 신경망을 기본 구성요소로 하는 자가부호화망(Auto Encoder)을 사용한다. 도 1을 참조하면, 본 발명에 따른 장치는 입력부, 부호화 처리부, 복호화 처리부 및 출력부 를 포함한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "본 발명에 따른 장치는 복수개의 문단으로 이루어진 텍스트로부터 추상적 요약문을 작성한다. 본 발명에 따 른 장치는 다중 시상수를 갖는 MTGRU(Multiple Timescale Gated Recurrent Unit)을 기본요소로 하는 시간 적 계층 모델을 사용하여 시퀀스 투 시퀀스(sequence to sequence, 이하 seq2seq) 모델을 구현한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "본 발명의 상기 장치는 복수 문단 텍스트의 추상적 요약문을 생성하기 위한 소프트웨어(애플리케이션)가 설치되어 실행될 수 있으며, 상기 입력부, 상기 부호화 처리부, 상기 복호화 처리부 및 상기 출"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "력부의 구성은 상기 장치에서 실행되는 상기 복수 문단 텍스트의 추상적 요약문을 생성하기 위한 소프 트웨어에 의해 제어될 수 있다. 상기 장치는 별도의 단말이거나 또는 단말의 일부 모듈일 수 있다. 또한, 상기 입력부, 상기 부호화 처리부, 상기 복호화 처리부 및 상기 출력부의 구성은 통합 모듈로 형성되거나, 하나 이상의 모 듈로 이루어 질 수 있다. 그러나, 이와 반대로 각 구성은 별도의 모듈로 이루어질 수도 있다. 상기 장치는 이동성을 갖거나 고정될 수 있다. 상기 장치는, 서버(server) 또는 엔진(engine) 형태일 수 있으며, 디바이스(device), 기구(apparatus), 단말(terminal), UE(user equipment), MS(mobile station), 무선기기(wireless device), 휴대기기(handheld device) 등 다른 용어로 불릴 수 있다. 상기 장치는 운영체제(Operation System; OS), 즉 시스템을 기반으로 다양한 소프트웨어를 실행하거나 제작 할 수 있다. 상기 운영체제는 소프트웨어가 장치의 하드웨어를 사용할 수 있도록 하기 위한 시스템 프로그램으 로서, 안드로이드 OS, iOS, 윈도우 모바일 OS, 바다 OS, 심비안 OS, 블랙베리 OS 등 모바일 컴퓨터 운영체제 및 윈도우 계열, 리눅스 계열, 유닉스 계열, MAC, AIX, HP-UX 등 컴퓨터 운영체제를 모두 포함할 수 있다. 상기 입력부는 문서를 문단들로 자동 구분하여 상기 부호화 처리부로 전달한다. 예를 들어, 상기 입 력부는 ArXiv.org에서 컴퓨터 과학(CS) 기사들의 새로운 데이터 세트를 작성하고, LaTeX 소스 파일로부터 소개(Introductions)를 추출하여 단락으로 구분할 수 있다. 상기 부호화 처리부는 상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하고, 다중 시상수를 갖는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)를 포함하는 회귀신경망을 통해 변환된 벡터를 내부표현(representation)으로 생성하여 상기 복호화 처리부로 전달할 수 있다. 이를 위해, 상기 부호화 처리부는 상기 입력부로부터 전달된 문단을 내부표현 벡터로 변환하는 내재 벡터화부와 MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 딥러닝 기술을 기초로 상기 변환된 벡터 를 내부표현(representation)으로 생성하여 상기 복호화 처리부에 전달하는 부호화 신경망을 포함할 수 있 다. 본 발명에서는 내부적 표현 문법을 생성하고 난 후 문장생성 기법을 적용하여 추상적 표현을 생성함으로써, 보"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "다 사람이 작성한 요약에 가까운 요약문을 생성한다. 본 발명은 단순 추출이 아닌 추상적 요약으로써 복수개의"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "문단으로 이루어진 장문의 텍스트 또한 요약이 가능하다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "본 발명에서는 과학 기사의 종단 간 추상 요약을 위한 첫 번째 중간 단계를 제시한다. 본 발명의 목표는"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "seq2seq 기반 요약을 보다 복잡한 요약 작업인 더 큰 텍스트로 확장하는 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "위의 과제를 해결하기 위해, 본 발명은 문단 핵심의 문장 체계를 통해 학습되는 문단성 요약 시스템을 제안한다. 용어 주파수 역 문서 빈도(Term Frequency-Inverse Document Frequency, TF-IDF) (Luhn, 1958; Jones, 1972)를 사용하여 각 문단에서 핵심 문장을 추출하는데 사용한다. 또한, 새로운 모델인 다중 시상수를 갖는 GRU(multiple timescale gated recurrent unit)를 제안한다. MTGRU는 다중 계층의 조합가능성(compositionality)을 처리할 수 있는 시간적 계층 구성 요소를 추가한다. 이것은 인간 의 두뇌에서 발견되는 시간적 계층 조직과 유사한 개념이며, 다중 RNN의 다른 계층을 다른 시간 스케일로 변조 함으로써 구현된다(Yamashita and Tani, 2008). 본 발명이 제안한 모델은 다중 문장 소스 텍스트의 의미를 이해하고 그것에 대해 중요한 것을 알 수 있으며, 이"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "는 추상적 요약으로의 필수적인 첫 번째 단계이다. 일 실시예로, 본 발명은 ArXiv.org에서 컴퓨터 과학(CS) 기사들의 새로운 데이터 세트를 작성하고, LaTeX 소스 파일에서 소개를 추출한다. 서론은 단락으로 분해되며, 각 단락은 담론의 자연스러운 단위가 된다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "마지막으로, 각 단락에 대해 생성된 요약을 연결하여 기사 소개의 비 전문가 요약을 작성하고, 실제 요약과 비"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "교하여 결과를 평가한다. 본 발명의 모델은 보이지 않는 데이터에서 가장 중요한 부분으로 여러 문장을 요약할"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "수 있다는 것을 보여주며, seq2seq 매핑 작업으로 더 큰 요약보기를 지원한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "본 발명은 MTGRU 모델이 추상적 요약 시스템의 주요 요구사항 중 일부를 충족함을 입증한다. 또한, MTGRU는 기 존의 RNN 인코더-디코더와 비교하여 학습 시간을 크게 단축할 수 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "이하, 본 발명의 모델과 관련된 배경에 대해 설명하고, 본 발명에서 새로 제시된 아키텍처와 요약에 대한 적용 을 자세히 설명한다. 도 2는 게이트 반복 단위(GRU)를 도시한 도면이다. 조합가능성(compositionality)의 원칙은 구성 단위의 구문 조합의 함수로서 언어 표현에 의해 전달되는 의미를 정의한다. 즉, 문장의 의미는 단어들이 서로 결합되는 방식에 따라 결정된다. 다중 문장 텍스트에서, 문장 수준 의 조합가능성(문장이 서로 결합되는 방식)은 전체 텍스트에 의미를 부여하는 추가 함수이다. 큰 텍스트를 다룰 때, 텍스트의 의미를 완전히 파악하기 위해서는 문장의 조합가능성 및 단락 수준까지 고려해 야 한다. 최근 문헌에서 연구된 접근법은 계층적 형식으로 전용 아키텍처를 만들어 이후의 조합가능성 수준을 파악하는 것이다. Li et al. 및 Nallapati et al. 은 성능을 향상시키기 위해 텍스트 단위의 다 른 수준(level)에서의 조합가능성을 파악한다. 그러나, 이와 같은 RNN 인코더-디코더에 대한 구조적인 변경은 학습 시간 및 메모리 사용 모두 증가한다는 결점 이 있다. 따라서, 본 발명은 이러한 오버 헤드 없이 성능을 향상시킬 수 있는 아키텍처에 대한 대안을 제안한다. 본 발명은 인간의 뇌에서 기능적 분화가 자연적으로 발생하여, 시간적인 계층을 발생시키는 신경 과 학으로부터 영감을 얻었다(Meunier et al., 2010; Botvinick, 2007). 뉴런은 자극에 대한 각기 다른 적응 속도를 가진 계층으로 스스로를 계층적으로 조직할 수 있다는 것이 잘 알려 져 있다. 이 현상의 전형적인 예는 짧은 시간 창(window)에서 음절 수준 정보가 더 긴 시간 창에서 단어 수준 정보로 통합되는 등의 청각 시스템이다. 이전 연구들은 이 개념을 운동 추적(Paine and Tani, 2004) 및 음성 인 식(Heinrich et al., 2012)에서 RNN에 적용하였다.도 3은 본 발명에서 제안된 다중 시상수를 갖는 GRU(multiple timescale gated recurrent unit)를 보여주는 도 면이다. 본 발명에서 제안된 다중 시상수(timescale)를 갖는 GRU(multiple timescale gated recurrent unit, MTGRU) 모"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "델은 RNN 인코더-디코더의 프레임 워크에서 seq2seq 텍스트 요약의 문제에 시간적 계층 개념을 적용한다. Multiple Timescale Recurrent Neural Network(MTRNN)와 같은 이전 연구(Yamashita and Tani, 2008)들은 움직 임 예측에 시간적 계층 구조를 사용했다. 그러나, MTRNN은 RNN에 존재하는 동일한 문제, 예를 들어 장기 종속성을 포착하는데 어려움이 있고, 경사도 (gradient) 문제가 사라지는 경향이 있다(Hochreiter et al., 2001). 단기 장기 기억 네트워크(Hochreiter et al., 2001)는 장기 의존성에 대한 학습을 돕기 위해 복잡한 게이팅 아키텍처를 사용하며, 기계 번역(Sutskever et al., 2014)과 같은 장기간의 시간 종속성을 갖는 작업에서 RNN보다 훨씬 우수한 성능을 보였다. LSTM(Chung et al., 2014)과 비교될 수 있는 것으로 입증된 Gated Recurrent Unit(Cho et al., 2014)은 유사한 복잡한 게이팅 아키텍처를 가지고 있지만 메모리가 더 적다. 표준 GRU 아키텍처는 도 2에 도시된다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "seq2seq 요약은 잠재적으로 많은 장거리 시간 종속성을 포함하기 때문에 본 발명의 모델은 시간적 계층을 GRU에 적용한다. 본 발명은 GRU의 단부에 시상수를 적용하여 본질적으로 과거와 현재 숨겨진 상태의 혼합을 조정하는 또 다른 상수 게이팅 단위를 추가한다. 리셋 게이트 rt, 업데이트 게이트 zt 및 후보 활성화 ut는 아래의 수학식 1에 보여지는 바와 같이 원래의 GRU의 것과 유사하게 계산된다. [수학식 1]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "시상수 τ가 MTGRU의 활성화 ht에 추가되는 것은 아래의 수학식 2와 같다. [수학식 2]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "τ는 각 GRU 셀의 시상수를 제어하는데 사용된다. 큰 τ는 느린 셀 출력을 의미하지만 동적 시퀀스 입력의 느린 기능에 중점을 두는 셀을 생성한다. 본 발명에서 제안된 MTGRU 모델은 도 3에 도시된다. 기존의 GRU는 τ=1일 때, MTGRU의 특별한 경우이며, 다른 시상수로 계층을 구성하려는 시도가 없다. 아래의 수학식 3은 정의된 순방향 프로세스와 시간 규칙을 통한 역 전파에 따라 MTGRU에 대해 유도된 학습 알고 리즘을 보여준다. [수학식 3] 는 시간 t-1에서 셀 출력의 오차이고, 는 셀 출력의 현재 기울기이다. 서로 다른 시상수는 각 계 층 마다 설정된다. 큰 τ는 느린 컨텍스트 단위를 의미하고, τ=1은 디폴트 또는 입력 시상수를 정의한다. 추후 계층이 더 느린 시상수에서 동작하는 특징을 학습해야 한다는 가설에 기반하여 계층을 올라갈수록, 큰 τ 를 설정하였다. 이때, 문제는 RNN에 의해 분석되는 단어 시퀀스가 인간 청각 시스템에 의해 수신된 연속 오디오 신호의 경우와 같이, 상이한 시간적 계층 구조를 통해 동작하는 정보를 소유하는지 여부이다. 본 발명은 RNN이 사용하는 가설과 단어 수준, 절 수준 및 문장 수준 조합가능성이 강력한 후보라고 가정한다. 이 점에서, 다중 시상수 수정은 신경망의 각 계층을 명시적으로 안내하여 구성 계층의 후속 레벨에 해당하는 점 차적으로 느린 시상수에서 동작하는 기능의 학습을 용이하게 한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "본 발명에서 새로 제안된 다중 시상수 모델을 요약에 적용하기 위해, 학술 자료의 새로운 데이터 세트를 구축한 다. 일 예로, arXiv preprint 서버의 CS. {CL, CV, LG, NE} 도메인에 있는 기사의 LaTeX 소스 파일을 수집하여"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "서론 및 초록을 추출한다. 서론을 단락으로 분해하고, 각 단락을 가장 중요한 문장을 타겟 요약으로 연결한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "이러한 타겟 요약은 널리 채택된 TF-IDF 채점을 사용하여 생성된다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 26, "content": "도 4는 본 발명의 요약 모델의 구조를 보여준다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 27, "content": "도 4를 참조하면, 구축한 데이터 세트는 풍부한 조합가능성 및 긴 텍스트 시퀀스를 포함하고 있어, 요약 문제의 복잡성을 증가시킨다. 시간적 계층 기능은 복잡한 구성 계층 구조가 입력 데이터에 있을 때 가장 큰 영향을 미"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "친다. 그러므로, 다중 시상수 개념은 Rush et al. 과 같은 이전의 요약 작업과 비교할 때 본 발명의 문맥 에서 더 큰 역할을 할 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "MTGRU를 사용하는 본 모델은 이러한 단락과 그들의 타겟을 사용하여 학습된다. 각 서론의 생성된 요약은 수집된"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "기사의 초록을 사용하여 평가된다. 초록은 골드 요약으로 선택되었는데, 그 이유는 일반적으로 목표, 관련 저작"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "물, 방법 및 결과와 같은 중요한 담화 구조를 포함하기 때문에 요약을 좋은 기준선으로 만들기 때문이다. 상기 복호화 처리부는 마찬가지로 MTGRU를 포함하는 회귀신경망을 통해 상기 부호화 처리부로부터 전 달받은 내부표현을 복호화하고, 언어 모델링을 이용하여 문장들을 생성한다. 이를 위해, 상기 복호화 처리부는 MTGRU의 다중층으로 이루어진 회귀신경망을 포함하고, 상기 부호화 처리 부로부터 전달받은 상기 내부표현을 복호화하는 복호화 신경망과 언어 모델링을 이용한 학습 내용에 따라 상기 복호화 신경망으로부터 전달받은 복호화된 내부표현으로부터 자연어로 표현되는 문장들을 생성하는 자연어 생성부를 포함한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "상기 출력부는 상기 자연어 생성부로부터 각 문단의 요약 출력을 수집하여 최종 추상적 요약을 출력 한다. 본 발명은 다중 시상수기반의 조합 가능성(compositionalities)을 구비함으로써 복수개의 문단으로 이루어진 긴"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "텍스트를 요약할 수 있는 기능을 가진다. 본 발명에 따라 생성된 추상적 요약은 기존 추출식 요약 시스템이 생"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "성한 요약문과는 달리 사람이 직접 작성한 요약과 유사하며 딥러닝 기술을 사용하여 자연스러운 문법을 가진다. 이하에서는, 본 발명에서 제안된 방법의 유효성을 검증하기 위해 기존의 RNN 인코더-디코더와 학습 속도 및 성 능 측면에서 비교하였다. 실험을 위해, 두 개의 seq2seq 모델, 즉 RNN 인코더 디코더에서 기존 GRU를 사용하는 첫 번째 모델과 본 발명에 서 제안된 MTGRU를 사용하는 두 번째 모델을 학습하였다. 두 모델 모두 기존의 하드웨어 기능에 맞는 최적의 구 성으로 동일한 하이퍼파라미터 설정을 사용하여 학습된다. Sutskever et al. 에 따라, 입력은 여러 개의 버킷으로 나뉜다. GRU 및 MTSGRU 모델은 모두 아래의 표 1 과 같이 4개의 계층과 1792개의 숨겨진 유닛으로 구성된다. [표 1]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 35, "content": "모델이 더 긴 입력 및 타겟 시퀀스 크기를 사용하므로, 숨겨진 유닛 크기와 계층 수가 제한된다. 두 네트워크 모두에 대해 512의 내장 크기가 사용되었다. 각 계층의 시상수 τ는 각각 1, 1.25, 1.5, 1.7로 설정된다. 모델"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 36, "content": "은 110,000 개의 텍스트 요약 쌍에 대해 학습된다. 원본 텍스트는 학술 논문의 서론에서 추출한 단락이고, 타겟 은 TF-IDF 점수를 사용하여 단락에서 추출한 가장 현저한 문장이다. 도 5는 GRU와 MTGRU 간의 학습 속도 비교를 보여주는 그래프이다. 모델의 학습 속도를 비교하기 위해, 도 5는 연속 퍼플릭시티(perplexity)가 9.5에 도달 할 때까지의 학습 곡선 의 플롯을 보여준다. 두 모델 모두 대략 4일 및 3일이 소요되는 Nvidia Ge-Force GTX Titan X GPU를 사용하여 학습된다. 테스트하는 동안, 갈망하는(greedy) 디코딩을 사용하여 원본 서론에서 가장 가능성 있는 출력을 생성 한다. 평가를 위해 Lin and Hovy 가 제안한 Recall-Oriented Understudy for Gisting Evaluation (ROUGE)"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 37, "content": "metrics(Lin, 2004)을 채택한다. ROUGE는 인간 평가와 강한 상관 관계가 있는 것으로 입증된 시스템 요약에 점"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 38, "content": "수를 매기는 리콜 중심 측정이다. 후보 요약과 골드 요약 사이의 n-gram 회수율을 측정한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 39, "content": "본 발명 논문의 초록인 하나의 골드 요약만 가지므로, ROUGE 점수는 Li et al. 에 주어진 대로 계산된다. ROUGE-1, ROUGE-2 및 ROUGE-L은 모델의 성능을 보고하는데 사용된다. 성능 평가를 위해, 두 모델 모두 GRU 및 MTGRU의 학습 퍼플릭시티(perplexity)가 표 2에 나와있는 74750 단계로 학습된다. [표 2]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 40, "content": "이 단계에서 조기 정지 지점으로 선택되었으므로, GRU 모델의 가장 낮은 테스트 퍼플릭시티(perplexity)를 얻는 다. 이러한 학습된 네트워크를 사용하여 계산된 ROUGE 점수는 각각 GRU 및 MTGRU 모델에 대해 표 3 및 표 4에 보여진다. [표 3]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 41, "content": "[표 4]"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 42, "content": "본 발명의 MTGRU 모델에 의해 생성된 샘플 요약은 도 6에 보여진다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 43, "content": "도 6은 MTGRU를 사용하여 생성된 요약의 예를 보여주는 도면이다. 도 7은 출력 요약과 추출된 타겟의 예를 보여"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 44, "content": "주는 도면이다. 도 8은 다중 시간 상수 간의 학습 성능을 비교한 그래프이다.도 5 내지 도 7을 참조하면, GRU와 MTGRU를 사용하여 요약 모델에 대해 얻은 ROUGE 점수는 다중 시상수 개념이 고도로 복잡한 아키텍처 계층이 없는 기존의 seq2seq 모델의 성능을 향상시킨다는 것을 보여준다. 또 하나의 큰 장점은 1 에포크(epoch) 정도의 학습 속도 향상이다. 더욱이, 도 6은 모델이 큰 단락을 한 줄의"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 45, "content": "요약으로 요약하는 어려운 작업을 성공적으로 일반화했다는 것을 보여준다. 시상수 τ의 매개 변수에 따라 다음과 같이 설명한다(Yamashita and Tani, 2008). 상위 계층이 더 느린 컨텍스 트 단위를 갖도록 계층을 올라 갈수록 τ는 점진적으로 증가한다. 또한, 도 8에 보여지는 바와 같이, τ의 다중 설정을 실험하고 학습 성능을 비교한다. MTRGU-2 및 MTRGU-3의 τ 는 각각 {1, 1.42, 2, 2.5} 및 {1,1,1.25,1.25}로 설정된다. MTGRU-1은 앞서 설명한 실험에서 채택한 최종 모델이다. MTGRU-2는 상대적으로 느린 컨텍스트 계층을 가지고 있 으며, MTGRU-3은 두 개의 빠른 컨텍스트 계층과 두 개의 느린 컨텍스트 계층을 가지고 있다. 비교에서 볼 수 있 듯이, MTRGU-1의 학습 성능은 다른 두 개보다 뛰어나므로 시상수 설정을 뒷받침하는 근거가 될 수 있다. 실험의 결과는 기능적 차별화와 같은 조직적 프로세스가 언어 작업에서 RNN에서 발생한다는 증거를 제공한다. MTGRU는 기존 GRU보다 1 에포크(epoch) 만큼 빠르게 학습할 수 있다. MTRGU가 RNN에서 계층을 다중 시상수로 안 내함에 의해 이미 발생하고 있는 일종의 기능 분화 프로세스를 촉진한다고 예상되며, 그렇지 않은 경우 이 시간 적 계층 구조는 더 점진적으로 발생한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 46, "content": "도 7을 참조하면, 추출된 요약에 대한 입력 단락의 생성된 요약의 비교를 나타낸다. 예제에서 볼 수 있듯이, 본"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 47, "content": "발명에 따른 모델은 여러 문장에서 주요 정보를 성공적으로 추출하여 한 줄 요약으로 재현한다. 시스템은 추출"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 48, "content": "요약에 대해서만 학습되었지만 모델의 일반화 능력으로 인해 전체 단락의 추상화가 가능하다. seq2seq 목적은 소스 시퀀스에서 컨디셔닝된 타겟 시퀀스의 조인트 확률을 최대화하는 것이다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 49, "content": "요약 모델이 소스-추출된 현저한 문장 쌍에 대해 학습되면, 목적은 두 가지 부 목적으로 구성되는 것으로 볼 수 있다. 하나는 가장 현저한 내용을 식별하기 위해 현출성 조사(중요도 추출)를 정확하게 수행하는 것이고, 다른 하나는 문장 타겟의 정확한 순서를 생성하는 것이다. 실제로, 학습 도중, 첫 번째 부 목적의 최적화가 두 번째 부 목표 이전에 달성된다는 것을 관찰하였다. 두 번째 부 목적은 학습 세트와 적합이 발생할 때만 완전히 달성 된다. 모델의 일반화 능력은 모델이 많은 학습 예가 보여 지듯이 주어진 단락 입력(단일 문장에 해당하는 단 하나의 현저한 섹션 만이 아님) 당 여러 가지 현저한 포인트를 학습할 것으로 예상된다는 사실에 기인한다. 이는 도 7 과 같은 결과가 본 모델로부터 어떻게 얻어지는지 설명한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 50, "content": "본 발명에 따른 작업은 seq2seq 추상화 요약에 의미 있는 영향을 미칠 것으로 예상된다. 첫째, 본 발명의 결과 는 테스트 시간에 외부 코퍼스를 참조하지 않고도 현저성(saliency) 식별을 수행하도록 인코더-디코더 모델을 학습할 수 있음을 확인한다. 이는 Rush et al. ; Nallapati et al. 과 같은 연구에서 이미 암시되 었고, 단락-현저한 문장 쌍으로 구성된 데이터의 선택으로 인해 본 발명에서 명백해졌다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 51, "content": "둘째, 확률론적 언어 모델은 추상적 요약의 핵심 기준 설정에 있어서 새로운 단어 생성의 과제를 해결할 수 있 다. Bengio et al. 은 원래 확률론적 언어 모델이 유사한 단어에 대해 훨씬 더 일반화를 달성할 수 있음 을 보여 주었다. 이것은 확률 함수가 단어 임베딩 벡터의 매끄러운 함수라는 사실에 기인한다. 유사한 단어가 유사한 임베딩 벡터를 갖기 때문에, 특징의 작은 변화는 예측된 확률의 작은 변화를 유도한다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 52, "content": "이것은 새로운 문장을 생성하는데 필요한 RNN 언어 모델의 추상적 요약을 위한 가장 유용한 솔루션으로 만든다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 53, "content": "예를 들어, 도 6에서 첫 번째 요약은 본 발명의 모델이 문서에 존재하지 않는 \"explored\"라는 단어를 생성함을"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 54, "content": "보여준다. 더욱이, 본 발명의 결과는 주어진 추상적 타겟이 있다면, 동일한 모델이 완전히 추상적인 요약 시스 템을 학습시킬 수 있음을 시사한다. 이와 같이, 본 발명에서는 내부적 표현 문법을 생성하고 난 후 문장생성 기법을 적용한 후 추상적 표현을 생성"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 55, "content": "함으로써 보다 사람이 작성한 요약에 가까운 요약문을 생성한다. 본 발명은 단순 추출이 아닌 추상적 요약으로"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 56, "content": "써 복수개의 문단으로 이루어진 장문의 텍스트 또한 요약 가능하다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 57, "content": "도 9는 본 발명의 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법의 흐름도이다. 본 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법은, 도 1의 장치와 실질적으로 동일한 구성 에서 진행될 수 있다. 따라서, 도 1의 장치와 동일한 구성요소는 동일한 도면부호를 부여하고, 반복되는 설"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 58, "content": "명은 생략한다. 또한, 본 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법은 복수 문단 텍스트의 추"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 59, "content": "상적 요약문 생성을 수행하기 위한 소프트웨어(애플리케이션)에 의해 실행될 수 있다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 60, "content": "도 9를 참조하면, 본 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법은, 문서를 문단들로 자동 구 분한다(단계 S10). 예를 들어 ArXiv.org에서 컴퓨터 과학(CS) 기사들의 새로운 데이터 세트를 작성하고, LaTeX 소스 파일로부터 소개(Introductions)를 추출하여 단락으로 구분할 수 있다. 문서를 문단들로 자동 구분한 후, 상기 문단을 내부표현 벡터로 변환한다(단계 S20). 본 발명에서는 내부적 표"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 61, "content": "현 문법을 생성하고 난 후 문장생성 기법을 적용한 후 추상적 표현을 생성함으로써 보다 사람이 작성한 요약에"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 62, "content": "가까운 요약문을 생성한다. 본 발명은 단순 추출이 아닌 추상적 요약으로써 복수개의 문단으로 이루어진 장문의"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 63, "content": "텍스트 또한 요약 가능하다. 다중 시상수를 갖는 GRU(Multiple Timescales Gated Recurrent Unit, 이하 MTGRU)의 다중층으로 이루어진 회귀 신경망을 통해, 딥러닝 기술을 기초로 상기 변환된 벡터를 내부표현(representation)으로 생성한다(단계 S30). 상기 회귀신경망은 다중 시상수 기반의 조합가능성(compositionality)을 구비한다. 상기 회귀신경망은 MTGRU을 기본 유닛으로 하는 시간적 계층 모델을 사용한 시퀀스-투-시퀀스(sequence-to- sequence) 모델을 이용한다. 상기 MTGRU는 각 계층마다 설정된 시상수를 가지며, 상기 시상수가 클수록 느린 컨 텍스트 단위를 구성한다. 예를 들어, 계층이 올라갈수록 상기 시상수가 커질 수 있다. 또한, MTGRU의 다중층으로 이루어진 회귀신경망을 통해 상기 내부표현을 복호화한다(단계 40). 이후, 언어 모델 링을 이용한 학습 내용에 따라 복호화된 내부표현으로부터 자연어로 표현되는 문장들을 생성한다(단계 50)."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 64, "content": "결과적으로, 각 문단의 요약 출력을 수집하여 최종 추상적 요약을 출력한다(단계 60). 본 발명은 다중 시상수기반의 조합 가능성(compositionalities)을 구비함으로써 복수개의 문단으로 이루어진 긴"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 65, "content": "텍스트를 요약할 수 있는 기능을 가진다. 본 발명에 따라 생성된 추상적 요약은 기존 추출식 요약 시스템이 생"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 66, "content": "성한 요약문과는 달리 사람이 직접 작성한 요약과 유사하며 딥러닝 기술을 사용하여 자연스러운 문법을 가진다."}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 67, "content": "이와 같은, 복수 문단 텍스트의 추상적 요약문 생성 방법은 애플리케이션으로 구현되거나 다양한 컴퓨터 구성요 소를 통하여 수행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있 다. 상기 컴퓨터 판독 가능한 기록 매체는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조 합하여 포함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거니와 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD- ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 수행하도록 특별히 구성된 하드웨 어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사 용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치는 본 발명에 따른 처 리를 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상에서는 실시예들을 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 산업상 이용가능성"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 68, "content": "본 발명에서 제안하는 장치는, 오늘날 요약이 필요한 모든 영역에서 핵심 기반기술로서 사용될 수 있다. 예를 들면, 구글의 검색 시스템이 그 예가 될 수 있다. 주 응용 시스템으로는 자동적으로 긴 문장에서 중요한 내용을"}
{"patent_id": "10-2017-0030546", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 69, "content": "찾아 요약해주는 시스템이 될 것이다. 특히, 빅데이터 시장이 확대됨에 따라 이러한 기능을 구비함으로써 큰 경 쟁력을 가질 수 있을 것으로 전망한다. 또한, IT 및 HCI(Human Computer Interface) 등의 분야에 활용 될 수있는 핵심 원천 기술. 자연어처리(NLP: Natural Language Processing) 기반 응용 시스템, 검색엔진, 스마트홈 대화 에이전트 등에 적용 가능하다."}
{"patent_id": "10-2017-0030546", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 장치의 블록도이다. 도 2는 게이트 반복 단위(GRU)를 도시한 도면이다. 도 3은 본 발명에서 제안된 다중 시상수를 갖는 GRU(multiple timescale gated recurrent unit)를 보여주는 도 면이다."}
{"patent_id": "10-2017-0030546", "section": "도면", "subsection": "도면설명", "item": 2, "content": "도 4는 본 발명의 요약 모델의 구조를 보여준다. 도 5는 GRU와 MTGRU 간의 학습 속도 비교를 보여주는 그래프이다."}
{"patent_id": "10-2017-0030546", "section": "도면", "subsection": "도면설명", "item": 3, "content": "도 6은 MTGRU를 사용하여 생성된 요약의 예를 보여주는 도면이다.도 7은 출력 요약과 추출된 타겟의 예를 보여주는 도면이다. 도 8은 다중 시간 상수 간의 학습 성능을 비교한 그래프이다."}
{"patent_id": "10-2017-0030546", "section": "도면", "subsection": "도면설명", "item": 4, "content": "도 9는 본 발명의 일 실시예에 따른 복수 문단 텍스트의 추상적 요약문 생성 방법의 흐름도이다."}
