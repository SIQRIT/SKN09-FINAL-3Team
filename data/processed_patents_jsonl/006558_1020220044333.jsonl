{"patent_id": "10-2022-0044333", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0145680", "출원번호": "10-2022-0044333", "발명의 명칭": "인공 지능을 이용한 명상 제공 장치 및 그 방법", "출원인": "어정현", "발명자": "어정현"}}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "입력부를 통해 수신된 음성 정보로부터 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태일 때, 제어부에 의해, 사용자가 눈을 감고 명상 자세를 확립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른자세와 관련한 콘텐츠를 음성 출력부를 통해 출력하는 단계;상기 입력부를 통해 수신된 추가 음성 정보로부터 추출된 추가 키워드가 미리 설정된 호흡 안정 상태와 관련된상태일 때, 상기 제어부에 의해, 사용자가 선택한 사건과 관련해서 장면을 떠올릴 수 있도록 상기 저장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 음성 출력부를 통해 출력하는 단계;상기 제어부에 의해, 순차로 출력되는 서로 다른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련해서 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용,인식, 영향, 바람 및 의도에 대한 정보를 수신하는 단계; 및상기 제어부에 의해, 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용,인식, 영향, 바람 및 의도를 근거로 사용자의 마음 상태를 분석하여, 사용자의 마음 상태 분석 결과를 생성하는단계를 포함하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 사용자가 선택한 사건과 관련해서 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의감정, 세부 감정 내용, 인식, 영향, 바람 및 의도에 대한 정보를 수신하는 단계는,사용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 사용자가선택한 사건과 관련한 상기 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각기관에 대한 정보를 수신하는 과정;사용자가 상기 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용을 수신하는 과정;사용자가 상기 장면에서 느끼는 감정 및 세부 감정 내용을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에따라 상기 장면에 대응해서 사용자가 느끼는 사용자의 감정과, 상기 사용자의 감정과 관련한 세부 감정 내용을수신하는 과정;사용자가 느꼈던 감정에 따른 상기 장면에서의 사용자의 인식을 확인하기 위한 안내 정보를 출력하고, 사용자입력에 따라 상기 장면에 대응해서 사용자가 느꼈던 감정에 따른 상기 장면에서의 사용자의 인식을 수신하는 과정;상기 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향을확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 느꼈던 감정에 따라사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향을 수신하는 과정;상기 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 확인하기 위한안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 수신하는 과정; 및상기 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도를 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도를 수신하는 과정을 포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "공개특허 10-2023-0145680-3-제 1 항에 있어서,상기 제어부에 의해, 상기 장면이 어린 시절에 경험한 사건인지 여부를 판단하는 단계;상기 판단 결과, 상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 과거에 경험한 적이 있음에 대한 정보가 포함된 상태일 때, 상기 제어부에 의해, 과거의 장면과 관련한 대상이 타인인지 또는 본인인지 여부를 확인하는 단계;상기 확인 결과, 상기 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태일 때, 상기 제어부에의해, 상기 장면에 대해서 상대방에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 의도표현 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따라 상기 사용자가 상기 장면에 대해서 상대방에게 하고 싶은 말을 수신하는 단계;미리 설정된 시간이 지나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 상기장면에 대해서 상대방에게 하고 싶은 말을 사용자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 표현 완료 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따라 상기 장면에 대해서 상대방에게 하고 싶은 말을 사용자가 다 했는지여부에 대한 응답 정보를 수신하는 단계; 및상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인하는 단계를 더 포함하는것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴질 때, 상기 제어부에 의해, 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에대한 느낌의 정보 및 느낌의 강도 정보를 수신하는 단계;상기 제어부에 의해, 상기 수신된 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 신체 부위에 대한 느낌의 강도 정보를 포함하는 다른 입력값을 미리 학습된 사용자 몸느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능을 수행하고, 다른 학습 기능 수행 결과에 따라 상기 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과를 생성하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자의 특정 신체 부위에서 느끼는 감정에 집중할 수 있도록 상기저장부에 미리 저장된 지시문 콘텐츠를 출력하는 단계;상기 제어부에 의해, 미리 설정된 시간 간격으로 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가강도 정보가 미리 설정된 기준값 이하인지 여부를 추가 판단하는 단계; 및상기 추가 판단 결과, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가강도 정보가 상기 미리 설정된 기준값 이하일 때, 상기 제어부에 의해, 사용자가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계를 더 포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,상기 제어부에 의해, 사용자 입력에 따른 사용자의 소감 정보를 수신하는 단계;상기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과 및 상기 사용자의소감 정보를 근거로 사용자에 대한 명상 분석 결과를 생성하는 단계; 및공개특허 10-2023-0145680-4-상기 제어부에 의해, 상기 생성된 명상 분석 결과를 출력하는 단계를 더 포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항에 있어서,상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 부위에서 느껴지지 않을 때, 상기 제어부에 의해,사용자가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따른 사용자의 다른 소감 정보를 수신하는 단계;상기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과 및 상기 사용자의 다른 소감 정보를 근거로 사용자에대한 다른 명상 분석 결과를 생성하는 단계; 및상기 제어부에 의해, 상기 생성된 다른 명상 분석 결과를 출력하는 단계를 더 포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 3 항에 있어서,상기 확인 결과, 상기 과거의 장면과 관련한 대상에 대한 정보에 자신이 포함된 상태일 때, 상기 제어부에의해, 상기 과거의 장면에서 어린 자신이 느끼는 감정, 상기 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정 및 상기 과거의 장면에서의 어린 자신에게 하고 싶은 의도에 대한 정보를 수신하는 단계;상기 제어부에 의해, 상기 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도에 대한 정보에 행동과 관련한 정보가 포함된 상태인지 여부를 판단하는 단계;상기 판단 결과, 상기 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의의도에 대한 정보에 상기 행동과 관련한 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용자가 상기 행동을 할 수 있도록 유도하는 안내 정보를 출력하는 단계;상기 제어부에 의해, 상기 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 다른 의도 표현 안내 콘텐츠를 출력하고, 사용자 입력에 따라 상기 사용자가 상기 과거의장면에 대해서 어린 자신에게 하고 싶은 말을 수신하는 단계;미리 설정된 시간이 지나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 다른 표현 완료 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따라 상기 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가다 했는지 여부에 대한 다른 응답 정보를 수신하는 단계; 및상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용자가 어린자신에게 행한 행동에 따라 상기 과거의 장면에서의 어린 자신이 현재 느끼는 추가 감정에 대한 정보를 수신하는 단계; 및상기 제어부에 의해, 상기 과거의 장면에서의 어린 자신으로부터 현재의 상태로 복귀하도록 과거 장면과의 헤어짐을 유도하는 안내 정보를 출력하는 단계를 더 포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이느껴지는지 여부를 확인하는 단계;상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴질 때, 상기 제어공개특허 10-2023-0145680-5-부에 의해, 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에대한 느낌의 정보 및 느낌의 강도 정보를 수신하는 단계;상기 제어부에 의해, 상기 수신된 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 신체 부위에 대한 느낌의 강도 정보를 포함하는 다른 입력값을 미리 학습된 사용자 몸느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능을 수행하고, 다른 학습 기능 수행 결과에 따라 상기 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과를 생성하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자의 특정 신체 부위에서 느끼는 감정에 집중할 수 있도록 상기저장부에 미리 저장된 지시문 콘텐츠를 출력하는 단계;상기 제어부에 의해, 미리 설정된 시간 간격으로 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가강도 정보가 미리 설정된 기준값 이하인지 여부를 추가 판단하는 단계;상기 추가 판단 결과, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가강도 정보가 상기 미리 설정된 기준값 이하일 때, 상기 제어부에 의해, 사용자가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따른 사용자의 또 다른 소감 정보를 수신하는 단계;상기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과 및 상기 사용자의또 다른 소감 정보를 근거로 사용자에 대한 또 다른 명상 분석 결과를 생성하는 단계; 및상기 제어부에 의해, 상기 생성된 또 다른 명상 분석 결과를 출력하는 단계를 더 포함하는 것을 특징으로 하는인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 3 항에 있어서,상기 판단 결과, 상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 어린 시절 경험이 아닌 정보가포함된 상태일 때, 상기 제어부에 의해, 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 다른 장면을 사용자가떠올릴 수 있도록 상기 저장부에 미리 저장된 다른 장면 떠올리기 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 순차로 출력되는 서로 다른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 떠올린 과거 다른 장면에 따른 사용자의 감각기관에 대한 정보, 내용 및 사용자 감정에 대한 정보를 수신하는 단계;상기 제어부에 의해, 상기 사용자가 떠올린 과거 다른 장면과 관련해서 투사 찾기를 수행할지 여부에 대한 사용자 입력값을 수신하는 단계;상기 제어부에 의해, 상기 수신된 사용자 입력값이 투사와 관련한 내용인지 여부를 추가 확인하는 단계;상기 추가 확인 결과, 상기 사용자 입력값에 투사 찾기와 관련한 정보가 포함된 상태일 때, 상기 제어부에의해, 상기 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에따른 두 장면의 공통점에 대한 정보를 수신하는 단계;미리 설정된 시간이 지나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따라 상기 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대공개특허 10-2023-0145680-6-해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신하는 단계;상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에따른 두 장면의 차이점에 대한 정보를 수신하는 단계;미리 설정된 시간이 지나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 사용자 입력에 따라 상기 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신하는 단계;상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 다른 장면을 사용자가 떠올릴 수 있도록 상기 저장부에 미리저장된 또 다른 장면 떠올리기 안내 콘텐츠를 출력하는 단계;상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 장면에 따른 사용자의인식, 영향, 바람 및 의도에 대한 정보를 수신하는 과정으로 복귀하여, 상기 장면과 관련해서 사용자가 느끼는몸 느낌에 대응하는 과거의 다른 장면에 따른 사용자의 인식, 영향, 바람 및 의도에 대한 정보를 수신하는단계; 및상기 제어부에 의해, 상기 수신된 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도를 근거로 사용자의 마음 상태를 분석하는 과정부터 재수행하는 단계를 더포함하는 것을 특징으로 하는 인공 지능을 이용한 명상 제공 방법."}
{"patent_id": "10-2022-0044333", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "사용자로부터 음성 정보를 수신하는 입력부; 및상기 수신된 음성 정보로부터 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태일 때 사용자가 눈을감고 명상 자세를 확립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 상기 음성 출력부를 통해 출력하고, 상기 입력부를 통해 수신된 추가 음성 정보로부터 추출된 추가 키워드가 미리설정된 호흡 안정 상태와 관련된 상태일 때 사용자가 선택한 사건과 관련해서 장면을 떠올릴 수 있도록 상기 저장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 음성 출력부를 통해 출력하고, 순차로 출력되는 서로 다른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련해서 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도에 대한 정보를 수신하고, 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향,바람 및 의도를 근거로 사용자의 마음 상태를 분석하여, 사용자의 마음 상태 분석 결과를 생성하는 제어부를 포함하는 인공 지능을 이용한 명상 제공 장치."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공 지능을 이용한 명상 제공 장치 및 그 방법을 개시한다. 즉, 본 발명은 사용자의 현재 상태에 따 른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시절을 생각하면서 느끼는 몸 느낌 상 태를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공함으로써, 실시간으로 확인되는 사용자의 마음 상태 및 몸 느낌 상태에 따라 효율적으로 명상 정보를 제공하고, 사용자 자신이 상황에 대한 이해와 공감을 통해 평정심 또는 생각멈춤을 높일 수 있다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공 지능을 이용한 명상 제공 장치 및 그 방법에 관한 것으로서, 특히 사용자의 현재 상태에 따른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시절을 생각하면서 느끼는 몸 느낌 상태 를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공하는 인공 지능을 이용한 명상 제공 장치 및 그 방법에 관한 것이다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "명상은 눈을 감고 차분한 마음으로 깊이 생각하는 것을 나타낸다. 이러한 명상은 상담자와 내담자 간의 유기적인 대화를 통해서 내담자의 상태에 따라 점진적으로 명상의 깊이를 조절하고 있으나, 사람과 사람 간의 대화 유도에 따라, 명상에 대한 집중도가 낮아질 수 있고, 명상 전후의 내담자의 상태에 대한 피드백 제공이 결여되고 있어, 명상의 효과를 제대로 파악하지 못하고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허 제10-2021-0004863호 [제목: 명상시스템 및 명상시스템의 운영방법]"}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 사용자의 현재 상태에 따른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시절을 생각하면서 느끼는 몸 느낌 상태를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공하는 인공 지능을 이용한 명상 제공 장치 및 그 방 법을 제공하는 데 있다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 방법은 입력부를 통해 수신된 음성 정보로부터 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태일 때, 제어부에 의해, 사용자가 눈을 감고 명상 자세를 확 립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 음성 출력부를 통해 출 력하는 단계; 상기 입력부를 통해 수신된 추가 음성 정보로부터 추출된 추가 키워드가 미리 설정된 호흡 안정 상태와 관련된 상태일 때, 상기 제어부에 의해, 사용자가 선택한 사건과 관련해서 장면을 떠올릴 수 있도록 상 기 저장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 음성 출력부를 통해 출력하는 단계; 상기 제어부에 의해, 순차로 출력되는 서로 다른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련 해서 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바 람 및 의도에 대한 정보를 수신하는 단계; 및 상기 제어부에 의해, 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도를 근거로 사용자의 마음 상태를 분석하 여, 사용자의 마음 상태 분석 결과를 생성하는 단계를 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 사용자가 선택한 사건과 관련해서 상기 장면에 따른 사용자의 감각기관에 대 한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도에 대한 정보를 수신하는 단계는, 사 용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 사용자가 선 택한 사건과 관련한 상기 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각기관에 대한 정보를 수신하는 과정; 사용자가 상기 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용을 수신하는 과정; 사용자가 상기 장면에서 느끼는 감정 및 세부 감정 내용을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 느끼는 사용자의 감정과, 상기 사용자의 감정과 관련한 세부 감정 내용을 수신하는 과정; 사용자가 느꼈던 감정에 따른 상기 장면에서의 사용자의 인식을 확인하기 위한 안내 정 보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 느꼈던 감정에 따른 상기 장면에서의 사용 자의 인식을 수신하는 과정; 상기 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었 는지에 대한 사용자의 영향을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향을 수신하는 과정; 상기 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 수신하는 과정; 및 상기 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동 에 대한 사용자의 의도를 확인하기 위한 안내 정보를 출력하고, 사용자 입력에 따라 상기 장면에 대응해서 사용 자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도를 수신하는 과정을 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 제어부에 의해, 상기 장면이 어린 시절에 경험한 사건인지 여부를 판단하는 단계; 상기 판단 결과, 상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 과거에 경험한 적이 있음 에 대한 정보가 포함된 상태일 때, 상기 제어부에 의해, 과거의 장면과 관련한 대상이 타인인지 또는 본인인지여부를 확인하는 단계; 상기 확인 결과, 상기 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태일 때, 상기 제어부에 의해, 상기 장면에 대해서 상대방에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 의도 표현 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따라 상기 사용자가 상기 장면에 대해서 상대방에게 하고 싶은 말을 수신하는 단계; 미리 설정된 시간이 지나도록 사용자 입력에 따 른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 상기 장면에 대해서 상대방에게 하고 싶은 말을 사용 자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 표현 완료 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따라 상기 장면에 대해서 상대방에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 응답 정보를 수신하는 단계; 및 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상 기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느 껴지는지 여부를 확인하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴질 때, 상기 제어부에 의해, 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대 응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신하는 단계; 상기 제어부에 의해, 상 기 수신된 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 신체 부위에 대한 느낌의 강도 정보를 포함하는 다른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능을 수행하고, 다른 학습 기능 수행 결과에 따라 상기 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과를 생성하는 단계; 상기 제어부에 의해, 상기 장면과 관련해서 사용자의 특정 신체 부 위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 지시문 콘텐츠를 출력하는 단계; 상기 제어 부에 의해, 미리 설정된 시간 간격으로 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응 하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신하는 단계; 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 미리 설정된 기준값 이하인지 여부를 추가 판단하는 단계; 및 상기 추가 판단 결과, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값 이하일 때, 상기 제어부에 의해, 사용자 가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 제어부에 의해, 사용자 입력에 따른 사용자의 소감 정보를 수신하는 단계; 상 기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과 및 상기 사용자의 소 감 정보를 근거로 사용자에 대한 명상 분석 결과를 생성하는 단계; 및 상기 제어부에 의해, 상기 생성된 명상 분석 결과를 출력하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 부위에서 느껴지지 않을 때, 상기 제어부에 의해, 사용자가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상 기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따른 사용자의 다른 소감 정보를 수신하는 단계; 상기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과 및 상기 사용자의 다른 소감 정보를 근거로 사용자에 대한 다른 명상 분석 결과를 생성하는 단 계; 및 상기 제어부에 의해, 상기 생성된 다른 명상 분석 결과를 출력하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 확인 결과, 상기 과거의 장면과 관련한 대상에 대한 정보에 자신이 포함된 상 태일 때, 상기 제어부에 의해, 상기 과거의 장면에서 어린 자신이 느끼는 감정, 상기 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정 및 상기 과거의 장면에서의 어린 자신에게 하고 싶은 의도에 대한 정보를 수 신하는 단계; 상기 제어부에 의해, 상기 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행 동에 대한 사용자의 의도에 대한 정보에 행동과 관련한 정보가 포함된 상태인지 여부를 판단하는 단계; 상기 판 단 결과, 상기 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도에 대한 정보에 상기 행동과 관련한 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용자가 상기 행동을 할 수 있도록 유도하는 안내 정보를 출력하는 단계; 상기 제어부에 의해, 상기 과거의 장면에 대해서 어린 자신에 게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 다른 의도 표현 안내 콘텐츠를 출력하고, 사용자 입력에 따라 상기 사용자가 상기 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 수신하는 단계; 미리 설정된 시간이 지나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 과거 의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미 리 저장된 다른 표현 완료 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따라 상기 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 다른 응답 정보를 수신하는단계; 및 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용자 가 어린 자신에게 행한 행동에 따라 상기 과거의 장면에서의 어린 자신이 현재 느끼는 추가 감정에 대한 정보를 수신하는 단계; 및 상기 제어부에 의해, 상기 과거의 장면에서의 어린 자신으로부터 현재의 상태로 복귀하도록 과거 장면과의 헤어짐을 유도하는 안내 정보를 출력하는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인하는 단계; 상기 장면과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴질 때, 상기 제어부에 의해, 사용자 입력에 따라 상기 장면과 관 련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신 하는 단계; 상기 제어부에 의해, 상기 수신된 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 신체 부위에 대한 느낌의 강도 정보를 포함하는 다른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능을 수행하고, 다른 학습 기능 수행 결과에 따 라 상기 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과를 생성하는 단계; 상기 제어부에 의해, 상기 장면 과 관련해서 사용자의 특정 신체 부위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 지시문 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 미리 설정된 시간 간격으로 사용자 입력에 따라 상기 장면과 관 련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신하는 단계; 상기 제 어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정 보가 미리 설정된 기준값 이하인지 여부를 추가 판단하는 단계; 상기 추가 판단 결과, 상기 장면과 관련해서 사 용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값 이하일 때, 상기 제어부에 의해, 사용자가 상기 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저 장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사 용자 입력에 따른 사용자의 또 다른 소감 정보를 수신하는 단계; 상기 제어부에 의해, 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과 및 상기 사용자의 또 다른 소감 정보를 근거로 사용자에 대한 또 다른 명상 분석 결과를 생성하는 단계; 및 상기 제어부에 의해, 상기 생성된 또 다른 명상 분석 결과를 출력하 는 단계를 더 포함할 수 있다. 본 발명과 관련된 일 예로서 상기 판단 결과, 상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 어 린 시절 경험이 아닌 정보가 포함된 상태일 때, 상기 제어부에 의해, 사용자 입력에 따라 상기 장면과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신하는 단계; 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 다른 장면을 사용자가 떠올릴 수 있도록 상기 저 장부에 미리 저장된 다른 장면 떠올리기 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 순차로 출력되는 서로 다른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 떠올린 과거 다른 장면에 따른 사용자의 감 각기관에 대한 정보, 내용 및 사용자 감정에 대한 정보를 수신하는 단계; 상기 제어부에 의해, 상기 사용자가 떠올린 과거 다른 장면과 관련해서 투사 찾기를 수행할지 여부에 대한 사용자 입력값을 수신하는 단계; 상기 제 어부에 의해, 상기 수신된 사용자 입력값이 투사와 관련한 내용인지 여부를 추가 확인하는 단계; 상기 추가 확 인 결과, 상기 사용자 입력값에 투사 찾기와 관련한 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사용 자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 공통점에 대한 정보를 수신하는 단계; 미리 설정된 시간이 지 나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 사건과 관련한 현재의 장면 과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따라 상기 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신하 는 단계; 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 사 용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자가 상기 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 차이점에 대한 정보를 수신하는 단계; 미리 설정된 시간이 지 나도록 사용자 입력에 따른 정보가 추가로 수신되지 않을 때, 상기 제어부에 의해, 사건과 관련한 현재의 장면 과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 사용자 입력에 따라 상기 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신하 는 단계; 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태일 때, 상기 제어부에 의해, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 다른 장면을 사용자가 떠올릴 수 있도록 상기 저장 부에 미리 저장된 또 다른 장면 떠올리기 안내 콘텐츠를 출력하는 단계; 상기 제어부에 의해, 상기 장면과 관련 해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 장면에 따른 사용자의 인식, 영향, 바람 및 의도에 대한 정보 를 수신하는 과정으로 복귀하여, 상기 장면과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거의 다른 장면 에 따른 사용자의 인식, 영향, 바람 및 의도에 대한 정보를 수신하는 단계; 및 상기 제어부에 의해, 상기 수신 된 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도를 근거로 사용자의 마음 상태를 분석하는 과정부터 재수행하는 단계를 더 포함할 수 있다. 본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 장치는 사용자로부터 음성 정보를 수신하는 입력부; 및 상기 수신된 음성 정보로부터 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태일 때 사용자가 눈을 감고 명상 자세를 확립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 상 기 음성 출력부를 통해 출력하고, 상기 입력부를 통해 수신된 추가 음성 정보로부터 추출된 추가 키워드가 미리 설정된 호흡 안정 상태와 관련된 상태일 때 사용자가 선택한 사건과 관련해서 장면을 떠올릴 수 있도록 상기 저 장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 음성 출력부를 통해 출력하고, 순차로 출력되는 서로 다 른 안내 정보에 대응하여 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련해서 상기 장면에 따른 사용자 의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도에 대한 정보를 수 신하고, 상기 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람 및 의도를 근거로 사용자의 마음 상태를 분석하여, 사용자의 마음 상태 분석 결과를 생성하는 제어부를 포 함할 수 있다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 사용자의 현재 상태에 따른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시 절을 생각하면서 느끼는 몸 느낌 상태를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공함으로써, 실시간으로 확인되는 사용자의 마음 상태 및 몸 느낌 상태에 따라 효율적으로 명상 정보를 제공하고, 사용자 자신이 상황에 대한 이해와 공감을 통해 평정심 또는 생각멈춤을 높일 수 있는 효과가 있다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에서 사용되는 기술적 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려 는 의도가 아님을 유의해야 한다. 또한, 본 발명에서 사용되는 기술적 용어는 본 발명에서 특별히 다른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 의미로 해석되어야 하며, 과도하게 포괄적인 의미로 해석되거나, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에 는 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용 되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 발명에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함한다. 본 발명에서 \"구성된다\" 또는 \"포함한다\" 등의 용어는 발명에 기재된 여러 구성 요소들 또는 여러 단계를 반드 시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수 도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 발명에서 사용되는 제 1, 제 2 등과 같이 서수를 포함하는 용어는 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요소들은 용어들에 의해 한정되어서는 안 된다. 용어들은 하나의 구성 요소를 다른 구성 요소 로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성 요소는 제 2 구성 요소로 명명될 수 있고, 유사하게 제 2 구성 요소도 제 1 구성 요소로 명명될 수 있다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, 도면 부호에 관계없이 동일 하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 발명의 사상을 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아니 됨을 유의해야 한다. 도 1은 본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 장치의 구성을 나타낸 블록도이다. 도 1에 도시한 바와 같이, 인공 지능을 이용한 명상 제공 장치는 통신부, 저장부, 입력부, 표시부, 음성 출력부 및 제어부로 구성된다. 도 1에 도시된 명상 제공 장치의 구성 요소 모두가 필수 구성 요소인 것은 아니며, 도 1에 도시된 구성 요소보다 많은 구성 요소에 의해 명상 제공 장치 가 구현될 수도 있고, 그보다 적은 구성 요소에 의해서도 명상 제공 장치가 구현될 수도 있다. 상기 명상 제공 장치는 인공지능 스피커, 스마트폰(Smart Phone), 휴대 단말기(Portable Terminal), 이동 단말기(Mobile Terminal), 폴더블 단말기(Foldable Terminal), 개인 정보 단말기(Personal Digital Assistant: PDA), PMP(Portable Multimedia Player) 단말기, 텔레매틱스(Telematics) 단말기, 내비게이션(Navigation) 단 말기, 개인용 컴퓨터(Personal Computer), 노트북 컴퓨터, 슬레이트 PC(Slate PC), 태블릿 PC(Tablet PC), 울 트라북(ultrabook), 웨어러블 디바이스(Wearable Device, 예를 들어, 워치형 단말기(Smartwatch), 글래스형 단 말기(Smart Glass), HMD(Head Mounted Display) 등 포함), 와이브로(Wibro) 단말기, IPTV(Internet Protocol Television) 단말기, 스마트 TV, 디지털방송용 단말기, AVN(Audio Video Navigation) 단말기, A/V(Audio/Video) 시스템, 플렉시블 단말기(Flexible Terminal), 디지털 사이니지 장치 등과 같은 다양한 단말 기에 적용될 수 있다. 상기 통신부는 유/무선 통신망을 통해 내부의 임의의 구성 요소 또는 외부의 임의의 적어도 하나의 단말기 와 통신 연결한다. 이때, 상기 외부의 임의의 단말기는 서버(미도시), 다른 단말(미도시) 등을 포함할 수 있다. 여기서, 무선 인터넷 기술로는 무선랜(Wireless LAN: WLAN), DLNA(Digital Living Network Alliance), 와이브 로(Wireless Broadband: Wibro), 와이맥스(World Interoperability for Microwave Access: Wimax), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), IEEE 802.16, 롱 텀 에볼루션(Long Term Evolution: LTE), LTE-A(Long Term Evolution-Advanced), 광대역 무선 이동 통신 서비스 (Wireless Mobile Broadband Service: WMBS) 등이 있으며, 상기 통신부는 상기에서 나열되지 않은 인터넷 기술까지 포함한 범위에서 적어도 하나의 무선 인터넷 기술에 따라 데이터를 송수신하게 된다. 또한, 근거리 통 신 기술로는 블루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association: IrDA), UWB(Ultra Wideband), 지그비(ZigBee), 인접 자장 통신(Near Field Communication: NFC), 초음파 통신(Ultra Sound Communication: USC), 가시광 통신(Visible Light Communication: VLC), 와이 파이(Wi-Fi), 와이 파이 다이렉트(Wi-Fi Direct) 등이 포함될 수 있다. 또한, 유선 통신 기술로는 전력선 통신 (Power Line Communication: PLC), USB 통신, 이더넷(Ethernet), 시리얼 통신(serial communication), 광/동축 케이블 등이 포함될 수 있다. 또한, 상기 통신부는 유니버설 시리얼 버스(Universal Serial Bus: USB)를 통해 임의의 단말과 정보를 상 호 전송할 수 있다. 또한, 상기 통신부는 이동통신을 위한 기술표준들 또는 통신방식(예를 들어, GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), CDMA2000(Code Division Multi Access 2000), EV-DO(Enhanced Voice-Data Optimized or Enhanced Voice-Data Only), WCDMA(Wideband CDMA), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE- A(Long Term Evolution-Advanced) 등)에 따라 구축된 이동 통신망 상에서 기지국, 상기 서버, 상기 다른 단말 등과 무선 신호를 송수신한다. 또한, 상기 통신부는 상기 제어부의 제어에 의해, 상기 서버로부터 제공되는 전용 앱을 수신한다. 또한, 상기 통신부는 상기 제어부의 제어에 의해, 해당 명상 제공 장치의 사용자가 명상 기능을 수행함에 따라 수집되거나 계산된 다양한 정보들을 상기 서버에 등록(또는 전송)한다. 상기 저장부는 다양한 사용자 인터페이스(User Interface: UI), 그래픽 사용자 인터페이스(Graphic User Interface: GUI) 등을 저장한다. 또한, 상기 저장부는 상기 명상 제공 장치가 작동하는데 필요한 데이터와 프로그램 등을 저장한다. 즉, 상기 저장부는 상기 명상 제공 장치에서 구동되는 다수의 응용 프로그램(application program 또 는 애플리케이션(application)), 명상 제공 장치의 작동을 위한 데이터들, 명령어들을 저장할 수 있다. 이 러한 응용 프로그램 중 적어도 일부는 무선 통신을 통해 외부 서버로부터 다운로드 될 수 있다. 또한, 이러한 응용 프로그램 중 적어도 일부는 명상 제공 장치의 기본적인 기능을 위하여 출고 당시부터 명상 제공 장치 상에 존재할 수 있다. 한편, 응용 프로그램은 상기 저장부에 저장되고, 명상 제공 장치에 설치 되어, 제어부에 의하여 상기 명상 제공 장치의 작동(또는 기능)을 수행하도록 구동될 수 있다. 또한, 상기 저장부는 플래시 메모리 타입(Flash Memory Type), 하드 디스크 타입(Hard Disk Type), 멀티 미디어 카드 마이크로 타입(Multimedia Card Micro Type), 카드 타입의 메모리(예를 들면, SD 또는 XD 메모리 등), 자기 메모리, 자기 디스크, 광디스크, 램(Random Access Memory: RAM), SRAM(Static Random Access Memory), 롬(Read-Only Memory: ROM), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory) 중 적어도 하나의 저장매체를 포함할 수 있다. 또한, 명상 제공 장치 는 인터넷(internet)상에서 저장부의 저장 기능을 수행하는 웹 스토리지(web storage)를 운영하거나, 또는 상기 웹 스토리지와 관련되어 작동할 수도 있다. 또한, 상기 저장부는 상기 제어부의 제어에 의해 상기 수신된 전용 앱을 해당 명상 제공 장치에 설치(또는 인스톨/저장)한다. 또한, 상기 저장부는 상기 제어부의 제어에 의해 해당 명상 제공 장치의 사용자가 명상 기능을 수행함에 따라 수집되거나 계산된 다양한 정보들을 저장한다. 상기 입력부는 오디오 신호 또는 비디오 신호를 입력받기 위한 적어도 하나 이상의 마이크(미도시)와 카메 라(미도시) 등을 포함할 수 있다. 상기 마이크는 통화 모드, 녹음 모드, 음성 인식 모드, 영상 회의 모드, 영상 통화 모드 등에서 마이크로폰(Microphone)에 의해 외부의 음향 신호(사용자의 음성(음성 신호 또는 음성 정보) 포함)를 수신하여 전기적인 음성 데이터로 처리한다. 또한, 상기 처리된 음성 데이터는 상기 음성 출력부 를 통해 출력하거나 또는 상기 통신부를 통하여 외부 단말기로 송신 가능한 형태로 변환되어 출력될 수 있 다. 또한, 상기 마이크는 외부의 음향 신호를 입력받는 과정에서 발생하는 잡음을 제거하기 위한 다양한 잡음 제거 알고리즘이 구현될 수도 있다. 또한, 상기 카메라는 영상 통화 모드, 촬영 모드, 영상 회의 모드 등에서 이미지 센서(카메라 모듈 또는 카메라)에 의해 얻어지는 정지영상 또는 동영상 등의 화상 프레임을 처리한다. 즉, 코덱(CODEC)에 따라 상기 이 미지 센서에 의해 얻어지는 해당 화상 데이터들을 각 규격에 맞도록 인코딩/디코딩한다. 상기 처리된 화상 프레 임은 상기 제어부의 제어에 의해 상기 표시부에 표시될 수 있다. 일 예로, 상기 카메라는 객체(또는 피사체)(사용자 영상)를 촬영하고, 그 촬영된 영상(피사체 영상)에 대응하는 비디오 신호를 출력한다. 또한, 상기 카메라에서 처리된 화상 프레임은 상기 저장부에 저장되거나 상기 통신부를 통해 단말(미 도시), 서버(미도시) 등에 전송될 수 있다. 또한, 상기 입력부는 사용자에 의한 버튼 조작 또는 임의의 기능 선택에 따른 신호를 수신하거나, 디스플 레이되는 화면을 터치/스크롤하는 등의 조작에 의해 생성된 명령 또는 제어 신호를 수신한다. 또한, 상기 입력부는 사용자에 의해 입력된 정보에 대응하는 신호를 수신하며, 키 패드(Key Pad), 돔 스위 치 (Dome Switch), 터치 패드(예를 들어 접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등 포함), 터치 스크린(Touch Screen), 조그 휠, 조그 스위치, 조그 셔틀(Jog Shuttle), 조이스틱, 마우스(mouse), 스타일러스 펜(Stylus Pen), 터치 펜(Touch Pen) 등의 다양한 장치가 사용될 수 있다. 또한, 상기 입력부는 사용자의 발화에 의한 음성 정보를 수신(또는 수집)한다. 이때, 상기 음성 정보는 음 성 정보 수신(또는 음성 인식 기능 수행)을 위해서 미리 설정된 명령어(예를 들어 위빠사나), 해당 명상 제공 장치의 사용자가 경험한 특정 사건에 대한 정보 등을 포함할 수 있다. 여기서, 상기 위빠사나는 산스크리 트어로, '위(Vi)'라는 단어와 '빠사나(Passana)'란 두 개의 단어가 결합하여 만들어진 단어이며, 위빠사나 명상 은 명상 방법 중 하나이다. 이때, 상기 입력부(또는 상기 입력부를 포함하는 상기 명상 제공 장치 )는 대기 상태를 유지하고 있던 상태에서, 사용자의 발화에 따른 상기 음성 정보를 수신할 수 있다. 상기 표시부(또는 디스플레이부)는 상기 제어부의 제어에 의해 상기 저장부에 저장된 사용자 인 터페이스 및/또는 그래픽 사용자 인터페이스를 이용하여 다양한 메뉴 화면 등과 같은 다양한 콘텐츠를 표시할수 있다. 여기서, 상기 표시부에 표시되는 콘텐츠는 다양한 텍스트 또는 이미지 데이터(각종 정보 데이터 포함)와 아이콘, 리스트 메뉴, 콤보 박스 등의 데이터를 포함하는 메뉴 화면 등을 포함한다. 또한, 상기 표시부 는 터치 스크린 일 수 있다. 또한, 상기 표시부는 액정 디스플레이(Liquid Crystal Display: LCD), 박막 트랜지스터 액정 디스플레이 (Thin Film Transistor-Liquid Crystal Display: TFT LCD), 유기 발광 다이오드(Organic Light-Emitting Diode: OLED), 플렉시블 디스플레이(Flexible Display), 3차원 디스플레이(3D Display), 전자잉크 디스플레이 (e-ink display), LED(Light Emitting Diode) 중에서 적어도 하나를 포함할 수 있다. 또한, 상기 표시부는 상기 제어부의 제어에 의해 앱 실행 결과 화면을 표시한다. 상기 음성 출력부는 상기 제어부에 의해 소정 신호 처리된 신호에 포함된 음성 정보를 출력한다. 여 기서, 상기 음성 출력부에는 리시버(receiver), 스피커(speaker), 버저(buzzer) 등이 포함될 수 있다. 또한, 상기 음성 출력부는 상기 제어부에 의해 생성된 안내 음성을 출력한다. 또한, 상기 음성 출력부는 상기 제어부의 제어에 의해 상기 앱 실행 결과 화면에 대응하는 음성 정보 등을 출력한다. 상기 제어부(controller, 또는 MCU(microcontroller unit)는 상기 명상 제공 장치의 전반적인 제어 기능을 실행한다. 또한, 상기 제어부는 상기 저장부에 저장된 프로그램 및 데이터를 이용하여 명상 제공 장치의 전반적인 제어 기능을 실행한다. 상기 제어부는 RAM, ROM, CPU, GPU, 버스를 포함할 수 있으며, RAM, ROM, CPU, GPU 등은 버스를 통해 서로 연결될 수 있다. CPU는 상기 저장부에 액세스하여, 상기 저장부 에 저장된 O/S를 이용하여 부팅을 수행할 수 있으며, 상기 저장부에 저장된 각종 프로그램, 콘텐츠, 데이터 등을 이용하여 다양한 작동을 수행할 수 있다. 또한, 상기 제어부는 상기 서버와의 연동에 의해, 상기 서버에서 제공하는 전용 앱 및/또는 웹 사이트를 통해 명상 제공 기능, 사용자의 명상 기능 수행에 따른 마음 상태 분석 기능, 몸 느낌 분석 기능, 명상 분석 결 과 제공 기능/평가 정보 제공 기능 등을 제공받기 위한 일반 사용자로 회원 가입하며, 개인 정보 등을 상기 서 버에 등록한다. 이때, 상기 개인 정보는 아이디, 이메일 주소, 패스워드(또는 비밀번호), 이름, 성별, 생년월일, 연락처, 주소지(또는 주소정보), 면허번호(또는 면허정보) 등을 포함한다. 또한, 상기 제어부는 해당 명상 제공 장치의 사용자가 가입한 SNS 계정 정보 또는 타사이트 계정 정 보를 이용하여 상기 서버에 사용자로 회원 가입할 수도 있다. 여기서, 상기 SNS 계정은 페이스북, 트위터, 인스 타그램, 카카오 스토리, 네이버 블로그 등과 관련한 정보일 수 있다. 또한, 상기 타사이트 계정은 유튜브, 카카 오, 네이버 등과 관련한 정보일 수 있다. 또한, 회원 가입 절차 수행 시, 상기 제어부는 본인 인증 수단(예를 들어 이동 전화, 신용카드, 아이핀 등 포함)을 통한 인증 기능을 완료해야 상기 서버에 대한 회원 가입 절차를 정상적으로 완료할 수 있다. 또한, 회원 가입이 완료된 후, 상기 제어부는 상기 서버에서 제공하는 서비스를 이용하기 위해서, 상기 서 버로부터 제공되는 전용 앱(또는 애플리케이션/응용 프로그램/특정 앱)을 해당 명상 제공 장치에 설치한다. 이때, 상기 전용 앱은 명상 제공 기능, 사용자의 명상 기능 수행에 따른 마음 상태 분석 기능, 몸 느 낌 분석 기능, 명상 분석 결과 제공 기능/평가 정보 제공 기능 등을 수행하기 위한 앱일 수 있다. 또한, 회원 가입이 완료된 후, 상기 제어부는 상기 서버에서 제공되는 할인 쿠폰을 해당 전용 앱을 통해 표시할 수 있다. 이때, 상기 할인 쿠폰은 해당 서버에서 제공하는 기능 등을 수행시 일정 비율의 할인 정보를 포함하는 할인 쿠폰일 수 있다. 또한, 상기 제어부는 상기 서버 및 결제 서버(미도시)와 연동하여, 해당 서버에서 제공하는 전용 앱을 통 한 명상 제공 기능 등을 수행하기 위해서 결제 기능(또는 구독 기능에 따른 결제 기능)을 수행한다. 또한, 결제가 실패한 경우, 상기 제어부는 상기 서버(또는 상기 결제 서버)로부터 전송되는 결제가 실패한 상태임을 나타내는 정보(예를 들어 잔액 부족, 한도 초과 등 포함)를 상기 통신부를 통해 수신하고, 상기 수신된 결제가 실패한 상태임을 나타내는 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력 (또는 표시)한다.또한, 결제가 성공한 경우, 상기 제어부는 상기 서버(또는 상기 결제 서버)로부터 전송되는 결제 기능 수 행 결과를 상기 통신부를 통해 수신하고, 상기 수신된 결제 기능 수행 결과를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력(또는 표시)한다. 여기서, 상기 결제 기능 수행 결과는 구독 기간, 결제 건 에 대한 정보, 총 결제 금액, 결제 날짜 및 시각 정보 등을 포함한다. 또한, 상기 제어부는 사전에 수집된 복수의 학습용 데이터세트를 지속적인 기계학습(또는 딥러닝)의 데이 터로 활용한다. 여기서, 상기 기계학습을 위한 입력 데이터세트(또는 학습용 데이터세트)는 미리 설정된 비율 (예를 들어 7:3)로 훈련 세트(train set)와 테스트 세트(test set)로 분할하여, 훈련 및 테스트 기능을 수행할 수 있다. 또한, 상기 기계학습을 위한 입력 데이터세트는 추후 수집되는 상기 명상 제공 장치의 사용자 입 력에 따른 입력 항목별 복수의 입력값 등을 포함한다. 또한, 상기 기계학습을 위한 출력 데이터세트는 예측하고 싶은 부분으로, 특정 사건의 특정 장면과 관련해서 사용자의 마음 상태에 대한 분석 결과, 특정 사건의 특정 장 면과 관련해서 사용자의 몸 느낌에 대한 분석 결과 등을 포함한다. 즉, 상기 제어부는 미리 설정된 학습용 데이터를 통해 사용자 마음 상태 분석 모델(또는 서포트 벡터 머신, Support Vector Machine: SVM)에 대해서 특정 로우 데이터와 관련해서 해당 입력값 등에 따른 특정 사건 의 특정 장면과 관련한 사용자의 마음 상태에 대한 분석 결과 제공을 위한 학습 기능을 수행한다. 또한, 상기 제어부는 미리 설정된 학습용 데이터를 통해 사용자 몸 느낌 상태 분석 모델에 대해서 특정 로 우 데이터와 관련해서 해당 다른 입력값 등에 따른 특정 사건의 특정 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과 제공을 위한 학습 기능을 수행한다. 이때, 상기 제어부는 로우 데이터(예를 들어 학습용 데이 터 등 포함)를 병렬 및 분산하여 저장하고, 저장된 로우 데이터(또는 학습용 데이터 등 포함) 내에 포함된 비정 형(Unstructed) 데이터, 정형(Structured) 데이터, 반정형 데이터(Semi-structured)를 정제하고, 메타 데이터 로 분류를 포함한 전처리를 실시하고, 전처리된 데이터를 데이터 마이닝(Data Mining)을 포함하는 분석을 실시 하고 적어도 하나의 종류의 기계학습에 기반하여 학습, 훈련 및 테스트를 진행하여 빅데이터를 구축할 수 있다. 이때, 적어도 하나의 종류의 기계학습은 지도 학습(Supervised Learning), 반지도 학습(Semi-Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning) 및 심층 강화 학습(Deep Reinforcement Learning) 중 어느 하나 또는 적어도 하나의 조합으로 이루어질 수 있다. 이와 같이, 상기 제어부는 상기 학습용 데이터 등을 통해서 뉴럴 네트워크(Neural Networks) 형태의 상기 사용자 마음 상태 분석 모델, 상기 사용자 몸 느낌 상태 분석 모델 등에 대해서 학습 기능을 수행한다. 명상의 경우, 그 대상이 몸(身), 느낌(愛), 마음(心) 및 법(法)일 수 있다. 여기서, 상기 몸은 호흡명상, 걷기 명상, 부정관(不淨觀: 인간의 몸이 더러운 것을 깨달아 탐욕을 없애는 마음을 수행하는 방법) 등의 집중명상이 나 관찰명상을 나타내고, 상기 느낌은 몸 느낌의 변화를 관찰하는 관찰명상을 나타내고, 상기 마음은 마음에서 일어나는 마음 현상과 대상을 알아차리는 의식(意識)을 관찰하는 관찰명상을 나타내고, 상기 법은 고유의 성질 을 가진 최소 단위를 나타낸다. 또한, 명상을 위한 감각기관으로는 눈, 귀, 코, 혀, 몸, 생각 등을 포함하고, 명상을 위한 대상으로는 물질, 소 리, 냄새, 맛, 촉감, 현상 등을 포함하고, 명상을 위한 의식으로는 안식(眼識), 이식(耳識), 비식(鼻識), 설식 (舌識), 신식(身識), 의식(意識) 등을 포함한다. 또한, 상기 제어부는 해당 명상 제공 장치가 인공지능 스피커로 작동하는 경우에, 해당 인공지능 스 피커에서의 하나 이상의 기능(예를 들어 명상 제공 기능, 사용자의 명상 기능 수행에 따른 마음 상태 분석 기능, 몸 느낌 분석 기능, 명상 분석 결과 제공 기능/평가 정보 제공 기능 등 포함)을 수행할 수 있다. 또한, 상기 제어부는 상기 수신된 음성 정보에 대해서 음성 인식 기능(automatic speech recognition function)을 수행한다. 즉, 상기 제어부는 상기 수신된 음성 정보에 대해서 딥러닝을 통해 음성 인식 기능을 수행한다. 또한, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 음성 정보에 포함된 음성을 텍스트 스크립트 (또는 텍스트)로 변환(또는 생성)한다. 즉, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 음성 정보와 매칭되는 텍스트 스크립트(또는 음 원을 텍스트로 인식한 결과)를 변환(또는 생성)한다. 이때, 상기 서버는 상기 음성 정보와 매칭되는 텍스 트 스크립트를 포함하는 텍스트 파일을 생성할 수도 있다."}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "또한, 상기 제어부는 상기 변환된(또는 생성된) 복수의 텍스트 스크립트(또는 복수의 텍스트/복수의 텍스 트 파일)에 대해 자연어 처리(Natural Language Processing: NLP)를 수행하여 키워드(또는 명령어/태그/요약된 결과)를 추출(keyword extraction)한다. 또한, 상기 제어부는 상기 추출된 키워드를 이용해서 해시태그를 생성한다. 이와 같이, 상기 제어부는 상기 수신된 음성 정보를 근거로 자동으로 내용을 분석하여 키워드를 추출하고, 키워드와 관련한 해시태그를 생성할 수 있다. 또한, 상기 제어부는 상기 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태인지 여부(또는 상 기 추출된 키워드가 상기 미리 설정된 사건 선택 항목에 포함된 상태인지 여부)를 판단(또는 확인/분석)한다. 여기서, 상기 사건 선택 항목(또는 사건/사례 선택 키워드/내용/단어)은 친구와 싸움, 부모님께 야단맞음, 애인 과 헤어짐 등을 포함한다. 이때, 상기 제어부는 상기 추출된 하나 이상의 키워드 중에서 첫 번째 키워드(또는 어느 하나의 키워드)가 해당 명상 제공 장치의 작동 시작을 나타내는 미리 설정된 명령어에 해당하는지 여부를 확인하고, 상기 추 출된 하나 이상의 키워드 중에서 첫 번째 키워드가 해당 명상 제공 장치의 작동 시작을 나타내는 명령어에 해당하는 경우에, 상기 추출된 하나 이상의 키워드 중에서 상기 첫 번째 키워드를 제외한 나머지 키워드를 대상 으로 상기 사건 선택 항목과 관련된 상태인지 여부를 확인하도록 구성할 수도 있다. 예를 들어, 상기 제어부는 상기 추출된 첫 번째 키워드(예를 들어 위빠사나)와 두 번째 키워드(예를 들어 부모님께 야단) 중에서 첫 번째 키워드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하는지 여부를 확인한다. 상기 확인 결과, 상기 추출된 첫 번째 키워드와 두 번째 키워드(예를 들어 부모님께 야단) 중에서 첫 번째 키워 드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하지 않은 경우, 상기 제어부는 음성 정보를 다 시 입력해줄 것을 안내하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사 용자로부터 다시 음성 정보를 수신하기 위한 대기 상태로 전환한다. 또한, 상기 확인 결과, 상기 추출된 첫 번째 키워드(예를 들어 위빠사나)와 두 번째 키워드(예를 들어 부모님께 야단) 중에서 첫 번째 키워드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하는 경우, 상기 제어부 는 상기 추출된 첫 번째 키워드와 두 번째 키워드 중에서 상기 첫 번째 키워드를 제외한 나머지 키워드인 상기 두 번째 키워드(예를 들어 부모님께 야단)가 상기 미리 설정된 사건 선택 항목과 관련된 상태인지 여부를 확인한다. 상기 판단 결과(또는 상기 확인 결과/상기 분석 결과), 상기 추출된 키워드가 상기 미리 설정된 사건 선택 항목 과 관련된 상태(또는 상기 추출된 키워드가 미리 설정된 사건 선택 항목에 포함된 상태)가 아닌 경우, 상기 제 어부는 음성 정보를 다시 입력해줄 것을 안내하는 안내 정보를 표시부 및/또는 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위해서 대기 상태를 유지(또는 전환)한다. 또한, 상기 판단 결과(또는 상기 확인 결과/상기 분석 결과), 상기 추출된 키워드가 상기 미리 설정된 사건 선 택 항목과 관련된 상태(또는 상기 추출된 키워드가 미리 설정된 사건 선택 항목에 포함된 상태)인 경우, 상기 제어부는 해당 명상 제공 장치의 사용자가 눈을 감고 명상 자세를 확립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 이에 따라, 상기 사용자는 상기 표시부 및/또는 상기 음성 출력부를 통해 출력되는 상기 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 근거로 명상을 진행하기 위해 준비 자세를 취할 수 있다. 또한, 상기 제어부는 상기 입력부를 통해 수신되는 사용자의 추가 음성 정보로부터 추가 키워드를 추 출한다. 여기서, 상기 사용자의 추가 음성 정보는 상기 출력 중인 호흡 명상을 위한 올바른 자세와 관련한 콘텐 츠에 따라서 준비 자세를 취하고 있는 사용자가 해당 사용자의 현재 상태(예를 들어 예 편안해요, 예 괜찮아요, 아직 불편해요, 아뇨 좀 더 집중해볼게요 등 포함)를 표현한 정보를 포함한다. 즉, 상기 제어부는 상기 입력부를 통해 수신되는 추가 음성 정보에 대해서 딥러닝을 통해 음성 인식 기능을 수행한다. 또한, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 추가 음성 정보에 포함된 추가 음성을 추가 텍스트 스크립트(또는 텍스트)로 변환(또는 생성)한다.즉, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 추가 음성 정보와 매칭되는 추가 텍스트 스크립 트(또는 추가 음원을 텍스트로 인식한 결과)를 변환(또는 생성)한다. 이때, 상기 서버는 상기 추가 음성 정보와 매칭되는 추가 텍스트 스크립트를 포함하는 추가 텍스트 파일을 생성할 수도 있다. 또한, 상기 제어부는 상기 변환된(또는 생성된) 추가 텍스트 스크립트(또는 추가 텍스트 파일)에 대해 자"}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "연어 처리를 수행하여 추가 키워드(또는 추가 명령어/태그/요약된 결과)를 추출한다. 또한, 상기 제어부는 상기 추출된 추가 키워드를 이용해서 추가 해시태그를 생성한다. 또한, 상기 추출된 추가 키워드가 미리 설정된 호흡 안정 상태와 관련한 상태인지 여부(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함된 상태인지 여부)를 확인(또는 판단)한다. 여기서, 상 기 호흡 안정 상태는 호흡 안정, 호흡 정상, 숨 정상, 편안해요, 괜찮아요 등을 포함한다. 이때, 상기 제어부는 상기 추출된 하나 이상의 추가 키워드 중에서 첫 번째 추가 키워드(또는 어느 하나의 추가 키워드)가 해당 명상 제공 장치의 작동 시작을 나타내는 미리 설정된 명령어에 해당하는지 여부를 확 인하고, 상기 추출된 하나 이상의 추가 키워드 중에서 첫 번째 추가 키워드가 해당 명상 제공 장치의 작동 시작을 나타내는 명령어에 해당하는 경우에, 상기 추출된 하나 이상의 추가 키워드 중에서 상기 첫 번째 추가 키워드를 제외한 나머지 추가 키워드를 대상으로 상기 호흡 안정 상태와 관련한 상태인지 여부를 확인하도록 구 성할 수도 있다. 예를 들어, 상기 제어부는 상기 추출된 열 한 번째 키워드(예를 들어 위빠사나)와 열 두 번째 키워드(예를 들어 호흡 안정 상태야) 중에서 열 한 번째 키워드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하는 지 여부를 확인한다. 상기 확인 결과, 상기 추출된 열 한 번째 키워드와 열 두 번째 키워드(예를 들어 호흡 안정 상태야) 중에서 열 한 번째 키워드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하지 않은 경우, 상기 제어부는 음 성 정보를 다시 입력해줄 것을 안내하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위한 대기 상태로 전환한다. 또한, 상기 확인 결과, 상기 추출된 열 한 번째 키워드(예를 들어 위빠사나)와 열 두 번째 키워드(예를 들어 호 흡 안정 상태야) 중에서 열 한 번째 키워드가 상기 미리 설정된 명령어(예를 들어 위빠사나)에 해당하는 경우, 상기 제어부는 상기 추출된 열 한 번째 키워드와 열 두 번째 키워드 중에서 상기 열 한 번째 키워드를 제 외한 나머지 키워드인 상기 열 두 번째 키워드(예를 들어 호흡 안정 상태야)가 상기 미리 설정된 호흡 안정 상 태와 관련한 상태인지 여부를 확인한다. 상기 확인 결과(또는 상기 판단 결과), 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태와 관련되 지 않은 상태인 경우(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함되지 않은 상태인 경우), 상기 제어부는 음성 정보를 다시 입력해줄 것을 안내하는 안내 정보를 상기 표시부 및 /또는 상기 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위해서, 대기 상태를 유지(또는 전환)한다. 또한, 상기 확인 결과(또는 상기 판단 결과), 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태와 관련된 상태인 경우(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함된 상태인 경우), 상기 제어부는 사용자가 선택한 사건과 관련해서 해당 장면(또는 상황)을 사용자가 떠올릴 수 있도 록 상기 저장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 여기서, 상기 장면(또는 상황)은 상기 사용자가 선택한 사건과 관련해서 해당 사용자가 기억하고 있는 복수의 장면 중에서 어느 하나의 장면(또는 일정 시간 동안의 연속된 장면/영상)(예를 들어 특정 시점에서의 하나의 장면, 5초 동안의 연속된 장면/영상 등 포함)일 수 있다. 즉, 상기 추출된 추가 키워드가 상기 사용자의 호흡 변화가 안정 상태임을 나타내는 정보(예를 들어 호흡 안정 상태, 호흡 정상, 숨 정상 등 포함)와 관련한 경우, 상기 제어부는 상기 장면 떠올리기 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 순차로 출력되는 서로 다른 안내 정보(또는 서로 다른 안내 콘텐츠)에 대응하여 사용 자 입력(또는 사용자 선택/터치/제어/사용자 음성 입력)에 따라 상기 사용자가 선택한 사건과 관련해서 해당 장 면(또는 사용자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 수신되는 정보가 음성 정보인 경우, 상기 제어부는 상기 수신된(또는 수집된) 음성 정보 형태의 해당 장면(또는 사용자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대 한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행에 따라 해당 음성 정보와 매칭되는 텍스트(또는 텍스트 스크립트)를 변환할 수도 있다. 즉, 상기 제어부는 사용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 상기 표시부 및/ 또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각기관에 대한 정보를 수신(또는 수집)한 다. 여기서, 상기 사용자가 접촉을 느끼는 감각기관은 불교에서 감각기관으로 정의하는 눈, 귀, 코, 혀, 몸(또 는 신체), 마음(또는 생각)일 수 있다. 또한, 상기 제어부는 사용자가 해당 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 상기 표 시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용(또는 주의 기울임/주의 기울 임 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 사용자가 해당 장면에서 느끼는 감정, 세부 감정 내용 등을 확인하기 위한 안내 정보 를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택 한 사건과 관련한 해당 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감 정, 해당 사용자의 감정과 관련한 세부 감정 내용(또는 세부 감정 종류) 등을 수신(또는 수집)한다. 또한, 상기 제어부는 사용자가 느꼈던 감정에 따른 해당 장면에서의 사용자의 인식을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 느꼈던 감정에 따른 해당 장면에서의 사용자의 인식(또는 사용자의 인식에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는 지에 대한 사용자의 영향을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통 해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향(또는 사용자에게 미친/끼친 영향에 대한 정보/사용자에게 미친 영향)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자 의 바람을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용 자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기 를 바랐는지에 대한 사용자의 바람(또는 사용자의 바람에 대한 정보/사용자가 바라는 내용)을 수신(또는 수집) 한다. 또한, 상기 제어부는 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자 의 의도를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용 자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도(또는 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 이때, 상기 수신되는 정보가 음성 정보인 경우, 상기 제어부는 상기 수신된(또는 수집된) 음성 정보 형태 의 상기 사용자가 선택한 사건과 관련해서 해당 장면(또는 사용자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행에 따라 해 당 음성 정보와 매칭되는 텍스트(또는 텍스트 스크립트)를 변환한다. 즉, 상기 제어부는 상기 수신된(또는 수집된) 상기 사용자가 선택한 사건과 관련해서 해당 장면(또는 사용 자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대한 정보에 대한 음성 정보, 사용자가 어떤 점을 불편하게 느끼는지에 대한 음성 정보, 해당 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감정에 대한 음성 정보, 해당 사용자의 감정과 관련한 세부 감정 내용에 대한 음성 정보, 해당 장면에서의 사용자의 인식에 대한 음성 정보, 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향에 대한 음성 정보, 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람에 대한 음성 정보, 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도에 대한 음성 정보 등에 대해서 음성 인식 기능을 각 각 수행하여, 해당 사용자의 감각기관에 대한 정보에 대한 음성 정보를 텍스트 형태의 해당 사용자의 감각기관 에 대한 정보로 변환하고, 해당 사용자가 어떤 점을 불편하게 느끼는지에 대한 음성 정보를 텍스트 형태의 해당 사용자가 어떤 점을 불편하게 느끼는지에 대한 정보로 변환하고, 해당 장면에 대응해서 화, 우울, 불안 등 중에 서 어떤 감정이 느껴지는지에 대한 사용자의 감정에 대한 음성 정보를 텍스트 형태의 해당 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감정에 대한 정보로 변환하고, 해당 사용자의 감정과 관련한 세부 감정 내용에 대한 음성 정보를 텍스트 형태의 해당 사용자의 감정과 관련한 세부 감정 내용 에 대한 정보로 변환하고, 해당 장면에서의 사용자의 인식에 대한 음성 정보를 텍스트 형태의 해당 장면에서의 사용자의 인식에 대한 정보로 변환하고, 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향에 대한 음성 정보를 텍스트 형태의 해당 장면에 대응해서 사용자가 느꼈 던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향에 대한 정보로 변환하고, 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람에 대한 음성 정보를 텍스트 형 태의 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람에 대한 정보 로 변환하고, 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도에 대 한 음성 정보를 텍스트 형태의 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사 용자의 의도에 대한 정보로 변환한다. 또한, 상기 제어부는 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 근거로 사용자의 마음 상태를 분석한다. 이때, 상기 제어 부는 상기 수신된 하나 이상의 음성 정보 형태의 데이터에 대해서 음성 인식 기능 수행 결과에 따른 하나 이상의 텍스트(또는 텍스트 스크립트)를 근거로 상기 사용자의 마음 상태를 분석할 수 있다. 즉, 상기 제어부는 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 포함하는 입력값(또는 텍스트 형태로 변환된 입력값)을 미리 학 습된 사용자 마음 상태 분석 모델(또는 서포트 벡터 머신: SVM)의 입력값으로 하여 학습 기능(또는 기계 학습/ 인공지능/딥 러닝)을 수행하고, 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용자의 마음 상태에 대한 분석 결과(또는 사용자의 마음 상태 분석 결과)를 생성한다. 여기서, 상기 사용자의 마음 상태 분 석 결과는 감정, 인식, 영향, 바람, 의도 등을 포함한다. 이와 같이, 상기 제어부는 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 포함하는 입력값(또는 텍스트 형태로 변환된 입력값)에 대 해서 인공 지능을 적용하여, 해당 사건의 해당 장면과 관련해서 사용자의 마음 상태에 대한 분석 결과(또는 사 용자의 마음 상태 분석 결과)를 생성할 수 있다. 또한, 상기 제어부는 해당 장면(또는 상기 사용자가 선택한 사건)이 어린 시절에 경험한 사건인지 여부(또 는 해당 장면을 해당 사용자가 과거에 경험한 적이 있는지 여부)를 판단(또는 확인)한다. 여기서, 상기 과거(또 는 어린 시절)는 유년 시절, 초등학생 시절, 중학생 시절, 고등학생 시절, 청소년 시절 등을 포함한다. 즉, 상기 제어부는 해당 장면(또는 상기 사용자가 선택한 사건)이 어린 시절에 경험한 사건인지 여부를 문 의하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정 보(또는 해당 장면이 과거/어린 시절에 경험한 적이 있는지에 대한 정보)는 어린 시절에 경험한 사건이었음, 어 린 시절에 경험한 사실이 있음, 어린 시절에 경험한 사실이 없음 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보를 근거로 해당 장면(또는 해당 사용자가 선택한 사건)을 해당 사용자가 과거(또는 어린 시절)에 경험한 적 이 있는지 여부를 판단(또는 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또 는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인 식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 장면(또는 해당 사용자가 선택한 사 건)을 해당 사용자가 과거(또는 어린 시절)에 경험한 적이 있는지 여부를 판단할 수도 있다. 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보에 과거에 경험한 적이 있음에 대한 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 과거에경험을 한 적이 있는 것으로 응답한 경우), 상기 제어부는 과거의 해당 장면과 관련한 대상이 타인인지 또 는 본인인지 여부를 확인한다. 즉, 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여 부에 대한 정보에 과거에 경험한 적이 있음에 대한 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 과 거에 경험을 한 적이 있는 것으로 응답한 경우), 상기 제어부는 해당 과거의 장면과 관련한 대상이 타인인 지 또는 본인인지 여부를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 과거의 장면과 관련한 대상에 대한 정보를 수 신(또는 수집)한다. 여기서, 상기 해당 과거의 장면과 관련한 대상에 대한 정보는 타인, 본인 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태인지 또는 본인이 포함된 상태인지를 확인한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과거 의 장면과 관련한 대상에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍 스트(또는 텍스트 스크립트)를 근거로 상기 수신된 정보에 타인이 포함된 상태인지 또는 본인이 포함된 상태인 지를 확인할 수도 있다. 상기 확인 결과, 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태인 경우(또는 상기 과거의 장면과 관련한 대상이 타인인 경우), 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 의도 표 현 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 상기 사용자가 해당 장면(또는 사 용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말(또는 상대방/자신에게 하고 싶은 말/내용) 을 수신(또는 수집)한다. 또한, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 표현 완료 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 응답 정보를 수신(또는 수집)한다. 여기서, 상기 응답 정보는 긍정 응답 정보(예를 들어 예/네 다 했어요, 네 다음 단계로 갈게요 등 포함), 부정 응답 정보(예를 들어 아직 못했어요, 좀 더 생각하고 추가로 말할게요 등 포함) 등을 포함한다. 또한, 상기 제어부는 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인지 여부를 판단(또는 확 인)한다. 즉, 상기 제어부는 상기 수신된 응답 정보에 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확 인)한다. 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수신된 응답 정보에 부정 응답 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어) 에 따라 상기 사용자로부터 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말(또는 상대방/자신에게 하고 싶은 말/내용)을 수신(또는 수집)하는 앞선 과정으로 복귀한다. 또한, 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 해 당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느 낌이 느껴지는지 여부를 확인(또는 판단)한다. 즉, 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위에서 해당 감정이 느껴지는지를 확인하기 위한 안내 정보를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보는 감정이 신체 어느 부위에서 어떤 느낌으로 느껴짐, 감정이 신체 부위에서 느껴지지 않음 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 근거로 해당 장면(또는 사용자 가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여 부를 확인(또는 판단)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립 트)를 근거로 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인할 수도 있다. 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는 경우, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 어떤 느낌으로 느껴지고, 강도는 어느 정도인지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사 용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 근거로 사용 자의 몸 느낌(또는 몸 느낌 상태)을 분석한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느 낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 사용자의 몸 느낌(또는 몸 느낌 상태)을 분석할 수도 있다. 즉, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정 에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 포함하는 다른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능(또는 기계 학습/인공 지능/딥 러닝)을 수행하고, 다른 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과(또는 사용자의 몸 느낌 분석 결과)를 생성한다. 여기서, 상기 사용자의 몸 느낌에 대한 분석 결과는 불안정, 안정, 평정심 유지 등을 포함한다. 이와 같이, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼 는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 포함하 는 다른 입력값에 대해서 인공 지능을 적용하여, 해당 사건의 해당 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과(또는 사용자의 몸 느낌 분석 결과)를 생성할 수 있다. 또한, 상기 제어부는 상기 사용자가 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자의 특정 신 체 부위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 지시문 콘텐츠(또는 명상 콘텐츠)를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 미리 설정된 시간 간격으로, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추 가 강도 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 강도가 어느 정도인지를 확인하기 위한 안내 정보를 미리 설정된 시간 간격으로 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이 하인지 여부를 추가 판단(또는 추가 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추 가 강도 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크 립트)를 근거로 상기 수신된 정보(또는 텍스트 형태의 추가 강도 정보)가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단할 수도 있다. 즉, 상기 제어부는 명상 과정을 통해 해당 사용자가 해당 장면과 관련해서 느끼는 감정에 대응하는 사용자 의 신체 부위에 대한 강도가 어느 정도 해소되었는지(또는 느낌이 소멸하였는지)를 확인하기 위해서, 상기 수신 된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단(또는 추가 확인)한다. 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해 서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값(예 를 들어 0%, 5% 등 포함)을 초과한 경우, 상기 제어부는 앞서 지시문 콘텐츠를 출력하는 과정을 반복 수행 한다. 또한, 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기 준값(예를 들어 0%, 5% 등 포함) 이하인 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐 츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력에 따른 사용자의 소감 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과, 상기 수신된 사용자의 소감 정보 등을 근거로 사용자에 대한 명상 분석 결과를 생성한다. 여기서, 상기 명상 분석 결과는 사 용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 예를 들어, 명상 시간 10분 중 사용자의 호흡 변화량이 미리 설정된 기준 범위 내에 9분 동안 존재했고 1분 동 안 해당 기준 범위 밖에 존재하고, 상기 사용자의 마음 상태 분석 결과에서 사용자가 적극적으로 의사 표현을 한 상태이고, 상기 사용자의 몸 느낌 분석 결과에서 사용자가 해당 사건과 관련해서 감각 기관을 통해 전달되는 몸의 느낌을 소멸한 상태이고, 상기 사용자의 소감 정보가 해당 사건과 관련해서 두려움을 없앤 상태임을 포함 하는 경우, 상기 제어부는 각 항목에 대해서 미리 설정된 점수(예를 들어 신체 변화 정보 항목에 25점, 마 음 상태 분석 결과 항목에 25점, 몸 느낌 분석 결과 항목에 25점, 소감 정보 항목에 25점)에서, 상기 정보들을 각각 반영하여, 신체 변화 정보 항목에서 25점 중 20점, 마음 상태 분석 결과 항목에서 25점 중 25점, 몸 느낌 분석 결과 항목에서 25점 중 25점, 소감 정보 항목에서 25점 중 25점을 합산하여, 총 95점의 명상 분석 결과를 생성한다. 또한, 상기 제어부는 상기 생성된 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼 는 감정이 사용자의 신체 부위에서 느껴지지 않은 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면 을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관 련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력에 따른 사용자의 다른 소감 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 다른 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 수신된 사용자의 다른 소감 정보 등을 근 거로 사용자에 대한 다른 명상 분석 결과를 생성한다. 여기서, 상기 다른 명상 분석 결과는 사용자가 해당 사건 과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 또한, 상기 확인 결과, 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 자신이 포함된 상태인 경우 (또는 상기 과거의 장면과 관련한 대상이 자신인 경우), 상기 제어부는 해당 과거의 장면에서 어린 자신이 느끼는 감정, 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정, 해당 과거의 장면에서의 어린 자신에게 하고 싶은 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과거의 장면에서 어린 자신이 느끼는 감정, 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정, 해당 과거의 장면에서의 어린 자신에게 하고 싶은 의도 등에 대한 정보에 대해서 음성 인식 기 능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있 다. 즉, 상기 제어부는 해당 과거의 장면에서 어린 자신이 어떤 감정을 느꼈는지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 과거의 장면에 대 응해서 화, 우울, 불안 등 중에서 어린 자신이 어떤 감정을 느꼈는지에 대한 어린 자신의 감정(또는 어린 자신 이 느낀 감정에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정을 확인하기 위한 안 내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정(또는 현재의 사용자의 감정에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 과거의 장면에서의 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도 를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력 에 따라 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또 는 어린 자신에 대한 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도) 에 대한 정보에 행동과 관련한 정보(또는 내용)가 포함된 상태인지 여부를 판단한다. 상기 판단 결과, 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도)에 대한 정보에 상기 행동과 관련한 정보가 포함된 상태인 경우, 상기 제어부는 상기 사용자가 해당 행동을 할 수 있도록 유도하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 이에 따라, 해당 사용자는 해당 행동을 수행할 수 있다. 또한, 상기 사용자가 해당 행동을 할 수 있도록 유도하는 안내 정보를 출력한 경우 또는, 상기 판단 결과, 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또 는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도)에 대한 정보에 상기 행동과 관 련한 정보가 포함되지 않은 상태인 경우, 상기 제어부는 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 다른 의도 표현 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 상기 사용자가 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말(또는 어린 자신에게 하고 싶은 말/내용)을 수신(또는 수집)한다. 이때, 상 기 제어부는 상기 수신된 음성 정보 형태의 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말(또는어린 자신에게 하고 싶은 말/내용)에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍 스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했 는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 다른 표현 완료 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상 기 다른 응답 정보는 긍정 응답 정보(예를 들어 예/네 다 했어요, 네 다음 단계로 갈게요 등 포함), 부정 응답 정보(예를 들어 아직 못했어요, 좀 더 생각하고 추가로 말할게요 등 포함) 등을 포함한다. 또한, 상기 제어부는 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 다른 응답 정보에 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말 을 사용자가 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수신된 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력(또는 사용자 선택/ 터치/제어)에 따라 상기 사용자로부터 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 수신(또는 수 집)하는 앞선 과정으로 복귀한다. 또한, 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부(16 0)는 상기 사용자가 어린 자신에게 행한 행동 등에 따라 해당 과거의 장면에서의 어린 자신이 현재 느끼는 추가 감정 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과 거의 장면에서의 어린 자신이 현재 느끼는 추가 감정 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음 성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 상기 사용자가 어린 자신에게 행한 행동 이후, 해당 과거의 장면에서 어린 자신이 어떤 감정을 추가로 느꼈는지 를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력 에 따라 해당 과거의 장면에 대응해서 화, 우울, 불안 등의 해소된 상태인지 또는 여전히 유지되고 있는 상태인 지에 대한 어린 자신의 추가 감정(또는 어린 자신의 느낀 추가 감정에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 과거의 장면에서의 어린 자신으로부터 현재의 상태로 복귀하도록 과거 장면과의 헤어짐을 유도하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인(또는 판단)한다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위에서 해당 감정이 느껴지는지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 사용자가 선택 한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대 한 정보는 감정이 신체 어느 부위에서 어떤 느낌으로 느껴짐, 감정이 신체 부위에서 느껴지지 않음 등을 포함한 다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 근거로 해당 장면(또는 사용자 가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여 부를 확인(또는 판단)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립 트)를 근거로 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느부위에서 어떤 느낌이 느껴지는지 여부를 확인할 수도 있다. 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는 경우, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 어떤 느낌으로 느껴지고, 강도는 어느 정도인지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사 용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 근거로 사용 자의 몸 느낌(또는 몸 느낌 상태)을 분석한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느 낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 사용자의 몸 느낌(또는 몸 느낌 상태)을 분석할 수도 있다. 즉, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정 에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 포함하는 또 다 른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 또 다른 학습 기능(또는 기계 학습 /인공지능/딥 러닝)을 수행하고, 또 다른 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용 자의 몸 느낌에 대한 분석 결과(또는 사용자의 몸 느낌 분석 결과)를 생성한다. 여기서, 상기 사용자의 몸 느낌 에 대한 분석 결과는 불안정, 안정, 평정심 유지 등을 포함한다. 또한, 상기 제어부는 상기 사용자가 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자의 특정 신 체 부위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 상기 지시문 콘텐츠(또는 상기 명상 콘텐츠)를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 상기 미리 설정된 시간 간격으로, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대 한 다른 추가 강도 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 강도가 어느 정도인지를 확인하기 위한 안내 정보를 미리 설정된 시간 간격으로 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 미리 설정된 기준값(예를 들어 0%, 5% 등 포 함) 이하인지 여부를 추가 판단(또는 추가 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대 한 다른 추가 강도 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 상기 수신된 정보(또는 텍스트 형태의 추가 강도 정보)가 상기 미리 설정된 기준값 (예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단할 수도 있다. 즉, 상기 제어부는 명상 과정을 통해 해당 사용자가 해당 장면과 관련해서 느끼는 감정에 대응하는 사용자 의 신체 부위에 대한 강도가 어느 정도 해소되었는지(또는 느낌이 소멸하였는지)를 확인하기 위해서, 상기 수신 된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단(또는 추가 확인)한다. 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해 서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정된 기준 값(예를 들어 0%, 5% 등 포함)을 초과한 경우, 상기 제어부는 앞서 지시문 콘텐츠를 출력하는 과정을 반복 수행한다. 또한, 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정 된 기준값(예를 들어 0%, 5% 등 포함) 이하인 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련 한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력에 따른 사용자의 또 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 사용자의 또 다른 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 또 다른 소감 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과, 상기 수신된 사용자의 또 다른 소감 정보 등을 근거로 사용자에 대한 또 다른 명상 분석 결과를 생성한다. 여기서, 상기 또 다른 명상 분석 결과는 사용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정 보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 또 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 또한, 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼 는 감정이 사용자의 신체 부위에서 느껴지지 않은 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면 을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관 련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력에 따른 사용자의 추가 또 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 사용자의 추가 또 다른 소감 정보에 대해서 음성 인식 기능 을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 추가 또 다른 소감 정보를 수신(또는 수집)한 다. 또한, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 수신된 사용자의 추가 또 다른 소감 정보 등을 근거로 사용자에 대한 추가 또 다른 명상 분석 결과를 생성한다. 여기서, 상기 추가 또 다른 명상 분석 결 과는 사용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 추가 또 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출 력부를 통해 출력한다. 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절 경험이 아닌 정보가 포함 된 경우(또는 해당 장면과 관련해서 사용자가 어린 시절 경험이 아닌 것으로 응답한 경우/상기 장면이 어린 시 절에 경험한 사건인지 여부에 대한 정보에 어린 시절 경험이 아닌 정보가 포함된 상태일 때), 상기 제어부(16 0)는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용 자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응 하는 사용자의 신체 부위에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절 경험이 아닌 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 어린 시절 경험이 아닌 것으로 응답한 경우/상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 어린 시절 경험이 아닌 정보가 포함된 상태일 때), 상기 제어부 는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 느껴지는지를 확 인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따 라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응 하는 과거(또는 어린 시절)의 다른 장면(또는 다른 상황)을 사용자가 떠올릴 수 있도록 상기 저장부에 미 리 저장된 다른 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한 다. 또한, 상기 제어부는 순차로 출력되는 서로 다른 안내 정보(또는 서로 다른 안내 콘텐츠)에 대응하여 사용 자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사용자가 떠올린 과거 다른 장면(또는 사용자가 떠올리고/생 각하고 있는 과거 다른 장면/상황)에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정 보 형태의 해당 사용자가 떠올린 과거 다른 장면(또는 사용자가 떠올리고/생각하고 있는 과거 다른 장면/상황) 에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각 기관에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 사용자가 접촉을 느끼는 감각기관은 불교에서 감각기관 으로 정의하는 눈, 귀, 코, 혀, 몸, 마음(또는 생각)일 수 있다. 또한, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 해당 과거 다른 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용(또는 주의 기울임/주의 기울임 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 해당 과거 다른 장면에서 느끼는 감정, 세부 감정 내용 등을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감정, 해당 사용자의 감정과 관련한 세부 감정 내용(또는 세부 감정 종류) 등을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾기를 수행할지 여부에 대한 사용자 입력값(또는 사용자 음성 정보)을 수신(또는 수집)한다. 여기서, 상기 투사 (projection)는 방어 기재와 관련한 용어로, 불안을 일으키는 충동이나 감정을 타인의 것으로 떠넘김으로써 자 신을 방어하는 방법을 나타낸다. 즉, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾기를 수행할지 여부를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾 기를 수행할지 여부에 대한 사용자 입력값을 수신(또는 수집)한다. 여기서, 상기 사용자 입력값은 투사 찾기, 투사 찾기 아님 등을 포함한다. 또한, 상기 제어부는 상기 수신된 사용자 입력값이 투사와 관련한 내용인지 여부(또는 상기 수신된 사용자 입력값에 투사와 관련한 내용/정보가 포함된 상태인지 여부)를 추가 확인(또는 추가 판단)한다. 이때, 상기 제 어부는 상기 수신된 음성 정보 형태의 사용자 입력값에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 상기 수신된 사용자 입력값(또는 상기 텍스트로변환된 사용자 입력값)이 투사와 관련한 내용인지 여부(또는 상기 수신된 사용자 입력값에 투사와 관련한 내용/ 정보가 포함된 상태인지 여부)를 추가 확인할 수도 있다. 즉, 상기 제어부는 상기 수신된 사용자 입력값에 투사 찾기(또는 투사)와 관련한 정보가 포함된 상태인지 여부를 추가 확인(또는 추가 판단)한다. 상기 추가 확인 결과(또는 상기 추가 판단 결과), 상기 수신된 사용자 입력값에 투사 찾기와 관련한 정보가 포 함된 상태인 경우(또는 상기 수신된 사용자 입력값이 투사 찾기인 경우/상기 사용자 입력에 투사와 관련한 내용 이나 정보가 포함된 경우), 상기 제어부는 상기 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장 면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입 력에 따른 두 장면의 공통점 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음 성 정보 형태의 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 공통점 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍 스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상기 또 다른 응답 정보는 긍정 응답 정보(예를 들어 다했어요, 이제 없어요, 됐어요 등 포함), 부정 응답 정보(예를 들어 모르겠어요, 못 찾겠어요 등 포함) 등을 포함한다. 또한, 상기 제어부는 상기 수신된 또 다른 응답 정보에 긍정 응답 정보(또는 공통점과 관련해서 응답을 완 료한 상태임을 나타내는 정보)가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 또 다른 응답 정보에 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수 신된 또 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우/상기 수신된 또 다른 응답 정보에 공통점과 관 련해서 응답을 완료한 상태가 아님을 나타내는 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력 (또는 사용자 선택/터치/제어)에 따라 상기 사용자로부터 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 공통점 등에 대한 정보를 수신(또는 수집)하는 앞선 과정으로 복귀한 다. 또한, 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우(또는 상기 수신 된 또 다른 응답 정보에 공통점과 관련해서 응답을 완료한 상태임을 나타내는 정보가 포함된 상태인 경우), 상 기 제어부는 상기 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 또 다른 지시문 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입 력에 따른 두 장면의 차이점 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음 성 정보 형태의 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 차이점 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍 스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다.또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상기 또 다른 응답 정보는 긍정 응답 정보(예를 들어 다했어요, 이제 없어요, 됐어요 등 포함), 부정 응답 정보(예를 들어 모르겠어요, 못 찾겠어요 등 포함) 등을 포함한다. 또한, 상기 제어부는 상기 수신된 또 다른 응답 정보에 긍정 응답 정보(또는 차이점과 관련해서 응답을 완 료한 상태임을 나타내는 정보)가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 또 다른 응답 정보에 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수 신된 또 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우/상기 수신된 또 다른 응답 정보에 차이점과 관 련해서 응답을 완료한 상태가 아님을 나타내는 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력 (또는 사용자 선택/터치/제어)에 따라 상기 사용자로부터 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 차이점 등에 대한 정보를 수신(또는 수집)하는 앞선 과정으로 복귀한 다. 또한, 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우(또는 상기 수신 된 또 다른 응답 정보에 차이점과 관련해서 응답을 완료한 상태임을 나타내는 정보가 포함된 상태인 경우), 상 기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거 (또는 어린 시절)의 다른 장면(또는 다른 상황)을 사용자가 떠올릴 수 있도록 상기 저장부에 미리 저장된 또 다른 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 상기 또 다른 장면 떠올리기 안내 콘텐츠를 출력한 후, 해당 장면(또는 사용자가 선 택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 장면에 따른 사용자의 인 식, 영향, 바람, 의도 등에 대한 정보를 수신하는 과정으로 복귀하여, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절) 의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 상기 또 다른 장면 떠올리기 안내 콘텐츠를 출력한 후, 해당 장면(또는 사용자가 선택 한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대해서 사용자 가 느꼈던 감정에 따른 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에서의 사 용자의 인식을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 느꼈던 감정에 따른 해당 과거(또는 어린 시절)의 다른 장면에서의 사용자의 인식(또는 사용자의 인식 에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향을 확인하기 위한 안 내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자 가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 느꼈던 감정에 따라 사 용자에게 어떤 영향을 주었는지에 대한 사용자의 영향(또는 사용자에게 미친/끼친 영향에 대한 정보/사용자에게 미친 영향)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느 낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람(또는 사용자의 바람에 대한 정보/사용자가 바라는 내용)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도를 확인하기 위한 안내 정보를 상기표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느 낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도(또는 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 또한, 상기 추가 확인 결과(또는 상기 추가 판단 결과), 상기 수신된 사용자 입력값에 투사 찾기와 관련한 정보 가 포함되지 않은 상태인 경우(또는 상기 수신된 사용자 입력값이 투사 찾기 아님인 경우/상기 사용자 입력값에 투사와 관련한 내용이나 정보가 포함되지 않은 경우), 상기 제어부는 해당 장면(또는 사용자가 선택한 사 건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신하는 과정으로 복귀하여, 해당 장면(또는 사용자가 선택한 사건)과 관 련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절) 의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 또한, 상기 제어부는 앞선 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 근거로 사용자의 마음 상태를 분석하는 과정부터 재수행한 다. 이와 같이, 상기 제어부는 상기 입력부를 통해 수신되는(또는 수집되는) 사용자의 음성 정보에 대해 서 음성 인식 기능을 수행하여, 음성 인식 기능 수행에 따라 해당 사용자의 음성 정보를 텍스트 형태의 정보로 변환(또는 생성)하고, 상기 변환된(또는 생성된) 텍스트 형태의 정보를 이용해서 해당 사용자가 명상을 정상적 으로 수행할 수 있는 다양한 콘텐츠를 제공(또는 출력/표시)할 수 있다. 본 발명의 실시예에서는, 상기 입력부에 포함된 마이크를 이용해서, 사용자 선택에 따른 기능을 수행하는 것을 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 제어부는 상기 표시부를 통한 사용자 입력(또는 사용자 터치/선택/제어)에 따른 정보를 근거로 해당 기능을 수행할 수도 있다. 또한, 본 발명의 실시예에서는, 해당 명상 제공 장치가 인공지능 스피커인 경우에 대해서 주로 설명하고 있으나, 이에 한정되는 것은 아니며, 상기 명상 제공 장치는 상기 서버에서 제공되는 전용 앱을 해당 명상 제공 장치에 설치한 후, 해당 명상 제공 장치에 설치된 전용 앱을 실행하여 상기 하나 이상의 기능 (예를 들어 상기 명상 제공 기능, 상기 사용자의 명상 기능 수행에 따른 마음 상태 분석 기능, 상기 몸 느낌 분 석 기능, 상기 명상 분석 결과 제공 기능/평가 정보 제공 기능 등 포함)을 수행하거나 또는, 상기 서버에서 제 공되는 웹 사이트를 통해 상기 하나 이상의 기능을 수행할 수도 있다. 즉, 상기 제어부는 해당 제어부를 포함하는 상기 명상 제공 장치에 미리 설치된 전용 앱을 실행 하고, 전용 앱 실행에 따른 앱 실행 결과 화면을 표시부에 표시한다. 이때, 상기 앱 실행 결과 화면은 상 기 명상 제공 장치의 사용자가 경험한 특정 사건에 대해서 명상을 수행하기 위한 명상 메뉴(또는 명상 버 튼/항목), 특정 사건과 관련한 명상 수행에 따른 종합 평가 정보를 제공하기 위한 명상 분석 결과 메뉴, 전용 앱 기능을 설정하기 위한 설정 메뉴 등을 포함한다. 여기서, 상기 명상 제공 장치는 해당 전용 앱을 제공 하는 서버(미도시)에 회원 가입한 상태로, 회원 가입에 따른 아이디 및 비밀번호, 상기 아이디를 포함하는 바코 드 또는 QR 코드 등을 이용해서 상기 전용 앱 실행 시 로그인 절차를 수행하여, 해당 전용 앱의 하나 이상의 기 능(예를 들어 명상 제공 기능, 사용자의 명상 기능 수행에 따른 마음 상태 분석 기능, 몸 느낌 분석 기능, 명상 분석 결과 제공 기능/평가 정보 제공 기능 등 포함)을 수행할 수 있다. 또한, 상기 표시부에 표시되는 앱 실행 결과 화면에서 미리 설정된 명상 메뉴가 선택되는 경우, 상기 제어 부는 상기 선택된 명상 메뉴에 대응하는 사건 선택 화면을 상기 표시부에 표시한다. 이때, 상기 사건 선택 화면(또는 사례 선택 화면)은 해당 명상 제공 장치의 사용자가 경험한 특정 사건에 대한 정보를 선택 (또는 입력)하기 위한 사례 선택 항목(예를 들어 친구와 싸움, 부모님께 야단맞음, 애인과 헤어짐 등 포함) 등 을 포함한다. 여기서, 상기 명상 제공 장치가 글래스형 단말기, 헤드 마운티드 디스플레이(HMD) 등과 같은 웨어러블 디바이스로 구성된 경우, 상기 제어부는 상기 웨어러블 디바이스의 전원이 온 상태로 전환된 후 (또는 대기 모드에서 온 상태로 전환된 후), 사용자에게 사건 선택을 요청하는 음성 안내 정보를 음성 출력부 를 통해 출력하고, 마이크 등의 입력부(미도시)를 통해 음성 형태의 사용자 입력에 따른 사건 정보(또는 사례 정보)를 수신하도록 구성할 수도 있다.이와 같이, 사용자의 현재 상태에 따른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시절을 생각하면서 느끼는 몸 느낌 상태를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공할 수 있다. 이하에서는, 본 발명에 따른 인공 지능을 이용한 명상 제공 방법을 도 1 내지 도 8을 참조하여 상세히 설명한다. 도 2 내지 도 8은 본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 방법을 나타낸 흐름도이다. 먼저, 입력부는 사용자의 발화에 의한 음성 정보를 수신(또는 수집)한다. 이때, 상기 음성 정보는 음성 정 보 수신(또는 음성 인식 기능 수행)을 위해서 미리 설정된 명령어(예를 들어 위빠사나), 해당 명상 제공 장치 의 사용자가 경험한 특정 사건에 대한 정보 등을 포함할 수 있다. 여기서, 상기 위빠사나는 산스크리트어 로, '위(Vi)'라는 단어와 '빠사나(Passana)'란 두 개의 단어가 결합하여 만들어진 단어이며, 위빠사나 명상은 명상 방법 중 하나이다. 이때, 상기 입력부(또는 상기 입력부를 포함하는 상기 명상 제공 장치(10 0))는 대기 상태를 유지하고 있던 상태에서, 사용자의 발화에 따른 상기 음성 정보를 수신할 수 있다. 일 예로, 제 1 명상 제공 장치인 인공지능 스피커에 포함된 제 1 입력부는 사용자 입력에 따른 제 1 음성 정보(예를 들어 위빠사나~ 나 부모님께 야단맞았어)를 수신한다(S210). 이후, 제어부는 상기 수신된 음성 정보에 대해서 음성 인식 기능을 수행한다. 즉, 상기 제어부는 상기 수신된 음성 정보에 대해서 딥러닝을 통해 음성 인식 기능을 수행한다. 또한, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 음성 정보에 포함된 음성을 텍스트 스크립트 (또는 텍스트)로 변환(또는 생성)한다. 즉, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 음성 정보와 매칭되는 텍스트 스크립트(또는 음 원을 텍스트로 인식한 결과)를 변환(또는 생성)한다. 이때, 상기 서버는 상기 음성 정보와 매칭되는 텍스 트 스크립트를 포함하는 텍스트 파일을 생성할 수도 있다. 일 예로, 제 1 제어부는 상기 수신된 제 1 음성 정보(예를 들어 위빠사나~ 나 부모님께 야단맞았어)에 대 해서 딥러닝을 통해 음성 인식 기능을 수행하여, 상기 제 1 음성 정보에 포함된 음성을 제 1 텍스트 스크립트 (예를 들어 위빠사나 나 부모님께 야단맞았어)로 변환한다(S220). 이후, 상기 제어부는 상기 변환된(또는 생성된) 텍스트 스크립트(또는 텍스트 파일)에 대해 자연어 처리"}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "(Natural Language Processing: NLP)를 수행하여 키워드(또는 명령어/태그/요약된 결과)를 추출한다. 일 예로, 상기 제 1 제어부는 상기 생성된 제 1 텍스트 스크립트에 대해 자연어 처리를 수행하여 제 1 키워드 (예를 들어 위빠사나) 및 제 2 키워드(예를 들어 부모님께 야단)를 추출한다(S230). 이후, 상기 제어부는 상기 추출된 키워드가 미리 설정된 사건 선택 항목과 관련된 상태인지 여부(또는 상 기 추출된 키워드가 상기 미리 설정된 사건 선택 항목에 포함된 상태인지 여부)를 판단(또는 확인/분석)한다. 여기서, 상기 사건 선택 항목(또는 사건/사례 선택 키워드/내용/단어)은 친구와 싸움, 부모님께 야단맞음, 애인 과 헤어짐 등을 포함한다. 이때, 상기 제어부는 상기 추출된 하나 이상의 키워드 중에서 첫 번째 키워드(또는 어느 하나의 키워드)가 해당 명상 제공 장치의 작동 시작을 나타내는 미리 설정된 명령어에 해당하는지 여부를 확인하고, 상기 추 출된 하나 이상의 키워드 중에서 첫 번째 키워드가 해당 명상 제공 장치의 작동 시작을 나타내는 명령어에 해당하는 경우에, 상기 추출된 하나 이상의 키워드 중에서 상기 첫 번째 키워드를 제외한 나머지 키워드를 대상 으로 상기 사건 선택 항목과 관련된 상태인지 여부를 확인하도록 구성할 수도 있다. 일 예로, 상기 제 1 제어부는 상기 추출된 제 1 키워드(예를 들어 위빠사나) 및 제 2 키워드(예를 들어 부모님 께 야단)가 상기 미리 설정된 사건 선택 항목과 관련된 상태인지 여부를 확인한다(S240). 상기 판단 결과(또는 상기 확인 결과/상기 분석 결과), 상기 추출된 키워드가 상기 미리 설정된 사건 선택 항목 과 관련된 상태(또는 상기 추출된 키워드가 미리 설정된 사건 선택 항목에 포함된 상태)가 아닌 경우, 상기 제 어부는 음성 정보를 다시 입력해줄 것을 안내하는 안내 정보를 표시부 및/또는 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위해서 대기 상태를 유지(또는 전환)한다. 이때, 상기 제어부는 사용자로부터 다시 음성 정보를 수신하기 위해서 앞선 단계(예를 들어 S210 단계)의 대기 상태를유지할 수 있다. 일 예로, 상기 추출된 제 1 키워드 및 제 2 키워드가 상기 미리 설정된 사건 선택 항목과 관련된 상태가 아닐 때, 상기 제 1 제어부는 음성 정보를 재입력해줄 것을 요청하는 안내 정보를 제 1 표시부 및/또는 제 1 음 성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위한 대기 상태로 전환한다(S250). 또한, 상기 판단 결과(또는 상기 확인 결과/상기 분석 결과), 상기 추출된 키워드가 상기 미리 설정된 사건 선 택 항목과 관련된 상태(또는 상기 추출된 키워드가 미리 설정된 사건 선택 항목에 포함된 상태)인 경우, 상기 제어부는 해당 명상 제공 장치의 사용자가 눈을 감고 명상 자세를 확립할 수 있도록 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 이에 따라, 상기 사용자는 상기 표시부 및/또는 상기 음성 출력부를 통해 출력되는 상기 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 근거로 명상을 진행하기 위해 준비 자세를 취할 수 있다. 일 예로, 상기 추출된 제 1 키워드(예를 들어 위빠사나) 및 제 2 키워드(예를 들어 부모님께 야단)가 상기 미리 설정된 사건 선택 항목과 관련된 상태일 때, 상기 제 1 제어부는 상기 제 2 키워드(예를 들어 부모님께 야단)에 대응하여 제 1 저장부에 미리 저장된 올바른 호흡 명상을 위한 제 1 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다. 여기서, 상기 제 1 콘텐츠는 호흡 명상을 위한 제 1 영상과, 호흡 명 상을 위한 올바른 자세 등에 대한 제 1 음성 정보(예를 들어 눈을 감고 몸을 편안하게 명상의 자세로 해주세요. 숨을 들이쉬고 숨을 내쉬고, 호흡이 편안해지도록 호흡을 해주세요) 등을 포함한다(S260). 이후, 상기 제어부는 상기 입력부를 통해 수신되는 사용자의 추가 음성 정보로부터 추가 키워드를 추 출한다. 여기서, 상기 사용자의 추가 음성 정보는 상기 출력 중인 호흡 명상을 위한 올바른 자세와 관련한 콘텐 츠에 따라서 준비 자세를 취하고 있는 사용자가 해당 사용자의 현재 상태(예를 들어 예 편안해요, 예 괜찮아요, 아직 불편해요, 아뇨 좀 더 집중해볼게요 등 포함)를 표현한 정보를 포함한다. 즉, 상기 제어부는 상기 입력부를 통해 수신되는 추가 음성 정보에 대해서 딥러닝을 통해 음성 인식 기능을 수행한다. 또한, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 추가 음성 정보에 포함된 추가 음성을 추가 텍스트 스크립트(또는 텍스트)로 변환(또는 생성)한다. 즉, 상기 제어부는 상기 음성 인식 기능 수행에 따라 상기 추가 음성 정보와 매칭되는 추가 텍스트 스크립 트(또는 추가 음원을 텍스트로 인식한 결과)를 변환(또는 생성)한다. 이때, 상기 서버는 상기 추가 음성 정보와 매칭되는 추가 텍스트 스크립트를 포함하는 추가 텍스트 파일을 생성할 수도 있다. 또한, 상기 제어부는 상기 변환된(또는 생성된) 추가 텍스트 스크립트(또는 추가 텍스트 파일)에 대해 자"}
{"patent_id": "10-2022-0044333", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "연어 처리를 수행하여 추가 키워드(또는 추가 명령어/태그/요약된 결과)를 추출한다. 일 예로, 상기 제 1 제어부는 상기 제 1 입력부를 통해 수신되는 제 1 사용자의 제 2 음성 정보(예를 들어 위빠 사나~ 호흡 안정 상태야)에 대해서 딥러닝을 통해 음성 인식 기능을 수행하여, 상기 제 2 음성 정보에 포함된 음성을 제 2 텍스트 스크립트(예를 들어 위빠사나 호흡 안정 상태야)로 변환한다. 또한, 상기 제 1 제어부는 상기 생성된 제 2 텍스트 스크립트(예를 들어 위빠사나 호흡 안정 상태야)에 대해 자 연어 처리를 수행하여 제 11 키워드(예를 들어 위빠사나) 및 제 12 키워드(예를 들어 호흡 안정 상태)를 추출한 다(S270). 이후, 상기 추출된 추가 키워드가 미리 설정된 호흡 안정 상태와 관련한 상태인지 여부(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함된 상태인지 여부)를 확인(또는 판단)한다. 여기서, 상 기 호흡 안정 상태는 호흡 안정, 호흡 정상, 숨 정상, 편안해요, 괜찮아요 등을 포함한다. 이때, 상기 제어부는 상기 추출된 하나 이상의 추가 키워드 중에서 첫 번째 추가 키워드(또는 어느 하나의 추가 키워드)가 해당 명상 제공 장치의 작동 시작을 나타내는 미리 설정된 명령어에 해당하는지 여부를 확 인하고, 상기 추출된 하나 이상의 추가 키워드 중에서 첫 번째 추가 키워드가 해당 명상 제공 장치의 작동 시작을 나타내는 명령어에 해당하는 경우에, 상기 추출된 하나 이상의 추가 키워드 중에서 상기 첫 번째 추가 키워드를 제외한 나머지 추가 키워드를 대상으로 상기 호흡 안정 상태와 관련한 상태인지 여부를 확인하도록 구 성할 수도 있다.일 예로, 상기 제 1 제어부는 상기 추출된 제 11 키워드(예를 들어 위빠사나) 및 제 12 키워드(예를 들어 호흡 안정 상태)가 상기 미리 설정된 호흡 안정 상태와 관련한 상태인지 여부를 확인한다(S280). 상기 확인 결과(또는 상기 판단 결과), 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태와 관련되 지 않은 상태인 경우(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함되지 않은 상태인 경우), 상기 제어부는 음성 정보를 다시 입력해줄 것을 안내하는 안내 정보를 상기 표시부 및 /또는 상기 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위해서, 대기 상태를 유지(또는 전환)한다. 이때, 상기 제어부는 상기 호흡 명상을 위한 올바른 자세와 관련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하는 과정으로 복귀하여, 사용자로부터 추가 음성 정보 를 입력받기 위한 대기 상태를 유지할 수 있다. 일 예로, 상기 추출된 제 11 키워드 및 제 12 키워드가 상기 미리 설정된 호흡 안정 상태와 관련한 상태가 아닐 때, 상기 제 1 제어부는 음성 정보를 재입력해줄 것을 요청하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 사용자로부터 다시 음성 정보를 수신하기 위한 대기 상태로 전환한다(S290). 또한, 상기 확인 결과(또는 상기 판단 결과), 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태와 관련된 상태인 경우(또는 상기 추출된 추가 키워드가 상기 미리 설정된 호흡 안정 상태 항목에 포함된 상태인 경우), 상기 제어부는 사용자가 선택한 사건과 관련해서 해당 장면(또는 상황)을 사용자가 떠올릴 수 있도 록 상기 저장부에 미리 저장된 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 여기서, 상기 장면(또는 상황)은 상기 사용자가 선택한 사건과 관련해서 해당 사용자가 기억하고 있는 복수의 장면 중에서 어느 하나의 장면(또는 일정 시간 동안의 연속된 장면/영상)(예를 들어 특정 시점에서의 하나의 장면, 5초 동안의 연속된 장면/영상 등 포함)일 수 있다. 즉, 상기 추출된 추가 키워드가 상기 사용자의 호흡 변화가 안정 상태임을 나타내는 정보(예를 들어 호흡 안정 상태, 호흡 정상, 숨 정상 등 포함)와 관련한 경우, 상기 제어부는 상기 장면 떠올리기 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 추출된 제 11 키워드(예를 들어 위빠사나) 및 제 12 키워드(예를 들어 호흡 안정 상태)가 해당 제 1 사용자의 호흡이 안정된 상태임을 나타낼 때, 상기 제 1 제어부는 상기 제 1 사용자가 선택한 제 1 사례인 부모님께 야단맞음과 관련해서 해당 제 1 사용자를 불편하게 한 하나의 장면을 떠올리도록 제 1 장면 떠올리기 안내 콘텐츠(예를 들어 사례에서 나를 불편하게 한 하나의 장면을 떠올려보세요)를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S300). 이후, 상기 제어부는 순차로 출력되는 서로 다른 안내 정보(또는 서로 다른 안내 콘텐츠)에 대응하여 사용 자 입력(또는 사용자 선택/터치/제어/사용자 음성 입력)에 따라 상기 사용자가 선택한 사건과 관련해서 해당 장 면(또는 사용자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또 는 수집)한다. 이때, 상기 수신되는 정보가 음성 정보인 경우, 상기 제어부는 상기 수신된(또는 수집된) 음성 정보 형태의 해당 장면(또는 사용자가 떠올리고/생각하고 있는 장면/상황)에 따른 사용자의 감각기관에 대 한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행에 따라 해당 음성 정보와 매칭되는 텍스트(또는 텍스트 스크립트)를 변환할 수도 있다. 즉, 상기 제어부는 사용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 상기 표시부 및/ 또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각기관에 대한 정보를 수신(또는 수집)한 다. 여기서, 상기 사용자가 접촉을 느끼는 감각기관은 불교에서 감각기관으로 정의하는 눈, 귀, 코, 혀, 몸(또 는 신체), 마음(또는 생각)일 수 있다. 또한, 상기 제어부는 사용자가 해당 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 상기 표 시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용(또는 주의 기울임/주의 기울 임 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 사용자가 해당 장면에서 느끼는 감정, 세부 감정 내용 등을 확인하기 위한 안내 정보 를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감 정, 해당 사용자의 감정과 관련한 세부 감정 내용(또는 세부 감정 종류) 등을 수신(또는 수집)한다. 또한, 상기 제어부는 사용자가 느꼈던 감정에 따른 해당 장면에서의 사용자의 인식을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 느꼈던 감정에 따른 해당 장면에서의 사용자의 인식(또는 사용자의 인식에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는 지에 대한 사용자의 영향을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통 해 출력하고, 사용자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향(또는 사용자에게 미친/끼친 영향에 대한 정보/사용자에게 미친 영향)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자 의 바람을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용 자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 상대방이 사용자에게 어떻게 해주기 를 바랐는지에 대한 사용자의 바람(또는 사용자의 바람에 대한 정보/사용자가 바라는 내용)을 수신(또는 수집) 한다. 또한, 상기 제어부는 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자 의 의도를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용 자 입력에 따라 상기 사용자가 선택한 사건과 관련한 해당 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도(또는 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 해당 제 1 사례(예를 들어 부모님께 야단맞음)와 관련해서 생각나는 제 1 장면(예 를 들어 아버지가 게임을 하고 있는 나에게 '맨날 게임만 하고 있니~'라고 말하는 장면)에 대해서 '나를 불편하 게 하는 것은 눈에 보이는 것인가요 아니면 귀에 들리는 소리인가요?' 등을 문의하는 안내 정보를 상기 제 1 표 시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 1 입력값(예를 들어 소리입니다)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '눈에 보이는 어떤 것이 나를 불편하게 하나요? 아니면 귀에 들리는 어떤 소리가 나를 불편하게 하나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 2 입력값(예를 들어 아버지가 나 에게 말하는 소리가 나를 불편하게 합니다)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '지금 화, 우울, 불안 등 중에서 어떤 감정이 드나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 3 입력값(예를 들어 화가 나요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '그 감정은 어떤 종류의 감정인 것 같으세요?' 등을 문의 하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 4 입력값(예를 들어 억울함에서 나오는 화인 것 같아요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '나는 그 상황을 어떻게 이해하고 받아들였나요? 나는 그 때 상대에게 어떻게 취급받았다고 느꼈나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 5 입력값(예를 들어 아버지가 나를 아 이 취급하는 것 같았어요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '그렇게 취급받는다는 것은 나에게 어떤 의미인가요?, 그 렇게 취급받으면 내 존재가 어떻게 될 것 같으세요?, 그렇게 취급받으면 내 존재가 어떻게 될 것 같기에 그런 감정이 올라올까요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력 하고, 상기 제 1 사용자의 음성 입력에 따른 제 6 입력값(예를 들어 나를 무시하는 것 같았어요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '그때 나는 상대가 어떻게 해주기를 바랐나요?' 등을 문 의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음 성 입력에 따른 제 7 입력값(예를 들어 나를 존중해줬으면 좋겠어요)을 수신한다.또한, 상기 제 1 제어부는 해당 제 1 장면과 관련해서 '그때 상대에게 어떻게 하고 싶었나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력 에 따른 제 8 입력값(예를 들어 아버지 저도 이제 성인이예요)을 수신한다(S310). 이후, 상기 제어부는 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 근거로 사용자의 마음 상태를 분석한다. 이때, 상기 제어 부는 상기 수신된 하나 이상의 음성 정보 형태의 데이터에 대해서 음성 인식 기능 수행 결과에 따른 하나 이상의 텍스트(또는 텍스트 스크립트)를 근거로 상기 사용자의 마음 상태를 분석할 수 있다. 즉, 상기 제어부는 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 포함하는 입력값(또는 텍스트 형태로 변환된 입력값)을 미리 학 습된 사용자 마음 상태 분석 모델(또는 서포트 벡터 머신: SVM)의 입력값으로 하여 학습 기능(또는 기계 학습/ 인공지능/딥 러닝)을 수행하고, 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용자의 마음 상태에 대한 분석 결과(또는 사용자의 마음 상태 분석 결과)를 생성한다. 여기서, 상기 사용자의 마음 상태 분 석 결과는 감정, 인식, 영향, 바람, 의도 등을 포함한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 1 입력값(예를 들어 소리입니다), 상기 제 2 입력값(예를 들 어 아버지가 나에게 말하는 소리가 나를 불편하게 합니다), 상기 제 3 입력값(예를 들어 화가 나요), 상기 제 4 입력값(예를 들어 억울함에서 나오는 화인 것 같아요), 상기 제 5 입력값(예를 들어 아버지가 나를 아이 취급하 는 것 같았어요), 상기 제 6 입력값(예를 들어 나를 무시하는 것 같았어요), 상기 제 7 입력값(예를 들어 나를 존중해줬으면 좋겠어요), 상기 제 8 입력값(예를 들어 아버지 저도 이제 성인이에요) 등을 상기 사용자 마음 상 태 분석 모델의 입력값으로 하여 학습 기능을 수행하고, 학습 기능 수행 결과에 따라 해당 제 1 사용자의 마음 상태 분석 결과(예를 들어 감정 상태에 해당하는 화남)를 생성한다(S320). 이후, 상기 제어부는 해당 장면(또는 상기 사용자가 선택한 사건)이 어린 시절에 경험한 사건인지 여부(또 는 해당 장면을 해당 사용자가 과거에 경험한 적이 있는지 여부)를 판단(또는 확인)한다. 여기서, 상기 과거(또 는 어린 시절)는 유년 시절, 초등학생 시절, 중학생 시절, 고등학생 시절, 청소년 시절 등을 포함한다. 즉, 상기 제어부는 해당 장면(또는 상기 사용자가 선택한 사건)이 어린 시절에 경험한 사건인지 여부를 문 의하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정 보(또는 해당 장면이 과거/어린 시절에 경험한 적이 있는지에 대한 정보)는 어린 시절에 경험한 사건이었음, 어 린 시절에 경험한 사실이 있음, 어린 시절에 경험한 사실이 없음 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보를 근거로 해당 장면(또는 해당 사용자가 선택한 사건)을 해당 사용자가 과거(또는 어린 시절)에 경험한 적 이 있는지 여부를 판단(또는 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또 는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인 식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 장면(또는 해당 사용자가 선택한 사 건)을 해당 사용자가 과거(또는 어린 시절)에 경험한 적이 있는지 여부를 판단할 수도 있다. 일 예로, 상기 제 1 제어부는 '해당 제 1 장면과 관련해서 어린 시절에 경험인가요?' 등을 문의하는 안내 정보 를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 9 입력값을 수신한다. 또한, 상기 제 1 제어부는 상기 수신된 제 9 입력값을 근거로 상기 제 1 사용자가 해당 제 1 장면과 관련해서 어린 시절 경험인지 여부를 판단한다(S330). 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여부에 대한 정보에 과거에 경험한 적이 있음에 대한 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 과거에 경험을 한 적이 있는 것으로 응답한 경우), 상기 제어부는 과거의 해당 장면과 관련한 대상이 타인인지 또 는 본인인지 여부를 확인한다. 즉, 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절에 경험한 사건인지 여 부에 대한 정보에 과거에 경험한 적이 있음에 대한 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 과거에 경험을 한 적이 있는 것으로 응답한 경우), 상기 제어부는 해당 과거의 장면과 관련한 대상이 타인인 지 또는 본인인지 여부를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 과거의 장면과 관련한 대상에 대한 정보를 수 신(또는 수집)한다. 여기서, 상기 해당 과거의 장면과 관련한 대상에 대한 정보는 타인, 본인 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태인지 또는 본인이 포함된 상태인지를 확인한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과거 의 장면과 관련한 대상에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍 스트(또는 텍스트 스크립트)를 근거로 상기 수신된 정보에 타인이 포함된 상태인지 또는 본인이 포함된 상태인 지를 확인할 수도 있다. 일 예로, 상기 수신된 제 9 입력값에 어린 시절에 경험한 정보가 포함된 상태일 때, 상기 제 1 제어부는 '상기 어린 시절에 경험한 상기 제 1 장면과 관련한 상대에게 의도를 표현해보실까요?' 등을 포함하는 안내 정보를 상 기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 10 입력값을 수신한다. 또한, 상기 제 1 제어부는 상기 수신된 제 10 입력값에 타인 및 본인 중 어느 정보가 포함된 상태인지를 확인한 다(S340). 상기 확인 결과, 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 타인이 포함된 상태인 경우(또는 상기 과거의 장면과 관련한 대상이 타인인 경우), 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 의도 표 현 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 상기 사용자가 해당 장면(또는 사 용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말(또는 상대방/자신에게 하고 싶은 말/내용) 을 수신(또는 수집)한다. 일 예로, 상기 수신된 제 10 입력값에 타인이 포함된 상태일 때, 상기 제 1 제어부는 '상대에게 또는 나에게 하 고 싶었던 것을 큰 소리로 표현해 보세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음 성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 11 입력값(예를 들어 아버지 저를 존중해 주세요)을 수신한다(S350). 이후, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 표현 완료 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 응답 정보를 수신(또는 수집)한다. 여기서, 상기 응답 정보는 긍정 응답 정보(예를 들어 예/네 다 했어요, 네 다음 단계로 갈게요 등 포함), 부정 응답 정보(예를 들어 아직 못했어요, 좀 더 생각하고 추가로 말할게요 등 포함) 등을 포함한다. 일 예로, 상기 제 11 입력값이 수신된 후 미리 설정된 20초가 지날때까지 사용자로부터 추가 입력값이 수신되지 않을 때, 상기 제 1 제어부는 '다 표현하셨나요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 1 응답 정보(예를 들어 예 다 했 어요)를 수신한다(S360). 이후, 상기 제어부는 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인지 여부를 판단(또는 확 인)한다. 즉, 상기 제어부는 상기 수신된 응답 정보에 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말을 사용자가 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확 인)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 1 응답 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트를 근거로 상기 수신된 제 1 응답 정보가 긍정 응답에 대응하는 미리 설정된 하고싶은 말을 완료한 상태임을 나타내는 정보에 해당하는지 여부를 판단한다(S370). 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수신된 응답 정보에 부정 응답 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어) 에 따라 상기 사용자로부터 해당 장면(또는 사용자가 선택한 사건)에 대해서 상대방(또는 자신)에게 하고 싶은 말(또는 상대방/자신에게 하고 싶은 말/내용)을 수신(또는 수집)하는 앞선 과정으로 복귀한다. 일 예로, 상기 판단 결과, 상기 제 1 응답 정보가 상기 미리 설정된 하고 싶은 말을 완료하지 않은 상태임을 나 타내는 정보에 해당할 때, 상기 제 1 제어부는 앞선 '상대에게 또는 나에게 하고 싶었던 것을 큰 소리로 표현해 보세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 입력값을 추가로 수신하는 과정(또는 S350 단계)으로 복귀한다(S380). 또한, 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 해 당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느 낌이 느껴지는지 여부를 확인(또는 판단)한다. 즉, 상기 판단 결과, 상기 수신된 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위에서 해당 감정이 느껴지는지를 확인하기 위한 안내 정보를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보는 감정이 신체 어느 부 위에서 어떤 느낌으로 느껴짐, 감정이 신체 부위에서 느껴지지 않음 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 근거로 해당 장면(또는 사용자 가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여 부를 확인(또는 판단)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립 트)를 근거로 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인할 수도 있다. 일 예로, 상기 판단 결과, 상기 제 1 응답 정보(예를 들어 예 다 했어요)가 상기 미리 설정된 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당할 때, 상기 제 1 제어부는 '그 감정이 신체 어느 부위에서 어떤 느낌으 로 느껴지나요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 12 입력값을 수신한다(S390). 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는 경우, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 어떤 느낌으로 느껴지고, 강도는 어느 정도인지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사 용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 근거로 사용 자의 몸 느낌(또는 몸 느낌 상태)을 분석한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느 낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 사용자의 몸 느낌(또는 몸 느낌 상태)을 분석할 수도 있다. 즉, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정 에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 포함하는 다른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 다른 학습 기능(또는 기계 학습/인공 지능/딥 러닝)을 수행하고, 다른 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용자의 몸 느낌에 대한 분석 결과(또는 사용자의 몸 느낌 분석 결과)를 생성한다. 여기서, 상기 사용자의 몸 느낌에 대한 분석 결과는 불안정, 안정, 평정심 유지 등을 포함한다. 일 예로, 상기 수신된 제 12 입력값에 해당 감정이 신체 부위에서 느껴져요가 포함된 상태일 때, 상기 제 1 제 어부는 '그 감정은 지금 내 몸 어디에 어떤 느낌으로 있나요?, 그 느낌은 0~100% 사이에서 몇 %로 느껴지세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 13 입력값(예를 들어 내 마음에서 느껴지고, 강도는 70%예요)을 수신한다. 또한, 상기 제 1 제어부는 상기 수신된 제 13 입력값(예를 들어 내 마음에서 느껴지고, 강도는 70%예요) 등을 상기 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 학습 기능을 수행하고, 학습 기능 수행 결과에 따라 해 당 제 1 사용자의 몸 느낌 분석 결과(예를 들어 불안정)를 생성한다(S400). 이후, 상기 제어부는 상기 사용자가 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자의 특정 신 체 부위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 지시문 콘텐츠(또는 명상 콘텐츠)를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 제 1 제어부는 '생각과 판단을 멈추고 신체 부위에서 느껴지는 느낌에만 집중합니다. 숨 들이쉬 면서 느껴보고, 숨 내쉬면서 느껴봅니다. 숨 들이 쉴 때 더 강하게 느껴지는지, 숨 내쉴 때 더 강하게 느껴지는 지 오롯이 신체 부위에서 느껴지는 느낌에만 집중합니다. 생각과 판단을 멈추고, 신체 부위에서 느껴지는 느낌 을 충분하게 느껴봅니다.' 등을 포함하는 지시문 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S410). 이후, 상기 제어부는 미리 설정된 시간 간격으로, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추 가 강도 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 강도가 어느 정도인지를 확인하기 위한 안내 정보를 미리 설정된 시간 간격으로 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보를 수신 (또는 수집)한다. 일 예로, 상기 제 1 제어부는 미리 설정된 1분 간격으로 '그 몸 느낌이 지금은 몇 %로 느껴지세요?' 등을 포함 하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 14 입력값(예를 들어 강도가 이제 없어요 0%예요)을 수신한다(S420). 이후, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이 하인지 여부를 추가 판단(또는 추가 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추 가 강도 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크 립트)를 근거로 상기 수신된 정보(또는 텍스트 형태의 추가 강도 정보)가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단할 수도 있다. 즉, 상기 제어부는 명상 과정을 통해 해당 사용자가 해당 장면과 관련해서 느끼는 감정에 대응하는 사용자 의 신체 부위에 대한 강도가 어느 정도 해소되었는지(또는 느낌이 소멸하였는지)를 확인하기 위해서, 상기 수신 된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단(또는 추가 확인)한다.일 예로, 상기 제 1 제어부는 상기 수신된 제 13 입력값이 상기 미리 설정된 기준값(예를 들어 5%) 이하인지 여 부를 추가 판단한다(S430). 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해 서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기준값(예 를 들어 0%, 5% 등 포함)을 초과한 경우, 상기 제어부는 앞서 지시문 콘텐츠를 출력하는 과정을 반복 수행 한다. 일 예로, 상기 수신된 제 14 입력값이 상기 미리 설정된 기준값(예를 들어 5%)보다 클 때, 상기 제 1 제어부는 상기 지시문 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하는 과정(예를 들어 앞선 S380 단계)을 수행한다(S440). 또한, 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 추가 강도 정보가 상기 미리 설정된 기 준값(예를 들어 0%, 5% 등 포함) 이하인 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련한 콘텐 츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 수신된 제 14 입력값(예를 들어 강도가 이제 없어요 0%예요)이 상기 미리 설정된 기준값(예를 들 어 5%) 이하일 때, 상기 제 1 제어부는 호흡 명상을 위한 제 2 영상과, 호흡 명상을 위한 올바른 자세 등에 대 한 제 2 음성 정보(예를 들어 자~ 이제 머리에서 장면을 지우고 호흡으로 돌아옵니다 호흡이 안정되면 눈을 뜹 니다)를 포함하는 제 2 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S450). 이후, 상기 제어부는 사용자 입력에 따른 사용자의 소감 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 제 1 제어부는 '오늘 새롭게 알게 되었거나 느꼈거나 통찰한 것이 있다면 소감 한마디 해주세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 15 입력값(예를 들어 과거에 아버지에게 서운했던 감정이 다 사라졌어요)을 수신한다 (S460). 이후, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과, 상기 수신된 사용자의 소감 정보 등을 근거로 사용자에 대한 명상 분석 결과를 생성한다. 여기서, 상기 명상 분석 결과는 사 용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 명상 시간 10분 중 상기 사용자의 마음 상태 분석 결과에서 사용자가 적극적으로 의사 표현을 한 상태 이고, 상기 사용자의 몸 느낌 분석 결과에서 사용자가 해당 사건과 관련해서 감각 기관을 통해 전달되는 몸의 느낌을 소멸한 상태이고, 상기 사용자의 소감 정보가 해당 사건과 관련해서 서운함을 없앤 상태임을 포함할 때, 상기 제 1 제어부는 각 항목에 대해서 미리 설정된 점수(예를 들어 마음 상태 분석 결과 항목에 40점, 몸 느낌 분석 결과 항목에 30점, 소감 정보 항목에 30점)에서, 상기 정보들을 각각 반영하여, 마음 상태 분석 결과 항목 에서 40점 중 38점, 몸 느낌 분석 결과 항목에서 30점 중 27점, 소감 정보 항목에서 30점 중 28점을 합산하여, 총 93점의 제 1 명상 분석 결과를 생성한다. 또한, 상기 제 1 제어부는 상기 생성된 제 1 명상 분석 결과를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력 부를 통해 출력한다(S470). 또한, 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼 는 감정이 사용자의 신체 부위에서 느껴지지 않은 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면 을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관 련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다.일 예로, 상기 수신된 제 12 입력값에 해당 감정이 신체 부위에서 느껴지지 않아요나 가슴이 시원해요 등이 포 함된 상태일 때, 상기 제 1 제어부는 호흡 명상을 위한 제 3 영상과, 호흡 명상을 위한 올바른 자세 등에 대한 제 3 음성 정보(예를 들어 자~ 이제 머리에서 장면을 지우고 호흡으로 돌아옵니다 호흡이 안정되면 눈을 뜹니다)를 포함하는 제 3 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S480). 이후, 상기 제어부는 사용자 입력에 따른 사용자의 다른 소감 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 다른 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 제 1 제어부는 '오늘 새롭게 알게 되었거나 느꼈거나 통찰한 것이 있다면 소감 한마디 해주세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 16 입력값(예를 들어 아버지와의 새로운 추억을 찾고 싶어요)을 수신한다(S490). 이후, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 수신된 사용자의 다른 소감 정보 등을 근 거로 사용자에 대한 다른 명상 분석 결과를 생성한다. 여기서, 상기 다른 명상 분석 결과는 사용자가 해당 사건 과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 일 예로, 명상 시간 8분 중 상기 사용자의 마음 상태 분석 결과에서 사용자가 적극적으로 의사 표현을 한 상태 이고, 상기 사용자의 다른 소감 정보가 해당 사건과 관련해서 새로운 시작을 희망하는 상태임을 포함할 때, 상 기 제 1 제어부는 각 항목에 대해서 미리 설정된 점수(예를 들어 마음 상태 분석 결과 항목에 50점, 소감 정보 항목에 50점)에서, 상기 정보들을 각각 반영하여, 마음 상태 분석 결과 항목에서 50점 중 49점, 소감 정보 항목 에서 50점 중 48점을 합산하여, 총 97점의 제 2 명상 분석 결과를 생성한다. 또한, 상기 제 1 제어부는 상기 생성된 제 2 명상 분석 결과를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력 부를 통해 출력한다(S500). 또한, 상기 확인 결과, 상기 수신된 해당 과거의 장면과 관련한 대상에 대한 정보에 자신이 포함된 상태인 경우 (또는 상기 과거의 장면과 관련한 대상이 자신인 경우), 상기 제어부는 해당 과거의 장면에서 어린 자신이 느끼는 감정, 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정, 해당 과거의 장면에서의 어린 자신에게 하고 싶은 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과거의 장면에서 어린 자신이 느끼는 감정, 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정, 해당 과거의 장면에서의 어린 자신에게 하고 싶은 의도 등에 대한 정보에 대해서 음성 인식 기 능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있 다. 즉, 상기 제어부는 해당 과거의 장면에서 어린 자신이 어떤 감정을 느꼈는지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 과거의 장면에 대 응해서 화, 우울, 불안 등 중에서 어린 자신이 어떤 감정을 느꼈는지에 대한 어린 자신의 감정(또는 어린 자신 이 느낀 감정에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정을 확인하기 위한 안 내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 과거의 장면에서의 어린 자신을 보는 현재의 사용자의 감정(또는 현재의 사용자의 감정에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 과거의 장면에서의 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도 를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력 에 따라 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또 는 어린 자신에 대한 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 일 예로, 상기 수신된 제 10 입력값에 자신이 포함된 상태일 때, 상기 제 1 제어부는 '어린 나의 마음은 어떤 것 같아요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 내면 아이의 감정에 대응하는 제 17 입력값(예를 들어 마음 아팠을 것 같아 요)을 수신한다. 또한, 상기 제 1 제어부는 '어린 나를 보니 지금 기분이 어떠세요?' 등을 포함하는 안내 정보를 상기 제 1 표시 부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 사용자의 현재 감정 에 대응하는 제 18 입력값(예를 들어 불쌍해요)을 수신한다. 또한, 상기 제 1 제어부는 '어린 나에게 어떻게 하고 싶으세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 19 입력값(예를 들 어 따스하게 안아주고 싶어요)을 수신한다(S510). 이후, 상기 제어부는 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도) 에 대한 정보에 행동과 관련한 정보(또는 내용)가 포함된 상태인지 여부를 판단한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 19 입력값에 행동과 관련한 내용이 포함된 상태인지 여부를 판단 한다(S520). 상기 판단 결과, 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도)에 대한 정보에 상기 행동과 관련한 정보가 포함된 상태인 경우, 상기 제어부는 상기 사용자가 해당 행동을 할 수 있도록 유도하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 이에 따라, 해당 사용자는 해당 행동을 수행할 수 있다. 일 예로, 상기 수신된 제 19 입력값(예를 들어 따스하게 안아주고 싶어요)에 행동과 관련한 내용(예를 들어 안 아주고)이 포함된 상태일 때, 상기 제 1 제어부는 '장면 속으로 들어가서 그렇게 해보세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S530). 또한, 상기 사용자가 해당 행동을 할 수 있도록 유도하는 안내 정보를 출력한 경우 또는, 상기 판단 결과, 상기 수신된 해당 과거의 장면에 대응해서 사용자가 어린 자신에게 하고 싶은 말이나 행동에 대한 사용자의 의도(또 는 어린 자신에 대한 사용자의 의도에 대한 정보/어린 자신에게 하고 싶은 의도)에 대한 정보에 상기 행동과 관 련한 정보가 포함되지 않은 상태인 경우, 상기 제어부는 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 할 수 있도록 상기 저장부에 미리 저장된 다른 의도 표현 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 상기 사용자가 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말(또는 어린 자신에게 하고 싶은 말/내용)을 수신(또는 수집)한다. 이때, 상 기 제어부는 상기 수신된 음성 정보 형태의 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말(또는 어린 자신에게 하고 싶은 말/내용)에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍 스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 '장면 속으로 들어가서 그렇게 해보세요' 등을 포함하는 안내 정보를 출력한 상태일 때 또는, 상 기 수신된 제 19 입력값에 행동과 관련한 내용이 포함되지 않은 상태일 때, 상기 제 1 제어부는 '어린 나에게 하고 싶은 말을 표현해 보세요, 또 어떤 말을 해주고 싶으세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 20 입력값(예를 들 어 괜찮아 다 잘 될꺼야)을 수신한다(S540). 이후, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했 는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 다른 표현 완료 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 사용자가 다 했는지 여부에 대한 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상 기 다른 응답 정보는 긍정 응답 정보(예를 들어 예/네 다 했어요, 네 다음 단계로 갈게요 등 포함), 부정 응답 정보(예를 들어 아직 못했어요, 좀 더 생각하고 추가로 말할게요 등 포함) 등을 포함한다.일 예로, 상기 제 20 입력값이 수신된 후 미리 설정된 20초가 지날때까지 사용자로부터 추가 입력값이 수신되지 않을 때, 상기 제 1 제어부는 '다 표현하셨나요?' 등을 포함하는 다른 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 2 응답 정보(예를 들어 네 다 했어요)를 수신한다(S550). 이후, 상기 제어부는 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 다른 응답 정보에 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말 을 사용자가 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 2 응답 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트를 근거로 상기 수신된 제 2 응답 정보가 긍정 응답에 대응하는 미리 설정된 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당하는지 여부를 판단한다(S560). 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수신된 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력(또는 사용자 선택/ 터치/제어)에 따라 상기 사용자로부터 해당 과거의 장면에 대해서 어린 자신에게 하고 싶은 말을 수신(또는 수 집)하는 앞선 과정으로 복귀한다. 일 예로, 상기 판단 결과, 상기 제 2 응답 정보가 상기 미리 설정된 하고 싶은 말을 완료하지 않은 상태임을 나 타내는 정보에 해당할 때, 상기 제 1 제어부는 앞선 '어린 나에게 하고 싶은 말을 표현해 보세요, 또 어떤 말을 해주고 싶으세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하 고, 상기 제 1 사용자의 음성 입력에 따른 입력값을 추가로 수신하는 과정(또는 S540 단계)으로 복귀한다 (S570). 또한, 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부(16 0)는 상기 사용자가 어린 자신에게 행한 행동 등에 따라 해당 과거의 장면에서의 어린 자신이 현재 느끼는 추가 감정 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 과 거의 장면에서의 어린 자신이 현재 느끼는 추가 감정 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음 성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 판단 결과, 상기 수신된 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우, 상기 제어부는 상기 사용자가 어린 자신에게 행한 행동 이후, 해당 과거의 장면에서 어린 자신이 어떤 감정을 추가로 느꼈는지 를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력 에 따라 해당 과거의 장면에 대응해서 화, 우울, 불안 등의 해소된 상태인지 또는 여전히 유지되고 있는 상태인 지에 대한 어린 자신의 추가 감정(또는 어린 자신의 느낀 추가 감정에 대한 정보)을 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 '지금은 어린 나의 기분이 어떤 것 같나요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 내면 아이의 추가 감정에 대응하는 제 21 입력값(예를 들어 다 털어버리고 괜찮아요)을 수신한다(S580). 이후, 상기 제어부는 해당 과거의 장면에서의 어린 자신으로부터 현재의 상태로 복귀하도록 과거 장면과의 헤어짐을 유도하는 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 제 1 제어부는 '자~ 이제 어린 나와 떨어져서 장면 밖으로 나옵니다' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S590). 이후, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인(또는 판단)한다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위에서 해당 감정이 느껴지는지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 해당 장면(또는 사용자가 선택 한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대 한 정보는 감정이 신체 어느 부위에서 어떤 느낌으로 느껴짐, 감정이 신체 부위에서 느껴지지 않음 등을 포함한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보를 근거로 해당 장면(또는 사용자 가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여 부를 확인(또는 판단)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립 트)를 근거로 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는지 여부를 확인할 수도 있다. 일 예로, 상기 제 1 제어부는 '그 감정이 신체 어느 부위에서 어떤 느낌으로 느껴지나요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따 른 제 22 입력값을 수신한다(S600). 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정이 사용자의 신체 어느 부위에서 어떤 느낌이 느껴지는 경우, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 즉, 상기 제어부는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 어떤 느낌으로 느껴지고, 강도는 어느 정도인지를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사 용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보 및 느낌의 강도 정보를 수신(또는 수집)한다. 또한, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 근거로 사용 자의 몸 느낌(또는 몸 느낌 상태)을 분석한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 느 낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 해당 사용자의 몸 느낌(또는 몸 느낌 상태)을 분석할 수도 있다. 즉, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정 에 대응하는 사용자의 신체 부위에 대한 느낌의 정보, 신체 부위에 대한 느낌의 강도 정보 등을 포함하는 또 다 른 입력값을 미리 학습된 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 또 다른 학습 기능(또는 기계 학습 /인공지능/딥 러닝)을 수행하고, 또 다른 학습 기능 수행 결과에 따라 해당 사건의 해당 장면과 관련해서 사용 자의 몸 느낌에 대한 분석 결과(또는 사용자의 몸 느낌 분석 결과)를 생성한다. 여기서, 상기 사용자의 몸 느낌 에 대한 분석 결과는 불안정, 안정, 평정심 유지 등을 포함한다. 일 예로, 상기 수신된 제 22 입력값에 해당 감정이 신체 부위에서 느껴져요가 포함된 상태일 때, 상기 제 1 제 어부는 '그 감정은 지금 내 몸 어디에 어떤 느낌으로 있나요?, 그 느낌은 0~100% 사이에 몇 %로 느껴지세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 23 입력값(예를 들어 내 마음에서 느껴지고, 강도는 30%예요)을 수신한다. 또한, 상기 제 1 제어부는 상기 수신된 제 23 입력값(예를 들어 내 마음에서 느껴지고, 강도는 30%예요) 등을 상기 사용자 몸 느낌 상태 분석 모델의 입력값으로 하여 학습 기능을 수행하고, 학습 기능 수행 결과에 따라 해 당 제 1 사용자의 다른 몸 느낌 분석 결과(예를 들어 안정)를 생성한다(S610). 이후, 상기 제어부는 상기 사용자가 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자의 특정 신 체 부위에서 느끼는 감정에 집중할 수 있도록 상기 저장부에 미리 저장된 상기 지시문 콘텐츠(또는 상기 명상 콘텐츠)를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 제 1 제어부는 '생각과 판단을 멈추고 신체 부위에서 느껴지는 느낌에만 집중합니다. 숨 들이쉬 면서 느껴보고, 숨 내쉬면서 느껴봅니다. 숨 들이 쉴 때 더 강하게 느껴지는지, 숨 내쉴 때 더 강하게 느껴지는 지 오롯이 신체 부위에서 느껴지는 느낌에만 집중합니다. 생각과 판단을 멈추고, 신체 부위에서 느껴지는 느낌 을 충분하게 느껴봅니다.' 등을 포함하는 상기 지시문 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력 부를 통해 출력한다(S620). 이후, 상기 제어부는 상기 미리 설정된 시간 간격으로, 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대 한 다른 추가 강도 정보를 수신(또는 수집)한다. 즉, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 강도가 어느 정도인지를 확인하기 위한 안내 정보를 미리 설정된 시간 간격으로 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보를 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 상기 미리 설정된 1분 간격으로 '그 몸 느낌이 지금은 몇 %로 느껴지세요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 24 입력값(예를 들어 강도 0%예요)을 수신한다(S630). 이후, 상기 제어부는 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감 정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 미리 설정된 기준값(예를 들어 0%, 5% 등 포 함) 이하인지 여부를 추가 판단(또는 추가 확인)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대 한 다른 추가 강도 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 상기 수신된 정보(또는 텍스트 형태의 추가 강도 정보)가 상기 미리 설정된 기준값 (예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단할 수도 있다. 즉, 상기 제어부는 명상 과정을 통해 해당 사용자가 해당 장면과 관련해서 느끼는 감정에 대응하는 사용자 의 신체 부위에 대한 강도가 어느 정도 해소되었는지(또는 느낌이 소멸하였는지)를 확인하기 위해서, 상기 수신 된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정된 기준값(예를 들어 0%, 5% 등 포함) 이하인지 여부를 추가 판단 (또는 추가 확인)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 24 입력값이 상기 미리 설정된 기준값(예를 들어 5%) 이하인지 여 부를 추가 판단한다(S640). 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해 서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정된 기준 값(예를 들어 0%, 5% 등 포함)을 초과한 경우, 상기 제어부는 앞서 지시문 콘텐츠를 출력하는 과정을 반복 수행한다. 일 예로, 상기 수신된 제 24 입력값이 상기 미리 설정된 기준값(예를 들어 5%)보다 클 때, 상기 제 1 제어부는 상기 지시문 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하는 과정을 수행한다 (S650). 또한, 상기 추가 판단 결과(또는 상기 추가 확인 결과), 상기 수신된 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 다른 추가 강도 정보가 상기 미리 설정 된 기준값(예를 들어 0%, 5% 등 포함) 이하인 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관련 한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 수신된 제 24 입력값(예를 들어 강도 0%예요)이 상기 미리 설정된 기준값(예를 들어 5%) 이하일 때, 상기 제 1 제어부는 호흡 명상을 위한 제 2 영상과, 호흡 명상을 위한 올바른 자세 등에 대한 제 2 음성 정 보(예를 들어 자~ 이제 머리에서 장면을 지우고 호흡으로 돌아옵니다 호흡이 안정되면 눈을 뜹니다)를 포함하는 상기 제 2 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S660). 이후, 상기 제어부는 사용자 입력에 따른 사용자의 또 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 사용자의 또 다른 소감 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 또 다른 소감 정보를 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 '오늘 새롭게 알게 되었거나 느꼈거나 통찰한 것이 있다면 소감 한마디 해주세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 25 입력값(예를 들어 아버지에 대한 서운했던 감정이 사라졌고, 미안해요)을 수신한 다(S670). 이후, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 사용자의 몸 느낌 분석 결과, 상기 수신된 사용자의 또 다른 소감 정보 등을 근거로 사용자에 대한 또 다른 명상 분석 결과를 생성한다. 여기서, 상기 또 다른 명상 분석 결과는 사용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정 보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 또 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력한다. 일 예로, 명상 시간 20분 중 상기 사용자의 마음 상태 분석 결과에서 사용자가 적극적으로 의사 표현을 한 상태 이고, 상기 사용자의 몸 느낌 분석 결과에서 사용자가 해당 사건과 관련해서 감각 기관을 통해 전달되는 몸의 느낌을 소멸한 상태이고, 상기 사용자의 소감 정보가 해당 사건과 관련해서 서운함을 없애고 미안함을 느끼고 있는 상태임을 포함할 때, 상기 제 1 제어부는 각 항목에 대해서 미리 설정된 점수(예를 들어 마음 상태 분석 결과 항목에 40점, 몸 느낌 분석 결과 항목에 30점, 소감 정보 항목에 30점)에서, 상기 정보들을 각각 반영하여, 마음 상태 분석 결과 항목에서 40점 중 35점, 몸 느낌 분석 결과 항목에서 30점 중 30점, 소감 정보 항목에서 30점 중 30점을 합산하여, 총 95점의 제 3 명상 분석 결과를 생성한다. 또한, 상기 제 1 제어부는 상기 생성된 제 3 명상 분석 결과를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력 부를 통해 출력한다(S680). 또한, 상기 확인 결과(또는 상기 판단 결과), 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼 는 감정이 사용자의 신체 부위에서 느껴지지 않은 경우, 상기 제어부는 사용자가 해당 사건과 관련한 장면 을 지우고 초기 상태로 돌아올 수 있도록, 상기 저장부에 미리 저장된 호흡 명상을 위한 올바른 자세와 관 련한 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 수신된 제 22 입력값에 해당 감정이 신체 부위에서 느껴지지 않아요나 가슴이 시원해요 등이 포 함된 상태일 때, 상기 제 1 제어부는 호흡 명상을 위한 제 3 영상과, 호흡 명상을 위한 올바른 자세 등에 대한 제 3 음성 정보(예를 들어 자~ 이제 머리에서 장면을 지우고 호흡으로 돌아옵니다 호흡이 안정되면 눈을 뜹니다)를 포함하는 상기 제 3 콘텐츠를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다 (S690). 이후, 상기 제어부는 사용자 입력에 따른 사용자의 추가 또 다른 소감 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 사용자의 추가 또 다른 소감 정보에 대해서 음성 인식 기능 을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 전체 명상 과정을 통한 사용자의 소감을 수신하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 출력하고, 사용자 입력에 따른 추가 또 다른 소감 정보를 수신(또는 수집)한 다. 일 예로, 상기 제 1 제어부는 '오늘 새롭게 알게 되었거나 느꼈거나 통찰한 것이 있다면 소감 한마디 해주세요' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 26 입력값(예를 들어 아버지 미안하고 사랑해요)을 수신한다(S700). 이후, 상기 제어부는 상기 사용자의 마음 상태 분석 결과, 상기 수신된 사용자의 추가 또 다른 소감 정보 등을 근거로 사용자에 대한 추가 또 다른 명상 분석 결과를 생성한다. 여기서, 상기 추가 또 다른 명상 분석 결 과는 사용자가 해당 사건과 관련해서 명상 수행을 효율적으로 수행했는지 여부를 나타내는 정보 등을 포함한다. 또한, 상기 제어부는 상기 생성된 추가 또 다른 명상 분석 결과를 상기 표시부 및/또는 상기 음성 출 력부를 통해 출력한다. 일 예로, 명상 시간 15분 중 상기 사용자의 마음 상태 분석 결과에서 사용자가 적극적으로 의사 표현을 한 상태 이고, 상기 수신된 제 26 입력값의 내용이 해당 사건과 관련해서 미안 마음인 상태임을 포함할 때, 상기 제 1 제어부는 각 항목에 대해서 미리 설정된 점수(예를 들어 마음 상태 분석 결과 항목에 50점, 소감 정보 항목에 50점)에서, 상기 정보들을 각각 반영하여, 마음 상태 분석 결과 항목에서 50점 중 50점, 소감 정보 항목에서 50 점 중 48점을 합산하여, 총 98점의 제 4 명상 분석 결과를 생성한다. 또한, 상기 제 1 제어부는 상기 생성된 제 4 명상 분석 결과를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력 부를 통해 출력한다(S710). 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절 경험이 아닌 정보가 포함 된 경우(또는 해당 장면과 관련해서 사용자가 어린 시절 경험이 아닌 것으로 응답한 경우/상기 장면이 어린 시 절에 경험한 사건인지 여부에 대한 정보에 어린 시절 경험이 아닌 정보가 포함된 상태일 때), 상기 제어부(16 0)는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용 자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신(또는 수집)한다. 여기서, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보(또는 몸 느낌 정보)는 머리, 가슴, 어깨, 팔, 다리, 손 등에서 나타나는 느낌 등일 수 있다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응 하는 사용자의 신체 부위에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 판단 결과(또는 상기 확인 결과), 상기 해당 장면(또는 해당 사건)이 어린 시절 경험이 아닌 정보가 포함된 경우(또는 해당 장면과 관련해서 사용자가 어린 시절 경험이 아닌 것으로 응답한 경우/상기 장면이 어린 시절에 경험한 사건인지 여부에 대한 정보에 어린 시절 경험이 아닌 정보가 포함된 상태일 때), 상기 제어부 는 해당 장면과 관련한 감정에 대해서 사용자의 신체 부위 중 어느 부위에서 해당 감정이 느껴지는지를 확 인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따 라 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 감정에 대응하는 사용자의 신체 부위에 대한 정보를 수신(또는 수집)한다. 일 예로, 상기 수신된 제 9 입력값에 어린 시절 경험이 아닌 정보가 포함된 상태일 때, 상기 제 1 제어부는 '그 감정은 지금 내 몸 어디에 어떤 느낌으로 있나요?' 등을 포함하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 27 입력값(예를 들어 내 마음에서 느껴져요)을 수신한다(S720). 이후, 상기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응 하는 과거(또는 어린 시절)의 다른 장면(또는 다른 상황)을 사용자가 떠올릴 수 있도록 상기 저장부에 미 리 저장된 다른 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한 다. 일 예로, 상기 제 1 제어부는 상기 제 1 장면과 유사한 몸 느낌을 느끼는 과거의 다른 장면을 제 1 사용자가 떠 올릴 수 있도록 제 2 장면 떠올리기 안내 콘텐츠(예를 들어 지금 몸의 느낌과 동일한 어린 시절의 기억을 떠올 려보세요)를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력한다(S730). 이후, 상기 제어부는 순차로 출력되는 서로 다른 안내 정보(또는 서로 다른 안내 콘텐츠)에 대응하여 사용 자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사용자가 떠올린 과거 다른 장면(또는 사용자가 떠올리고/생 각하고 있는 과거 다른 장면/상황)에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정 보 형태의 해당 사용자가 떠올린 과거 다른 장면(또는 사용자가 떠올리고/생각하고 있는 과거 다른 장면/상황) 에 따른 사용자의 감각기관에 대한 정보, 내용(또는 주의 기울임/주의 기울임 정보), 사용자의 감정 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 접촉을 느끼는 감각기관을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 사용자의 감각기관 중에서 사용자가 접촉을 느끼는 감각기관에 대한 정보를 수신(또는 수집)한다. 여기서, 상기 사용자가 접촉을 느끼는 감각기관은 불교에서 감각기관 으로 정의하는 눈, 귀, 코, 혀, 몸, 마음(또는 생각)일 수 있다. 또한, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 해당 과거 다른 장면에서 불편함을 느끼는 내용을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 사용자가 어떤 점을 불편하게 느끼는지에 대한 내용(또는 주의 기울임/주의 기울임 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면과 관련해서 사용자가 해당 과거 다른 장면에서 느끼는 감정, 세부 감정 내용 등을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부 를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면에 대응해서 화, 우울, 불안 등 중에서 어떤 감정이 느껴지는지에 대한 사용자의 감정, 해당 사용자의 감정과 관련한 세부 감정 내용(또는 세부 감정 종류) 등을 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 해당 제 1 장면(예를 들어 아버지가 게임을 하고 있는 나에게 '맨날 게임만 하고 있니~'라고 말하는 장면)과 관련한 유사한 경험에 따른 제 3 장면(예를 들어 어머니가 텔레비전을 보고 있는 나 에게 '공부는 다 했니~'라고 말하는 장면)에 대해서 '나를 불편하게 하는 것은 눈에 보이는 것인가요 아니면 귀 에 들리는 소리인가요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 28 입력값(예를 들어 소리입니다)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '눈에 보이는 어떤 것이 나를 불편하게 하나요? 아니면 귀에 들리는 어떤 소리가 나를 불편하게 하나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 29 입력값(예를 들어 어머니가 나 에게 말하는 소리가 나를 불편하게 합니다)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '지금 화, 우울, 불안 등 중에서 어떤 감정이 드나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용 자의 음성 입력에 따른 제 30 입력값(예를 들어 화가 나요)을 수신한다(S740). 이후, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾기를 수행할지 여부에 대한 사용자 입력값(또는 사용자 음성 정보)을 수신(또는 수집)한다. 여기서, 상기 투사 (projection)는 방어 기재와 관련한 용어로, 불안을 일으키는 충동이나 감정을 타인의 것으로 떠넘김으로써 자 신을 방어하는 방법을 나타낸다. 즉, 상기 제어부는 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾기를 수행할지 여부를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 떠올린 과거 다른 장면(또는 과거 다른 상황)과 관련해서 투사 찾 기를 수행할지 여부에 대한 사용자 입력값을 수신(또는 수집)한다. 여기서, 상기 사용자 입력값은 투사 찾기, 투사 찾기 아님 등을 포함한다. 일 예로, 상기 제 1 제어부는 해당 제 3 장면(예를 들어 어머니가 텔레비전을 보고 있는 나에게 '공부는 다 했 니~'라고 말하는 장면)에 대해서 '투사를 수행할까요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 31 입력값(예를 들어 예, 투 사 찾기를 수행할게요)을 수신한다(S750). 이후, 상기 제어부는 상기 수신된 사용자 입력값이 투사와 관련한 내용인지 여부(또는 상기 수신된 사용자 입력값에 투사와 관련한 내용/정보가 포함된 상태인지 여부)를 추가 확인(또는 추가 판단)한다. 이때, 상기 제 어부는 상기 수신된 음성 정보 형태의 사용자 입력값에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 근거로 상기 수신된 사용자 입력값(또는 상기 텍스트로 변환된 사용자 입력값)이 투사와 관련한 내용인지 여부(또는 상기 수신된 사용자 입력값에 투사와 관련한 내용/ 정보가 포함된 상태인지 여부)를 추가 확인할 수도 있다. 즉, 상기 제어부는 상기 수신된 사용자 입력값에 투사 찾기(또는 투사)와 관련한 정보가 포함된 상태인지 여부를 추가 확인(또는 추가 판단)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 31 입력값에 투사 찾기와 관련한 내용이 포함된 상태인지 여부를 추가 확인한다(S760).상기 추가 확인 결과(또는 상기 추가 판단 결과), 상기 수신된 사용자 입력값에 투사 찾기와 관련한 정보가 포 함된 상태인 경우(또는 상기 수신된 사용자 입력값이 투사 찾기인 경우/상기 사용자 입력에 투사와 관련한 내용 이나 정보가 포함된 경우), 상기 제어부는 상기 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장 면을 비교할 수 있도록 상기 저장부에 미리 저장된 다른 지시문 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입 력에 따른 두 장면의 공통점 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음 성 정보 형태의 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 공통점 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍 스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 수신된 제 31 입력값(예를 들어 예, 투사 찾기를 수행할게요)에 투사 찾기와 관련한 내용이 포함 된 상태일 때, 상기 제 1 제어부는 '스크린의 왼쪽에는 현재의 장면이, 스크린의 오른쪽에는 과거 어린 시절의 다른 장면이 있습니다. 보이시나요? 두 장면에서 공통점을 찾아보세요' 등을 안내하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 32 입력값 (예를 들어 공통점이 XXXXX 입니다) 등을 수신한다(S770). 이후, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상기 또 다른 응답 정보는 긍정 응답 정보(예를 들어 다했어요, 이제 없어요, 됐어요 등 포함), 부정 응답 정보(예를 들어 모르겠어요, 못 찾겠어요 등 포함) 등을 포함한다. 일 예로, 상기 제 32 입력값이 수신된 후 미리 설정된 20초가 지날때까지 사용자로부터 추가 입력값이 수신되지 않을 때, 상기 제 1 제어부는 '두 장면에서 공통점을 다 찾으셨나요?' 등을 포함하는 안내 정보를 상기 제 1 표 시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 3 응답 정보 (예를 들어 이제 없어요)를 수신한다(S780). 이후, 상기 제어부는 상기 수신된 또 다른 응답 정보에 긍정 응답 정보(또는 공통점과 관련해서 응답을 완 료한 상태임을 나타내는 정보)가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 또 다른 응답 정보에 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 공통점에 대해서 말을 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 3 응답 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트를 근거로 상기 수신된 제 3 응답 정보가 긍정 응답에 대응하는 미리 설정된 공통점 과 관련해서 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당하는지 여부를 판단한다(S790). 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수 신된 또 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우/상기 수신된 또 다른 응답 정보에 공통점과 관 련해서 응답을 완료한 상태가 아님을 나타내는 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력 (또는 사용자 선택/터치/제어)에 따라 상기 사용자로부터 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 공통점 등에 대한 정보를 수신(또는 수집)하는 앞선 과정으로 복귀한 다. 일 예로, 상기 판단 결과, 상기 제 3 응답 정보가 상기 미리 설정된 공통점과 관련해서 하고 싶은 말을 완료하 지 않은 상태임을 나타내는 정보에 해당할 때, 상기 제 1 제어부는 '또 어떤 공통점이 있나요?'를 안내하는 안 내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 입력값을 추가로 수신하는 과정(또는 S770 단계)으로 복귀한다(S800). 또한, 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우(또는 상기 수신 된 또 다른 응답 정보에 공통점과 관련해서 응답을 완료한 상태임을 나타내는 정보가 포함된 상태인 경우), 상기 제어부는 상기 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교할 수 있도록 상기 저장부에 미리 저장된 또 다른 지시문 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자가 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입 력에 따른 두 장면의 차이점 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음 성 정보 형태의 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 차이점 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍 스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 판단 결과, 상기 제 3 응답 정보(예를 들어 이제 없어요)가 상기 미리 설정된 공통점과 관련해서 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당할 때, 상기 제 1 제어부는 '스크린의 왼쪽에는 현재의 장면이, 스크린의 오른쪽에는 과거 어린 시절의 다른 장면이 있습니다. 보이시나요? 두 장면에서 차이점을 찾아 보세요' 등을 안내하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 33 입력값(예를 들어 차이점은 YYYYYY 입니다) 등을 수신한다(S810). 이후, 미리 설정된 시간(예를 들어 20초)이 지나도록 사용자 입력에 따른 정보(또는 음성 정보)가 추가로 수신 되지 않는 경우, 상기 제어부는 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부를 확인하기 위해서 상기 저장부에 미리 저장된 또 다른 표현 완료 안내 콘텐츠를 상 기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 또한, 상기 제어부는 사용자 입력(또는 사용자 선택/터치/제어)에 따라 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 다 했는지 여부에 대한 또 다른 응답 정보를 수신(또는 수집)한다. 여기서, 상기 또 다른 응답 정보는 긍정 응답 정보(예를 들어 다했어요, 이제 없어요, 됐어요 등 포함), 부정 응답 정보(예를 들어 모르겠어요, 못 찾겠어요 등 포함) 등을 포함한다. 일 예로, 상기 제 33 입력값이 수신된 후 미리 설정된 20초가 지날때까지 사용자로부터 추가 입력값이 수신되지 않을 때, 상기 제 1 제어부는 '두 장면에서 차이점을 다 찾으셨나요?' 등을 포함하는 안내 정보를 상기 제 1 표 시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 4 응답 정보 (예를 들어 이제 없어요)를 수신한다(S820). 이후, 상기 제어부는 상기 수신된 또 다른 응답 정보에 긍정 응답 정보(또는 차이점과 관련해서 응답을 완 료한 상태임을 나타내는 정보)가 포함된 상태인지 여부를 판단(또는 확인)한다. 즉, 상기 제어부는 상기 수신된 또 다른 응답 정보에 해당 사건과 관련한 현재의 장면과 과거 다른 장면 간의 차이점에 대해서 말을 완료한 상태임을 나타내는 정보가 포함된 상태인지 여부를 판단(또는 확인)한다. 일 예로, 상기 제 1 제어부는 상기 수신된 제 4 응답 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기 능 수행 결과에 따른 텍스트를 근거로 상기 수신된 제 4 응답 정보가 긍정 응답에 대응하는 미리 설정된 차이점 과 관련해서 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당하는지 여부를 판단한다(S830). 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함되지 않은 상태인 경우(또는 상기 수 신된 또 다른 응답 정보에 부정 응답 정보가 포함된 상태인 경우/상기 수신된 또 다른 응답 정보에 차이점과 관 련해서 응답을 완료한 상태가 아님을 나타내는 정보가 포함된 상태인 경우), 상기 제어부는 사용자 입력 (또는 사용자 선택/터치/제어)에 따라 상기 사용자로부터 해당 사건과 관련한 현재의 장면과 과거 다른 장면을 비교한 후, 사용자 입력에 따른 두 장면의 차이점 등에 대한 정보를 수신(또는 수집)하는 앞선 과정으로 복귀한 다. 일 예로, 상기 판단 결과, 상기 제 4 응답 정보가 상기 미리 설정된 차이점과 관련해서 하고 싶은 말을 완료하 지 않은 상태임을 나타내는 정보에 해당할 때, 상기 제 1 제어부는 '또 어떤 차이점이 있나요?'를 안내하는 안 내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 입력값을 추가로 수신하는 과정(또는 S810 단계)으로 복귀한다(S840). 또한, 상기 판단 결과, 상기 수신된 또 다른 응답 정보에 긍정 응답 정보가 포함된 상태인 경우(또는 상기 수신 된 또 다른 응답 정보에 차이점과 관련해서 응답을 완료한 상태임을 나타내는 정보가 포함된 상태인 경우), 상 기 제어부는 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거 (또는 어린 시절)의 다른 장면(또는 다른 상황)을 사용자가 떠올릴 수 있도록 상기 저장부에 미리 저장된또 다른 장면 떠올리기 안내 콘텐츠를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력한다. 일 예로, 상기 판단 결과, 상기 제 4 응답 정보(예를 들어 이제 없어요)가 상기 미리 설정된 차이점과 관련해서 하고 싶은 말을 완료한 상태임을 나타내는 정보에 해당할 때, 상기 제 1 제어부는 상기 제 1 장면과 유사한 몸 느낌을 느끼는 과거의 다른 장면인 상기 제 3 장면을 제 1 사용자가 떠올릴 수 있도록 제 3 장면 떠올리기 안내 콘텐츠(예를 들어 지금 어린 시절의 기억을 떠올려보세요)를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부 를 통해 출력한다(S850). 이후, 상기 제어부는 상기 또 다른 장면 떠올리기 안내 콘텐츠를 출력한 후, 해당 장면(또는 사용자가 선 택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 장면에 따른 사용자의 인 식, 영향, 바람, 의도 등에 대한 정보를 수신하는 과정으로 복귀하여, 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절) 의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 즉, 상기 제어부는 상기 또 다른 장면 떠올리기 안내 콘텐츠를 출력한 후, 해당 장면(또는 사용자가 선택 한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대해서 사용자 가 느꼈던 감정에 따른 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에서의 사 용자의 인식을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 느꼈던 감정에 따른 해당 과거(또는 어린 시절)의 다른 장면에서의 사용자의 인식(또는 사용자의 인식 에 대한 정보)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 사용자가 느꼈던 감정에 따라 사용자에게 어떤 영향을 주었는지에 대한 사용자의 영향을 확인하기 위한 안 내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자 가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 느꼈던 감정에 따라 사 용자에게 어떤 영향을 주었는지에 대한 사용자의 영향(또는 사용자에게 미친/끼친 영향에 대한 정보/사용자에게 미친 영향)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람을 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느 낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 상대방이 사용자에게 어떻게 해주기를 바랐는지에 대한 사용자의 바람(또는 사용자의 바람에 대한 정보/사용자가 바라는 내용)을 수신(또는 수집)한다. 또한, 상기 제어부는 해당 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응 해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도를 확인하기 위한 안내 정보를 상기 표시부 및/또는 상기 음성 출력부를 통해 출력하고, 사용자 입력에 따라 해당 사용자가 느끼는 몸 느 낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 대응해서 사용자가 상대방에게 하고 싶었던 말이나 행동에 대한 사용자의 의도(또는 사용자의 의도에 대한 정보)를 수신(또는 수집)한다. 일 예로, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '나는 그 상황을 어떻게 이해하고 받아들였나요? 나는 그때 상대에게 어떻게 취급받았다고 느꼈나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 34 입력값(예를 들어 어머니가 나를 어린아이 취급했어요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '그렇게 취급받는다는 것은 나에게 어떤 의미인가요?, 그 렇게 취급받으면 내 존재가 어떻게 될 것 같으세요?, 그렇게 취급받으면 내 존재가 어떻게 될 것 같기에 그런 감정이 올라올까요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력 하고, 상기 제 1 사용자의 음성 입력에 따른 제 35 입력값(예를 들어 나를 무시하는 것 같았어요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '그때 나는 상대가 어떻게 해주기를 바랐나요?' 등을 문 의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력에 따른 제 36 입력값(예를 들어 어린아이 취급하지 않았으면 해요)을 수신한다. 또한, 상기 제 1 제어부는 해당 제 3 장면과 관련해서 '그때 상대에게 어떻게 하고 싶었나요?' 등을 문의하는 안내 정보를 상기 제 1 표시부 및/또는 상기 제 1 음성 출력부를 통해 출력하고, 상기 제 1 사용자의 음성 입력 에 따른 제 37 입력값(예를 들어 날 어린아이 취급하지 마세요)을 수신한다(S860). 또한, 상기 추가 확인 결과(또는 상기 추가 판단 결과), 상기 수신된 사용자 입력값에 투사 찾기와 관련한 정보 가 포함되지 않은 상태인 경우(또는 상기 수신된 사용자 입력값이 투사 찾기 아님인 경우/상기 사용자 입력값에 투사와 관련한 내용이나 정보가 포함되지 않은 경우), 상기 제어부는 해당 장면(또는 사용자가 선택한 사 건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신하는 과정으로 복귀하여, 해당 장면(또는 사용자가 선택한 사건)과 관 련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절)의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보를 수신(또는 수집)한다. 이때, 상기 제어부는 상기 수신된 음성 정보 형태의 해당 장면(또는 사용자가 선택한 사건)과 관련해서 사용자가 느끼는 몸 느낌에 대응하는 과거(또는 어린 시절) 의 다른 장면에 따른 사용자의 인식, 영향, 바람, 의도 등에 대한 정보에 대해서 음성 인식 기능을 수행하고, 음성 인식 기능 수행 결과에 따른 텍스트(또는 텍스트 스크립트)를 생성(또는 변환)할 수도 있다. 일 예로, 상기 수신된 제 31 입력값에 투사 찾기와 관련한 내용이 포함되지 않은 상태일 때, 상기 제 1 제어부 는 해당 제 3 장면과 관련해서 상기 제 1 사용자의 음성 입력에 따른 제 38 입력값(예를 들어 어머니가 나를 어 린아이 취급했어요), 제 39 입력값(예를 들어 나를 무시하는 것 같았어요), 제 40 입력값(예를 들어 어린아이 취급하지 않았으면 해요), 제 41 입력값(예를 들어 날 어린아이 취급하지 마세요) 등을 수신한다(S870). 이후, 상기 제어부는 앞선 상기 수신된 해당 장면에 따른 사용자의 감각기관에 대한 정보, 내용, 사용자의 감정, 세부 감정 내용, 인식, 영향, 바람, 의도 등을 근거로 사용자의 마음 상태를 분석하는 과정부터 재수행한 다. 일 예로, 상기 제 1 제어부는 앞선 S320 단계부터 재수행한다(S880). 본 발명의 실시예는 앞서 설명된 바와 같이, 사용자의 현재 상태에 따른 정보를 제공하여 해당 사용자의 마음 상태를 관찰하고, 사용자가 유년 시절을 생각하면서 느끼는 몸 느낌 상태를 관찰하고, 관찰되는 사용자의 마음 상태나 몸 느낌 상태에 따라 명상 수행 전후의 상태를 분석하여 명상에 따른 효과를 제공하여, 실시간으로 확인 되는 사용자의 마음 상태 및 몸 느낌 상태에 따라 효율적으로 명상 정보를 제공하고, 사용자 자신이 상황에 대 한 이해와 공감을 통해 평정심 또는 생각멈춤을 높일 수 있다. 전술된 내용은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어 나지 않는 범위에서 수정 및 변형이 가능할 것이다. 따라서, 본 발명에 개시된 실시예들은 본 발명의 기술 사상 을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 실시예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2022-0044333", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 장치의 구성을 나타낸 블록도이다. 도 2 내지 도 8은 본 발명의 실시예에 따른 인공 지능을 이용한 명상 제공 방법을 나타낸 흐름도이다."}
