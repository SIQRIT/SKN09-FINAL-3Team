{"patent_id": "10-2021-0155221", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0068918", "출원번호": "10-2021-0155221", "발명의 명칭": "클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치 및 방법", "출원인": "한국전력공사", "발명자": "장민영"}}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "딥러닝 학습 도구의 GPU 자원 소요량 계산을 위한 기초 데이터를 수집하는 딥러닝 학습 도구 로그 수집부;상기 기초 데이터를 활용하여 상기 딥러닝 학습 도구를 구성하는 프로세서의 단위별 행렬(Matrix) 연산량을 실시간으로 계산하는 딥러닝 학습 연산량 계산부;상기 딥러닝 학습 도구에 대하여 GPU 클러스터의 GPU 자원 스케쥴링을 수행하되, 디폴트 자원 스케쥴링과 스케일링 자원 스케쥴링으로 구분하여 수행하는 메모리 기반 GPU 자원 할당부; 및상기 딥러닝 학습 도구에 최종적으로 할당한 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을계산하여 자원을 회수하는 GPU 유휴자원 회수부를 포함하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 딥러닝 학습 도구 로그 수집부는, 상기 딥러닝 학습 도구의 GPU 자원 소요량 계산을 위해 상기 딥러닝 학습 도구의 사용자 입력 Command에 기반하여 행렬 덧셈과 곱셉 연산량 계산용 기초 데이터를 수집하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 딥러닝 학습 도구 로그 수집부는, Jupyter Notebook 딥러닝 학습 도구의 Kernel과 연계하여 로그 생성 및 추출 기능을 수행하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,발전 데이터 저장 시스템으로부터 메타 데이터 정보를 추출하고, 각각의 메타 데이터에 대한 요약 및 통계를 생성하는 발전 데이터 인터페이스를 더 포함하고,상기 딥러닝 학습 도구는,발전소 관리를 위한 각 파라미터를 예측하기 위한 어플리케이션을 지원하는 클라우드 기반 딥러닝 환경에서의GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2023-0068918-3-제4항에 있어서,상기 발전 데이터 인터페이스는,발전소 운전 시스템에서 수집한 센서 데이터와 메타 정보, 속성 정보 및 표준 명칭정보를 제공하되, 초 단위로생성되는 센서 데이터의 평균, 표준편차 통계치와 누락값 비율을 계산하여 제공하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 메모리 기반 GPU 자원 할당부는,상기 프로세서에 기본 단위의 GPU 자원 스케쥴링을 수행하는 디폴트 자원 스케쥴러; 및프로세서별 연산량을 기반으로 GPU 자원을 동적으로 스케쥴링하는 스케일링 자원 스케쥴러를 포함하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 GPU 유휴자원 회수부는, 프로세서에 할당된 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하고, 종료시점 이후에도 사용 중인 커널을 강제적으로 종료하고, 유휴 GPU 자원을 공유 클러스터로 회수하는 클라우드 기반 딥러닝환경에서의 GPU 자원 관리 최적화 장치."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "클러스터내 사용자 또는 상기 사용자의 딥러닝 학습 도구에 스케일링된 자원을 포함한 최적의 자원을 할당하는자원 스케쥴링을 수행하는 단계;스케일링된 자원을 상기 사용자 또는 상기 딥러닝 학습 도구의 실행환경에 실시간으로 반영하는 단계; 및상기 실행환경의 실 사용량에 기반하여 스케일링된 자원을 회수하는 단계를 포함하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 자원 스케쥴링을 수행하는 단계는,상기 딥러닝 학습 도구의 각 프로세서에 디폴트 자원을 할당하는 단계;상기 각 프로세서에 대한 연산량 파라미터 정보를 수집하는 단계;상기 연산량 파라미터 정보로부터 상기 딥러닝 학습 도구의 프로세서별 연산량을 산출하는 단계;조기 종료 가중치에 따라 산출된 프로세서별 연산량을 보정하는 단계;각 프로세서에 대한 최적 GPU 자원 할당량을 계산하는 단계; 및각 프로세서에 대한 최적 GPU 자원 할당량이 각 프로세서에 할당된 디폴트 자원 할당량 보다 많은지 확인하는단계공개특허 10-2023-0068918-4-를 포함하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 딥러닝 학습 도구의 알고리즘은 MLP(Multi Laer Perceptron), CNN(Convolution Neural Network),RNN(Recurrent Nueral network) 중 하나 이상이며,상기 연산량 파라미터 정보를 수집하는 단계에서는, 해당 알고리즘을 GPU 기반으로 연산을 실행하는 시점에 하이퍼 파라미터 데이터로서, 훈련(train) 데이터 크기,딥러닝 학습 layer 수, 학습 반복 횟수(epochs)와 반복연산의 시점을 종료하는 조기종료 콜백함수(early_stopping) 사용 여부 중 적어도 2개 이상을 수집하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 프로세서별 연산량을 산출하는 단계에서는,callback 함수 실행에 따른 조기종료 평균 가중치를 반영한 연산량을 산출하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서별 연산량을 보정하는 단계에서는,pre_trained 나이브 베이즈 모델로 callback 함수 사용시 Total 연산 조기종료에 따른 Total 연산량을 확률적으로 예측하고, 예측치와 계산된 Total 연산량 데이터의 차이를 비교하여 callback 함수 실행에 따른 조기종료 평균 가중치를 대체하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 스케일링된 자원을 실시간으로 반영하는 단계에서는,하기 수학식에 따라 자원 스케쥴링을 수행하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법.(TWD : 전체 프로세서의 총 작업시간(Total Working Duration),CPi = 현재 실행중인 i번째 프로세서의 연산량 (Current Processor),CMi = i번째 프로세서에 할당된 GPU 메모리 용량(Current allocated GPU Memory),NPj = 신규로 인입된 j번째 프로세서의 연산량 (New Processor),NMj = j번째 프로세서에 할당 예정인 GPU 메모리 용량(New allocating GPU Memory),공개특허 10-2023-0068918-5-θ = 현재 실행중인 프로세서와 새로 할당 예정인 메모리의 weight 매개변수)"}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 스케일링된 자원을 실시간으로 반영하는 단계는,스케일링 GPU 스케쥴러에서 자원이 더 필요한 프로세서에 추가적으로 자원을 할당하는 방식으로 수행되는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제8항에 있어서,상기 스케일링된 자원을 회수하는 단계는,상기 각 프로세서의 예상 작업시간 경과 여부를 모니터링하는 단계; 예상 작업시간이 경과한 각 프로세서의 GPU 사용량이 기준값 보다 많으면, 작업시간을 재산정하는 단계; 상기 각 프로세스의 GPU 사용량이 기준값 보다 적으면, 작업시간이 경과한 것으로 판정하여 해당 프로세서의 커널을 강제 종료시키는 단계; 및 종료된 커널에서 GPU 유휴 자원을 회수하여 GPU 자원 관리 모듈에 반환하는 단계를 포함하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치는, 딥러닝 학습 도구의 GPU 자원 소요량 계산을 위한 기초 데이터를 수집하는 딥러닝 학습 도구 로그 수집부; 상기 기초 데이터를 활용하여 상기 딥러닝 학습 도구를 구성하는 프로세서의 단위별 행렬(Matrix) 연산량을 실시간으로 계산하는 딥러닝 학습 연산량 계산 부; 상기 딥러닝 학습 도구에 대하여 GPU 클러스터의 GPU 자원 스케쥴링을 수행하되, 디폴트 자원 스케쥴링과 스 케일링 자원 스케쥴링으로 구분하여 수행하는 메모리 기반 GPU 자원 할당부; 및 상기 딥러닝 학습 도구에 최종적 으로 할당한 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하여 자원을 회수하는 GPU 유 휴자원 회수부를 포함할 수 있다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 인공지능 개발 지원 플랫폼 등과 같은 동일 클러스터 환경에서 GPU 자원을 효율적으로 스케쥴 링 할 수 있는 클라우드 기반 딥러닝 학습 환경에서의 GPU 자원 관리 최적화 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(Aritificial Intelligence)은 인간의 학습 능력, 추론 능력, 지각 능력 등을 인공적으로 구현한 컴퓨 터 프로그램이다. 오랫동안 침체기에 있던 인공지능 기술은 클라우드 환경의 급속한 발전과 빅데이터가 뒷받침 되어 급속한 발전을 이루면서 4차 산업혁명의 핵심기술로 부상했다. 인공지능 기술의 한 분야인 딥러닝(Deep Learining)은 특징 학습(Feature Learning) 또는 표현 학습 (Representation)을 위한 기술이며, 연속된 층(Layer)을 활용하여 데이터로부터 특징을 추출하는데 강점이 있다. 딥러닝 학습을 위해서는 대량의 행렬(Matrix) 연산을 고속으로 처리하는 기술이 필요한데, 다양한 연산 방식을 지원하는 CPU와 비교하여 GPU는 부동소수점 연산에 특화된 아키텍쳐로 딥러닝 학습 시간을 10~100배 단 축시킬 수 있다. 한국전력공사는 전력분야 인공지능 기술 개발을 위해 클라우드 기반의 인공지능 개발 플랫폼을 구축하였다. 현 재는 지능형 어플리케이션 개발을 위한 딥러닝 학습 환경이 사용자(=분석가)마다 독립적인 가상머신(Virtual Machine) 환경으로 구성되어 있고, 딥러닝 학습을 위한 고속 GPU 자원은 클러스터 환경에서 공유되지 못하는 기 술적 제한이 있다. 다수의 GPU 자원을 클라우드 환경에서 동일 클러스터로 구성한다면 여러명의 사용자가 딥러닝 학습을 할 때 전 체 클러스터 내 GPU 자원을 공유하여 활용할 수 있는 이점이 있다. 하지만 딥러닝 학습의 특성상 동일 클러스터 내 모든 자원을 동적으로 할당할 경우 단일 사용자가 클러스터 전체의 자원을 독점하는 문제가 발생 가능하다. (이미지 학습의 경우 단일 노드에서 수십일 동안의 GPU 자원을 단독으로 소모하기도 한다.) 그렇기 때문에 동일클러스터 환경에서 GPU 자원을 효율적으로 스케쥴링 할 수 있는 방법이 필요하다. GPU 자원 관리는 실제 연산을 수행하는 GPU core 관리와 연산에 필요한 데이터를 적재하는 메모리 관리가 해당 된다. 딥러닝 학습을 위해서는 대량의 연산 데이터를 GPU에 로드하여야 하기 때문에, 연산에 사용되는 GPU 메모 리 가용성 관리가 매우 중요하다. CPU 기반의 딥러닝 학습 환경은 CPU 연산 속도에 비하여 고용량의 메모리 환 경(128~512GB)이 제공되는 반면에 GPU 기반의 딥러닝 학습환경에서는 다수의 GPU core를 기반으로 고속 연산 성 능이 발휘되었지만 연산 데이터 적재용 메모리 환경은 11~32GB 정도로 저용량 환경을 제공하고 있어 대용량의 데이터를 반복적으로 학습하는 딥러닝 연산에 매우 큰 영향을 끼친다. 종래의 기술은 CPU + GPU 서버로 구성된 클라우드 클러스터 환경에서 딥러닝 학습에 필요한 GPU의 자원 관리를 위해 CPU 사용량(Metric) 기반의 Auto-scaling 기술을 제공한다. Auto-Scaling 기술은 전체 자원을 효율적으로 활용하기 위한 핵심기술로 실제 사용량이 많은 프로세서에 더 많은 자원을 할당하는 기술이다. 하지만, 이는 CPU 사용량에 기반하고 있어 실제 딥러닝 학습용 GPU 자원의 효율적 활용을 위해서는 적용하기 어 렵고 GPU 메모리 사용량 기반의 scaling 최적화 기술이 필요하다. 또한, 딥러닝 학습 연산이 종료되었을 경우 GPU 메모리 자원을 회수하여야 하는데, 실제 학습에 사용된 데이터 가 GPU 메모리에 계속 상주하고 있어 유휴 자원으로 판정하기가 어렵다. 종료된 프로세서에서 유휴 메모리 자원 회수를 위해서는 사용자가 분석 Kernel을 종료하여 메모리에 있는 데이터를 소거해야만 클러스터가 GPU 유휴 자 원으로 인식할 수 있는 문제가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개공보 10-2019-0081306호 : 빅데이터 분석 소프트웨어에 대한 자원 할당 방법"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 동일 클러스터 환경에서 GPU 자원을 효율적으로 스케쥴링할 수 있는 클라우드 기반 딥러닝 학습 환경 에서의 메모리 기반의 GPU 자원 관리 최적화 방법을 제공하고자 한다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 측면에 따른 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치는, 딥러닝 학습 도구의 GPU 자원 소요량 계산을 위한 기초 데이터를 수집하는 딥러닝 학습 도구 로그 수집부; 상기 기초 데이터를 활용 하여 상기 딥러닝 학습 도구를 구성하는 프로세서의 단위별 행렬(Matrix) 연산량을 실시간으로 계산하는 딥러닝 학습 연산량 계산부; 상기 딥러닝 학습 도구에 대하여 GPU 클러스터의 GPU 자원 스케쥴링을 수행하되, 디폴트 자원 스케쥴링과 스케일링 자원 스케쥴링으로 구분하여 수행하는 메모리 기반 GPU 자원 할당부; 및 상기 딥러닝 학습 도구에 최종적으로 할당한 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하여 자원 을 회수하는 GPU 유휴자원 회수부를 포함할 수 있다. 여기서, 상기 딥러닝 학습 도구 로그 수집부는, 상기 딥러닝 학습 도구의 GPU 자원 소요량 계산을 위해 상기 딥 러닝 학습 도구의 사용자 입력 Command에 기반하여 행렬 덧셈과 곱셉 연산량 계산용 기초 데이터를 수집할 수 있다. 여기서, 상기 딥러닝 학습 도구 로그 수집부는, Jupyter Notebook 딥러닝 학습 도구의 Kernel과 연계하여 로그 생성 및 추출 기능을 수행할 수 있다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "여기서, 발전 데이터 저장 시스템으로부터 메타 데이터 정보를 추출하고, 각각의 메타 데이터에 대한 요약 및 통계를 생성하는 발전 데이터 인터페이스를 더 포함하고, 상기 딥러닝 학습 도구는, 발전소 관리를 위한 각 파라미터를 예측하기 위한 어플리케이션을 지원할 수 있다. 여기서, 상기 발전 데이터 인터페이스는, 발전소 운전 시스템에서 수집한 센서 데이터와 메타 정보, 속성 정보 및 표준 명칭정보를 제공하되, 초 단위로 생성되는 센서 데이터의 평균, 표준편차 통계치와 누락값 비율을 계산 하여 제공할 수 있다. 여기서, 상기 메모리 기반 GPU 자원 할당부는, 상기 프로세서에 기본 단위의 GPU 자원 스케쥴링을 수행하는 디 폴트 자원 스케쥴러; 및 프로세서별 연산량을 기반으로 GPU 자원을 동적으로 스케쥴링하는 스케일링 자원 스케 쥴러를 포함할 수 있다. 여기서, 상기 GPU 유휴자원 회수부는, 프로세서에 할당된 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하고, 종료시점 이후에도 사용 중인 커널을 강제적으로 종료하고, 유휴 GPU 자원을 공유 클러 스터로 회수할 수 있다. 본 발명의 다른 측면에 따른 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법은, 클러스터내 사용 자 또는 상기 사용자의 딥러닝 학습 도구에 스케일링된 자원을 포함한 최적의 자원을 할당하는 자원 스케쥴링을 수행하는 단계; 스케일링된 자원을 상기 사용자 또는 상기 딥러닝 학습 도구의 실행환경에 실시간으로 반영하는 단계; 및 상기 실행환경의 실 사용량에 기반하여 스케일링된 자원을 회수하는 단계를 포함할 수 있다. 여기서, 상기 자원 스케쥴링을 수행하는 단계는, 상기 딥러닝 학습 도구의 각 프로세서에 디폴트 자원을 할당하 는 단계; 상기 각 프로세서에 대한 연산량 파라미터 정보를 수집하는 단계; 상기 연산량 파라미터 정보로부터 상기 딥러닝 학습 도구의 프로세서별 연산량을 산출하는 단계; 조기 종료 가중치에 따라 산출된 프로세서별 연 산량을 보정하는 단계; 각 프로세서에 대한 최적 GPU 자원 할당량을 계산하는 단계; 및 각 프로세서에 대한 최 적 GPU 자원 할당량이 각 프로세서에 할당된 디폴트 자원 할당량 보다 많은지 확인하는 단계를 포함할 수 있다. 여기서, 상기 딥러닝 학습 도구의 알고리즘은 MLP(Multi Laer Perceptron), CNN(Convolution Neural Network), RNN(Recurrent Nueral network) 중 하나 이상이며, 상기 연산량 파라미터 정보를 수집하는 단계에서는, 해당 알고리즘을 GPU 기반으로 연산을 실행하는 시점에 하 이퍼 파라미터 데이터로서, 훈련(train) 데이터 크기, 딥러닝 학습 layer 수, 학습 반복 횟수(epochs)와 반복연 산의 시점을 종료하는 조기종료 콜백함수(early_stopping) 사용 여부 중 적어도 2개 이상을 수집할 수 있다. 여기서, 상기 프로세서별 연산량을 산출하는 단계에서는, callback 함수 실행에 따른 조기종료 평균 가중치를 반영한 연산량을 산출할 수 있다. 여기서, 상기 프로세서별 연산량을 보정하는 단계에서는, pre_trained 나이브 베이즈 모델로 callback 함수 사 용시 Total 연산 조기종료에 따른 Total 연산량을 확률적으로 예측하고, 예측치와 계산된 Total 연산량 데이터 의 차이를 비교하여 callback 함수 실행에 따른 조기종료 평균 가중치를 대체할 수 있다. 여기서, 상기 스케일링된 자원을 실시간으로 반영하는 단계에서는, 하기 수학식에 따라 자원 스케쥴링을 수행할 수 있다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "(TWD : 전체 프로세서의 총 작업시간(Total Working Duration), CPi = 현재 실행중인 i번째 프로세서의 연산량 (Current Processor), CMi = i번째 프로세서에 할당된 GPU 메모리 용량(Current allocated GPU Memory), NPj = 신규로 인입된 j번째 프로세서의 연산량 (New Processor), NMj = j번째 프로세서에 할당 예정인 GPU 메모리 용량(New allocating GPU Memory), θ = 현재 실행중인 프로세서와 새로 할당 예정인 메모리의 weight 매개변수) 여기서, 상기 스케일링된 자원을 실시간으로 반영하는 단계는, 스케일링 GPU 스케쥴러에서 자원이 더 필요한 프 로세서에 추가적으로 자원을 할당하는 방식으로 수행될 수 있다. 여기서, 상기 스케일링된 자원을 회수하는 단계는, 상기 각 프로세서의 예상 작업시간 경과 여부를 모니터링하 는 단계; 예상 작업시간이 경과한 각 프로세서의 GPU 사용량이 기준값 보다 많으면, 작업시간을 재산정하는 단 계; 상기 각 프로세스의 GPU 사용량이 기준값 보다 적으면, 작업시간이 경과한 것으로 판정하여 해당 프로세서 의 커널을 강제 종료시키는 단계; 및 종료된 커널에서 GPU 유휴 자원을 회수하여 GPU 자원 관리 모듈에 반환하 는 단계를 포함할 수 있다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 사상에 따른 클라우드 기반 딥러닝 학습 환경에서의 메모리 기반의 GPU 자원 관리 최적화 방법을 실 시하면, 동일 클러스터 환경에서 GPU 자원을 효율적으로 스케쥴링할 수 있는 이점이 있다. 본 발명의 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 방법은, 클라우드 환경에서 GPU 자원 최적화 기술을 사용하면 다수의 사용자가 공동으로 컴퓨팅 연산 자원을 효율적으로 사용할 수 있어, 데이터센터 유지비용을 절감하는 이점이 있다. 즉, 다수의 사용자가 공동으로 GPU 자원을 사용함으로써 유휴 자원을 최소화 함으로써 클라우드 자원 활용을 극대화 할 수 있다. 본 발명의 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 방법은, 각각의 사용자가(=분석가) 단 독으로 GPU 자원을 활용하여 리소스 관리에 낭비가 발생하는 것을 방지하는 이점이 있다. 본 발명의 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 방법은, 메모리 기반 GPU 스케쥴러를 활용하여 추가 자원이 필요한 사용자에게 효과적으로 자원을 배분하고 딥러닝 학습 모델링 시간을 단축하여 최 적화하는 이점이 있다. 이는 실제 인공지능 어플리케이션 개발 기간을 30% 이상 단축하는 효과가 있다. 본 발명의 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 방법은, GPU 자원 최적화 활용에 따라 인공지능 개발 Cost를 최적화하여 운영비용을 낮추고 효율적으로 인공지능을 개발할 수 있는 이점이 있다. 예컨 대, 본 발명을 통해 GPU 자원을 공유 활용한다면 20억원/년 규모의 GPU 서버 구입 비용을 절감할 수 있다."}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명을 설명함에 있어서 제 1, 제 2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요 소들은 용어들에 의해 한정되지 않을 수 있다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적 으로만 된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제 1 구성요소는 제 2 구성요소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 연결되어 있다거나 접속되어 있다고 언급되는 경우는, 그 다른 구성요소에 직 접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해 될 수 있다. 본 명세서에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 본 명세서에서, 포함하다 또는 구비하다 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것으로서, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해될 수 있다. 또한, 도면에서의 요소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 도 1은 본 발명의 사상에 따른 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 장치를 구비한 컨 테이너 기반 클라우드에서 딥러닝 학습 플랫폼 시스템을 도시한 블록도이다. 본 발명에서는 컨테이너 기반 클라우드에서 딥러닝 학습을 위한 GPU 자원 관리 최적화 기술을 제안한다. 발명에 서 제안하는 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 장치의 기술적 구성요소들로서, 도 1과 같 이 딥러닝 학습 도구 로그 수집부, 딥러닝 학습 연산량 계산부, 메모리 기반 GPU 자원 할당부, GPU 유휴자원 회수부로 구분될 수 있다. 즉, 도시한 본 발명의 사상에 따른 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 장치는, 사용 자의 GPU 자원 소요량 계산을 위해 사용자 입력 Command에 기반하여 행렬 덧셈과 곱셉 연산량 계산용 기초 데이 터를 수집하는 딥러닝 학습 도구 로그 수집부; 딥러닝 학습 도구 로그 수집부에서 수집한 기초 데이터를 활용하여 사용자가 실행한 딥러닝 학습 프로세서 단위별 행렬(Matrix) 연산량을 실시간으로 계산하는 딥러닝 학 습 연산량 계산부; GPU 클러스터 내 사용자에게 GPU 자원 스케쥴링을 수행하되, GPU 자원 스케쥴러는 Default 자원 스케쥴러와 Scaling 자원 스케쥴러로 구성되는 메모리 기반 GPU 자원 할당부; 및 사용자에게 최종적으로 할당한 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하여 자원을 회수하는 GPU 유휴자원 회수부를 포함할 수 있다. 예컨대, 상기 딥러닝 학습 도구 로그 수집부는, Jupyter Notebook과 같은 딥러닝 학습 도구의 Kernel과 연 계하여 로그 생성 및 추출 기능을 수행할 수 있다. 예컨대, 상기 메모리 기반 GPU 자원 할당부가 GPU 자원 스케쥴링을 수행함에 있어서 이용되는 GPU 자원 스 케쥴러는, 디폴트(Default) 자원 스케쥴러와 스케일링(Scaling) 자원 스케쥴러로 구성될 수 있다. 여기서, 디폴 트(Default) 자원 스케쥴러는 클러스터내 사용자에게 기본 단위로 자원 스케쥴링을 수행하며, 스케일링 (Scaling) 자원 스케쥴러는 사용자 프로세서별 연산량을 기반으로 GPU 자원을 동적으로 스케쥴링 한다. 구현에 따라, 상기 디폴트 자원 스케쥴러는, 클러스터내 사용자별로 고정된 기본 단위의 GPU 자원을 할당하거나, 딥러닝 학습 도구별로(즉, 어플리케이션 별로) 고정된 기본 단위의 GPU 자원을 할당할 수 있다. 예컨대, GPU 유휴자원 회수부는 사용자에게 최종적으로 할당한 GPU 자원량과 프로세서 연산량을 활용하여 프로세서 종료 시점을 계산하고, 종료시점 이후에도 사용중인 사용자 연산 프로세서를 강제적으로 종료하고, 유 휴 GPU 자원을 공유 클러스터로 회수할 수 있다. 도시한 발전 데이터 인터페이스는 발전 데이터 카달로그는 발전 데이터 저장 시스템으로부터 메타 데이터"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "정보를 추출하고, 각각의 메타 데이터에 대한 요약 및 통계를 생성한다. 발전 데이터 고속 검색은 이기종 데이 터베이스의 통합 인덱싱 및 검색 질의 답변을 수행한다. 발전 데이터 병렬 쿼리 장치는 분산 저장된 발전 저장 시스템에 병렬로 접근하여 대용량 데이터를 로드한다. 발전 데이터 대용량 다운로더는 스케쥴러 기반 데이터를 순차 분할하여 임시파일 저장소에 저장한다. 발전 데이터 연계 모니터링은 발전 데이터로부터 수집하는 데이터 의 레코드 건수를 기반으로 통계치 생성 및 전일 대비 증감을 비교한다. 도시한 발전 인공지능 어플리케이션은 딥러닝 학습 도구와 연동하여 발전소 운전 데이터를 분석하고 예측 을 수행하는 인공지능 시스템을 예시한 것이다. 발전소 증기온도 등 정형 데이터와 열화상 이미지 데이터 등 비 정형 데이터를 수집하여 특징분석, 머신러닝 및 딥러닝 모델링을 통해 발전설비 효율을 최적화하고 고장을 사전 에 예지한다. 도시한 본 발명의 사상에 따른 GPU 자원 관리 최적화 장치는 컨테이너 기반의 클라우드 환경의 플랫폼에서 동작 하는 사용자 딥러닝 분석 어플리케이션에 GPU 자원을 최적으로 할당하고 관리하는 것을 목표로 한다. 사용자 딥 러닝 분석 어플리케이션은 파이썬, R 등의 언어를 사용하는 딥러닝 프레임워크를 말하며 tensorflow, pytorch가 대표적이다. 딥러닝 학습을 위해서는 먼저 사용자가 딥러닝 분석 어플리케이션(application)을 실행하여 자원을 요청하면 메 모리 기반 GPU 자원 스케쥴러가 사용자에게 GPU 자원을 디폴트(default)로 할당한다. 사용자가 파이썬, R 등의 언어를 기반으로 딥러닝 학습 프로세스를 실행하면, 딥러닝 학습 도구 로그 수집부에서 입력된 코드 데이터와 실행환경에서 발생하는 연산량 데이터를 수집한다. 이후 딥러닝 학습 연산량 계산부에서 수집한 연산 데이터를 기반으로 프로세서별 연산량을 추정한 후에 이 를 메모리 기반 GPU 할당부에 전달하면 스케일링(scaling) GPU 스케쥴러가 최적의 자원 배분량을 결정하여 사용자 어플리케이션에 할당된 자원의 양을 수평적으로 스케일링한다. 스케일링은 사용자 어플리케이션에 할당 된 컨테이너의 수평적 확장을 통해서 이루어지며 사용자 실행환경에 실시간으로 반영된다. GPU 유휴자원 회수부 에서 계산한 딥러닝 학습 프로세서 예상 작업시간이 경과하면 딥러닝 학습 프로세서가 실행되는 커널을 강 제 종료시키고 유휴 상태로 변한 GPU 자원을 회수하여 Worker Node에 반환한다. 도 2는 본 발명의 사상에 따른 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법(프로세스)을 도시 한 흐름도이다. 도시한 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법은, 클러스터내 사용자 또는 상기 사용자의 딥러닝 학습 도구에 스케일링된 자원을 포함한 최적의 자원을 할당하는 자원 스케쥴링을 수행하는 단계(S100); 스케일링된 자원을 상기 사용자 또는 상기 딥러닝 학습 도구의 실행환경 에 실시간으로 반영하는 단계(S200); 및 상기 실행환경의 실 사용량에 기반하여 스케일링된 자원을 회수하는 단 계(S300)를 포함할 수 있다. 도시한 바와 같이, 상기 자원 스케쥴링을 수행하는 단계(S100)는, 상기 딥러닝 학습 도구의 각 프로세서에 디폴 트 자원을 할당하는 단계(S110); 상기 각 프로세서에 대한 연산량 파라미터 정보를 수집하는 단계(S120); 상기 연산량 파라미터 정보로부터 상기 딥러닝 학습 도구의 프로세서별 연산량을 산출하는 단계(S130); 조기 종료 가 중치에 따라 산출된 프로세서별 연산량을 보정하는 단계(S140); 각 프로세서에 대한 최적 GPU 자원 할당량을 계 산하는 단계(S150); 및 각 프로세서에 대한 최적 GPU 자원 할당량이 각 프로세서에 할당된 디폴트 자원 할당량 보다 많은지 확인하는 단계(S160)를 포함할 수 있다. 예컨대, 상기 스케일링된 자원을 실시간으로 반영하는 단계(S200)는, 스케일링 GPU 스케쥴러에서 자원이 더 필 요한 프로세서에 추가적으로 자원을 할당하는 방식으로 수행될 수 있다. 도시한 바와 같이, 상기 스케일링된 자원을 회수하는 단계(S300)는, 상기 각 프로세서의 예상 작업시간 경과 여 부를 모니터링하는 단계(S310); 예상 작업시간이 경과한 각 프로세서의 GPU 사용량이 기준값 보다 많으면, 작업 시간을 재산정하는 단계(S330); 상기 각 프로세서의 GPU 사용량이 기준값 보다 적으면, 작업시간이 경과한 것으 로 판정하여 해당 프로세서의 커널을 강제 종료시키는 단계(S340); 및 종료된 커널에서 GPU 유휴 자원을 회수하 여 GPU 자원 관리 모듈에 반환하는 단계(S360)를 포함할 수 있다. 다음, 도 2의 흐름도에 따른 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법을 수행하는 주체로서, 도 1의 GPU 자원 관리 최적화 장치의 각 구성요소들의 동작에 대하여, 발전 분야로 구체적으로 예시 하여 설명하겠다. 도시한 발전 데이터 인터페이스의 발전 데이터 카달로그는 발전소 운전 시스템에서 수집한 센서 데이터와 발전 자산 데이터를 분석가들이 쉽게 이해할 수 있도록 메타 정보, 속성 정보 및 표준 명칭정보를 제공한다. 초 단위로 생성되는 센서 데이터의 평균, 표준편차 통계치와 누락값 비율을 계산한다. 발전 데이터 고속 검색은 관계형 데이터베이스, 시계열 데이터베이스 등 이기종 데이터베이스간의 데이터를 통 합 인덱싱하며, 인덱싱 구조는 디렉터리 구조를 기반으로 루트 디렉토리는 설비 마스터 정보가 위치하고 2- depth 디렉토리에는 고장, 유지보수 등 운영정보, 3-depth 디렉토리에는 센서 취득정보를 연결하여 설비 단위의 고속 검색 기능을 수행한다. 발전 데이터 병렬 쿼리 장치는 분산병렬처리저장 시스템과 연동하여 사용자 단일 쿼리에 대해 분산 저장된 발전 운전 데이터에 접근하기 위한 병렬 쿼리를 수행한다. 분산 수집 브로커는 매개변수를 기반으로 쿼리와 관련된 세그먼트를 식별하여 각 세그먼트를 병렬로 처리하고 분산된 데이터 서버로부터 부분적인 결과를 받아 최종 결 과를 병합하여 호출자에게 반환한다. 발전 데이터 대용량 다운로더는 사용자가 요청한 데이터의 용량에 비례하여 데이터를 분할하고 스케쥴러를 이용 하여 데이터를 순차적으로 추출한다. 작업 예상 소요시간을 계산하여 사용자에게 표시하고 작업이 완료되면 데이터를 병합하여 임시 파일 저장소에 저장한다. 사용자가 파일 다운로드를 완료하면 임시 파일 저장소의 데이터 를 삭제한다. 발전 데이터 연계 모니터링은 발전 센서 데이터의 일별 수집량을 기반으로 통계치를 생성하고 일일 수집량 데이 터 로그를 적재한다. 평균 통계치 대비 수집량 및 건수에 변동이 발생하면 알람 로그 및 사용자 통지를 생성한 다. 다음, 딥러닝 학습 도구 로그 수집부는, 클러스터 내 전체 사용자의 딥러닝 학습 도구 커널(Kernel)로부터 GPU 자원 사용량 계산에 필요한 딥러닝 학습 모델 하이퍼 파라미터 데이터를 수집한다(S120). 그리고 딥러닝 학 습 모델 프로세서가 실제 사용한 자원량과 학습에 걸린 시간 데이터를 클러스터 관리 도구로부터 수집한다. 수 집한 데이터는 로그 파일로 저장한다. 딥러닝 학습에 사용되는 알고리즘은 MLP(Multi Laer Perceptron), CNN(Convolution Neural Network), RNN(Recurrent Nueral network) 등이며 해당 알고리즘을 GPU 기반으로 연산을 실행하는 시점에 하이퍼 파라미터 데이터를 수집한다. 수집 대상 하이퍼 파라미터는 훈련(train) 데이터 크기, 딥러닝 학습 layer 수, 학습 반복 횟수(epochs)와 반복연산의 시점을 종료하는 조기종료 콜백함수(early_stopping) 사용 여부이다. 딥러닝 학습 실제 연산량과 학습에 걸린 시간 데이터는 클러스터 관리 도구의 로그 데이터를 기반으로 생성한다. 예컨대, 딥러닝 학습 알고리즘의 학습 시작 명령어(fit)가 입력된 시점에서 딥러닝 학습 프로세서가 종료된 시점(GPU Core 사용량 < Threshold)까지 사용된 GPU 자원량 데이터(GPU Core 사용량, GPU Memory)와 작 업 시간 데이터를 수집하여 로그 파일로 저장한다. 다음, 딥러닝 학습 연산량 계산부는, 사용자로부터 수집한 딥러닝 학습 모델 파라미터를 기반으로 프로세 스 단위로 연산량을 계산한다(S130). 예컨대, 연산량 계산은 하기 수학식 1로 전개될 수 있다. 수학식 1"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "Train_data_size : 훈련 데이터의 행렬(matrix) 크기 Numbor of Layers : 훈련 데이터 행렬 곱 실행횟수 Epochs : 훈련 데이터 행렬 곱 반복횟수 Weight of Callback : Callback 함수 실행에 따른 조기종료 평균 가중치 상기 수학식 1에 따라 계산된 연산량 데이터는 딥러닝 학습 도구 로그 수집부로부터 수집한 최종 연산결과 데이터를 사용하여 보정한다(S140). Callback 함수 실행시 분석 프로세서의 조기종료에 따른 영향도를 보정하기 위하여 pre-trained 나이브 베이즈 모델을 사용한다. pre_trained 나이브 베이즈 모델은 callback 함수 사용시 Total 연산 조기종료에 따른 Total 연산량을 확률적으 로 예측한다. 예측치와 계산된 Total 연산량 데이터의 차이를 비교하여 Weight of Callback 값을 대체한다. pre-trained 나이브 베이즈 모델은 딥러닝 학습 로그 수집부에서 수집한 실제 연산량 이력 데이터를 활용 하고, 사용자가 입력한 하이퍼 파라미터 데이터(입력 변수, x)와 실제 학습에 소요된 연산량 데이터(출력 변 수)를 사용하여 제작한다. pre-trained 나이브 베이즈 모델에 따른 확률 예측식은 하기 수학식 2와 같다. 수학식 2"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": ": 입력변수 x에 따른 Weight of Callback 사후확률 : Weight of Callback 값에 대한 입력변수 x 확률 × Weight of Callback 의 사전확률 의 수열의 곱 P(X) : 전체 Weight of Callback 확률 다음, 메모리 기반 GPU 자원 할당부는 디폴트(Default) 스케쥴러와 스케일링(Scaling) 스케쥴러로 구성된 다. 디폴트 스케쥴러는 일반적인 컨테이너 스케쥴러 기능을 제공한다. 사용자가 딥러닝 학습 도구를 실행시키면 디폴트 스케쥴러가 동작하여 사용자에게 컨테이너 기반으로 GPU 자원을 할당한다(S110). 컨테이너는 GPU 서버 클러스터 내 노드 중 하나에 생성된다. 컨테이너를 가장 적합한 노드에 생성하는 고급 컨테이너 스케쥴링 기능 을 제공한다. 스케일링 스케쥴러는 GPU 자원을 동적으로 할당하는 기능을 제공한다(S200). 딥러닝 학습 연산량 계산부에서 계 산한 프로세서별 연산량 데이터를 사용하여 추가적인 자원 스케쥴링을 계획한다. 예컨대, 스케일링은 하기 수학 식 3에 따라 실행될 수 있다. 수학식 3"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "TWD : 전체 프로세서의 총 작업시간(Total Working Duration) CPi = 현재 실행중인 i번째 프로세서의 연산량 (Current Processor) CMi = i번째 Processor에 할당된 GPU 메모리 용량(Current allocated GPU Memory) NPj = 신규로 인입된 j번째 프로세서의 연산량 (New Processor) NMj = j번째 프로세서에 할당 예정인 GPU 메모리 용량(New allocating GPU Memory) θ = 현재 실행중인 프로세서와 새로 할당 예정인 메모리의 weight 매개변수 TWD는 현재 실행중인 각각의 프로세서의 연산량을 GPU 메모리 용량으로 나눈 합과 신규로 인입되는 프로세서의 GPU 메모리 용량의 전체 합에 각각의 프로세서에 대한 가중치 매개변수 θ를 곱하여 계산한다. 전체 프로세서의 총 작업시간(TWD)을 최적화 하기 위해 미분기를 사용하여 TWD′(θ)=0인 조건에서 θ 매개변수 를 구하여 각 프로세서에 추가로 할당할 GPU 메모리 용량을 계산한다. TWD를 최소화하기 θ값에 대한 최적해 연 산을 통해 추가적인 GPU 자원 스케쥴링을 실행한다. 최적해 연산을 통해 구한 GPU 할당량이 Default GPU 자원 할당량보다 작은 딥러닝 학습 프로세서에는 추가적인 GPU 자원 스케쥴링을 하지 않는다. scaling 스케줄러는 사용자로부터 신규 프로세서가 인입될 때마다 가중치를 재계산하여 Auto-Scaling 기능을 구 현한다. 추가 자원 할당에 필요한 GPU 자원량 소진시 기존 프로세서가 종료될 때까지 추가 스케줄링은중지한다. 각 프로세서에 할당할 수 있는 추가 할당 메모리 가중치(weight) 한계(threshold)를 설정하여 단독 사용자에게 GPU 자원이 독점되는 것을 막고, 전체 클러스터의 GPU 자원이 조기 소진되는 것을 방지한다. 다음, 상기 GPU 유휴자원 회수부는 사용자의 프로세서가 추정작업시간을 경과(S310)하여 GPU core는 사용 되지 않는데(s320), 일정시간 동안 GPU 메모리 사용이 계속될 경우 이를 자동으로 종료(Termination)해서(S340) GPU 메모리를 유휴 자원으로 회수하는 역할을 한다(S360). 상술한 자동 종료(Auto Termination)를 위해 하기 수 학식 4를 적용할 수 있다. 수학식 4"}
{"patent_id": "10-2021-0155221", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "WDi : i번째 Processor의 총 작업시간 : 추정 작업시간(실행중인 i번째 Processor의 연산량* 기본+추가 할당된 GPU 메모리 용량-1) DT : 지연 시간(Delay Time) 총 작업 시간은 현재 실행중인 딥러닝 학습 프로세서의 추정작업 시간에 DT(지연 시간)을 더하여 총 작업시간을 구한다. 지연 시간은 추정 작업시간에 비례하여 설정한다. 예상 작업시간이 종료시 딥러닝 학습 프로세서의 GPU core 사용량을 측정하여(S320) Threshold값 미만시 해당 프로세서 커널을 강제로 종료한다(S340). Threshold는 유휴중인 딥러닝 학습도구의 평균 GPU core 사용량으로 설정할 수 있다. GPU core 사용량이 Threshold값 이상일 경우 GPU 자원이 아직 사용중이라 판단하여 총 작업시 간에 지연 시간을 추가적으로 적용하여(1*Delay Time) 재산정하고(S330), 프로세서가 재산정한 총 작업시간을 경과할 때까지 다시 모니터링 한다(S310). S360 단계에서 회수된 GPU 자원은 클러스터의 유휴 자원으로 편입되며, 메모리 기반 GPU 자원 할당부에서 새로 운 프로세서에 자원을 할당하는데 사용된다. 다음, 발전 인공지능 어플리케이션의 각 구현 예시를 설명한다. 발전플랜트 열성능 관리 어플리케이션은 발전소 운전 데이터를 기반으로 열, 물질 해석을 수행하여 연소열성능 상태를 평가하고 운전조건에서 정상 상태를 시뮬레이션하여 실제 상태와의 차이를 분석하여 설비에 대한 성능과 신뢰도를 계산한다. 화력발전 보일러 최적연소 어플리케이션은 발전소 연소불안정 현상을 개선하기 위해 소비연료 특성과 슬래그 발 생량을 분석하여 운전변수 최적화 계산을 수행한다. 발전소 디지털 트윈 어플리케이션은 공정 모델, 제어 모델, HMI로 구성된다. 공정 모델은 발전설비의 공정을 수 학적으로 모델링한다. 제어 모델은 발전소 제어시스템을 블록 단위로 모델링한다. HMI는 중앙제어실의 운전전 조작 환경을 모델링한다. 보일러 연소 진단 및 예측 어플리케이션은 보일러 노내 연소 특성을 분석하기 위해 발전소 출력과 석탄 발열량, 공기온도 등 데이터를 기반으로 연소 해석을 수행하여 보일러 연소 상태의 적정성을 판단하고 출력을 예측한다. 가스터빈 연소기 진단 어플리케이션은 가스터빈 연소기 내부에서 고압/고온의 공기와 연료가 혼합되고 연소되는 과정에서 발생하는 압력 변화를 분석하여 가스터빈 이상을 조기에 감지하고 연소 불안정 발생을 예방한다. 이상의 설명은 본 발명의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 발명이 속하는 기술 분야에 서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변형이 가 능할 것이다. 따라서, 본 발명에 개시된 실시예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 실시예에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2021-0155221", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 사상에 따른 클라우드 학습 환경에서 메모리 기반의 GPU 자원 관리 최적화 장치를 구비한 컨 테이너 기반 클라우드에서 딥러닝 학습 플랫폼 시스템을 도시한 블록도. 도 2는 본 발명의 사상에 따른 클라우드 기반 딥러닝 환경에서의 GPU 자원 관리 최적화 방법을 도시한 흐름도."}
