{"patent_id": "10-2023-0098893", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0017902", "출원번호": "10-2023-0098893", "발명의 명칭": "인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법", "출원인": "주식회사 아이닉스", "발명자": "윤치원"}}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능에 기반한 음성 인식 시스템에 있어서,제1 음성 신호를 처리하여, 인공지능 모델에 입력하기 위한 제1 데이터를 생성하는 음성 신호 처리부; 및상기 제1 데이터를 입력받아, 상기 인공지능 모델을 이용하여 분류하는 인공지능부;를 포함하는, 음성 인식 시스템."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 데이터는,상기 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하여 생성된 영상 데이터인, 음성 인식시스템."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 영상 데이터는, 상기 제2 음성 신호를 입력받아, STFT(Short-Time Fourier Transform)를 실시 후 그 절대값을 취하고, 상기 절대값을 dB 스케일로 변환한 스펙트로그램(Spectrogram)인, 음성 인식 시스템."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 인공지능부는, CNN(Convolutional Neural Networks) 모델을 이용하여 n개의 클래스로 분류하되, 상기 CNN 모델에서 사용하는 손실 함수는, 상기 n개의 클래스에 속하지 않는 데이터인 백그라운드 데이터를 사용하고,상기 n은 2 이상의 자연수인, 음성 인식 시스템."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 손실 함수는, 상기 제1 데이터가 상기 백그라운드 데이터에 해당하는 경우, 다음의 [수학식]으로 주어지는, 음성 인식시스템.(상기 [수학식]에서 LOSS는 상기 손실 함수, 는 클래스 i에 대한 예측값을 나타낸다.)"}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2025-0017902-3-제4항에 있어서,상기 음성 인식 시스템은,상기 인공지능부로부터 출력된 상기 n개의 클래스 분류값이 모두 제2 값 미만인 경우, 해당 클래스를 백그라운드 클래스로 분류하는 후처리부;를 더 포함하는, 음성 인식 시스템."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "인공지능에 기반한 음성 인식 방법에 있어서,제1 음성 신호를 처리하여, 인공지능 모델에 입력하기 위한 제1 데이터를 생성하는 음성 신호 처리 단계; 및상기 제1 데이터를 입력받아, 상기 인공지능 모델을 이용하여 분류하는 분류 단계;를 포함하는, 음성 인식방법."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제1 데이터는,상기 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하여 생성된 영상 데이터인, 음성 인식방법."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 영상 데이터는, 상기 제2 음성 신호를 입력받아, STFT(Short-Time Fourier Transform)를 실시 후 그 절대값을 취하고, 상기 절대값을 dB 스케일로 변환한 스펙트로그램(Spectrogram)인, 음성 인식 방법."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항 내지 제9항 중 어느 한 항에 있어서,상기 분류 단계에서는, CNN(Convolutional Neural Networks) 모델을 이용하여 n개의 클래스로 분류하되, 상기 CNN 모델에서 사용하는 손실 함수는, 상기 n개의 클래스에 속하지 않는 데이터인 백그라운드 데이터를 사용하고,상기 n은 2 이상의 자연수인, 음성 인식 방법."}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 손실 함수는, 상기 제1 데이터가 상기 백그라운드 데이터에 해당하는 경우, 다음의 [수학식]으로 주어지는, 음성 인식 방법.(상기 [수학식]에서 LOSS는 상기 손실 함수, 는 클래스 i에 대한 예측값을 나타낸다.)"}
{"patent_id": "10-2023-0098893", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,공개특허 10-2025-0017902-4-상기 음성 인식 방법은,상기 분류 단계로부터 출력된 상기 n개의 클래스 분류값이 모두 제2 값 미만인 경우, 해당 클래스를 백그라운드클래스로 분류하는 후처리 단계;를 더 포함하는, 음성 인식 방법."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능에 기반한 음성 인식 시스템은, 제1 음성 신호를 처리하여, 인공지능 모델에 입력하기 위한 제1 데이터 를 생성하는 음성 신호 처리부; 및 상기 제1 데이터를 입력받아, 상기 인공지능 모델을 이용하여 분류하는 인공 지능부;를 포함하고, 상기 인공지능부는, CNN(Convolutional Neural Networks) 모델을 이용하여 n개의 클래스로 분류하되, 상기 CNN 모델에서 사용하는 손실 함수는, 상기 n개의 클래스에 속하지 않는 데이터인 백그라운드 데 이터를 사용한다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예들은 인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법에 관한 것이다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "CNN(Convolutional Neural Networks) 모델 기반의 인공 신경망은 영상 분류, 객체 인식, 화질 개선 등 다양한 분야에서 시도되고 있다. 최근 엣지(Edge) 및 임베디드 시스템 향으로 키워드 스포팅(Keyword Spotting, KWS) 같은 간단한 응용 분야에 대한 관심도가 높아지고 있다. 기존의 음성 신호를 처리하기 위해 사용되던 RNN(Recurrent Neural Networks) 모델 방식이 아닌 CNN 모델 기반 의 추론 방식으로 KWS를 처리하는 기법들이 학계 및 산업계에서 활발히 연구되고 있다. 음성 인식 솔루션의 경우 상황 인지 또는 위급 상황 판단을 요구하는 어플리케이션에서 높은 수준의 정확도를 요구하고 있고, 네트워크 경량화와 함께 정확도를 높이기 위한 연구들이 필요하다. 미리 정의된 음성 핵심어를 인식 및 분류하여 높은 정확도와 효율성을 갖는 CNN 모델 기반의 KWS을 구현할 필요 가 있다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서 개시되는 실시예들은, 미리 정의된 음성 핵심어를 인식 및 분류하여 높은 정확도와 효율성을 갖 는 인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법을 제공하는 것에 그 목적이 있다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "인공지능에 기반한 음성 인식 시스템은, 제1 음성 신호를 처리하여, 인공지능 모델에 입력하기 위한 제1 데이터 를 생성하는 음성 신호 처리부; 및 상기 제1 데이터를 입력받아, 상기 인공지능 모델을 이용하여 분류하는 인공 지능부;를 포함한다. 상기 제1 데이터는, 상기 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하여 생성된 영상 데이터이다. 아울러, 상기 영상 데이터는, 상기 제2 음성 신호를 입력받아, STFT(Short-Time Fourier Transform)를 실시 후 그 절대값을 취하고, 상기 절대값을 dB 스케일로 변환한 스펙트로그램(Spectrogram)이다. 구체적으로, 상기 인공지능부는, CNN(Convolutional Neural Networks) 모델을 이용하여 n개의 클래스로 분류하 되, 상기 CNN 모델에서 사용하는 손실 함수는, 상기 n개의 클래스에 속하지 않는 데이터인 백그라운드 데이터를 사용하고, 상기 n은 2 이상의 자연수이다. 상기 손실 함수는, 상기 제1 데이터가 상기 백그라운드 데이터에 해당하는 경우, 으로 주어진다. (상기 [수학식]에서 LOSS는 상기 손실 함수, 는 클래스 i에 대한 예측값을 나타낸다.) 또한, 상기 음성 인식 시스템은, 상기 인공지능부로부터 출력된 상기 n개의 클래스 분류값이 모두 제2 값 미만 인 경우, 해당 클래스를 백그라운드 클래스로 분류하는 후처리부;를 더 포함할 수 있다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법에 따르면, 미리 정의된 음성 핵심어를 인식 및 분류 하여 높은 정확도와 효율성을 갖는다."}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하면서 본 개시의 실시예들에 따른 인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법에 대해 상세히 설명하기로 한다. 본 개시의 하기의 실시예들은 본 개시를 구체화하기 위한 것일 뿐 본 개시의 권리 범위를 제한하거나 한정하는 것이 아님은 물론이다. 본 개시의 상세한 설명 및 실시예들로부터 본 개시가 속하는 기술 분야의 전문가가 용이하게 유추할 수 있는 것은 본 개시의 권리 범위에 속하는 것으로 해석된다. 먼저, 도 1은 일실시예에 따른 인공지능에 기반한 음성 인식 시스템의 구성도를 나타낸다. 일실시예에 따른 인공지능에 기반한 음성 인식 시스템은, 적어도 하나의 프로세서를 이용하여 구현 가능하 다. 도 1로부터 알 수 있는 바와 같이, 일실시예에 따른 인공지능에 기반한 음성 인식 시스템은, 음성 신호 처 리부, 인공지능부 및 후처리부를 포함하여 구성될 수 있다. 음성 신호 처리부 및 후처리부(3 0)는, CPU의 적어도 일부를 이용하여 구현할 수 있고, 인공지능부는 GPU 또는 NPU의 적어도 일부를 이용하 여 구현할 수 있다. 경우에 따라서는, 인공지능부도 CPU의 적어도 일부를 이용하여 구현할 수도 있을 것이다. 음성 신호 처리부는, 입력받은 음성 신호를 처리하여, 인공지능 모델에 입력하기 위한 제1 데이터를 생성하 는 역할을 한다. 즉, 음성 신호 처리부는, 음성 신호를 인공지능 모델인 CNN(Convolutional Neural Networks) 모델 기반의 네트워크인 Mobilenetv2의 입력으로 사용하여 학습 및 평가하기 위해 전처리를 한다. 제1 데이터는, 입력받은 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하여 생성된 영상 데이터 이다. 즉, 제1 데이터는 스펙트로그램(Spectrogram) 이미지가 된다. 구체적으로, 음성 신호 처리부는, 입력받은 제1 음성 신호 중 크기가 제1 임계값 이상인지를 판단하여, 입 력받은 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 출력한다. 아울러, 음성 신호 처리부는, 제2 음성 신호를 입력받아, STFT(Short-Time Fourier Transform)를 실시하고, STFT를 실시한 결과값의 절대값을 취하고, 절대값을 dB 스케일로 변환한 스펙트로그램을 생성한다. STFT는 다음의 [수학식 1]과 같이 실시될 수 있다. 수학식 1"}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "즉, STFT는 음성 신호를 작은 구간으로 나눕니다. 일반적으로 이 구간은 윈도우라고 불리며, 일반적인 크기는 20~40ms 정도이다. 각 윈도우에 대해 푸리에 변환을 수행하여 주파수 성분을 계산한다. 윈도우의 크기 및 오버 랩 크기에 따라 STFT 결과의 해상도와 시간-주파수 표현이 결정된다. STFT 기법은 음성 신호를 짧은 시간 단위로 나누어 푸리에 변환을 진행하여 어느 시점에서 주파수가 변화했는지 알 수 있다는 장점이 있다. 다양한 푸리에 변환 기법 중 STFT는 음성 신호를 분석하는데 효과적이다. 기존의 푸리에 변환 기법은 시간의 흐름에 따라 주파수가 변화했을 때, 어느 시점에서 주파수가 변화했는지 알 수 없다는 단점이 있다. STFT 결과는 복소수 형태로 나타난다. 음성 신호 처리부는, 이 결과에서 절대값을 취하여 음성 신호의 진폭 정보만을 남긴다. 참고로 이 단계에서는 주파수의 위상 정보가 손실되므로, 신호의 위상 변화는 알 수 없게 된 다. 절대값을 취한 STFT 결과는 주로 세기 정보로 이루어져 있습니다. 이 세기 정보를 dB 스케일로 변환하여 스펙트 로그램을 생성한다. dB 스케일은 로그 스케일로 신호의 상대적인 강도를 표현하는데, 음성 신호의 크기 범위를 더 잘 표현할 수 있다. dB 변환은 주로 20log10(|X(t, w)|)로 계산되며, 여기서 |X(t, w)|는 STFT 결과의 절대 값을 나타낸다. 결과적으로, 스펙트로그램은 시간을 x축으로, 주파수를 y축으로 나타내며, 각 시간-주파수 위치에서의 신호 세 기를 색상 또는 밝기로 표현한다. 이를 통해 신호의 시간적인 특성과 주파수 성분의 분포를 시각적으로 파악할 수 있게 된다. 도 2는 음성 신호 처리부에서 생성된 스펙트로그램의 예시도이다. 스펙트로그램은 시간의 흐름에 따른 음성 신호의 변화를 시각화하여 분석할 때 유용하다. 음성 신호 처리부에서는, 데이터셋으로 Speech_commands_v0.02를 사용한다. 음성 신호 처리부에서는, 키워드(Keyword)로 구성된 음성 데이터셋을 스펙트로그램 이미지로 변환하고, 이를 인공지능 모델의 입력으로 사용하여 학습 및 정확도를 평가한다. 인공지능부는 제1 데이터를 입력받아, 인공지능 모델을 이용하여 분류한다. 인공지능부는, 인공지능 모델로 CNN(Convolutional Neural Networks) 모델을 이용하여 n개의 클래스로 분 류한다. n은 2 이상의 자연수이다. 구체적으로 인공지능 모델은, CNN 모델 기반의 네트워크 중 하나인 Mobilenetv2를 이용할 수 있다. CNN 모델을 기반으로 이미지 분류를 할 때 학습 방법도 성능에 영향을 미친다. 이미지 분류에 주로 사용되는 손실 함수(Loss Function)에는 MSE(Mean Squared Error)와 CE(Cross Entropy)가 있다. MSE와 CE는 각각 다음의 [수학식 2] 및 [수학식 3]에 의해 나타낼 수 있다. 수학식 2"}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 3"}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[수학식 2] 및 [수학식 3]에서 는 클래스(Class) i에 대한 예측값, 는 실제값을 나타낸다. 즉, 는 CNN 모델의 출력값이고, 는 라벨링 값이다. 다만, 학습시킨 음성 이외의 음성이 모델의 입력으로 들어가는 경우에 대해서도 고려하면 오인식률을 낮추어 더 좋은 성능을 얻을 수 있다. MSE나 CE를 손실 함수로 사용하는 경우에는 학습 시 백그라운드(Background) 데이터를 사용하지 않는 경우가 대 부분이다. 인공지능부는, 위의 단점을 보완하기 위하여 정답값을 원-샷-인코딩(One-Hot-Encoding) 형태가 아닌 각 클 래스마다 균등한 값을 부여하여 손실값을 업데이트하는 OSE(Open-Set Error) 손실 함수를 사용한다. OSE 손실 함수는 다음의 [수학식 4] 같이 나타낼 수 있다. 수학식 4"}
{"patent_id": "10-2023-0098893", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "[수학식 4]에서도 는 클래스 i에 대한 예측값, 는 실제값을 나타낸다. 후처리부는 인공지능부로부터 출력된 n개의 클래스 분류값이 모두 임계값인 제2 값 미만인 경우, 해당 클래스를 백그라운드 클래스로 분류한다. 즉, 후처리부는 추론 결과값이 미리 설정한 임계값인 제2 값보다 작으면 이를 백그라운드 클래스로 가정한 다. 도 3은 OSE 손실 함수를 이용할 경우 백그라운드 클래스의 분류에 대한 설명도를 나타낸다. 정확도를 향상시키기 위하여 비핵심어 및 노이즈를 백그라운드로 학습시키는 OSE 손실 함수를 적용하여 정확도 를 평가한 후, 이를 기존의 손실 함수인 MSE 또는 CE를 사용했을 경우와 비교하기로 한다. 손실 함수는 파라미터 개수 및 연산량에 영향을 미치는 요소가 아니라는 점을 고려하면 성능이 더 우수한 손실 함수를 선택해야 한다. 도 4는 OSE, MSE 및 CE 손실 함수에 의한 정확도의 실험 결과를 나타낸다. 도 4로부터 알 수 있는 바와 같이, Mobilenetv2 모델을 GPU에서 실시하였을 경우, 실험 결과 CE와 OSE가 백그라 운드 데이터 없이 정확도를 측정했을 경우에는 비슷한 성능을 보이지만 백그라운드 데이터가 테스트셋에 존재할 경우에는 OSE가 CE 대비 약 14% 높은 성능을 갖는 것을 확인할 수 있었다. 아울러, Mobilenetv2 모델을 NPU에서 실시하였을 경우, 실험 결과 NPU 역시 GPU와 동일하게 백그라운드 데이터 가 테스트셋에 존재할 경우 OSE가 가장 높은 성능을 보였고, CE 대비 약 14% 높은 성능을 보였다 하기에 일실시예에 따른 인공지능에 기반한 음성 인식 방법에 대해 설명하기로 한다. 일실시예에 따른 인공지능 에 기반한 음성 인식은, 상술한 인공지능에 기반한 음성 인식에 의해 실시되므로 별도의 설명이 없더라도 인공 지능에 기반한 음성 인식의 모든 특징을 포함함은 물론이다. 아울러, 일실시예에 따른 인공지능에 기반한 음성 인식 방법은, 적어도 하나의 프로세서에 의해 실시되는 컴퓨 터 프로그램의 형태로 구현될 수도 있다. 도 5는 일실시예에 따른 인공지능에 기반한 음성 인식 방법의 흐름도를 나타낸다. 도 5로부터 알 수 있는 바와 같이 일실시예에 따른 인공지능에 기반한 음성 인식 방법은, 제1 음성 신호를 처리 하여, 인공지능 모델에 입력하기 위한 제1 데이터를 생성하는 음성 신호 처리 단계(S10); 제1 데이터를 입력받 아, 인공지능 모델을 이용하여 분류하는 분류 단계(S20); 및 분류 단계(S20)로부터 출력된 n개의 클래스 분류값 이 모두 제2 값 미만인 경우, 해당 클래스를 백그라운드 클래스로 분류하는 후처리 단계(S30);를 포함한다. S10 단계는, 해당 제1 음성 신호의 크기가 제1 임계값 이상지 여부를 판단하는 단계(S11); S11 단계의 판단 경 과 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하는 단계(S12); 제2 음성 신호를 입력받 아, STFT를 실시하는 단계(S13); 및 S13 단계의 절대값을 산출하고, dB 스케일로 변환하여 스펙트로그램을 생성 하는 단계(S14);를 포함한다. 즉, 제1 데이터는, 제1 음성 신호 중 크기가 제1 임계값 이상인 제2 음성 신호를 변환하여 생성된 영상 데이터 인 스펙트로그램이 된다. 분류 단계(S20)에서는, CNN 모델을 이용하여 n개의 클래스로 분류한다. CNN 모델에서 사용하는 손실 함수는, n개의 클래스에 속하지 않는 데이터인 백그라운드 데이터를 사용한다. n은 2 이상의 자연수이다. 구체적으로 제1 데이터가 상기 백그라운드 데이터에 해당하는 경우, 손실 함수는, [수학식 4]와 같이 산출될 수 있다. 실시예들에 따른 인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법의 특징을 정리하면 다음과 같 다. 테스트셋을 이용하여 입력된 음성이 어떤 핵심어인지 분류하여 정확도를 평가할 수 있다. 아울러, 핵심어 및 노 이즈를 백그라운드로 학습시키는 OSE 손실 함수를 사용하여 정확도를 향상시킨다.음성 인식 시스템 및 그 음성 인식 방법은, NPU가 탑재된 임베디드 시스템에서 발화 시 해당 음성이 정확 하게 분류되는지 평가할 수 있다. 또한, RNN 기반의 음성 인식 기술이 아닌, CNN 기반의 네트워크 모델로 핵심어를 분류 및 인식할 수 있다. CNN 기반 네트워크 모델의 학습 데이터로 사용하기 위해 음성 데이터에서 특징을 추출한 후 이를 이미지로 변환하여 데이터셋을 구축할 수 있다. 그리고, Mobilenetv2 모델을 사용하여 임베디드 시스템에서도 KWS 등의 음성 인식 기술이 가능하도록 한다. 아울러, 전통적인 방식의 손실 함수인 MSE 또는 CE가 아닌 비핵심어를 백그라운드로 학습시키는 OSE 손실 함수 를 적용함으로써 정확도가 향상된다. 미리 정의된 핵심어가 아닌 비핵심어 및 노이즈를 백그라운드로 학습킴으로써 비핵심어에 대한 신뢰값 (Confidence Score)을 현저하게 낮출 수 있다. 아울러, 강도나 절도, 폭행, 화재 등의 위급상황에 대한 음성 데 이터셋을 수집하면 CCTV, 블랙박스 등의 보안 산업에서 유용하게 사용할 수 있다. 상술한 바와 같이, 실시예들에 따른 인공지능에 기반한 음성 인식 시스템 및 그 음성 인식 방법에 따르면, 미리 정의된 음성 핵심어를 인식 및 분류하여 높은 정확도와 효율성을 갖는 것을 알 수 있다."}
{"patent_id": "10-2023-0098893", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일실시예에 따른 인공지능에 기반한 음성 인식 시스템의 구성도. 도 2는 음성 신호 처리부에서 생성된 스펙트로그램의 예시도. 도 3은 OSE 손실 함수를 이용할 경우 백그라운드 클래스의 분류에 대한 설명도. 도 4는 OSE, MSE 및 CE 손실 함수에 의한 정확도의 실험 결과. 도 5는 일실시예에 따른 인공지능에 기반한 음성 인식 방법의 흐름도."}
