{"patent_id": "10-2023-0099366", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0018595", "출원번호": "10-2023-0099366", "발명의 명칭": "텍스트 입력 기반의 발화가 가능한 가상인물 합성영상을 생성하는 방법 및 시스템", "출원인": "씨제이올리브네트웍스 주식회사", "발명자": "문종현"}}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "가상인물 합성영상을 생성하는 방법에 있어서,(a) 임의의 텍스트를 수신하는 단계;(b) 상기 텍스트를 분석한 결과를 기초로 음성 및 얼굴을 선택하는 단계;(c) 인물이 포함된 이미지프레임들을 포함하는 원본영상을 로드하는 단계;(d) 발화음성 - 상기 발화음성은, 상기 (b)단계에서 선택된 음성과 상기 (a)단계에서 입력된 텍스트가 합성된것임 - 및 상기 원본영상을 기초로 발화영상을 생성하는 단계; 및(e) 상기 발화영상 내 인물의 얼굴을 상기 선택된 얼굴데이터로 변환하여 가상인물 합성영상을 생성하는 단계;를 포함하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 (b) 단계에서의 상기 음성 또는 얼굴은, 개별 인덱스가 부여된 복수 개의 음성 또는 얼굴들이 저장된 데이터베이스로부터 선택되는 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 (b) 단계는,상기 서비스 서버가, 상기 텍스트를 분석한 결과에 따라 음성 및 얼굴이 선택되는 단계인 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 텍스트는 발화문 및 지시문을 포함하는 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 지시문은 상기 음성 또는 얼굴을 선택하는 데에 활용 가능한 인물서술정보를 포함하는 것을 특징으로하는,공개특허 10-2025-0018595-3-가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 (b)단계는, (b1) 상기 텍스트 내 발화문과 지시문을 구별하는 단계;(b2) 상기 발화문 또는 지시문 중 적어도 하나에 대한 자연어 처리를 실행하는 단계;(b3) 상기 자연어 처리 결과에 따라 음성 및 얼굴을 탐색하는 단계;를 포함하는 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 (b3)단계는, 상기 (b2)단계에 의해 파악된 발화자의 상태와 유사도가 기 설정값 이상인 음성 및 얼굴을 각각 적어도 하나 이상 탐색하는 단계이고, 상기 유사도는 상기 발화자의 상태와 상기 데이터베이스에 저장된 각 음성 또는 얼굴에대응되는 인덱스데이터 간 유사한 정도를 나타내는 값인 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 (c)단계는, 실시간 스트리밍 되는 영상이 상기 원본영상으로서 로드되는 단계인 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 (c)단계 이후, 상기 원본영상으로부터 상기 (b)단계에서 선택된 얼굴이 합성될 합성영역을 식별하는 단계;를 더 포함하고,상기 합성영역은 상기 원본영상으로부터 추출 가능한 폐곡선에 의해 형성되는 영역인 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,(d)단계는,립 제너레이션 알고리즘을 실행시켜 상기 발화음성 및 원본영상을 기초로 발화영상을 생성하는 단계이되, 상기공개특허 10-2025-0018595-4-발화영상은 상기 인물이 상기 텍스트에 포함된 적어도 일부의 문장 또는 단어를 발음하는 입술움직임이 포함된영상인 것을 특징으로 하는,가상인물 합성영상 생성 방법."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "가상인물 합성영상을 생성하는 서비스 서버에 있어서, 상기 서비스 서버는 중앙처리유닛 및 메모리를 포함하며,상기 중앙처리유닛은, 상기 메모리에 저장되어 있는 가상인물 합성영상 생성 방법을 실행시키기 위한 명령어들을 실행시키는 것을 특징으로 하되,상기 가상인물 합성영상 생성 방법은,임의의 텍스트를 수신하는 단계;상기 텍스트를 분석한 결과를 기초로 음성 및 얼굴을 선택하는 단계;인물이 포함된 이미지프레임들을 포함하는 원본영상을 로드하는 단계;발화음성 - 상기 발화음성은, 상기 선택된 음성과 상기 입력된 텍스트가 합성된 것임 - 및 상기 원본영상을 기초로 발화영상을 생성하는 단계; 및상기 발화영상 내 인물의 얼굴을 상기 선택된 얼굴데이터로 변환하여 가상인물 합성영상을 생성하는 단계;를 포함하는,서비스 서버."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,데이터베이스를 더 포함하되,상기 데이터베이스에는 개별 인덱스가 부여된 복수 개의 음성 또는 얼굴들이 저장된 것을 특징으로 하는,서비스 서버."}
{"patent_id": "10-2023-0099366", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 데이터베이스에는, 상기 개별 인덱스에 매칭되는 인덱스데이터가 함께 저장되되, 상기 인덱스데이터는 각음성 또는 얼굴의 특징들이 기록된 항목들의 집합인 것을 특징으로 하는,서비스 서버."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 텍스트 입력 기반의 발화가 가능한 가상인물 합성영상을 생성하는 방법 및 시스템에 관한 것이다. 구 체적으로, 본 발명은 사용자에 의해 임의의 텍스트가 입력되었을 때, 텍스트로부터 획득될 수 있는 정보들을 활 용하여 가상인물을 생성하고, 나아가 이 가상인물이 텍스트에 따라 말하는 것처럼 보이도록 하는 합성영상을 생 성하는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 텍스트 입력 기반의 발화가 가능한 가상인물 합성영상을 생성하는 방법 및 시스템에 관한 것이다. 구 체적으로, 본 발명은 사용자에 의해 임의의 텍스트가 입력되었을 때, 텍스트로부터 획득될 수 있는 정보들을 활 용하여 가상인물을 생성하고, 나아가 이 가상인물이 텍스트에 따라 말하는 것처럼 보이도록 하는 합성영상을 생 성하는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 가상인물 혹은 가상인간에 대한 관심이 크게 높아지고 있다. 가상인물을 주인공으로 하는 광고, 예능 프로 그램들이 소비자들에게 노출되고 있으며, 이 외에도 다양한 분야에서 가상인물에 대한 수요가 지속적으로 증가 하고 있다. 그러나 현재까지 가상인물의 생성은 매우 많은 시간과 자원이 소요되는 작업이며, 하나의 가상인물로 하여금 말 을 하게 하는 짧은 영상을 생성해 내는 데에도 상당한 시간과 비용이 소요되므로 가상인물을 보편적인 주변 서 비스에서 활용하기에는 아직까지 무리가 있다. 본 발명은 이러한 문제점에 착안하여 제안된 것으로, 사용자에 의해 입력된 텍스트를 기반으로 발화가 가능한 가상인물을 비교적 빠르고 쉽게 합성하여 소비자에게 제공하기 위해 발명되었다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허 제10-2022-011100호(2022.01.27. 공개)"}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 텍스트 입력만으로 가상인물이 실제 말을 하는 듯한 영상, 즉 가상인물 합성영상이 생성될 수 있게 하는 것을 목적으로 한다. 또한 본 발명은 위 가상인물 합성영상이 생성될 때에 발화에 맞는 자연스러운 입술모양을 구현해 내는 것을 목 적으로 한다. 또한 본 발명은 필요한 얼굴, 필요한 음성을 미리 데이터베이스화 해 둠으로써 사용자로부터의 텍스트 입력이 있을 시 매우 빠르게 가상인물 합성영상의 생성이 가능하게 하는 것을 목적으로 한다. 한편, 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명은 위와 같은 문제점을 해결하기 위한 것으로, 본 발명에 따른 가상인물 합성영상을 생성하는 방법은, (a) 임의의 텍스트를 수신하는 단계; (b) 상기 텍스트를 분석한 결과를 기초로 음성 및 얼굴을 선택하는 단계; (c) 인물이 포함된 이미지프레임들을 포함하는 원본영상을 로드하는 단계; (d) 발화음성 - 상기 발화음성은, 상 기 (b)단계에서 선택된 음성과 상기 (a)단계에서 입력된 텍스트가 합성된 것임 - 및 상기 원본영상을 기초로 발 화영상을 생성하는 단계; 및 (e) 상기 발화영상 내 인물의 얼굴을 상기 선택된 얼굴데이터로 변환하여 가상인물 합성영상을 생성하는 단계;를 포함할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 (b) 단계에서의 상기 음성 또는 얼굴은, 개별 인덱스가 부여된 복수 개의 음성 또는 얼굴들이 저장된 데이터베이스로부터 선택되는 것을 특징으로 할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 (b) 단계는, 상기 서비스 서버가, 상기 텍스트를 분석 한 결과에 따라 음성 및 얼굴을 선택하는 단계인 것을 특징으로 할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 텍스트는 발화문 및 지시문을 포함하는 것을 특징으로 할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 지시문은 상기 음성 또는 얼굴을 선택하는 데에 활용 가능한 인물서술정보를 포함할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 (b)단계는, (b1) 상기 텍스트 내 발화문과 지시문을 구 별하는 단계; (b2) 상기 발화문 또는 지시문 중 적어도 하나에 대한 자연어 처리를 실행하는 단계; (b3) 상기자연어 처리 결과에 따라 음성 및 얼굴을 탐색하는 단계; 를 포함할 수 있다. 또한 이 때, 상기 (b3)단계는, 상기 (b2)단계에 의해 파악된 발화자의 상태와 유사도가 기 설정값 이상인 음성 및 얼굴을 각각 적어도 하나 이상 탐색하는 단계이고, 상기 유사도는 상기 발화자의 상태와 상기 데이터베이스 에 저장된 각 음성 또는 얼굴에 대응되는 인덱스데이터 간 유사한 정도를 나타내는 값일 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, 상기 (c)단계는, 실시간 스트리밍 되는 영상이 상기 원본영 상으로서 로드되는 단계일 수 있다. 또한, 이 때 상기 (c)단계 이후, 상기 원본영상으로부터 상기 (b)단계에서 선택된 얼굴이 합성될 합성영역을 식 별하는 단계; 를 더 포함하고, 상기 합성영역은 상기 원본영상으로부터 추출 가능한 폐곡선에 의해 형성되는 영 역인 것을 특징으로 할 수 있다. 또한 상기 가상인물 합성영상 생성 방법에 있어서, (d)단계는, 립 제너레이션 알고리즘을 실행시켜 상기 발화음 성 및 원본영상을 기초로 발화영상을 생성하는 단계이되, 상기 발화영상은 상기 인물이 상기 텍스트에 포함된 적어도 일부의 문장 또는 단어를 발음하는 입술움직임이 포함된 영상인 것을 특징으로 할 수 있다. 한편, 본 발명의 또 다른 실시예에 따른 가상인물 합성영상을 생성하는 서비스 서버에 있어서, 상기 서비스 서 버는 중앙처리유닛 및 메모리를 포함하며, 상기 중앙처리유닛은, 상기 메모리에 저장되어 있는 가상인물 합성영 상 생성 방법을 실행시키기 위한 명령어들을 실행시키는 것을 특징으로 하되, 상기 가상인물 합성영상 생성 방 법은, 임의의 텍스트를 수신하는 단계; 상기 텍스트를 분석한 결과를 기초로 음성 및 얼굴을 선택하는 단계; 인 물이 포함된 이미지프레임들을 포함하는 원본영상을 로드하는 단계; 발화음성 - 상기 발화음성은, 상기 선택된 음성과 상기 입력된 텍스트가 합성된 것임 - 및 상기 원본영상을 기초로 발화영상을 생성하는 단계; 및 상기 발 화영상 내 인물의 얼굴을 상기 선택된 얼굴데이터로 변환하여 가상인물 합성영상을 생성하는 단계;를 포함할 수 있다. 또한, 상기 서비스 서버는 데이터베이스를 더 포함하되, 상기 데이터베이스에는 개별 인덱스가 부여된 복수 개 의 음성 또는 얼굴들이 저장된 것을 특징으로 할 수 있다. 또한, 상기 서비스 서버에 있어서, 상기 데이터베이스에는, 상기 개별 인덱스에 매칭되는 인덱스데이터가 함께 저장되되, 상기 인덱스데이터는 각 음성 또는 얼굴의 특징들이 기록된 항목들의 집합인 것을 특징으로 할 수 있 다."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 가상인물의 텍스트 입력만으로 발화하는 가상인물을 구현해 낼 수 있는 효과가 있다. 또한 본 발명에 따르면 입력한 텍스트 내에 포함되어 있는 발화문과 지시문을 분석한 후 해당되는 상황에 맞는 가상인물의 생성이 가능한 효과가 있다. 이에 따라 다양한 분야, 다양한 목적을 위한 가상인물 구현이 빠르고 정확하게 이루어질 수 있는 효과가 있다. 또한 본 발명에 따르면 가상인물의 발화 시 시청자들에 의해 가장 주요하게 보이게 되는 입술모양을 자연스럽게 구현해 낼 수 있고, 특히 가상얼굴과의 관계에서 발생할 수 있는 이질감을 최소화 할 수 있는 효과가 있다. 한편, 본 발명에 의한 효과는 이상에서 언급한 것들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 효과들 은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 목적과 기술적 구성 및 그에 따른 작용 효과에 관한 자세한 사항은 본 발명의 명세서에 첨부된 도면 에 의거한 이하의 상세한 설명에 의해 보다 명확하게 이해될 것이다. 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 명세서에서 개시되는 실시예들은 본 발명의 범위를 한정하는 것으로 해석되거나 이용되지 않아야 할 것이다. 이 분야의 통상의 기술자에게 본 명세서의 실시예를 포함한 설명은 다양한 응용을 갖는다는 것이 당연하다. 따 라서, 본 발명의 상세한 설명에 기재된 임의의 실시예들은 본 발명을 보다 잘 설명하기 위한 예시적인 것이며 본 발명의 범위가 실시예들로 한정되는 것을 의도하지 않는다. 도면에 표시되고 아래에 설명되는 기능 블록들은 가능한 구현의 예들일 뿐이다. 다른 구현들에서는 상세한 설명 의 사상 및 범위를 벗어나지 않는 범위에서 다른 기능 블록들이 사용될 수 있다. 또한, 본 발명의 하나 이상의 기능 블록이 개별 블록들로 표시되지만, 본 발명의 기능 블록들 중 하나 이상은 동일 기능을 실행하는 다양한 하드웨어 및 소프트웨어 구성들의 조합일 수 있다. 또한, 어떤 구성요소들을 포함한다는 표현은 “개방형”의 표현으로서 해당 구성요소들이 존재하는 것을 단순히 지칭할 뿐이며, 추가적인 구성요소들을 배제하는 것으로 이해되어서는 안 된다. 나아가 어떤 구성요소가 다른 구성요소에 “연결되어” 있다거나 “접속되어” 있다고 언급될 때에는, 그 다른 구성요소에 직접적으로 연결 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 한다. 도 1은 본 발명에 따라 가상인물 합성영상이 생성되어 실제 실행되는 모습을 이해하기 위한 도면이다. 참고로, 본 상세한 설명에서는 “가상인물 합성영상”이라는 용어가 자주 사용되는데, 이는 구현하고자 하는 가상인물이 포함된 영상을 뜻할 수 있다. 다시 도 1을 참고할 때, 가상인물 합성영상은 기본적으로 사용자에 의해 텍스트가 입력되면 그 텍스트의 내용에 맞추어 음성데이터 및 얼굴데이터가 선택되고, 선택된 음성데이터 및 얼굴데이터가 활용되어 생성된 가상의 인 물이 실제 텍스트에 포함되어 있는 문장을 읽는 모습이 포함된 영상이다. 예를 들어, 사용자가 “(초등학교 학생이 아침에 등교하면서 밝은 표정으로) 선생님, 안녕하세요!”라는 텍스트 를 입력하였을 때, 가상인물 합성영상은 기 저장되어 있는 초등학생의 목소리들 중 어느 하나를 음성데이터로, 그리고 기 저장되어 있는 초등학생의 얼굴들 중 어느 하나를 얼굴데이터로 선택하여 합성해 낸 가상인물이 포함 된 영상일 수 있다. 다른 한편, 본 발명에 의해 생성될 수 있는 가상인물 합성영상은 반드시 현실 세계에 존재하지 않는, 가공된 인 물이 아닐 수도 있다. 즉, 구현되는 가상인물은 현실에 존재하는 특정인의 음성데이터, 특정인의 얼굴데이터를 활용하여 생성된 것일 수 있다. 이 경우, 가상인물의 발화 행위 자체는 실제인물이 행하지 않은, 컴퓨터 소프트 웨어에 의해 가상으로 구현된 것일 수 있다. 본 발명에 의해 생성될 수 있는 가상인물 합성영상은 반드시 익명 성, 불특정성의 인물에 관한 것으로 한정되지 않으며, 필요에 따라 현실 세계에 존재하는, 또는 현실 세계에 존 재하였던 특정 인물에 관한 것일 수도 있음을 이해한다. 한편, 도면으로는 따로 도시하지 않았으나 본 발명에 따른 가상인물 합성영상은 중앙처리유닛 및 메모리를 갖춘 연산장치에 의해 생성되는 것을 전제로 한다. 이 때 중앙처리유닛은 컨트롤러(controller), 마이크로 컨트롤러 (microcontroller), 마이크로 프로세서(microprocessor), 마이크로 컴퓨터(microcomputer) 등으로도 불릴 수 있다. 또한 중앙처리유닛은 하드웨어(hardware) 또는 펌웨어(firmware), 소프트웨어, 또는 이들의 결합에 의해 구현될 수 있는데, 하드웨어를 이용하여 구현하는 경우에는 ASIC(application specific integrated circuit) 또는 DSP(digital signal processor), DSPD(digital signal processing device), PLD(programmable logic device), FPGA(field programmable gate array) 등으로, 펌웨어나 소프트웨어를 이용하여 구현하는 경우에는 위와 같은 기능 또는 동작들을 수행하는 모듈, 절차 또는 함수 등을 포함하도록 펌웨어나 소프트웨어가 구성될 수 있다. 또한, 메모리는 ROM(Read Only Memory), RAM(Random Access Memory), EPROM(Erasable ProgrammableRead Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), 플래쉬(flash) 메모리, SRAM(Static RAM), HDD(Hard Disk Drive), SSD(Solid State Drive) 등으로 구현될 수 있다. 연산장치의 종류에는 제한이 없다 할 것이나 바람직하게는 서버 컴퓨터, 휴대용 단말기(랩탑 컴퓨터, 스마트폰 등), 데스크탑 컴퓨터 등이 연산장치의 대표적인 예시들이 될 수 있다. 또한 경우에 따라 상기 연산장치는 클라 우드 서버의 형태로 이용 가능한 것일 수 있다. 도 2는 본 발명의 일 실시예에 따른 가상인물 합성영상 생성 방법을 순서에 따라 도시한 것이다. 도면을 참고할 때, 가상인물 합성영상 생성 방법은 가장 먼저 텍스트 입력을 수신하는 단계(S100)로부터 시작될 수 있다. 텍스트 입력을 수신하는 주체는 앞서 언급하였던 연산장치일 것이며, 사용자는 상기 연산장치에 연결 되어 있는 입력수단을 활용하여 특정 텍스트를 입력할 수 있다. 또는, 사용자는 상기 연산장치에 네트워크로 연 결되어 있는 별도의 사용자 단말기를 이용하여서도 특정 텍스트를 입력할 수 있다. 가령, 상기 연산장치가 서버 컴퓨터라 할 때, 사용자는 스마트폰(사용자 단말기) 상에서 터치입력 혹은 음성입력을 함으로써 네트워크로 연 결된 서버 컴퓨터 측에 텍스트 입력을 전달할 수 있다. 한편, 사용자에 의해 입력되는 텍스트는 바람직하게는 도 3에 도시되어 있는 것과 같이 지시문과 발화문을 포함 할 수 있다. 지시문이란, 인물의 동작, 표정, 심리, 말투 따위를 지시하거나 서술하기 위한 텍스트를 의미하며, 발화문은 인물이 실제 소리를 내어 말을 하는 텍스트를 의미한다. 경우에 따라서는 발화문만 포함된 텍스트가 입력될 수도 있겠으나, 바람직하게는 가상인물의 정확한 생성을 위하여 지시문이 포함될 수 있다. 지시문에는 특히 인물과 관련된 정보, 다시 말해 인물에 대한 상황, 특징을 서술하는 인물서술정보들을 포함할 수 있는데, 이러한 인물서술정보에는 발화를 하는 인물의 나이, 나이대, 성별, 인종, 국적, 직업, 직업군, 회사명, 직급, 발화자의 행위, 발화자의 위치, 발화자가 처해 있는 환경, 발화자의 기분, 또는 현재 시간 등이 포함될 수 있다. 위의 나열된 인물서술정보의 항목들은 예시적으로 언급된 것이며, 이 밖에 해당 인물에 대한 추가적인 항 목들이 더 포함될 수 있음을 이해한다. 도면의 예를 참고할 때, “오늘 학생식당 점심메뉴는 뭐야? (여대생이 발랄하게)”라는 텍스트 중 발화문은 “ 오늘 학생식당 점심메뉴는 뭐야?”, 지시문은 “(여대생이 발랄하게)”에 해당한다. 입력된 텍스트 내에서 지시 문과 발화문의 구분은 문장 부호, 바람직하게는 괄호 부호, 또는 큰따옴표 등과 같이 문장 또는 문구를 구분할 수 있는 문장 부호에 의해 이루어질 수 있다. 문장 부호를 사용하지 않는 경우엔 “[발화문] 오늘 학생식당 점 심메뉴는 뭐야? [지시문] 여대생이 발랄하게”와 같이 ‘발화문’과 ‘지시문’을 구분하여 입력하도록 함으로 써 둘 간의 구별이 이루어질 수 있다. 한편, S100 단계 이후에는 상기 입력된 텍스트를 분석하는 단계와 분석 결과에 따라 음성 및/또는 얼굴을 선택 하는 단계(S200)가 실행될 수 있다. S200 단계를 본격적으로 설명하기에 앞서, 가상인물 구현을 위한 음성 및 얼굴에 대해 먼저 설명하기로 한다. 음성 및 얼굴은 복수 개의 획득 또는 생성된 음성, 그리고 복수 개의 획득 또는 생성된 얼굴들이 데이터베이스화 되어 존재할 수 있다. 본 상세한 설명에서는 이들을 각각 음성데이터 DB, 얼굴데이터 DB라 칭하기로 한다. 음성데이터 DB는 가상인물의 발화 음성을 출력시키기 위해 구축된 데이터베이스를 의미하는 것으로, 음성데이터 DB 내에는 복수 개의 합성된 음성들 또는 녹음에 의해 획득된 음성들이 저장될 수 있다. 각각의 음성들은 여성, 남성, 어린이, 노인 등과 같이 특정 인물의 속성에 의해 구별될 수 있으며, 더 구체적으로는 10대 여성, 15세 남자 중학생, 40세 여성 직장인, 54세 남성 미화원 등과 같이 해당 음성과 매칭되는 실제 인물의 속성들로 구별 될 수도 있다. 다시 말해, 음성데이터 DB에 저장되는 복수 개의 음성들은 성별에 따라 검색될 수 있음은 물론, 국적, 나이, 나이대, 직업, 음성이 녹음된 시간대, 또는 음성이 녹음된 장소 등과 같이 해당 음성이 녹음될 당 시의 인물 속성, 환경 속성에 따라 검색이 가능하도록 구분(인덱싱)될 수 있다. 여기서 합성된 음성들이란, 엄밀하게는 음성 합성 기술(Speech Synthesis)을 활용하여 텍스트가 음성으로 변환 된 것들을 의미할 수 있으며, 예를 들어 VITS모델이 활용되어 많은 양의 텍스트들이 음성으로 변환된 것들일 수 있다. 한편, 인덱싱 방식에는 다양한 방법들이 존재할 수 있는데, 예를 들어 [10대 여성, 대학생]은 v_index#1, [20대 여성, 직장인]은 v_index#2로 저장되거나, [14세, 남성, 중학생]은 v_index#3, [59세, 남성, 중학교 선생님]은 v_index#4로 저장되거나, [21세, 여성, 직장인, 출근시간대]은 v_index#5, [72세, 남성, 법인대표, 점심식사 중]은 v_index#6으로 저장되거나 하는 등의 방식으로 구분될 수 있다. 이처럼 각 음성은 음성이 최초 생성될 때설정값으로서 사용되었던 발화자의 나이, 나이대, 성별, 인종, 국적, 직업, 직업군, 회사명, 직급, 발화자의 행 위, 발화자의 위치, 발화자가 처해 있는 환경, 발화자의 기분, 녹음 당시의 시간대 등의 항목들, 또는 음성이 녹음된 것이라면 녹음될 당시의 발화자의 나이, 나이대, 성별, 직업, 직업군, 회사명, 직급, 발화자의 녹음 당 시의 행위, 발화자의 위치, 발화자가 처해 있는 환경, 발화자의 기분, 녹음 당시의 시간대 등의 항목들이 조합 된 채 하나의 인덱스데이터를 구성할 수 있고, 그 인덱스데이터마다 인덱스가 부여될 수 있다. 얼굴데이터 DB는 가상인물의 얼굴을 데이터베이스로 구축해 둔 것을 의미하며, 당연히 얼굴데이터 DB에는 복수 개의 인물 얼굴들이 저장될 수 있다. 얼굴데이터 DB에 저장된 얼굴들은 실제 인물의 얼굴을 촬영함으로써 획득 한 얼굴면, 인공지능 소프트웨어에 의해 생성된 얼굴, 또는 사용자가 임의로 그려냄으로써 획득한 얼굴 등을 포 함할 수 있다. 또한, 반드시 얼굴면은 실사 이미지만 포함되는 것은 아니며 필요에 따라 애니메이션 인물의 얼 굴, 동물 얼굴 등이 더 포함될 수 있다. 얼굴데이터 DB 역시 앞서 설명한 음성데이터 DB와 마찬가지로 인덱싱이 이루어질 수 있다. 예를 들어 [얼굴이미 지1]는 f_index#1, [얼굴이미지2]는 f_index#2로 저장되거나, [얼굴이미지3, 40대, 남성]은 f_index#3, [얼굴 이미지4, 60대, 여성]은 f_index#4로 저장되거나, 또는 [얼굴이미지5, 35세, 남성, 고등학교 선생님]은 f_index#5, [얼굴이미지6, 11세, 여성, 초등학생]은 f_index#6 등과 같이 해당 얼굴이미지가 생성될 시 참고되 었던 항목들이 인덱스데이터와 함께 저장될 수 있다. 참고로 인덱스데이터는 음성 또는 얼굴 각각의 특징을 기록한 항목들의 집합일 수 있으며, 개별 항목들의 조합 에 의해 각 음성 또는 각 얼굴이 구별될 수 있고, 각각의 음성 또는 얼굴에 부여된 인덱스는 각각의 인덱스데이 터와 매칭되어 데이터베이스(음성데이터 DB, 얼굴데이터 DB)에 저장될 수 있다. 다시 S200단계에 대한 설명으로 돌아와서, S200 단계는 사용자로부터 입력된 텍스트를 분석한 결과에 따라 음성 또는 얼굴 중 적어도 하나를 선택하는 단계인데, 이와 관련하여서는 도 4에 개념적으로 도시가 되어 있다. 도 4 를 참고하면, 입력된 텍스트가 “오늘 학생식당 점심메뉴는 뭐야? (여대생이 발랄하게)\"일 때, 분석은 연산장치 에 의해 이루어질 수 있는데, 연산장치는 위 텍스트에 대한 자연어 처리 알고리즘을 실행시킴으로써 음성 또는 얼굴 중 적어도 하나를 선별할 수 있다. 도면에서는 v_index#2의 음성과 f_index#5의 얼굴이 선별된 실시 예가 도시되어 있다. 이 때 연산장치란, 사용자가 소지하고 있는 사용자 단말기(스마트폰, 태블릿PC 등), 또는 상기 사용자 단말기로부터 위 텍스트를 전달받은 서버 컴퓨터일 수 있다. 연산장치는 적어도 (i) 발화문과 지시문을 구별하는 단계 (ii) 발화문 또는/및 지시문에 대한 자연어 처리를 실 행하는 단계 (iii) 자연어 처리 결과에 따라 음성 또는/및 얼굴을 탐색하는 단계를 실시할 수 있다. 발화문과 지시문을 구별하는 단계는 앞서 도 3에서 설명한 텍스트의 구조처럼 발화문과 지시문이 애초부터 구분 되어 있는 상태라면 경우에 따라 생략될 수도 있다. 입력된 텍스트에 특별히 발화문, 지시문 구분이 되어 있지 않는 경우엔 (ii)단계의 자연어 처리를 실행하는 단계에서 발화문과 지시문이 구분될 수도 있다. 발화문 또는/및 지시문에 대한 자연어 처리를 실행하는 단계에서는 형태소 분석, 구문 분석, 의미 분석, 또는 화용 분석 등이 이루어질 수 있으며, 본 단계에서는 발화문 및 지시문 분석을 통해 발화자의 상태가 파악될 수 있다. 발화자의 상태는, 발화자의 나이, 나이대, 성별, 직업, 직업군, 직급, 현재 발화자의 행위, 발화자의 위 치, 발화자가 처해 있는 환경, 발화자의 기분, 또는 녹음 당시의 시간대 등 텍스트로부터 파악 가능한 요소들을 모두 포함할 수 있다. 한편, (ii)단계의 본격적인 실행에 앞서 연산장치는, 특히 텍스트 내 지시문으로부터 획득 가능한 정보로부터 발화자의 상태를 1차적으로 파악할 수 있으며, 이 밖에 더 필요한 정보들은 발화문, 또는 지시문 분석을 실행함 으로써 얻을 수 있다. 다시 말해, 연산장치는 특별히 자연어 처리를 실행하지 않고도 얻을 수 있는 정보가 있는 경우에는 자연어 처리 실행에 따른 텍스트 분석을 하지 않고도 다음 단계로 진행할 수 있다. 예를 들어, 지시문 에 발화자의 나이, 성별, 장소가 드러나 있는 경우, 이러한 정보만으로도 후속 단계(음성 또는/및 얼굴 탐색)를 실시할 수 있다고 판단된 경우에는 자연어 처리를 실행하지 않고도 (iii)단계로 나아갈 수 있다. 가령 지시문에 “(28세의 신입 남자직원이 회사 식당 입구에서 식권을 구매하면서)”와 같은 정보들이 포함되어 있을 경우, 발 화자의 나이, 성별, 장소를 이미 파악할 수 있으므로 별도의 자연어 처리 실행이 필요하지 않을 수 있다. (iii)단계는 연산장치가 앞선 단계에서 텍스트 내 정보로부터 획득한 발화자의 상태, 또는 분석에 의해 파악된 발화자의 상태에 매칭되는 음성 또는/및 얼굴을 탐색하는 단계이다. 연산장치는 앞서 설명한 음성데이터 DB 또 는/및 얼굴데이터 DB를 참조하여 전 단계에서 획득되거나 파악된 발화자의 상태에 맞는 음성 또는/및 얼굴을 탐 색할 수 있다. 탐색의 기준은 발화자의 상태와 각 음성 또는/및 얼굴의 인덱스데이터 간 유사도가 될 것이며,유사도가 높을수록 최종 매칭 가능성이 더 높아질 수 있다. 파악된 발화자의 상태가 10가지 요소들을 포함한다 고 가정할 때, 연산장치는 각 요소들에 우선순위를 두어 유사도 판단을 할 수 있는데, 예를 들어 성별(남성 또 는 여성)이 가장 먼저 동일한지를 살핀 후 나이대, 직업군, 음성이 녹음될 당시의 시간대 등의 순으로 적어도 일부의 요소들에 대해서는 우선순위를 두어 유사도 판단을 함으로써 발화자의 상태와 매칭되는 음성 또는/및 얼 굴을 탐색할 수 있다. 참고로, 연산장치는 음성 또는 얼굴 중 어느 하나에 대해서만 유사도를 연산하고 탐색하여 하나의 음성이나 얼 굴이 특정되면, 특정된 음성 또는 얼굴에 기 연계되어 있는 얼굴이나 음성을 자동으로 찾을 수 있도록 설계될 수도 있다. 가령 (남성, 17세, 고등학생, 오전 6시 30분)의 정보를 기초로 얼굴이 탐색되었다면, 해당 얼굴에 기 연계되어 있는 음성까지도 자연스럽게 함께 세트로 탐색이 가능하게 함으로써 굳이 동일한 정보를 기초로 음 성, 얼굴 각각에 대한 연산들을 별개로 실시하지 않게 할 수 있다. 도 5는 연산장치가 참조할 수 있는 지시문 DB의 일 예시를 도시한 것으로, 지시문 DB에는 지시문, 지시문과 매 칭되는 음성 및 얼굴이 미리 매칭되어 저장되어 있을 수 있다. 예를 들어, 특정 지시문이 “(어린 학생이 울면 서)”일 때, 이에 매칭되는 음성은 v_index#9, 얼굴은 f_index#4로 미리 정해져 있을 수 있다. 연산장치는 사용 자에 의해 입력된 텍스트가 지시문을 포함하고, 이 때 지시문의 포함한 키워드들이 지시문 DB 내 기 저장되어 있는 지시문과 동일하거나 기 설정값 이상의 유사도를 가질 경우, 해당 지시문에 매칭되는 음성 및 얼굴을 자동 으로 탐색해 낼 수 있다. 이상 S200단계에서 사용자로부터 입력된 텍스트를 기초로 음성 또는/및 얼굴을 선택하는 과정에 대해 살펴보았 다. 다시 도 2를 참고할 때, S200 단계 이후에는 원본영상을 로드(load)하는 단계(S300)가 실행될 수 있다. 이러한 원본영상은 사용자 의도에 따라 직접 업로드 또는 스트리밍 된 영상이거나, 혹은 사용자의 명령입력에 따라 연산장치가 네트워크로부터 획득한 임의의 영상일 수 있으며, 상기 원본영상은 바람직하게는 인물(사람)의 얼굴면이 화면 내에 포함되는 것일 수 있다. 일 실시예에서, 원본영상 로드 단계는 사용자가 스마트폰 어플리케이션을 실행시킨 후 스마트폰의 전면 카메라 를 통해 사용자 자신의 모습을 촬영한 영상이 상기 스마트폰 또는 그 밖의 사용자 단말기의 메모리 상에 로드 (load)되는 단계일 수 있다. 또 다른 실시예에서, 원본영상 로드 단계는 사용자가 스마트폰 또는 그 밖의 사용 자 단말기를 이용해 검색한 인물 중심의 동영상이 메모리 상에 로드(load)되는 단계일 수 있다. 원본영상은 반드시 인물을 포함하여야 할 필요는 없는데, 예를 들어 원본영상은 강아지, 고양이 등과 같은 동물 들, 해바라기 등과 같은 식물들, 또는 물건들을 포함할 수 있다. 다만, 원본영상 내에는 가상인물의 얼굴이 합 성될 수 있는 영역, 즉 합성영역이 식별 가능할 정도로는 존재하여야 한다. 합성영역은 폐곡선에 의해 형성되는 영역일 수 있으며, 바람직하게는 얼굴의 형상이 합성될 수 있도록 원형 내지 타원형의 영역일 수 있다. 사각형, 삼각형, 또는 그 밖에 정규 도형으로 정의될 수 없는 영역이라 할지라도 폐곡선에 의한 합성영역이 식별 가능하 다면 해당 원본영상은 합성에 적절한 영상으로 판단될 수 있다. 합성영역의 식별은, 예를 들어 원본영상 내로부 터 연산장치에 의한 폐곡선 인식이 가능 (이 때 폐곡선은 색깔과 색깔의 경계선에 의한 것일 수도 있고 실제 영 상 내에 실선이 존재하는 것일 수도 있는 등 다양한 상황에 의해 존재할 수 있음)하여 식별이 이루어질 수 있으 며, 나아가 얼굴임을 구별할 수 있는 눈, 코, 입, 귀, 머리카락 등의 존재여부가 폐곡선 인식에 참고될 수 있다. 참고로, 위 원본영상으로부터 식별되는 합성영역(얼굴영역) 외의 영역들은 최종적으로 가상인물 합성영상에서 사용자의 제스쳐, 또는 움직임 등을 나타낼 수 있도록 그대로 활용될 수 있다. 즉, 원본영상 내 사용자가 박수 를 치는 행위를 하였을 때, 최종 가상인물 합성영상에서는 사용자의 얼굴영역만 바뀐 채 박수 치는 행위는 동일 하게 출력될 수 있다. 경우에 따라 원본영상으로부터 인식된 사용자의 제스쳐 또는 움직임은 서비스 서버에 의 하여 재구성되어 최종 가상인물 합성영상에서 출력될 수도 있다. 예를 들어, 원본영상 내 사용자가 달리기를 하 는 제스쳐를 취하였을 때, 최종 가상인물 합성영상에서는 사용자의 얼굴영역도 바뀜과 동시에 가상인물이 실제 운동장에서 달리는 듯한 애니메이션으로 재구성되어 출력될 수 있다. 한편, 상기 원본영상 내에 사용자로부터 직접 설정된 영역이 합성영역으로 지정될 수 있다. 예를 들어 연산장치 는 원본영상을 로드하면서 사용자에게 가상인물의 얼굴을 어느 영역에 합성시킬 것인지를 정하도록 화면 인터페 이스를 제공할 수 있으며, 사용자는 마우스, 터치펜 등을 이용하여 원하는 영역에 폐곡선을 그림으로써, 혹은이미 연산장치에 의해 선택가능한 여러 개의 합성영역 후보군들 중 하나를 선택함으로써 합성영역을 지정할 수 있다. 원본영상을 로드하는 단계 이후에는 발화영상을 생성하는 단계(S400)가 실행될 수 있다. 발화영상은 립 제너레 이션(lip generation) 기술에 의해 생성되는 영상으로, 립 제너레이션 기술은 발화음성과 인물영상을 입력으로 하여 영상 속 인물이 해당 음성을 발화하는 것과 같은 영상을 생성하는 기술을 의미한다. 도 6은 발화영상을 생성하는 단계를 개념적으로 도시한 것으로, 연산장치는 발화음성 및 원본영상을 입력으로 받아 립 제너레이션 알고리즘으로 처리하여 발화영상을 생성할 수 있다. 즉, 발화자가 앞서 입력된 텍스트 에 포함된 문장 또는 단어들, 더 바람직하게는 발화문을 구성하는 문장 또는 단어들을 발음하는 입술움직임이 포함된 영상을 생성할 수 있다. 이 때 원본영상은 앞서 설명한 것과 동일하며, 발화음성은 연산장치에 의해 선 택된 음성의 스타일과 사용자에 의해 입력된 텍스트(발화문)가 음성합성(speech synthesis)기술에 의해 생성된 것일 수 있다. 예를 들어, 사용자가 셀피영상을 촬영하여 가지고 있다고 가정할 때, 위 셀피영상이 원본영상이 될 수 있으며, 상기 사용자가 입력한 특정 텍스트 “오늘 학생식당 점심메뉴는 뭐야?(여대생이 발랄하게)”가 주어지면, 위 셀피영상과 발화음성이 립 제너레이션 알고리즘에 의해 합성되어 마치 셀피영상 속 사용자가 위 텍스트에 따른 말을 하는 것과 같은 발화영상이 생성될 수 있다. 또 다른 예로, 사용자가 영상통화 서비스를 이용하고자 할 때에 스마트폰의 전면 카메라로 자신의 모습을 촬영하면 연산장치(이 때 연산장치는 스마트폰일 수 있음)에서는 위 실시간으로 촬영 중인 사용자 영상을 원본영상으로, 그리고 상기 사용자(또는 상기 사용자 외에 또 다른 사용자)에 의해 입력된 텍스트에 따른 발화음성이 합성되어 실시간으로 발화영상이 생성될 수 있 다. 발화영상을 생성하는 단계 이후에는 가상인물 합성영상이 생성(S500)될 수 있다. 가상인물 합성영상이란, 위 발 화영상 내에서의 얼굴이 앞서 연산장치에 의해 선택된 얼굴로 변환된 영상을 의미하는 것으로, 원본영상에서의 인물 얼굴이 얼굴데이터 DB로부터 탐색 및 선택된 얼굴로 바뀐 영상으로도 이해될 수 있다. 도 7에는 가상인물 생성을 위한 얼굴들 중 f_index#5의 얼굴이 선택되어 합성에 활용되고, 앞서 설명한 발화영 상 또한 합성에 활용되는 점이 도시되어 있으며, 이렇게 발화영상과 연산장치에 의해 선택된 얼굴이 얼굴합성 알고리즘에 의해 합성되어 최종적으로 가상인물 합성영상이 생성될 수 있음이 도시되어 있다. 얼굴데이터 DB로부터 선택된 얼굴은 앞서 설명한 S400 단계 이전에, 다시 말해 발화영상이 생성되기 이전에 원 본영상과 합성이 가능할 수도 있겠으나, 본 발명은 발화영상이 생성된 이후 얼굴을 변환하는 것을 하나의 중요 한 특징으로 할 수 있다. 즉, 립 제너레이션 알고리즘에 의해 발화영상이 생성된 후, 발화영상 전체를 대상으로 얼굴 변환을 위한 알고리즘을 실행시키는 경우, 얼굴 변환을 먼저한 후 립 제너레이션 알고리즘을 실행시키는 것에 비해 더 자연스러운 얼굴 변환이 가능할 수 있다. 참고로 본 상세한 설명에서 언급되고 도면에 도시된 자연어 처리 알고리즘, 립 제너레이션 알고리즘, 얼굴합성 알고리즘은 용어를 ‘알고리즘’으로 기재하긴 하였으나, 각각은 연산장치 또는 서비스 서버 에 포함되는 모듈로 이해될 수 있다. 즉, 자연어 처리 알고리즘은 자연어 처리 알고리즘을 실행시키 기 위한 모듈, 립 제너레이션 알고리즘은 립 제너레이션 알고리즘을 실행시키기 위한 모듈, 얼굴합성 알고 리즘은 얼굴합성 알고리즘을 실행시키기 위한 모듈로 이해될 수 있다. 또한, 상기 모듈들은 반드시 하나의 연산장치 내에 포함되어야만 하는 것은 아니며, 각각의 모듈은 별개의 독립된 연산장치로 존재할 수도 있다. 이 경우 상호 간에는 네트워크망을 통한 데이터 송수신이 존재할 수 있다. 이상 본 발명에 따른 가상인물 합성영상을 생성하는 방법 및 이를 위한 시스템에 대해 살펴보았다. 한편, 본 발 명은 상술한 특정의 실시예 및 응용예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이"}
{"patent_id": "10-2023-0099366", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "없이 당해 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 구별되어 이해되어서는 안 될 것이다."}
{"patent_id": "10-2023-0099366", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 가상인물 합성영상이 생성되어 텍스트 입력에 따라 발화하는 모습을 도시한 것이다. 도 2는 본 발명에 따른 가상인물 합성영상 생성 방법의 각 단계들을 순서에 따라 도시한 것이다. 도 3은 사용자에 의해 입력되는 텍스트의 구조를 도시한 것이다. 도 4는 입력된 텍스트가 자연어 처리 알고리즘에 의해 분석된 후 특정 음성데이터 및 얼굴데이터가 선정되는 과 정을 도시한 것이다. 도 5는 지시문과 매칭되는 음성데이터 및 얼굴데이터가 사전에 저장되어 있는 데이터베이스의 구조를 도시한 것 이다.도 6은 발화음성과 원본영상을 합성시켜 발화영상을 생성하는 과정을, 도 7은 발화영상 내 얼굴을 스왑시켜 최 종적으로 가상인물 합성영상을 생성하는 과정을 도시한 것이다."}
