{"patent_id": "10-2022-0135831", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0055490", "출원번호": "10-2022-0135831", "발명의 명칭": "머신 러닝 파이프라인을 통한 지속적 학습 방법 및 이를 수행하는 컴퓨팅 장치", "출원인": "주식회사 카카오뱅크", "발명자": "김남기"}}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨팅 장치에서 수행되는 모델 서빙 방법에 있어서,사용자의 서빙 요청에 대응을 위하여 모델 이미지 내 정의된 프로세스를 가상화된 OS(Operating System) 상에서컨테이너로 실행하는 단계;상기 모델의 입력 값으로 특징 데이터를 수신하는 단계; 및상기 컨테이너로 실행된 상기 프로세스 내에서 상기 수신된 특징 데이터와 사용자의 입력 값에 대한 상기 모델의 추론 값을 출력하는 단계를 포함하고,상기 모델 이미지는,트리거 값에 따라 학습 또는 예측을 동적으로 수행하는 학습 파이프라인 상에서 서빙을 수행하는 상기 모델의모델 파일을 포함하는 아티팩트를 도커 이미지로 패킹함으로써 생성되는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 학습 파이프라인은,상기 모델에 입력되는 특징 데이터를 생성하는 특징 데이터 파이프라인 및상기 생성된 특징 데이터를 통한 상기 모델의 학습 또는 예측을 수행하는 학습 모델 파이프라인을 포함하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 트리거 값은 지속적 학습을 위해 결정된 제1 주기에 따라 수집된 특징 데이터로 상기 모델의 학습을 수행하도록 상기 학습 모델 파이프라인을 실행하는 제1 트리거인 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 학습 모델 파이프라인은,상기 모델에 대하여 미리 생성된 기초 이미지로부터 생성된 모델 이미지를 상기 컨테이너로 실행하는 단계;상기 트리거 값에 따라 상기 컨테이너로 실행된 모델을 상기 생성된 특징 데이터로 학습시키는 단계;상기 학습된 모델의 성능을 평가하는 단계;상기 평가 결과에 따라 상기 모델의 유효성을 검증하는 단계; 및상기 유효성이 검증된 모델 파일 및 상기 모델의 성능 평가 결과를 저장하는 단계를 포함하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2 항에 있어서,상기 트리거 값은 배치 추론을 위해 결정된 제2 주기에 따라 수집된 데이터를 통해 상기 모델의 예측을 수행하도록 상기 학습 모델 파이프라인을 실행하는 제2 트리거인 것을 특징으로 하는 모델 서빙 방법.공개특허 10-2024-0055490-3-청구항 6 제 5 항에 있어서,상기 학습 모델 파이프라인은,상기 트리거 값에 따라 상기 생성된 특징 데이터를 배치 단위로 상기 컨테이너로 실행된 모델에 입력하고 예측값을 생성하는 단계; 및상기 생성된 예측 값을 통해 상기 모델의 성능을 모니터링하는 단계를 포함하는 것을 특징으로 하는 모델 서빙방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 성능을 모니터링하는 단계는 상기 모델의 이전 예측 값 분포와 상기 생성된 예측 값의 분포 차이를 기준값과 비교하여 모델의 유효 여부를 판단하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 2 항에 있어서,상기 특징 데이터 파이프라인은,특징 데이터를 추출하는 단계;상기 추출된 특징 데이터를 이전 모델의 생성에 이용된 과거 특징 데이터와 비교하여 데이터의 유효성을 검증하는 단계; 및상기 검증된 특징 데이터를 임베딩하는 단계를 포함하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 데이터의 유효성을 검증하는 단계는 이전 시점에 추출된 특징 데이터 대비 현재 추출된 특징 데이터의 분포 차이를 기준 값과 비교하여 데이터의 유효 여부를 판단하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서,상기 모델 이미지를 컨테이너로 실행하는 단계는,모델 저장소에 저장된 모델 파일을 구분된 네트워크 상의 모델 생성 서버로 제공하고,상기 모델 생성 서버에서 상기 모델 파일 및 상기 모델 파일의 실행을 위한 조건들을 포함하는 모델 아티팩트를패킹 후 푸시한 모델 이미지를 상기 컨테이너로 실행하는 것을 특징으로 하는 모델 서빙 방법."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "프로세서; 및상기 프로세서와 통신하는 메모리를 포함하고,상기 메모리는 상기 프로세서로 하여금 동작들을 수행하게 하는 명령들을 저장하고,상기 동작들은, 사용자의 서빙 요청에 대응을 위하여 모델 이미지 내 정의된 프로세스를 가상화된 OS 상에서 컨테이너로 실행하는 동작,상기 모델의 입력 값으로 특징 데이터를 수신하는 동작, 및상기 컨테이너로 실행된 상기 프로세스 내에서 상기 수신된 특징 데이터와 사용자의 입력 값에 대한 상기 모델공개특허 10-2024-0055490-4-의 추론 값을 출력하는 동작을 포함하고,상기 모델 이미지는,트리거 값에 따라 학습 또는 예측을 동적으로 수행하는 학습 파이프라인 상에서 서빙을 수행하는 상기 모델의모델 파일을 포함하는 아티팩트를 도커 이미지로 패킹함으로써 생성되는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 학습 파이프라인은,상기 모델에 입력되는 특징 데이터를 생성하는 특징 데이터 파이프라인 및상기 생성된 특징 데이터를 통한 상기 모델의 학습 또는 예측을 수행하는 학습 모델 파이프라인을 포함하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,상기 트리거 값은 지속적 학습을 위해 결정된 제1 주기에 따라 수집된 특징 데이터로 상기 모델의 학습을 수행하도록 상기 학습 모델 파이프라인을 실행하는 제1 트리거인 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 학습 모델 파이프라인은,상기 모델에 대하여 미리 생성된 기초 이미지로부터 생성된 모델 이미지를 상기 컨테이너로 실행하는 동작,상기 트리거 값에 따라 상기 컨테이너로 실행된 모델을 상기 생성된 특징 데이터로 학습시키는 동작,상기 학습된 모델의 성능을 평가하는 동작,상기 평가 결과에 따라 상기 모델의 유효성을 검증하는 동작, 및상기 유효성이 검증된 모델 파일 및 상기 모델의 성능 평가 결과를 저장하는 동작을 포함하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12 항에 있어서,상기 트리거 값은 배치 추론을 위해 결정된 제2 주기에 따라 수집된 데이터를 통해 상기 모델의 예측을 수행하도록 상기 학습 모델 파이프라인을 실행하는 제2 트리거인 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 15 항에 있어서,상기 학습 모델 파이프라인은,상기 트리거 값에 따라 상기 생성된 특징 데이터를 배치 단위로 상기 컨테이너로 실행된 모델에 입력하고 예측값을 생성하는 동작, 및상기 생성된 예측 값을 통해 상기 모델의 성능을 모니터링하는 동작을 포함하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서,상기 성능을 모니터링하는 동작은 상기 모델의 이전 예측 값 분포와 상기 생성된 예측 값의 분포 차이를 기준공개특허 10-2024-0055490-5-값과 비교하여 모델의 유효 여부를 판단하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 12 항에 있어서,상기 특징 데이터 파이프라인은,특징 데이터를 추출하는 동작,상기 추출된 특징 데이터를 이전 모델의 생성에 이용된 과거 특징 데이터와 비교하여 데이터의 유효성을 검증하는 동작, 및상기 검증된 특징 데이터를 임베딩하는 동작을 포함하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 18 항에 있어서,상기 데이터의 유효성을 검증하는 동작은 이전 시점에 추출된 특징 데이터 대비 현재 추출된 특징 데이터의 분포 차이를 기준 값과 비교하여 데이터의 유효 여부를 판단하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 11 항에 있어서,상기 모델 이미지를 컨테이너로 실행하는 동작은,모델 저장소에 저장된 모델 파일을 구분된 네트워크 상의 모델 생성 서버로 제공하고,상기 모델 생성 서버에서 상기 모델 파일 및 상기 모델 파일의 실행을 위한 조건들을 포함하는 모델 아티팩트를패킹 후 푸시한 모델 이미지를 상기 컨테이너로 실행하는 것을 특징으로 하는 컴퓨팅 장치."}
{"patent_id": "10-2022-0135831", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제 1 항 내지 제 10 항 중 어느 한 항에 따른 모델 서빙 방법을 수행하는 프로그램이 저장된 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 학습 모델의 지속적인 통합과 배포를 위한 방법에 관한 것이다. 본 발명의 일 실시예에 따른 컴퓨팅 장치에서 수행되는 모델 서빙 방법은 사용자의 서빙 요청에 대응을 위하여 모델 이미지 내 정의된 프로세스를 가 상화된 OS(Operating System) 상에서 컨테이너로 실행하는 단계; 상기 모델의 입력 값으로 특징 데이터를 수신하 (뒷면에 계속)"}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 학습 모델의 지속적인 학습과 배포를 위한 방법에 관한 것이다."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "소프트웨어의 개발과 운영을 구분하지 않고 개발의 생산성과 운영의 안정성을 최적화하는 데 널리 사용되는 DevOps는 지속적 통합(Continuous integration, CI)과 지속적 배포(Continuous deployment, CD)를 운영 과정에 도입해 개발 주기 단축, 배포 속도 증가, 안정적인 출시 등의 효과를 제공한다. 이러한 전통적인 프로그래밍 기반의 서비스 관리를 용이하게 하는 작업이 DevOps(Development operations)라면, 최근에는 인공지능 기술의 산업 전반에 이르는 적용의 확대로 머신러닝 기반의 소프트웨어 서비스 운영을 효율 화 하고자 MLOps(Machine learning operations)라는 개념이 새롭게 확산되고 있다. 머신러닝 기반의 시스템 역시 소프트웨어 시스템이므로 시스템을 안정적으로 생성하고 운영할 수 있도록 DevOps 와 유사한 방식이 적용될 수 있으나, 머신러닝은 단순한 소프트웨어의 지속적인 개발 외에도 대규모의 데이터에 대한 분석을 위한 측면의 운영이 필요하다. 즉, MLOps는 데이터 사이언스, 다양한 클라우드 환경에서 머신러닝 을 위한 인프라의 활용 측면에서의 ML 시스템 개발(Dev)과 ML 시스템 운영(Ops)을 통합하는 것을 목표로 한다. 이러한 MLOps는 기능의 통합, 테스트, 출시, 배포, 인프라 관리를 비롯하여 ML 시스템 구성의 모든 단계에서 자 동화 및 모니터링을 지원할 수 있다. 이와 관련하여 선행특허(미국등록특허공보 US11,310,141 (등록일2022.04.19)는 MLOps를 통해 머신러닝 모델의 성능의 모니터링을 수행하고, 모니터링 결과에 따라 변칙적인 성능 저하가 발생한 경우 이에 대한 정정 조치를 동적으로 수행하는 방법을 개시하고 있다."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 머신러닝 환경에서 데이터의 변화에 따른 모델의 성능 변화를 판단하고 이를 통해 모델의 개선을 수 행하는 방법을 제공하는 것을 목적으로 한다. 또한, 본 발명은 지속적으로 수집되는 데이터를 통한 모델의 학습과 성능의 예측을 통한 배치(batch) 기반의 학 습으로 모델을 업데이트하는 방법을 제공하는 것을 목적으로 한다. 또한, 본 발명은 사용자의 요청에 따른 실시간 응답과 함께 응답 결과를 활용하여 머신러닝 모델의 학습에 이용 할 수 있도록 하는 것을 목적으로 한다. 또한, 본 발명은 지속적으로 수집되는 배치단위의 예측과 예측에 따른 모델의 성능을 데이터 측면에서 모니터링 할 수 있도록 하는 것을 목적으로 한다."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 기술적 과제를 해결하기 위해 본 발명의 일 실시예에 따른 컴퓨팅 장치에서 수행되는 모델 서빙 방법은 사 용자의 서빙 요청에 대응을 위하여 모델 이미지 내 정의된 프로세스를 가상화된 OS(Operating System) 상에서 컨테이너로 실행하는 단계; 상기 모델의 입력 값으로 특징 데이터를 수신하는 단계; 및 상기 컨테이너로 실행된 상기 프로세스 내에서 상기 수신된 특징 데이터와 사용자의 입력 값에 대한 상기 모델의 추론 값을 출력하는 단 계를 포함하고, 상기 모델 이미지는, 트리거 값에 따라 학습 또는 예측을 동적으로 수행하는 학습 파이프라인 상에서 서빙을 수행하는 상기 모델의 모델 파일을 포함하는 아티팩트를 도커 이미지로 패킹함으로써 생성되는 것이 바람직하다. 상기 학습 파이프라인은, 상기 모델에 입력되는 특징 데이터를 생성하는 특징 데이터 파이프라인 및 상기 생성 된 특징 데이터를 통한 상기 모델의 학습 또는 예측을 수행하는 학습 모델 파이프라인을 포함한다. 상기 트리거 값은 지속적 학습을 위해 결정된 제1 주기에 따라 수집된 특징 데이터로 상기 모델의 학습을 수행 하도록 상기 학습 모델 파이프라인을 실행하는 제1 트리거인 것이 바람직하다. 상기 학습 모델 파이프라인은, 상기 모델에 대하여 미리 생성된 기초 이미지로부터 생성된 모델 이미지를 상기 컨테이너로 실행하는 단계; 상기 트리거 값에 따라 상기 컨테이너로 실행된 모델을 상기 생성된 특징 데이터로 학습시키는 단계; 상기 학습된 모델의 성능을 평가하는 단계; 상기 평가 결과에 따라 상기 모델의 유효성을 검 증하는 단계; 및 상기 유효성이 검증된 모델 파일 및 상기 모델의 성능 평가 결과를 저장하는 단계를 포함한다. 상기 트리거 값은 배치 추론을 위해 결정된 제2 주기에 따라 수집된 데이터를 통해 상기 모델의 예측을 수행하 도록 상기 학습 모델 파이프라인을 실행하는 제2 트리거인 것이 바람직하다. 상기 학습 모델 파이프라인은, 상기 트리거 값에 따라 상기 생성된 특징 데이터를 배치 단위로 상기 컨테이너로 실행된 모델에 입력하고 예측 값을 생성하는 단계; 및 상기 생성된 예측 값을 통해 상기 모델의 성능을 모니터 링하는 단계를 포함한다. 상기 성능을 모니터링하는 단계는 상기 모델의 이전 예측 값 분포와 상기 생성된 예측 값의 분포 차이를 기준 값과 비교하여 모델의 유효 여부를 판단한다. 상기 특징 데이터 파이프라인은, 특징 데이터를 추출하는 단계; 상기 추출된 특징 데이터를 이전 모델의 생성에 이용된 과거 특징 데이터와 비교하여 데이터의 유효성을 검증하는 단계; 및 상기 검증된 특징 데이터를 임베딩 하는 단계를 포함한다. 상기 데이터의 유효성을 검증하는 단계는 이전 시점에 추출된 특징 데이터 대비 현재 추출된 특징 데이터의 분 포 차이를 기준 값과 비교하여 데이터의 유효 여부를 판단하는 것이 바람직하다. 상기 모델 이미지를 컨테이너로 실행하는 단계는, 모델 저장소에 저장된 모델 파일을 구분된 네트워크 상의 모 델 생성 서버로 제공하고, 상기 모델 생성 서버에서 상기 모델 파일 및 상기 모델 파일의 실행을 위한 조건들을포함하는 모델 아티팩트를 패킹 후 푸시한 모델 이미지를 상기 컨테이너로 실행하는 것이 바람직하다. 상기 기술적 과제를 해결하기 위해 컴퓨팅 장치에서 수행되는 컴퓨팅 장치는 프로세서; 및 상기 프로세서와 통 신하는 메모리를 포함하고, 상기 메모리는 상기 프로세서로 하여금 동작들을 수행하게 하는 명령들을 저장하고, 상기 동작들은, 사용자의 서빙 요청에 대응을 위하여 모델 이미지 내 정의된 프로세스를 가상화된 OS 상에서 컨 테이너로 실행하는 동작, 상기 모델의 입력 값으로 특징 데이터를 수신하는 동작, 및 상기 컨테이너로 실행된 상기 프로세스 내에서 상기 수신된 특징 데이터와 사용자의 입력 값에 대한 상기 모델의 추론 값을 출력하는 동 작을 포함하고, 상기 모델 이미지는, 트리거 값에 따라 학습 또는 예측을 동적으로 수행하는 학습 파이프라인 상에서 서빙을 수행하는 상기 모델의 모델 파일을 포함하는 아티팩트를 도커 이미지로 패킹함으로써 생성되는 것이 바람직하다. 상기 학습 파이프라인은, 상기 모델에 입력되는 특징 데이터를 생성하는 특징 데이터 파이프라인 및 상기 생성 된 특징 데이터를 통한 상기 모델의 학습 또는 예측을 수행하는 학습 모델 파이프라인을 포함하는 것이 바람직 하다. 상기 트리거 값은 지속적 학습을 위해 결정된 제1 주기에 따라 수집된 특징 데이터로 상기 모델의 학습을 수행 하도록 상기 학습 모델 파이프라인을 실행하는 제1 트리거인 것이 바람직하다. 상기 학습 모델 파이프라인은, 상기 모델에 대하여 미리 생성된 기초 이미지로부터 생성된 모델 이미지를 상기 컨테이너로 실행하는 동작, 상기 트리거 값에 따라 상기 컨테이너로 실행된 모델을 상기 생성된 특징 데이터로 학습시키는 동작, 상기 학습된 모델의 성능을 평가하는 동작, 상기 평가 결과에 따라 상기 모델의 유효성을 검 증하는 동작, 및 상기 유효성이 검증된 모델 파일 및 상기 모델의 성능 평가 결과를 저장하는 동작을 포함한다. 상기 트리거 값은 배치 추론을 위해 결정된 제2 주기에 따라 수집된 데이터를 통해 상기 모델의 예측을 수행하 도록 상기 학습 모델 파이프라인을 실행하는 제2 트리거인 것이 바람직하다. 상기 학습 모델 파이프라인은, 상기 트리거 값에 따라 상기 생성된 특징 데이터를 배치 단위로 상기 컨테이너로 실행된 모델에 입력하고 예측 값을 생성하는 동작, 상기 생성된 예측 값을 통해 상기 모델의 성능을 모니터링하 는 동작을 포함한다. 상기 성능을 모니터링하는 동작은 상기 모델의 이전 예측 값 분포와 상기 생성된 예측 값의 분포 차이를 기준 값과 비교하여 모델의 유효 여부를 판단하는 것이 바람직하다. 상기 특징 데이터 파이프라인은, 특징 데이터를 추출하는 동작, 상기 추출된 특징 데이터를 이전 모델의 생성에 이용된 과거 특징 데이터와 비교하여 데이터의 유효성을 검증하는 동작, 및 상기 검증된 특징 데이터를 임베딩 하는 동작을 포함한다. 상기 데이터의 유효성을 검증하는 동작은 이전 시점에 추출된 특징 데이터 대비 현재 추출된 특징 데이터의 분 포 차이를 기준 값과 비교하여 데이터의 유효 여부를 판단하는 것이 바람직하다. 상기 모델 이미지를 컨테이너로 실행하는 동작은, 모델 저장소에 저장된 모델 파일을 구분된 네트워크 상의 모 델 생성 서버로 제공하고, 상기 모델 생성 서버에서 상기 모델 파일 및 상기 모델 파일의 실행을 위한 조건들을 포함하는 모델 아티팩트를 패킹 후 푸시한 모델 이미지를 상기 컨테이너로 실행하는 것이 바람직하다."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 머신러닝 환경에서 시간에 따른 데이터들의 특징 변화로 인한 모델의 성능 저하를 유연하게 대처할 수 있으며 이를 통해 사용자들에게 지속적인 양질의 서비스를 제공해 줄 수 있다. 또한, 본 발명은 모델의 학습 및 운영과 관련된 파라미터를 통해 실시간으로 수집되는 데이터를 통한 모델의 학 습과 특정 주기로 수집된 데이터들의 세트를 이용한 성능을 예측함으로써 효율적인 서비스가 가능하다. 또한, 본 발명은 사용자의 요청에 따른 실시간 응답과 함께 응답 결과를 활용하여 머신러닝 모델의 학습에 이용 함으로써 지속적인 성능 향상을 꾀할 수 있다. 또한, 본 발명은 지속적으로 수집되는 데이터에 대한 배치단위의 예측을 통해 모델의 갱신 주기를 관리하고 갱 신된 모델을 배포함으로써 전체 서비스 품질을 향상시킬 수 있다."}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하의 내용은 단지 발명의 원리를 예시한다. 그러므로 당업자는 비록 본 명세서에 명확히 설명되거나 도시 되 지 않았지만 발명의 원리를 구현하고 발명의 개념과 범위에 포함된 다양한 장치를 발명할 수 있는 것이다. 또한, 본 명세서에 열거된 모든 조건부 용어 및 실시 예들은 원칙적으로, 발명의 개념이 이해되도록 하기 위한 목적으로만 명백히 의도되고, 이외같이 특별히 열거된 실시 예들 및 상태들에 제한적이지 않는 것으로 이해되어 야 한다. 상술한 목적, 특징 및 장점은 첨부된 도면과 관련한 다음의 상세한 설명을 통하여 보다 분명해질 것이며, 그에"}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "따라 발명이 속하는 기술분야에서 통상의 지식을 가진 자가 발명의 기술적 사상을 용이하게 실시할 수 있을 것 이다. 또한, 발명을 설명함에 있어서 발명과 관련된 공지 기술에 대한 구체적인 설명이 발명의 요지를 불필요하게 흐 릴 수 있다고 판단되는 경우에 그 상세한 설명을 생략하기로 한다. 이하에는 첨부한 도면을 참조하여 본 발명의 바람직한 실시 예에 대해 상세하게 설명한다. 도 1은 본 발명의 일 실시예에 따른 모델 서빙 방법을 나타내는 흐름도이다. 본 개시의 일 실시예에 따른 모델은 머신러닝(Machine Learning, ML) 모델 또는 딥러닝 기반의 학습된 신경망 모델을 포함할 수 있으며, 인공지능 기반으로 의사결정이나 예측을 수행하기 위해 컴퓨팅 장치 상에서 동작하는 알고리즘들을 포함한다. 본 실시예에서 모델 서빙은 사용자에게 모델의 예측 결과를 효율적으로 전달하는 것으로, 머신러닝 또는 딥러닝 을 통해 학습된 모델을 통해 입력 값에 따른 추론 값을 제공하는 일련의 서비스 과정을 포함하며, 모델 서빙은 사용자의 요청에 따른 응답 외에 모델의 학습을 위한 예측(Prediction) 과정을 모두 포함할 수 있다. 도 1을 참고하면 본 발명의 일 실시예에 따른 모델 서빙 방법은 모델 서빙 서버가 사용자의 서빙 요청에 대응되 는 모델에 따라 푸시된 모델 이미지를 읽고, 가상화된 OS(Operating System) 상에서 컨테이너로 모델 이미지 내 에 정의된 일련의 프로세스를 실행(run)하는 것으로 수행될 수 있다(S100). 본 실시예에서 모델 이미지는 트리거 값에 따라 학습 또는 예측을 수행하는 파이프라인을 통해 생성된 모델 파 일과 학습 과정의 학습 데이터, 또는 학습 조건 등의 결과물들을 가상화된 OS 상에서 도커 컨테이너(Docker Container)로 실행하기 위해 하나의 도커 이미지로 패킹함으로써 생성될 수 있다. 도커는 리눅스 기반의 OS 상에서 컨테이너를 생성하는 방식 중 하나로 프로세스를 격리해서 실행할 수 있도록 하며, 계층화된 파일 시스템에 기반하여 프로세스의 실행 환경을 도커 이미지 단위로 구축함으로써 효율적인 프 로세스의 관리가 가능하도록 한다. 따라서, 본 실시예에서는 모델 서빙을 위한 일련의 프로세스와 프로세스의 실행 환경을 모델 이미지로 패킹하여 생성함으로써 각각의 모델이 동작하는데 필요한 실행 환경을 다양한 플랫폼에서 재현 가능하도록 하며 모델의 학습과 예측이 가능한 파이프라인을 이미지로 구성하여 효율적으로 배포할 수 있도록 한다.이때, 모델 서빙을 통해 제공되는 서비스는 서비스 고유의 특성에 기인하여 정보보안 및 운영 안정성을 보장하 기 위한 목적으로 물리적으로 분리된 망에서 수행될 수 있다. 따라서 모델의 서빙 과정에서 수집되는 정보들을 이용한 모델의 학습 과정과 학습된 모델을 이미지로 패킹하는 과정은 분리된 망에서 서버 간의 인터페이스로 구 현될 수 있다. 따라서 본 실시예에서는 모델 이미지를 생성하는 서버와 생성된 이미지를 통해 실제 서빙을 수행하는 서버는 구 분될 수 있다. 예를 들어, 본 실시예는 MLOps 환경에서 모델 이미지를 생성하는 모델 생성 서버와 모델 생성 서 버에서 생성된 모델 이미지를 푸시하고, 푸시된 모델 이미지에 따른 프로세스를 컨테이너로 실행하는 모델 서빙 서버 간의 망 분리를 통해 서비스를 제공할 수 있다. 구체적으로 본 실시예에 따라 MLOps 기반의 모델 서빙 방법을 제공하는 시스템은, 모델 서빙 서버가 독립적으로 모델을 이미지로 수신하여 컨테이너를 통해 실행함으로써 이미지 내에 정의된 학습 또는 예측 파이프라인을 수 행하고, 학습된 모델 및 모델의 메타 정보와 학습에 이용된 일련의 데이터들을 모델 저장소로 저장하여 관리하 도록 하는 운영망과, 생성된 모델 파일을 제공받아 컨테이너 상에서 실행되는 도커 기반의 이미지를 생성하는 별도의 모델 생성 서버를 관리하는 개발망으로 분리 구성될 수 있다. 또한, 이미지를 패킹하는 모델 생성 서버를 관리하는 개발망은, 운영망과 달리 제한된 조건하에서 외부와 통신 이 가능한 경영망으로부터 실시간 서빙을 위한 다양한 파라미터들을 정의하는 코드 정보들을 수신하는 것도 가 능하다. 또한, 모델 생성 서버는 경영망으로부터 수신한 실시간 서빙을 위한 코드와 운영망으로부터 수신하여 개발망 내에 저장된 배치 서빙을 위한 코드 및 모델 파일 들을 이용하여 모델 이미지를 생성할 수 있다. 이어서 모델 서빙 서버는 컨테이너로 실행되는 프로세스 내에서 추론을 수행하는 모델의 결과 출력을 위한 입 력 값으로 특징 데이터를 수신한다(S200). 특징 데이터는 모델의 입력을 위해 추출과 임베딩 과정을 통해 생성될 수 있으며 본 실시예에서는 학습 파이프 라인에 따른 데이터 처리 과정을 거쳐 실시간 서빙을 위해 입력된 사용자의 입력 값을 임베딩하여 특징 데이터 를 생성하거나 모델의 서빙에 필요한 다양한 특징들을 저장하는 온/오프라인 특징 스토어로부터 임베딩된 데이 터들을 전달받음으로써 특징 데이터를 수신하는 과정은 수행될 수 있다. 다음으로, 컨테이너로 실행된 프로세스 내에서 수신된 특징 데이터와 사용자의 입력 값에 대한 모델의 추론 값 을 출력한다(S300). 본 실시예에서 모델의 추론은 사용자의 입력 값에 따른 응답을 제공하기 위해 수행될 수 있으며, 사용자의 피드 백을 통한 지속적인 학습, 또는 모델의 성능을 검증하거나 예측하기 위한 별도의 파이프라인을 통해 수행될 수 있다. 이하에서는 도 2를 참고하여 본 실시예에 따른 파이프라인을 통한 모델 서빙 방법을 제공하는 시스템(이하, 시 스템이라 함)의 구성에 대하여 보다 상세히 설명한다. 도 2를 참고하면 본 실시예에서 시스템은 복수의 망으로 분리하여 구성될 수 있으며, 복수의 망간의 인터페이스 에 따라 학습 파이프라인을 실행함으로써 모델의 서빙과 학습을 수행할 수 있다. 본 실시예에서 시스템은 실시간으로 API(Application Programming Interface)를 통한 응답을 제공하기 위한 모 델의 다양한 파라미터 및 모델의 동작을 위한 설정 정보들을 실시간 서빙 코드로 제공하는 경영망과 모델 파일 및 학습에 필요한 다양한 특징 데이터, 리소스 조건 등의 정보를 하나의 이미지로 패킹하는 모델 생성 서 버를 포함하는 개발망 및 개발망에서 생성된 이미지를 통해 학습에서 배치 단위의 예측까지 수행하는 모델 서빙 서버를 포함하는 운영 망으로 구분될 수 있다. 도시하지는 않았으나 운영망과 외부의 사용자간의 요청과 응답을 송신하기 위한 채널 서버(미도시)가 더 포함될 수 있으며 채널 서버는 사용자와 모델 서빙 서버간의 정보의 송수신을 위한 인터페이스를 제공할 수 있다. 먼저 개발망 내 모델 생성 서버는 실시간 서빙을 위한 실시간 서빙 코드를 경영망으로부터 수 신하되 배치 단위의 예측을 위한 조건 및 설정 정보들을 정의하는 배치 서빙 코드를 개발망 내의 일 저 장소 (예를 들어 Gitlab)로부터 수신하여 모델 파일과 함께 모델 이미지를 생성할 수 있다. 실시간 서빙 코드는 온라인을 통해 사용자로부터 요청된 정보를 API 기반으로 실시간으로 응답하는 과 정에서 산출되는 복수의 결과물들을 실시간 서빙 아티팩트로 포함할 수 있다.또한, 배치 서빙 코드는 스케줄링에 따라 소정 주기로 수집된 데이터 셋에 대한 배치 단위의 예측 결과를 산 출하는 과정에서의 결과물들을 배치 서빙 아티팩트로 포함할 수 있다. 이때 실시간 서빙 아티팩트 또는 배치 서빙 아티팩트는 모델 파일 외 모델의 학습을 위한 조건 파라미터와 갱신 된 가중치 등의 값을 포함할 수 있으며, 학습 과정에서 이용된 손실 함수(Loss Function) 또는 학습에 이용된 데이터셋의 정보 및 학습에 따른 성능을 포함하는 학습 결과물들을 모두 포함할 수 있다. 모델 파일은 모델의 소스 코드 외에 모델의 동작을 위한 파라미터들을 포함할 수 있다. 본 실시예에서 모델 파일은 운영 망의 모델 저장소로부터 제공받을 수 있으며 모델 생성 서버의 패킹 부는 모델 파일의 저장 경로를 통해 저장소에 접근하는 것으로 모델 파일을 추출하고 패킹을 수행함으로써 이미지를 생성할 수 있다. 구체적으로 이상의 데이터들을 통한 모델 생성 서버의 모델 이미지 생성 과정에 대하여 도 3을 참고하여 상세히 설명한다. 도 3은 본 실시예에 따른 모델 생성 서버에서 상술한 실시간 및 배치 서빙 코드를 통해 모델 이미지를 생 성하기 위한 동작을 나타내는 예시도이다. 도 3을 참조하면, 모델 생성 서버 내 패킹부는 실시간 서빙 코드로 수신한 실시간 서빙 아티팩트 와 모델 파일을 이용하여, 컨테이너 생성에 필요한 다양한 설정 값들을 이미지화 할 수 있다. 또한 패킹부는 배치 서빙 코드로 수신한 배치 서빙 아티팩트를 추가로 이용하여 모델 이미지를 생성함으로 써, 생성된 모델 이미지 내 모델이 실시간 추론과 배치 단위의 예측을 독립하여 수행할 수 있도록 한다. 구체적으로 생성된 모델 이미지는 운영망 내 학습 파이프라인에 따라 실시간 추론 또는 배치 단위의 예측을 위해 컨테이너로 실행되며, 모델 저장소에 저장된 결과물들을 통해 학습된 모델의 성능 검증을 수 행함으로써 지속적인 학습과 높은 성능의 모델을 지속적으로 배포할 수 있도록 한다. 일 실시예에 따르면, 학습 파이프라인은 MLOps의 학습 파이프라인을 의미할 수 있다. 또한, 본 실시예에서는 모델의 학습 결과물들을 정의하는 아티팩트를 실시간 서빙 아티팩트와 배치 서빙 아티팩트로 구분함에 따라 학습에 이용된 모델의 소스코드 및 파라미터들을 모델 파일로 아티팩트와 별개로 구분하여 설명하였으나 아티팩트는 학습된 모델 파일을 포함하는 것으로 해석될 수 있으며 이상의 구분 에 따라 본 실시예에 따른 모델 생성 서버의 동작이 제한되거나 반드시 데이터들을 구분하여 입력 받는 것으로 한정되지 않는다. 다시 도 2를 참조하여 생성된 모델 이미지를 통한 운영망 내의 학습 파이프라인의 동작 과정에 대하여 설명한다. 본 실시예에서 학습 파이프라인은 모델 생성 서버의 모델 이미지 패킹 경로(예, hdfsPath: /user/rollingpin/mpd/models/{model_name}/ {model_version})가 입력되는 것으로 해당 모델을 실행할 수 있다. 모델 저장소에서 모델 파일을 모델 생성 서버로 제공하는 것으로 모델의 생성 요청이 발생하면 모델 생성 서버는 경영망에서 gitlab-ci로 배포한 실시간 서빙 코드를 이용하여 모델 이미지를 패킹하고 운영망 내의 도커 레지스트리로 패킹된 모델 이미지를 푸시할 수 있다. 푸시된 모델 이미지는 모델의 실행을 위해 모델 서빙 서버내 컨테이너에 탑재됨으로써 실행된다. 탑재된 모델들은 운영망 내의 학습 파이프라인을 통해 정의된 프로세스로 학습 또는 예측 과정을 수행 하며, 실시간 서빙의 경우 사용자의 API 요청에 포함된 입력 값에 따른 실시간 서빙을 수행하고 응답 결과 를 리턴할 수 있다. 모델 서빙 서버는 모델의 추론 과정에서 필요한 특징들을 각각의 특징 정보를 저장하는 온/오프라인 스토 어(160, 170)로 제공받을 수 있으며 실시간 서빙의 경우 온라인 특징 스토어로부터 특징 값들을 제공받고 모델에 입력할 수 있다. 배치 서빙의 경우 오프라인 특징 스토어에 저장된 특징 데이터들을 이용하여 예측 을 수행할 수 있다. 구체적으로 운영망 내에서 모델을 통한 예측이나 지속적 학습을 제공하는 학습 파이프라인은 지속적인 학습을 위한 실시간 서빙 또는 데이터간 특징의 비교를 위한 배치 서빙으로 구분될 수 있으며, 배치 서빙은 실시간 서빙을 통한 학습 주기에 비해 비교적 장기간 단위로 적재된 데이터를 받아 추론을 수행할 수 있다. 실시간 서빙은 모델 저장 후에 모델 서빙 서버를 통해 동적으로 결과를 생성해주는 것으로 사용자가 입력 값을 API로 요청하면 이에 대한 응답을 위한 추론과정이 바로 수행될 수 있으며, 응답을 위해 저장된 모델 을 개발망을 통해 모델 이미지로 생성하고, 푸시된 모델 이미지를 컨테이너로 실행함으로써 실시간 서빙이 가능하도록 한다. 반면 배치 서빙은 특정 데이터베이스에 계속해서 적재되고 있는 데이터를 매 주기 마다 끌어와서 배치 단위의 예측을 수행하고 예측의 결과 및 예측에 이용된 데이터를 모델 저장소에 적재시킴으로써 데이터 간 비교를 수행할 수 있다. 이때 본 실시예에 따른 배치 서빙과 실시간 서빙은, 서로 다른 주기에 따라 발생되는 트리거 값에 따라 구분하 여 동작될 수 있다. 예를 들어 지속적 학습을 위해 결정된 제1 주기에 따른 제1 트리거를 발생시킴으로써 수집 된 특징 데이터로 실시간 서빙이 수행될 수 있다. 또는 배치 추론을 위해 결정된 제2 주기에 따른 제2 트리거를 통해 수집된 데이터를 통해 상기 모델의 예측이 수행될 수 있다. 따라서, 모델 서빙 서버는 이상의 주기에 따라 학습 파이프라인 동작을 개시하도록 스케줄링을 수행하는 별도의 스케줄링 모듈을 포함할 수 있으며, 스케줄링 모듈은 실시간 또는 배치 서빙의 주기에 따라 학습 또는 예측을 수행할 수 있도록 트리거를 발생시킬 수 있다. 구체적으로 본 실시예에 따른 시스템은 산출되는 결과물을 이용하여 모델의 성능을 모니터링하고 상호 검증 결 과를 통해 지속적인 학습과 최신의 모델이 배포될 수 있도록 상술한 서빙 과정을 구분하여 학습 파이프라인 을 구성할 수 있다. 이하 도 4를 참고하여 본 실시예에 따른 학습 파이프라인의 구성에 대하여 보다 상세히 설명한다. 도 4를 참고하면 학습 파이프라인은 학습을 위한 데이터들을 정비하고 특징(feature) 데이터들을 추출하는 특징 데이터 파이프라인과, 추출된 특징 데이터들을 통해 학습을 수행하는 학습 모델 파이프라인으로 구성될 수 있다. 특징 데이터 파이프라인은 데이터의 추출과 전처리 과정을 수행할 수 있으며, 학습 모델 파이프라인 은 상술한 트리거로 결정되는 분기 조건에 따라 실시간 서빙에 따른 학습을 수행하거나 배치 단위의 예측을 수 행할 수 있다. 구체적으로 트리거에 따라 학습 파이프라인을 호출 시 지속적 학습(Continuous Training, CT) 여부를 결정 하는 파라미터를 함께 호출할 수 있다. 트리거에 포함되는 파라미터는 모델 실행 유형코드(mdel_exec_type)로 'PREDICTION' 또는 'CT' 라는 스트링 값으로 정의될 수 있으며 스트링 값에 따라 학습 파이프라인은 학습 모델 파이프라인 내 학습 또는 예측 과정을 수행할 수 있다. 본 실시예에서 학습 파이프라인은 학습 또는 예측 과정에서 공통적으로 이용되는 데이터 가공 과정을 별도 의 특징 데이터 파이프라인으로 구분하여 구성함으로써 코드의 중복을 줄이고 분기 조건에 따라 효율적으 로 리소스들을 활용할 수 있도록 한다. 구체적으로 공통적으로 수행되는 특징 데이터 파이프라인에 대하여 도 5를 참고하여 설명한다. 도 5를 참고하면, 특징 데이터 파이프라인은 데이터 추출부(132-1)에서 데이터를 추출하는 것으로 프로세 스가 실행될 수 있다. 본 실시예에 따른 데이터 추출부(132-1)에서 추출되는 데이터는 학습을 위한 비정형 데이터와 비정형 데이터에 대한 타겟 값들을 포함할 수 있다. 모델을 학습시키기 위한 데이터를 추출하기 위하여 데이터 추출부(132-1)는 구조화된 질의어로 SQL(Structured Query Language), Python, Spark과 같은 언어를 이용하여 추출 과정을 처리할 수 있으며, 이상의 언어로 작성된 파일로 소스 데이터를 데이터 추출부에 주입하면 순차적으로 데이터의 추출 작업을 수행한다. 이때, 실시간 서 빙으로 발생한 데이터는 별도의 추출과정 대신 자동으로 온라인 특징 스토어에 저장될 수 있다. 다음 데이터 검증부(132-2)는 추출된 데이터들의 분포를 이용하여 이전의 데이터들의 분포와 기준 이상의 차이 를 보이는 경우, 성능을 검증할 필요가 있는 것으로 판단하고 이전 데이터로 학습된 모델을 재 학습 시킬 수 있 다.예를 들어 아래의 수학식을 통해 산출된 지수(Population Stability Index, PSI)를 통하여 특정 시점에서 수집 된 데이터의 특징 간 차이를 수치적으로 산출할 수 있으며, 설정된 기준 값 보다 지수가 큰 경우에는 모델의 성 능에 영향을 미치는 유의미한 데이터의 변화가 발생한 것으로 판단할 수 있다. [수학식]"}
{"patent_id": "10-2022-0135831", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "(여기서 %E:이전시점 데이터 구성비, %O:현재시점 데이터 구성비) 또는 데이터 검증부(132-2)는 데이터의 통계적 특징을 산출하고 데이터의 적정성을 판단하는 것도 가능하다. 예 를 들어, 통계적 특징으로 계수(count), 평균(mean), 표준편차(std), 최소값(min), 각 변위 별 분포 값(25%, 50%, 75%) 또는 최대값(max)을 확인하여 상호 데이터의 특징을 비교할 수 있다. 이때, 통계적 특징 간 차이가 한계치를 넘거나 산출된 지수로 PSI 값이 기준 값(예를 들어 0.25)를 초과하면 데 이터의 변화가 상당히 크다고 확인할 수 있으며, 데이터 정합성에 대한 이슈가 있다고 판단하고 별도의 알람 메 시지를 분석가에게 전송하는 것도 가능하다. 본 개시의 일 실시예에 따르면, 학습된 모델을 통계적 특징에 따라 동적으로 학습시키는 것 외에 분석가를 통해 확인을 수행하고 수기입력에 따라 학습을 수행하거나 기존의 학습 된 모델을 지속적으로 이용하도록 결정할 수 있다. 다음으로, 데이터 전처리부(132-3)는 검증된 데이터를 전처리 한다. 데이터의 전처리는 머신러닝 모델의 입력을 위해 데이터들을 벡터화하는 임베딩 과정으로 수행될 수 있다. 임베딩 과정 역시 데이터 처리를 위한 프로그래 밍 언어로 작성된 머신러닝 모델 프레임워크에서 원핫 인코딩(One-Hot Encoding)과 같은 벡터화 작업을 통해 수 행될 수 있으며, 임베딩 처리에 필요한 코드파일을 읽어, 데이터를 임베딩 처리할 수 있다. 이상의 과정을 통해 전처리된 데이터는 학습 파이프라인에서 분기 조건에 따라 학습을 수행하거나 예측을 수행하는데 이용될 수 있다. 도 6을 참고하면 본 실시예에서는 지속적 학습(CT) 여부를 결정하는 파라미터에 따라 학습 또는 예측이 수행될 수 있다. 일 실시예에 따르면, 도 6에서 지속적 학습 여부를 결정하는 파라미터가 학습을 위한 파라미터인 경우, 학습 모 델 파이프라인은 모델을 학습시키기 위한 환경을 만들고 모델 학습을 위한 코드를 실행하는 것으로 수행될 수 있다. 본 실시예에서 학습 모델 파이프라인은 상술한 시스템의 망 분리에 따라 코드의 실행 전에 모델을 개발한 환경(ML모델 프레임워크)의 의존성(dependency)을 미리 확인하고, 준비된 도커 기초 이미지를 바탕으로 개발망 에서 해당 모델의 모델 이미지를 모델 생성 서버에서 생성하는 것을 전제로 수행될 수 있다. 이어서 학습 모델 파이프라인은 개발망에서 생성 후 푸시된 모델 이미지를 제공받고 운영망 내의 모델 서빙 서버 상의 컨테이너로 모델 이미지를 실행하는 것으로 수행될 수 있다. 구체적으로 모델 학습부(134-1)는 컨테이너를 통해 모델 이미지 내 모델 학습 코드를 실행하고 학습된 모델 파 일과 평가 관련 데이터를 아티팩트로 지정된 모델 저장소에 저장할 수 있다. 다음 모델 평가부(134-2)는 모델의 학습에 따른 성능지표를 산출할 수 있다. 본 실시예에서 학습 과정은 실시간 서빙 또는 단기의 배치 서빙을 통해 수행되며, 따라서 모델 평가부(134-2)는 수집된 특징 값과 특징 값에 대한 모델의 추론 시 정답(Ground Truth)을 정의하는 타겟 값을 기초로 정확도(accuracy), 구간 별 향상도(lift), 모 델이 True라고 분류한 것 중에서 실제 True인 것의 비율(정밀도(Precision)), 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율(재현율(Recall))과 같은 성능지표를 학습 과정에서 산출할 수 있다. 즉 모델 평가부(134-2)는 이전 모델의 학습 과정에서 산출된 성능지표들이 저장된 모델 저장소에서 모델 평가 데이터를 불러와 모델의 성능을 비교하고, 성능지표들을 함께 모델 저장소에 저장할 수 있다. 모델 평가부(134-2)는 평가 결과 지속적 학습 과정에서의 이전 모델 또는 모델 저장소에 저장된 기존 모델의 성 능지표가 우수하면, 분석가에게 알람을 발송할 수 있으며 분석가의 수기 판단에 따라 학습된 모델을 배포할 수 있도록 한다. 또한, 성능을 비교하는 것 외에 다른 데이터 그룹에서의 적적성을 보이는지를 판단하기 위해 모델 검증부(134- 3)는 모델의 유효성을 검증할 수 있다. 모델 검증부(134-3)는 모델의 학습기간과 다른 예측기간을 두어 모델의 성능이 실시간 서빙 외의 다른 기간의 데이터에도 신뢰할 만한 성능을 보이는 지의 적정성을 판단하는 OOT(Out Of Time) 테스트 또는 실험군과 대조군 을 구분하여 적정성을 판단하는 AB테스트를 이용하여 모델의 유효성을 판단한다. 이상의 과정을 통해 유효성이 검증된 모델은 검증 결과와 함께 모델 저장소에 저장되며, 배포 여부의 의사 결정에 따라 모델 파일과 학습 결과물들은 모델 생성 서버로 제공될 수 있다. 반면, 도 6에서 지속적 학습(CT) 여부를 결정하는 파라미터가 예측을 위한 파라미터인 경우는 예측 과정으로 분 기될 수 있다. 예측 과정에서, 학습 모델 파이프라인 내 예측부(134-4)는 모델을 가지고 스코어 값을 만들어 낼 수 있다. 예측 과정에서도 상술한 학습 과정과 동일한 컨테이너 환경이 필요하며, 학습 과정과 동일하게 모델 생성 서버 를 통해 모델 이미지가 생성될 수 있다. 구체적으로 모델 생성 서버는 상술한 바와 같이 배치 서빙 코드를 이용하여 모델 이미지를 생성하며, 예측부(134-4)는 생성된 모델 이미지에 정의된 배치 단위의 추론을 위한 파라미터들을 이용하여 학습 모델 파이프라인 내에서 예측을 수행할 수 있다. 예측부(134-4)는 미리 결정된 기간(예를 들어, 일/주/월 또는 시간/분) 동안 수집된 데이터에 대하여 배치 단위 의 추론을 수행할 수 있으며, 수집된 데이터를 모델 저장소로부터 바로 불러와서 모델에 입력하고 결과를 출력할 수 있다. 배치 단위의 추론 결과로 예측 값은 모델 저장소에 저장될 수 있다. 성능 모니터링부(134-5)는 예측부(134-4)에서 출력된 예측 값과 모델 저장소에 저장된 모델을 학습시켰던 이전 시점 데이터를 이용하여 데이터의 스코어별 분포를 산출함으로써 성능을 판단할 수 있다. 본 실시예에서 학습 모델 파이프라인은 성능 모니터링부(134-5)를 파이프라인 내에 포함하여 예측부(134- 4)에서 출력된 예측 값으로 모델의 성능 이상을 직접 모니터링할 수 있도록 한다. 즉, 예측부(134-4)에서 배치 단위의 예측 값이 출력되면 성능 모니터링부(134-5)가 출력된 예측 값을 이용하여 모델 성능과 관련된 데이터를 산출함으로써 동일 파이프라인 내에서 모델의 성능을 바로 모니터링할 수 있도록 한다. 구체적으로 성능 모니터링부(134-5)는 모델 성능과 관련된 데이터로 이전 시점의 예측 값의 분포와 현재 예측 과정에서의 배치 단위의 예측 값의 분포 간의 통계적 특징으로 PSI를 계산함으로써 모델의 성능을 모니터링 할 수 있다. 일 실시예에 따르면, 예측 값은 연속형 파라미터일 수 있기 때문에 구간화(Binning)될 수 있으며, 0에서 1사이 의 확률 값이 산출되는 경우 10개의 세그먼트로 나누어 분포의 비교를 수행하는 방식으로 구간화가 수행될 수도 있다. 이상 본 발명에 따른 학습 또는 예측을 위한 학습 모델 파이프라인은 상술한 바와 같이 개발망에서 생 성된 모델의 모델 이미지를 컨테이너로 실행함으로써 수행되며, 학습 모델 파이프라인과 컨테이너를 운영 하는 모델 서빙 서버는 상호 연계하여 학습 모델 파이프라인 내 프로세스들을 수행할 수 있다. 도 7은 본 발명의 일 실시예에 따른 학습 파이프라인에 따라 모델 서빙을 위한 운영망의 구성을 나타낸 예 시도이다. 상술한 상호 연계과정은 도 7을 통해 구체적으로 설명된다. 도 7을 참고하면, 학습 파이프라인의 결과 중 특징 데이터들은 온라인/오프라인 특징 스토어(160, 170)에 저장될 수 있으며, 모델 서빙 서버에서 서빙에 필요한 것으로 요청한 특징 값들은 온라인/오프라인 특징 스토어(160, 170)에서 직접 추출되어 모델 서빙 서버로 제공될 수 있도록 함으로써 모델 서빙 서버가 보다 효율적으로 작업을 수행할 수 있도록 한다. 본 실시예에서 온라인/오프라인 특징 스토어(160, 170)는 학습과 서빙에 사용되는 모든 특징 데이터들을 모아둔 일종의 저장소로서 대용량의 배치 서빙과 실시간 서빙을 모두 지원할 수 있다. 예를 들어 온라인 특징 스토어 는 실시간 서빙을 위해 최신화 된 데이터를 저장하며, 채널 서버로부터 외부의 응답 결과들 로부터 추출된 특징 값을 모델 서빙 서버로 제공함으로써 실시간 서빙에 의한 모델의 추론 결과를 출력할 수 있도록 한다. 또한, 모델 서빙 서버에서 생성되는 데이터들은 메시지 큐(예를 들어 Kafka) 및 스트림 모듈을 통해 순차적으로 스트리밍 될 수 있으며 처리 결과들은 모니터링부를 통해 통계적 지표로 모니터링된다. 모니터링 결과는 대시보드를 통해 사용자 인터페이스 형태로 관리자에게 표시될 수 있다. 이하에서는 학습 파이프라인의 결과물로 모델 및 모델에 대한 아티팩트들이 관리되는 모델 저장소의 구성에 대하여 도 8을 참고하여 보다 상세히 설명한다. 도 8을 참고하면, 본 실시예에서 모델 저장소는 모델 파일 자체 외에 모델의 성능 모니터링에 대한 결과, 학습 결과, 학습 파이프라인의 배포 이력 등을 관리할 수 있으며 다양한 메타 데이터와 결과물들을 아티팩 트로 저장할 수 있다. 모델 등록부는 모델의 버전, 모델 파일들을 관리하고 모델 모니터링부는 모델의 성능 모니터링 결과 를 저장할 수 있다. 학습 결과부는 모델의 학습에 따른 결과를 다양한 성능지표로 저장함으로써 성능의 저하 또는 학습된 모델 이 기존 모델 이상의 성능을 보이는 경우 교체하거나 롤백할 수 있도록 한다. 배포 이력 관리부는 모델 및 파이프라인의 배포를 시간을 기준으로 관리함으로써 특정 주기에 따라 파이프라인을 수행하거나 모델의 학습 주기를 스케줄링 할 수 있도록 한다. 그 외 모델의 메타데이터는 모델 메타데이터 DB에 저장되며, 학습에 따른 결과물들은 모델 아티팩트 DB에 각각 저장될 수 있다. 또한, 모델 서빙 서버는 실시간 서빙 또는 배치 서빙을 결정된 주기에 따라 학습 파이프라인을 동작을 개 시할 수 있도록 트리거를 발생시키는 스케줄링 모듈(미도시)을 포함할 수 있으며, 스케줄링 모듈은 실시간 또는 배치 서빙의 주기에 따라 학습 또는 예측을 수행하도록 트리거를 발생시킬 수 있다. 이하, 도 9 내지 10을 참고하여 본 실시예에서 사용자 요청에 따른 모델 서빙 서버의 실시간 서빙 과 정에 대하여 보다 상세히 설명한다. 모델 서빙 서버는 도커 기반의 이미지를 컨테이너로 실행하는 모델 서빙 도커를 포함한다. 모델 서빙 도커는 컨테이너로 실행 중인 모델 서비스를 내부 서버의 IP 주소와 함께 디스커버리 서버(32 4)로 먼저 등록을 요청하고(S2) 디스커버리 서버는 응답을 제공한다(S4). 반면 내부 서버의 자원 관리를 위해 컨테이너의 실행을 중지하는 경우 모델 서빙 도커는 삭제를 요청할 수 있으며(S2) 디스커버리 서버 는 삭제 후 응답을 다시 모델 서빙 도커에 제공한다(S4). 게이트웨이는 사용자의 요청에 대응하는 모델 서비스를 제공하는 내부 서버의 IP 주소를 디스커버리 서버를 통해 요청하고 콘픽 서버를 통해 서버 설정 값들을 참조하여 이용 가능한 컨테이너를 통 해 서빙을 수행한다. 콘픽 서버는 모델 서빙을 위한 서비스의 환경 설정 정보를 별도로 관리하며 서빙 시 저장된 설정 정보를 모델 서빙 도커로 내려 줌으로써 추론을 수행할 수 있도록 한다. 구체적으로 사용자가 운영망에 접속하여 서빙을 위한 입력 값을 API로 요청하면(S10) 먼저 로드밸런싱 (Load Balancing, LB) 모듈은 각 서버의 점유 정도, 유휴 자원을 고려하여 게이트웨이에 사용자 의 요청을 입력 값과 함께 전달한다(S12). 게이트웨이는 사용자의 요청에 대응하는 모델 서비스를 서빙하는 내부 서버의 주소를 디스커버리 서 버에 요청하고(S16) 응답을 수신한다(S18). 게이트웨이는 응답에 따라 서비스의 존재를 확인하고 (S20) 만약 대응하는 서비스가 존재하지 않는 경우에는 로드밸런싱 모듈을 통해 에러를 응답하며(S23), 존 재하는 경우에는 응답 주소를 통해 모델 서빙 서버에 추론을 요청한다(S22). 모델 서빙 서버는 모델 서빙 도커의 컨테이너를 통해 대응되는 모델 이미지를 실행함으로써 추론을 수행하며 추론에 필요한 추가적인 특징 정보들은 요청하여(S24) 온라인 특징 스토어를 통해 제공받을 수 있다(S26). 추론 요청에 대한 응답 결과는 게이트웨이로 제공되고(S28), 게이트웨이는 로드밸런싱 모듈을 통해 사용자에게 응답으로 제공할 수 있다(S30, S32). 이하, 본 발명의 일 실시예에 따른 신경망 모델의 학습을 수행하는 모델 서빙 서버의 구체적인 하드웨어 구현에 대하여 설명한다. 도 11을 참조하면, 본 발명의 몇몇 실시예들에서 모델 서빙 서버는 컴퓨팅 장치의 형태로 구현될 수 있다. 모델 서빙 서버를 구성하는 각각의 모듈 중 하나 이상은 범용 컴퓨팅 프로세서 상에서 구현되며 따라서 프 로세서(processor), 입출력 I/O, 메모리 (memory), 인터페이스(interface) 및 버스(314, bus)를 포함할 수 있다. 프로세서, 입출력 장치, 메모리 및/또는 인터페이스는 버스(31 4)를 통하여 서로 결합될 수 있다. 버스는 데이터들이 이동되는 통로(path)에 해당한다. 구체적으로, 프로세서는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit), 마이크로프로세서, 디지털 신호 프로세스, 마이크로컨트롤 러, 어플리케이션 프로세서(AP, application processor) 및 이들과 유사한 기능을 수행할 수 있는 논리 소자들 중에서 적어도 하나를 포함할 수 있다. 입출력 장치는 키패드(keypad), 키보드, 터치스크린 및 디스플레이 장치 중 적어도 하나를 포함할 수 있다. 메모리 장치는 데이터 및/또는 프로그램 등을 저장할 수 있다. 인터페이스는 통신 네트워크로 데이터를 전송하거나 통신 네트워크로부터 데이터를 수신하는 기능을 수행 할 수 있다. 인터페이스는 유선 또는 무선 형태일 수 있다. 예컨대, 인터페이스는 안테나 또는 유무 선 트랜시버 등을 포함할 수 있다. 메모리 는 프로세서의 동작을 향상시키되, 개인정보의 보호를 위 한 휘발성의 동작 메모리로서, 고속의 디램 및/또는 에스램 등을 더 포함할 수도 있다. 또한, 메모리 내에는 여기에 설명된 일부 또는 모든 모듈의 기능을 제공하는 프로그래밍 및 데이터 구성을 저장한다. 예를 들어, 상술한 학습 방법의 선택된 양태들을 수행하도록 하는 로직을 포함할 수 있다. 메모리 에 저장된 상술한 학습 방법을 수행하는 각 단계를 포함하는 명령어들의 집합으로 프로그램 또는 어플리케이션을 로드하고 프로세서가 각 단계를 수행할 수 있도록 한다. 예를 들어, 사용자의 서빙 요청에 대응 을 위하여 모델 이미지 내 정의된 프로세스를 가상화된 OS 상에서 컨테이너로 실행하는 동작, 상기 모델의 입력 값으로 특징 데이터를 수신하는 동작, 및 상기 컨테이너로 실행된 상기 프로세스 내에서 상기 수신된 특징 데이 터와 사용자의 입력 값에 대한 상기 모델의 추론 값을 출력하는 동작 등이 포함된 컴퓨터 프로그램이 프로세서 에 의해 각 수행될 수 있다. 나아가, 모델 생성 서버 역시 컴퓨팅 장치의 형태로 구현될 수 있으며, 컴퓨팅 장치 내 메모리에는 실시간 서빙 코드로 수신한 실시간 서빙 아티팩트와 모델 파일을 이용하여, 컨테이너 생성에 필요한 다양한 설정 값들 을 이미지화 하는 동작 및 배치 서빙 코드로 수신한 배치 서빙 아티팩트를 추가로 이용하여 모델 이미지를 생성 하는 동작 등이 포함된 컴퓨터 프로그램이 저장될 수 있으며 컴퓨팅 장치 내 프로세서에 의해 각 동작들이 수행 될 수 있다. 이상 여기에 설명되는 다양한 실시예는 예를 들어, 소프트웨어, 하드웨어 또는 이들의 조합된 것을 이용하여 컴 퓨터 또는 이와 유사한 장치로 읽을 수 있는 기록매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 여기에 설명되는 실시예는 ASICs (application specific integrated circuits), DSPs (digital signal processors), DSPDs (digital signal processing devices), PLDs (programmable logic devices), FPGAs (field programmable gate arrays, 프로세서(processors), 제어기(controllers), 마이크로 컨 트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행을 위한 전기적인 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시예들이 제어 모듈 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시예들은 별도의 소프트웨어 모 듈들로 구현될 수 있다. 상기 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 적절한 프로그램 언어로 쓰여진 소프트웨어 어플리케이션으로 소프트웨어 코드가 구현될 수 있 다. 상기 소프트웨어 코드는 메모리 모듈에 저장되고, 제어모듈에 의해 실행될 수 있다. 이상의 설명은 본 발명의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 발명이 속하는 기술 분야에 서 통상의 지식을 가진 자라면 본 발명의 본질적인 특성에서 벗어나지 않는 범위 내에서 다양한 수정, 변경 및 치환이 가능할 것이다. 따라서, 본 발명에 개시된 실시 예 및 첨부된 도면들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명 하기 위한 것이고, 이러한 실시 예 및 첨부된 도면에 의하여 본 발명의 기술 사상의 범위가 한정되는 것은 아니 다. 본 발명의 보호 범위는 아래의 청구 범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기 술 사상은 본 발명의 권리 범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2022-0135831", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 모델 서빙 방법의 흐름을 나타낸 예시도이다. 도 2는 모델 서빙 방법을 제공하는 시스템 구성을 나타낸 예시도이다. 도 3은 본 발명의 일 실시예에 따라 서빙되는 모델을 생성하는 서버의 동작을 나타낸 예시도이다. 도 4는 본 발명의 일 실시예에 서빙 되는 모델의 학습을 위한 파이프라인을 나타낸 예시도이다. 도 5 내지 6은 본 발명의 일 실시예에 따른 학습 파이프라인의 세부 구성을 나타낸 예시도이다. 도 7은 본 발명의 일 실시예에 따른 학습 파이프라인에 따라 모델 서빙을 위한 운영망의 구성을 나타낸 예 시도이다. 도 8은 본 발명의 일 실시예들에 따라 서빙되는 모델이 저장되는 모델 저장소의 구성을 나타낸 예시도이다. 도 9는 본 발명의 일 실시예에 따른 모델 서빙 서버의 구성을 나타낸 예시도이다. 도 10은 본 발명의 일 실시예에 따른 모델 서빙 서버를 포함하는 운영망의 동작을 나타낸 예시도이다. 도 11은 본 발명의 일 실시예에 따른 모델 서빙 서버의 컴퓨팅 장치 형태로의 구현을 나타낸 예시도이다."}
