{"patent_id": "10-2022-0039405", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0140756", "출원번호": "10-2022-0039405", "발명의 명칭": "영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법", "출원인": "한국전자통신연구원", "발명자": "이형극"}}
{"patent_id": "10-2022-0039405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법으로서, 장면을 구성하는 샷을 분할하고 일부를 샘플링하는 단계;상기 샘플링된 샷을 분류하는 단계;상기 장면 내에 등장인물들의 대화 내용 중 장소/시간/날씨 관련 키워드를 추출하는 단계;분류기 결과의 샷 구도에 따라 장소/시간/날씨를 식별하는 단계; 및상기 추출한 키워드와 상기 식별 결과를 종합하여 장면 단위의 최종 장소/시간/날씨 정보를 식별하는 단계를 포함하는공간 및 시간 정보를 식별하는 방법."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법이 제공된다. 상기 방법은, 장면을 구성하는 샷을 분할하 고 일부를 샘플링하는 단계; 상기 샘플링된 샷을 분류하는 단계; 상기 장면 내에 등장인물들의 대화 내용 중 장 소/시간/날씨 관련 키워드를 추출하는 단계; 분류기 결과의 샷 구도에 따라 장소/시간/날씨를 식별하는 단계; 및 상기 추출한 키워드와 상기 식별 결과를 종합하여 장면 단위의 최종 장소/시간/날씨 정보를 식별하는 단계를 포 함할 수 있다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법에 관한 것이다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "컴퓨터 비전 기술 분야의 발전에 기초하여, 인공지능 기술을 이용하여 영상 콘텐츠 내의 정보를 자동으로 인식 하는 기술, 즉 인공지능 기술 기반으로 영상 콘텐츠를 분석하여 해당 콘텐츠를 구성하는 장소, 시간, 객체, 인 물 및 행위 등의 종류를 자동으로 도출할 수 있는 기술이 발전하고 있다. 그런데, 장소, 시간 등 이미지 정보 중 배경에 있는 정보를 자동으로 추출하기 위해 인공지능 기술을 활용하는 경우에는 지도학습 기반 학습 방법론 을 이용하는 경우가 다수이며, 해당 방법론에 대한 학습 데이터는, 장소, 시간 및 날씨에 해당하는 정보만을 포 함하고 이외의 다른 객체를 포함하지 않도록 구성된다. 그러나, 방송 콘텐츠의 경우, 인물의 캐릭터, 인물 간 관계 등이 이미지 내에서 중요한 비중을 차지하고 있기 때문에, 기존의 인공지능 방법론을 이용하여 방송 콘텐츠 내에 포함된 공간 및 시간 정보를 확인하기에는 어려 운 점이 많다. 특히, 다양한 영상 콘텐츠가 대량으로 제작 및 방송/서비스되고 있는 상황에서, 기존 방송/미디 어에서 인력 기반으로 분석 및 편집하여 부가서비스를 제공하는 방법은 한계가 있다. 따라서, 이와 같은 한계를 극복하고, 인공지능 기술을 활용하여 영상 콘텐츠 내에 포함된 장소, 시간, 객체, 인물 및 행위의 종류를 자동 으로 분류/식별할 수 있는 기술에 대한 연구가 활발하다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 과제는, 증가하는 방송 또는 OTT(Over the top) 콘텐츠를 인공지능 기술에 기반하여 공간 및 시간 정보를 자동 분석하여 부가 서비스로 활용할 수 있도록 하기 위해 추가 정보를 획득할 수 있는, 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법을 제공하는 것이다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른 공간 및 시간 정보를 식별하는 방법은, 장면을 구성하는 샷을 분할하고 일부를 샘 플링하는 단계; 상기 샘플링된 샷을 분류하는 단계; 상기 장면 내에 등장인물들의 대화 내용 중 장소/시간/날씨 관련 키워드를 추출하는 단계; 분류기 결과의 샷 구도에 따라 장소/시간/날씨를 식별하는 단계; 및 상기 추출한 키워드와 상기 식별 결과를 종합하여 장면 단위의 최종 장소/시간/날씨 정보를 식별하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시 예들에 따르면, 방송 콘텐츠를 비롯한 영상 콘텐츠를 자동으로 분석함으로써 분석을 위한 시간, 자원 등을 절감할 수 있고, 분류된 정보를 통해 동일한 장소 별로 콘텐츠를 관리하거나, 유사한 시간대 별로 콘 텐츠를 편집하는 등 다양한 미디어 관련 부가서비스에 대한 활용성을 높일 수 있다."}
{"patent_id": "10-2022-0039405", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해 서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 명세서 및 청구범위 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재 가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재된 \"...부\", \"...기\", \"모듈\" 등의 용어는 본 명세서에서 설명되는 적어도 하나의 기능이나 동작을 처리할 수 있는 단위를 의미할 수 있으며, 이는 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 최근 다양한 OTT 영상 콘텐츠가 제작되고, 이를 기반으로 한 후속 서비스 또한 각광받고 있다. 본 발명의 일 실 시 예는, 영상 콘텐츠의 신규 제작뿐 아니라 기존에 제작된 영상 콘텐츠를 활용하여 부가적으로 다양한 맞춤형 서비스 제공을 위해, 영상 콘텐츠에 포함된 정보를 지능적으로 추출할 수 있는 영상 콘텐츠로부터 공간 및 시간 정보를 샷의 종류에 따라 식별하는 방법을 제공할 수 있다. 또한, 본 발명의 일 실시 예는, 방송, OTT, 유튜브 등 다양한 영상 콘텐츠를 이용하여 방송 콘텐츠에 특화된 공 간 및 시간 정보를 판별할 수 있도록, 장면 간에는 공간과 시간 정보가 변하지 않는다는 가정 하에, 공간 및 시 간 정보를 잘 식별할 수 있는 샷을 중심으로 샘플링을 수행하여 방송 콘텐츠의 장면 단위로 공간 및 시간 정보 를 식별할 수 있는 방법을 제공할 수 있다. 영상 콘텐츠는 프레임, 샷 및 장면으로 이루어질 수 있다. 영상 콘텐츠는 카메라를 통해 일정 시간 동안 연속적 으로 취득된 이미지를 모아서 제작될 수 있다. 여기서, '프레임'은 영상 콘텐츠에서의 각 이미지를 의미하고, '샷'은 카메라 녹화가 시작된 후 종료될 때까지 연속적으로 촬영한 구간을 의미하며, '장면'은 동일한 장소에 서 촬영한 샷들의 집합을 의미할 수 있다. 장면 단위 콘텐츠가 가진 공간 및 시간 정보는 동일하다고 가정할 수 있으므로, 장면 내 몇몇의 샷이 가진 공간 및 시간 정보를 통해 장면이 가진 공간 및 시간 정보를 유추하여 장 면 인식(Scene Recognition)을 수행할 수 있다. 공간 정보와 시간 정보를 기반으로 만들어진 이미지 데이터를 학습하여 식별을 수행하는 인공지능 기반 기술과 달리, 방송 콘텐츠의 경우에는 공간 정보와 시간 정보가 등장인물에 의해 왜곡되는 경우가 많다는 점에서, 상기 의 방식으로 학습한 인공지능 기술의 성능은, 방송 콘텐츠가 아닌 일반적인 상황에서 객체 인식을 수행하는 성 능 수준에 미치기 어렵다. 카메라로 이미지를 취득할 때의 구도(샷의 종류)에 따라 공간 및 시간 정보를 직접적으로 유추하기 어려운 경우 도 있다. 이를테면 드라마 장르의 콘텐츠 중 가장 많은 비중을 차지하는 인물 샷에서는 배경에 초점이 맞지 않는 경우가 많기 때문에, 인물 샷 위주인 상황에서 공간 및 시간 정보를 식별하기는 어려울 수 있다. 인물 주변 의 배경이 빠르게 변화하는 트래킹 샷의 경우에도 공간 및 시간 정보를 식별하기 어려울 수 있다. 이와 다르게, 고정 샷 또는 롱 샷의 경우에는 등장인물 대비 배경 정보가 정적이기 때문에 장소 및 시간 정보를 식별하기 용 이할 수 있다. 한편, 방송 콘텐츠의 등장인물들이 나누는 대화 내용을 바탕으로 공간 및 시간 정보를 추측할 수도 있다. 이를 테면 물건 가격을 물어보고 가격을 지불하는 대화가 이루어지는 경우 해당 대화가 이루어지는 공간은 상점이라 고 유추할 수 있고, 미세먼지에 대한 대화가 진행되는 경우 현재 날씨는 흐리다고 추측할 수 있다. 이와 같이, 콘텐츠의 영상 정보와 콘텐츠 속 등장인물 간 대화 내용을 통해 장소/시간/날씨 정보를 식별할 수 있으며, 이하에서 그 상세한 내용에 대해 설명하도록 한다. 도 1은 본 발명의 일 실시 예에 따른 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법을 설명하기 위한 블록도이다. 도 1을 참조하면, 영상 콘텐츠는 장면/샷 분할 모듈에 입력되어 장면 및 샷 단위 프레임으로 분할될 수 있으며, 한 장면 내에 포함된 내용이 어떤 공간 및 시간 정보를 포함하고 있는지를 식별하는 과정은 크게 2 가 지 방식으로 처리될 수 있다. 첫 번째 방식으로, 장면 단위 정보 입력 모듈은 장면 단위로 정보를 입력받고, 구성 샷 분류기는 입력 된 정보 중 일정 비율로 장면을 구성한 샷을 샘플링한 후, 샘플링된 샷을 분류할 수 있다. 구체적으로, 구성 샷 분류기는 샘플링된 샷을 줌(인/아웃)샷, 롱샷/정지샷, 패닝샷/틸팅샷, 트래킹샷 및 인 물 샷(클로즈업샷, 바스트샷, 웨이스트샷, 니샷 등)으로 분류할 수 있다. 이 중 트래킹샷 및 인물 샷 은 배경 정보에 초점이 맞지 않는 경우가 많기 때문에, 줌(인/아웃)샷, 롱샷/정지샷 및 패닝샷/ 틸팅샷만 영상 기반 장소/시간/날씨 식별기에 입력된다. 영상 기반 장소/시간/날씨 식별기는 해당 식별기가 고려하는 모든 클래스에 대해 개별적으로 기대값을 출력할 수 있다. 두 번째 방식으로, 장면 단위 정보 입력 모듈은 장면 단위로 정보를 입력받고, 대화 키워드 기반 장소/시간 /날씨 식별기는, 장면 단위 정보 내에서 나누는 등장인물 간 대화 중 장소/시간/날씨 관련 키워드를 추출하 고 해당 장면의 공간 및 시간 정보를 식별함으로써 장면 단위 정보를 처리할 수 있다. 대화 키워드 기반 장소/ 시간/날씨 식별기는, 영상 기반 장소/시간/날씨 식별기와 마찬가지로, 해당 식별기가 고려하는 모든 클 래스에 대하여 개별적으로 기대값을 출력할 수 있다. 영상 기반 장소/시간/날씨 식별기로부터 출력되는 기대값과, 대화 키워드 기반 장소/시간/날씨 식별기 로부터 출력되는 기대값은 장소/시간/날씨 분류 모듈에 전달되어 구체적인 구현 목적에 따라 활용될 수 있 다. 구체적인 예를 들면, 장소, 시간 및 날씨 등 분류할 대상(또는 클래스)는 다음과 같이 정의될 수 있다. Place class = {class_1, class_2, ..., class_Np} Time class = {class_1, class_2, ..., class_Nt} Weather class = {class_1, class_2, ..., class_Nw} 여기서 Np는 장소 식별기가 분류하는 장소 클래스의 총 개수, Nt는 시간 식별기가 분류하는 시간 클래스의 총 개수, Nw는 날씨 식별기가 분류하는 날씨 클래스의 총 개수를 나타낸다. 만일 영상 기반 장소/시간/날씨 식별기로부터 장소 클래스에 대한 기대값이 {0.5, 0.01, ..., 0.001}로 출 력되고, 대화 키워드 기반 장소/시간/날씨 식별기로부터 장소 클래스에 대한 기대값이 {0.1, 0.4, ..., 0.0}으로 출력된 경우, 장소/시간/날씨 분류 모듈은 상기 2 가지 기대값에서 각 확률의 최대값을 선택함으 로써 장소 클래스를 최종적으로 분류할 수 있다. 예를 들어, Max{{0,5, 0.01, ..., 0.001}, {0.1, 0.4, ..., 0.0}} = {0.5, 0.4, ..., 0.001}로 계산하여, 첫 번째 장소 클래스 값으로 분류할 수 있다. 시간 정보 및 날씨 정보에 대해서도 이와 동일한 방식으로 분류를 진행할 수 있다. 도 2는 본 발명의 일 실시 예에 따른 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법을 설명하기 위한 흐름도이다. 도 2를 참조하면, 본 발명의 일 실시 예에 따른 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법은, 장면 을 구성하는 샷을 분할하고 일부를 샘플링하는 단계(S210), 샘플링된 샷을 분류하는 단계(S220), 장면 내에 등 장인물들의 대화 내용 중 장소/시간/날씨 관련 키워드를 추출하는 단계(S230), 분류기 결과의 샷 구도에 따라 장소/시간/날씨를 식별하는 단계(S240) 및 대화 키워드와 식별 결과를 종합하여 장면 단위의 최종 장소/시간/날 씨 정보를 식별하는 단계(S250)를 포함할 수 있다. 즉, 영상 콘텐츠에 포함된 장면 단위 이미지가 포함하고 있는 정보를 식별하는 데에 있어서, 샷 기반 영상 분석 방법과, 장면 내에서 이루어지는 대화의 키워드를 분석하는 방법을 기반으로 공간 및 시간 정보를 식별할 수 있 다. 이 때, 장면을 구성하는 전체 샷을 이용하지 않고 샘플링된 일부 샷만을 이용할 수 있다. 한편, 샘플링 샷 중에서 공간 및 시간 정보 파악을 위해 줌(인/아웃)샷, 롱샷/정지샷, 패닝샷/틸팅샷 등을 위주 로 공간 및 시간 정보를 식별하거나, 장면 단위 콘텐츠에서 나누는 대화를 분석하여 공간 및 시간 정보와 관련 된 키워드를 추출할 수 있다. 또한, 식별한 공간 및 시간 정보와, 추출한 키워드 정보를 종합하여 최종적으로 공간 및 시간 정보를 식별할 수 있으며, 이를 미디어 간 연관 관계에 활용할 수 있다. 도 3은 본 발명의 일 실시 예에 따른 카메라로 이미지 취득하는 상황을 설명하기 위한 도면이다. 도 3을 참조하면, 장면 단위 정보 입력 모듈에 대해 입력되는 샷을 예시한 것이다. 해당 예시의 장면은 등 장인물 간 통화를 나누는 장면으로 인물 샷(바스트샷) 위주의 샷으로 구성된 것이다. 따라서 인물이 등장한 이 미지의 배경정보는 초점이 맞지 않아 영상 기반 정보 공간 및 시간 정보를 명확하게 확인할 수는 없다. 이러한 경우, 등장인물 간 대화 내용 중 공간 및 시간 키워드를 추출하여 활용할 수는 있다. 도 4는 본 발명의 일 실시 예에 따른 카메라로 이미지 취득하는 상황을 설명하기 위한 도면이다. 도 4를 참조하면, 장면 단위 정보 입력 모듈에 대해 입력되는 샷을 예시한 것이다. 해당 예시의 장면은 정 지샷이나 롱샷 위주의 장면으로 인공지능 기반 영상 인식 기술로 공간 및 시간 정보를 확인할 수 있다. 또한 등 장인물 간 대화 내용 중 1번째 샷의 경우 '패스', '득점' 또는 '덥다' 등의 키워드를 통해서 해당 대화가 이루 어지는 공간 및 시간 정보를 함께 유추할 수 있다. 도 5는 본 발명의 일 실시 예에 따른 컴퓨팅 장치를 설명하기 위한 블록도이다. 도 5를 참조하면, 컴퓨팅 장치는 버스를 통해 통신하는 프로세서, 메모리, 사용자 인터페이 스 입력 장치, 사용자 인터페이스 출력 장치 및 저장 장치 중 적어도 하나를 포함할 수 있다. 컴퓨팅 장치는 또한 네트워크, 예컨대 무선 네트워크에 전기적으로 접속되는 네트워크 인터페이스(57 0)를 포함할 수 있다. 네트워크 인터페이스는 네트워크를 통해 다른 네트워크 엔티티와 신호를 송신 또는 수신할 수 있다. 프로세서는 AP(Application Processor), CPU(Central Processing Unit), GPU(Graphic Processing Unit) 등과 같은 다양한 종류들로 구현될 수 있으며, 메모리 또는 저장 장치에 저장된 명령을 실행하 는 임의의 반도체 장치일 수 있다. 프로세서는 도 1 내지 도 4와 관련하여 전술한 기능 및 방법들을 구현 하도록 구성될 수 있다. 메모리 및 저장 장치는 다양한 형태의 휘발성 또는 비 휘발성 저장 매체를 포함할 수 있다. 예를 들 어, 메모리는 ROM(read-only memory) 및 RAM(random access memory)를 포함할 수 있다. 본 발명의 일 실시 예에서 메모리는 프로세서의 내부 또는 외부에 위치할 수 있고, 메모리는 이미 알려진 다양한 수단을 통해 프로세서와 연결될 수 있다. 또한, 본 발명의 실시 예들에 따른 공간 및 시간 정보를 식별하는 방법은 컴퓨팅 장치에서 실행되는 프로그 램 또는 소프트웨어로 구현될 수 있고, 프로그램 또는 소프트웨어는 컴퓨터로 판독 가능한 매체에 저장될 수 있 다. 또한, 본 발명의 실시 예들에 따른 공간 및 시간 정보를 식별하는 방법은 컴퓨팅 장치와 전기적으로 접속될 수 있는 하드웨어로 구현될 수도 있다. 이제까지 설명한 본 발명의 실시 예들에 따르면, 방송 콘텐츠를 비롯한 영상 콘텐츠를 자동으로 분석함으로써 분석을 위한 시간, 자원 등을 절감할 수 있고, 분류된 정보를 통해 동일한 장소 별로 콘텐츠를 관리하거나, 유 사한 시간대 별로 콘텐츠를 편집하는 등 다양한 미디어 관련 부가서비스에 대한 활용성을 높일 수 있다. 이상에서 본 발명의 실시 예들에 대하여 상세하게 설명하였지만 본 발명의 권리 범위는 이에 한정되지 않으며, 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자의 여러 변형 및 개량 형태 또한 본 발명의 권리 범위에 속한다."}
{"patent_id": "10-2022-0039405", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법을 설명하기 위한 블록도이다. 도 2는 본 발명의 일 실시 예에 따른 영상 콘텐츠로부터 공간 및 시간 정보를 식별하는 방법을 설명하기 위한 흐름도이다. 도 3은 본 발명의 일 실시 예에 따른 카메라로 이미지 취득하는 상황을 설명하기 위한 도면이다. 도 4는 본 발명의 일 실시 예에 따른 카메라로 이미지 취득하는 상황을 설명하기 위한 도면이다. 도 5는 본 발명의 일 실시 예에 따른 컴퓨팅 장치를 설명하기 위한 블록도이다."}
