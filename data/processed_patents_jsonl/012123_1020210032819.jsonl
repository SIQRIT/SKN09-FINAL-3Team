{"patent_id": "10-2021-0032819", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0128142", "출원번호": "10-2021-0032819", "발명의 명칭": "강화학습을 이용한 입고물품의 적치순서 최적화 방법", "출원인": "서울대학교산학협력단", "발명자": "우종훈"}}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "항 내지 청구항 9항 중 어느 한 항에 기재된 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 이용하여 입고되는 물품의 적치계획을 수립하는 적치계획 수립부; 및 크레인을 포함하는 이송수단을 포함하여, 상기 적치계획 수립부에 의해 수립된 적치계획에 따라 상기 물품의 적치를 수행하도록 이루어지는 물품이송부를 포함하여 구성되는 것을 특징으로 하는 물품 적치 시스템."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서, 상기 학습단계는, 상기 에이전트의 행동(Action)에 따라 상기 환경으로부터 피드백되는 상태(State)와 보상(Reward)에 근거하여상기 에이전트의 인공신경망(neural network)의 가중치를 업데이트하는 마르코프 결정과정(Markov decisionprocess ; MDP)에 기반한 강화학습 알고리즘에 따라 학습이 이루어지는 처리가 수행되며, 상기 상태는 상기 에이전트가 행동을 결정하는 시점에서의 상기 물품의 입고현황과 적치현황으로 정의되고, 상기 행동(Action)은 입고된 상기 물품을 적치할 위치를 결정하는 것으로 정의되며, 상기 보상(Reward)은 크레인을 포함하는 상기 물품의 이송수단에 대한 사용횟수를 기준으로 정의되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2항에 있어서, 상기 학습단계에서, 상기 입고현황은 적치될 위치가 아직 결정되지 않은 채로 대기하고 있는 물품에 대한 정보이고, 상기 적치현황은 이미 적치장에 적치되어 있는 물품에 대한 정보로 구성되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서, 상기 학습단계에서, 상기 물품에 대한 정보는, 각각의 물품마다 계획된 작업공정 투입일 또는 반출일까지의 남은 시간 또는 일자를의미하는 잔여기간에 대한 정보를 포함하여 구성되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법. 공개특허 10-2022-0128142-3-청구항 5 제 4항에 있어서, 상기 학습단계는, 상기 에이전트에 의해 선택된 위치에 처음으로 상기 물품이 적치되는 경우 상기 보상이 미리 정해진 제 1 값으로 설정되고, 상기 에이전트에 의해 선택된 위치에 상기 물품을 적치하였을 때 상기 에이전트에 의해 선택된 위치에 적치된물품들이 하단부터 차례대로 상기 잔여기간이 긴 순서대로 정렬되어 있지 않은 경우 상기 보상이 미리 정해진제 2 값으로 설정되며, 상기 에이전트에 의해 선택된 위치에 상기 물품을 적치하였을 때 상기 에이전트에 의해 선택된 위치에 적치된물품들이 하단부터 차례대로 상기 잔여기간이 긴 순서대로 정렬된 경우 상기 보상이 미리 정해진 제 3 값으로설정되도록 하는 처리가 수행되도록 구성되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5항에 있어서, 상기 학습단계에서, 상기 제 1 값은 상기 제 2 값보다 작고, 상기 제 2 값은 상기 제 3 값보다 작게(제 1 값 < 제 2 값 < 제 3 값)설정되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서, 상기 학습단계에서, 상기 제 2 값은 상기 에이전트에 의해 선택된 위치에 적치된 물품을 반출하기 위해 사용되는 크레인의 최대 사용횟수에 근거하여 결정되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7항에 있어서, 상기 제 2 값은, 상기 에이전트에 의해 선택된 위치에 적치되어 있는 각각의 물품에 대하여 해당 물품보다 위쪽에 적치되어 있는물품들 중 해당 물품보다 상기 잔여기간이 긴 물품의 수를 각각 계산하고, 계산된 값들 중 최대값을 구하여 상기 최대값의 역수로 설정되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법."}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1항에 있어서, 상기 강화학습 알고리즘은, A3C(Asynchronous Advantage Actor-Critic) 알고리즘을 이용하여 구성되는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법. 공개특허 10-2022-0128142-4-청구항 10"}
{"patent_id": "10-2021-0032819", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "물품 적치 시스템에 있어서,"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 강화학습(Reinforcement Learning) 머신러닝(machine Learning) 기술을 이용하여 컨테이너 부두, 강 재 적치장 등과 같이 적치 및 반출 효율이 중요한 곳에서 크레인의 이동을 최소화하기 위한 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 관한 것이다. 또한, 본 발명은 선별작업이 요구되는 종래기술의 조선소의 강 (뒷면에 계속)"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 강화학습(Reinforcement Learning)과 같은 머신러닝(machine Learning) 기술을 이용하여 컨테이너 부두, 강재 적치장 등과 같이 적치 및 반출 효율이 중요한 곳에서 크레인의 이동을 최소화하기 위한 적치위치 및 순서의 최적화 방법에 관한 것으로, 더 상세하게는, 종래, 단순히 입고일을 기준으로 물품을 적치함으로 인 해 계획된 일정에 따라 반출하기 위하여는 반출될 물품을 다시 정돈하는 선별작업이 요구되었던 기존의 적치방 식의 문제점을 해결하기 위해, 인공지능 알고리즘을 이용하여, 물품의 반출일정을 고려하여 적치순서를 최적화 하는 것에 의해 선별작업을 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법 에 관한 것이다. 또한, 본 발명은, 일반적으로, 조선소에서는 선박건조를 위한 조선용 후판을 대량으로 발주하므로 강재의 입고 가 공정일정과 무관하게 이루어지고, 이에, 대부분의 조선소는 가공공정 투입 전까지 입고된 강재를 보관하기 위하여 강재 적치장을 운영하고 있으나, 실제 조선소에서는 입고된 강재가 단순히 입고일을 기준으로 적치됨으 로 인해 강재를 계획된 가공공정에 따라 반출하기 위하여 반출될 강재를 미리 다른 적치장으로 옮겨 순서대로 정돈하는 선별작업이 필수적으로 수행되어야 하므로, 선박의 생산과는 직접적인 연관이 없고 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별도로 수행할 필요가 없는 선별작업이 요구됨으로 인해 전체적인 작업기 간 및 비용 증가의 요인이 되는 문제가 있었던 종래기술의 조선소의 강재 적치방법의 문제점을 해결하기 위해, 강화학습(reinforcement learning) 알고리즘을 이용하여, 컨테이너나 강재를 적치할 때 투입 또는 반출 일자를 고려한 위치선정을 통해 크레인의 이동 횟수를 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적 치순서 최적화 방법에 관한 것이다. 아울러, 본 발명은, 상기한 바와 같이 선박의 생산과는 직접적인 연관이 없고 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별도로 수행할 필요가 없는 선별작업이 요구되는 종래기술의 조선소의 강재 적치방법의 문제점을 해결하기 위해, 강화학습 문제의 상태를 각 강재의 계획된 반출일까지의 남은 시간을 포함하도록 정의 하고, 빠른 반출일을 가지는 강재가 파일의 위쪽에 적치되는 경우에 보상이 더 큰 값을 갖도록 설정하여 투입일 정에 따라 강재가 정렬되는 방식으로 에이전트의 학습이 이루어지도록 구성됨으로써, 강재의 가공일정을 고려하 여 입고된 강재의 적치파일을 결정하는 것에 의해 선별작업을 최소화하는 적치계획을 수행할 수 있으며, 그것에 의해, 크레인의 사용을 최소화하고 전체적인 작업시간 및 비용을 절감하여 생산성 향상에 기여할 수 있도록 구 성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 관한 것이다."}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래, 선박의 건조에 있어서, 조선소에서 선박 건조를 위한 강재(steel plate)의 발주는 일반적으로 강재 시황 과 중장기 생산 전략에 따라 벌크로 주문이 되므로 선박 건조를 위한 강재가 가공공정의 일정에 맞추어 입고되 지 않는다. 이에, 대부분의 조선소에서는, 별도의 강재적치장(steel stock yard)을 마련하여 입고되는 강재를 가공공정에 투입되기 전까지 적치하고 가공공정 시작 날자에 따라 순차적으로 반출하여 공정에 투입하고 있으며, 이와 같이 대부분의 강재가 가공 및 투입 순서와 무관하게 단순히 입항되는 순서대로 강재 적치장에 입고됨으로 인해 별도 의 선별(sorting) 작업을 통해 가공 투입 일정에 맞추어 강재의 적치순서를 관리하는 과정이 필요하다. 여기서, 상기한 바와 같이 조선소에서 강재의 적치위치 및 순서를 관리하기 위한 종래기술의 예로는, 예를 들면, 한국 등록특허공보 제10-1307075호에 제시된 바와 같은 \"적치장 내 블록의 적치 위치 결정 방법\"이 있다. 더 상세하게는, 상기한 한국 등록특허공보 제10-1307075호는, 초기 블록의 적치순서 s를 임의로 생성하는 단계 와, s의 적치 순서를 평가함수를 이용하여 평가하는 단계와, 평가가 완료된 s를 최적해 s*으로 설정하는 단계와, s* 외에 또 다른 적치순서 s'을 생성하는 단계와, s'의 적치 순서를 평가함수를 이용하여 평가하는 단 계와, s'의 평가값과 s*의 평가값을 비교하는 단계 및 s'의 평가값과 s*의 평가값 중 작은 값을 새로운 최적해 s*으로 설정하는 단계를 포함하여, 블록 적치로 인한 적치장 공간의 낭비 및 간섭 발생을 최소화할 수 있도록 구성되는 적치장 내 블록의 적치 위치 결정 방법에 관한 것이다. 또한, 상기한 바와 같이 조선소에서 강재의 적치위치 및 순서를 관리하기 위한 종래기술의 다른 예로는, 예를 들면, 한국 등록특허공보 제10-1249435호에 제시된 바와 같은 \"강재 적치관리 시스템\"이 있다. 더 상세하게는, 상기한 한국 등록특허공보 제10-1249435호는, 개별 강재에 고유의 식별정보를 포함하는 강재식 별태그를 생성하고 관리하는 식별태그관리부와, 식별태그관리부와 연결되어 강재의 입고, 적치, 선별, 절단 및 출고작업을 제어하는 중앙서버와, 중앙서버와 연결되어 강재 입출고 정보를 실시간으로 관리하는 신호수단말기 와, 신호수단말기 및 중앙서버와 연결되어 강재 권상작업을 수행하는 크레인단말기 및 중앙서버와 연결되어 강 재의 절단작업을 실시간으로 관리하는 절단작업부를 포함하여, 강재식별태그를 이용하여 강재 적치상황을 실시 간으로 파악하고 관리하는 것에 의해 적치관리 시스템의 효율성을 향상시킬 수 있도록 구성되는 강재 적치관리 시스템에 관한 것이다. 상기한 바와 같이, 종래, 조선소에서 강재의 적치위치 및 순서를 관리하기 위한 여러 가지 기술내용이 제시된 바 있으나, 상기한 바와 같은 종래기술의 내용들은 다름과 같은 문제점이 있는 것이었다. 더 상세하게는, 일반적으로, 강재 적치장의 강재는 수직방향으로 파일(pile) 형태로 적치되므로 이동시켜야할 강재 위에 다른 강재가 있으면 크레인을 이용하여 다른 곳으로 먼저 이동시켜야 하나, 현재 대부분의 조선소에 서는 초기에 적치되는 순서가 관리되고 있지 않음으로 인해 1차 선별, 2차 선별 작업을 추가하여 가공 계획 일 자에 투입될 수 있도록 관리하고 있다. 여기서, 만약, 입고순서를 고려하여 초기에 적치되는 순서를 작업 일정에 따라 적절히 배치한다면 이러한 선별 작업을 감소할 수 있으므로 강재 적치장 관리비용을 절감할 수 있을 것으로 기대되며, 이론적으로는 계획 대상 이 되는 강재들을 투입순서와 역순으로 적치하면 되는 간단한 문제가 될 수 있다. 그러나 실제 강재 적치장 환경은 바지선이나 선박을 이용하여 강재가 대량으로 일괄 하역되어 입고되므로, 계획 대상이 되는 강재들의 입고순서를 임의로 결정할 수 없다는 문제가 있다. 즉, 각각의 강재들의 입고시에 강재별로 적치위치(pile)를 결정해야 하나 각각의 강재들은 서로 다른 임의의 가 공 시작 날짜를 가지고 있기 때문에 정형화될 수 없고, 이에 더하여, 강재가 특정 파일에 적치되는 순간 다음 강재의 적치위치 결정에 대한 적치장 상태가 변경되기 때문에 기존의 최적화 문제로 정식화하기 어려운 문제가 있다. 더욱이, 상기한 한국 등록특허공보 제10-1307075호 및 한국 등록특허공보 제10-1249435호에 제시된 바와 같은 종래기술의 내용들은 단지 블록 적치로 인한 적치장 공간의 낭비 및 간섭 발생을 최소화하거나, 강재식별태그를 이용하여 강재 적치상황을 관리하는 기술내용만을 제시하고 있을 뿐, 상기한 바와 같이 선박의 생산일정을 고려 하여 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별도로 수행할 필요가 없는 선별작업이 요구되는 기존의 적치방식의 문제점을 보완할 수 있는 방안에 대하여는 제시된 바 없었다. 따라서 상기한 바와 같이, 단순히 입고되는 순서대로 강재가 적치됨으로 인해 작업 일정에 맞추어 강재의 적치 순서를 변경하는 선별작업이 요구되었던 기존의 적치방식들의 문제점을 해결하기 위하여는, 예를 들면, 인공지 능 알고리즘을 이용하여, 강재와 같은 입고물품의 반출일정을 고려하여 적치순서를 최적화하는 것에 의해 선별 작업을 최소화할 수 있도록 구성되는 새로운 구성의 적치순서 최적화 방법을 제공하는 것이 바람직하나, 아직까 지 그러한 요구를 모두 만족시키는 장치나 방법은 제시되지 못하고 있는 실정이다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국 등록특허공보 제10-1307075호 (2013.09.04.) (특허문헌 0002) 한국 등록특허공보 제10-1249435호 (2013.03.26.)"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 바와 같은 종래기술의 문제점을 해결하고자 하는 것으로, 따라서 본 발명의 목적은, 단순히 입고일을 기준으로 물품을 적치함으로 인해 계획된 일정에 따라 반출하기 위하여는 반출될 물품을 다시 정돈하 는 선별작업이 요구되었던 기존의 적치방식의 문제점을 해결하기 위해, 인공지능 알고리즘을 이용하여, 물품의 반출일정을 고려하여 적치순서를 최적화하는 것에 의해 선별작업을 최소화할 수 있도록 구성되는 강화학습을 이 용한 입고물품의 적치순서 최적화 방법을 제시하고자 하는 것이다. 또한, 본 발명의 다른 목적은, 선박의 생산일정을 고려하여 강재가 투입일정에 따라 정렬된 상태로 적치되어 있 다면 별도로 수행할 필요가 없는 선별작업이 요구됨으로 인해 전체적인 작업기간 및 비용 증가의 요인이 되는 문제가 있었던 종래기술의 조선소의 강재 적치방법의 문제점을 해결하기 위해, 강화학습(reinforcement learning) 알고리즘을 이용하여, 컨테이너나 강재를 적치할 때 투입 또는 반출 일자를 고려한 위치선정을 통해 크레인의 이동 횟수를 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 제 시하고자 하는 것이다. 아울러, 본 발명의 또 다른 목적은, 상기한 바와 같이 선별작업이 요구되었던 종래기술의 조선소의 강재 적치방 법의 문제점을 해결하기 위해, 강화학습 문제의 상태를 각 강재의 계획된 반출일까지의 남은 시간을 포함하도록정의하고, 빠른 반출일을 가지는 강재가 파일의 위쪽에 적치되는 경우에 보상이 더 큰 값을 갖도록 설정하여 투 입일정에 따라 강재가 정렬되는 방식으로 에이전트의 학습이 이루어지도록 구성됨으로써, 강재의 가공일정을 고 려하여 입고된 강재의 적치파일을 결정하는 것에 의해 선별작업을 최소화하는 적치계획을 수행할 수 있으며, 그 것에 의해, 크레인의 사용을 최소화하고 전체적인 작업시간 및 비용을 절감하여 생산성 향상에 기여할 수 있도 록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 제시하고자 하는 것이다."}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 바와 같은 목적을 달성하기 위해, 본 발명에 따르면, 강화학습(Reinforcement Learning)을 이용한 입고 물품의 적치순서 최적화 방법에 있어서, 미리 정의된 강화학습 알고리즘을 이용하여, 에이전트(agent)와 환경 (environment)의 상호작용을 통해 물품의 적치계획에 대한 학습을 행하여 상기 물품의 적치순서를 최적화 하기 위한 에이전트(agent) 모델을 구축하는 처리가 수행되는 학습단계; 및 상기 학습단계에서 구축된 상기 에이전트 모델을 상기 물품의 적치문제에 적용하여 적치계획을 수립하고, 수립된 상기 적치계획에 따라 입고되는 물품을 적치하는 처리가 수행되는 적용단계를 포함하는 처리가 전용의 하드웨어 또는 컴퓨터를 통해 실행되도록 구성되 는 것을 특징으로 하는 강화학습을 이용한 입고물품의 적치순서 최적화 방법이 제공된다. 여기서, 상기 학습단계는, 상기 에이전트의 행동(Action)에 따라 상기 환경으로부터 피드백되는 상태(State)와 보상(Reward)에 근거하여 상기 에이전트의 인공신경망(neural network)의 가중치를 업데이트하는 마르코프 결정 과정(Markov decision process ; MDP)에 기반한 강화학습 알고리즘에 따라 학습이 이루어지며, 상기 상태는 상 기 에이전트가 행동을 결정하는 시점에서의 상기 물품의 입고현황과 적치현황으로 정의되고, 상기 행동(Actio n)은 입고된 상기 물품을 적치할 위치를 결정하는 것으로 정의되며, 상기 보상(Reward)은 크레인을 포함하는 상 기 물품의 이송수단에 대한 사용횟수를 기준으로 정의되는 것을 특징으로 한다. 또한, 상기 학습단계에서, 상기 입고현황은 적치될 위치가 아직 결정되지 않은 채로 대기하고 있는 물품에 대한 정보이고, 상기 적치현황은 이미 적치장에 적치되어 있는 물품에 대한 정보로 구성되는 것을 특징으로 한다. 아울러, 상기 물품에 대한 정보는, 각각의 물품마다 계획된 작업공정 투입일 또는 반출일까지의 남은 시간 또는 일자를 의미하는 잔여기간에 대한 정보를 포함하여 구성되는 것을 특징으로 한다. 더욱이, 상기 학습단계는, 상기 에이전트에 의해 선택된 위치에 처음으로 상기 물품이 적치되는 경우 상기 보상 이 미리 정해진 제 1 값으로 설정되고, 상기 에이전트에 의해 선택된 위치에 상기 물품을 적치하였을 때 상기 에이전트에 의해 선택된 위치에 적치된 물품들이 하단부터 차례대로 상기 잔여기간이 긴 순서대로 정렬되어 있 지 않은 경우 상기 보상이 미리 정해진 제 2 값으로 설정되며, 상기 에이전트에 의해 선택된 위치에 상기 물품 을 적치하였을 때 상기 에이전트에 의해 선택된 위치에 적치된 물품들이 하단부터 차례대로 상기 잔여기간이 긴 순서대로 정렬된 경우 상기 보상이 미리 정해진 제 3 값으로 설정되도록 하는 처리가 수행되도록 구성되는 것을 특징으로 한다. 여기서, 상기 학습단계에서, 상기 제 1 값은 상기 제 2 값보다 작고, 상기 제 2 값은 상기 제 3 값보다 작게(제 1 값 < 제 2 값 < 제 3 값) 설정되는 것을 특징으로 한다. 또한, 상기 학습단계에서, 상기 제 2 값은 상기 에이전트에 의해 선택된 위치에 적치된 물품을 반출하기 위해 사용되는 크레인의 최대 사용횟수에 근거하여 결정되는 것을 특징으로 한다. 아울러, 상기 제 2 값은, 상기 에이전트에 의해 선택된 위치에 적치되어 있는 각각의 물품에 대하여 해당 물품 보다 위쪽에 적치되어 있는 물품들 중 해당 물품보다 상기 잔여기간이 긴 물품의 수를 각각 계산하고, 계산된 값들 중 최대값을 구하여 상기 최대값의 역수로 설정되는 것을 특징으로 한다. 더욱이, 상기 강화학습 알고리즘은, A3C(Asynchronous Advantage Actor-Critic) 알고리즘을 이용하여 구성되는 것을 특징으로 한다. 또한, 본 발명에 따르면, 상기에 기재된 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 컴퓨터나 전용 의 하드웨어에 실행시키도록 구성되는 프로그램이 기록된 컴퓨터에서 판독 가능한 기록매체가 제공된다. 아울러, 본 발명에 따르면, 물품 적치 시스템에 있어서, 상기에 기재된 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 이용하여, 입고되는 물품의 적치계획을 수립하는 적치계획 수립부; 및 크레인을 포함하는 이송수 단을 포함하여, 상기 적치계획 수립부에 의해 수립된 적치계획에 따라 상기 물품의 적치를 수행하도록 이루어지 는 물품이송부를 포함하여 구성되는 것을 특징으로 하는 물품 적치 시스템이 제공된다."}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기한 바와 같이, 본 발명에 따르면, 강화학습 문제의 상태를 각 강재의 계획된 반출일까지의 남은 시간을 포 함하도록 정의하고, 빠른 반출일을 가지는 강재가 파일의 위쪽에 적치되는 경우에 보상이 더 큰 값을 갖도록 설 정하여 투입일정에 따라 강재가 정렬되는 방식으로 에이전트의 학습이 이루어지도록 구성되는 강화학습을 이용 한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 강재의 가공일정을 고려하여 입고된 강재의 적치파일을 결정하는 것에 의해 선별작업을 최소화하는 적치계획을 수행할 수 있으며, 그것에 의해, 크레인의 사용을 최소 화하고 전체적인 작업시간 및 비용을 절감하여 생산성 향상에 기여할 수 있다. 또한, 본 발명에 따르면, 상기한 바와 같이 컨테이너나 강재를 적치할 때 투입 또는 반출 일자를 고려한 위치선 정을 통해 크레인의 이동 횟수를 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 선박의 생산일정을 고려하여 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별 도로 수행할 필요가 없는 선별작업이 요구됨으로 인해 전체적인 작업기간 및 비용 증가의 요인이 되는 문제가 있었던 종래기술의 조선소의 강재 적치방법의 문제점을 해결할 수 있다. 아울러, 본 발명에 따르면, 상기한 바와 같이 물품의 반출일정을 고려하여 적치순서를 최적화하는 것에 의해 선 별작업을 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 단순히 입고일을 기준으로 물품을 적치함으로 인해 계획된 일정에 따라 반출하기 위하여는 반출될 물품을 다시 정돈하는 선별작업이 요구되었던 기존의 물품 적치방식들의 문제점을 해결할 수 있다."}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여, 본 발명에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법의 구체적 인 실시예에 대하여 설명한다. 여기서, 이하에 설명하는 내용은 본 발명을 실시하기 위한 하나의 실시예일 뿐이며, 본 발명은 이하에 설명하는 실시예의 내용으로만 한정되는 것은 아니라는 사실에 유념해야 한다. 또한, 이하의 본 발명의 실시예에 대한 설명에 있어서, 종래기술의 내용과 동일 또는 유사하거나 당업자의 수준 에서 용이하게 이해하고 실시할 수 있다고 판단되는 부분에 대하여는, 설명을 간략히 하기 위해 그 상세한 설명 을 생략하였음에 유념해야 한다. 즉, 본 발명은, 후술하는 바와 같이, 단순히 입고일을 기준으로 물품을 적치함으로 인해 계획된 일정에 따라 반 출하기 위하여는 반출될 물품을 다시 정돈하는 선별작업이 요구되었던 기존의 적치방식의 문제점을 해결하기 위 해, 인공지능 알고리즘을 이용하여, 물품의 반출일정을 고려하여 적치순서를 최적화하는 것에 의해 선별작업을 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 관한 것이다. 아울러, 본 발명은, 후술하는 바와 같이, 선박의 생산일정을 고려하여 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별도로 수행할 필요가 없는 선별작업이 요구됨으로 인해 전체적인 작업기간 및 비용 증가의 요 인이 되는 문제가 있었던 종래기술의 조선소의 강재 적치방법의 문제점을 해결하기 위해, 강화학습 알고리즘을 이용하여, 컨테이너나 강재를 적치할 때 투입 또는 반출 일자를 고려한 위치선정을 통해 크레인의 이동 횟수를 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 관한 것이다. 더욱이, 본 발명은, 후술하는 바와 같이, 선별작업이 요구되었던 종래기술의 조선소의 강재 적치방법의 문제점 을 해결하기 위해, 강화학습 문제의 상태를 각 강재의 계획된 반출일까지의 남은 시간을 포함하도록 정의하고, 빠른 반출일을 가지는 강재가 파일의 위쪽에 적치되는 경우에 보상이 더 큰 값을 갖도록 설정하여 투입일정에 따라 강재가 정렬되는 방식으로 에이전트의 학습이 이루어지도록 구성됨으로써, 강재의 가공일정을 고려하여 입 고된 강재의 적치파일을 결정하는 것에 의해 선별작업을 최소화하는 적치계획을 수행할 수 있으며, 그것에의해, 크레인의 사용을 최소화하고 전체적인 작업시간 및 비용을 절감하여 생산성 향상에 기여할 수 있도록 구 성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 관한 것이다. 계속해서, 도면을 참조하여, 본 발명에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법의 구체적인 내용에 대하여 설명한다. 여기서, 본 발명에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법의 구체적인 내용에 대하여 설명하 기 전에, 먼저, 본 발명에 적용된 강화학습(reinforcement learning) 알고리즘에 대하여 설명하면, 강화학습은 머신러닝(machine learning) 기술의 한 영역이며, 특히, 순차적인 행동의 결정 문제를 푸는데 적합한 방법으로, 에이전트(agent)와 환경(environment)의 상호작용 방식을 정의하는 것에 의해 에이전트가 환경에서 얻은 정보로 학습을 수행하여 원하는 목적을 달성하도록 고안된 알고리즘이다. 이때, 에이전트와 환경의 상호작용은 상태(State), 행동(Action), 보상(Reward)으로 구성된 마르코프 결정과정 (Markov decision process ; MDP)이라는 프레임워크를 바탕으로 정의되며, 에이전트는 환경으로부터 주어진 정 보(상태)를 인식하여 어떤 행동을 할지를 결정하고, 환경은 해당 행동이 정해진 목적을 달성하는데 얼마나 유효 한지에 대한 정보인 보상과 다음 상태를 에이전트에 제공한다. 또한, 본 발명은 강재의 가공일정을 고려한 적치 알고리즘에 관한 것으로, 본 발명에서 다루고 있는 강재와 같 은 물품의 적치문제 역시 적치장에 순서대로 입고되는 강재(물품)에 대하여 적치할 파일(위치)을 결정하는 순차 적 행동 결정 문제라고 할 수 있다. 아울러, 이하에 설명하는 본 발명의 실시예에 있어서, 입고되는 강재를 적치할 때 강재의 가공 일정을 고려하여 강재 적치장에서의 불필요한 선별작업을 최소화하도록 적치하기 위해, 강화학습 환경으로서 강재 적치장에 대한 간단한 시뮬레이션 모델을 도입하였고, 에이전트와 환경의 상호작용을 구성하는 상태, 행동, 보상을 다음과 같 이 정의하였다. 먼저, 상태(State)는 에이전트가 행동을 결정하는 시점에서 강재 적치장에 입고된 강재현황과 강재 적치현황으 로 정의하였으며, 구체적으로는, 입고된 강재현황은 아직 적치될 파일이 결정되지 않은 채로 대기하고 있는 강 재에 대한 정보이고, 강재 적치현황은 이미 강재 적치장에 쌓여 있는 강재에 대한 정보를 의미한다. 이때, 강재의 가공일정을 고려하기 위하여 강재에 대한 정보로는 강재마다 계획된 가공공정 투입일까지 남은 시 간을 사용하였으며, 상기한 상태(State)는, 예를 들면, 강재 적치장이 총 8개의 파일로 구성되어 있고 각 파일 에는 최대 10개의 강재를 쌓을 수 있다고 가정하면, 10×9의 크기를 가지는 2차원 행렬로 나타낼 수 있다. 이때, 첫번째 열은 입고된 강재에 대한 정보를 담고 있고, 두 번째 열부터 아홉번째 열까지는 강재 적치장의 파 일을 의미하며, 각 파일에 적치되어 있는 강재에 대한 정보를 담고 있다. 다음으로, 행동(Action)은, 입고된 강재를 적치할 파일을 결정하는 것으로 정의하였으며, 즉, 상기한 바와 같은 상태 공간을 가지는 강재 적치장에서 에이전트가 취할 수 있는 행동은 총 8개의 파일 중 하나를 선택하는 것이 된다. 마지막으로, 보상(Reward)은 크레인과 같은 물품 이송수단의 사용횟수를 기준으로 정의하였으며, 보상을 계산하 는 방식은 다음과 같이 크게 세 가지 경우로 구분할 수 있다. 더 상세하게는, 먼저, 에이전트에 의해 선택된 위치에 처음으로 물품이 적치되는 경우 보상은 미리 정해진 제 1 값으로 설정되고, 에이전트에 의해 선택된 위치에 물품을 적치하였을 때 에이전트에 의해 선택된 위치에 적치된 물품들이 하단부터 차례대로 잔여기간이 긴 순서대로 정렬되어 있지 않은 경우는 보상이 미리 정해진 제 2 값으 로 설정되며, 에이전트에 의해 선택된 위치에 물품을 적치하였을 때 에이전트에 의해 선택된 위치에 적치된 물 품들이 하단부터 차례대로 잔여기간이 긴 순서대로 정렬된 경우는 보상이 미리 정해진 제 3 값으로 설정되도록 구성될 수 있다. 여기서, 상기한 제 1 값은 제 2 값보다 작고, 제 2 값은 제 3 값보다 작게 설정되도록 구성될 수 있다(즉, 제 1 값 < 제 2 값 < 제 3 값). 또한, 이하에 설명하는 본 발명의 실시예에 있어서는, 상기한 첫번째 경우와 같이 에이전트에 의해 선택된 파일 에 처음으로 강재가 적치되는 상황일 때의 보상은 0의 값으로 설정하였고, 상기한 세번째 경우와 같이 에이전트 가 선택한 파일에 강재를 적치하였을 때 강재가 파일의 하단부터 차례대로 가공공정 투입일까지 남은 시간이 긴 순으로 정렬된 상황일 때의 보상은 2의 값으로 설정하였다. 아울러, 상기한 두번째 경우와 같이 에이전트가 선택한 파일에 강재를 적치하였을 때 강재가 가공공정 투입일까 지 남은 시간 순서대로 정렬되어 있지 않은 상황일 때에는 각 강재마다 해당 강재를 계획된 가공공정 투입일에 반출하기 위해서 상부의 강재를 이동시키는데 사용되는 크레인의 작업 횟수를 계산하였다. 다시 말해, 파일에 적치되어 있는 각 강재마다 해당 강재보다 위쪽에 적치되어 있는 강재들 중 해당 강재보다 더 늦은 반출일을 가지는 강재의 수를 계산하였고, 이 값들 중 최대값을 구하여 최대값의 역수로 보상을 설정하 였다. 여기서, 상기한 바와 같은 보상값의 내용 및 이하에 설명하는 본 발명의 실시예에 제시된 내용들은 단지 본 발 명을 설명하기 위한 하나의 예로서 제시된 것이며, 즉, 본 발명은 상기한 내용 및 이하의 실시예에 제시된 내용 으로만 한정되는 것이 아니라, 본 발명의 취지 및 본질을 벗어나지 않는 범위 내에서 당업자에 의해 필요에 따 라 다양하게 수정 및 변경하여 적용 가능한 것임에 유념해야 한다. 또한, 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법은, 크게 나누어, 상기한 바와 같이 정의되는 내용에 따라 에이전트가 학습을 진행하는 학습단계와, 학습된 에이전트 모델을 이용하여 해 당 물품에 대한 적치계획을 수립하고 실행하는 적용단계의 두 단계를 포함하여 구성될 수 있다. 더 상세하게는, 먼저, 학습단계에서는, 에이전트가 반복적으로 환경과 상호작용하면서 최적의 적치계획에 대한 학습을 진행하며, 이때, 학습을 위해서는 먼저 학습대상이 되는 강재 적치장의 크기를 정의해야 한다. 더 상세하게는, 강재 적치장의 크기는 전체 파일의 개수와 각 파일에 적치할 수 있는 최대 강재 개수로 정의되 고, 문제가 정의된 후에는 강재 데이터를 로딩하여 학습을 위한 환경을 초기화하며, 이후 과정은 상기한 바와 같이 하여 미리 정의된 강화학습 알고리즘에 따라 학습이 진행되고, 주기적으로 학습된 모델을 저장한다. 다음으로, 적용단계에서는, 학습된 에이전트 모델을 실제 강재 적치장의 강재 적치문제에 적용하여 계획을 수립 하고 실행하며, 이를 위해, 먼저 학습된 모델을 로딩하고 로딩된 모델에 적치대상 강재 데이터를 차례대로 입력 하여 모델의 결과로서 강재가 적치될 파일을 받아 계획을 수행한다. 즉, 도 1을 참조하면, 도 1은 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법의 전체적인 구성을 개략적으로 나타내는 플로차트이다. 도 1에 나타낸 바와 같이, 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법은, 크 게 나누어, 미리 정의된 강화학습 알고리즘에 따라 에이전트가 반복적으로 환경과 상호작용하면서 최적의 적치 계획에 대한 학습을 진행하는 학습단계(S10)와, 학습된 에이전트 모델을 실제 적치문제에 적용하여 적치계획을 수립하고 실행하는 적용단계(S20)를 포함하여 구성될 수 있다. 여기서, 상기한 강화학습 알고리즘은, 상기한 바와 같이, 에이전트가 행동을 결정하는 시점에서 입고된 강재현 황과 적치현황으로 정의되는 상태(State)와, 입고된 강재를 적치할 파일을 결정하는 행동(Action) 및 크레인 사 용횟수를 기준으로 정의되는 보상(Reward)으로 구성되는 마르코프 결정과정(Markov decision process ; MDP)에 근거하여 구성될 수 있다. 즉, 도 2를 참조하면, 도 2는 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 적용되는 강화학습 알고리즘의 기본적인 개념을 개략적으로 나타내는 도면이다. 도 2에 나타낸 바와 같이, 본 발명의 실시예에 따른 강화학습 알고리즘은, 중간 부분의 환경(environment)에 선 택된 행동(action) 정보를 전달하고, 환경으로부터 피드백되는 상태(state)와 보상(reward)를 수신하여 인공신 경망(neural network)의 가중치를 업데이트하도록 구성될 수 있으며, 이때, 중간 부분의 환경은 실제 강재 적치 장에서 학습을 위해 필요한 부분(적치 정보)만 추출하여 학습 알고리즘과 연동될 수 있도록 구현될 수 있다. 여기서, 상기한 바와 같이 구성되는 본 발명의 실시예에 따른 강화학습 알고리즘은, 예를 들면, DQN(Deep Q- Network), A2C(Advantage Actor-critic) 및 A3C(Asynchronous Advantage Actor-Critic) 등과 같은 학습 알고 리즘을 사용하여 구성될 수 있으며, 바람직하게는, 후술하는 바와 같이 하여, 여러 가지 학습 알고리즘의 학습 결과를 비교하고 최적의 알고리즘을 선택하여 적용하는 처리가 수행되도록 구성될 수 있다. 계속해서, 상기한 바와 같이 하여 구성되는 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 실제 조선소 강재 적치장에 적용하여 그 성능을 검증한 결과에 대하여 설명한다. 즉, 본 발명자들은, 후술하는 바와 같이, 외부에서 입고된 강재가 강재 적치장에서 가공공정에 투입될 때까지 선별작업(또는 크레인 이동 횟수)을 최소화할 수 있는 최적의 적치위치(pile)를 결정할 수 있는 강화학습 알고 리즘과 환경을 구현하기 위해, 우선 강재 적치장에서 강재 적치위치를 결정하는 문제에 대해 학습이 가능하도록 선택 가능한 액션과 적치장의 상태를 정의하고, 다음으로 정의된 문제에 따라 학습이 가능한 강재 적치장 환경 (environment)을 구성하기 위해, 강재 적치장 환경(environment)에 대하여 선택 가능한 액션, 액션에 따른 상태 변화 및 선택된 액션에 대한 보상(reward)을 각각 정의하였다. 더 상세하게는, 도 3을 참조하면, 도 3은 실제 조선소의 강재 적치장에 대한 강재 입출고 과정을 개략적으로 나타내는 도면이다. 도 3에 나타낸 바와 같이, 실제 문제의 대상이 되는 조선소 강재 적치장의 강재 입출고 과정은, 임의의 순서로 선박(또는 바지선)을 통해 강재가 조선소 안벽에 도착하면, 도착한 강재는 입고된 순서대로 임시 적치장으로 이 송되어 적치되며(Unloading bay → Main Stock), 이때, 이러한 메인스톡(main stock)은 약 20개 정도의 파일 (pile)로 구성된다. 이어서, 생산계획으로부터 1주 가공계획이 수립되면 해당 강재들을 메인스톡에서 제 1 적치구역(1st sorting area)으로 이송하고(Main Stock → 1st Sorting area), 제 2 이송구역(2nd Sorting area)에는 1차 선별 적치장 에 있는 1주일치 강재를 일별로 선별한 강재가 적치된다(1st Sorting area → 2nd Sorting area). 제 2 적치영역(2nd Sorting area)은 가공 공장으로의 이송을 위한 컨베이어 이송장치와 인접해 있으며, 제 2 이 송구역(2nd sorting area)에 적치된 강재는 각 강재의 가공 공장 투입일정에 따라 크레인을 이용하여 가공 공장 컨베이어로 이송된다. 여기서, 강재의 적치위치 결정과 선별은 독립된 작업이나 적치위치 결정을 최적화함으로써 선별을 최소화 할 수 있으므로, 이에, 본 발명자들은, 적치위치 결정에 대한 학습에 집중하여, 각 이송구간에 대한 학습은 동일한 알 고리즘으로 상태 정의와 강재 입력 변경만으로 수행이 가능하며, 환경을 매개변수로 설계할 수 있도록 하고, 다 양한 상태에 대한 강재 적치위치 결정 인공신경망을 구현하였다. 다음으로, 본 발명의 실시예에 적용된 강화학습 알고리즘에 대하여 설명하면, 본 발명자들은 DQN(Deep Q- Network)과 A2C(Advantage Actor-critic) 및 A3C(Asynchronous Advantage Actor-Critic) 알고리즘을 사용하여 각각 학습을 수행하고 그 결과를 비교하였다. 먼저, DQN(Deep Q-Networks) 알고리즘은, 딥살사(Deep SARSA) 알고리즘으로부터 발전된 알고리즘으로 행동에 대 한 큐함수를 Q-Network라 부르는 인공신경망으로 근사하고, 학습을 진행함에 따라 최대의 보상을 받도록 Q- Network의 가중치를 갱신하는 알고리즘이다. 이때, 현재상태(s)에서 행동(a)을 취한 다음 환경으로부터 그에 대한 보상(r)과 다음 상태(s')를 받고 다시 다 음 행동(a')을 취하는 것을 하나의 샘플(s, a, r, s', a')로 사용하여 이하의 [수학식 1]과 같이 정의된 평균제 곱오차(MSE)를 손실함수로 적용하여 Q-Network의 가중치를 업데이트한다. [수학식 1]"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "즉, 오프폴리시(Off-Policy) 알고리즘인 DQN은 온폴리시(On-Policy) 알고리즘인 딥살사(Deep-Sarsa)와는 달리 실제 행동의 선택과 Q-Network를 업데이트하기 위한 행동의 선택을 분리시키고, 이를 구현하기 위해 리플레이 메모리를 활용한다. 또한, 에피소드에서 한 스텝을 진행함에 따라 획득한 샘플들을 리플레이 메모리에 저장하고 리플레이 메모리에 서 배치 단위로 샘플들을 무작위로 뽑아 Q-Network의 학습에 사용함으로써 학습 효율을 높인다. 다음으로, A2C(Advantage Actor-Critic) 알고리즘에서는 가치함수를 크리틱(Critic)이라는 가치신경망으로 근사 함은 물론 정책도 액터(Actor)라는 정책신경망으로 근사시키며, 이때, A2C에서 정책 신경망 손실함수의 미분값 은 교차 엔트로피 함수에 큐함수를 곱한 값으로 유도되는데, 큐함수 값의 변동이 크기 때문에 가치함수를 베이 스라인(baseline)으로 하여 이하의 [수학식 2]에 나타낸 바와 같이 큐함수에서 가치함수를 뺀 값을 어드벤티지 (advantage) 함수로 정의하고, 이를 큐함수 대신에 사용한다. [수학식 2]"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다만, 실제 학습에서는 보다 효율적인 계산을 위하여 큐함수를 근사하기 위한 Q-Network를 따로 두지 않고 큐함 수 자체도 가치신경망으로부터 근사한 값을 사용하며, 현재 상태에서 취한 행동에 대한 큐함수는 이하의 [수학 식 3]에 나타낸 바와 같이 감가율이 곱해진 다음 상태의 가치함수에 보상을 더한 값으로 근사하게 된다. [수학식 3]"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "최종적으로, 근사 어드벤티지 함수를 포함하여 정책신경망 매개변수를 업데이트하는 식은 이하의 [수학식 4]와 같이 표현된다. [수학식 4]"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "또한, 가치신경망의 경우는 DQN에서 Q-Network를 업데이트하는 것과 비슷한 방식으로 이하의 [수학식 5]와 같이 평균제곱오차(MSE)를 손실함수로 하여 가중치를 갱신한다. [수학식 5]"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "또한, A3C(Asynchronous Advantage Actor-Critic) 알고리즘은 기존의 A2C 알고리즘으로부터 발전된 알고리즘으 로, A2C를 통해 학습을 수행하는 복수의 에이전트를 두고 각 에이전트에서 독립적으로 학습한 결과로 글로벌 네 트워크를 업데이트한다. 더 상세하게는, A3C는 비동기식 학습구조(asynchronous learning structure)를 가지는 것이 A2C와의 차이점이 며, 즉, A3C 알고리즘은 Volodymyr 등이 도입한 정책기반(policy-based) 강화학습 알고리즘으로서, 복수의 에이전트가 각각 독립적인 환경에서 학습을 수행하며, A3C 알고리즘은 독립적인 학습 에이전트를 도입하여 데이터간 의 상관관계를 감소하는 메커니즘을 가지고 있고, 각각의 학습 에이전트에는 A2C 알고리즘이 적용되며, A2C 알 고리즘을 통한 각 에이전트의 학습결과는 글로벌 네트워크에 비동기적으로 업데이트된다. 즉, 도 4를 참조하면, 도 4는 A3C 알고리즘의 의사 코드(pseudo code)를 나타내는 도면이다. 도 4에 나타낸 바와 같이, 학습 알고리즘은 모든 단계 카운터 T를 0으로 초기화하여 시작하고, 여기서, T는 모 든 에이전트가 공유하는 변수로서, 각 에이전트에 의해 수행된 단계 수(number of steps)의 합을 기록하고, 코 드에서 global_episodes라는 변수로 설정된다. 다음으로, 스레드의 스텝 카운터 t는 1로 초기화되어 개별 에이전트가 위치한 시간 스텝을 기록하고, 코드에서 total_steps라는 변수로 설정되며, 그 후, 글로벌 정책 신경망(global policy neural network)의 가중치 기울 기(slope of the weight)는 0으로 초기화되고, 이에 대응하는(corresponding) 로컬 정책 신경망(local policy neural network)의 가중치는 글로벌 정책 신경망의 가중치로 초기화된다. 또한, n-step 부트스트래핑(bootstrapping)의 시작시간 단계(starting time step)인 tstart은 현재 에이전트의 시간 단계(time step)로 초기화되며, 여기서, n-step 부트스트래핑은 매 단계마다 가중치를 업데이트하는 시간 차방법(temporal difference method) 및 에피소드가 끝날 때 가중치를 업데이트하는 몬테카를로(Monte Carlo) 방법과 달리, 정의된 n 단계마다 가중치를 업데이트하는 방법을 의미한다. 아울러, 학습 알고리즘은 환경으로부터 상태 st를 수신하고 샘플을 수집하는 반복단계(iterative step)에 진입 하여, 정책 신경망(policy neural network) π(αt│st;θ')로부터 작업 at를 선택하고 이에 대응하는 보상 rt 및 다음 상태 st+1을 수신하며, 모든 스텝 카운터 T와 스레드 스텝(thread stp) 카운터 t는 1씩 증가하고, 이러 한 과정은 터미널 상태(terminal state) 또는 프리셋(preset) tmax 시간 단계(time step)에 도달할 때까지 반복 된다. 더욱이, 반환값 R은 환경으로부터 수신된 보상에서 계산되고, 이 경우, 터미널 상태에 대한 반환값은 0이고, 감 가율(discounting factor)을 고려하여 터미널 상태가 아닌 다른 상태에 대한 반환값이 계산되며, 정책 및 가치 신경망에 대한 기울기는 반환값을 이용하여 업데이트된다. 이러한 방식으로 샘플의 모든 상태에 대하여 기울기가 반복적으로 계산되고 누적되며, 이와 같이 하여 누적된 값을 이용하여 글로벌 네트워크의 가중치가 최종적으로 업데이트됩니다. 계속해서, 환경(environment)의 구체적인 구성에 대하여 설명하면, 환경은 학습 알고리즘으로부터 선택된 행동 (action)을 입력받아 시뮬레이션을 통해 나온 새로운 상태(state)와 보상(reward)를 피드백하는 역할을 하며, 본 발명의 실시예에 있어서 환경은 다음과 같이 구성될 수 있다. 먼저 도 5를 참조하면, 도 5는 본 발명의 실시예에서 강화학습을 위해 인공신경망에 입력되는 환경의 상태 (state)에 대한 정의를 개략적으로 나타내는 개념도이다. 도 5에 있어서, 왼쪽의 단일 열(single column)은 투입되는 강재를 나타내고, 각 숫자는 해당 강재가 가공공정 에 투입될 때까지 남은 시간을 의미하며, 우측의 그리드(Grid)는 강재 적치장에 대한 환경으로, 수평방향은 강 재가 적치될 수 있는 파일(pile)의 수이고, 수직방향은 각 파일에 적치될 수 있는 최대 강재 수량이다. 여기서, 본 실시예에서는, 모든 파일의 최대 높이를 동일하게 정의하였으며, 그리드 내부의 숫자는 초기에는 모 두 0이고, 적치 시뮬레이션이 진행되면서 투입되는 강재의 투입 잔여기간으로 채워지게 된다. 또한, 시간 진행(본 실시예의 알고리즘에서는 단계별(step-by-step)에 따라 투입시간이 도달한 강재는 위에 적 치되어 있는 강재들의 이동없이 삭제되도록 단순화하였고, 신경망에 의해 업데이트되는 상태(state)는 투입 강 재와 강재 적치장의 적치정보를 함께 포함하도록 구현되었다. 아울러, 도 6 및 도 7을 참조하면, 도 6은 본 발명의 실시예에서 강화학습 알고리즘의 학습에 사용된 강재정보 의 일부를 표로 정리하여 나타낸 도면이고, 도 7은 본 발명의 실시예에서 사용된 입력함수(input function)들을 표로 정리하여 나타낸 도면이다. 도 6의 표에 나타낸 바와 같이, 본 실시예에서는, 자재번호(plate number), 입고일(Loading date) 및 절단일자 (Fabrication date)에 대한 데이터를 강재의 정보로서 환경(environment)에 로딩하게 되며, 문제의 크기에 따라 학습에 포함되는 강재의 수 및 학습전략에 따른 학습 범위를 조정하도록 하였고, 이러한 데이터의 입력을 위해 도 7의 표에 나타낸 바와 같은 입력함수들이 사용되었다. 계속해서, 보상(Reward)에 대하여 설명하면, 먼저, 본 발명의 실시예에서 적용된 보상을 설명하기 위해, 4개의 파일(pile)이 존재하고 각 파일당 최대로 적치 가능한 강재의 수가 4개라고 가정하여, 4×4 크기의 상태공간을 가지는 강재적치장 문제를 가정한다. 이때, 각각의 파일에 있어서, 스택(stack)은 가장 아래(bottom)부터 차례대로 넘버링되며, 즉, 가장 아래에 위 치한 스택이 0번 스택이고, 에이전트(agent)가 첫번째 파일(pile)에 강재를 쌓는 행동(action)을 취했다고 가정 한다. 더 상세하게는, 먼저, 도 8을 참조하면, 도 8은 파일에 적치되어 있는 강재의 수가 1개인 경우를 나타내는 도면 이다. 도 8에 나타낸 바와 같이, 특정 파일에 적치되는 강재의 수가 1개이면 선별을 위해 크레인을 사용할 일이 없고, 또한, 아래에 적치되어 있는 강재도 없으므로 크레인의 추가 사용 여부를 판단할 수 있는 기준이 없으며, 본 실 시예에서는 이러한 경우 보상(reward)을 0으로 정의하였다. 다음으로, 도 9를 참조하면, 도 9는 1개의 파일에 2개 이상의 강재가 적치되고 추가적인 크레인 사용횟수는 0인 경우를 나타내는 도면이다. 도 9에 나타낸 바와 같이, 첫번째 스택에 위치한 강재의 경우 불출일이 3이고 그 위에 쌓여있는 강재들 중 이보 다 더 늦은 불출일을 가진 강재의 수는 0이므로, 첫번째 스택에 있는 강재의 불출일에 강재를 불출하기 위해 필 요한 추가 크레인 사용횟수는 0회이다. 이어서, 두번째 스택에 위치한 강재의 경우에도 불출일이 2이므로 그 위에 쌓여있는 강재들 중 이보다 더 늦은 불출일을 가진 강재의 수는 0이고, 이에, 두번째 스택에 있는 강재의 불출일에 해당 강재를 빼내기 위한 추가적 인 크레인 사용횟수도 0회이며, 따라서 해당 파일에서 강재를 불출하기 위해 필요한 추가 크레인 사용횟수는 0 이 된다. 아울러, 이러한 적치상황은 바람직한 경우에 해당되므로, 본 실시예에서는 가장 높은 보상값인 2를 부여하도록 하였다. 다음으로, 도 10을 참조하면, 도 10은 1개의 파일에 2개 이상의 강재가 적치되고 추가적인 크레인 사용횟수가 0 이 아닌 경우를 나타내는 도면이다. 도 10에 나타낸 바와 같이, 첫번째 스택에 위치한 강재의 경우 불출일이 1이므로 0번 스택의 위에 쌓여있는 강 재들 중 이보다 더 늦은 출고일을 가진 강재의 수는 3개이며, 따라서 첫번째 스택에 있는 강재의 불출일에 해당 강재를 빼내기 위한 추가적인 크레인 사용횟수는 3회이다. 이어서, 두번째 스택에 위치한 강재의 경우 불출일이 2이므로 두번째 스택 위에 쌓여있는 강재들 중 이보다 더 늦은 출고일을 가진 강재는 세번째 스택에 위치한 강재(불출일 : 4)와 네번째 스택에 위한 강재(불출일: 3)의 2 개이며, 따라서 두번째 스택에 있는 강재(불출일 : 2)의 출고일에 해당 강재를 빼내기 위한 추가적인 크레인 사 용횟수는 2회이다. 또한, 나머지 세번째와 네번째의 경우 강재 수가 2개로 이미 첫번째 스택에 대한 계산값(추가 크레인 사용횟수 3) 3보다 작으므로 해당 파일에 대한 보상(reward) 계산은 종료되며, 이 경우 본 실시예에서는 최종적으로 추가 크레인 사용횟수의 최대값인 3의 역수를 보상값으로 할당하였다. 계속해서, 상기한 바와 같이 하여 구성되는 본 발명의 실시예에 따른 환경(environment), 상태(state) 및 보상 (reward)을 이용하여 DQN(Deep Q-Network), A2C(Advantage Actor-Critic), A3C(Asynchronous Advantage Actor-Critic) 알고리즘을 각각 적용한 결과에 대하여 설명한다. 먼저, DQN 알고리즘의 경우, 본 실시예에서는 적은 수의 레이어(layer)를 가지는 인공신경망으로 시작하여 다음 과 같이 레이어를 추가해 가면서 학습이 잘되는 방향으로 모델을 수정하였다. Case 1 : 10개 노드를 가진 1개의 은닉층으로 이루어진 인공신경망 Case 2 : 2개의 컨벌루션층(convolution layer)을 추가(4×4 필터(filter), 2×2 필터) Case 3 : 모델 2와 동일한 인공신경망에 강재 데이터의 수를 증가 즉, 도 11을 참조하면, 도 11은 상기한 각각의 경우에 대하여 DQN 알고리즘의 시험결과를 나타내는 도면이다. 도 11에 나타낸 바와 같이, Case 1과 같이 단일 은닉층의 인공신경망으로 학습하였을 경우 작은 크기의 문제에 대해서도 보상이 수렴하지 않는 것을 확인할 수 있었으며, 이에 대한 개선을 위해 Case 2와 같이 CNN 층을 2개 추가하고 학습을 시켰을 경우 Case 1과 동일한 모델에 대하여는 보상(reward)이 수렴되어 학습이 가능함을 확인 하였다. 그러나 Case 3과 같이 문제의 크기가 커지게 되면 보상이 수렴하지 않게 되어 DQN 학습 알고리즘은 강재 적치문 제에 적합하지 않은 것으로 판단하였다. 다음으로, A2C 학습 알고리즘의 경우, DQN과 마찬가지로 2개의 레이어를 가지는 인공신경망에서 시작하여 다음 과 같이 레이어를 추가하는 방향으로 수정하여 비교하였다. Case 1 : 각 15개 노드를 가진 2개의 은닉층으로 이루어진 인공신경망 Case 2 : 2개의 컨벌루션층(convolution layer)을 추가하고(4×4 필터(filter), 2×2 필터) 강재 데이터의 수 를 증가 즉, 도 12를 참조하면, 도 12는 상기한 각각의 경우에 대하여 A2C 알고리즘의 시험결과를 나타내는 도면이다. 도 12에 나타낸 바와 같이, Case 1에 대한 보상(reward) 결과로부터 입고 강재의 수가 적은 경우에 대하여는 DQN과 마찬가지로 A2C의 경우에도 원하는 방향으로 학습이 진행됨을 확인하였으나, Case 2에 대한 보상(reward) 결과에서 확인할 수 있듯이 문제의 크기(입고 강재의 수가 증가)가 커지면 보상이 수렴하지 않게 되어 A2C 또한 실제 문제에는 적용이 어렵다고 판단하였다. 다음으로, A3C 학습 알고리즘의 경우, DQN 및 A2C에서 실패한 입고 강재의 수가 많은 경우에 대하여도 제대로 학습이 이루어짐을 확인할 수 있었으며, 이에, A3C 학습에 대하여는 고정된 강재 입고순서에 대한 학습의 경우 와 임의의 순서를 가지는 강재 입고순서에 대한 비교를 수행하였다. 또한, A3C부터는 환경(environment)의 학습과정에 대한 에피소드(episode)별 거동을 시각적으로 확인하기 용이 한 GIF(Graphic Interchange Format)로 출력하여 결과를 비교하였으며, 이하에 설명하는 실시예에 있어서, GIF 이미지에서 색깔이 진할수록 입고일이 많이 남은 강재를 나타낸다. 아울러, 본 실시예에서는, 학습결과에 대한 추가적인 분석을 위해 \"이동(move)\"이라는 지표(indicator)를 도입 하여 강재 이송에 필요한 추가적인 크레인 이용횟수를 계산하였으며, 크레인 시뮬레이션이 정교하지는 않지만 이동 지표의 도입을 통해 분석 사례(analysis case) 사이의 상대적인 결과를 합리적으로 비교할 수 있다. 먼저, 고정된 강재 입고순서에 대한 학습에 대하여 설명하면, 본 실시예에서는 파일(pile) 수 8, 최대 스택높이 (stack height) 15의 적치장(분석대상 조선소의 제 1 적치구역(1st sorting area))에 38개의 강재를 적치하는 경우에 대한 학습을 수행하였으며, 38개의 강재를 적치하는 것을 하나의 에피소드로 하여 10,000회의 에피소드 를 진행하였고, 38개 강재의 순서는 일정하게 유지하였다. 상기한 바와 같이 하여 학습을 수행하였을 경우 보상은 약 17,000 에피소드 후 거의 수렴함을 확인하였으며, 에 피소드별 GIF 이미지 확인을 통해 적치기간이 긴 강재가 파일 하위에 적치되는 것을 확인할 수 있었다. 즉, 도 13 내지 도 17을 참조하면, 도 13 내지 도 17은 상기한 바와 같이 하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 각각 나타내는 도면으로, 도 13은 각 에피소드별 보상을 그래프로 나타낸 도면이고, 도 14는 학습결과로서 길이(Length), 보상(reward) 및 이동(move)을 표로 나타낸 도면이며, 도 15 내지 도 17은 각각 1,000, 10,000 및 50,000 에피소드 후의 적치결과를 나타내는 도면이다. 계속해서, 임의의 강재 입고순서에 대한 학습결과에 대하여 설명하면, 본 실시예에서는, 상기한 바와 같은 동일 한 환경에 대하여 38개 강재의 투입순서를 에피소드마다 임의로 변경하여 인공신경망의 범용성을 시험하였다. 또한, 이 경우 임의의 강재 입고순서로 인하여 문제가 복잡해졌기 때문에 50,000회의 에피소드에 대한 학습을 진행하였고, 학습결과 약 40,000회의 에피소드 이후 보상(reward)이 수렴하는 것을 확인하였으며, 이는 고정된 강재 입고순서에 대한 학습과 비교하여 약 6 ~ 7배의 연산이 필요함을 의미한다. 즉, 도 18 내지 도 22를 참조하면, 도 18 내지 도 22는 상기한 바와 같이 임의의 강재 입고순서에 대한 A3C 알 고리즘의 학습결과를 각각 나타내는 도면으로, 도 18은 각 에피소드별 보상을 그래프로 나타낸 도면이고, 도 19 는 학습결과로서 길이(Length), 보상(reward) 및 이동(move)을 표로 나타낸 도면이며, 도 20 내지 도 22는 각각 5,000, 30,000 및 50,000 에피소드 후의 적치결과를 나타내는 도면이다. 도 18 내지 도 22에 나타낸 바와 같이, 상기한 도 13 내지 도 17의 경우와 마찬가지로 에피소드별 GIF 이미지 확인을 통해 적치기간이 긴 강재가 파일 하위에 적치되는 것을 확인할 수 있었다. 다음으로, 본 발명자들은, 파일(pile) 수 20, 최대 스택(stack) 높이 20의 주 적치장(main stock)에 대하여 254개(약 1주일치 입고 강재 수량)의 입고 강재를 적치하는 경우에 대한 학습결과를 시험하였다. 먼저, 첫번째 테스트에서 각 파일의 최대높이를 20으로 설정했을 때 에피소드마다 254개의 강재를 모두 적치할 수 없었고, 81.7%의 에피소드에서 파일의 최대높이가 20개를 초과하면 학습이 강제로 종료되었으며, 이러한 경 우가 발생하면 학습결과에 바람직하지 않으므로, 다음 학습에서 파일의 최대높이를 20에서 30으로 증가시켰다. 즉, 도 23 및 도 24를 참조하면, 도 23은 파일의 최대높이가 20일때 주 적치구역에 대한 A3C 알고리즘의 학습에 서 각 에피소드의 길이를 그래프로 나타낸 도면이고, 도 24는 파일의 최대높이가 30일때 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습의 보상을 그래프로 나타낸 도면이다. 이러한 최대 스택 높이 증가에 따라 고정 입력과정(fixed input sequence)(입력날짜가 고정된 254개의 강재)에 대하여 80,000개의 에피소드가 학습되었고, 마찬가지로, 입력날짜가 고정된 샘플에 대하여 각 에피소드의 순서 가 동일한 확률분포로 변경된 랜덤 입력과정(random input sequence)에 대해 200,000개의 에피소드를 학습하였 다. 여기서, 상기에 설명한 단순화 모델에서 고정 및 가변 순서에 대한 보상 수렴(reward convergence)의 경향을 고 려하여, 랜덤 입력과정에서의 수렴속도가 고정 입력과정보다 2 ~ 3배 느리므로 학습 횟수가 다르다. 더 상세하게는, 도 25 내지 도 28을 참조하면, 도 25는 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타낸 도면이고, 도 26 내지 도 28은 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 80,000 에피소드의 80, 96, 254 단계에서의 적치결과를 각각 나 타내는 도면이다. 도 26에 나타낸 바와 같이, 약 80 단계까지 양호한 학습이 계속되고, 반면, 도 27에 나타낸 바와 같이, 수령일 이 늦은 강재는 다음 단계를 위해 강재가 수신될 때 적치 위치가 부족함으로 인해 상대적으로 수령일자가 가까운 강재 위에 적치되었으며, 도 28에 나타낸 바와 같이, 강재가 배치될 때 바람직하지 않은 상태를 유발하는 이 러한 반전상태가 해결되지 않은 상태로 계속되었다. 또한, 도 25의 표에 나타낸 바와 같이, 보상과 이동의 지표는 학습 에피소드가 증가할수록 개선(감소)되나, 이 는 최종 수렴상태가 최적이 아님을 나타낸다. 다음으로, 도 29 내지 도 33을 참조하면, 도 29 내지 도 33은 랜덤 입력과정에 대한 학습결과를 나타내는 도면 으로, 이러한 결과는 고정 입력과정과 비교하여 학습결과에 현저한 차이를 나타내지 않으며, 즉, 도 31 내지 도 33에 나타낸 바와 같이 바람직하지 않은 반전상태가 증가한다. 상기한 바와 같은 시험결과로부터, 작은 크기(small-size) 및 입력문제에 대한 이전의 학습결과와 달리 입력 및 상태의 크기가 증가함으로 인해 바람직하지 않은 학습으로 이어지는 것을 확인 할 수 있으며, 이러한 결과는 입 력된 강재의 수가 많거나 상대적으로 작은 상태공간이 고려됨을 나타낸다. 즉, 254개의 강재에 대한 성공적인 학습을 위하여는 테스트에 사용되는 상태 공간의 크기인 20개의 파일과 최대 스택인 30개보다 더 큰 공간이 필요하며, 이는, 충분한 컴퓨팅 파워가 있으면 더 큰 모델에 대하여 양호한 인공 신경망을 얻을 수 있음을 의미한다. 반면, 실험실 규모에서 사용 가능한 서버 컴퓨터상에서 250 에피소드에 대하여 약 40 ~ 50분 소요되므로, 100,000 에피소드를 학습하기 위해 약 11일이 걸리는 상황에서 다양한 실험을 진행하기에는 한계가 있다(실제로, 200,000 에피소드의 학습에는 약 3주가 소요됨). 이에, 본 발명의 실시예에서는, 최대 20개의 파일과 25개의 스택을 가지는 상태공간에서 성공적으로 학습이 이 루어질 수 있는 입력 강재의 수를 분석하였다. 즉, 도 34 및 도 35를 참조하면, 도 34 및 도 35는 20개의 파일과 25개의 최대 스택 및 150 에피소드을 가지는 경우에 대한 A3C 알고리즘의 학습에서의 보상 및 이동을 각각 그래프로 나타낸 도면이다. 또한, 도 36 및 도 37을 참조하면, 도 36은 상기한 경우에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타 낸 도면이고, 도 37은 상기한 경우에 대한 A3C 알고리즘의 학습결과로서 150 에피소드의 150 단계에서의 적치결 과를 나타내는 도면이다. 상기한 바와 같이, 본 발명의 실시예에서는, 강화학습을 이용하여 조선소의 강재 적치장에 적치된 강재의 최적 위치를 결정할 수있는 인공신경망을 개발하기 위해, 조선소에서 강재의 크레인 운송 횟수를 최소화하는 알고리 즘을 제시하였으며, 실험결과 A3C 알고리즘이 A2C 및 DQN 알고리즘보다 성능이 더 높은 것으로 나타났고, 이에, A3C 알고리즘을 이용하여 다양한 경우에 강판의 위치 결정을 위한 인공신경망 학습을 수행하였으며, 그 결과, 20개의 파일 모델에 대해 적용 가능한 학습결과를 도출하기 위해 최대 150개의 강재에 대하여 성공적인 학습이 가능함을 확인하였다. 여기서, 상기한 본 발명의 실시예에서는 조선소의 강재 적치장을 대상으로 하여 본 발명의 실시예에 따른 강화 학습을 이용한 입고물품의 적치순서 최적화 방법을 설명하였으나, 본 발명은 반드시 이러한 경우로만 한정되는 것은 아니며, 즉 본 발명은 상기한 조선소의 강재 적치장뿐만 아니라 이와 유사한 특성을 가지는 물류 시스템에도 동일 내지 유사하게 하여 적용이 가능한 것임에 유념해야 한다. 즉, 예를 들면, 컨테이너 부두의 컨테이너 터미널에서 컨테이너 박스를 효율적으로 적재하기 위해 본 발명의 실 시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 적용할 수도 있고, 또는, 비교적 규모가 큰 제품을 다루는 화물창고 등에 대하여 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 적용할 수 있다. 아울러, 예를 들면, 메타휴리스틱(Meta-Heuristic)이나 제약만족기법(Constraints Satisfaction Problem ; CSP) 또는 정수최적화 등과 같은 기존의 최적화 알고리즘들은 정해진 문제에 대한 해답만을 도출하므로 문제 또 는 문제에 포함되는 변수들의 값이 변경되면 최적화 계산을 다시 해야 하고, 문제의 크기에 따라 계산시간이 크 게 늘어나는 문제가 있으나, 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 이용하면, 컨테이너 부두 또는 강재 적치장에 임의의 순서로 반입되는 물품들의 적치순서를 각 물품들의 반출일 자를 고려하여 선별작업을 위한 이동 횟수가 최소화할 수 있는 데 더하여, 강화학습과 심층 인공신경망 기술을 적용하여 입력정보가 변경되어도 재계산 없이 적용이 가능한 장점을 가지는 것이다. 상기한 바와 같이, 본 발명에서는, 상기한 바와 같이 하여 실제 조선소의 강재 적치장의 데이터를 적용하여 강 재 적치장에 임의의 반출일자를 가지고 투입되는 강재들의 적치위치를 결정하는 문제에 본 발명의 실시예에 따 른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 적용하여 성능을 검증하였으며, 그 결과, 본 발명은 다양한 환경 구성을 통해 다양한 인공지능을 학습시키는 것에 의해 화물 및 물류에 관련된 다양한 분야에 걸쳐 폭넓게 적용 가능한 것임을 확인하였다. 따라서 상기한 바와 같이 하여 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법을 구현할 수 있으며, 그것에 의해, 본 발명에 따르면, 강화학습 문제의 상태를 각 강재의 계획된 반출일까지의 남 은 시간을 포함하도록 정의하고, 빠른 반출일을 가지는 강재가 파일의 위쪽에 적치되는 경우에 보상이 더 큰 값 을 갖도록 설정하여 투입일정에 따라 강재가 정렬되는 방식으로 에이전트의 학습이 이루어지도록 구성되는 강화 학습을 이용한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 강재의 가공일정을 고려하여 입고된 강재의 적치파일을 결정하는 것에 의해 선별작업을 최소화하는 적치계획을 수행할 수 있으며, 그것에 의해, 크레인의 사용을 최소화하고 전체적인 작업시간 및 비용을 절감하여 생산성 향상에 기여할 수 있다. 또한, 본 발명에 따르면, 상기한 바와 같이 컨테이너나 강재를 적치할 때 투입 또는 반출 일자를 고려한 위치선 정을 통해 크레인의 이동 횟수를 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 선박의 생산일정을 고려하여 강재가 투입일정에 따라 정렬된 상태로 적치되어 있다면 별 도로 수행할 필요가 없는 선별작업이 요구됨으로 인해 전체적인 작업기간 및 비용 증가의 요인이 되는 문제가 있었던 종래기술의 조선소의 강재 적치방법의 문제점을 해결할 수 있다. 아울러, 본 발명에 따르면, 상기한 바와 같이 물품의 반출일정을 고려하여 적치순서를 최적화하는 것에 의해 선 별작업을 최소화할 수 있도록 구성되는 강화학습을 이용한 입고물품의 적치순서 최적화 방법이 제공됨으로써, 단순히 입고일을 기준으로 물품을 적치함으로 인해 계획된 일정에 따라 반출하기 위하여는 반출될 물품을 다시 정돈하는 선별작업이 요구되었던 기존의 물품 적치방식들의 문제점을 해결할 수 있다. 이상, 상기한 바와 같은 본 발명의 실시예를 통하여 본 발명에 따른 강화학습을 이용한 입고물품의 적치순서 최 적화 방법의 상세한 내용에 대하여 설명하였으나, 본 발명은 상기한 실시예에 기재된 내용으로만 한정되는 것은"}
{"patent_id": "10-2021-0032819", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "아니며, 따라서 본 발명은, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 설계상의 필요 및 기 타 다양한 요인에 따라 여러 가지 수정, 변경, 결합 및 대체 등이 가능한 것임은 당연한 일이라 하겠다. 도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17 도면18 도면19 도면20 도면21 도면22 도면23 도면24 도면25 도면26 도면27 도면28 도면29 도면30 도면31 도면32 도면33 도면34 도면35 도면36 도면37"}
{"patent_id": "10-2021-0032819", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법의 전체적인 구성을 개략 적으로 나타내는 플로차트이다. 도 2는 본 발명의 실시예에 따른 강화학습을 이용한 입고물품의 적치순서 최적화 방법에 적용되는 강화학습 알 고리즘의 기본적인 개념을 개략적으로 나타내는 도면이다. 도 3은 실제 조선소의 강재 적치장에 대한 강재 입출고 과정을 개략적으로 나타내는 도면이다. 도 4는 A3C 알고리즘의 의사 코드(pseudo code)를 나타내는 도면이다. 도 5는 본 발명의 실시예에서 강화학습을 위해 인공신경망에 입력되는 환경의 상태(state)에 대한 정의를 개략적으로 나타내는 개념도이다. 도 6은 본 발명의 실시예에서 강화학습 알고리즘의 학습에 사용된 강재정보의 일부를 표로 정리하여 나타낸 도 면이다. 도 7은 본 발명의 실시예에서 사용된 입력함수들을 표로 정리하여 나타낸 도면이다. 도 8은 파일에 적치되어 있는 강재의 수가 1개인 경우를 나타내는 도면이다. 도 9는 1개의 파일에 2개 이상의 강재가 적치되고 추가적인 크레인 사용횟수는 0인 경우를 나타내는 도면이다. 도 10은 1개의 파일에 2개 이상의 강재가 적치되고 추가적인 크레인 사용횟수가 0이 아닌 경우를 나타내는 도면 이다. 도 11은 각각의 경우에 대하여 DQN 알고리즘의 시험결과를 나타내는 도면이다. 도 12는 각각의 경우에 대하여 A2C 알고리즘의 시험결과를 나타내는 도면이다. 도 13은 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습에서 보상을 그래프로 나타낸 도면이다. 도 14는 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타낸 도면이다. 도 15는 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 1,000 에피소드 후의 적치결과를 나타내는 도면이다. 도 16은 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 10,000 에피소드 후 적치결과를 나타내는 도면이다. 도 17은 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 50,000 에피소드 후 적치결과를 나타내는 도면이다. 도 18은 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습에서 보상을 그래프로 나타낸 도면이다. 도 19는 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타낸 도면이다. 도 20은 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 5,000 에피소드 후의 적치결과를 나타내는 도면이다. 도 21은 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 30,000 에피소드 후 적치결과를 나타내는 도면이다. 도 22는 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 50,000 에피소드 후 적치결과를 나타내는 도면이다. 도 23은 주 적치구역에 대한 A3C 알고리즘의 학습에서 각 에피소드의 길이를 그래프로 나타낸 도면이다. 도 24는 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습에서 보상을 그래프로 나타낸 도면이다. 도 25는 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타낸 도면이다. 도 26은 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 8,000 에피소드의 80 단계에서의 적치결과를 나타내는 도면이다. 도 27은 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 80,000 에피소드의 96 단계에서의 적치결과를 나타내는 도면이다. 도 28은 주 적치구역에 대하여 고정된 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 80,000 에피소드의 254 단계에서의 적치결과를 나타내는 도면이다. 도 29는 주 적치구역에 대하여 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습에서 보상을 그래프로 나타낸 도면이다. 도 30은 주 적치구역에 대하여 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과를 표로 정리하여 나타낸도면이다. 도 31은 주 적치구역에 대하여 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 200,000 에피소드의 80 단계에서의 적치결과를 나타내는 도면이다. 도 32는 주 적치구역에 대하여 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 200,000 에피소드의 98 단계에서의 적치결과를 나타내는 도면이다. 도 33은 주 적치구역에 대하여 임의의 강재 입고순서에 대한 A3C 알고리즘의 학습결과로서 200,000 에피소드의 254 단계에서의 적치결과를 나타내는 도면이다. 도 34는 20개의 파일과 25개의 최대 스택 및 150 에피소드을 가지는 경우에 대한 A3C 알고리즘의 학습에서 보상 을 그래프로 나타낸 도면이다. 도 35는 20개의 파일과 25개의 최대 스택 및 150 에피소드을 가지는 경우에 대한 A3C 알고리즘의 학습에서 이동 을 그래프로 나타낸 도면이다. 도 36은 20개의 파일과 25개의 최대 스택 및 150 에피소드을 가지는 경우에 대한 A3C 알고리즘의 학습결과를 표 로 정리하여 나타낸 도면이다. 도 37은 20개의 파일과 25개의 최대 스택 및 150 에피소드을 가지는 경우에 대한 A3C 알고리즘의 학습결과로서 150 에피소드의 150 단계에서의 적치결과를 나타내는 도면이다."}
