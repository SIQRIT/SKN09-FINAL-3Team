{"patent_id": "10-2023-0112621", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0031321", "출원번호": "10-2023-0112621", "발명의 명칭": "한글 기본단위 임베딩 합성 방법 장치, 방법 및 이를 구현하기 위한 컴퓨터로 판독 가능한 저", "출원인": "고려대학교 산학협력단", "발명자": "이상근"}}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "한국어 데이터를 토큰화하여 기본 단위 시퀀스를 생성하는 단계; 및상기 기본 단위 시퀀스를 서브워드 시퀀스로 합성하는 단계;를 포함하는, 한글 기본단위 임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 기본단위 시퀀스는 BTS 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 단계는,상기 BTS 시퀀스를 자모 단위 시퀀스로 합성하는 단계; 상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하는 단계; 및상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 단계;를 포함하는, 한글 기본단위 임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 기본단위 시퀀스는 자모 단위 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 단계는,상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하는 단계; 및상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 단계;를 포함하는, 한글 기본단위 임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 2항 또는 제 3항에 있어서,상기 음절 단위 시퀀스로 합성하는 단계는,상기 자모 단위 시퀀스 중 초성과 중성을 병합하는 단계; 및병합된 상기 초성과 중성을 1행으로 배치하고, 종성을 2행으로 배치하는 2 X N 행렬을 생성하는 단계; 및상기 2 X N 행렬을 CNN 계층에 입력하여 음절 단위 시퀀스로 합성하는 단계; 를 포함하며, 여기서 N은 음절 단위 시퀀스의 길이인, 한글 기본단위 임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 2항 또는 제 3항에 있어서,상기 음절 단위 시퀀스로 합성하는 단계는,상기 자모 단위 시퀀스를 초성, 중성, 종성, 및 특수기호로 배치된 2 X N 행렬을 구성하는 단계; 및상기 2 X N 행렬을 CNN 계층 및 평균 풀링 계층에 순차적으로 입력하여 음절 단위 시퀀스로 합성하는 단계;를공개특허 10-2025-0031321-3-포함하며 여기서 N은 음절 단위 시퀀스의 길이인, 한글 기본단위 임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 2항 또는 제 3항에 있어서,서브워드 시퀀스로 합성하는 단계는,상기 음절 단위 시퀀스를 서브워드 경계 예측을 수행하여 서브워드 시퀀스로 합성하는 단계인, 한글 기본단위임베딩 합성 방법."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 하나의 항에 따른 상기 한글 기본단위 임베딩 합성 방법을 수행하기 위한 컴퓨터 프로그램이 기록된 컴퓨터로 판독 가능한 저장 매체"}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "한글 기본단위 임베딩 합성 장치로서, 프로세서; 상기 프로세서에 연결되는 메모리를 포함하되, 상기 메모리는, 한국어 데이터를 토큰화하여 기본 단위 시퀀스를 생성하며,상기 기본 단위 시퀀스를 서브워드 시퀀스로 합성하도록 상기 프로세서에 의해 실행되는 프로그램 명령어들을저장하는, 한글 기본단위 임베딩 합성 장치."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8항에 있어서,상기 기본단위 시퀀스는 BTS 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 것은,상기 BTS 시퀀스를 자모 단위 시퀀스로 합성하고; 상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하며;상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 것을 포함하는, 한글 기본단위 임베딩 합성 장치."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 8항에 있어서,상기 기본단위 시퀀스는 자모 단위 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 것은,상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하며; 및상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 것을 포함하는, 한글 기본단위 임베딩 합성 장치.공개특허 10-2025-0031321-4-청구항 11 제 9항 또는 제 10항에 있어서,상기 음절 단위 시퀀스로 합성하는 것은,상기 자모 단위 시퀀스 중 초성과 중성을 병합하고,병합된 상기 초성과 중성을 1행으로 배치하고, 종성을 2행으로 배치하는 2 X N 행렬을 생성하며, 상기 2 X N 행렬을 CNN 계층에 입력하여 음절 단위 시퀀스로 합성하는 것을 포함하며, 여기서 N은 음절 단위 시퀀스의 길이인, 한글 기본단위 임베딩 합성 장치."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 9항 또는 제 10항에 있어서,상기 음절 단위 시퀀스로 합성하는 단계는,상기 자모 단위 시퀀스를 초성, 중성, 종성, 및 특수기호로 배치된 2 X N 행렬을 구성하고,상기 2 X N 행렬을 CNN 계층 및 평균 풀링 계층에 순차적으로 입력하여 음절 단위 시퀀스로 합성하는 것을 포함하며, 여기서 N은 음절 단위 시퀀스의 길이인, 한글 기본단위 임베딩 합성 장치."}
{"patent_id": "10-2023-0112621", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 9항 또는 제 10항에 있어서,서브워드 시퀀스로 합성하는 것은,상기 음절 단위 시퀀스를 서브워드 경계 예측을 수행하여 서브워드 시퀀스로 합성하는 것인, 한글 기본단위 임베딩 합성 장치."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 한국어 데이터를 토큰화하여 기본 단위 시퀀스를 생성하는 단계; 및 상기 기본 단위 시퀀스를 서브워 드 시퀀스로 합성하는 단계;를 포함하는, 한글 기본단위 임베딩 합성 방법 및 장치에 관한 것이다. 이에 의해 적 대적 공격 및 오탈자 노이즈 데이터에서 언어모델의 강건성을 높이기 위해 입력을 작은 단위 토큰으로 분해하는 과정에서 발생하는 긴 시퀀스 길이 문제를 효과적으로 압축하여 해결한다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 한글 기본단위 임베딩 합성 방법 장치, 방법 및 이를 구현하기 위한 컴퓨터로 판독 가능한 저장 매체 로서, 더욱 상세하게는 작은 단위를 합성하여 상위 개념 단어인 형태소를 포함한 서브워드를 표현하기 때문에 큰 어휘사전이 필요하지 않아 계산 비용을 효과적으로 줄일 수 있는 한글 기본단위 임베딩 합성 방법 장치, 방 법 및 이를 구현하기 위한 컴퓨터로 판독 가능한 저장 매체에 관한 것이다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "문자가 나열된 텍스트 내 문장을 언어모델(Language model)에 입력(Input)으로 넣기 위해서는 모델이 이해할 수 있는 단위인 ‘토큰(Token)’으로 분해해야 한다. 토큰으로 분해하는 과정을 ‘토큰화(Tokenization)’라 하며, 토큰화를 수행하는 주체를 ‘토크나이저(Tokenizer)’라 한다. 토큰의 단위에는 언어 및 분해 규칙에 따라 매우 다양한 종류가 있으며, 문장을 분해하여 표현한 이 토큰들의 나열을 ‘시퀀스(Sequence)’라고 한다. 어느 수준 의 토큰 단위로 문장을 분해하여 시퀀스를 언어모델에 넣는지에 따라 모델의 성능이 크게 좌우된다. 특히 한국 어를 분해할 수 있는 토큰 단위에는 다음과 같이 세 가지 대분류가 존재한다. 1) 규칙기반으로 분류하는 음절 (Syllable), 자음과 모음(Consonant and Vowel)(이하 ‘자모’라 칭함), 스트로크(Stroke), 천지인(Cji), 비티 에스(BTS; Basic, Tiniest Subword) (스트로크, 천지인은 비티에스 단위의 파생이므로 이 세 가지 단위들을 통 칭하여 이하 ‘BTS유닛’이라 칭함), 2) 데이터에 등장하는 글자 및 단어들의 등장빈도를 기준으로 바이트 페어 인코딩(Byte Pair Encoding, BPE)을 적용해 만든 서브워드(Subword), 3) 형태학적으로 풍부한 교착어인 한국어 의 언어적 특성을 기반으로 하는 형태소(Morpheme) 그리고 형태소로 분해된 데이터에 BPE를 적용하여 만든 형태소인식 서브워드(Morpheme-aware subword)가 있다. 그 중 현재 자연어이해 및 기계번역과 같은 다양한 자연어처 리 작업에서 가장 높은 성능을 보이는 것은 형태소인식 서브워드 단위를 기반으로 하는 언어모델이다. 하지만, 형태소인식 서브워드 단위로 토큰화하는 방법은 의미적(Semantic) 작업을 잘 수행하지만, 구문론적 (Syntactic) 작업에서는 강건함을 보이지 못하고 적대적 공격(Adversarial attack) 및 희귀 단어(Rare word)나 비표준어(Non-standard word)가 많은 노이지 데이터(Noisy data)에서는 취약한 모습을 보인다. 또한, 서브워드 로 단어를 표현하기 위해서는 데이터로부터 얻은 서브워드 토큰들을 모아 놓은 모음집인 ‘어휘사전(Lexicon)’ 이 필요한데, 어휘사전의 크기가 커질수록 비례하여 막대한 계산 비용이 소모된다. 특히 최근 자연어처리에서 가장 높은 성능을 달성하는 언어모델인 트랜스포머(Transformer) 계열 모델에서는 어휘사전의 크기가 커질수록 기하급수적으로 큰 계산 비용이 발생하게 된다. 이러한 문제를 해결하는 종래기술의 핵심은 토큰단위를 더욱 작 게 분해하여 작은 단위 토큰으로 상위 개념의 토큰을 표현하는 것이다. 좀 더 구체적으로는 서브워드를 더욱 작 게 자모, BTS유닛과 같은 단위로 분해함으로써 여러 개의 토큰들로 나눠 표현한 후 이를 순차적으로 합성하여 상위 개념 토큰인 음절 및 형태소를 포함한 서브워드로 표현하는 것이다. 작은 토큰 단위들만으로 서브워드를 표현하게 되면 어휘 사전도 그에 맞게 기본적으로 필요한 최소 단위 개수만 필요로 하게 되고, 어휘사전 밖 단 어(Out-of-vocabulary), 즉 표현하지 못하는 단어가 거의 존재하지 않게 된다. 이러한 특징으로 인해 작게 분해 할수록 구문론적 작업에서는 효과적이지만, 의미론적 작업에서는 서브워드처럼 큰 단위를 이용하는 언어모델보 다 뒤처지는 성능을 보인다. 또한, 작게 분해하면 분해할수록 한 문장을 구성하는 시퀀스의 길이, 즉 토큰의 개 수가 기하급수적으로 많아지게 되며 이는 언어모델에서 또 다른 계산 비용의 증가로 이어진다. 이를 해결하기 위해 최근 영미권에서는 영어의 가장 작은 단위인 character로 분해되어 길어진 시퀀스를 효과적으로 다운샘플 링(Downsampling)하여 시퀀스의 길이를 줄임과 동시에 의미론적 작업에서 성능도 높이는 방법을 제시했다. 여기 서 ‘다운샘플링’이라 함은 시퀀스 길이를 줄이는 다양한 모든 행위를 의미한다. 대표적인 예로 Charformer와 CANINE 같은 언어모델들이 있으며, CNN, Local Transformer, Block Scoring과 같은 신경망을 이용해 시퀀스 길 이를 다운샘플링했다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상기와 같은 문제점을 해결하기 위한 본 발명의 목적은 적대적 공격 및 노이지 데이터에서 언어모델의 강건성을 높이기 위해 입력을 작은 단위 토큰으로 분해하는 과정에서 발생하는 긴 시퀀스 길이 문제를 효과적으로 압축하 여 해결하는 한글 기본단위 임베딩 합성 방법 장치, 방법 및 이를 구현하기 위한 컴퓨터로 판독 가능한 저장 매 체를 제공하는 데 있다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 본 발명의 목적을 실현하기 위한 일 실시예에 따른 한글 기본단위 임베딩 합성 방법은, 한국어 데이터를 토큰화하여 기본 단위 시퀀스를 생성하는 단계; 및 상기 기본 단위 시퀀스를 서브워드 시퀀스로 합성하는 단 계;를 포함한다. 또한, 상기 기본단위 시퀀스는 BTS 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 단계는, 상기 BTS 시퀀스를 자모 단위 시퀀스로 합성하는 단계; 상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하는 단계; 및 상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 단계;를 포함할 수도 있다. 또한, 상기 기본단위 시퀀스는 자모 단위 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 단계는, 상기 자모 단 위 시퀀스를 음절 단위 시퀀스로 합성하는 단계; 및 상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 단계;를 포함할 수도 있다. 또한, 상기 음절 단위 시퀀스로 합성하는 단계는, 상기 자모 단위 시퀀스 중 초성과 중성을 병합하는 단계; 및 병합된 상기 초성과 중성을 1행으로 배치하고, 종성을 2행으로 배치하는 2 X N 행렬을 생성하는 단계; 및 상기 2 X N 행렬을 CNN 계층에 입력하여 음절 단위 시퀀스로 합성하는 단계를 포함하며, 여기서 N은 음절 단위 시퀀 스의 길이이다. 또한, 상기 음절 단위 시퀀스로 합성하는 단계는, 상기 자모 단위 시퀀스를 초성, 중성, 종성, 및 특수기호로 배치된 2 X N 행렬을 구성하는 단계; 및 상기 2 X N 행렬을 CNN 계층 및 평균 풀링 계층에 순차적으로 입력하여음절 단위 시퀀스로 합성하는 단계를 포함하며 여기서 N은 음절 단위 시퀀스의 길이이다. 또한, 서브워드 시퀀스로 합성하는 단계는, 상기 음절 단위 시퀀스를 서브워드 경계 예측을 수행하여 서브워드 시퀀스로 합성하는 단계일 수도 있다. 본 발명의 목적을 실현하기 위한 또 다른 실시예인 컴퓨터 프로그램이 기록된 컴퓨터로 판독 가능한 저장 매체 는 전술한 한글 기본단위 임베딩 합성 방법을 수행하기 위한 컴퓨터 프로그램이 기록되어 있다. 본 발명의 목적을 실현하기 위한 또 다른 실시예인 한글 기본단위 임베딩 합성 장치는, 프로세서; 상기 프로세 서에 연결되는 메모리를 포함하되, 상기 메모리는, 한국어 데이터를 토큰화하여 기본 단위 시퀀스를 생성하며, 상기 기본 단위 시퀀스를 서브워드 시퀀스로 합성하도록 상기 프로세서에 의해 실행되는 프로그램 명령어들을 저장한다. 또한, 상기 기본단위 시퀀스는 BTS 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 것은, 상기 BTS 시퀀스를 자 모 단위 시퀀스로 합성하고; 상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하며; 상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 것을 포함할 수도 있다. 또한, 상기 기본단위 시퀀스는 자모 단위 시퀀스이며, 상기 서브워드 시퀀스로 합성하는 것은, 상기 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하며; 및 상기 음절 단위 시퀀스를 서브워드 시퀀스로 합성하는 것을 포함할 수도 있다. 또한, 상기 음절 단위 시퀀스로 합성하는 것은, 상기 자모 단위 시퀀스 중 초성과 중성을 병합하고, 병합된 상 기 초성과 중성을 1행으로 배치하고, 종성을 2행으로 배치하는 2 X N 행렬을 생성하며, 상기 2 X N 행렬을 CNN 계층에 입력하여 음절 단위 시퀀스로 합성하는 것을 포함하며, 여기서 N은 음절 단위 시퀀스의 길이이다. 또한, 상기 음절 단위 시퀀스로 합성하는 단계는, 상기 자모 단위 시퀀스를 초성, 중성, 종성, 및 특수기호로 구성된 2 X N 행렬을 배치하고, 상기 2 X N 행렬을 CNN 계층 및 평균 풀링 계층에 순차적으로 입력하여 음절 단 위 시퀀스로 합성하는 것을 포함하며 여기서 N은 음절 단위 시퀀스의 길이이다. 또한, 서브워드 시퀀스로 합성하는 것은, 상기 음절 단위 시퀀스를 서브워드 경계 예측을 수행하여 서브워드 시 퀀스로 합성일 수도 있다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의해, 적대적 공격 및 오탈자 노이즈 데이터에서 언어모델의 강건성을 높이기 위해 입력을 작은 단위 토큰으로 분해하는 과정에서 발생하는 긴 시퀀스 길이 문제를 효과적으로 압축하여 해결한다. 또한, 자모 단위 시퀀스를 압축하여 음절을 표현하기 위해 음절을 구성하는 초성, 중성 종성의 위치 관계를 고 려하여 입체적으로 배열하고 컨볼루션(Convolution) 계층에 입력으로 넣음으로써 한글 고유의 언어적 특성을 반 영하여 효과적인 한국어 이해능력을 배울 수 있다. 또한, 작은 단위를 합성하여 상위 개념 단어인 음절 및 형태소를 포함한 서브워드를 표현하기 때문에 큰 어휘사 전을 필요로 하지 않아 계산 비용을 효과적으로 줄일 수 있다."}
{"patent_id": "10-2023-0112621", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명 의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의 해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된 다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유 사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가진 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 본 발명을 설 명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 종래의 종래의 서브워드 기반의 언어모델을 사용하는 시스템의 구조이다. 도 1을 참조하면, 인공지능 모 델이 학습을 진행하기 전 한국어 데이터를 전처리 하는 한국어 전처리부과 학습 가능부 두 단계 로 구성된다. 도 2는 본 발명의 일 실시예인 서브워드 기반의 언어모델을 사용하는 시스템의 구조이다. 도 2를 참조하면, 본 발명의 일 실시예인 서브워드 기반의 언어모델을 사용하는 시스템은 한국어 전처리부 에서 출력된 입력 기 본단위 시퀀스 를 학습하는 학습 가능부 를 포함한다. 학습 가능부 에서 출력딘 출력 서브워드 시퀀스 는 언어 모델로 입력된다. 종래의 서브워드 기반의 언어모델을 사용하는 시스템을 나타낸 도 1과 비교하여 보면, 한국어 전처리 구간(10 0)에서 한국어 기본단위 토크나이저 를 통해 토큰화되는 시퀀스의 구성 단위가 다르다. 도 1에서는 한국어 데이터가 한국어 서브워드 토크나이저를 통해 입력 서브워드 시퀀스로 분해되는 반면, 도 2에서 는 한국어 기본단위 토크나이저를 통해 입력 기본단위 시퀀스로 분해된다. 여기서 '기본단위’의 의 미는 컴퓨터에서 한 글자로 인식되는 한글의 최대 단위인 음절을 구성하는 단위로, 자모 단위 또는 BTS유닛을 의미한다. 이하 본 발명에서 '기본단위’의 기본값은 BTS 유닛으로 설명하지만, 자모 유닛부터 기본단위가 될 수도 있다. 언어모델로 입력되는 입력 서브워드 시퀀스가 한국어 서브워드 토크나이저를 통해 바로 생성되 는 도 1과 달리, 본 발명의 시스템인 도 2에서는 입력 기본단위 시퀀스를 입력 서브워드 시퀀스로 합성하는 과정이 추가되는 것이 특징이다. 이는 기본단위 임베딩 합성 모듈을 통해 수행된다. 임베딩이란 각 토큰을 대체하는 고유의 벡터값으로 학습 가능하다. 또한 ‘단위 합성’은 표면상으로는 문자의 합성으로 보이지만, 컴퓨터 내 계산상으로는 토큰의 대체값(벡터)인 임베딩이 하나의 임베딩으로 합성되는 것을 의미하므로 ‘임베딩 합성’과 동일한 의미로 사용한다. 도 3은 본 발명의 일 실시예인 기본단위 임베딩 합성 모듈의 구조도이다. 도 3을 참조하면, 기본단위 임베딩 합 성 모듈은 BTS유닛 단위 시퀀스 에서 입력 서브워드 시퀀스 로 합성하는 과정일 수도 있다. 또 는 자모 단위 시퀀스 에서 입력 서브워드 시퀀스 로 합성하는 과정일 수도 있다. 즉, 전술한 바와 같 이 한국어 기본단위 토크나이저 에 의해 입력 기본단위 시퀀스가 는 BTS유닛 단위 시퀀스 또는 자모 단위 시퀀스 일 수 있다. 여기서는 전술한 바와 같이 BTS유닛 단위 시퀀스 에서 입력 서브워드 시퀀스 로 합성하는 과정을 상 세히 설명한다. 입력 기본단위 시퀀스를 입력 서브워드 시퀀스로 합성하는 과정은 도 3에 도시된 바 와 같이 3개의 모듈(BTS 유닛 임베딩 합성 모듈 , 자모 임베딩 합성 모듈 , 및 서브워드 경계 예측 모듈 )을 거쳐 세 번의 단위 합성이 이루어진다. 첫 번째 단계는 BTS유닛 단위 시퀀스를 자모 단위 시퀀스로 변환하는 과정은 첫 번째 변환 모듈인 BTS유닛 임베딩 합성 모듈을 통해 변환된다. BTS유닛 임베딩 합성 모듈은 두 단계로 진행된다. 우선 각 BTS유닛 토큰들 중 같은 자모 단위 토큰으로 대응되는 토큰들 끼리 구분하고, 구분해 놓은 토큰끼리 단위 합 성을 하여 하나의 자모 단위로 변환한다. 임베딩 합성 방법으로는 합, 평균, GRU (Gated Recurrent Unit)계층일 수도 있다. 두 번째 단계는 자모 단위 시퀀스를 음절 단위 시퀀스로 합성하는 과정이다. 이 합성 과정은 도 3의 두 번째 변환 모듈인 자모 임베딩 합성 모듈을 통해 진행된다. 자모 임베딩 합성 모듈은 총 네 단계 로 진행된다. 첫 번째로 초성, 중성, 종성에 해당하는 세 개의 토큰마다 묶어 한 음절의 구성요소로 구분한다. 이때 초성, 중 성, 종성 3 개가 채워지지 않는 모든 글자들은 빈 자모 공간을 나타내는 특수기호()를 추가하여 구성한다. 두 번째로 초성, 중성, 종성 토큰 뒤에 한 개의 의미없는 패드토큰([PAD])을 더해 총 4개로 구성하고 이를 2x2 크기 행렬모양 시퀀스로 재배열한다. 이 경우, 일반적인 한글 음절의 모양을 고려해 초성, 중성을 1행에 배치하 고 종성과 패드 토큰을 2행에 배치한다. 세 번째로 재배열된 시퀀스를 2x2 크기 커널을 갖는 CNN 계층(도 4의 522(a))에 입력함으로써 한 음절 내 초성, 중성, 종성의 상대적 위치 정보와 이어지는 각 음절 사이의 상대적 위치 정보를 학습시킨다. 마지막으로 평균 풀링 계층(도 4의 522(b))을 통과시킴으로써 음절 단위 시퀀스를 출력으로 얻게 된다. 기본단위 임베딩 합성 모듈의 마지막 단계는 음절 단위 시퀀스를 입력 서브워드 시퀀스로 합성 하는 과정이다. 합성 과정은 서브워드 경계 예측 모듈을 통해 진행된다. 서브워드 경계 예측 모듈에 서는 두 개의 선형(Linear) 계층을 지난 후, 시그모이드(sigmoid)함수를 통해 정규화 된다. 이후 중앙값(0.5)를 기점으로 0 또는 1로 값이 이분화된다. 1의 값이 서브워드 단위의 경계를 의미하고, 1 값을 갖는 토큰을 경계로 그 사이에 있는 토큰들과 우측 경계에 있는 토큰들끼리 같은 집단으로 분류한다. 이후 각 집단에 속한 토큰들의 임베딩을 평균하여 서브워드 임베딩을 계산한다. 이 과정을 마지막으로 BTS유닛 단위 시퀀스가 입력 서브 워드 시퀀스로 합성되어 기본단위 임베딩 합성 모듈의 적용이 완료된다. 이후의 과정은 종래의 기술을 도시한 도 1과 동일하다. 언어모델에 합성한 입력 서브워드 시퀀스을 입력하고 출력 서브워드 시퀀스를 도출한다. 언어모델로는 BERT, RoBERTa 등 트랜스포머 인코더 계열 모델들 뿐만 아니라 GPT와 같은 디코더 계열 모델들에도 적용 가능하여 모든 언어모델에 일반적으로 적용 가능 하다. 도 4는 본 발명에 따라 2X2 행렬을 통해 서브워드 서브워드 시퀀스로 합성 과정의 예를 나타낸다. 도 4를 참조 하면, 가장 첫 번째 순서로 “한영 자판”이라는 한국어 데이터가 한국어 기본단위 토크나이저를 통 해 BTS유닛 단위 시퀀스 인 [ㅇ, -, ㅏ, ㄴ, ㅇ, ㅕ, ㅇ, _ , ㅅ, -, ㅏ, , ㅁ, -, -, ㅏ, ㄴ] 으로 변환된다. 이때, 특수기호(_)는 띄어쓰기를 의미하는 글자이며, 특수기호 '' 전술한 바와 같이 빈 자모 공간 을 대신하는 글자를 의미한다. 특수기호 ''는 초성 혹은 중성이 단독으로 존재하거나 종성 없이 초성과 중성만 있는 경우에 사용되고, 한글 외 다른 글자가 와도 2개의 특수기호()가 붙어 3개가 하나의 구성으로 토큰 화가 된다. 예를 들어, 'a’ 라는 글자가 오면 [a, , ]처럼 2개가 연속적으로 붙게 된다. 이는 후속 과정 인 자모 임베딩 합성 모듈에서 자모 단위 토큰들의 재배열을 원활히 진행하기 위해 3개의 자모 단위 토큰 을 만들기 위함이다. BTS유닛 단위 시퀀스를 [(ㅇ, -), (ㅏ), (ㄴ), (ㅇ), (ㅕ), (ㅇ), (_), (ㅅ, -), (ㅏ), (), (ㅁ, -, -), (ㅏ), (ㄴ)]처럼 같은 자모 단위 토큰으로 대응되는 BTS유닛 토큰들끼리 구분하고, 묶인 집합 내 토큰들끼 리 합성을 하기 위해, GRU 계층으로 사용된 BTS유닛 임베딩 합성 모듈 을 적용한 후 각 집합 내 마지막 위 치에 있는 토큰의 은닉 상태(hidden state)를 자모 단위 토큰의 임베딩으로 사용한다. 이 과정을 통해 자모 단 위 시퀀스인 [ㅎ, ㅏ, ㄴ, ㅇ, ㅕ, ㅇ, _, ㅈ, ㅏ, , ㅍ, ㅏ, ㄴ]를 구한다. 자모 단위 시퀀스를 3개 토큰마다 순서대로 묶고, 각 묶음에 패드토큰([PAD])을 하나씩 추가함으로써 각 토큰 묶음 당 4개의 토큰으로 구성하였다. 단 띄어쓰기를 나타내는 특수기호(_)가 있는 묶음은 단 하나의 자모 단위 토큰으로만 구성되어 있으므로 3개의 패드토큰을 추가하였다. 이렇게 4개의 자모 단위 토큰으로 묶은 시퀀 스는 [(ㅎ, ㅏ, ㄴ, [PAD]), (ㅇ, ㅕ, ㅇ, [PAD]), (_, [PAD], [PAD], [PAD]), (ㅈ, ㅏ, , [PAD]), (ㅍ, ㅏ, ㄴ, [PAD])] 이다. 각 묶음 내 4개의 자모 단위 토큰들을 도 4과 같이 2x2행렬로 재배열하여 2개 층을 갖는 시퀀스를 형성한다. 이 경우, 일반적인 한글 음절의 모양을 고려해 초성, 중성을 1행에, 종성과 패드 토큰을 2행에 배치한다. 이후 전 술한대로 재배열된 시퀀스를 2x2 크기 커널을 갖는 CNN 계층 (522(a))에 입력으로 넣음으로써 한 음절 내 초성, 중성, 종성의 상대적 위치 정보와 이어지는 각 음절 사이의 상대적 위치 정보를 학습시킨다. 이후 스트라이드 (stride)가 2인 평균 풀링 계층 (522(b))을 통과시킴으로써 음절 단위 시퀀스인 [한, 영, _, 자, 판]을 출 력으로 얻는다. 마지막으로 음절 단위 시퀀스를 서브워드 경계 예측 모듈을 이용해 서브워드 단위 토큰으로 합성한다. 음절 단위 시퀀스 [한, 영, _, 자, 판]를 서브워드 경계 예측 모듈에 입력하여 서브워드 경계 여부를 나타내는 수열을 다음과 같이 얻는다. [0, 1, 1, 0, 1]. 수열 값이 1이면 서브워드의 경계를 나타 내는 것으로 본 예시에서는 ‘영’, ‘_’, ‘판’ 가 해당된다. 경계를 기준으로 하여 음절 단위 시퀀스 를 묶으면[(한, 영), (_), (자, 판)]와 같이 묶이게 된다. 각 묶음 내 음절 단위 토큰들을 평균 합성하여 최종 적으로 입력 서브워드 시퀀스 [한영, _, 자판]을 얻는다. 이 서브워드 시퀀스가 언어모델의 입 력으로 들어감으로써 학습이 진행된다. 도 5는 한글의 초성, 중성, 종성의 결합을 블록 구조로 나타낸다. 도 5를 참조하면, 유형 1과 같이 '달', '장', '해' 와 같은 단어는 초성과 중성이 종성 위에 나란히 배치되고, 유형 2와 같이 '권', '월', '의' 과 같 은 단어는 종성위에 중성이 초성을 둘러쌍은 모양으로 배치되며, 유형 3과 같은 '봄', '글', '수'와 같은 단어 는 아래부터 종성, 중성, 초성 순으로 적층된 구조로 배치된다. 도 6은 종래 한글의 초성, 중성, 종성의 결합 구조의 분해를 나타낸다. 도 6을 참조하면, 도 6은 도 5에 도신된 유형 1 내지 유형 3의 초성, 중성, 종성의 블록을 각각 분해하여 도시한다. 도 7은 본 발명의 한글의 초성과 중성의 병합을 블록 구조를 나타낸다. 도 7을 참조하면, 각각 유형 1의 내지 유형 3의 예시인 '달', '권', '봄' 과 같은 단어의 블록 구조를, 초성과 중성을 병합한 경우, 우측과 같이 동일 한 블록 구조가 형성된다. 즉 모든 유형에서 초성과 중성을 동일한 블록으로 병합한 것을 볼 수 있다. 도 8은 본 발명에 따라 초성과 중성을 병합을 통해 서브워드 서브워드 시퀀스로 합성 과정의 예를 나타낸다. 도 8을 참조하면, 가장 첫 번째 순서로 “감사합니다”라는 한국어 데이터가 한국어 기본단위 토크나이저 를 통해 BTS유닛 단위 시퀀스 인 [ㄱ, ㅏ, ㅁ, ㅅ, ㅏ, , ㅇ, -, ㅏ, ㅁ, -, ㄴ, l, , ㄷ, ㅏ, , ., , ] 으로 변환된다. 이때, 특수기호 '' 전술한 바와 같이 빈 자모 공간을 대신하는 글자 를 의미한다. 특수기호 ''는 종성이 없는 경우에 사용되고, 마침표 '.'와 같이 중성이 없는 경우 두개가 붙 어 3개가 하나의 구성으로 토큰화 된다. BTS유닛 단위 시퀀스에 GRU 계층으로 사용된 BTS유닛 임베딩 합성 모듈 을 적용하여 자모 단위 시퀀 스인 [ㄱ, ㅏ, ㅁ, ㅅ, ㅏ, , ㅎ, ㅏ, ㅂ, ㄴ, l, , ㄷ, ㅏ, , . , ]를 구한다. 자모 단위 시퀀스를 도 7에서 설명한 바와 같이 초성과 중성을 병합한다. 이 과정은 초성 중성 합성 모듈 (522(c))을 통해 진행된다. 초성과 중성이 병합되어 초성 중성 병합 시퀀스 이 생성된다. 초성 중성 병합 시퀀스 는 [가, ㅁ, 사, , 하, ㅂ, 니, , 다, , ., ] 로 구성된다. 초성 중성 병합 시퀀스 2xN 행렬로 재배열하여 2개 층을 갖는 시퀀스를 형성한다. 이 경우, 일반적인 한 글 음절의 모양을 고려해 초성과 중성의 병합을 1행에, 종성 및 패드 토큰을 2행에 배치한다. 이후 전술한대로 재배열된 시퀀스를 2xN 크기 커널을 갖는 CNN 계층 (522(d))에 입력한다. 자모 단위 합성 모듈 (522(e))을 통과 시킴으로써 음절 단위 시퀀스인 [감, 사, 합, 니, 다, .]을 출력으로 얻는다. 마지막으로 음절 단위 시퀀스를 서브워드 경계 예측 모듈을 이용해 서브워드 단위 토큰으로 합성한다. 음절 단위 시퀀스 [감, 사, 합, 니, 다, .]를 서브워드 경계 예측 모듈에 입력하여 서브워 드 경계 여부를 나타내는 수열을 다음과 같이 얻는다. [0, 1, 1, 0, 1, 1]. 수열 값이 1이면 서브워드의 경계를 나타내는 것으로 본 예시에서는 '사', '합', '다', '.' 가 해당된다. 경계를 기준으로 하여 음절 단위 시퀀스 를 묶으면[(감, 사), (합), (니, 다), (.)]와 같이 묶이게 된다. 각 묶음 내 음절 단위 토큰들을 평균 합 성하여 최종적으로 입력 서브워드 시퀀스 [감사, 합, 니다, .]을 얻는다. 이 서브워드 시퀀스가 언어 모델의 입력으로 들어감으로써 학습이 진행된다. 도 9는 본 발명의 오탈자 노이즈 테스트의 결과를 나타낸다. 도 9를 참조하면, 노이즈 0%에서 가장 우수한 성능 을 보여주는 MorphemeSubword(SOTA)은 노이즈가 80% 인 경우 견고함의 손실이 -48.04%임에 비해, 본 발명의 모 델은 노이즈 80%에서 -19.50% 를 나타내며, 0%에서도 75.28의 우수한 수치를 나타낸다. 또한, 전술한 한글 기본단위 임베딩 합성 방법은 애플리케이션으로 구현되거나 다양한 컴퓨터 구성요소를 통하 여 수행될 수 있는 프로그램 명령어의 형태로 구현되어 컴퓨터 판독 가능한 기록 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능한 기록 매체는 프로그램 명령어, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포 함할 수 있다. 상기 컴퓨터 판독 가능한 기록 매체에 기록되는 프로그램 명령어는 본 발명을 위하여 특별히 설계되고 구성된 것들이거니와 컴퓨터 소프트웨어 분야의 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능한 기록 매체의 예에는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD_ROM, DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto- opticalmedia), 및 ROM, RAM, 플래시 메모리 등과 같은 프로그램 명령어를 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령어의 예에는, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사 용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드도 포함된다. 상기 하드웨어 장치는 본 발명에 따른 처 리를 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 이상에서는 실시예들을 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0112621", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 서브워드 기반의 언어모델을 사용하는 시스템의 구조이다. 도 2는 본 발명의 일 실시예인 서브워드 기반의 언어모델을 사용하는 시스템의 구조이다. 도 3은 본 발명의 일 실시예인 기본단위 임베딩 합성 모듈의 구조이다. 도 4는 본 발명에 따라 기본단위 시퀀스를 서브워드 시퀀스로 합성하는 과정의 예를 나타낸다. 도 5는 한글의 초성, 중성, 종성의 결합 구조를 나타낸다. 도 6은 종래 한글의 초성, 중성, 종성의 결합 구조의 분해를 나타낸다. 도 7은 본 발명의 한글의 초성과 중성의 병합 구조를 나타낸다.도 8은 본 발명에 따라 기본단위 시퀀스르 순차적인 병합을 통해 서브워드 시퀀스로 합성 과정의 예를 나타낸다. 도 9는 본 발명의 오탈자 노이즈 테스트의 결과를 나타낸다."}
