{"patent_id": "10-2023-7026041", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0133966", "출원번호": "10-2023-7026041", "발명의 명칭": "인공 신경망을 훈련시키기 위한 시스템", "출원인": "마이크로소프트 테크놀로지 라이센싱, 엘엘씨", "발명자": "골럽 맥시밀리안"}}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시스템으로서,하나 이상의 인공 지능(AI) 프로세서와,하나 이상의 제어 프로세서와,상기 하나 이상의 제어 프로세서에 의해 실행 가능한 프로그램 코드가 저장된 컴퓨터 판독 가능 저장 매체를 포함하되,상기 프로그램 코드는 상기 하나 이상의 제어 프로세서로 하여금:인공 신경망 모델을 수신하고, 복수의 훈련 파라미터에 기초하여, 상기 하나 이상의 AI 프로세서 상에서 훈련프로세스를 실행하도록 상기 모델을 구성하고, 상기 훈련 프로세스의 실행 시 생성된 복수의 통계를 모니터링하고, 상기 복수의 통계 중 적어도 하나를 사전결정된 범위 내에서 유지하기 위해 상기 복수의 통계 중 하나 이상의통계에 기초하여 상기 복수의 훈련 파라미터 중 하나 이상의 훈련 파라미터를 조정하게 하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 조정 단계는, 상기 모델의 타겟 레이어와 관련된 정밀도를 조정하여, 상기 타겟 레이어와 관련된 상기 복수의 통계 중 적어도 하나를 상기 사전결정된 범위 내에서 유지하는 것을 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 모델 크기가 증가함에 따라 상기 모델의 적어도 일부의 정밀도를 더 감소시키는 것을 더 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 모니터링 단계는 상기 모델의 타겟 레이어와 관련된 하나 이상의 통계를 모니터링하는 것을 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 하나 이상의 통계는 신경망 그레디언트 노이즈의 측정을 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,공개특허 10-2023-0133966-3-상기 조정 단계는 상기 모델의 타겟 레이어와 연관된 하나 이상의 훈련 파라미터를 조정하는 것을 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 타겟 레이어는 숨겨진 레이어이고, 상기 하나 이상의 훈련 파라미터는 상기 타겟 레이어에 대한 정밀도를구성하는 파라미터를 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 타겟 레이어는 숨겨진 레이어이고, 상기 하나 이상의 훈련 파라미터는 상기 타겟 레이어에 대한 희소성(sparsity)을 구성하는 파라미터를 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6항에 있어서,상기 타겟 레이어는 숨겨진 레이어이고, 상기 하나 이상의 훈련 파라미터는 상기 타겟 레이어와 연관된 다수의노드를 구성하는 파라미터를 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에 있어서,상기 타겟 레이어는 숨겨진 레이어이고, 상기 하나 이상의 훈련 파라미터는 상기 타겟 레이어 내의 하나 이상의노드에 대한 학습 속도를 구성하는 파라미터를 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제6항에 있어서,상기 타겟 레이어는 숨겨진 레이어이고, 상기 하나 이상의 훈련 파라미터는 상기 모델과 연관된 다수의 레이어를 구성하는 파라미터를 포함하는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서,상기 복수의 훈련 파라미터는 상기 모델과 연관된 정밀도를 구성하는 파라미터를 포함하고, 상기 모델은 상기모델의 제1 레이어와 연관된 제1 정밀도 및 상기 모델의 제2 레이어와 연관된 제2 정밀도로 구성되며, 상기 제1정밀도는 상기 제2 정밀도보다 높고, 상기 제1 레이어는 상기 모델의 입력에 더 가까운 레이어이고, 상기 제2레이어는 상기 모델의 출력에 더 가까운 레이어인,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서,상기 복수의 훈련 파라미터는 희소성을 구성하는 파라미터를 포함하고, 상기 모델은 상기 모델의 제1 레이어와공개특허 10-2023-0133966-4-연관된 제1 희소성 및 상기 모델의 제2 레이어와 연관된 제2 희소성으로 구성되고, 상기 제1 희소성은 상기 제2희소성과 상이하며, 상기 제1 레이어는 상기 모델의 입력에 더 가까운 레이어이고, 상기 제2 레이어는 상기 모델의 출력에 더 가까운 레이어인,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "시스템으로서,인공 신경망 모델에 대한 훈련 프로세스를 실행하도록 구성된 하나 이상의 인공 지능(Al) 프로세서- 상기 훈련프로세스는 연관된 훈련 파라미터 세트를 포함하고, 상기 훈련 프로세스의 실행은 복수의 통계를 생성함 -와,상기 하나 이상의 AI 프로세서에 결합된 하나 이상의 제어 프로세서를 포함하되, 상기 하나 이상의 제어 프로세서는, 상기 복수의 통계 중 하나 이상의 통계에 기초하여, 상기 복수의 통계 중 적어도 하나를 상기 훈련 프로세스의 실행 동안 사전결정된 범위 내에서 유지하도록 상기 훈련 파라미터 중 하나 이상을 조정하도록구성되는,시스템."}
{"patent_id": "10-2023-7026041", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "방법으로서,인공 신경망 모델을 수신하고, 복수의 훈련 파라미터에 기초하여 하나 이상의 AI 프로세서 상에서 훈련 프로세스를 실행하도록 상기 모델을 구성하는 단계와,상기 훈련 프로세스의 실행에 따라 생성된 복수의 통계를 모니터링하는 단계와,상기 복수의 통계 중 적어도 하나의 통계를 사전결정된 범위 내에서 유지하기 위해 상기 복수의 통계 중 하나이상의 통계에 기초하여 상기 복수의 훈련 파라미터 중 하나 이상의 훈련 파라미터를 조정하는 단계를포함하는,방법."}
{"patent_id": "10-2023-7026041", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 실시예는 복수의 훈련 파라미터에 기초하여 훈련 프로세스를 실행하도록 모델을 구성하고, 훈련 프로 세스의 실행 시 생성된 복수의 통계를 모니터링하며, 통계들 중 적어도 하나의 통계를 사전결정된 범위 내에서 유지하도록 하나 이상의 통계에 기초하여 하나 이상의 훈련 파라미터를 조정함으로써 인공 신경망을 최적화하는 시스템을 포함한다. 일부 실시예에서, 인공 지능(AI) 프로세서는 모델에 대한 훈련 프로세스를 실행할 수 있으 며, 훈련 프로세스는 연관된 훈련 파라미터 세트를 갖는다. 훈련 프로세스의 실행은 복수의 통계를 생성할 수 있다. 인공지능 프로세서(들)에 결합된 제어 프로세서(들)는 통계를 수신할 수 있고, 이에 따라, 훈련 프로세스 를 실행하는 동안 적어도 하나의 통계를 사전결정된 범위 내에서 유지하도록 훈련 파라미터 중 하나 이상을 조정 할 수 있다."}
{"patent_id": "10-2023-7026041", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "본 개시는 컴퓨팅 시스템에 관한 것이다. 보다 상세하게, 본 개시는 인공 신경망을 훈련시키기 위한 기술에 관 한 것이다. 인공 지능(AI) 시스템은 자연어 처리 및 컴퓨터 비전과 같은 다양한 분야에서 주요한 발전을 가능하게 했다. AI 시스템은 일반적으로 복수의 레이어로 구성된 AI 모델(예컨대, 신경망 모델)을 포함한다. 각 레이어에는 일 반적으로 다른 레이어의 노드에 연결된 노드(일명 뉴런)가 포함된다. 노드 간의 연결은 연결의 강도를 높이거 나 낮추기 위해 훈련 가능한 가중치와 연관된다. 작동 시, 데이터 세트가 모델의 입력 레이어에 적용되고 출력 레이어에서 출력이 생성된다. 출력은 입력 데이터 세트의 특정 특징에 대한 분류, 인식 또는 예측에 해당할 수 있다. 신경망을 훈련시키기 위해, 출력은 입력 데이터 세트에 대해 알려진 출력과 비교되고, 오류는 모델을 통 해 역전파되고 모델의 파라미터는 조정된다. 신경망 모델의 한 가지 문제점은, 일반적으로 모델이 클수록 처리하는 데 더 많은 컴퓨팅 리소스 및/또는 시간 이 필요하다는 것이다. 이는 주로 계산을 필요로 하는 그러한 모델과 관련된 파라미터의 수에 기인한다."}
{"patent_id": "10-2023-7026041", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하의 설명에서는, 본 개시의 완전한 이해를 제공하기 위해, 설명의 목적을 위해, 수많은 실시예 및 구체적인 세부 사항들이 제시된다. 이러한 실시예 및 상세한 설명은 청구범위의 요소 또는 청구범위 전체를 부당하게 제 한하는 것으로 해석되어서는 안 된다. 다른 청구범위의 언어에 기초하여, 당업자에게는, 청구된 주제가 이들 실시예에 기재된 특징의 일부 또는 전부를 단독으로 또는 조합하여 포함할 수 있고, 본 명세서에 기재된 특징및 기술의 수정 및 균등물을 더 포함할 수 있음이 명백할 것이다. 딥 러닝의 경우, 인공 지능(AI) 모델(예컨대, 신경망 모델)은 일반적으로 크기(예컨대, 레이어, 노드, 연결 등 의 수)의 증가에 따라 예측의 정확성이 증가한다. 이는 종종 훈련 중에, 검증 손실의 바람직한 감소(예컨대, 예측의 정확도 향상)로 측정된다. 그러나, 모델 크기의 증가는 일반적으로 모델을 처리하기 위한 컴퓨팅 리소스 및/또는 시간의 증가를 요구한다. 이는 모델 크기의 증가와 관련된 파라미터의 수가 증가하고, 이에 따라 추가 계산이 필요하기 때문이다. 예를 들어, 신경망(NN) 모델의 각 노드에 대해, 로 표현되는 포워드 패스 계산이 실행될 수 있는데, 여기서, y는 노드의 출력 값을 나타내고, x는 연결된 노드(0 내지 n)의 입력 값을 나 타내고, w는 노드로부터의 연결과 연관된 훈련 가능한 가중치(예를 들어, 파라미터)를 나타낸다. 훈련 중에, (예컨대, 마지막 레이어에서의) 모델의 출력은 입력 데이터 세트에 대해 알려진 출력에 비교될 수 있다. 그런 다음 유사한 백워드 패스 계산(예컨대, 역전파)을 실행하여 그레디언트 및 가중치 업데이트를 결정할 수 있다. 예를 들어, 확률적 경사 하강법(stochastic gradient descent: SGD)으로 알려진 프로세스에서, 역전파는 훈련 데이터 세트의 하위 집합에 대해 여러 번(예컨대, 반복적으로) 수행될 수 있다. 포워드 및 백워드 패스의 계산 은 일반적으로 모델의 각 레이어에 대해 여러 번 실행되는 행렬 곱셈(일명, Mat-Mui) 연산에 의해 수행된다. 그 결과, 모델 훈련에 필요한 계산의 수는 모델 크기가 증가함에 따라 빠르게 증가할 수 있다. 모델을 처리하기 위한 계산 리소스 및/또는 시간을 줄이기 위한 한 가지 기술은 모델과 관련된 계산의 정밀도 (일명, 비트 폭)를 감소시키는 것이다. 예를 들어, IEEE 754 단정밀도 32비트 부동 소수점 포맷(일명, FP32)과 같은 높은 정밀도의 계산을 필요로 하는 모델을 처리하기 위한 계산 리소스 및/또는 시간은, 8비트 정수(일명, INT8)와 같은 낮은 정밀도의 계산을 갖는 모델을 처리하기 위한 계산 리소스 및/또는 시간과 비교될 때 감소될 수 있다. 그러나, 정밀도가 특정 임계값 이하로 감소되면, 모델은 종종 예측의 정확도를 향상시키지 못한다. 또한, 모델 은 지속적인 훈련에 따라 예측의 정확도가 악화되는 경우도 있다(예컨대, 발산). 본 개시의 특징 및 장점은, 특정 통계를 사전결정된 범위 내에서 유지하기 위해, 모델과 연관된 하나 이상의 통 계에 기초하여, 모델과 연관된 하나 이상의 훈련 파라미터를 자동으로 조정함으로써, 신경망 모델의 훈련을 개 선하는 것을 포함한다. 일부 실시예에서, 모델과 연관된 통계에 기초하여 훈련 파라미터를 조정하는 것은 \"내성 적(introspective)\" 훈련이라고 지칭될 수 있다. 본원에 개시된 기법은, 예를 들어, 컴퓨팅 리소스의 확장을 줄이면서 모델 크기에 따라 확장되는 향상된 훈련 성능을 지원할 수 있다. 따라서, 본 개시의 일부 실시예는 신경망 모델에 대한 자동 스케일링 내성적 형태의 훈련을 제공한다. 유리하게, 이는 매우 큰 모델에 대한 컴퓨 팅 리소스 및/또는 처리 시간의 감소(예컨대, 컴퓨팅 사이클의 감소)를 제공하면서 수렴에 대한 보다 낮은 검증 손실(예컨대, 예측의 정확도 향상)을 가능하게 할 수 있다. 아래에 설명된 특정 실시예는 (예를 들어, 통계에 기초하여) 파라미터를 조정하기 위한 자동화된 기술을 더 포함할 수 있다. 도 1은 실시예에 따른 인공 신경망을 최적화하기 위한 시스템을 도시한다. 이 예에서, 하나 이상의 제어 프로세서(들)는 하나 이상의 AI 프로세서(들)와 통신할 수 있다. 제어 프로세서(들)는 예를 들 어, 전통적인 CPU, FPGA, 시스템 온 칩(SoC), 애플리케이션 특정 집적 회로(ASIC), 또는 임베디드 ARM 제어기, 또는 소프트웨어를 실행하고 소프트웨어의 명령어에 기초하여 AI 프로세서(들)와 통신할 수 있는 다른 프 로세서를 포함할 수 있다. AI 프로세서(들)는 그래픽 프로세서(GPU), AI 가속기, 또는 AI 연산(예컨대, x86 프로세서와 같은 폰 노이만 아키텍처 프로세서 대비 행렬 곱셈)에 최적화된 다른 디지털 프로세서를 포함할 수 있다. 예시적인 AI 프로세서(들)는 예를 들어, GPU(예컨대, 800 코어 및 64 멀티어큐뮬레이터를 갖는 엔비디 아 볼타®) 또는 텐서 프로세서 유닛(TPU)(예컨대, 병렬로 16k 연산을 갖는 4 코어)을 포함할 수 있다. 이 예에서, 제어 프로세서는 제어 프로세서에 의해 실행 가능한 프로그램 코드를 저장하는 메모리 (예컨대, 비일시적 컴퓨터 판독 가능 저장 매체)에 결합될 수 있다. 제어 프로세서는 신경망 모델 (이하, \"모델\") 및 모델을 구성하기 위한 복수의 훈련 파라미터를 수신(예컨대, 로드)한다. 모 델은 예를 들어, 레이어의 노드가 다른 레이어의 노드에 연결되고 노드 간의 연결이 훈련 가능한 가중치와 연관되는 신경망의 복수의 레이어를 정의하는 그래프로 구성될 수 있다. 훈련 파라미터(튜닝 파라미터 또 는 모델 파라미터라고도 함)는 모델의 구성 및/또는 실행에 영향을 미치도록 조정될 수 있는 하나 이상의 값으로 구성될 수 있다. 다양한 실시예에서 사용될 수 있는 훈련 파라미터는 모델 크기, 배치 크기(batch size), 학습 속도, 정밀도(예를 들어, 데이터 값의 이진 표현의 비트 수) 및/또는 희소성(sparsity)(예를 들어,데이터 값에서의 0의 개수), 정규화(예를 들어, 가중치 감쇠 또는 L2 정규화), 엔트로피 및/또는 훈련 단계, 및 본 개시에 비추어 당업자에게 명백할 수 있는 특성화 및 조정될 수 있는 다른 파라미터를 포함할 수 있다. 일부 실시예에서, 훈련 파라미터는 당업자에게 알려진 바와 같이 하나 이상의 하이퍼파라미터(예컨대, 신경망의 학습 을 제어하는 데 사용되는 파라미터)를 포함할 수 있다. 제어 프로세서는 또한 신경망 컴파일러를 실행할 수도 있다. 신경망 컴파일러는 실행될 때, 모 델 및 훈련 파라미터를 수신하고, 하드웨어에서 모델을 구현 및 실행하기 위해 하나 이상의 AI 프로세서 상에서 리소스들을 구성할 수 있는 프로그램을 포함할 수 있다. 예를 들어, 신경망 컴파일러 는 모델을 수신하고, 훈련 파라미터(들)에 기초하여 모델을 구성하여, AI 프로세서에 서 실행되는 훈련 프로세스를 실행할 수 있다. 신경망 컴파일러는 훈련 프로세스를 수행하기 위해 입력 활성화, 가중치, 역전파 등의 계산을 구현하도록 AI 프로세서(들)에 구성될 수 있다. 이어서, AI 프로세서 (들)는 신경망 컴파일러에 의해 결정된 바와 같이, 모델에 대한 훈련 데이터를 수신 및 처 리하기 위해 리소스(예컨대, 훈련 프로세스)를 사용할 수 있다. 예를 들어, 리소스는 모델을 구 현하기 위한 연산을 수행하는 데 사용되는 레지스터, 곱셈기, 가산기, 버퍼 및 기타 디지털 블록을 포함할 수 있다. 따라서, AI 프로세서(들)는 포워드 패스에서 수많은 행렬 곱셈 계산을 수행하고, 출력을 훈련 데이 터의 서브세트에 대해 알려진 출력에 비교하고, 예를 들어 그레이디언트 또는 가중치 업데이트를 결정하기 위해 백워드 패스에서 추가적인 행렬 곱셈 계산을 수행할 수 있다. 이 프로세스는 훈련 데이터가 처리될 때 여 러 반복을 통해 계속될 수 있다. 일부 실시예에서, AI 프로세서(들)는 예를 들어, 신경망 컴파일러 에 의해 구성될 수 있는 확률적 경사 하강법(SGD), 적응 모멘트 추정법(ADAM) 등과 같은 역전파 알고리즘에 따 라 가중치 업데이트를 결정할 수 있다. 모델의 실행 동안, AI 프로세서는 각 반복에서, 예를 들어, 모델의 각 레이어에서, 활성화, 가중치, 그레디언트, 및 가중치 업데이트에 대한 복수의 값을 생성할 수 있다. 본 개시의 특징 및 장점은 신경망 내의 특정 위치에서 그러한 값의 통계를 모니터링하고, 그러한 통계에 기초하여 훈련 파라미터를 (예를 들어, 레이어 단위로) 조정함으로써 훈련을 개선하는 것을 포함한다. 모니터링되는 통계는, 예를 들어, 아래에서 더 상세히 설명되는 바와 같이, 역전파 그레디언트, 그레디언트 노이즈, 노드 활성화, 가중치, 가중치 업데이트 등 의 측정값 중 하나 이상을 포함할 수 있다. 통계 측정값에는 평균 측정값, 표준 편차 측정값, 0 값의 백분율 측 정값 등이 포함될 수 있다. 일부 실시예에서, 제어 프로세서는 매핑 시스템을 실행할 수 있다. 예를 들어, 매핑 시스템은 통계를 처리하여 모델의 일부와 연관된 하나 이상의 훈련 파라미터를 자동으로 조정하여 하나 이상의 통계 가 사전결정된 범위(예컨대, 모델 내의 특정 위치) 내에서 유지될 수 있도록 할 수 있다. 조정된 훈련 파라미터는, AI 프로세서(들)에 의한 후속 실행을 위해 AI 프로세서(들)에서 모델 의 구현을 업데이트하도록 신경망 컴파일러에 제공될 수 있다. 이 프로세스는 모델이 사전결정된 수 의 훈련 단계를 통해 수렴할 때까지 훈련 데이터의 서브세트에 대해 반복적으로 반복될 수 있다. 궁극적 으로, 예를 들어, 훈련된 모델은 주어진 애플리케이션에 배치되도록 생성될 수 있다. 바람직하게는, 통계 가 사전결정된 범위(들) 내에서 유지되도록 훈련 파라미터를 조정함으로써, 결과적으로 훈련된 모델은 예 를 들어, 처리를 위한 컴퓨팅 리소스 및/또는 시간의 감소(예를 들어, 컴퓨팅 사이클의 감소)를 통해 예측의 정 확성을 향상시키도록 최적화될 수 있다. 일부 실시예에서, 상이한 훈련 파라미터가 동일한 모델을 훈련시키는 데 사용되는 훈련 데이터의 상이한 배치 (batches)에 대해 사용될 수 있다. 이것은 하이브리드 모드라고 지칭될 수 있다. 예를 들어, 상이한 프로세서 (또는 동일한 훈련 배치에서 작동하는 프로세서 그룹)에서 실행되는 상이한 배치에 대해 훈련되는 모델의 상이 한 부분에 상이한 정밀도가 사용될 수 있다. 하이브리드 모드는 예를 들어 각 노드가 자체 정밀도로 실행되는 다수의 상이한 훈련 노드에 걸쳐 훈련을 분산할 수 있는 기능의 확장일 수 있다. 예를 들어, 정밀도가 감소하 면 그레디언트 노이즈가 증가하여 훈련 발산 및 최적의 성능에 미치지 못하는 결과를 초래할 수 있다. 일부 실 시예에서는, 분산된 훈련 실행에 고정밀 가이드 노드를 몇 개 추가하는 것은 전체 그레디언트 노이즈를 낮추고 모델의 손실을 개선하는 효과적인 방법이다. 데이터 병렬 설정에서, 이것은 예를 들어, 더 높은 정밀도로 N개의 총 배치 중 1 내지 M개를 샘플링하는 것으로 볼 수 있으며, N << M이다. 전술한 바와 같이, 일부 실시예들에서, 모델은 다수의 AI 프로세서에서 실행되도록 분할될 수 있다. 예를 들어, 모델의 제1 부분은 제1 AI 프로세서 상에서 실행될 수 있고, 모델의 제2 부분은 제2 AI 프로세서 상에서 실행될 수 있다. 이 예에서, 복수의 값의 측정은 하나 이상의 제어 프로세서에 의한 분석을 위해 복수의 AI 프로세서로부터 수신될 수 있다. 다양한 실시예에서, 이것은 모델을 최적화하면서 더 빠른 전개를 위한 훈련 프로세스의 효율적인 분배를 허용할 수 있다. 다양한 실시예에서, 값들에 기초한 통계는, 예를 들어, AI 프로세서(들) 또는 제어 프로세서(들)에 의해 생성될 수 있다. 도 2는 실시예에 따른 인공 신경망의 훈련 방법을 예시한다. 이 예에서, 202에서, 하나 이상의 제어 프로세서 (들)는 주어진 애플리케이션에 대한 신경망 모델을 수신할 수 있다. 제어 프로세서(들)는 제어 프로세서 (들)와 유사할 수 있다. 204에서, 제어 프로세서(들)는 훈련 파라미터(들)에 기초하여, 하나 이상의 AI 프 로세서(들)에서 훈련 프로세스(들)를 실행하도록 모델을 구성할 수 있다. AI 프로세서(들)는 AI 프로세서 (들)와 유사할 수 있다. 206에서, 제어 프로세서(들)는 AI 프로세서(들)에 의해 훈련 프로세스(들)가 실행 될 때 생성된 복수의 통계를 모니터링할 수 있다. 208에서, 제어 프로세서(들)는 하나 이상의 통계가 사전결정 된 범위 내에 있는지를 판단할 수 있다. 하나 이상의 통계(들)가 사전결정된 범위 내에 있는 경우(예), 제어 프 로세서(들)는 206에서 복수의 통계를 하나의 루프 내에서 계속 모니터링할 수 있는데, 이러한 모니터링은 모델 이 사전결정된 수의 훈련 단계를 통해 수렴할 때까지 계속된다. 그러나, 하나 이상의 통계가 사전결정된 범위 내에 있지 않은 경우(아니오), 제어 프로세서는 통계에 기초하여, 하나 이상의 통계를 사전결정된 범위 내 에 유지하도록 훈련 파라미터(들)를 조정할 수 있다. 그런 다음, 204로 복귀하여, 제어 프로세서(들)는 조정된 훈련 파라미터(들)에 기초하여, AI 프로세서(들)에서 훈련 프로세스를 실행하도록 모델을 구성할 수 있다. 도 3a는 실시예에 따른 인공 신경망의 훈련 파라미터 및 모니터링된 값을 도시한다. 이 예에서, 모델은 훈련 프로세스의 실행을 위해 하나 이상의 AI 프로세서에 로딩된다. AI 프로세서는 모델 크기, 배치 크기, 학습 속도, 정밀도 및 희소성과 같은 훈련 파라미터로 추가로 구성된다. 아래 에서 더 상세히 설명하는 바와 같이, 상이한 훈련 파라미터의 상이한 구성은 훈련 중에 신경망 모델의 동작을 변경할 수 있다. 예를 들어, 훈련 프로세스는 모델 크기, 배치 크기, 학습 속도, 정밀도 및 희소성(들)을 갖는 모델을 실행하도록 구성될 수 있다. 아래에 더 자세히 설명된 바와 같이, 훈련 파라미터의 변경은 망이 훈 련 중인 내내 상이한 값을 발생시킨다. 그레디언트 노이즈, 활성화, 그레디언트, 가중치 및 가중치 업데이트의 값은 망의 특정 위치 및 훈련 과정 중 특정 시간에 모니터링될 수 있고, 훈련 파라 미터를 수정하는 데 사용될 수 있다. 본 개시의 특징 및 장점은 모델 내의 특정 위치(예컨대, 하나 이상의 특 정 레이어)에서 특정 훈련 파라미터(예컨대, 학습 속도, 정밀도, 희소성)가 훈련 성능을 개선하는 특정 값을 갖 도록 구성할 수 있다. 유사하게, 훈련 파라미터는, 예를 들어, 훈련 프로세스를 최적화하고 컴퓨팅을 감소시키 고 및/또는 정확도를 향상시키기 위해 시간에 따라 변경될 수 있다. 도 3b는 실시예에 따른 인공 신경망을 훈련시키기 위한 매핑 시스템을 예시한다. 이 예에서, 매핑 시스템 은 통계(들) 모니터, 매핑 규칙 및 훈련 파라미터(들) 조정 출력을 포함할 수 있다. 매핑 시스 템은, 예를 들어, 매핑 시스템의 일 실시예일 수 있다. 일부 실시예에서, 통계(들) 모니터는 하나 이상의 AI 프로세서 상에서 실행되는 주어진 모델과 연관된 값 을 수신하고, 통계(들) 처리(예를 들어, 평균, 표준 편차, 또는 0의 백분율을 결정)를 수행하고, 통계(들)를 매 핑 규칙으로 전송할 수 있다. 모델 실행 중에 생성된 값은 AI 프로세서(들)에 의한 훈련 프로세스의 실행 동안 다양한 시점에서 모델의 다양한 부분을 실행하는 AI 프로세서로부터 수신될 수 있다. 이 예에서, 통계(들)는 예를 들어, 훈련 중인 모델의 특정 레이어에서 취해진, 그레디언트 노이즈, 활성화, 역전 파 그레디언트, 가중치 및 가중치 업데이트의 측정값을 포함할 수 있다. 전술한 바와 같이, 통 계 측정은 예를 들어 평균, 표준 편차 및/또는 0 값(또는 이에 상응하는, 0이 아닌 값)의 백분율 중 하나 이상 을 포함할 수 있다. 매핑 규칙은 하나 이상의 모니터링된 통계와 하나 이상의 훈련 파라미터 사이의 관계 를 지정한다. 매핑 규칙은 예를 들어, 더 작은 모델 크기를 사용하여 모델의 훈련을 특성화함으로써 생성될 수 있으며, 통계와 훈련 파라미터 간의 관계를 설정하고, 예를 들어 훈련을 최적화하기 위해 통계에 기초하여 훈련 파라미터를 제어하는 매핑 규칙을 정의할 수 있다. 따라서, 이러한 통계는 예를 들어, 훈련 정확도를 개선하고 /하거나 훈련 프로세스의 계산 효율을 증가시키기 위해, 모델 크기, 배치 크기, (예를 들어, 특정 위 치 또는 특정 시간에서의) 학습 속도, (예를 들어, 특정 노드 또는 레이어에서의) 정밀도 및 희소성 을 동적으로 조정하는 데 사용될 수 있다. 본 개시의 실시예는 신경망 모델 내의 특정 위치에서 그레디언트 노이즈를 모니터링하고, 사전결정된 범위 내에서 그레디언트 노이즈를 제어하기 위해 훈련 파라미터를 조정할 수 있다. 당업자에게 알려진 바와 같이, 그 레디언트 노이즈는 역전파 그레디언트의 노이즈를 의미한다. 예를 들어, 그레디언트 노이즈는 그레디언트의 신 호 대 노이즈 비율로 표현될 수 있다. 경우에 따라, 그레디언트 노이즈는 양자화 효과(예컨대, 이진 값으로 그 레디언트 표현)와 관련될 수 있다. 따라서, 그레디언트 노이즈는 신호가 이상적인 그레디언트 값인 양자화 노이즈 대 신호 비율(QNSR)의 함수일 수 있다. 망의 특정 노드 또는 레이어에 \"건강한\" 분량의 그레디언트 노이 즈가 존재할 때 훈련이 개선될 수 있다. 따라서, 본 개시의 실시예는 망의 특정 위치 및/또는 훈련 중 시간에 서 그레디언트 노이즈를 모니터링하고 특정 훈련 파라미터를 조정하여 그레디언트 노이즈를 사전결정된 범위 내 에서 유지할 수 있다. 예를 들어, 그레디언트 노이즈는 배치 크기의 함수이며, 배치 크기가 작을수록 그레디언트 노이즈가 많아지고 배치 크기가 클수록 그레디언트 노이즈가 줄어든다(예컨대, 배치 그레디언트는 이상적인 그레디언트에 가까워짐). 또 다른 예로, 정밀도(예컨대, 비트 수)가 증가하면 양자화 노이즈(예컨대, QNSR)가 감소하고 정밀 도가 감소하면 양자화 노이즈가 증가한다. 양자화 노이즈가 증가하면 그레디언트 노이즈도 증가한다. 따라서, 망의 그레디언트 노이즈에 영향을 미치는 훈련 파라미터는, 예를 들어, 모델의 특정 부분(예컨대, 노드 또는 레 이어)에서 모니터링된 그레디언트 노이즈에 기초하여 훈련 중에 조정되어, 그러한 위치 및/또는 시간에서의 그 레디언트 노이즈를 최적의 값 범위 내에서 유지할 수 있다. 유사하게, 그레디언트 노이즈, 활성화, 그레디언트, 가중치 및 가중치 업데이트에 대 한 통계는 단독으로 또는 다양한 조합으로 매핑되어 모델 크기, 배치 크기, 학습 속도, 정밀도 또는 희소성을 제어할 수 있다. 바람직하게는, 학습 속도, 정밀도 및/또는 희소성 은 망 모델의 특정 부분을 대상으로 할 수 있으므로, 예를 들어, 훈련 중인 모델의 상이한 부분이 훈련 최적화 를 위해 다르게 구성될 수 있다. 따라서, 일부 예시적인 실시예에서, 모니터링된 값은 제어된 통계(C1...CN)에 예시된 바와 같이 효율적인 훈련을 촉진하기 위해 최적의 범위(예를 들어, a > x, a < x < b, x < a)에서 유지 될 수 있다. 도 4는 일 실시예에 따른 인공 신경망의 훈련 방법을 예시한다. 이 실시예에서, 더 작은 신경망 모델(예를 들어, 더 적은 수의 노드, 레이어 또는 가중치)은 더 큰 신경망 모델을 훈련시키기 위한 훈련 파라미터와 모니 터링된 통계 사이의 관계를 효율적으로 특성화하도록 훈련될 수 있다. 402에서, 하나 이상의 제어 프로세서(들)는 주어진 애플리케이션에 대한 제1 모델 크기(예를 들어, 더 작은 크 기)를 갖는 제1 신경망 모델을 수신할 수 있다. 404에서, 제어 프로세서(들)는 훈련 파라미터(들)에 기초하여, 하나 이상의 AI 프로세서(들)에서 제1 훈련 프로세스를 실행하도록 제1 모델을 구성할 수 있다. 406에서, 제어 프로세서(들)는 AI 프로세서(들)에 의해 제1 훈련 프로세스(들)가 실행될 때 생성된 제1 모델의 다양한 위치에 서 복수의 통계를 모니터링할 수 있다. 일부 실시예에서, 제어 프로세서(들)는 훈련 파라미터 값(또는 설정)의 변경에 기초하여 통계가 어떻게 변화하는지를 모니터링할 수 있다. 408에서, 제어 프로세서(들)는 통계와 훈련 파라미터들 사이의 관계(예를 들어, 훈련 파라미터들의 변경에 기초하여 통계가 변화하는 방식)를 특징짓는 하 나 이상의 매핑을 생성할 수 있다. 다양한 실시예에서, 매핑은 통계와 훈련 파라미터들 사이의 관계를 상이한 방식으로 특성화할 수 있다. 예를 들 어, 매핑은 모델 크기, 배치 크기, 학습 속도, 정밀도, 희소성, 가중치 감쇠, 훈련 단계 등의 조정(예컨대, 훈 련 파라미터)과 역전파 그레디언트, 그레디언트 노이즈, 노드 활성화, 가중치, 가중치 업데이트 등의 측정(예컨 대, 통계) 사이의 관계를 특성화할 수 있다. 특정 실시예에서, 매핑은 모델 크와 그레디언트 노이즈 사이, 배 치 크기와 그레디언트 노이즈 사이, 배치 크기와 역전파 그레디언트 사이, 학습 속도와 역전파 그레디언트 사이, 학습 속도와 그레디언트 노이즈 사이, 정밀도와 역전파 그레디언트 사이, 정밀도와 그레디언트 노이즈 사 이, 가중치 감쇠와 활성화 사이, 가중치 감쇠와 그레디언트 노이즈 사이, 학습 속도와 활성화 사이, 희소성과 그레디언트 노이즈 사이 등의 관계를 특성화할 수 있다. 일부 실시예에서, 특성화는 더 큰 모델에 효율적으로 적용하기 위해 더 작은 모델(예를 들어, 제1 모델)의 훈련 중에 경험적으로 결정될 수 있다. 일부 실시예에서, 매핑은 통계의 경험적 측정에 기초할 수 있다. 통계와 훈련 파라미터 사이의 특성화 및 매핑의 예가 아래에 제 공된다. 도 4의 예로 돌아가서, 410에서, 제어 프로세서(들)는 제2 모델 크기(예컨대, 더 큰 크기)를 갖는 제2 신경망 모델을 수신할 수 있다. 412에서, 제어 프로세서(들)는 매핑에 따라 조정된 훈련 파라미터를 사용하여 AI 프로 세서(들)에서 제2 훈련 프로세스를 실행하도록 제2 모델을 구성하기 위해 408에서 생성된 매핑을 로드할 수 있 다. 유리하게, AI 프로세서(들)는 제1 모델에 대한 제1 훈련 프로세스(들)로부터의 특성화에 기초하여 제2 모 델에 대한 제2 훈련 프로세스(들)를 실행하도록 구성될 수 있다. 제2 모델은 AI 프로세서(들)에 의해 훈련될 준비가 될 수 있다. 따라서, 414에서, 제어 프로세서(들)는 AI 프 로세서(들)에 의해 제2 훈련 프로세스가 실행될 때 생성된 제2 모델 내의 특정 위치에서 복수의 통계를 모니터 링할 수 있다. 416에서, 제어 프로세서(들)는 하나 이상의 통계(들)가 사전결정된 범위 내에 있는지 여부를 결정할 수 있다. 하나 이상의 통계가 사전결정된 범위 내에 있는 경우(예), 제어 프로세서(들)는 훈련 과정 동안 414에서 복수의 통계를 계속 모니터링할 수 있다. 그러나, 하나 이상의 통계가 사전결정된 범위 내에 있지 않 은 경우(아니요), 제어 프로세서(들)는, 418에서 하나 이상의 통계(들)가 사전결정된 범위 내에 유지되도록 통 계(들)에 기초하여 훈련 파라미터를 조정할 수 있다. 그런 다음, 420에서, 제어 프로세서(들)는 조정된 훈련 파라미터(들)에 기초하여, 제2 훈련 프로세스를 실행하고 414로 복귀하도록 모델을 구성할 수 있다. 다양한 실 시예에서, 제1 모델은 제2 모델보다 상당히 작을 수 있으며(예를 들어, 수 배 정도), 따라서 특성화는 더 작은 모델에서 계산적으로 더 빠르고 더 실현가능하며, 더 큰 모델에서는 계산적으로 비실용적일 수 있다. 유리하게, 제2 모델에 대한 후속 적용을 위해 제1 모델에 대해 모니터링된 통계와 훈련 파라미터 사이의 관계를 특성화함으로써, 제2 모델을 훈련시키기 위한 컴퓨팅 리소스 및/또는 처리 시간(예를 들어, 컴퓨팅 사이클의 감 소)의 감소가 달성될 수 있다. 일 예에서, 도 5a는 일 실시예에 따라, 제1 모델(510a)을 훈련시켜 매핑을 생성하도록 구성된 시스템을 예시하 고, 도 5b는 제2 모델(510b)을 훈련시키기 위해 매핑을 로딩하도록 구성된 시스템을 예시한다. 이 예에서, 도 5a에 도시된 바와 같이, 제어 프로세서(들)는 주어진 애플리케이션에 대한 제1 모델 크기(예를 들어, 더 작은 크기)를 갖는 제1 모델(510a)을 수신할 수 있다. 제어 프로세서(들)는 하나 이상의 AI 프로세서 (들)에서 제1 훈련 데이터(516a)를 실행하도록, 훈련 파라미터(들)에 기초하여, 제1 모델(510a)을 구 성할 수 있다. 제어 프로세서(들)는 제1 훈련 데이터(516a)를 사용하여 훈련 프로세스를 실행할 때 생성된 제1 모델(510a)의 각 레이어에서 통계를 모니터링할 수 있다. 이 예에서, 제어 프로세서(들)는 통계와 훈 련 파라미터 사이의 관계를 특징짓는 하나 이상의 매핑을 생성하기 위해 매핑 생성기(532a)를 구현할 수 있다. 일부 실시예에서, 매핑 시스템은 주어진 예산에 대해 모델 성능을 최대화할 수 있다. 예를 들어, 매핑 생성기 (532A)는 모델 정밀도, 크기, 그레디언트 노이즈 등과 같은 상이한 망 하이퍼파라미터의 동작이 단일 또는 일련 의 소규모 망의 훈련 동안 훈련 통계에 어떻게 영향을 미치는지를 관찰하는 맞춤 모델을 피팅함으로써 매핑 시 스템(532B)을 생성하는 데 사용될 수 있다. 이전의 소규모 실행에서 생성된 매핑은 훈련 통계 및 모델 성능 데 이터와 결합될 수 있다. 이 정보는 모델 크기, 모델 정밀도 등 일련의 입력 하이퍼파라미터가 주어졌을 때 모 델 성능을 예측하는 모델(예컨대, 신경망 모델)을 훈련하는 데 사용된다. 이 매핑 시스템을 사용하면, 더 큰 모델에 대해 최적의 하이퍼파라미터를 생성할 수 있다. 매핑 시스템은 모델 크기와 컴퓨팅 예산을 기반으로 모 델의 성능을 예측하고 사용자에게 해당 예측에 대한 신뢰도를 표시할 수 있다. 매핑 시스템은 이전 실행에서 자동으로 훈련되므로, 추가 사용자 입력이 필요하지 않을 수 있으므로 딥 러닝 모델의 다양한 하이퍼파라미터를 수동으로 조정할 필요가 없어진다. 따라서, 특정 실시예에서, 예를 들어, 각 구성(예컨대, 다른 정밀도 또는 희소성)에 대한 제로 샷 설정에서 모델 동작을 (예컨대, 정확도 측면에서) 예측할 수 있는 자동화된 확률 도구 로서 멱법칙 및 일반적인 모델 통계를 사용할 수 있다. 도 5b에 도시된 바와 같이, 제어 프로세서(들)는 제2 모델 크기(예를 들어, 더 큰 모델 크기)를 갖는 제2 모델(510b)을 수신할 수 있다. 제어 프로세서(들)는 매핑 생성기(532a)에 의해 생성된 매핑을 로딩하도록 매핑 시스템(532b)을 구현할 수 있다. 그런 다음, 매핑 시스템(532b)은 훈련 파라미터 및 훈련 데이터 (516b)를 사용하여 AI 프로세서(들)에서 제2 훈련 프로세스를 실행하도록 제2 모델(510b)을 구성할 수 있 다. 유리하게, 더 작은 모델을 사용하여 매핑을 결정하는 데 사용된 동일한 파라미터가 이제 더 큰 모델에서 모 니터링된 통계에 기초하여 조정되어 훈련의 성능을 향상시킬 수 있다. 구체적으로, 제어 프로세서(들)는 제2 훈련 프로세스의 실행에 따라 생성된 제2 모델(510b)에서 다양한 위치에서 통계(예컨대, 더 작은 모델에서 와 동일한 레이어에서의 동일한 통계)를 모니터링할 수 있다. 하나 이상의 통계가 사전결정된 범위(들) 내에 있지 않은 경우, 제어 프로세서(들)는 하나 이상의 통계를 사전결정된 범위(들) 내에 유지하도록 모니터링 된 통계(들) 측정값에 기초하여 제2 훈련 파라미터 중 하나 이상을 조정할 수 있다. 제2(더 큰) 모델 (510b)에 대한 후속 적용을 위해 제1(더 작은) 모델(510a)에 대한 모니터링 통계와 훈련 파라미터 사이의 관계 를 특성화함으로써, 제2 모델(510b)을 훈련시키는 데 있어서 계산 리소스 및/또는 처리 시간의 감소(예를 들어, 계산 사이클의 감소)가 달성될 수 있다. 도 5a의 매핑 생성기 단계 및 도 5b의 통계 모니터링 및 훈련 파라미터 조정 단계는 다양한 실시예에서 동일하 거나 다른 제어 프로세서(들)에서 수행될 수 있다는 것이 이해되어야 한다. 도 6은 실시예에 따라 통계를 생성할 수 있는 예시적인 신경망을 도시한다. 이 예에서, 신경망은 모 델과 같은 신경망 모델을 나타낼 수 있으며, 이 신경망 모델은 하나 이상의 AI 프로세서에서 훈련 프로세 스를 실행하기 위한 훈련 파라미터로 구성될 수 있다. 신경망은, 예를 들어, 레이어 내의 노드가 다른 레 잉 내의 노드에 연결되고 노드들 간의 연결이 훈련 가능한 가중치(예를 들어, 파라미터)와 연관되는 신경망의\"n\" 레이어(LI - Ln으로 표시되며, 여기서 n은 정수)을 정의하는 그래프를 포함할 수 있다. 제1 레이어(L1)는 훈련 데이터가 적용될 수 있는 입력 레이어일 수 있다. 마지막 레이어(Ln)는 출력이 생성될 수 있는 출력 레이 어일 수 있다. L1과 Ln 사이의 레이어(예컨대 L2, L3 등)는 제1 레이어(L1)와 마지막 레이어(Ln) 사이에 숨겨 진 레이어일 수 있다. 제2 레이어(L2)와 같이 제1 레이어(L1)에 가까운 레이어(예컨대, 신경망의 중간 지점을 기준으로 입력 쪽이거나, 입력에 더 가까운 레이어)는 하위 레이어 또는 이전 레이어로 지칭될 수 있다. 마지 막 레이어(미도시) 전에 있는 레이어(n-1)와 같이 출력 레이어(Ln)에 더 가까운 레이어(예를 들어, 출력에 가깝 거나 신경망의 중간 지점을 기준으로 출력에 더 가까운 레이어)는, 상위 레이어 또는 이후 레이어로 지칭될 수 있다. 동작의 일 예에서, 모델의 각 노드에 대해, AI 프로세서(들)는 y = f (x0w0 + x1w1 + .... + xnwn)로 표현되는 (포워드 패스라고 라벨링된) 포워드 패스 계산을 실행할 수 있는데, 여기서, y는 노드의 출력 값을 나타내고, x 는 연결된 노드(0 내지 n)의 입력 값을 나타내고, w는 노드로부터의 연결과 연관된 훈련 가능한 가중치(예컨대, 파라미터)를 나타낸다. 훈련 중에 (예컨대, 출력 레이어에서의) 모델의 출력은 해당 입력 데이터 세트에 대해 알려진 출력과 비교될 수 있다. 그런 다음, AI 프로세서(들)는 그레디언트 및 가중치 업데이트를 결정하기 위 해 백워드 패스 계산(예컨대, 역전파)(백워드 패스라고 라벨링됨)을 수행할 수 있다. 전술한 계산은 예를 들어 행렬 곱셈(일명, Mat-Mui) 연산을 사용하여 수행될 수 있다. 활성화 및 가중치는 특정 정밀도(예컨대, 특정 비 트 수 및 특정 이진 표현)를 갖도록 구성될 수 있는 곱셈기 및 가산기(예컨대, 곱셈-누산기)를 사용하여 처리될 수 있다. 모델을 실행하는 동안, AI 프로세서는 각 반복마다 값을 생성할 수 있으며, 이 값은 신경망의 특정 지점에서 측 정될 수 있으며, 이 값으로부터 통계가 생성될 수 있다. 위에서 언급한 바와 같이, 측정된 값은 활성화 값, 가 중치 값, 그레디언트 값, 그레디언트 노이즈 값, 가중치 업데이트 값을 포함할 수 있다. 이 예에서, 측정된 값 의 통계(위에서 설명함)는 모델의 특정 레이어에 대해(예를 들어, 레이어별로) 결정될 수 있으며, 여기에는 포 워드 패스 백워드 패스 중 하나 또는 둘 모두에서 발생하는 숨겨진 레이어에서의 값이 포함될 수 있다. 예를 들어, 포워드 및 백워드 패스 동안, 마지막 레이어(예컨대, 출력 레이어 또는 Ln)는 Ln 통계를 생성할 수 있고, 계속해서 제1 레이어(예컨대, 입력 레이어 또는 L1)까지 이어지며, 이 제1 레이어는 L1 통계를 생성할 수 있다. 전술한 바와 같이, 일 실시예에서, 제어 프로세서는 모델의 각 레이어에 대해 측정된 값을 수신하고 통계를 생 성할 수 있다. 유리하게, 개별 레이어에서의 통계를 모니터링하는 것은 하나 이상의 훈련 파라미터를 조정함으 로써 특정 타겟 레이어에서의 통계를 제어하는 능력을 제공할 수 있다. 본 실시예에서는 피드포워드 신경망이 도시되어 있지만, 본원에 설명된 기법들은 다른 신경망 토폴로지의 훈련 을 개선하기 위해 적용될 수 있다는 것을 이해해야 한다. 도 7은 실시예에 따른 모델에 대한 예시적인 모니터링된 통계를 나타내는 그래프를 도시한 도면이다. 본 실시예는 신경망 모델의 특정 부분에서 특정 통계의 범위를 제어하는 것이 어떻게 훈련 성능을 향상시킬 수 있 는지를 예시한다. 이 예에서는 자연어 처리 애플리케이션(\"NLP\")에서 예시적인 12-레이어 변환기 모델(L0- L11)의 각 레이어에서 평균 그레디언트 노이즈가 모니터링된다. 여기서, 훈련 파라미터를 조정하여 이후 레이 어(예컨대, 출력에 가까운 레이어)가 이전 레이어(예컨대, 입력에 가까운 레이어)보다 더 높은 그레디언트 노이 즈를 갖도록 함으로써 훈련이 개선된다. 도 7에 도시된 바와 같이, 702에서 모델의 마지막 레이어(L11)는 가장 높은 그레디언트 노이즈를 가질 수 있고, 704에서 모델의 제1 레이어(L0)는 가장 낮은 그레디언트 노이즈를 가 질 수 있으며, 그 사이의 레이어(L1-L10)는 모델에서 감소하는 레이어 위치에 따라 감소하는 그레디언트 노이즈 레벨을 가질 수 있다. 예를 들어, 이후 레이어가 이전 레이어보다 더 높은 그레디언트 노이즈를 갖도록 제어하 면 NLP에서 계산 주기가 줄어들어 성능과 수렴이 유리하게 향상될 수 있다. 그러나, 다른 영역(예컨대, 비 NL P)의 애플리케이션을 위한 다른 모델들에서는, 모델 전반에 걸친 최적의 그레디언트 노이즈 및 기타 통계적 분 포가 다를 수 있으며, 이는 예를 들어, 전술한 특성화 기법을 통해 얻어질 수 있다. 도 8은 실시예에 따른 신경망의 다양한 부분과 관련된 정밀도를 조정하는 일 실시예를 도시한다. 일반적으 로, 더 높은 정밀도(예를 들어, 부동 소수점 16 또는 32)는 모델의 정확도를 증가시키지만, 컴퓨팅 리소스도 증 가시킨다. 본 개시의 특징 및 장점은 훈련 개선을 위해 모델의 다른 부분 또는 위치에서 정밀도(예를 들어, 값 을 나타내는 비트 수)를 조정하는 것을 포함한다. 따라서, 훈련 프로세스를 실행하는 모델은 일부 위치(예컨대, 특정 레이어 또는 노드)에서 높은 정밀도로 구성될 수 있고, 정밀도가 필요하지 않을 수 있는 다 른 위치에서는 낮은 정밀도(예컨대, 8비트 정수)로 구성될 수 있다. 예를 들어, 앞서 설명한 것처럼, 정밀도는 그레디언트 노이즈에 영향을 줄 수 있다. 보다 구체적으로, 정밀도는 양자화 노이즈에 영향을 미친다(예컨대,값을 나타내는 비트 수가 적으면 양자화 노이즈가 증가하여 그레디언트 노이즈가 증가함). 훈련 중인 모델 전 체 또는 모델의 특정 부분(예컨대, 특정 레이어 또는 특정 노드)에서 그레디언트 노이즈를 일정 범위 내에서 유 지하는 것이 바람직할 수 있다. 도 7의 예에 도시된 바와 같이, 서로 다른 레이어에 걸쳐 그레디언트 노이즈를 조정하는 것이 바람직할 수 있다. 따라서, 그레디언트 노이즈가 모니터링될 수 있고, 각 레이어에서 사용되는 정밀도는 훈련을 개선하기 위해 사전결정된 범위 내에서 그레디언트 노이즈를 유지하도록 제어될 수 있다. 예를 들어, 도 8에서, 입력에 대한 레이어(예를 들어, L2 및 L3)는 출력에 대한 레이어(예를 들어, Ln-1 및 Ln-2)보 다 더 높은 정밀도로 구성될 수 있다. 보다 구체적으로, 본 개시의 특징 및 장점은 강력한 협정밀 훈련(robust narrow precision training)을 위해 양자화 노이즈 대 신호 비율(QNSR) 및 기타 하이퍼파라미터의 함수로서 그 레디언트 노이즈를 모델링할 수 있으며, 이는 훈련에 필요한 컴퓨팅 리소스를 감소시킨다. 도 9는 실시예에 따른 신경망의 다양한 부분과 연관된 희소성을 조정하는 일례를 도시한다. 이 예에서, 제어 프로세서(들)와 같은 제어 프로세서(들)는 모델의 하나 이상의 타겟 레이어 및/또는 타겟 레이어의 노드와 연관된 희소성을 조정할 수 있다. 제어 프로세서(들)는 타겟 레이어 및/또는 노드와 연관된 하나 이상 의 통계를 사전결정된 범위 내에서 유지하기 위해 희소성을 조정할 수 있다. 예를 들어, 모델은 초기에, 낮은 희소성을 갖도록 구성되어 건너뛰는 계산과 0으로 강제로 설정되는 값이 보다 적을 수 있다. 제2 레이어(L2)의 낮은 그레디언트 노이즈와 같은 하나 이상의 통계에 기초하여, 제어 프로세서(들)는 제2 레이어(L2)와 관련된 희소성을 조정하여, 다른 레이어들이 초기에 구성된 낮은 희소성을 유지하는 동안, 0으로 강제되는 값과 건너 뛰는 계산이 더 많아지는 높은 희소성을 제공한다. 이러한 제2 레이어(L2)의 조정은 제2 레이어(L2)에서 하나 이상의 통계가 사전결정된 범위 내에서 유지될 수 있게 할 수도 있다. 이 예에서 계속하여, 제어 프로세서 (들)는 추가적으로 또는 대안적으로, 제3 레이어(L3)의 마지막 노드와 같은 타겟 레이어의 주어진 노드와 관련 된 희소성을 조정할 수 있다. 제3 레이어(L3)에서의 이러한 조정은 제3 레이어(L3)와 연관된 하나 이상의 다른 통계를 사전결정된 범위(들) 내에서 유지하도록 허용할 수 있다. 모델의 다양한 부분에서 희소성을 다르게 조정 하기 위한 많은 다른 구성이 가능하다는 것을 인식해야 한다. 도 10은 실시예에 따른 신경망의 다양한 부분과 관련된 모델 크기를 조정하는 일 실시예를 도시한다. 이 예에서, 제어 프로세서(들)는 타겟 레이어와 연관된 다수의 노드 및/또는 모델과 연관된 다수의 레이어를 조정 할 수 있다. 제어 프로세서(들)은 타겟 레이어 및/또는 타겟 레이어의 노드들과 연관될 수 있는 하나 이상의 통 계를 사전결정된 범위(들) 내에서 유지하도록 모델 크기를 조정할 수 있다. 예를 들어, 모델은 처음에 n개의 레이어(L1~Ln)로 구성될 수 있다. 하나 이상의 통계에 따라, 제어 프로세서(들)는 제3 레이어(L3)에 인접한 새 로운 레이어(예컨대, 측정된 통계의 레이어 앞 또는 뒤에 있는 추가의 제4 레이어(L4))를 추가하여 레이어의 수 를 조정하여, 모델이 n + 1 레이어(L1 ~ Ln+1)로 조정되도록 할 수 있다. 이러한 모델 크기의 조정(예컨대, 제 4 레이어(L4)의 추가)을 통해 하나 이상의 통계를 미리 정해진 범위(들) 내에 유지할 수 있다. 이 예에서 계속 하여, 제어 프로세서(들)는 추가적으로 또는 대안적으로, 제2 레이어(L2)의 노드 수에 추가하는 것과 같이 타겟 레이어와 연관된 노드의 수를 조정할 수 있다. 제2 레이어(L2)에서의 이러한 조정은 제2 레이어(L2)와 연관된 하나 이상의 다른 통계를 사전결정된 범위(들) 내에서 유지할 수 있게 할 수 있다. 모델의 다양한 부분에서 모 델 크기를 다르게 조정하기 위해 다수의 다양한 구성이 가능하다는 점을 이해해야 한다. 도 11은 일 실시예에 따라 인공 신경망을 확장하기 위해 AI 프로세서(들)를 사용하는 방법을 예시한다. 이 예 에서, 1102에서, 하나 이상의 AI 프로세서(들)는 신경망 모델 상에서, 훈련 파라미터(들) 세트와 연관된 훈련 프로세스를 실행하도록 구성될 수 있다. AI 프로세서(들)는 AI 프로세서(들)와 유사할 수 있다. 1104에서, AI 프로세서(들)는 모델의 상이한 부분에서 복수의 통계를 갖는 값을 생성하기 위해 훈련 프로세스 (들)를 실행할 수 있다. 1106에서, 시스템은 하나 이상의 통계가 하나 이상의 사전결정된 범위 내에 있는지를 결정할 수 있다. 통계(들)가 사전결정된 범위 내에 있는 경우(예), 1104에서, AI 프로세서(들)는 모델이 사전결 정된 수의 훈련 단계를 통해 수렴할 때까지 모델에 대한 복수의 통계를 생성하기 위한 훈련 프로세스(들)를 계 속 실행할 수 있다. 그러나, 통계(들)가 사전결정된 범위 내에 있지 않은 경우(아니오), 1108에서, AI 프로세 서(들)는 통계(들)를 사전결정된 범위(들) 내에 유지하도록 조정된 훈련 파라미터(들)를 통해 훈련 프로세스(들)를 실행하도록 구성될 수 있다. 그런 다음, 1104로 복귀하여, AI 프로세서(들)는 모델에서 복수의 통계를 생성하기 위해 훈련 프로세스(들)를 다시 실행할 수 있다. 전술한 기법들은 인공 신경망을 처리하도록 구성된 광범위한 컴퓨터 시스템들에서 구현될 수 있다 도 12는, 전술한 개시에 설명된 기술들을 구현하는 데 사용될 수 있는 예시적인 컴퓨터 시스템의 단순화된 블록 다 이어그램을 도시한다. 일부 실시예들에서, 컴퓨터 시스템은 예를 들어 제어 프로세서를 구현하는 데 사용될 수 있다. 도 12에 도시된 바와 같이, 컴퓨터 시스템은 버스 서브시스템을 통해 다수의주변 장치와 통신하는 하나 이상의 프로세서를 포함한다. 이러한 주변 장치들은 저장 서브시스템 (예를 들어, 메모리 서브시스템 및 파일 저장 서브시스템) 및 네트워크 인터페이스 서브시스템 을 포함할 수 있다. 일부 컴퓨터 시스템은 사용자 인터페이스 입력 디바이스 및/또는 사용자 인터 페이스 출력 디바이스를 더 포함할 수 있다. 버스 서브시스템은 컴퓨터 시스템의 다양한 컴포넌트 및 서브시스템이 의도된 바와 같이 서로 통신 할 수 있도록 하는 메커니즘을 제공할 수 있다. 버스 서브시스템은 단일 버스로 개략적으로 도시되어 있 지만, 버스 서브시스템의 다른 실시예들은 다수의 버스를 활용할 수 있다. 네트워크 인터페이스 서브시스템은 컴퓨터 시스템과 다른 컴퓨터 시스템 또는 네트워크 사이에서 데이터를 통신하기 위한 인터페이스로서 작용할 수 있다. 네트워크 인터페이스 서브시스템의 실시예들은, 예를 들어, 이더넷, 와이파이 및/또는 셀룰러 어댑터, 모 뎀(전화, 위성, 케이블, ISDN 등), 디지털 가입자 회선(DSL) 유닛, 및/또는 이와 같은 것을 포함할 수 있다. 저장 서브시스템은 메모리 서브시스템 및 파일/디스크 저장 서브시스템을 포함한다. 서브시 스템(1208, 1210) 및 본 명세서에 설명된 다른 메모리들은 본 개시의 실시예의 기능을 제공하는 실행 가능한 프 로그램 코드 및/또는 데이터를 저장할 수 있는 비일시적 컴퓨터 판독 가능 저장 매체의 예이다. 메모리 서브시스템은 프로그램 실행 중 명령어 및 데이터의 저장을 위한 메인 랜덤 액세스 메모리 (RAM) 및 고정된 명령어들이 저장되는 읽기 전용 메모리(ROM)를 포함하는 다수의 메모리를 포함한 다. 파일 저장 서브시스템은 프로그램 및 데이터 파일에 대한 영구적인(예를 들어, 비휘발성) 저장을 제 공할 수 있으며, 자기 또는 솔리드 스테이트 하드 디스크 드라이브, 관련 이동식 매체(예를 들어, CD-ROM, DVD, 블루레이 등)와 함께 하는 광학 드라이브, 이동식 플래시 메모리 기반 드라이브 또는 카드 및/또는 당업자에게 알려진 다른 유형의 저장 매체를 포함할 수 있다. 컴퓨터 시스템은 예시적인 것이며, 시스템보다 더 많거나 더 적은 컴포넌트를 갖는 다른 많은 구성 이 가능하다는 것을 이해해야 한다. 도 13은 일부 실시예에 따른 인공 신경망 처리 시스템을 예시한다. 다양한 실시예들에서, 본 개시에 따른 신경 망(예컨대, 신경망 600)은 하나 이상의 신경망 프로세서(예컨대, AI 프로세서(들))를 포함하는 하드웨어 환경에 서 구현 및 훈련될 수 있다. 신경망 프로세서는 다양한 그래픽 처리 장치(GPU)(예컨대, 엔비디아(Nvidia Corp®)에서 생산한 신경망 처리용 GPU), 필드 프로그래머블 게이트 어레이(FPGA)(예컨대, 자일링스(Xilinx®) 에서 생산한 신경망 처리용 FPGA) 또는 신경망 연산에 최적화된 하드웨어 아키텍처를 포함하는 다양한 애플리케 이션 특정 집적 회로(ASIC) 또는 신경망 프로세서를 지칭할 수 있다. 이 예시적 환경에서, 도 12에 도시된 아 키텍처를 포함할 수 있는 하나 이상의 서버는 통신 네트워크(예를 들어, 스위치, 라우터 등)를 통 해 복수의 제어기(1310(l)-1310(M))에 결합될 수 있다. 제어기(1310(l)-1310(M))는 또한 도 12에 도시된 아키 텍처를 포함할 수도 있다. 각 제어기(1310-1310(M))는 예를 들어, 처리 유닛(1311-1311(N) 및 1312- 1312(N))과 같은 하나 이상의 신경망(NN) 프로세서에 결합될 수 있다. NN 처리 유닛(1311-1311(N) 및 1312-1312(N))은 훈련 또는 추론과 같은 신경망 처리에 최적화된 다양한 기능 처리 블록 및 메모리의 구성을 포함할 수 있다. NN 프로세서는 신경망 연산에 최적화되어 있다. 서버는 예를 들어, 병렬로 배치된 NN 처리 유닛(131 11(N) 및 1312-1312(N))에 의해 로드 및 실행될 수 있는, 모델에 대한 입력 데이터뿐만 아니 라 NN 모델을 갖는 제어기를 구성할 수 있다. 모델은 예를 들어, 전술한 바와 같이 레이어 및 관련 가중 치를 포함할 수 있다. NN 처리 유닛은 모델을 로드하고 입력을 적용하여 출력 결과를 생성할 수 있다. NN 처리 유닛은 또한 예를 들어, 본원에 설명된 훈련 알고리즘을 구현할 수도 있다. 도 14는 실시예에 따른, 훈련을 위한 컴퓨팅 리소스 및 시간(계산 단위 당 페타플롭/초-일수 * 비용) 대비 검증 손실(예를 들어, 예측의 정확도)의 최적화를 나타내는 예시적 그래프를 도시한다. 이 예에서, 제1 점선 (오른쪽 가장 아래쪽 점선으로 표시됨)은 더 높은 정밀도 또는 비트폭(예컨대, FP32)과 관련된 다양한 크 기 모델에 대한 최적의 파레토 프론티어를 계산한 도표이다. 예를 들어, 선을 교차하여 표시된 모델은 곡 선(1402a)으로 표시된 1011개의 파라미터(예컨대, 가중치)에서 곡선(1402b)으로 표시된 105개의 파라미터에 이르 기까지 다양한 모델 크기를 갖는다. 파라미터가 더 많은 모델(예컨대, 1402a)은 파라미터가 더 적은 모델(예컨 대, 1402b)보다 적은 검증 손실(예컨대, 예측 정확도 향상)을 달성한다. 그러나, 더 많은 파라미터를 갖는 모 델(예컨대, 1402a)은 더 적은 파라미터를 갖는 모델(예컨대, 1402b)보다 처리하는 데 더 많은 컴퓨팅 리소스 및 시간을 필요로 한다.연산 최적 파레토 프론티어는 정밀도를 낮춤으로써(예를 들어, 7 비트) 제2 점선(중간 아래쪽 점선으로 도시됨)으로 이동될 수 있다. 마찬가지로, 정밀도를 3비트까지 낮추면 점선으로 표시된 또 다른 파레토 프론티어가 생성될 수 있다. 각각의 경우, 모델 크기가 확장됨에 따라 정확도가 증가하지만 컴퓨터(및 비용)도 증가한다. 그러나, 이 예에서는 파레토 곡선을 겹치면 1405에 표시된 보다 최적의 새로운 곡선이 생성된다는 것을 보여주다. 파레토 곡선의 경우, 모델 크기가 증가함에 따라 정밀도를 줄이면 주어진 정확도에 대한 컴퓨팅 리소스가 감소할 수 있다. 따라서, 본 개시의 일부 실시예는 모델 크기가 증가함에 따라 훈련되는 모델 의 적어도 일부 부분에서 정밀도를 감소시켜, 예를 들어, 특정 검증 손실을 달성하기 위한 훈련 단계(및 컴퓨팅 사이클)의 수를 줄일 수 있다. 예시적인 특성화 및 매핑 본 명세서에서 논의된 바와 같이, 다양한 실시예에서, 도 1의 매핑 시스템과 같은 매핑 시스템에 의해 구 현된 매핑은 신경망의 훈련을 최적화하도록 훈련 파라미터를 조정하기 위해 통계 및 훈련 파라미터 사이의 관계 를 효율적으로 특성화할 수 있다. 다음은 훈련을 개선하기 위해 훈련 파라미터에 매핑된 망 통계의 예이다. 제1 예에서, 6개의 레이어로 구성된 더 큰 변환기 모델(예컨대, 2천만 개의 파라미터)을 훈련할 때, 훈련 파라 미터를 선택적으로 조정하여 그레디언트 노이즈(예컨대, 통계)를 모니터링하고 제어할 수 있다. 모델의 경우, 그레디언트 노이즈의 다음과 같은 추세가 특징화될 수 있는데, 즉 그레디언트 노이즈는 수렴을 위한 전체 훈련 단계 수의 1-10% 후에 원하는 임계값 이내로 안정화될 수 있고, 정밀도를 낮추면(예를 들어, 양자화에서 비트 수를 낮추면) 그레디언트 노이즈가 증가할 수 있고, 학습 속도를 낮추면 그레디언트 노이즈가 증가할 수 있고, 학습 속도를 높이면 그레디언트 노이즈가 감소할 수 있고, 배치 크기를 낮추면 그레디언트 노이즈가 증가할 수 있고, 배치 크기를 높이면 그레디언트 노이즈가 감소할 수 있다. 따라서, 그레디언트 노이즈는 전술한 훈련 파 라미터(예컨대, 정밀도, 학습 속도 및/또는 배치 크기) 중 하나 이상을 수정하여 제어될 수 있다. 예를 들어, 사전결정된 범위에 대한 목표 그레디언트 노이즈는 알려진 폐쇄형 추정치(예컨대, 를 사용하여 결정될 수 있으며, 여기서 B는 배치 크 기를 나타내고, L은 교차 엔트로피 손실을 나타내며, a는 스케일링을 위한 멱법칙 지수를 나타낸다. 또한, 또 는 대안적으로, 목표 그레디언트 노이즈는 알려진 구성으로 사전결정된 시간 동안 훈련을 모니터링함으로써 결 정될 수 있다. 도 15a를 참조하여, 예시적인 변환기 모델을 더 높은 정밀도 또는 비트폭(예를 들어, 16비트)으로 훈련할 때, 그래디언트 노이즈는 도시된 바와 같이 각 레이어에서 약 104.5에서 안정화되도록 모니터링될 수 있다. 도 15d 를 추가로 참조하면, 이것은 다수의 훈련 단계에 걸쳐 주어진 검증 손실(예컨대, 예측의 정확도)에서 더 높은 정밀도 모델의 수렴을 나타내는 그래프를 초래할 수 있다. 도 15b를 참조하면, 예시 모델을 더 낮은 정밀도 또는 비트폭(예를 들어, 12비트)으로 훈련할 때, 그리고 훈련 파라미터(들)에 대한 조정 없이(예를 들어, 최적화 없이), 그레디언트 노이즈는 특히 도시된 바와 같이 처음 두 레이어에서 현저히 증가할 수 있다. 도 15d를 다시 참조하면, 이것은 다수의 훈련 단계에 걸쳐, 최적화 없이, 낮은 정밀도 모델의 발산(예컨대, 예측의 정확도 손실)을 나타내는 그래프를 초래할 수 있다. 그러나, 이 예에서, 낮은 정밀도 모델은 그레디언트 노이즈를 제어하기 위해 하나 이상의 훈련 파라미터를 조정 함으로써 최적화될 수 있다. 예를 들어, 학습 속도는 네트워크의 첫 번째 레이어와 마지막 레이어 사이의 비율 에 맞는 멱급수 맞춤(power series fit)에 의해 스케일링될 수 있다. 즉, f(x)=axk이다. 이 방정식을 첫 번째 레이어의 경우 x=l로, 그리고 마지막 레이어의 경우 x=6으로 맞추면, 레이어별 학습 속도 스케일이 사용될 수 있다. 학습 속도는 그레디언트 노이즈를 제어하기 위해 사용될 수 있는 하나의 훈련 파라미터이며, 각 레이어 를 제1 레이어에 대해 상대적으로 스케일링함으로써, 낮은 정밀도 모델(예를 들어, 12비트)에 대해 그레디언트 노이즈가 제어될 수 있다. 도 15d를 다시 참조하면, 이것은 다수의 훈련 단계에 걸쳐, 최적화와 함께, 낮은 정밀도 모델의 수렴(예컨대, 예측 정확도의 증가)을 나타내는 그래프를 초래할 수 있다. 따라서, 낮은 정밀도 모델의 레이어에서 학습 속도 및/또는 가중치 감소를 조정하여 레이어에서의 그레디언트를 제어하는 것은 모델의 성공적인 수렴을 초래 할 수 있다. 이러한 관계들은 모델에 대해 특성화될 수 있고 그에 따라 매핑될 수 있다. 제2 예에서, 2개의 레이어를 갖는 변환기 모델을 훈련할 때, 활성화의 표준 편차 및 그레디언트의 표준 편차는 학습 속도 및/또는 가중치 감쇠(예를 들어, 훈련 파라미터)를 조정함으로써 모니터링 및 제어될 수 있다. 도16a를 참조하면, 그래프는 다수의 훈련 단계에 걸쳐 수렴(예컨대, 안정화되는 검증 손실 감소)을 초래하 는 더 높은 정밀도 또는 비트폭과 연관된 모델을 나타낸다. 그러나, 그래프는 다수의 훈련 단계에 걸쳐 발산(예를 들어, 증가하는 검증 손실)을 초래하는 (예를 들어, 최적화되지 않은) 더 낮은 정밀도 또는 비트폭과 연관된 모델을 나타낸다. 도 16b를 참조하면, (예를 들어, 변환기 모델의 어텐션 레이어(attention layer)에서) 레이어 활성화의 표준 편 차는 높은 정밀도 모델(그래프)과 낮은 정밀도 모델(그래프) 사이에서 비교될 수 있다(예컨대, 최 적화 없이). 특히, 낮은 정밀도 모델은 높은 정밀도 모델과 비교할 때 노드 활성화를 크게 증가시킨다. 따라서, 레이어 활성화의 표준 편차와 학습 속도 및/또는 가중치 감쇠와 같은 훈련 파라미터 사이의 관계는 낮 은 정밀도 모델을 제어하고 최적화하기 위해 특성화될 수 있다. 또한, 도 16c를 참조하면, 레이어 그레디언트의 표준 편차는 높은 정밀도 모델(그래프)과 낮은 정밀도 모 델(그래프) 사이에서 비교될 수 있다(예컨대, 최적화없이). 특히, 낮은 정밀도 모델은 높은 정밀도 모델 과 비교했을 때 훈련의 초기 부분에서 상당히 작은 그레디언트를 야기한다. 따라서, 레이어 그레디언트의 표준 편차와 학습 속도 및/또는 가중치 감쇠와 같은 훈련 파라미터 사이의 관계는 낮은 정밀도 모델을 제어 및 최적 화하도록 특성화될 수 있다. 도 17a-b는 정밀도와 0의 백분율 사이의 관계를 예시한다. 도 17a를 참조하면, 그래프는 다수의 훈련 단 계에 걸쳐 상이한 정밀도(또는 비트 폭)에 대한 손실 곡선을 나타낸다. 플롯은 다수의 훈련 단계에 걸쳐 발산(예컨대, 검증 손실 증가)을 초래하는 낮은 정밀도 또는 비트 폭(예컨대, 최적화되지 않음)과 연관된 모델 을 나타낸다. 다양한 정밀도 레벨과 연관된 모델에 대한 추가 그래프가 그래프와 그래프 사이에 도 시되어 있다. 도 17b를 참조하면, 레이어 활성화(예컨대, 셀프어텐션 레이어)의 0 값의 백분율은 낮은 정밀도 모델(그래프 )(예컨대, 최적화 없음)과 증가하는 정밀도 레벨을 갖는 모델 사이에서 비교될 수 있다. 특히, 낮은 정밀 도 모델은 증가하는 정밀도 레벨을 갖는 모델에 비해 더 큰 백분율의 0을 포함한다. 끝으로, 도 17c를 추가로 참조하면, 레이어 활성화(예컨대, 다중 레이어 퍼셉트론)의 0 값의 백분율은 낮은 정 밀도 모델(그래프)(예컨대, 최적화 없이)과 증가하는 정밀도 레벨을 갖는 모델 사이에서 비교될 수 있다. 특히, 낮은 정밀도 모델은 증가하는 정밀도 레벨을 갖는 모델에 비해, 더 큰 백분율의 0을 포함하며, 이 경우 약 6만 반복에서 이 레이어에서 활성화 값의 35%가 0이 된다. 따라서, 특정 레이어 활성화의 0 값의 백분율과 훈련 파라미터 사이의 이러한 관계는 훈련을 제어하고 최적화하 도록 특성화될 수 있다. 추가 실시예들 다양한 실시예에서, 본 개시는 인공 신경망을 최적화하기 위한 시스템, 방법, 및 장치를 포함한다. 본 명세서 에 기술된 기술들은 컴퓨터 시스템에 의해 실행 가능한 프로그램을 저장하는 비일시적 기계 판독 가능 매체로 구체화될 수 있으며, 프로그램은 본 명세서에 기술된 기술들을 수행하기 위한 명령어 세트를 포함한다. 일부 실 시예에서, 시스템은 하나 이상의 제어 프로세서 및 하나 이상의 제어 프로세서 중 적어도 하나에 의해 실행될 때 적어도 하나의 제어 프로세서가 상술한 기술들을 수행하도록 하는 명령어를 저장하는 비일시적 머신 판독 가 능 매체를 포함한다. 일부 실시예에서, 비-일시적 머신 판독 가능 매체는 예를 들어, 하나 이상의 제어 프로세 서 또는 하나 이상의 인공 지능 프로세서에 결합될 수 있는 메모리일 수 있다. 다음 기술들은 단독으로 또는 상이한 조합으로 구현될 수 있으며, 본원에 설명된 다른 기술들과 함께 추가로 구 현될 수 있다. 예를 들어, 일 실시예에서, 본 개시는 하나 이상의 인공 지능(AI) 프로세서, 하나 이상의 제어 프로세서, 및 하 나 이상의 제어 프로세서에 의해 실행 가능한 프로그램 코드가 저장된 비일시적 컴퓨터 판독 가능 저장 매체를 포함하되, 프로그램 코드는 하나 이상의 제어 프로세서로 하여금, 인공 신경망 모델을 수신하고, 복수의 훈련 파라미터에 기초하여, 하나 이상의 AI 프로세서에서 훈련 프로세스를 실행하도록 모델을 구성하고, 훈련 프로세 스의 실행 시 생성된 복수의 통계를 모니터링하고, 복수의 통계 중 적어도 하나를 사전결정된 범위 내에서 유지 하기 위해 복수의 통계 중 하나 이상의 통계에 기초하여 복수의 훈련 파라미터 중 하나 이상의 훈련 파라미터를 조정하게 한다. 일 실시예에서, 조정 단계는, 모델의 타겟 레이어와 관련된 정밀도를 조정하여, 타겟 레이어와 관련된 복수의 통계 중 적어도 하나를 사전결정된 범위 내에서 유지하는 것을 포함한다. 일 실시예에서, 시스템은 모델 크기가 증가함에 따라 모델의 적어도 일부의 정밀도를 더 감소시킨다. 일 실시예에서, 모니터 단계는 모델의 타겟 레이어와 관련된 하나 이상의 통계를 모니터링하는 것을 포함한다. 일 실시예에서, 하나 이상의 통계는 신경망 그레디언트 노이즈의 측정을 포함한다. 일 실시예에서, 조정 단계는 모델의 타겟 레이어와 연관된 하나 이상의 훈련 파라미터를 조정하는 것을 포함한 다. 일 실시예에서, 타겟 레이어는 숨겨진 레이어이다. 이 실시예에서, 하나 이상의 훈련 파라미터는 타겟 레이어에 대한 정밀도를 구성하는 파라미터를 포함한다. 일 실시예에서, 타겟 레이어는 숨겨진 레이어이다. 이 실시예에서, 하나 이상의 훈련 파라미터는 타겟 레이어에 대한 희소성을 구성하는 파라미터를 포함한다. 일 실시예에서, 타겟 레이어는 숨겨진 레이어이다. 이 실시예에서, 하나 이상의 훈련 파라미터는 타겟 레이어와 연관된 다수의 노드를 구성하는 파라미터를 포함한다. 일 실시예에서, 타겟 레이어는 숨겨진 레이어이다. 이 실시예에서, 하나 이상의 훈련 파라미터는 모델과 연관된 다수의 레이어를 구성하는 파라미터를 포함한다. 일 실시예에서, 복수의 훈련 파라미터는 모델과 연관된 정밀도를 구성하는 파라미터를 포함한다. 이 실시예에서, 모델은 모델의 제1 레이어와 연관된 제1 정밀도 및 모델의 제2 레이어와 연관된 제2 정밀도로 구성 되며, 제1 정밀도는 제2 정밀도보다 높고, 제1 레이어는 모델의 입력에 더 가까운 레이어이고, 제2 레이어는 모 델의 출력에 더 가까운 레이어다. 일 실시예에서, 복수의 훈련 파라미터는 희소성을 구성하는 파라미터를 포함한다. 일 실시예에서, 모델은 모델 의 제1 레이어와 연관된 제1 희소성 및 모델의 제2 레이어와 연관된 제2 희소성으로 구성되고, 제1 희소성은 제 2 희소성과 상이하며, 제1 레이어는 모델의 입력에 더 가까운 레이어이고, 제2 레이어는 모델의 출력에 더 가까 운 레이어이다. 일 실시예에서, 모델은 제2 크기를 갖는 제2 모델이고, 훈련 프로세스는 제2 훈련 프로세스이다. 이 실시예에 서, 수신 단계 이전에, 프로그램 코드는 하나 이상의 제어 프로세서로 하여금: 제1 모델을 수신하고, 복수의 훈 련 파라미터에 기초하여, 제1 훈련 프로세스를 실행하도록 제1 모델을 구성하고- 제1 모델은 제2 모델의 더 작 은 버전이고, 제1 모델은 제1 모델 크기를 가지며, 제1 모델 크기는 제2 모델 크기보다 실질적으로 작음 -, 제1 훈련 프로세스의 실행 시 생성되는 복수의 통계를 모니터링하며, 복수의 통계와 복수의 훈련 파라미터 사이의 하나 이상의 매핑을 생성하도록 한다. 일 실시예에서, 프로그램 코드는 하나 이상의 제어 프로세서로 하여금 제2 훈련 프로세스의 실행을 위해 하나 이상의 통계에 기초하여 하나 이상의 훈련 파라미터를 조정하기 위해 하나 이상의 매핑을 로드하도록 한다. 일 실시예에서, 프로그램 코드는 하나 이상의 제어 프로세서로 하여금 통계와 파라미터들 사이의 관계를 형성하 는 훈련된 모델을 포함하는 하나 이상의 매핑을 로드하게 한다. 일 실시예에서, 복수의 훈련 파라미터는 모델 크기를 구성하는 파라미터, 배치 크기를 구성하는 파라미터, 학습 속도를 구성하는 파라미터, 정밀도를 구성하는 파라미터, 및 희소성을 구성하는 파라미터 중 하나 이상을 포함 한다. 일 실시예에서, 복수의 통계는 신경망 그레디언트의 측정, 신경망 그레디언트 노이즈의 측정, 신경망 노드 활성 화의 측정, 신경망 가중치의 측정, 및 신경망 가중치 업데이트의 측정 중 하나 이상을 포함한다. 일 실시예에서, 하나 이상의 측정은 평균 측정, 표준 편차 측정, 및 영값의 백분율 측정 중 하나 이상을 포함한 다. 일 실시예에서, 훈련 프로세스는 복수의 AI 프로세서들 상에서 실행되도록 분할된다. 이 실시예에서, 복수의 통 계의 측정값은 복수의 AI 프로세서로부터 수신된다. 일 실시예에서, 본 개시는 인공 신경망 모델에 대한 훈련 프로세스를 실행하도록 구성된 하나 이상의 AI 프로세 서- 훈련 프로세스는 연관된 훈련 파라미터 세트를 가지며, 훈련 프로세스의 실행이 복수의 통계를 생성함 -와,하나 이상의 AI 프로세서에 결합된 하나 이상의 제어 프로세서를 포함하되, 하나 이상의 제어 프로세서는 복수 의 통계를 수신하도록 구성되고, 이에 따라 훈련 프로세스의 실행 동안 복수의 통계 중 적어도 하나를 사전결정 된 범위 내에서 유지하도록 훈련 파라미터 중 하나 이상을 조정한다. 상기의 설명은 본 개시의 다양한 실시예와 함께 특정 실시예의 측면들이 어떻게 구현될 수 있는지에 대한 예들 을 예시한다. 상기 실시예들은 유일한 실시예 로 간주되어서는 안 되며, 다음 청구범위에 정의된 바와 같은 특 정 실시예들의 유연성 및 장점을 설명하기 위해 제시된다. 상기 개시 및 다음의 청구범위에 기초하여, 청구범 위에 정의된 바와 같은 본 개시의 범위를 벗어나지 않고 다른 배열, 실시예, 구현 및 균등물들이 사용될 수 있 다."}
{"patent_id": "10-2023-7026041", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시의 다양한 실시예가 첨부된 도면에서 제한이 아닌 예시로서 설명된다. 도 1은 실시예에 따른 인공 신경망을 훈련시키기 위한 시스템을 도시한다. 도 2는 실시예에 따른 인공 신경망의 훈련 방법을 도시한다. 도 3a는 실시예에 따른 인공 신경망의 훈련 파라미터 및 모니터링 값을 도시한다. 도 3b는 실시예에 따른 인공 신경망의 훈련을 위한 매핑 시스템을 도시한다. 도 4는 다른 실시예에 따른 인공 신경망의 훈련 방법을 도시한다. 도 5a는 일 실시예에 따라 더 작은 네트워크를 훈련시켜 매핑을 생성하는 시스템을 도시한다. 도 5b는 실시예에 따라 더 큰 네트워크를 훈련시키기 위한 매핑을 로드하는 시스템을 도시한다. 도 6은 실시예에 따라 통계를 생성할 수 있는 예시적인 신경망을 도시한다. 도 7은 실시예에 따른 예시적인 모니터링 통계를 나타내는 그래프를 도시한다. 도 8은 실시예에 따른 신경망의 다양한 부분과 연관된 정밀도를 조정하는 예를 도시한다. 도 9는 실시예에 따른 신경망의 다양한 부분과 연관된 희소성(sparsity)을 조정하는 예를 도시한다. 도 10은 실시예에 따른 신경망의 다양한 부분과 연관된 모델 크기를 조정하는 예를 도시한다. 도 11은 실시예에 따른 인공 신경망의 스케일링을 위해 AI 프로세서를 사용하는 방법을 도시한다. 도 12는 실시예에 따른 예시적인 컴퓨터 시스템의 단순화된 블록 다이어그램을 도시한다. 도 13은 실시예에 따른 인공 신경망 처리 시스템을 도시한다. 도 14는 실시예에 따른 컴퓨팅 리소스 및 시간 대비 검증 손실의 최적화를 나타내는 예시적인 그래프를 도시한 다. 도 15a는 실시예에 따른 고정밀 모델에 의해 생성되는 그레디언트 노이즈를 나타내는 그래프를 도시한다. 도 15b는 실시예에 따른 최적화되지 않은 저정밀 모델에 의해 생성된 그레디언트 노이즈를 나타내는 그래프를 도시한다. 도 15c는 실시예에 따라 최적화된 저정밀 모델에서 제어되는 그레디언트 노이즈를 나타내는 그래프를 도시한다. 도 15d는 실시예에 따른 고정밀 모델의 수렴, 최적화되지 않은 저정밀 모델의 발산, 및 최적화된 저정밀 모델의 수렴을 나타내는 그래프를 도시한다. 도 16a는 실시예에 따른 고정밀 모델의 수렴 및 최적화되지 않은 저정밀 모델의 발산을 나타내는 그래프를 도시 한 도면이다. 도 16b는 실시예에 따른 고정밀 모델에 의해 생성된 레이어 활성화와 최적화되지 않은 저정밀 모델에 의해 생성 된 레이어 활성화를 비교하는 그래프를 도시한다. 도 16c는 실시예에 따라, 고정밀 모델에 의해 생성된 레이어 그레디언트와 최적화되지 않은 저정밀 모델에 의해 생성된 레이어 그레디언트를 비교하는 그래프를 도시한다. 도 17a는 실시예에 따라 상이한 정밀도를 갖는 모델의 검증 손실을 나타내는 그래프를 도시한다. 도 17b는 실시예에 따라 상이한 정밀도를 갖는 모델들에 의해 생성된 레이어 활성화의 0 값의 백분율의 제1 예 를 비교하는 그래프를 도시한다. 도 17c는 실시예에 따라 상이한 정밀도를 갖는 모델에 의해 생성된 레이어 활성화의 0 값의 백분율의 제2 예를 비교하는 그래프를 도시한다."}
