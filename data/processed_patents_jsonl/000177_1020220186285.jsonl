{"patent_id": "10-2022-0186285", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0103788", "출원번호": "10-2022-0186285", "발명의 명칭": "FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치 및 방법", "출원인": "(주)네오와인", "발명자": "이효승"}}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치에 있어서,학습 완료된 ONNX 모델로부터 오퍼레이터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 CONNX모델로 변환하는 모델 변환부와,상기 모델 변환부에서 출력된 CONNX 모델을 파싱하고 추론을 위한 입력 데이터를 입력받아 추론을 수행하는 추론부를 포함하는 인공지능 모델 추론장치."}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 CONNX 모델은 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 것을 특징으로 하는 인공지능 모델 추론장치."}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 CONNX 파일은 상기 ONNX 모델의 메타정보를 포함하고, 상기 텍스트 파일은 상기 ONNX 모델의 그래프 구조정보 및 메모리 정보를 포함하고, 상기 데이터 파일은 상기 그래프 구조에서 각 오프레이터가 사용하는 가중치정보를 바이너리 데이터 형식으로 포함하는 것을 특징으로 하는 인공지능 모델 추론 장치."}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 추론부는 상기 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU가 처리할 오퍼레이터와 NPU가 처리할 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기(write)하여 추론 준비를 수행하는 모델 파싱부와,상기 NPU의 컨벌루션 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가중치 데이터 영역 및 출력 데이터영역으로 구분하고 연산을 위한 데이터를 벡터 형태로 변환하여 처리하는 모델 실행부를 포함하는 것을 특징으로 하는 인공지능 모델 추론 장치."}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "프로세서에 의해 수행되는 FPGA 기반의 NPU에 적용되는 인공지능 모델 추론 방법에 있어서,학습 완료된 ONNX 모델을 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 CONNX 모델로 변환하는 단계와, 상기 CONNX 파일을 읽어 그래프에 포함된 각 오퍼레이터(Operator)를 계산하여 추론을 수행하는 단계를 포함하는 인공지능 모델 추론 방법."}
{"patent_id": "10-2022-0186285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 추론을 수행하는 단계는 상기 CONNX 파일의 메타데이터를 읽어 버전 정보 및 그래프 개수 정보를 확인하는단계와,상기 확인한 그래프 개수에 따라 상기 n개의 텍스트 파일을 읽어 그래프 내부의 메모리 설정 정보 및 오퍼레이터 구성 정보를 확인하는 단계와, 공개특허 10-2024-0103788-3-각 그래프의 가중치 정보를 포함하는 데이터 파일의 리스트를 확인하는 단계와,추론을 위한 입력 데이터를 입력받아 메모리에 할당하는 단계와,상기 데이터 파일의 리스트에서 각 항목의 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계와, 그래프의 각 오퍼레이터를 수행하는 단계를 포함하는 것을 특징으로 하는 인공지능 모델 추론 방법."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능 모델 추론 기술에 관한 것으로서, 상세하게는 기존 딥 러닝 프레임워크(Deep-Learning Framework)로 학습된 인공지능 모델이 NPU 환경에서 효율적인 연산을 수행할 수 있도록 한 FPGA 기반 NPU에 적용 되는 인공지능 모델 추론 장치 및 방법에 관한 것이다. 이를 위해 본 발명에 따른 인공지능 모델 추론 장치는 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치로서, 학습 완료된 ONNX 모델로부터 오퍼레이터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 CONNX 모델로 변환하는 모델 변환부와, 상기 모델 변환부에서 출력된 CONNX 모델을 파싱하고 추론을 위한 입력 데이터를 입력받아 추론을 수행하는 추론부를 포함한다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 모델 추론 기술에 관한 것으로서, 상세하게는 기존 딥 러닝 프레임워크(Deep-Learning Framework)로 학습된 인공지능 모델이 NPU 환경에서 효율적인 연산을 수행할 수 있도록 한 FPGA 기반 NPU에 적 용되는 인공지능 모델 추론 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥 러닝 사용을 위한 소프트웨어(S/W) 기술은 크게 학습용 S/W와 추론용 S/W로 나뉘며, 일반적으로 GPU 환경에 서 학습용 S/W와 추론용 S/W가 서로 통합된 형태로 개발되었다. 그러나 NPU(Neural Processing Unit) 연산 가속 환경에서는 추론용 S/W만 최적화되어 사용되고 있으며 학습에서 는 PyTorch, Tensorflow 등의 기존 딥 러닝 프레임워크가 사용되고 있다. 이로 인해 기존 딥 러닝 프레임워크로 학습된 모델을 NPU 환경에서 추론하기 위한 S/W가 필요하다. NPU는 GPU와 다르게 AI 모델의 추론 연산에 최적화되어 설계된 H/W로 연산속도는 GPU보다 매우 빠르지만 GPU처 럼 모든 AI모델을 추론할 수 있는 구조가 아니고, Convolution 등 특정 AI 모델 연산에만 최적화되어 있어서 설 계 내용에 따라 실행할 수 있는 연산 범위가 제한적이다. 특히 NPU 연산을 위한 메모리 용량 등 전체적인 H/W 사양이 GPU보다 낮아 메모리를 효율적으로 잘 사용해야 하 며, NPU에서 처리할 수 없는 연산들은 CPU를 사용하여 연산해야 한다. 또한 AI 모델 추론 수행 시 NPU와 CPU간에 연산 전환을 최소화해야 빠른 연산 처리가 가능하므로 이를 위해 효 율적인 메모리 활용 방법 및 CPU 연산 최적화 기술 등이 필요하다. 이에 따라 기존 딥 러닝 S/W 또는 프레임워크를 활용하여 학습된 모델이 FPGA 기반으로 구현된 NPU에서 AI 모델 추론을 수행할 수 있도록 하는 방법을 개발할 필요가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허공보 제10-2022-0136806호"}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기 문제점을 해결하기 위해 창안된 것으로서, 본 발명의 목적은 기존 인공지능 학습 프레임워크 (ONNX)를 사용하여 획득한 모델이 FPGA 기반으로 구현된 NPU에서도 빠르게 인공지능 추론을 수행할 수 있도록 하는 것이다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이를 위해 본 발명에 따른 인공지능 모델 추론 장치는 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치로서, 학습 완료된 ONNX 모델로부터 오퍼레이터 정보, 가중치 정보 및 속성 정보를 추출하여 상기 ONNX 모델을 CONNX 모델로 변환하는 모델 변환부와, 상기 모델 변환부에서 출력된 CONNX 모델을 파싱하고 추론을 위한 입력 데이터 를 입력받아 추론을 수행하는 추론부를 포함한다. 상기 CONNX 모델은 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 것을 특징으로 한다. 상기 CONNX 파일은 상기 ONNX 모델의 메타정보를 포함하고, 상기 텍스트 파일은 상기 ONNX 모델의 그래프 구조 정보 및 메모리 정보를 포함하고, 상기 데이터 파일은 상기 그래프 구조에서 각 오프레이터가 사용하는 가중치 정보를 바이너리 데이터 형식으로 포함하는 것을 특징으로 하는 인공지능 모델 추론 장치. 상기 추론부는 상기 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU가 처리할 오퍼레이터와 NPU가 처 리할 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기(write)하여 추론 준비를 수행하는 모 델 파싱부와, 상기 NPU의 컨벌루션 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가중치 데이터 영역 및 출력 데이터 영역으로 구분하고 연산을 위한 데이터를 벡터 형태로 변환하여 처리하는 모델 실행부를 포함하 는 것을 특징으로 한다. 또한 본 발명에 따른 인공지능 모델 추론 방법은 프로세서에 의해 수행되는 FPGA 기반의 NPU에 적용되는 인공지 능 모델 추론 방법으로서, 학습 완료된 ONNX 모델을 CONNX 파일, 텍스트 파일 및 데이터 파일로 구성된 CONNX 모델로 변환하는 단계와, 상기 CONNX 파일을 읽어 그래프에 포함된 각 오퍼레이터(Operator)를 계산하여 추론을 수행하는 단계를 포함한다. 상기 추론을 수행하는 단계는 상기 CONNX 파일의 메타데이터를 읽어 버전 정보 및 그래프 개수 정보를 확인하는 단계와, 상기 확인한 그래프 개수에 따라 상기 n개의 텍스트 파일을 읽어 그래프 내부의 메모리 설정 정보 및 오퍼레이터 구성 정보를 확인하는 단계와, 각 그래프의 가중치 정보를 포함하는 데이터 파일의 리스트를 확인하 는 단계와, 추론을 위한 입력 데이터를 입력받아 메모리에 할당하는 단계와, 상기 데이터 파일의 리스트에서 각 항목의 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계와, 그래프의 각 오퍼레이터를 수행하는 단 계를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상기와 같은 본 발명에 따르면, ONNX 모델을 CONNX 모델로 변환하고 이를 CONNX 엔진(Engine)을 통해 모델을 실 행 및 추론함으로써 FPGA 환경에서 구현된 NPU에서 활용이 가능하도록 하며, CPU와 NPU을 동시에 활용하여 AI 모델 추론 시 메모리 Read/Write를 최소화하여 기존 다른 딥러닝 프레임워크(Deep Learning Framework)보다 빠 른 속도의 인공지능 모델 추론이 가능한 효과가 있다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명에 따른 실시예를 상세하게 설명한다. 본 발명의 구성 및 그에 따른 작용 효과는 이하의 상세한 설명을 통해 명확하게 이해될 것이다. 본 발명의 상세한 설명에 앞서, 동일한 구성요소에 대해서는 다른 도면상에 표시되더라도 가능한 동일한 부호로 표시하며, 공지된 구성에 대해서는 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 구체적인 설명은 생략하기로 함에 유의한다. ONNX(Open Neural Network Exchange)는 Tensorflow, PyTorch 등과 같은 서로 다른 DNN(Deep Neural Network) 프레임워크(framework) 환경에서 만들어진 모델들이 호환되도록 한 공유 플랫폼을 말한다. Tensorflow에서 모델 을 만들고 이를 ONNX 모델로 변환하면 이후에 PyTorch 등과 같은 다른 프레임워크에서도 Tensorflow에서 만든 모델을 사용할 수 있다. 그리고 CONNX는 C 언어로 구현한 ONNX를 말한다. 도 1은 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치의 개략적인 구성을 나타낸 것이다. 도 1을 참조하면, 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치는 ONNX 모델을 CONNX 모 델로 변환하는 모델 변환부 및 CONNX 모델을 파싱하여 추론하는 추론부로 구성된다. CONNX 모델은 NPU 에 적합한 형식의 모델이다. 모델 변환부는 학습이 완료된 ONNX 모델을 입력받아 기정의된 ONNX의 프로토콜 버퍼(protocol buffer) 구조 에 따라 표 1과 같이 주요 정보를 파싱한다. 표 1"}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "표 1에 나타낸 ONNX 모델의 주요 정보 외에 버전과 관련한 메타데이터(metadata) 및 각 정보에 대한 설명 (human-readable information)이 포함되어 있다. 모델 변환부는 주요 정보 외에 불필요한 정보는 사용하지 않고 모델 추론에 필요한 주요 정보만을 사용하여 ONNX 모델을 CONNX 모델로 변환한다. 이때 ONNX 모델을 CONNX 모델 구조로 변환하는 세부 절차는 3가지의 파일 생성 프로세스를 포함한다. 1) CONNX 파일 생성 프로세스 CONNX 파일은 CONNX의 버전 정보, ONNX의 opset 버전 정보, 그래프(Graph)의 개수 정보를 포함한다. CONNX의 버전 정보는 ONNX 모델에서 획득한 정보가 아닌 별도로 지정된 버전 정보를 사용한다. opset 버전 정보는 학습 된 ONNX 모델의 opset 정보를 파싱하여 활용한다. 그래프의 개수 정보는 학습된 ONNX 모델의 GraphProto로부터 정보를 파싱하여 활용한다. CONNX 파일은 아래와 같이 지정된 포맷으로 작성된다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "2) TEXT 파일 생성 프로세스 TEXT 파일은 그래프의 구성 정보를 포함한다. 그래프의 구성 정보는 그래프 내의 오퍼레이터(Operator) 구성 정 보 및 메모리 설정 정보를 가진다. 오퍼레이터 구성 정보는 학습된 ONNX 모델에서 NodeProto로부터 정보를 파싱 하여 활용한다. TEXT 파일은 아래와 같은 지정된 포맷으로 작성된다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "3) DATA 파일 생성 프로세스 DATA 파일은 학습된 ONNX 모델의 가중치가 바이너리 형식으로 작성된 리스트를 포함한다. 리스트는 ONNX 모델에 서 TensorProto로부터 정보를 파싱하여 활용한다. DATA 파일은 별도의 정해진 포맷이 없다. 상술한 3가지의 파일 생성 프로세스를 통해, 도 2에 도시된 바와 같이, 모델 변환부는 ONNX 모델 (model.onnx)로부터 CONNX 파일(model.ponnx), n개의 텍스트 파일({n}.text), 텍스트 파일당 m개의 데이터 파 일({n}_{m}.data)을 생성한다. 추론부는 모델 변환부로부터 출력된 CNNX 모델의 파일을 NPU에서 추론하기 위해 모델 파싱부와 모 델 실행부로 구성된다. 모델 파싱부는 CONNX 모델의 파일을 파싱한다. 모델 파싱부는 CONNX 파일을 읽어 버전 정보(CONNX의 버 전 정보 및 ONNX의 opset 버전 정보) 및 그래프 개수 정보를 출력한다. 또한 모델 파싱부는 텍스트 파일을 읽어 ONNX 그래프의 구조 정보를 출력하고, 데이터 파일을 읽어 ONNX 그 래프의 각 오퍼레이터 정보를 출력한다. 즉, 모델 파싱부는 ONNX 그래프 내의 오퍼레이터(Operator) 구성 정보 및 메모리 설정 정보를 추출하고, ONNX 그래프의 각 오퍼레이터에 의해 사용되는 가중치 정보가 바이너리 데이터로 구성된 리스트를 추출한다. 이 에 따라 모델 파싱부는 CONNX 모델로부터 오퍼레이터(Operator) 정보를 읽어 CPU가 처리할 오퍼레이터와 NPU가 처리할 오퍼레이터를 구분하고, 오퍼레이터의 가중치 정보를 메모리에 쓰기(write)하여 추론 준비를 수행 하게 된다. 모델 실행부는 추론을 위한 입력 데이터를 입력받아 메모리에 할당하고, 리스트에서 바이너리 데이터를 읽 어 그래프의 가중치를 초기화한 후 그래프의 각 오퍼레이터를 수행한다. 모델 실행부는 NPU의 컨벌루션 연산을 위한 FPGA의 메모리 영역을 입력 데이터 영역, 가중치 데이터 영역 및 출력 데이터 영역으로 구분하고 연산을 위한 데이터를 벡터 형태로 변환하여 처리한다. 모델 실행부(CONNX Executor)는 도 3에 도시된 구조로 설계된다. 모델 실행부는 CONNX 모델의 3가지 파 일의 파싱 결과에 근거해 모델 실행을 위한 가중치 초기화 기능 및 메모리 관리 기능을 수행한다. 구체적으로 모델 실행부에는 NPU 환경에서 메모리 사용을 위해 시스템 콜 명령어 연동을 위한 HAL(hardware Abstraction Layer), CONNX 모델 실행 시 입출력 데이터 정의를 위한 Tensor, 그래프의 각 노드(Node)에 대한 연산 시 오퍼레이터(Operator) 설정을 위한 Opset 등이 설계되어 있다. 도 4는 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 방법의 순서도를 나타내고, 도 5는 파싱 이후의 실제 추론 과정을 나타낸 것이다. 도 4 및 도 5에 도시된 각 처리 단계는 본 발명에 따른 추론부에서 수행된다. 도 4에서, 먼저 CONNX 모델이 로딩되면(S10), CONNX 파일을 파싱하여 버전 정보 및 그래프 개수 정보를 확인하 는 모델 파싱(Model Parsing) 단계(S20)를 수행한다. 다음, 그래프 개수 정보에 따라 텍스트 파일을 파싱하여 그래프 구조 정보를 확인하고, 그래프 구조 정보에 따 라 데이터 파일을 파싱하여 그래프의 각 오퍼레이터에 의해 사용되는 가중치 정보가 바이너리 데이터로 구성된 리스트를 확인하는 그래프 파싱(Graph parsing) 단계(S30)를 수행한다. 파싱이 완료되면 실제 추론 과정 (Run)(S40)이 수행된다 도 5에서, 실제 추론 과정(S40)은 추론을 위한 입력 데이터(Inference Data)가 입력되면, 입력 데이터를 메모리 에 할당하는 단계(Set Input), 리스트에서 바이너리 데이터를 읽어 그래프의 가중치를 초기화하는 단계 (Initialize value_info), 입력 데이터 및 가중치를 이용하여 그래프의 각 오퍼레이터를 수행하는 단계(Execute operators), 추론된 결과값을 출력하는 단계(Set Output), 그래프의 가중치를 리셋하는 단계(Clear value_info)를 포함한다. 이와 같이 도 4 및 도 5에서 상술한 CONNX 모델 추론 프로세스는 다음과 같이 정리될 수 있다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 5, "content": "도 6은 본 발명의 실시예에 따라 CONNX 모델로 변환하기 전 MNIST 데이터셋을 학습한 ONNX 모델의 구조를 시각 화한 것이고, 도 7은 도 6의 학습된 ONNX 모델을 CONNX 모델로 변환한 결과를 나타낸 것이다. 도 6 및 도 7을 참조하면, 도 6의 구조를 가진 ONNX 모델을 CONNX 모델로 변환했을 때, 파일 폴더 안에 1개의 CONNX 파일(model.ponnx), 1개의 텍스트 파일(0.text), 8개의 데이터 파일(0_1.data ~ 0_8.data)이 생성되어 있음을 알 수 있다. 도 8은 본 발명의 실시예에 따라 변환된 CONNX 모델의 text 파일의 출력 내용을 나타내고, 도 9는 본 발명의 실 시예에 따라 CONNX 모델을 추론한 결과를 나타낸 것이다. 도 9에 도시된 추론 결과는 MNIST 데이터셋이 학습된 CONNX 모델을 추론한 결과로서, 여기서 MNIST 데이터셋은 0~9로 이루어진 숫자 이미지 데이터셋을 말하며, 필기체 숫자 인식을 위한 데이터 셋이다. 이러한 CONNX 모델에 입력 데이터로 숫자 2에 해당되는 영상 데이터를 입력한 결과, 3번째 요소의 값이 6573.3687로 가장 높게 나왔으며, 이는 배열 인덱스 2에 해당되므로 정상적인 추론 결과가 나왔음을 알 수 있다."}
{"patent_id": "10-2022-0186285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이상의 설명은 본 발명을 예시적으로 설명한 것에 불과하며, 본 발명이 속하는 기술분야에서 통상의 지식을 가 진 자에 의해 본 발명의 기술적 사상에서 벗어나지 않는 범위에서 다양한 변형이 가능할 것이다. 따라서 본 발명의 명세서에 개시된 실시예들은 본 발명을 한정하는 것이 아니다. 본 발명의 범위는 아래의 특허 청구범위에 의해 해석되어야 하며, 그와 균등한 범위 내에 있는 모든 기술도 본 발명의 범위에 포함되는 것으로 해석해야 할 것이다."}
{"patent_id": "10-2022-0186285", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 장치의 개략적인 구성도. 도 2는 본 발명에 따른 모델 변환부에서 ONNX 모델이 CONNX 모델로 변환되었을 때 생성되는 파일을 나타낸 도면. 도 3은 본 발명에 따른 추론부의 내부 구성을 나타낸 도면. 도 4는 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 방법의 순서도. 도 5는 본 발명에 따른 FPGA 기반 NPU에 적용되는 인공지능 모델 추론 방법에서 파싱 이후의 실제 추론 과정을 나타낸 순서도. 도 6은 본 발명의 실시예에 따라 CONNX 모델로 변환하기 전 MNIST 데이터셋을 학습한 ONNX 모델의 구조를 시각 화한 도면. 도 7은 본 발명의 실시예에 따라 도 6의 학습된 ONNX 모델을 CONNX 모델로 변환한 결과를 나타낸 도면 도 8은 본 발명의 실시예에 따라 변환된 CONNX 모델의 text 파일의 내용을 출력한 도면 도 9는 본 발명의 실시예에 따라 PONNX 모델을 추론한 결과를 나타낸 도면."}
