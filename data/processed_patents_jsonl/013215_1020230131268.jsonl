{"patent_id": "10-2023-0131268", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0047032", "출원번호": "10-2023-0131268", "발명의 명칭": "음성 데이터 전처리 시스템 및 음성 데이터 전처리 서버", "출원인": "주식회사 구일이 커뮤니케이션", "발명자": "윤정원"}}
{"patent_id": "10-2023-0131268", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "수집 단말기와 데이터를 송신 및 수신하는 음성 데이터 전처리 서버를 포함하는 음성 데이터 전처리 시스템에있어서, 상기 음성 데이터 전처리 서버는,상기 수집 단말기로부터 음성 데이터를 제공 받기 위한 통신부;상기 통신부를 통해 입력된 음성 데이터를 저장하기 위한 저장부; 및상기 저장부에 저장된 음성 데이터를 입력받기 위한 음성 관리 프로그램을 상기 수집 단말기로 제공하기 위한제어부를 포함하되,상기 저장부는,식별 번호 및 음성 데이터 파일 정보을 저장하기 위한 음성 데이터 DB;음성 데이터로부터 추출한 대본 데이터를 저장하기 위한 대본 데이터 DB;음성 데이터의 특징을 나타내는 분류 데이터를 저장하기 위한 분류 데이터 DB를 포함하며,상기 제어부는, 상기 수집 단말기로부터 수집한 음성 데이터를 음성 데이터 DB에 저장하고 관리하는 음성 데이터 관리 유닛;음성 데이터로부터 생성한 대본 데이터를 대본 데이터 DB에 저장하고 관리하는 대본 데이터 관리 유닛; 및음성 데이터의 특징 분류 정보를 분류 데이터 DB에 저장하고 관리하는 분류 데이터 관리 유닛을 포함하는 음성데이터 전처리 시스템."}
{"patent_id": "10-2023-0131268", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 음성 데이터 관리 유닛은,상기 수집 단말기로부터 음성 데이터를 수집하고,수집한 음성 데이터에서 잡음을 제거하고,잡음을 제거한 음성 데이터에서 음성 사이의 공백을 조정하고,공백을 조정한 음성 데이터의 길이를 미리 정해진 길이 범위로 조정하고, 길이를 조정한 음성 데이터의 샘플링 레이트를 미리 정해진 샘플링 레이트로 조정하는 음성 데이터 전처리 시스템."}
{"patent_id": "10-2023-0131268", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서, 상기 미리 정해진 샘플링 레이트는 20㎑인 음성 데이터 전처리 시스템."}
{"patent_id": "10-2023-0131268", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 분류 데이터 관리 유닛은,음성 데이터의 특징 분류 정보로 발화자의 감정 정보, 발화자의 성별 정보, 방언에 관한 정보, 발화 스타일 정공개특허 10-2025-0047032-3-보, 발화자의 어조 정보 및 발화자의 연령대 정보를 관리하되,상기 발화자의 감정 정보는 기쁨, 슬픔, 환호, 분노, 안정, 불안, 상처, 당황 및 중립을 포함하고,상기 발화자의 성별 정보는 남성 및 여성을 포함하고,상기 방언에 관한 정보는 광역지방자치단체명을 포함하고, 상기 발화 스타일 정보는 구연체, 낭독체, 뉴스체, 대화체, 중계체 및 독백체를 포함하고, 상기 발화자의 어조 정보는 밝은 어조, 중립 어조 및 어두운 어조를 포함하고, 상기 발화자의 연령대 정보는 아동, 청소년, 중년, 장년 및 노년을 포함하는 음성 데이터 전처리 시스템."}
{"patent_id": "10-2023-0131268", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제어부를 포함하는 음성 데이터 전처리 서버에 있어서,상기 제어부는, 음성 데이터를 정규화하고, 정규화된 음성 데이터로부터 대본 데이터를 생성하도록 구성되며,상기 음성 데이터를 정규화하는 것은, 상기 음성 데이터에서 잡음을 제거하는 것, 발화자의 음성 사이의 공백을조정하는 것, 상기 음성 데이터의 길이를 미리 정해진 길이의 범위로 조정하는 것, 그리고 음성 데이터의 샘플링 레이트를 미리 정해진 샘플링 레이트로 조정하는 것을 포함하는 음성 데이터 전처리 서버."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "수집 단말기와 데이터를 송신 및 수신하는 음성 데이터 전처리 서버를 포함하는 음성 데이터 전처리 시스템에 있 어서, 음성 데이터 전처리 서버는, 수집 단말기로부터 음성 데이터를 제공 받기 위한 통신부, 통신부를 통해 입 력된 음성 데이터를 저장하기 위한 저장부, 및 저장부에 저장된 음성 데이터를 입력받기 위한 음성 관리 프로그 (뒷면에 계속)"}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음성 데이터 전처리 시스템에 관한 것으로, 더욱 인공지능 머신이 학습하고자 하는 음성 데이터의 전 처리를 수행하는 음성 데이터 전처리 서버에 관한 것이다."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(A.I, Artificial Intelligence)은 우리의 일상, 산업, 연구 등 다양한 분야에서 혁신을 주도하고 있다. 이러한 인공지능(A.I)기술의 핵심에는 데이터를 기반으로 패턴을 학습하고 예측하는 능력이 있다. 인공지 능(A.I)은 다양한 데이터로부터 복잡한 패턴을 파악하며, 이를 통해 효율적인 결정을 내릴 수 있다. 인공지능(A.I)이 효율적인 결정을 하기 위해서는 정확한 데이터를 대량으로 학습해야 한다. 데이터는 AI의 지식 과 기능을 발전시키는 원료와 같으며, 데이터가 풍부하고 정확할수록 AI의 성능도 향상된다. 음성 데이터를 학습하기 위해서는 스크립트가 필요하다. 이유는 지도 학습 방식에서 AI는 입력 데이터에 해당하 는 음성 데이터와 정답에 해당하는 스크립트 파일을 함께 제공받아서 음성의 내용과 텍스트 사이의 관계를 학습 한다. 이를 통해 새로운 음성 데이터가 주어졌을 때 해당하는 텍스트를 예측할 수 있고, 텍스트가 주어졌을 때 해당하 는 음성을 예측할 수 있다. 스크립트는 음성 데이터의 정확한 텍스트 표현으로, AI의 학습에 있어 중요한 역할 을 한다. 그러나 음성 데이터와 스크립트의 쌍을 구하는 것은 쉬운 일이 아니다. 첫 번째 방법으로는 스크립트를 읽게 하 여 녹음하는 방법이 있지만, 이 경우 음성이 대본에 지나치게 의존적이게 되어, 자연스러운 대화나 다양한 발화 패턴을 학습하기 어렵다. 또 다른 방법은 직접 음성 데이터를 듣고 타이핑하여 스크립트를 작성하는 것이다. 이 방법은 매우 정확한 데이터를 제공할 수 있지만, 작업에 많은 시간과 노력이 소요되는 문제가 있다."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 과제를 해결하기 위한 것으로서, 본 발명의 목적은 일반적인 음성 데이터에서 인공지능 학습 을 위한 음성 데이터의 처리를 수행하는 음성 데이터 전처리 시스템을 제공하는데 있다."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따르면, 수집 단말기와 데이터를 송신 및 수신하는 음성 데이터 전처리 서버를 포함하 는 음성 데이터 전처리 시스템에 있어서, 상기 음성 데이터 전처리 서버는, 상기 수집 단말기로부터 음성 데이 터를 제공 받기 위한 통신부; 상기 통신부를 통해 입력된 음성 데이터를 저장하기 위한 저장부; 및 상기 저장부 에 저장된 음성 데이터를 입력 받기 위한 음성 관리 프로그램을 상기 수집 단말기로 제공하기 위한 제어부를 포 함한다. 일 실시 예에서, 상기 저장부는, 식별 번호 및 음성 데이터 파일 정보을 저장하기 위한 음성 데이터 DB;음성 데 이터로부터 추출한 대본 데이터를 저장하기 위한 대본 데이터 DB; 음성 데이터의 특징을 나타내는 분류 데이터 를 저장하기 위한 분류 데이터 DB를 포함할 수 있다. 일 실시 예에서, 상기 제어부는, 상기 수집 단말기로부터 수집한 음성 데이터를 음성 데이터 DB에 저장하고 관 리하는 음성 데이터 관리 유닛; 음성 데이터로부터 생성한 대본 데이터를 대본 데이터 DB에 저장하고 관리하는 대본 데이터 관리 유닛; 및 음성 데이터의 특징 분류 정보를 분류 데이터 DB에 저장하고 관리하는 분류 데이터 관리 유닛을 포함할 수 있다. 본 발명의 다른 실시 예에 따르면, 음성 데이터 전처리 서버가 음성 데이터를 수집하는 음성 데이터 수집 단계; 음성 데이터 전처리 서버가 음성 데이터를 정규화하는 음성 데이터 정규화 단계; 음성 데이터 전처리 서버가 음 성 데이터로부터 대본 데이터를 생성하는 대본 데이터 생성 단계; 및 음성 데이터 전처리 서버가 음성 데이터의 특징 분류 정보에 따라 분류하는 음성 데이터 분류 단계를 포함한다. 일 실시 예에서, 상기 음성 데이터 정규화 단계는, 음성 데이터 전처리 서버가 음성 데이터에서 잡음을 제거하 는 잡음 제거 단계; 음성 데이터 전처리 서버가 발화자의 음성 사이의 공백을 조정하는 공백 조정 단계; 음성 데이터 전처리 서버가 음성 데이터의 길이를 미리 정해진 길이의 범위로 조정하는 길이 조정 단계; 및 음성 데 이터 전처리 서버가 음성 데이터의 샘플링 레이트를 미리 정해진 샘플링 레이트로 조정하는 샘플링 레이트 조정 단계를 포함할 수 있다. 일 실시예에 있어서, 제어부를 포함하고, 상기 제어부는, 음성 데이터를 정규화하고, 정규화된 음성 데이터로부 터 대본 데이터를 생성하도록 구성되며, 상기 음성 데이터를 정규화하는 것은, 상기 음성 데이터에서 잡음을 제 거하는 것, 발화자의 음성 사이의 공백을 조정하는 것, 상기 음성 데이터의 길이를 미리 정해진 길이의 범위로 조정하는 것, 그리고 음성 데이터의 샘플링 레이트를 미리 정해진 샘플링 레이트로 조정하는 것을 포함하는 음 성 데이터 전처리 서버가 제공될 수 있다."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 음성 데이터 전처리 시스템은 인공지능 학습용 음성 데이터와 그에 대응되는 대본의 쌍을 자 동으로 생성할 수 있다."}
{"patent_id": "10-2023-0131268", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 본 발명의 기술 분야에서 통상의 지식을 가진 자가 본 발명을 용이하게 실시할 수 있을 정도로, 본 발명의 실시 예들이 명확하고 상세하게 기재될 것이다.도 1은 본 발명의 실시 예에 따른 음성 데이터 전처리 시스템을 예시적으로 보여주는 구성도이다. 도 1을 참조하면, 음성 데이터 전처리 시스템은 수집 단말기들(1100, 1101, 1200, 1201)과 음성 데이터 전처리 서버를 포함할 수 있다. 사용자는 여러 수집 단말기들(1100, 1101, 1200, 1201)을 이용하여 자신 의 음성을 녹음하여 음성 데이터를 생성할 수 있다. 사용자는 여러 수집 단말기들(1100, 1101, 1200, 1201)을 이용하여 음성 데이터 전처리 서버에 접속할 수 있다. 수집 단말기들(1100, 1101, 1200, 1201)은 스마트 폰, 태블릿 PC, 랩탑 PC, 데스크탑 PC 등과 같은 전자 장치일 수 있다. 수집 단말기들(1100, 1101, 1200, 1201)과 음성 데이터 전처리 서버는 유선 또는 무선으로 통신할 수 있다. 사용자는 제1 또는 제2 수집 단 말기(1100, 1200)를 이용하여 음성 데이터 전처리 서버에 접속함으로, 음성 데이터 전처리 서버에 텍스트 데이터 또는 음성 데이터를 제공할 수 있다. 예를 들어, 텍스트 데이터는 음성 데이터에 대응하는 대본 데이터일 수 있다. 음성 데이터 전처리 서버는 음성 데이터 전처리 서버에 제공된 음성 데이터를 인공지능이 학습하기에 적합한 음성데이터로 전처리 할 수 있다. 도 2는 도 1에 도시된 음성 데이터 전처리 시스템을 예시적으로 설명하기 위한 블록도이다. 도 2를 참조하면, 음성 데이터 전처리 시스템은 제1 수집 단말기, 제2 수집 단말기, 그리고 음성 데이터 전처리 서버를 포함한다. 제1 수집 단말기는 스마트 폰이고, 제2 수집 단말기는 데스크탑 PC일 수 있다. 예로서, 제1 수집 단말기는 앱(APP) 기반 단말기이고, 제2 수집 단말기는 웹(WEB) 기반 단말기일 수 있다. 일 실시예에서, 제1 및 제2 수집 단말기들(1100, 1200) 중 적어도 하나는 앱 기반이나 웹 기반 음성 관리 프로그램이 모두 설치되고 구동할 수 있는 단말기일 수 있다. 제1 및 제2 수집 단말기들(1100, 1200)은 통신망을 통해 서로 연결될 수 있다. 제1 및 제2 수집 단말기들(1100, 1200)의 각각은 통신망을 통해 음성 데이터 전처리 서버와 연결될 수 있다. 제1 및 제2 수집 단말기들 (1100, 1200)은 음성 데이터 전처리 서버에 음성 데이터를 전송하거나 음성 데이터 전처리 서버로 부터 음성 데이터를 수신할 수 있다. 제1 및 제2 수집 단말기들(1100, 1200)은 음성 데이터 관리 애플리케이션 을 통해 사용자가 제공하는 정보를 입력 받고, 사용자가 필요로 하는 음성 데이터를 제공할 수 있다. 제1 및 제2 수집 단말기들(1100, 1200)은 화면부(1110, 1210)와 구동부(1120, 1220)를 포함할 수 있다. 제1 수 집 단말기의 구동부는 음성 데이터 관리 애플리케이션을 실행할 수 있다. 제2 수집 단말기의 구동부는 웹 서비스(예를 들어, 홈페이지)를 이용하여 화면 상에서 음성 데이터를 제공할 수 있다. 구동 부(1120, 1220)는 음성 데이터 관리 애플리케이션이나 웹 서비스를 구동하기 위한 하드웨어 또는 소프트웨어를 포함할 수 있다. 구동부(1120, 1220)의 내부 구성 및 동작 원리는 이하에서 더 상세하게 설명될 것이다. 음성 데이터 전처리 서버는 통신부, 저장부, 그리고 제어부를 포함할 수 있다. 음성 데이터 전처리 서버는 제1 및 제2 수집 단말기들(1100, 1200)로부터 입력된 데이터를 저장하고, 저장된 데이터를 제1 및 제2 수집 단말기들(1100, 1200)로 전송할 수 있다. 통신부는 제1 및 제2 수집 단말기들(1100, 1200)로부터 음성 데이터를 수신할 수 있다. 통신부는 원거리용 네트워크 인터페이스 및 근거리용 네트워크 인터페이스 중 적어도 하나를 포함할 수 있다. 예를 들어, 원거리용 네트워크 인터페이스는 3G 모듈, LTE 모듈, LTE-A 모듈, Wi-Fi 모듈, 와이기그(WiGig) 모듈, UWB(Ultra Wide Band) 모듈, 및 랜카드 중 적어도 하나를 포함할 수 있다. 예를 들어, 근거리용 네트워크 인터 페이스는 마그네틱 보안 전송(MST, Magnetic Secure Transmission) 모듈, 블루투스 모듈, NFC 모듈, RFID 모듈, 지그비(ZigBee) 모듈, Z-Wave 모듈, 및 적외선 모듈 중 적어도 하나를 포함할 수 있다. 통신부는 제어부의 제어에 따라, 음성 데이터를 저장부에 제공할 수 있다. 저장부는 음성 데이터, 대본 데이터, 및 분류 데이터를 저장할 수 있다. 음성 데이터, 대본 데이터, 및 분류 데이터는 상세히 후술된다. 일 실시예에서, 저장부는 전원이 차단되어도 데이터를 잃지 않는 비휘발 성 메모리를 포함할 수 있다. 저장부는, 예를 들어, 플래시(flash) 메모리, ROM, RAM, EEROM, EPROM, EEPROM, HDD, 및 SSD 중 적어도 하나를 포함할 수 있다. 저장부는, 예를 들어, 파일 시스템, 데이터베이 스, 또는 임베디드 데이터베이스 등을 포함할 수 있다. 제어부는 음성 데이터 전처리 서버에 포함된 각각의 구성요소 및 음성 데이터 전처리 서버의 동작을 제어할 수 있다. 제어부는 사용자에게 음성 데이터를 제공하는 동작을 수행할 수 있다. 제어부 는 제1 및 제2 수집 단말기들(1100, 1200)로부터 수신된 음성 데이터를 이용하여 음성 데이터 전처리 서버의 동작을 제어할 수 있다. 제어부는 음성 데이터, 대본 데이터, 및 분류 데이터를 관리하고 업 데이트 하는 동작을 수행할 수 있다. 예를 들어, 제어부는 중앙 처리 유닛(Central Processing Unit, CPU), 그래픽 처리 유닛(Graphic Processing Unit, GPU), 마이크로 컨트롤러 유닛(Micro Controller Unit, MCU), 및 마이크로프로세서(microprocessor) 중 적어도 하나를 포함할 수 있다. 도 3은 도 2에서 도시된 음성 데이터 전처리 서버를 예시적으로 보여주는 블록도이다. 도 4는 도 3에 도시된 음 성 데이터 DB를 예시적으로 보여주는 다이어그램이다. 도 5는 도 3에 도시된 분류 데이터 DB를 예시적으로 보여 주는 다이어그램이다. 도 3을 참조하면, 음성 데이터 전처리 서버는 통신부, 저장부, 그리고 제어부를 포함 한다. 음성 데이터 전처리 서버는 제1 및 제2 수집 단말기들(1100, 1200)로부터 음성 데이터를 수신할 수 있다. 음성 데이터 전처리 서버는 수신된 음성 데이터를 자동으로 데이터베이스화 하거나, 정보 요청에 응답하여 자동으로 음성 데이터 제공에 관한 기본 자료를 산출하거나, 정보 요청한 사용자에게 음성 데이터를 제공할 수 있다. 통신부는 제1 또는 제2 수집 단말기(1100, 1200)와 통신을 수행할 수 있다. 통신부는 CDMA(code division multiple access), GSM(global system for mobile communication), WCDMA(wideband CDMA), CDMA- 2000, TDMA(time division multiple access), LTE(long term evolution), Wimax(worldwide interoperability for microwave access), WLAN(wireless LAN), UWB(ultrawide band), 블루투스(bluetooth), WI-DI(wireless display), 인터넷, LAN 등과 같은 유무선 통신을 지원할 수 있다. 통신부는 입력부와 출력부를 포함할 수 있다. 입력부는 제1 또는 제2 수집 단말기 (1100, 1200)로부터 음성 데이터 및 분류 데이터를 수신할 수 있다. 입력부는 수신된 음성 데이터 및 분 류 데이터를 저장부 또는 제어부로 전송할 수 있다. 음성 데이터 전처리 서버가 제1 또는 제 2 수집 단말기(1100, 1200)로부터 음성 데이터 요청을 받은 경우에, 출력부는 저장부에 저장된 음 성 데이터 또는 제어부가 생성한 음성 데이터를 사용자에게 제공할 수 있다. 저장부는 입력부에서 수신하는 음성 데이터를 저장할 수 있다. 저장부는 음성 데이터 DB, 대본 데이터 DB 그리고 분류 데이터 DB를 포함할 수 있다. 도 4를 참조하면, 음성 데이터 DB에 식별 번호 및 음성 데이터 정보 파일 정보가 저장될 수 있다. 식별 번호 및 음성 데이터 정보 파일 정보는 음성 데이터의 관리를 위해 필요한 정보이다. 음성 데이터 파일 정보는 파일의 명칭, 파일의 길이, 파일의 용량 또는 파일의 샘플링 레이트 등이 포함될 수 있다. 대본 데이터 DB에는 음성 데이터에서 추출한 대본 데이터가 저장될 수 있다. 예를 들어, 대본 데이터는 음성이 담고 있는 정보를 변환한 텍스트, 대본에서 변환된 텍스트가 나타난 타이밍, 및 사용된 언어의 종류가 저장될 수 있다. 도 5를 참조하면, 분류 데이터 DB에는 음성 데이터의 특징을 나타내는 분류 데이터가 저장될 수 있다. 분 류 데이터는 음성 데이터를 통해 알 수 있는 분류 가능한 정보를 포함할 수 있다. 예를 들어, 분류 데이터는 음 성 데이터에 나타난 발화자의 감정 정보, 발화자의 성별 정보, 방언에 대한 정보, 발화 스타 일 정보, 발화자의 어조 정보 및 발화자의 연령대 정보 등에 관한 정보들을 포함할 수 있다. 일 예에서, 발화자의 감정 정보는 기쁨, 슬픔, 환호, 분노, 안정, 불안, 상처, 당황 및 중립 등을 포함할 수 있다. 발화자의 성별 정보는 남성 및 여성을 포함할 수 있다. 방언에 대한 정보는 사투리에 대 한 정보로 서울, 부산, 제주특별자치도 등과 같은 광역지방자치단체명을 포함할 수 있다. 발화 스타일 정보(는 구연체, 낭독체, 뉴스체, 대화체, 중계체 및 독백체 등을 포함할 수 있다. 발화자의 어조 정보는 밝은 어 조, 중립 어조, 어두운 어조 등을 포함할 수 있다. 발화자의 연령대 정보는 아동, 청소년, 중년, 장년 및 노년 등을 포함할 수 있다. 제어부는 프로세서, 애플리케이션 관리 유닛, 음성 데이터 관리 유닛, 대본 데이터 관 리 유닛, 그리고 분류 정보 관리 유닛을 포함할 수 있다. 제어부는 하드웨어나 소프트웨어로 구현될 수 있다. 예를 들면, 프로세서는 하드웨어로 구현되고, 애플리케이션 관리 유닛, 음성 데이 터 관리 유닛, 대본 데이터 관리 유닛, 그리고 분류 정보 관리 유닛은 알고리즘이나 소프트웨어로 구현될 수 있다. 프로세서는 음성 데이터 전처리 서버의 전반적인 동작을 제어할 수 있다. 예를 들어, 프로세서 는 저장부의 음성 데이터 DB, 대본 데이터 DB 그리고 분류 데이터 DB에 접근 (access)하고, 음성 데이터 관리 유닛, 대본 데이터 관리 유닛 그리고 분류 정보 관리 유닛 을 구성하는 알고리즘이나 프로그램 명령을 실행함으로, 제어부를 구동할 수 있다. 프로세서는 음 성 데이터 전처리 서버의 여러 구성 요소들을 제어하는 컨트롤러들, 인터페이스들, 그래픽 엔진 등을 포 함할 수 있다. 프로세서는 SoC(system-on-chip), ASIC(application specific integrated circuit), FPGA(field programmable gate array) 등의 형태로 제공될 수 있다. 애플리케이션 관리 유닛은 제1 수집 단말기에 음성데이터 관리 애플리케이션을 제공하거나 음성 데 이터 서비스 프로그램을 설치하도록 지원할 수 있다. 애플리케이션 관리 유닛은 제2 수집 단말기를 통해 음성 데이터 서비스 프로그램을 실행하도록 할 수 있다. 애플리케이션 관리 유닛은 제1 및 제2 수집 단말기들(1100, 1200)을 통해 음성 데이터 전처리 서버로부터 음성 데이터를 제공받거나 음성 데이터를 제공하도록 할 수 있다. 음성 데이터 관리 유닛은 저장부의 음성 데이터 DB에 접근하여 음성 데이터를 저장하고 관리 할 수 있다. 음성 데이터는 학습 데이터 생성을 위해 별도로 녹음될 파일일 수 있고, 발표 음성의 녹음 파일, 일반적인 대화의 녹음 파일, 통화 녹음 파일 등일 수 있다. 음성 데이터 관리 유닛은 제1 수집 단말기를 통해 음성 데이터를 수집하는 경우에, 음성 데이터 파 일에 식별번호를 부여하고, 음성 데이터 DB에 식별번호 및 음성 데이터 관리 정보를 저장할 수 있다. 일 예에서, 음성 데이터 관리 정보는 파일의 명칭, 파일의 길이, 파일의 용량 또는 파 일의 샘플링 레이트를 저장할 수 있다. 음성 데이터 관리 유닛은 음성 데이터가 변경되는 경우에, 음성 데이터 DB에 저장된 음성 데이터를 업데이트할 수 있다. 음성 데이터 관리 유닛은 음성 데이터를 정규화 할 수 있다. 음성 데이터의 정규화는 음성 데이터를 일정 한 규격으로 정돈하는 것을 의미한다. 음성 데이터 관리 유닛은 다양한 장치에 의해 녹음된 음성 데이터 를 수집할 수 있다. 이에 따라, 일정한 규격으로 조정하는 정규화 과정이 필요하다. 음성 데이터 관리 유닛은 음성 데이터에서 잡음을 제거할 수 있다. 잡음은 발화자의 음성을 제외한 모든 소리를 포함할 수 있다. 예를 들면, 백색 소음(white noise), 배경 잡음, 숨소리 및 가청 주파수 범위를 벗어나 는 소리를 포함할 수 있다. 음성 데이터는 다양한 환경에서 녹음될 수 있으므로 다양한 종류의 잡음이 섞일 수 있고, 음성 인식에 방해가 되기 때문에 제거되어야 한다. 음성 데이터 관리 유닛은 인공지능(AI) 학습에 사용되는 발화자의 음성 외의 소리를 제거할 수 있다. 음성 데이터 관리 유닛은 발화자의 음성 사이의 공백을 조정할 수 있다. 음성 데이터 관리 유닛은 음성 사이의 공백을 최소 공백 시간 내지 최대 공백 시간 범위로 조정할 수 있다. 인공지능 머신에 의해 학습이 되는 정보는 발화자의 음성이므로, 음성 데이터 관리 유닛은 학습 효율을 향상시키기 위해 음성 사이의 공백을 임의의 시간으로 조정할 수 있다. 다만, 음성 사이의 공백도 발화자의 스타일이므로 동일한 값이 아닌 최소 공백 시간 내지 최대 공백 시간 범위내의 값으로 조정할 수 있다. 일 예에서, 최소 공백 시간은 0.2초, 0.3초와 같이 지정된 값으로 결정하거나 음성 데이터에서 발견되는 공백 시간 중에서 가장 짧은 값으로 결정할 수 있다. 최대 공백 시간은 0.8초, 1초와 같이 지정된 값으로 결정하거나, 최소 공백 시간에서 2배와 같은 비율 로 결정할 수 있다. 또는 최대 공백 시간을 최소 공백 시간의 2배로 하되, 1초 이하로 결정하는 것처럼 혼합된 형태로 결정할 수 있다. 예를 들면, 음성 데이터 전처리 서버는 최소 공백 시간이 0.6초라면, 최대 공백 시간은 1.2초이므로 1초로 결정할 수 있다. 음성 데이터 관리 유닛은 음성 데이터의 길이를 임의의 길이로 조정할 수 있다. 예를 들어, 음성 데이터 의 임의의 길이는 3초 내지 15초 정도의 길이일 수 있다. 인공지능(AI)이 음성 데이터에서 특징(feature)을 뽑 아내기 위해서는 문장 단위의 음성 데이터가 필요하고, 하나의 문장에 한명의 발화자만 포함되어야 한다. 대화 에서 하나의 문장이 발화되는 시간은 3초 정도이므로 음성 데이터의 최소 길이는 3초 이상일 수 있다. 음성 데 이터의 용량을 고려하여 음성 데이터의 최대 길이는 15초 이하일 수 있다. 이에 따라, 음성 데이터 전처리 서버 는 음성 데이터에서 발화자가 여러 명이면 한 명에 대한 음성만 남길 수 있다. 음성 데이터 관리 유닛 은 발화자가 한 명인 남은 음성 데이터의 길이가 3초 미만이면 음성 데이터를 제거할 수 있다. 음성 데이터 관리 유닛은 발화자가 한 명인 음성 데이터의 길이가 15초를 초과하면 초기 발화 또는 후기 발화의 데 이터를 제거하여 15초 이하의 길이로 조정할 수 있다. 음성 데이터 관리 유닛은 음성 데이터의 샘플링 레이트를 임의의 샘플링 레이트로 조정할 수 있다 여기에 서, 임의의 샘플링 레이트는 20㎑로 결정될 수 있다. 최대 샘플링 레이트는 고음질인 음악 CD를 기준으로 약 44.1㎑이다. 음성 인식을 위한 용도의 샘플링 레이트로 16㎑ 또는 8㎑가 일반적으로 사용된다. 샘플링 레이트 가 높을수록 음성 데이터의 용량이 커질 수 있다. 음성 데이터는 음성인식이 목적이기 때문에 최대 샘플링 레이 트보다 작은 샘플링 레이트 값을 가질 수 있다. 일 실시예에서, 음성 데이터 관리 유닛은 모든 음성 데이 터의 샘플링 레이트를 20㎑로 정규화 할 수 있다. 대본 데이터 관리 유닛은 저장부의 대본 데이터 DB에 접근하여 대본 데이터를 관리할 수 있 다. 대본 데이터 관리 유닛은 제1 또는 제2 수집 단말기(1100, 1200)을 통해 음성 데이터가 입력되는 경 우에, 음성 데이터로부터 대본 데이터를 생성할 수 있다. 대본 데이터 DB에 음성이 담고 있는 정보를 변 환한 텍스트, 대본에서 변환된 텍스트가 나타난 타이밍 및 사용된 언어의 종류 등을 저장할 수 있다. 대본 데이 터 관리 유닛은 기존 음성 데이터가 변경되는 경우에, 대본 데이터 DB에 저장된 대본 데이터를 업 데이트할 수 있다. 대본 데이터 관리 유닛은 음성 데이터로부터 대본 데이터를 생성할 수 있다. 대본 데이터 관리 유닛 은 생성된 대본 데이터를 음성 데이터와 매치하여 관리하고, 저장할 수 있다. 대본 데이터 관리 유닛 은 전처리된 음성 데이터를 읽어들여서 발화자의 말을 텍스트로 변환할 수 있다. 대본 데이터 관리 유닛 은 음성 데이터에서 발화자가 말하는 타이밍을 기록할 수 있다. 일 예에서, 대본 데이터 관리 유닛(222 0)은 \"음성 데이터의 샘플링 레이트는 이십키로헤르쯔입니다.\"라는 음성 데이터를 읽어들이면, \"<start=0000>음 성 데이터의 샘플링 레이트는 이십키로헤르쯔입니다.<end=1150>\", \"<start=0000>음성 data의 sampling rate는 20kHz입니다.<end=1150>\" 또는 \"<start=0000>음성데이터의 <start=0400>샘플링레이트는 <start=0820>이십키로 헤르쯔입니다.<end=1150>\" 등과 같은 형태의 텍스트로 변환될 수 있다.대본 데이터 관리 유닛은 변환된 텍스트 파일을 저장할 수 있다. 분류 정보 관리 유닛은 음성 데이터의 특징 분류 정보을 분류하고 관리할 수 있다. 예를 들면, 분류 정보 관리 유닛은 음성 데이터에 나타난 발화자의 감정 정보, 발화자의 성별 정보, 방언에 관한 정보, 발화 스타일 정보, 발화자의 어조 정보 및 발화자의 연령대 정보와 같은 정보를 지정할 수 있다. 예를 들면, 발화자가 부산 출신의 20대 남성이 무덤덤한 어조로 인사하는 음성 데이터인 경우 에, 분류 정보 관리 유닛은 발화자의 감정 정보는 중립, 발화자의 성별 정보는 남성, 방언에 대한 정보는 경상남도, 발화 스타일 정보는 대화체, 발화자의 어조 정보는 중립 어조, 발화 자의 연령대 정보는 청소년으로 분류할 수 있다. 분류 정보 관리 유닛은 저장부의 분류 데이 터 DB에 접근하여 상술한 음성 데이터의 특징 분류 정보을 저장하고 관리할 수 있다. 분류 데이터 관리 유닛은 음성 데이터를 분류할 수 있다. 분류 데이터 관리 유닛은 음성 데이터의 특징 분류 정보에 따라 분류할 수 있다. 분류 데이터 관리 유닛은 분류 정보를 음성 데이터와 매치하여 저장하고 관리할 수 있다. 여기에서 음성 데이터의 특징 분류 정보은 음성 데이터에 나타난 발화자의 감정(기쁨, 슬픔, 환호, 분노, 안정, 불안, 상처, 당황 및 중립), 발화자의 성별, 방언에 대한 정보, 발화 스타 일(구연체, 낭독체, 뉴스체, 대화체, 중계체 및 독백체 등), 발화자의 어조(밝은 어조, 중립 어조 및 어두운 어 조) 및 발화자의 연령대(아동, 청소년, 중년, 장년 및 노년) 등을 포함할 수 있다. 일 예에서, 최초의 음성 데 이터에는 발화자가 50대 서울 여성으로 자기가 진행할 프로젝트 결과를 보고하는 경우에, 분류 데이터 관리 유 닛은 발화자의 감정으로 안정, 발화자의 성별은 여성, 방언에 대한 정보는 서울, 발화 스타일은 중계체, 발화자의 어조는 밝은 어조 발화자의 연령대는 장년으로 분류할 수 있다. 분류 데이터 관리 유닛은 음성 파일에 대해 안정, 여성, 서울, 중계체, 밝은, 장년과 같이 태그 정보의 형태로 저장할 수 있다. 도 6은 도 3에 도시된 음성 데이터 전처리 서버의 음성 데이터 전처리 방법을 예시적으로 설명하기 위한 순서도 이고, 도 7은 도 6에 도시된 음성 데이터 정규화 단계를 구체적으로 설명하기 위한 순서도이다. 도 6을 참조하면, 음성 데이터 전처리 서버는 음성 데이터의 전처리를 수행할 수 있다. S100단계에서, 음성 데이터 전처리 서버는 음성 데이터를 수집할 수 있다. 음성 데이터 전처리 서버 는 수집 단말기들(1100, 1101, 1200, 1201)로부터 인공지능이 학습하기에 적절한 시간 길이의 음성 데이 터를 수집할 수 있다. 여기에서, 음성 데이터는 학습 데이터 생성을 위해 의도적으로 별도로 녹음될 파일일 수있고, 발표 음성의 녹음 파일, 일반적인 대화의 녹음 파일, 통화 녹음 파일 등일 수 있다. S200단계에서, 음성 데이터 전처리 서버는 음성 데이터를 정규화 할 수 있다. 여기에서, 음성 데이터의 정규화는 음성 데이터를 일정한 규격으로 정돈하는 것을 의미한다. 음성 데이터 전처리 서버는 다양한 장 치에 의해 녹음된 음성 데이터를 수집할 수 있다. 이에 따라, 일정한 규격으로 조정하는 정규화 과정이 필요하 다. 도 7을 참조하면, 음성 데이터 전처리 서버는 잡음 제거 단계(S310), 공백 조정 단계(S320), 길이 조정 단계(S330) 및 샘플링 레이트 조정 단계(S340)를 수행하여 음성 데이터를 정규화할 수 있다. 음성 데이터의 잡음 제거 단계(S310)에서 음성 데이터 전처리 서버는 음성 데이터에서 잡음을 제거할 수 있다. 여기에서 잡음은 발화자의 음성을 제외한 모든 소리를 포함할 수 있다. 예를 들면, 백색 소음(white noise), 배경 잡음, 숨소리 및 가청 주파수 범위를 벗어나는 소리를 포함할 수 있다. 음성 데이터는 다양한 환 경에서 녹음될 수 있으므로 다양한 종류의 잡음이 섞일 수 있고, 음성 인식에 방해가 되기 때문에 제거해야 한 다. 음성 데이터 전처리 서버는 인공지능(AI) 학습에 사용되는 발화자의 음성을 제외한 모든 소리를 제거 할 수 있다. 공백 조정 단계(S320)에서 음성 데이터 전처리 서버는 발화자의 음성 사이의 공백을 조정할 수 있다. 음 성 데이터 전처리 서버는 음성 사이의 공백을 최소 공백 시간 내지 최대 공백 시간 범위로 조정할 수 있 다. 인공지능 머신에 의해 학습이 되는 정보는 발화자의 음성이므로, 음성 데이터 전처리 서버는 학습 효 율을 향상시키기 위해 음성 사이의 공백을 임의의 시간으로 조정할 수 있다. 다만, 음성 사이의 공백도 발화자 의 스타일이므로 동일한 값이 아닌 최소 공백 시간 내지 최대 공백 시간 범위내의 값으로 조정할 수 있다. 일 예에서, 최소 공백 시간은 0.2초, 0.3초와 같이 지정된 값으로 결정하거나 음성 데이터에서 발견되는 공백 시간 중에서 가장 짧은 값으로 결정할 수 있다. 최대 공백 시간은 0.8초, 1초와 같이 지정된 값으로 결정하거나, 최 소 공백 시간에서 2배와 같은 비율로 결정할 수 있다. 또는 최대 공백 시간을 최소 공백 시간의 2배로 하되, 1 초 이하로 결정하는 것처럼 혼합된 형태로 결정할 수 있다. 예를 들면, 음성 데이터 전처리 서버는 최소 공백 시간이 0.6초라면, 최대 공백 시간은 1.2초이므로 1초로 결정할 수 있다. 길이 조정 단계(S330)에서 음성 데이터 전처리 서버는 음성 데이터의 길이를 임의의 길이로 조정할 수 있 다. 여기에서, 음성 데이터의 임의의 길이는 3초 내지 15초 정도의 길이일 수 있다. 인공지능(AI)이 음성 데이 터에서 특징(feature)을 뽑아내기 위해서는 문장 단위의 음성 데이터가 필요하고, 하나의 문장에는 한명의 발화 자만 포함되어야 한다. 대화에서 하나의 문장이 발화되는 시간은 3초 정도이므로 음성 데이터의 최소 길이는 3 초 이상일 수 있다. 또한, 음성 데이터의 용량을 고려하여 음성 데이터의 최대 길이는 15초 이하일 수 있다. 이 에 따라, 음성 데이터 전처리 서버는 음성 데이터에서 발화자가 여러 명이면 한 명에 대한 음성만 남길 수 있다. 음성 데이터 전처리 서버는 발화자가 한 명인 남은 음성 데이터의 길이가 3초 미만이면 음성 데 이터를 제거할 수 있다. 음성 데이터 전처리 서버는 발화자가 한 명인 음성 데이터의 길이가 15초를 초과 하면 맨 앞 또는 맨 뒤의 데이터를 제거하여 15초 이하의 길이로 조정할 수 있다. 샘플링 레이트 조정 단계(S340)에서 음성 데이터 전처리 서버는 음성 데이터의 샘플링 레이트를 임의의 샘플링 레이트로 조정할 수 있다 여기에서, 임의의 샘플링 레이트는 20㎑로 결정될 수 있다. 최대 샘플링 레이 트는 고음질인 음악 CD를 기준으로 약 44.1㎑이지만, 음성 인식을 위한 용도로는 16㎑ 또는 8㎑가 일반적으로 사용된다. 샘플링 레이트가 높을수록 음성 데이터의 용량이 커지고, 학습 데이터는 음성인식이 목적이기 때문에 최대 샘플링 레이트보다 작은 값을 가질 수 있다. 이에 따라, 음성 데이터 전처리 서버는 모든 음성 데이 터의 샘플링 레이트를 20㎑로 정규화 할 수 있다. S300단계에서, 음성 데이터 전처리 서버는 음성 데이터로부터 대본 데이터를 생성할 수 있다. 음성 데이 터 전처리 서버는 생성된 대본 데이터를 음성 데이터와 매치하여 관리하고, 저장할 수 있다. 음성 데이터 전처리 서버는 전처리된 음성 데이터를 읽어들여서 발화자의 말을 텍스트로 변환할 수 있다. 음성 데이터 전처리 서버는 음성 데이터에서 발화자가 말하는 타이밍을 기록할 수 있다. 일 예에서, 음성 데이터 전처 리 서버는 \"음성 데이터의 샘플링 레이트는 이십키로헤르쯔입니다.\"라는 음성 데이터를 읽어들이면, \"<start=0000>음성 데이터의 샘플링 레이트는 이십키로헤르쯔입니다.<end=1150>\", \"<start=0000>음성 data의 sampling rate는 20kHz입니다.<end=1150>\" 또는 \"<start=0000> 음성데이터의 <start=0400> 샘플링레이트는 <start=0820> 이십키로헤르쯔입니다. <end=1150>\" 등과 같은 형태의 텍스트로 변환될 수 있다. 음성 데이터 전 처리 서버는 변환된 텍스트 파일을 저장할 수 있다.S400단계에서, 음성 데이터 전처리 서버는 음성 데이터를 분류할 수 있다. 음성 데이터 전처리 서버 는 음성 데이터의 특징 분류 정보에 따라 분류할 수 있다. 음성 데이터 전처리 서버는 분류 정보를 음성 데이터와 매치하여 저장하고 관리할 수 있다. 여기에서 음성 데이터의 특징 분류 정보은 음성 데이터에 나 타난 발화자의 감정(기쁨, 슬픔, 환호, 분노, 안정, 불안, 상처, 당황 및 중립), 발화자의 성별, 방언에 대한 정보, 발화 스타일(구연체, 낭독체, 뉴스체, 대화체, 중계체 및 독백체 등), 발화자의 어조(밝은 어조, 중립 어 조 및 어두운 어조) 및 발화자의 연령대(아동, 청소년, 중년, 장년 및 노년) 등을 포함할 수 있다. 일 예에서, 최초의 음성 데이터에는 발화자가 50대 서울 여성으로 자기가 진행할 프로젝트 결과를 보고하는 경우에, 음성 데이터 전처리 서버는 발화자의 감정으로 안정, 발화자의 성별은 여성, 방언에 대한 정보는 서울, 발화 스타일은 중계체, 발화자의 어조는 밝은 어조 발화자의 연령대는 장년으로 분류할 수 있다. 음성 데이터 전처리 서버는 음성 파일에 대해 안정, 여성, 서울, 중계체, 밝은, 장년과 같이 태그 정보의 형태로 저장할 수 있다. 음성 데이터 전처리 서버는 음성 데이터의 요청이 있으면, 분류 데이터에서 적절한 음성 데이터를 검색하 여 음성 데이터를 제공할 수 있다. 일 예에서, 음성 데이터 전처리 서버는 10대 서울 남학생이 기뻐하는 목소리를 요청받은 경우, 기쁨, 남성, 서울, 대화체, 밝은, 아동으로 검색하여 검색된 음성 데이터를 제공할 수 있다. 상술한 내용은 본 발명을 실시하기 위한 구체적인 실시 예들이다. 본 발명은 상술한 실시 예들 이외에도, 단순 하게 설계 변경되거나 용이하게 변경할 수 있는 실시 예들도 포함될 것이다. 또한, 본 발명은 실시 예들을 이용 하여 용이하게 변형하여 실시할 수 있는 기술들도 포함될 것이다. 따라서, 본 발명의 범위는 상술한 실시 예들 에 국한되어 정해져서는 안 되며, 후술하는 특허청구범위뿐만 아니라 이 발명의 특허청구범위와 균등한 것들에 의해 정해져야 할 것이다."}
{"patent_id": "10-2023-0131268", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 음성 데이터 전처리 시스템을 예시적으로 보여주는 구성도이다. 도 2는 도 1에 도시된 음성 데이터 전처리 시스템을 예시적으로 설명하기 위한 블록도이다. 도 3은 도 2에서 도시된 음성 데이터 전처리 서버를 예시적으로 보여주는 블록도이다. 도 4는 도 3에 도시된 음성 데이터 DB를 예시적으로 보여주는 다이어그램이다. 도 5는 도 3에 도시된 분류 데이터 DB를 예시적으로 보여주는 다이어그램이다. 도 6은 도 3에 도시된 음성 데이터 전처리 서버의 음성 데이터 전처리 방법을 예시적으로 설명하기 위한 순서도 이다. 도 7은 도 6에 도시된 음성 데이터 정규화 단계를 구체적으로 설명하기 위한 순서도이다."}
