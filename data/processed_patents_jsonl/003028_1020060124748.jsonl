{"patent_id": "10-2006-0124748", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2008-0052940", "출원번호": "10-2006-0124748", "출원인": "한국전자통신연구원", "발명자": "박기영"}}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "캐릭터가 투입된 게임 상황을 분석하는 단계; 및상기 분석 결과에 따라, 상기 캐릭터의 행동을 제어하는 단계를 포함하여 이루어지는 것을 특징으로 하는 게임캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 분석 단계와 제어 단계는 하나의 알고리즘 내에서 수행되는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 분석 단계와 제어 단계는 다층 퍼센트론으로 수행되는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서, 상기 다층 퍼셉트론은,입력층에서 현재 모드에서 캐릭터의 위치를 입력하고,은닉층에서 상기 입력층에 입력된 캐릭터의 위치로부터, 다음 모드에서 캐릭터의 위치를 계산하며,출력층에서 상기 계산된 캐릭터의 위치를 출력하는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서, 상기 은닉층은,오차 역전파법에 의하여 학습된 패턴에 따라, 상기 다음 모드에서 캐릭터의 위치를 계산하는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항에 있어서, 상기 다층 퍼셉트론 방법은,교사학습 방법에 의하여 입력값에 따른 출력값을 미리 학습한 후, 게임 캐릭터의 행동을 제어하는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서, 상기 교사학습 방법은,평균 자승 오차를 최소화하기 위하여 출력층의 가중치를 정하는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 출력층의 가중치는 아래와 같이 주어지는 수학식에 의하여 구하여지는 것을 특징으로 하는 게임 캐릭터 제어방법.(여기서, ωｉｊ는 출력층의 가중치이고, η은 학습률이고, δℓｊ는 각 은닉층 뉴런값에 대한 출력층 오차의 미분치이다.)- 3 -공개특허 10-2008-0052940청구항 9 제 8 항에 있어서,상기 각각의 은닉층 뉴런값에 대한 출력층 오차의 미분치는 아래와 같이 구하여지는 것을 특징으로 하는 게임캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 4 항에 있어서, 상기 은닉층은,상기 입력층과 상기 출력층의 임의의 지점을 연결하는 적어도 하나의 노드를 포함하여 이루어지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 4 항에 있어서,상기 입력층과 은닉층 및 출력층은 뉴런을 통하여 서로 연결되고,상기 뉴런을 연결하는 시냅스 가중치들은 오차 역전파법에 의하여 각각의 층별로 구하여지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 4 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 11 항에 있어서, 상기 시냅스 가중치들을 형성하는 단계는,게임 모드에 따른 캐릭터의 위치가 입력된 데이터 베이스를 준비하는 단계;출력 노드의 목표값을 설정하는 단계; 및상기 목표값과 출력값이 일치하도록 상기 시냅스 가중치들을 구하는 단계를 포함하여 이루어지는 것을 특징으로하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 11 항에 있어서,상기 현재 모드 내에서 개릭터의 위치가 입력되면, 상기 시냅스 가중치들에 따라서 입력값과 출력값을 구하여다음 모드 내에서 캐릭터의 위치를 구하는 것을 특징으로하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 캐릭터의 위치를 구하는 단계는,목표값을 정하고, 상기 입력값과의 차가 최소가 되는 목표값을 출력값으로 하여, 상기 게임 캐릭터의 위치를 구하는 것을 특징으로 하는 게임 캐릭터 제어방법.- 4 -공개특허 10-2008-0052940청구항 16 제 15 항에 있어서,상기 캐릭터의 위치는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법.(여기서, x는 입력 노드 값의 벡터이고, E는 출력층에서 계산된 오차값, 그리고 는 가중치를 학습하는 정도를 나타낸 상수값이다.)"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 16 항에 있어서,상기 입력 노드 값의 벡터는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 17 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법.xnew = x old + Δx"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 17 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 17 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법.W = F × d(여기서, W는 캐릭터가 한 일을 나타내고, F는 캐릭터에 가해진 힘을 나타내고, d는 캐릭터의 이동거리를 나타낸다.)"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제 17 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는, 상기 현재 모드 내에서 아래와 같이 표현되는 상기 캐릭터의 가속도로부터 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법.a=F/m"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "제 17 항에 있어서,- 5 -공개특허 10-2008-0052940상기 다음 모드 내에서 상기 캐릭터의 위치는, 상기 현재 모드 내에서 아래와 같이 표현되는 상기 캐릭터의 속도로부터 구해지는 것을 특징으로 하는 게임 캐릭터 제어방법.anew=aold + aΔt"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "제 17 항에 있어서,상기 다음 모드 내에서 상기 캐릭터의 위치는, 아래와 같이 구해지는 것을 특징으로 하는 게임 캐릭터제어방법.xnew=xold + voldΔt + 1/2aΔt2"}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제 14 항에 있어서,상기 구하여진 다음 모드 내에서의 위치로 캐릭터를 이동한 후,상기 다음 모드 내에서 게임을 진행하고 게임 상황의 분석 및 상기 분석 결과에 따라, 상기 캐릭터의 행동을 제어하는 단계를 더 포함하여 이루어지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "게임 캐릭터의 제어방법에 있어서,다층 퍼셉트론에 의하여 가중치를 초기화하는 단계; 및현재 모드 내에서 캐릭터의 위치를 입력한 후 처리하여, 상기 가중치를 수정하는 단계를 포함하여 이루어지는것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제 25 항에 있어서, 상기 처리 단계는,현재 캐릭터의 위치를 전방향으로 전파한 후 목표값을 설정하고,상기 목표값과 출력층의 오차값에 따라 가중치를 수정하는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제 26 항에 있어서, 상기 가중치의 수정은,오차 역전파법에 따라 이루어지는 것을 특징으로 하는 게임 캐릭터 제어방법."}
{"patent_id": "10-2006-0124748", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제 25 항에 있어서,수정된 가중치에 따라, 상기 캐릭터의 위치를 수정하는 단계를 더 포함하는 게임 캐릭터 제어방법.명 세 서발명의 상세한 설명 발명의 목적 발명이 속하는 기술 및 그 분야의 종래기술본 발명은 게임용 캐릭터의 제어방법에 관한 것으로서, 더욱 상세하게는 게임의 각각의 모드 내에서 캐릭터의 <4>위치를 인식하고 다음 모드에서의 캐릭터의 위치를 결정하는 방법에 관한 것이다.본 발명은 컴퓨터가 조정하는 캐릭터가 1개 이상 존재하고, 각 캐릭터의 위치가 게임 상황에 중요한 요소로 작 <5>- 6 -공개특허 10-2008-0052940용하는 그룹형 스포츠 게임, 전략 시물레이션 게임, 캐주얼 게임 등의 게임에서 현재 게임 상황을 자동으로 인식하고 인식된 상황에 따라 개별 캐릭터의 움직임을 제어하는 방법에 관한 것이다.여기서 게임 상황이란 예를 들어 축구 경기의 경우 (1) 일반적 공격 상황 (2) 슛 찬스 상황 (3) 일반적 수비 상 <6>황 (4) 실점 위기 상황 등을 의미하며 사용자가 임의로 정의하여 각 출력 노드에 매핑 할 수 있다.상술한 게임에서는, 대전에 참여하는 각각의 독립 캐릭터 데이터를 준비하여 게임을 진행한다. 그리고, 유저가 <7>스스로 자신의 캐릭터 데이터를 준비할 수 없다면, 게임 프로그램 내에 미리 준비된 캐릭터가 사용된다.그런데, 프로그램 내에 미리 준비된 캐릭터는 한정되어 있을 뿐더러, 게임의 변화를 거의 반영하지 못한다는 문제점이 있었다. 이러한, 문제점들을 해결하기 위하여 게임의 캐릭터 형성에, 인공지능의 기법을 접목하여는 시도가 있었다. 컴퓨터 게임에서 인공 지능의 역할은 게임에 등장하는 캐릭터들이 사람처럼 행동도록 하기 위한것으로 이를 통하여 사용자는 게임에 더욱 몰입할 수 있다.따라서 게임의 캐릭터 형성에, 인공지능의 기법을 접목하여는 시도가 있었다. 그동안의 시도에서는 인공지능 기 <8>법으로서 주로 유한 상태 기계(Finite State Machine:FSM), 퍼지 상태 기계(Fuzzy State Machine:FuSM) 및 인공생명 등을 사용하였다.먼저, 인공생명이란 생명체가 나타내는 현상을 컴퓨터, 로봇 등과 같은 인공 매체 상에 재현함으로써 생명의 일 <9>반적인 특성에 대해 연구하는 학문이다. 생물학이 생물 현상에 대해 분석적인 방법으로 접근하였다면, 인공생명은 종합적인 방법으로 접근한다. 게임 개발자들은 인공생명 기법을 게임에 적용하려고 오래 전부터 시도를 해왔다. 여기서, 인공생명의 예측 불가능한 특성 때문에 게임 분야에는 제한적으로 사용되어 왔으나, 최근 들어 다시 인공생명을 게임에 적용하려는 시도가 되고 있다. 즉, 인공생명의 기본적인 특성인 적응성, 창의성 등을 게임에 적용하게 되면, 복잡한 환경이나 사용자의 조작에 적응하거나 전혀 예상치 못한 창의적인 행동으로 인하여게임의 흥미를 높일 수 있기 때문이다. 그러나 지금까지 연구의 초점은 주로 캐릭터들이 군집을 형성하는 게임에서 전체적인 전략을 결정하는 것에 중점을 둔 연구로서 아직 기초적인 단계에 머물고 있다.그리고, FSM은 상태들 간의 전이에 의해 통제되는 그래프 내에 유한 개의 상태들이 연결되어 있는 규칙 기반 시 <10>스템으로서, 현재 가장 널리 사용되는 인공 지능 기법이다. FSM은 단순히 if-else나 switch-case 문장만으로 구현할 수 있기 때문에 이해하기 쉽고 프로그램으로 구현하기 쉬워 널리 사용된다. 예를 들어 캐릭터가 이동 중에적을 발견하게 되면, 이동 상태에서 추적 상태로 바뀌어 발견한 적을 쫓아간다. 그리고 적이 일정한 거리 내에들어오면, 공격 상태로 바뀌어 적을 공격한다. 이와 같이 FSM을 이용하면 구현이 쉽고 행동이 정확히 정의되는장점이 있으나 게임의 진행이 미리 정의된 방식으로만 동작하는 단점이 있다. 즉 게임의 상대방이 FSM으로 구현된 경우 상대방의 행동 양식이 일정하기 때문에 일정시간 게임을 한 후에는 쉽게 예측될 수 있다는 것이다. 상대방의 행동을 예측할 수 있다는 것은 게임의 흥미를 반감시키는 요인으로 작용한다. 이러한 단점을 보완하기위해 FuSM이 사용되고 있다.FSM에 퍼지 이론을 접목한 FuSM의 경우에는 입력과 출력에 퍼지 함수를 적용하여 어느 정도 무작위적으로 동작 <11>할 수 있도록 한다. 무작위 요소가 포함되어 있기 때문에 게임에서의 상대방이 동일한 상황에서 다른 행동을 할가능성이 있어서 상대방의 행동을 예측하기가 어렵게 된다. 그러나, FSM과 FuSM은 캐릭터의 상태의 수가 적을때는 간단하게 구현할 수 있지만, 상태의 수가 많아지게 되면, 상태 다이어그램을 정리하기도 어렵고, 프로그램이 급격하게 복잡해지는 단점이 있다. 또한 FSM 과 FuSM 모두 새로운 행동 양식을 추가하기 위해서는 새롭게 프로그램을 해야만 하는 단점이 있다.상술한 바와 같이, 종래의 인공지능 기법은 대부분이 보드 게임 등과 같이 게임 전체의 상황을 인식하여 대처하 <12>는 게임을 위한 것으로, 게임 설계자가 모든 상황을 미리 규정해 설계하고 일일이 코딩해야 하는 단점이 있다.또한 이렇게 제작된 게임이라고 할지라도 스스로 환경을 인식하여 적응하는 능력이 없어 상황이나 게임의 규칙이 변화하면 다시 코딩을 해야 한다. 또한 인공 생명과 같은 복잡한 인공 지능 기술의 경우에는 지나치게 높은계산량으로 인해 실제 게임에서는 사용되기 어려운 단점이 있어 왔다. 발명이 이루고자 하는 기술적 과제본 발명은 상기와 같은 문제점을 해결하기 위한 것으로서, 본 발명의 목적은 현재 게임 상황을 인식하여 각 상 <13>황별로 다른 행동을 하도록 게임 캐릭터를 제어하는 방법을 제공하는 것이다. 이러한 상황 인식은 종래에는 규칙기반 인공지능에서 주로 구현 가능했으며, 신경회로망 기반의 인공 지능에서는 구현의 복잡성으로 인하여 실제 게임에 적용이 어려웠다.- 7 -공개특허 10-2008-0052940둘째, 상황의 인식과 인식에 따른 행동제어를 동일 알고리즘에 의하여 동시에 수행함으로써 계산량을 줄이고 이 <14>에 따라 보다 적은 컴퓨터 리소스 만으로도 높은 수준의 인공지능을 구현하는 게임 캐릭터 제어방법을 제공하는것이다.셋째, 게임 상황에 따라 캐릭터의 행동 규칙을 각각 구현할 필요 없이, 상황에 따른 게임 데이터베이스를 이용 <15>한 신경회로망의 학습으로 상황을 인식하고 행동하는 게임 캐릭터 제어방법을 제공하는 것이다. 발명의 구성 및 작용상술한 목적을 달성하기 위하여, 본 발명은 본 발명은 캐릭터가 투입된 게임 상황을 분석하는 단계; 및 상기 분 <16>석 결과에 따라, 상기 캐릭터의 행동을 제어하는 단계를 포함하여 이루어지는 것을 특징으로 하는 게임 캐릭터제어방법을 제공한다.이하 상기의 목적을 구체적으로 실현할 수 있는 본 발명의 바람직한 실시예를 첨부한 도면을 참조하여 <17>설명한다.종래와 동일한 구성 요소는 설명의 편의상 동일 명칭 및 동일 부호를 부여하며 이에 대한 상세한 설명은 생략한 <18>다.본 발명에서는 게임 상황의 인식 및 캐릭터 제어를 위하여 다층 퍼셉트론(MLP: Multi-Layer Perceptron)을 이용 <19>한다. 다층 퍼셉트론은 정해진 차원의 입력 및 출력을 갖는다. 우선 다층 퍼셉트론의 출력층은 인식하고자 하는게임 상황의 종류와 1대 1로 매핑되며 출력 노드 중 가장 값이 큰 노드에 해당하는 게임 상황이 현재의 게임 진행 상황으로 인식된다. 그리고, 다층 퍼셉트론의 입력층은 현재 캐릭터의 위치가 벡터화되어 인가된다. 즉 N개의 캐릭터가 게임 내에 <20>존재하고, 각 캐릭터의 위치가 (xi, yi)인 경우 입력 노드의 개수는 2N개가 되며, 입력 벡터는 (x1,y1,...,xn,yn)와 같이 주어진다. 입력층에는 실제 캐릭터의 위치 뿐만 아니라 게임 진행 정보와 연결되며 게임 상황을 인식하는데 필요한 모든 정보가 주어질 수 있다. 예를 들어 축구 경기의 경우 각 선수의 위치 뿐만 아니라 공의 위치도 입력 벡터에 포함될 수 있다.중간층(은닉층)의 노드 개수는 사용자가 임의로 설정할 수 있으며 노드 개수에 따라 인식 및 제어 성능이 달라 <21>질 수 있으므로 여러 번의 실험에 의하여 노드 개수를 최적화하도록 한다.각 노드가 정의되면 노드를 연결하는 가중치를 학습하는 과정이 필요하다. 이를 위하여 우선 사용자는 미리 정 <22>의된 게임 상황에 맞는 DB를 준비하여 각 상황별로 적당한 입력, 출력을 인가하여 다층 퍼셉트론의 가중치 값을학습시키게 된다. 학습과정은 기존의 오차 역전파법을 그대로 사용하게 되며 이 과정은 후술한다.그리고, 다양한 DB에 의해서 다층 퍼셉트론이 충분히 학습되면 실제 게임 중에서 이를 이용하여 상황을 인식하 <23>고 인식된 상황에 따라서 캐릭터가 움직여야할 위치를 알아낼 수 있다. 게임 상황의 인식은 현재 게임 캐릭터의상황을 입력층에 인가하여 출력층의 어떤 노드에서 가장 큰 값이 출력되는지를 검사함으로써 쉽게 이루어진다. 게임 캐릭터의 행동 제어는 다층 퍼셉트론의 학습 방법인 오차 역전파법의 확장을 이용하여 이루어진다. 기존의 <24>오차 역전파법은 출력층의 오차를 은닉층까지만 역전파하여 입력층과 은닉층, 은닉층과 출력층을 연결하는 가중치만을 학습하는데 반하여 오차 역전파법을 입력층까지 확장함으로써 입력층의 패턴을 학습하는 방법이 발명자의 선행특허에서 제안된 바 있다. 본 발명에서는 이러한 학습 방법을 이용하여 출력층의 오차를 최소화하기 위하여 현재 캐릭터들이 어떤 방향으로 이동하여야할 지가 결정된다.매 프레임에서 캐릭터의 움직임을 결정하기 위하여 입력층의 학습 방법을 사용함으로써 각 캐릭터의 이동 방향 <25>을 구하고 이에 따라 캐릭터의 위치를 제어한 후 다시 상황을 인식하는 과정을 반복하게 된다.여기서, 본 발명은 다층 퍼셉트론과 그 학습 방법인 오차 역전파법을 확장 적용하여 게임의 상황을 인식하고 또 <26>한 인식된 상황을 바탕으로 게임 내의 캐릭터의 움직임을 제어하는 것을 특징으로 한다.도 1은 다층 퍼셉트론 형태의 신경회로망을 사용하는 지능 캐릭터의 유전자 알고리즘을 개략적으로 나타낸 도면 <27>이고, 도 2는 본 발명에 따른 다층 퍼셉트론의 학습방법의 일실시예의 흐름도이다.다층 퍼셉트론은 도 1에 도시되어 있는 바와 같이 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)이 존재 <28>하는 층 구조의 신경회로망으로 몇 개의 단층 퍼셉트론이 직렬로 연결된 형태이다. 입력층에 인가된 입력값은- 8 -공개특허 10-2008-0052940각 입력 뉴런에 연결된 시냅스 가중치가 곱해져 인접한 은닉층의 뉴런 별로 그 합이 계산되고, 이 뉴런의 출력값이 다시 다음 은닉층의 입력이 되는 형태로 차례로 출력층까지 전달된다. 여기서, ℓ번째 은닉층의 j번째 뉴런의 입력은 아래의 수학식 1과 같이 계산된다.수학식 1<29>수학식 1에서 wℓj0는 의 바이어스이고 h0ℓ-1=1이며, wℓjk는 ℓ-1번째 은닉층의 k번째 뉴런과 ℓ번째 은닉층 <30>의 j번째 뉴런을 연결하는 시냅스 가중치를, 또 hℓ-1k은 ℓ-1번째 은닉층의 k번째 뉴런의 출력값을 의미하며 N은ℓ-1번째 은닉층의 은닉 뉴런의 개수를 의미한다.여기서, 은닉층의 뉴런의 입력이 과 같이 주어졌을 때 그 뉴런의 출력은 아래의 수학식 2와 같이 주어진 <31>다.수학식 2<32>이러한 구조를 갖는 다층 퍼셉트론이 인식기로 올바르게 동작하기 위해서는 각 뉴런을 연결하는 시냅스 가중치 <33>들이 적당한 값으로 조절되어야 한다. 이러한 가중치의 조절 과정이 다층 퍼셉트론의 학습 과정이며 이는 오차역전파법에 의하여 층별로 계산된다. 다층퍼셉트론의 학습은 P개의 학습 패턴을 입력으로 받아들이고 각각의 학습 패턴에 해당하는 원하는 출력값을 <34>출력층의 목표치로 설정하여 출력층의 실제 출력값과 목표치 사이의 평균 자승 오차(MSE: Mean Squared Error)를 최소로하는 시냅스 가중치를 구함으로써 이루어진다. 목표치는 학습에 사용하는 데이터가 실제로 어떤 출력을 내야하는지를 의미하는 값으로 이러한 목표치에 의한 학습방법을 교사학습(Supervised Learning) 방법이라고한다.즉, 가중치를 초기화하고(S200), P번째 학습패턴과 목표값을 인가한 후(S210) 이를 전방향전파한다(S220). 그리 <35>고, 출력층의 오차값을 계산하여(S230), 이 값이 문턱값보다 작으면 가중치를 저장한다(S270). 그러나, 그러하지 아니하면 오차 역전파법(S250)에 따라 가중치를 수정한다(S260).구체적으로 설명하면 다음과 같다. P개의 학습 패턴 xp와 그에 따른 출력 벡터 yp, 목표 벡터 tp에 대한 평균 자 <36>승 오차는 아래의 수학식 3과 같이 계산된다.수학식 3<37>여기서, 오차 역전파법은 수학식 3의 MSE를 최소화시키기 위하여 출력 층의 가중치를 아래의 수학식 4와 같은 <38>방법으로 반복적으로 적용하는 것이다.- 9 -공개특허 10-2008-0052940수학식 4<39>수학식 4에서 η는 학습률을 나타내며 δℓj는 각 은닉층 뉴런값에 대한 출력층 오차의 미분치를 나타내는 항으 <40>로 아래의 수학식 5와 같이 정의된다.수학식 5 <41>요약하면 오차 역전파법은 주어진 입력 벡터와 목표 벡터에 대하여 수학식 1에 따른 전방향 전파를 통하여 수학 <42>식 3과 같은 출력층의 총오차를 계산하고 이를 최소화하기 위하여, 수학식 5와 같이 각 은닉층 뉴런의 값에 대하여 출력층 오차를 미분하여 오차가 작아지는 방향으로 수학식 4와 같이 시냅스 가중치를 변경하는 과정을 P개의 학습 패턴에 대하여 반복적으로 적용하는 알고리즘이다. 이러한 과정은 도 2에 그 절차를 간략하게 정리하였다.전통적인 오차 역전파법을 개선하여 학습과정을 입력층까지 확장함으로써 보다 나은 인식 성능을 얻을 수 있다. <43>(수학식5)를 입력층까지 확장하여 생각하면 입력층에서의 δ항은 아래 수학식 6과 같게 된다.수학식 6<44>즉, 수학식 6은 출력층의 오차를 감소시키기 위하여 입력값이 어떤 방향으로 변화되어야 하는지를 나타낸다. 이 <45>때 δ0i값은 기존의 오차 역전파법을 확장하여 아래 수학식 7과 같이 쉽게 구할 수 있다.수학식 7<46>여기서, δ0j은 기존의 오차 역전파법에서와 같이 첫 번째 은닉층의 j번째 뉴런의 δ항을 의미한다. <47>도 3은 본 발명에 따른 다층 퍼셉트론을 이용하여 상황을 인식하고, 인식된 상황에 따라 캐릭터의 위치를 제어 <48>하는 방법의 일실시예의 흐름도이다. 도 3을 참조하여 본 발명에 따른 게임 캐릭터 제어방법에서, 다양한 퍼셉트론에 대하여 데이타 베이스를 학습하는 과정의 일실시예를 설명하면 다음과 같다.먼저, 가중치를 종래에 저장된 가중치로 초기화한다(S300). 그리고, 현재 캐릭터의 위치 등을 입력한 후(S310), <49>이를 전방향 전파한다(S320). 여기서, 다층 퍼셉트론의 입력층은 현재 게임 진행 정보와 연결되며 게임 상황을인식하는데 필요한 어떤 정보든 주어질 수 있다. 예를 들어 축구 경기의 경우 각 선수의 위치 및 공의 위치를각 입력 노드에 인가한다. 게임 내에서 이동하는 캐릭터가 N개 존재하는 경우 번째 캐릭터의 현재 위치를(xi,yi)라고 했을 때, 입력층의 값은 아래와 같이 정의된다.수학식 8<50>- 10 -공개특허 10-2008-0052940중간층(은닉층)의 노드 개수는 사용자가 임의로 설정할 수 있으며 노드 개수에 따라 인식 및 제어 성능이 달라 <51>질 수 있으므로 여러 번의 실험에 의하여 노드 개수를 최적화하도록 한다.각 노드가 정의되면 노드를 연결하는 가중치를 학습하는 과정이 필요하다. 이를 위하여 우선 사용자는 미리 정 <52>의된 게임 상황에 맞는 데이터베이스를 준비하여 각 상황별로 적당한 입력, 출력을 인가하여 다층 퍼셉트론의가중치 값을 학습시키게 된다. 학습 과정은 아래와 같다.먼저 게임 상황을 인식하는데(S330), 구체적으로 데이터베이스의 데이터를 입력으로 인가한다. 이때 이 데이터 <53>에는 어떠한 상황인지가 기록되어 있어야 하며 이를 바탕으로 출력 노드의 목표값이 설정된다(S340). 출력 노드의 목표값은 데이터의 상황에 해당하는 노드는 1의 값을 그 이외의 노드는 0(또는 -1)의 값을 갖는 방식으로 정해진다.수학식 9 <54>입력과 목표값이 정해지면 입력값에 해당하는 출력값과 목표값이 같아지도록 가중치가 학습된다. 출력값은 수학 <55>식 1과 수학식 2에 의하여 계산되고, 가중치의 학습은 수학식 3을 최소화하는 방향으로 수학식 4와 수학식 5에의하여 계산된다. 충분히 많은 데이터베이스에 대하여 위의 과정을 반복하여 가중치를 학습시키도록 한다. 즉,출력층의 오차값을 계산한고(S350), 이를 최소화하기 위하여 오차 역전파법에 의하여 가중치를 수정한다(S360).가중치가 학습되면 이제 실시간 게임이 진행되면서 현재 게임 상황을 판단할 수 있다. 이러한 과정은 현재 게임 <56>상황에 해당하는 데이터를 입력에 인가하고 수학식 1과 수학식 2에 의하여 출력값을 계산한 후, 최대값을 출력하는 노드에 해당하는 상황을 선택함으로서 쉽게 구현된다. 상황이 인식되면 현재 상황에 따라 다음 프레임에서의 캐릭터의 위치를 계산하게 되며, 이 과정은 아래와 같다.우선 인식된 현재 상태에 해당하는 목표값을 설정한다. 목표값 설정의 방법은 상기와 같다. 즉, 인식된 상황에 <57>해당하는 노드는 1의 값을 그 이외의 노드는 0(또는 -1)의 값을 갖도록 한다. 이후 현재 출력값과 설정된 목표값과의 오차를 수학식 3과 같이 계산한다. 이제 다음 프레임에서의 캐릭터들의 새로운 위치는 이 오차를 최소화하는 방향으로 캐릭터들의 위치를 수정함으로써 계산될 수 있다. 즉, 입력층을 수정하고(S370), 이어서 캐릭터의 위치를 그에 따라 수정하게 되는 것이다(S380). 수학식 10<58>여기서 x는 입력 노드 값의 벡터이고, E는 출력층에서 계산된 오차값, η는 가중치를 학습하는 정도를 나타내 <59>는 상수값이다. 결과적으로 수학식 10은 현재 상황을 가정했을 때, 현재 상황에 맞는 출력값을 내기 위해서 입력들이 변화되어야하는 방향을 의미한다. 그리고, 수학식 10의 값을 효율적으로 계산하기 위해서 상기에 기술된수학식 8의 방법에 의하여 입력층의 δ0i값을 계산한다. 이를 이용하여 수학식 10은 아래와 같이 계산될 수있다.수학식 11<60>수학식 11을 이용하여 캐릭터의 새로운 위치는 아래와 같이 계산될 수 있다. <61>수학식 12<62>수학식 12는 현재 프레임에서의 위치로부터 다음 프레임에서의 위치를 직접 계산하는 식이다. 일반적으로 게임 <63>- 11 -공개특허 10-2008-0052940에서 캐릭터의 움직임은 캐릭터의 위치를 직접 지정하는 것 이외에 캐릭터의 속도를 지정하여 다음 프레임의 위치를 간접적으로 계산할 수 있다. 수학식 13<64>수학식 13에서 T는 위치가 계산되는 두 시간 사이의 간격, 즉 프레임 간격이고, Δx는 위치의 변화량으로 수학 <65>식 11으로부터 계산될 수 있다. 또한 캐릭터의 직접적인 위치와 속도 이외에 캐릭터에 가해지는 힘의 크기와 방향을 지정하는 방법으로도 지정 <66>될 수 있다. 특히 실세계에서와 유사한 물리적 움직임을 갖게 하기 위하여 캐릭터에 가해지는 힘을 지정함으로써 캐릭터의 속도와 위치를 계산하는 방법을 사용한다. 수학식 14W = F × d <67>여기서 F는 캐릭터가 자신의 위치를 변경하기 위해서 낼 수 있는 힘의 크기로 실제 게임에서는 캐릭터의 신체적 <68>특징, 현재 가지고 있는 체력 등에 의해서 결정되는 값이고, d는 힘을 가하는 방향을 나타내는 단위벡터로 수학식 11을 이용하여 계산될 수 있다.<69>여기서 ∥­∥는 벡터의 크기를 계산하는 기호이다. 이와 같은 방법으로 캐릭터의 위치를 계산하는 방법의 경우 <70>수학식 11에 나타나는 η의 크기에 영향을 받지 않는다는 효과를 같이 얻을 수 있다. 수학식 14로부터 캐릭터의속도 또는 다음 위치를 계산하는 방법은 게임에 사용되는 물리엔진에 따라 달라질 수 있으며 예를 들어 아래와같이 계산될 수 있다.수학식 15<71>캐릭터의 새로운 위치가 계산되면 새로운 위치에 따라 게임을 진행하고 이 새로운 위치를 다층 퍼셉트론의 입력 <72>으로 인가하여 위의 과정을 다시 반복하게 된다.본 발명은 상술한 실시예에 한정되지 않으며, 첨부된 청구범위에서 알 수 있는 바와 같이 본 발명이 속한 분야 <73>의 통상의 지식을 가진 자에 의해 변형이 가능해도 이러한 변형은 본 발명의 범위에 속한다. 발명의 효과상술한 본 발명에 따른 게임 캐릭터 제어방법의 효과를 설명하면 다음과 같다. <74>첫째, 현재 게임 상황을 인식하여 각 상황별로 다른 행동을 하도록 제어함으로써 보다 사실감 있는 게임을 제작 <75>할 수 있다.둘째, 상황의 인식과 인식에 따른 행동제어를 동일 알고리즘에 의하여 동시에 수행함으로써 계산량을 줄이고 <76>이에 따라 보다 적은 컴퓨터 리소스 만으로도 높은 수준의 인공지능을 구현할 수 있게 된다.셋째, 본 발명의 기술을 이용할 경우 게임 개발자는 게임 상황에 따라 캐릭터의 행동 규칙을 각각 구현할 필요 <77>없이 상황에 따른 게임 데이터베이스를 이용한 신경회로망의 학습으로 상황을 인식하고 상황에 따른 캐릭터의행동제어를 하게 된다. 따라서 보다 적은 개발 비용으로 고품질의 게임을 개발할 수 있게 된다.- 12 -공개특허 10-2008-0052940도면의 간단한 설명도 1은 다층 퍼셉트론 형태의 신경회로망을 사용하는 지능 캐릭터의 행동제어 알고리즘을 개략적으로 나타낸 도 <1>면이고,도 2는 본 발명에 따른 다층 퍼셉트론의 학습방법의 일실시예의 흐름도이고, <2>도 3은 본 발명에 따른 다층 퍼셉트론을 이용하여 상황을 인식하고, 인식된 상황에 따라 캐릭터의 위치를 제어 <3>하는 방법의 일실시예의 흐름도이다.도면 도면1 도면2- 13 -공개특허 10-2008-0052940 도면3- 14 -공개특허 10-2008-0052940"}
{"patent_id": "10-2006-0124748", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 게임 캐릭터의 제어방법에 관한 것이다. 본 발명은 캐릭터가 투입된 게임 상황을 분석하는 단계; 및 상기 분석 결과에 따라, 상기 캐릭터의 행동을 제어 하는 단계를 포함하여 이루어지는 것을 특징으로 하는 게임 캐릭터 제어방법을 제공한다. 따라서, 본 발명에 의하면 게임 내에서 상황의 인식과 인식에 따른 행동제어를 동일 알고리즘에 의하여 동시에 수행함으로써 계산량을 줄이고 이에 따라 보다 적은 컴퓨터 리소스 만으로도 높은 수준의 인공지능을 구현할 수 있고, 게임 개발자는 게임 상황에 따라 캐릭터의 행동 규칙을 각각 구현할 필요 없이 상황에 따른 게임 데이터베 이스를 이용한 신경회로망의 학습으로 상황을 인식하고 상황에 따른 캐릭터의 행동제어를 할 수 있다."}
{"patent_id": "10-2006-0124748", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 본 발명에 따른 게임 캐릭터 제어방법의 효과를 설명하면 다음과 같다. <74> 첫째, 현재 게임 상황을 인식하여 각 상황별로 다른 행동을 하도록 제어함으로써 보다 사실감 있는 게임을 제작 <75> 할 수 있다. 둘째, 상황의 인식과 인식에 따른 행동제어를 동일 알고리즘에 의하여 동시에 수행함으로써 계산량을 줄이고 <76> 이에 따라 보다 적은 컴퓨터 리소스 만으로도 높은 수준의 인공지능을 구현할 수 있게 된다. 셋째, 본 발명의 기술을 이용할 경우 게임 개발자는 게임 상황에 따라 캐릭터의 행동 규칙을 각각 구현할 필요 <77> 없이 상황에 따른 게임 데이터베이스를 이용한 신경회로망의 학습으로 상황을 인식하고 상황에 따른 캐릭터의 행동제어를 하게 된다. 따라서 보다 적은 개발 비용으로 고품질의 게임을 개발할 수 있게 된다. - 12 -공개특허 10-2008-0052940"}
