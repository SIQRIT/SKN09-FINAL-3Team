{"patent_id": "10-2018-0036385", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2019-0114150", "출원번호": "10-2018-0036385", "발명의 명칭": "비디오 번역 및 립싱크 방법 및 시스템", "출원인": "네오사피엔스 주식회사", "발명자": "김태수"}}
{"patent_id": "10-2018-0036385", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 언어를 구사하는 제1 화자의 음성을 포함하는 비디오 데이터에서, 상기 음성 부분을 줄이거나 삭제하는 단계;상기 음성의 텍스트를 제2 언어의 텍스트로 번역하는 단계;상기 번역된 텍스트, 제1 화자 정보에 기초하여 제1 화자의 특성을 포함하는 음성을 합성하는 단계; 상기 합성된 음성에 기초하여 상기 제1 화자의 얼굴 이미지 중에서 입모양에 대한 키포인트(keypoint)를 생성하는 단계;상기 생성된 키포인트에 기초하여 상기 제1 화자의 입모양을 합성한 이미지를 생성하는 단계; 및상기 합성된 음성 및 상기 제1 화자의 입모양의 합성 이미지를 비디오 데이터에 결합하는 단계를 포함하는,비디오 번역 방법."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 특정 언어의 음성을 포함하는 비디오를 번역하여 다른 언어의 음성을 포함하는 비디오를 제공하는 방법이 제공된다. 이 방법은, 제1 언어를 구사하는 제1 화자의 음성을 포함하는 비디오 데이 터에서, 상기 음성 부분을 줄이거나 삭제하는 단계, 상기 음성의 텍스트를 제2 언어의 텍스트로 번역하는 단계, 상기 번역된 텍스트, 제1 화자 정보에 기초하여 제1 화자의 특성을 포함하는 음성을 합성하는 단계, 상기 합성된 음성에 기초하여 상기 제1 화자의 얼굴 이미지 중에서 입모양에 대한 키포인트(keypoint)를 생성하는 단계, 상기 생성된 키포인트에 기초하여 상기 제1 화자의 입모양을 합성한 이미지를 생성하는 단계; 및 상기 합성된 음성 및 상기 제1 화자의 입모양의 합성 이미지를 비디오 데이터에 결합하는 단계를 포함한다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 특정 언어의 음성을 포함하는 비디오를 다른 언어의 음성을 포함하는 비디오로 번역하고, 번역된 비 디오의 립싱크를 제공하는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "일반적으로 TTS(Text-To-Speech)라고도 불리는 음성 합성 기술은, 안내방송, 네비게이션, 인공지능 비서 등과 같이 사람의 음성이 필요한 어플리케이션에서 실제 사람의 음성을 사전에 녹음해 두지 않고 필요한 음성을 재생 하기 위해 사용되는 기술이다. 음성 합성의 전형적인 방법은, 음성을 음소 등 아주 짧은 단위로 미리 잘라서 저 장해두고, 합성할 문장을 구성하는 음소들을 결합하여 음성을 합성하는 concatenative TTS 와, 음성의 특징을 parameter로 표현하고 합성할 문장을 구성하는 음성 특징들을 나타내는 parameter들을 vocoder를 이용해 문장에 대응하는 음성으로 합성하는 parametric TTS가 있다. 한편, 최근에는 인공 신경망(artificial neural networks) 기반의 음성 합성 방법이 활발히 연구되고 있으며, 이 음성 합성 방법에 따라 합성된 음성은, 기존의 방법에 비해 훨씬 자연스러운 음성 특징을 보여주고 있다. 하 지만, 인공 신경망 기반의 음성 합성 방법으로 새로운 목소리의 음성 합성기를 구현하기 위해서는 그 목소리에 해당하는 많은 데이터가 필요하고, 이 데이터를 이용한 신경망 모델의 재학습이 요구된다. 또한, 특정 언어의 텍스트를 다른 언어의 텍스트로 번역하여, 번역된 언어의 음성으로 합성하는 연구도 진행되 고 있다. 여기서, 번역된 언어의 음성 합성에는 그 언어의 대표적인 특징을 갖는 화자의 음성 데이터가 사용될 수 있다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 특정 언어의 음성을 포함하는 비디오 데이터를 번역하여 다른 언어의 음성을 포함하는 비디오로 변환 하고, 번역된 비디오에 대한 립싱크를 제공하는 방법 및 시스템을 제공한다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시에서는 특정 언어를 구사하는 화자의 음성을 포함하는 비디오 데이터에서, 해당 음성 부분을 줄이거나 삭제한다. 또한, 해당 음성의 텍스트를 다른 언어의 텍스트로 번역한 후, 번역된 텍스트에 기초하여 해당 음성의 화자의 특성을 포함하는 음성을 합성한다. 한편, 합성된 음성에 기초하여 그 음성의 화자의 얼굴 이미지 중 에서 입모양에 대한 키포인트(keypoint)를 생성하고, 생성된 키포인트에 기초하여 화자의 입모양을 합성한 이미 지를 생성한다. 이렇게 합성된 음성 및 화자의 입모양의 합성 이미지는 비디오 데이터에 다시 결합되어, 번역된 언어(translated language)의 음성과 화자의 입모양이 동기화되어 합성된 비디오 데이터를 얻는다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 다양한 실시예들에 따르면, 번역된 비디오 데이터에 포함된 음성은 번역되기 전의 음성의 화자의 특 징을 포함하여, 원래의 화자가 외국어의 억양을 모사하는 듯한 음성 합성 결과를 얻을 수 있다. 또한, 합성된 음성에 기초하여 원래의 화자의 입모양을 합성하여 립싱크를 제공함으로써, 원래의 화자가 외국어의 억양을 모 사하는 효과를 더욱 증가시킬 수 있다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 개시의 실시예들을 상세히 설명한다. 다만, 통상의 기술자는 이들 도면에 관 해 여기에 주어진 상세한 설명이 예시의 목적을 위함이며, 본 개시는 이들 제한된 실시예들을 넘어 확장된다는 것을 쉽게 이해할 것이다. 도 1은 일 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 도면이다. 도시된 바와 같이, 비디오 번역 시스템에서, 특정 언어(original language)를 구사하는 화자의 음성을 포함하는 비디오 데이터에서, 해당 음성 부분을 줄이거나 삭제한다. 한편, 해당 음성의 텍스트를 다른 언어의 텍스트로 번역한 후, 번역된 텍스트(pre-translated text)에 기초하여 해당 음성의 화자의 특성을 포함하는 음성을 합성 한다. 여기서 사용되는 음성합성기에는, 번역된 텍스트, 화자 id(speaker id), 화자 특징 및/또는 타이밍 정보 (또는 동기화 정보)가 입력될 수 있고, 입력된 정보에 기초하여 음성을 합성한다. 한편, 키포인트 생성기 (keypoint generation)는, 합성된 음성에 기초하여 그 음성의 화자의 얼굴 이미지 중에서 입모양에 대한 키포인 트(keypoint)를 생성한다. 또한, 이미지 생성기(image generation)는, 생성된 키포인트에 기초하여 화자의 입모 양을 합성한 이미지를 생성한다. 이렇게 합성된 음성 및 화자의 입모양의 합성 이미지는 비디오 데이터에 다시 결합되어, 번역된 언어(translated language)의 음성과 화자의 입모양이 동기화되어 합성된 비디오 데이터를 얻 는다. 도 2는 일 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성기의 구성을 나타내는 도면이다. 도 2에 도시된 음성 합성기는 도 1에 도시된 시스템에서 음성 합성에 사용될 수 있다. 도 2에 있어서, 인코더(encoder)는 텍스트를 발음 정보로 변환한다. 디코더(decoder)는 복수의 언어(lang 1, lang 2, 쪋, lang N)의 입력 텍스트를 character embedding으로 만들고, fully-connected layer로 구성된 pre- net을 통과하여 1D convolution bank, max pooling, highway network, bidirectional GRU 로 구성된 CBHG 모듈을 통과하도록 구성된다. 한편, 디코더(decoder)는, fully-connected layer로 구성된 pre-net 과 GRU로 구성된 attention RNN, residual GRU로 구성된 decoder RNN 으로 구성되고, 그 출력은 mel-scale spectrogram으로 나온다. 또한, 최종적으로 디 코더의 출력은 CBHG로 들어가 linear-scale spectrogram이 출력으로 나오며, 이때 출력은 magnitude spectrogram이며, phase는 Griffin-Lim 알고리즘을 통해 예측되어 inverse short-time fourier transform을 통 해 time domain의 음성 신호로 나오게 된다. 이러한 인공 신경망 기반의 음성 합성기는, 다국어의 텍스트와 음성 신호의 쌍으로 존재하는 대용량의 데이터 베이스를 이용하여 학습된다. 입력으로 텍스트를 넣고 나온 출력을 해당하는 정답 음성 신호와 비교하여 손실 함수(loss function)을 정의 하게 되고, 이를 오차 역전파 (error back propagation) 알고리즘을 통해 학습하 면, 최종적으로 임의의 텍스트를 입력했을 때 원하는 음성 출력이 나오는 인공 신경망을 얻을 수 있다. 위 구성에서 다국어의 음성을 합성하는 다화자 음성합성을 위해, 음성의 화자에 해당하는 정보(one-hot speaker id)를 입력으로 넣고 이것을 임베딩 벡터로 만든 후, attention RNN 및 decoder RNN 에 입력으로 주어 화자마다 다르게 디코딩을 하도록 인공 신경망의 구조를 구성한다. 또한, 이 인공 신경망을 학습하기 위해서는, 텍스트, 화자 인덱스, 음성 신호의 쌍으로 존재하는 데이터 베이스를 이용한다. 텍스트, 화자 정보를 각각 인공 신경망 의 입력으로 하고 해당 음성 신호를 정답으로 하여 앞서 언급한 방법과 같이 학습함으로써, 텍스트와 화자 정보 를 입력으로 주었을 때 해당 화자의 음성을 출력할 수 있는 음성 합성기를 얻는다. 도 3은 다른 실시예에 따른 특정 언어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성기의 구성을 나타내 는 도면이다. 도 3에 도시된 음성 합성기는 도 1에 도시된 시스템에서 음성 합성에 사용될 수 있다. 도 3의 음성 합성기는, 도 2에서와 같이 화자의 정보를 one hot vector로 구성된 인덱스로 입력 받는 대신에, 그 화자의 음성에서 추출된 특징 정보를 입력으로 받는다. 화자의 음성에서 특징 정보를 추출하기 위한 네트워 크(speaker identification network)는, 화자를 구분할 수 있는 특징을 추출할 수 있는 다양한 형태의 네트워크 가 가능하다. 그 외의 구성이나 기능은 도 2의 음성 합성기와 동일 또는 유사하므로 여기서는 상세한 설명을 생 략한다. 도 4는, 도 2 또는 도 3의 인공 신경망 모델을 이용하여 다국어 음성 합성기를 학습시키는 방법을 나타내는 도 면이다. 도시된 바와 같이 음성 합성기에 한국어 텍스트와 한국인 화자 정보를 입력하여 학습함으로써, 그 화자의 음성 을 합성할 수 있다(case 1). 또한, 음성합성기에 영어 텍스트와 영어를 모국어로 구사하는 화자 정보를 입력하 여 학습함으로써, 그 화자의 음성을 합성할 수 있다(case 2). 도 5는, 도 2 또는 도 3의 학습된 음성 합성기 인공 신경망 모델을 이용하여 다국어의 음성을 합성하는 방법을 나타내는 도면이다. 도시된 바와 같이, 도 4의 방법에 따라 학습된 음성 합성기에 영어 텍스트와 한국인 화자 정보를 입력하면, 한 국인 화자의 특징을 갖는 영어 음성이 합성될 수 있다(case 1). 또한, 도 4의 방법에 따라 학습된 음성 합성기 에 한국어 텍스트와 영어를 모국어로 구사하는 화자 정보를 입력하면, 영어 화자의 특징을 갖는 한국어 음성이 합성될 수 있다(case 2). 도 6은, 일 실시예에 따른 음성 특징에 기초한 키포인트 생성 방법을 설명하는 도면이다. 키포인트 생성기(keypoint generation)는, 음성 합성기에 의해 합성된 음성의 특징을 입력으로 받아 얼굴영상의 입술에 해당하는 부분의 키포인트를 생성한다. 여기서 입력으로 받는 음성의 특징은, 음성의 멜 필터 뱅크(mel filter bank) 특징, mfcc(mel-frequency cepstral coefficients), 보코더 파라메터 등이 될 수 있다. 그리고 출력에 해당하는 키포인트는 입술의 중요 부분에 대한 위치를 나타내는 좌표 값이다. 키포인트 생성기는, LSTM (Long short-term memory network)과 같은 RNN (Recurrent neural network)으로 구현할 수 있다. 이렇게 구현 된 신경망을 학습하기 위해서는 입력과 출력의 쌍이 필요한데, 이를 위해서 학습에 쓰일 비디오 데이터로부터 음성과 해당 음성에 대응되는 얼굴 영상을 추출해 낸다. 추출된 음성에서는 음성 특징을 추출하고, 얼굴 영상에 서는 입술에 해당하는 키포인트를 추출해 낸다. 추출된 키포인트에 대해 얼굴 위치, 회전 및 크기의 정규화와 PCA (principal component analysis)등의 전처리를 실시하여 좀 더 학습이 잘 되도록 할 수도 있다. 이렇게 준 비된 학습데이터를 인공 신경망의 입력으로 사용하고 손실 함수(loss function)를 정의하고, 오차역전파 알고리 즘을 통해 학습하면, 새로운 음성 특징 입력에 대하여 입술의 키포인트를 얻어내는 키포인트 생성기를 위한 인공 신경망 모델을 얻을 수 있다. 여기서, 서로 다른 언어를 사용하는 복수의 화자에 대한 입술 키포인트를 생성하기 위해서는, 앞에서 설명한 인 공신경망의 학습을 위해 음성 특징 입력과 키포인트 출력의 쌍을 구성할 때, 화자의 정보를 함께 인공 신경망의 입력으로 준비하여 학습할 수 있다. 이렇게 학습된 인공 신경망을 이용한 키포인트 생성기에서는, 영어 음성과 한국어 화자의 정보를 입력으로 하면, 해당 한국어 화자가 영어를 말할 때의 입술 키포인트를 얻을 수 있게 된 다. 도 7은, 일 실시예에 따른 음성 특징에 기초한 키포인트 생성 방법에서 사용될 수 있는 시간지연 LSTM을 설명하 는 도면이다. 도시된 바와 같이, 일반적으로 화자의 입은 발음하기 전에 미리 움직이므로, 키포인트 생성기가 과거에 발음된 음성 입력에만 입모양을 맞추는 것으로는 충분한 립싱크가 이루어지지 않는다. 따라서, 과거에 발음된 음성 뿐 아니라 미래의 문맥도 고려하기 위해, 키포인트 생성기는 출력에 시간 지연을 추가할 수 있다. 즉, 키포인트 생 성기는, 네트워크의 출력을 목표 지연(target delay; 예를 들어 2)로써 전방향으로 쉬프트할 수 있다. 시간지연 LSTM을 쓰는 대신 양방향 LSTM을 써서 과거와 미래의 입력을 동시에 반영하는 방법도 있다. 도 8은, 일 실시예에 따른 키포인트에 기초하여 화자의 입모양 이미지를 합성하는 방법을 설명하는 도면이다. 도시된 바와 같이, 이미지 생성기는, 화자의 얼굴 이미지 중에서 입 영역과 그 주변이 삭제되어, 윤곽선이 표시 된 이미지를 입력 받는다. 여기서, 화자의 얼굴 이미지 중에서 입 영역은, 키포인트 주변의 경계선(bounding box)을 이용하여 제거되며, OpenCV(open source computer vision library)를 이용하여 그 윤곽선이 표시된다. 이미지 생성기는, 위와 같은 이미지를 기초로 하여, 입 영역의 내부가 합성된 완전한 얼굴 이미지를 생성한다. 합성된 입 영역 이미지가 비디오 데이터에 있어서 얼굴과 호환될 수 있도록 역정규화(denormalization)이 실행 될 수 있다. 이미지 생성기는 이상과 같은 얼굴 이미지 합성에 Pix2Pix를 사용할 수 있다(\"Image-to-Image Translation with Conditional Adversarial Networks,\" Phillip Isola, et al., Computer Vision and Pattern Recognition, 2017 참조). 여기서, 서로 다른 언어를 사용하는 복수의 화자에 대한 입모양 이미지를 합성하기 위해서, Pix2Pix 네트워크에 복수의 언어의 화자의 정보를 함께 입력하여 해당 화자에 적합한 입모양을 생성할 수 있도록 학습한다. 이렇게 학습된 이미지 생성기에서는, 영어 음성과 한국어 화자의 정보를 입력으로 하여, 해당 한국어 화자가 영어를 말 할 때의 입모양을 생성해 낼 수 있다."}
{"patent_id": "10-2018-0036385", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 명세서에서는 본 개시가 일부 실시예들과 관련하여 설명되었지만, 본 발명이 속하는 기술분야의 통상의 기술 자가 이해할 수 있는 본 개시의 범위를 벗어나지 않는 범위에서 다양한 변형 및 변경이 이루어질 수 있다는 점 을 알아야 할 것이다. 또한, 그러한 변형 및 변경은 본 명세서에 첨부된 특허청구의 범위 내에 속하는 것으로 생각되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2018-0036385", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 비디오 번역 시스템의 구성을 나타내는 도면이다. 도 2는 일 실시예에 따른 다국어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성기의 구성을 나타내는 도 면이다. 도 3은 다른 실시예에 따른 다국어의 목소리 모사를 위한 인공 신경망 기반의 음성 합성기의 구성을 나타내는 도면이다. 도 4는, 도 2 또는 도 3의 인공 신경망을 이용하여 다국어의 음성 합성기 모델을 학습시키는 방법을 나타내는 도면이다. 도 5는, 도 2 또는 도 3의 학습된 음성 합성기 인공 신경망 모델을 이용하여 다국어의 음성을 합성하는 방법을 나타내는 도면이다. 도 6은, 일 실시예에 따른 음성 특징에 기초한 키포인트 생성 방법을 설명하는 도면이다. 도 7은, 일 실시예에 따른 음성 특징에 기초한 키포인트 생성 방법에서 사용되는 시간지연 LSTM을 설명하는 도 면이다. 도 8은, 일 실시예에 따른 키포인트에 기초하여 화자의 입모양 이미지를 합성하는 방법을 설명하는 도면이다."}
