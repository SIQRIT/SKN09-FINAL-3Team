{"patent_id": "10-2020-7005087", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0031154", "출원번호": "10-2020-7005087", "발명의 명칭": "인공 신경망을 이용한 심층 문맥 기반 문법 오류 정정", "출원인": "링고챔프 인포메이션 테크놀로지", "발명자": "린, 후이"}}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해, 문장을 수신하는 단계;상기 적어도 하나의 프로세서에 의해, 하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 상기 문장 내의 하나 이상의 목표 단어(target word)를 식별하는 단계 - 상기 하나 이상의 목표 단어의 각각은 상기 하나 이상의 문법 오류 유형의 적어도 하나에 대응함 -;상기 하나 이상의 목표 단어의 적어도 하나에 대하여, 상기 적어도 하나의 프로세서에 의해, 상기 문법 오류 유형에 대하여 훈련된 인공 신경망(artificial neural network) 모델을 이용하여 대응하는 상기 문법 오류 유형에대하여 상기 목표 단어의 분류를 추정하는 단계 - 상기 모델은, (i) 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 문법 오류 유형에 대한 상기 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -; 및상기 적어도 하나의 프로세서에 의해, 상기 목표 단어 및 상기 목표 단어의 추정된 상기 분류에 적어도 부분적으로 기초하여 상기 문장에서 문법 오류를 검출하는 단계를 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 추정하는 단계는,상기 2개의 순환 신경망을 이용하여 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 상기 문맥 벡터를 제공하는 단계; 및상기 피드포워드 신경망을 이용하여 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 문법오류 유형에 대한 상기 목표 단어의 상기 분류값을 제공하는 단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 목표 단어의 상기 문맥 벡터는 상기 목표 단어의 단어 기본형(lemma)에 적어도 부분적으로 기초하여 제공되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 추정하는 단계는,제1 단어 임베딩 벡터(word embedding vector) 세트를 생성하는 단계 - 상기 제1 단어 임베딩 벡터 세트 내의각각의 단어 임베딩 벡터는 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어 중 해당하는 단어에 기초하여 생성됨 -; 및제2 단어 임베딩 벡터 세트를 생성하는 단계 - 상기 제2 단어 임베딩 벡터 세트 내의 각각의 단어 임베딩 벡터는 상기 문장 내의 상기 목표 단어 후의 적어도 하나의 단어 중 해당하는 단어에 기초하여 생성됨 -를 더 포함하는, 문법 오류 검출 방법.공개특허 10-2020-0031154-3-청구항 5 제4항에 있어서,각각의 단어 임베딩 벡터의 디멘전(dimension)의 수는 적어도 100인, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 목표 단어 전의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 전의 모든 단어를 포함하고; 그리고상기 목표 단어 후의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 후의 모든 단어를 포함하는, 문법오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 목표 단어 전의 적어도 하나의 단어의 개수 및/또는 상기 목표 단어 후의 적어도 하나의 단어의 개수는 상기 문법 오류 유형에 적어도 부분적으로 기초하여 결정되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2항에 있어서,상기 추정하는 단계는,상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어 및 상기 목표 단어 후의 적어도 하나의 단어에 적어도부분적으로 기초하여 상기 목표 단어의 문맥 가중치 벡터(context weight vector)를 제공하는 단계; 및상기 문맥 가중치 벡터를 상기 문맥 벡터에 적용하는 단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제4항에 있어서,상기 문맥 벡터를 제공하는 단계는,상기 2개의 순환 신경망 중 제1 순환 신경망을 이용하여 상기 제1 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제1 문맥 벡터를 제공하는 단계;상기 2개의 순환 신경망 중 제2 순환 신경망을 이용하여 상기 제2 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제2 문맥 벡터를 제공하는 단계; 및상기 제1 및 제2 문맥 벡터를 연결(concatenating)하여 상기 문맥 벡터를 제공하는 단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제1 단어 임베딩 벡터 세트는 상기 문장의 시작에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기 제1순환 신경망에 제공되고; 그리고상기 제2 단어 임베딩 벡터 세트는 상기 문장의 마지막에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기제2 순환 신경망에 제공되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,공개특허 10-2020-0031154-4-상기 2개의 순환 신경망의 각각에서의 은닉(hidden) 유닛의 개수는 적어도 300인, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1항에 있어서,상기 피드포워드 신경망은,상기 문맥 벡터에 대한 완전 연결 선형 연산(fully connected linear operation)의 제1 활성화 함수(activation function)를 갖는 제1 층; 및상기 제1 층에 연결되고, 상기 분류값을 생성하기 위한 제2 활성화 함수를 갖는 제2 층을 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서,상기 분류값은 상기 문법 오류 유형과 연관된 복수의 클래스에 대한 상기 목표 단어의 확률 분포인, 문법 오류검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제1항에 있어서,상기 검출하는 단계는,상기 목표 단어의 추정된 분류를 상기 목표 단어의 실제 분류와 비교하는 단계; 및상기 실제 분류가 상기 목표 단어의 추정된 분류와 일치하지 않을 때 상기 문장에서 상기 문법 오류를 검출하는단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제1항에 있어서,상기 문장에서 상기 문법 오류를 검출하는 것에 응답하여, 상기 목표 단어의 추정된 분류에 적어도 부분적으로기초하여 상기 목표 단어의 문법 오류 정정을 제공하는 단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제1항에 있어서,상기 하나 이상의 목표 단어의 각각에 대하여, 상기 문법 오류 유형에 대하여 훈련된 해당하는 인공 신경망 모델을 이용하여 대응하는 상기 문법 오류 유형에 대하여 상기 목표 단어의 해당하는 분류를 추정하고, 상기 목표단어의 추정된 분류를 상기 목표 단어의 실제 분류와 비교하여 상기 목표 단어의 문법 오류 결과를 생성하는 단계;대응하는 상기 문법 오류 유형에 적어도 부분적으로 기초하여 상기 하나 이상의 목표 단어의 상기 문법 오류 결과의 각각에 가중치를 적용하는 단계; 및상기 하나 이상의 목표 단어의 상기 문법 오류 결과 및 상기 가중치에 기초하여 상기 문장의 문법 점수를 제공하는 단계를 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 문법 점수는 상기 문장이 수신되는 사용자와 연관된 정보에 적어도 부분적으로 기초하여 제공되는, 문법공개특허 10-2020-0031154-5-오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서,상기 모델은 원시(native) 훈련 샘플에 의해 훈련되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제1항에 있어서,상기 2개의 순환 신경망 및 상기 피드포워드 신경망은 공동으로(jointly) 훈련되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항에 있어서,상기 모델은,상기 문맥 벡터를 생성하기 위하여 상기 2개의 순환 신경망에 입력될 초기 문맥 벡터 세트를 출력하도록 구성된다른 순환 신경망; 및상기 문맥 벡터에 적용될 문맥 가중치 벡터를 출력하도록 구성된 다른 피드포워드 신경망을 더 포함하는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제20항에 있어서,모든 상기 순환 신경망 및 피드포워드 신경망은 원시 훈련 샘플에 의해 공동으로 훈련되는, 문법 오류 검출 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_22", "content": "메모리; 및상기 메모리에 결합된 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는,문장을 수신하고;하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 상기 문장 내의 하나 이상의 목표 단어(targetword)를 식별하고 - 상기 하나 이상의 목표 단어의 각각은 상기 하나 이상의 문법 오류 유형의 적어도 하나에대응함 -;상기 하나 이상의 목표 단어의 적어도 하나에 대하여, 상기 문법 오류 유형에 대하여 훈련된 인공 신경망(artificial neural network) 모델을 이용하여 대응하는 상기 문법 오류 유형에 대하여 상기 목표 단어의 분류를 추정하고 - 상기 모델은, (i) 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 생성하도록 구성된 2개의 순환 신경망(recurrent neural network) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도부분적으로 기초하여 상기 문법 오류 유형에 대한 상기 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -; 그리고상기 목표 단어 및 상기 목표 단어의 추정된 상기 분류에 적어도 부분적으로 기초하여 상기 문장에서 문법 오류를 검출하도록 구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_23", "content": "공개특허 10-2020-0031154-6-제22항에 있어서,상기 목표 단어의 분류를 추정하기 위하여, 상기 적어도 하나의 프로세서는,상기 2개의 순환 신경망을 이용하여 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 상기 문맥 벡터를 제공하고; 그리고상기 피드포워드 신경망을 이용하여 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 문법오류 유형에 대한 상기 목표 단어의 상기 분류값을 제공하도록구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_24", "content": "제23항에 있어서,상기 목표 단어의 상기 문맥 벡터는 상기 목표 단어의 단어 기본형(lemma)에 적어도 부분적으로 기초하여 제공되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_25", "content": "제23항에 있어서,상기 목표 단어의 분류를 추정하기 위하여, 상기 적어도 하나의 프로세서는,제1 단어 임베딩 벡터(word embedding vector) 세트를 생성하고 - 상기 제1 단어 임베딩 벡터 세트 내의 각각의단어 임베딩 벡터는 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어 중 해당하는 단어에 기초하여 생성됨 -; 그리고제2 단어 임베딩 벡터 세트를 생성하도록 - 상기 제2 단어 임베딩 벡터 세트 내의 각각의 단어 임베딩 벡터는상기 문장 내의 상기 목표 단어 후의 적어도 하나의 단어 중 해당하는 단어에 기초하여 생성됨 -구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_26", "content": "제25항에 있어서,각각의 단어 임베딩 벡터의 디멘전(dimension)의 수는 적어도 100인, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_27", "content": "제22항에 있어서,상기 목표 단어 전의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 전의 모든 단어를 포함하고; 그리고상기 목표 단어 후의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 후의 모든 단어를 포함하는, 문법오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_28", "content": "제22항에 있어서,상기 목표 단어 전의 적어도 하나의 단어의 개수 및/또는 상기 목표 단어 후의 적어도 하나의 단어의 개수는 상기 문법 오류 유형에 적어도 부분적으로 기초하여 결정되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_29", "content": "제23항에 있어서,상기 목표 단어의 분류를 추정하기 위하여, 상기 적어도 하나의 프로세서는,상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어 및 상기 목표 단어 후의 적어도 하나의 단어에 적어도부분적으로 기초하여 상기 목표 단어의 문맥 가중치 벡터(context weight vector)를 제공하고; 그리고공개특허 10-2020-0031154-7-상기 문맥 가중치 벡터를 상기 문맥 벡터에 적용하도록구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_30", "content": "제25항에 있어서,상기 목표 단어의 상기 문맥 벡터를 제공하기 위하여, 상기 적어도 하나의 프로세서는,상기 2개의 순환 신경망 중 제1 순환 신경망을 이용하여 상기 제1 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제1 문맥 벡터를 제공하고;상기 2개의 순환 신경망 중 제2 순환 신경망을 이용하여 상기 제2 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제2 문맥 벡터를 제공하고; 그리고상기 제1 및 제2 문맥 벡터를 연결(concatenating)하여 상기 문맥 벡터를 제공하도록구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_31", "content": "제30항에 있어서,상기 제1 단어 임베딩 벡터 세트는 상기 문장의 시작에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기 제1순환 신경망에 제공되고; 그리고상기 제2 단어 임베딩 벡터 세트는 상기 문장의 마지막에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기제2 순환 신경망에 제공되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_32", "content": "제22항에 있어서,상기 2개의 순환 신경망의 각각에서의 은닉(hidden) 유닛의 개수는 적어도 300인, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_33", "content": "제22항에 있어서,상기 피드포워드 신경망은,상기 문맥 벡터에 대한 완전 연결 선형 연산(fully connected linear operation)의 제1 활성화 함수(activation function)를 갖는 제1 층; 및상기 제1 층에 연결되고, 상기 분류값을 생성하기 위한 제2 활성화 함수를 갖는 제2 층을 포함하는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_34", "content": "제22항에 있어서,상기 분류값은 상기 문법 오류 유형과 연관된 복수의 클래스에 대한 상기 목표 단어의 확률 분포인, 문법 오류검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_35", "content": "제22항에 있어서,문법 오류를 검출하기 위하여, 상기 적어도 하나의 프로세서는,상기 목표 단어의 추정된 분류를 상기 목표 단어의 실제 분류와 비교하고; 그리고상기 실제 분류가 상기 목표 단어의 추정된 분류와 일치하지 않을 때 상기 문장에서 상기 문법 오류를 검출하도공개특허 10-2020-0031154-8-록구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_36", "content": "제22항에 있어서,상기 적어도 하나의 프로세서는,상기 문장에서 상기 문법 오류를 검출하는 것에 응답하여, 상기 목표 단어의 추정된 분류에 적어도 부분적으로기초하여 상기 목표 단어의 문법 오류 정정을 제공하도록 더 구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_37", "content": "제22항에 있어서,상기 적어도 하나의 프로세서는,상기 하나 이상의 목표 단어의 각각에 대하여, 상기 문법 오류 유형에 대하여 훈련된 해당하는 인공 신경망 모델을 이용하여 대응하는 상기 문법 오류 유형에 대하여 상기 목표 단어의 해당하는 분류를 추정하고, 상기 목표단어의 추정된 분류를 상기 목표 단어의 실제 분류와 비교하여 상기 목표 단어의 문법 오류 결과를 생성하고;대응하는 상기 문법 오류 유형에 적어도 부분적으로 기초하여 상기 하나 이상의 목표 단어의 상기 문법 오류 결과의 각각에 가중치를 적용하고; 그리고상기 하나 이상의 목표 단어의 상기 문법 오류 결과 및 상기 가중치에 기초하여 상기 문장의 문법 점수를 제공하도록더 구성되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_38", "content": "제37항에 있어서,상기 문법 점수는 상기 문장이 수신되는 사용자와 연관된 정보에 적어도 부분적으로 기초하여 제공되는, 문법오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_39", "content": "제22항에 있어서,상기 모델은 원시(native) 훈련 샘플에 의해 훈련되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_40", "content": "제22항에 있어서,상기 2개의 순환 신경망 및 상기 피드포워드 신경망은 공동으로(jointly) 훈련되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_41", "content": "제22항에 있어서,상기 모델은,상기 문맥 벡터를 생성하기 위하여 상기 2개의 순환 신경망에 입력될 초기 문맥 벡터 세트를 출력하도록 구성된다른 순환 신경망; 및상기 문맥 벡터에 적용될 문맥 가중치 벡터를 출력하도록 구성된 다른 피드포워드 신경망을 더 포함하는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_42", "content": "공개특허 10-2020-0031154-9-제41항에 있어서,모든 상기 순환 신경망 및 피드포워드 신경망은 원시 훈련 샘플에 의해 공동으로 훈련되는, 문법 오류 검출 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_43", "content": "적어도 하나의 컴퓨터 장치에 의해 실행될 때 상기 적어도 하나의 컴퓨팅 장치가,문장을 수신하는 동작;하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 상기 문장 내의 하나 이상의 목표 단어(targetword)를 식별하는 동작 - 상기 하나 이상의 목표 단어의 각각은 상기 하나 이상의 문법 오류 유형의 적어도 하나에 대응함 -;상기 하나 이상의 목표 단어에 대하여, 상기 문법 오류 유형에 대하여 훈련된 인공 신경망(artificial neuralnetwork) 모델을 이용하여 대응하는 상기 문법 오류 유형에 대하여 상기 목표 단어의 분류를 추정하는 동작 -상기 모델은, (i) 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 문법 오류 유형에 대한 상기 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -; 및상기 목표 단어 및 상기 목표 단어의 추정된 상기 분류에 적어도 부분적으로 기초하여 상기 문장에서 문법 오류를 검출하는 동작을 포함하는 동작들을 수행하게 하는 명령어가 저장된 유형의(tangible) 컴퓨터 판독 가능한 장치."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_44", "content": "적어도 하나의 프로세서에 의해, 문법 오류 유형에 대하여 문장 내의 목표 단어(target word)의 분류를 추정하기 위하여 인공 신경망(artificial neural network) 모델을 제공하는 단계 - 상기 모델은, (i) 상기 문장 내의상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neuralnetwork) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 목표 단어의 분류값을출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -;상기 적어도 하나의 프로세서에 의해, 훈련 샘플 세트를 획득하는 단계 - 상기 훈련 샘플 세트 내의 각각의 훈련 샘플은 상기 문법 오류 유형에 대한 목표 단어를 포함하는 문장과 상기 문법 오류 유형에 대한 상기 목표 단어의 실제 분류를 포함함 -; 및상기 적어도 하나의 프로세서에 의해, 각각의 훈련 샘플에서의 상기 목표 단어의 상기 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 상기 순환 신경망과 연관된 제1 파라미터 세트 및 상기 피드포워드 신경망과 연관된 제2 파라미터 세트를 공동으로(jointly) 조정하는 단계를 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_45", "content": "제44항에 있어서,각각의 훈련 샘플은 문법 오류가 없는 원시(native) 훈련 샘플인, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_46", "content": "제44항에 있어서,상기 순환 신경망은 게이트 순환 유닛(gated recurrent unit(GRU)) 신경망이고, 상기 피드포워드 신경망은 다층퍼셉트론(multi-layer perceptron(MLP)) 신경망인, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_47", "content": "공개특허 10-2020-0031154-10-제44항에 있어서,상기 모델은,상기 문맥 벡터에 적용될 문맥 가중치 벡터(context weight vector)를 출력하도록 구성된 다른 피드포워드 신경망을 더 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_48", "content": "제47항에 있어서,상기 공동으로 조정하는 단계는,각각의 훈련 샘플에서의 상기 목표 단어의 상기 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 상기 제1 및 제2 파라미터 세트 및 상기 다른 피드포워드 신경망과 연관된 제3 파라미터 세트를 공동으로 조정하는 단계를 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_49", "content": "제44항에 있어서,각각의 훈련 샘플에 대하여,제1 단어 임베딩 벡터(word embedding vector) 세트를 생성하는 단계 - 상기 제1 단어 임베딩 벡터 세트에서의각각의 단어 임베딩 벡터는 상기 훈련 샘플에서의 상기 목표 단어 전의 적어도 하나의 단어 중 해당하는 단어에적어도 부분적으로 기초하여 생성됨 -; 및제2 단어 임베딩 벡터 세트를 생성하는 단계 - 상기 제2 단어 임베딩 벡터 세트에서의 각각의 단어 임베딩 벡터는 상기 훈련 샘플에서의 상기 목표 단어 후의 적어도 하나의 단어 중 해당하는 단어에 적어도 부분적으로 기초하여 생성됨 -를 더 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_50", "content": "제49항에 있어서,각각의 단어 임베딩 벡터의 디멘전(dimension)의 수는 적어도 100인, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_51", "content": "제49항에 있어서,상기 목표 단어 전의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 전의 모든 단어를 포함하고; 그리고상기 목표 단어 후의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 후의 모든 단어를 포함하는, 인공신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_52", "content": "제49항에 있어서,각각의 훈련 샘플에 대하여,상기 2개의 순환 신경망 중 제1 순환 신경망을 이용하여 상기 제1 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제1 문맥 벡터를 제공하는 단계;상기 2개의 순환 신경망 중 제2 순환 신경망을 이용하여 상기 제2 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제2 문맥 벡터를 제공하는 단계; 및상기 제1 및 제2 문맥 벡터를 연결(concatenating)하여 상기 문맥 벡터를 제공하는 단계공개특허 10-2020-0031154-11-를 더 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_53", "content": "제52항에 있어서,상기 제1 단어 임베딩 벡터 세트는 상기 문장의 시작에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기 제1순환 신경망에 제공되고; 그리고상기 제2 단어 임베딩 벡터 세트는 상기 문장의 마지막에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기제2 순환 신경망에 제공되는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_54", "content": "제52항에 있어서,상기 제1 및 제2 문맥 벡터는 상기 훈련 샘플에서 상기 문장의 의미 특징(semantic feature)을 포함하지 않는,인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_55", "content": "제44항에 있어서,상기 2개의 순환 신경망의 각각에서의 은닉(hidden) 유닛의 개수는 적어도 300인, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_56", "content": "제44항에 있어서,상기 피드포워드 신경망은,상기 문맥 벡터에 대한 완전 연결 선형 연산(fully connected linear operation)의 제1 활성화 함수(activation function)를 갖는 제1 층; 및상기 제1 층에 연결되고, 상기 분류값을 생성하기 위한 제2 활성화 함수를 갖는 제2 층을 포함하는, 인공 신경망 모델 훈련 방법."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_57", "content": "메모리; 및상기 메모리에 결합된 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는,문법 오류 유형에 대하여 문장 내의 목표 단어(target word)의 분류를 추정하기 위하여 인공 신경망(artificialneural network) 모델을 제공하고 - 상기 모델은, (i) 상기 문장 내의 상기 목표 단어 전의 적어도 하나의 단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -;훈련 샘플 세트를 획득하고 - 상기 훈련 샘플 세트 내의 각각의 훈련 샘플은 상기 문법 오류 유형에 대한 목표단어를 포함하는 문장과 상기 문법 오류 유형에 대한 상기 목표 단어의 실제 분류를 포함함 -; 및각각의 훈련 샘플에서의 상기 목표 단어의 상기 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 상기 순환 신경망과 연관된 제1 파라미터 세트 및 상기 피드포워드 신경망과 연관된 제2 파라미터 세트를 공동으로(jointly) 조정하도록구성되는, 인공 신경망 모델 훈련 시스템.공개특허 10-2020-0031154-12-청구항 58 제57항에 있어서,각각의 훈련 샘플은 문법 오류가 없는 원시(native) 훈련 샘플인, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_59", "content": "제57항에 있어서,상기 순환 신경망은 GRU 신경망이고, 상기 피드포워드 신경망은 MLP 신경망인, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_60", "content": "제57항에 있어서,상기 모델은,상기 문맥 벡터에 적용될 문맥 가중치 벡터(context weight vector)를 출력하도록 구성된 다른 피드포워드 신경망을 더 포함하는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_61", "content": "제60항에 있어서,제1 파라미터 세트 및 제2 파라미터 세트를 공동으로 조정하기 위하여, 상기 적어도 하나의 프로세서는,각각의 훈련 샘플에서의 상기 목표 단어의 상기 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 상기 제1 및 제2 파라미터 세트 및 상기 다른 피드포워드 신경망과 연관된 제3 파라미터 세트를 공동으로 조정하도록구성되는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_62", "content": "제57항에 있어서,상기 적어도 하나의 프로세서는, 각각의 훈련 샘플에 대하여,제1 단어 임베딩 벡터(word embedding vector) 세트를 생성하고 - 상기 제1 단어 임베딩 벡터 세트에서의 각각의 단어 임베딩 벡터는 상기 훈련 샘플에서의 상기 목표 단어 전의 적어도 하나의 단어 중 해당하는 단어에 적어도 부분적으로 기초하여 생성됨 -; 및제2 단어 임베딩 벡터 세트를 생성하도록 - 상기 제2 단어 임베딩 벡터 세트에서의 각각의 단어 임베딩 벡터는상기 훈련 샘플에서의 상기 목표 단어 후의 적어도 하나의 단어 중 해당하는 단어에 적어도 부분적으로 기초하여 생성됨 -더 구성되는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_63", "content": "제62항에 있어서,각각의 단어 임베딩 벡터의 디멘전(dimension)의 수는 적어도 100인, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_64", "content": "제62항에 있어서,상기 목표 단어 전의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 전의 모든 단어를 포함하고; 그리고상기 목표 단어 후의 적어도 하나의 단어는 상기 문장 내의 상기 목표 단어 후의 모든 단어를 포함하는, 인공신경망 모델 훈련 시스템.공개특허 10-2020-0031154-13-청구항 65 제62항에 있어서,상기 적어도 하나의 프로세서는, 각각의 훈련 샘플에 대하여,상기 2개의 순환 신경망 중 제1 순환 신경망을 이용하여 상기 제1 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제1 문맥 벡터를 제공하고;상기 2개의 순환 신경망 중 제2 순환 신경망을 이용하여 상기 제2 단어 임베딩 벡터 세트에 적어도 부분적으로기초하여 상기 목표 단어의 제2 문맥 벡터를 제공하고; 그리고상기 제1 및 제2 문맥 벡터를 연결(concatenating)하여 상기 문맥 벡터를 제공하도록더 구성되는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_66", "content": "제65항에 있어서,상기 제1 단어 임베딩 벡터 세트는 상기 문장의 시작에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기 제1순환 신경망에 제공되고; 그리고상기 제2 단어 임베딩 벡터 세트는 상기 문장의 마지막에 있는 단어의 단어 임베딩 벡터로부터 시작하여 상기제2 순환 신경망에 제공되는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_67", "content": "제65항에 있어서,상기 제1 및 제2 문맥 벡터는 상기 훈련 샘플에서 상기 문장의 의미 특징(semantic feature)을 포함하지 않는,인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_68", "content": "제57항에 있어서,상기 2개의 순환 신경망의 각각에서의 은닉(hidden) 유닛의 개수는 적어도 300인, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_69", "content": "제57항에 있어서,상기 피드포워드 신경망은,상기 문맥 벡터에 대한 완전 연결 선형 연산(fully connected linear operation)의 제1 활성화 함수(activation function)를 갖는 제1 층; 및상기 제1 층에 연결되고, 상기 분류값을 생성하기 위한 제2 활성화 함수를 갖는 제2 층을 포함하는, 인공 신경망 모델 훈련 시스템."}
{"patent_id": "10-2020-7005087", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_70", "content": "적어도 하나의 컴퓨터 장치에 의해 실행될 때 상기 적어도 하나의 컴퓨팅 장치가,문법 오류 유형에 대하여 문장 내의 목표 단어(target word)의 분류를 추정하기 위하여 인공 신경망(artificialneural network) 모델을 제공하는 동작 - 상기 모델은, (i) 상기 문장 내의 상기 목표 단어 전의 적어도 하나의단어와 상기 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 상기 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network) 및 (ii) 상기 목표 단어의 상기 문맥 벡터에 적어도 부분적으로 기초하여 상기 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 포함함 -;공개특허 10-2020-0031154-14-훈련 샘플 세트를 획득하는 동작 - 상기 훈련 샘플 세트 내의 각각의 훈련 샘플은 상기 문법 오류 유형에 대한목표 단어를 포함하는 문장과 상기 문법 오류 유형에 대한 상기 목표 단어의 실제 분류를 포함함 -; 및각각의 훈련 샘플에서의 상기 목표 단어의 상기 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 상기 순환 신경망과 연관된 제1 파라미터 세트 및 상기 피드포워드 신경망과 연관된 제2 파라미터 세트를 공동으로(jointly) 조정하는 동작을 포함하는 동작들을 수행하게 하는 명령어가 저장된 유형의(tangible) 컴퓨터 판독 가능한 장치."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "문법 오류 검출을 위한 방법 및 시스템이 본 명세서에 개시된다. 일례에서, 문장이 수신된다. 문장 내의 적어도 하나의 목표 단어(target word)는 하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 식별된다. 하나 이 상의 목표 단어의 각각은 하나 이상의 문법 오류 유형의 적어도 하나에 대응한다. 하나 이상의 목표 단어의 적어 도 하나에 대하여, 대응하는 문법 오류 유형에 대한 목표 단어의 분류가 문법 오류 유형에 대하여 훈련된 인공 신경망(artificial neural network) 모델을 이용하여 추정된다. 문장에서의 문법 오류가 목표 단어 및 목표 단어 의 추정된 분류에 적어도 부분적으로 기초하여 검출된다."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시 내용은 일반적으로 인공 지능에 관한 것으로, 더욱 상세하게는, 인공 신경망(artificial neural network)을 이용한 문법 오류 정정에 관한 것이다."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자동화된 문법 오류 정정(grammatical error correction(GEC))은 제2 언어로서 영어를 학습하는 수백만의 사람 들을 위한 필수적이고 유용한 도구이다. 작가들은 표준 교정 도구로는 해결되지 않는 다양한 문법 및 어법 실수 를 범한다. 문법 오류 검출 및/또는 정정을 위하여 높은 정밀도(precision)와 리콜(recall) 기능을 갖는 자동화 된 시스템을 개발하는 것이 자연어 프로세스(natural language process(NLP))에서 빠르게 성장하는 영역이 된다. 이러한 자동화된 시스템에 대한 많은 가능성이 있지만, 알려진 시스템은 다양한 문법적 오류 패턴의 제한된 커 버리지 및 정교한 언어적 특징 엔지니어링 또는 인간-주석 훈련 샘플의 요구와 같은 문제에 직면하였다. 본 개시 내용은 일반적으로 인공 지능에 관한 것으로, 더욱 상세하게는, 인공 신경망(artificial neural network)을 이용한 문법 오류 정정에 관한 것이다. 일례에서, 문법 오류 검출 방법이 개시된다. 문장이 수신된다. 문장 내의 하나 이상의 목표 단어(target word) 가 하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 식별된다. 하나 이상의 목표 단어의 각각은 하나 이상의 문법 오류 유형의 적어도 하나에 대응한다. 하나 이상의 목표 단어의 적어도 하나에 대하여, 대응하는 문법 오류 유형에 대한 목표 단어의 분류가 문법 오류 유형에 대하여 훈련된 인공 신경망 모델을 이용하여 추정 된다. 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어와 목표 단어 후의 적어도 하나의 단어에 적어도 부 분적으로 기초하여 목표 단어의 문맥 벡터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network)을 포함한다. 모델은 목표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 문법 오류 유형에 대한 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망(feedforward neural network)을 더 포함한다. 문장 내의 문법 오류가 목표 단어 및 목표 단어의 추정된 분류에 적어도 부분적으로 기초하여 검출된다. 다른 예에서, 인공 신경망 모델 훈련 방법이 제공된다. 문법 오류 유형에 대하여 문장 내의 목표 단어의 분류를 추정하기 위한 인공 신경망 모델이 제공된다. 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어와 목표 단 어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 2개의 순환 신경망을 포함한다. 모델은 목표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망을 더 포함한다. 훈련 샘플 세트가 획득된다. 훈련 샘플 세트 내의 각각의 훈련 샘플은 문법 오류 유형에 대한 목표 단어를 포함하는 문장과 문법 오류 유형에 대한 목표 단어의 실제 분 류를 포함한다. 순환 신경망과 연관된 제1 파라미터 세트 및 피드포워드 신경망과 연관된 제2 파라미터 세트가 각각의 훈련 샘플에서의 목표 단어의 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 공 동으로(jointly) 훈련된다. 다른 예에서, 문법 오류 검출 시스템은 메모리 및 메모리에 결합된 적어도 하나의 프로세서를 포함한다. 적어도 하나의 프로세서는, 문장을 수신하고, 하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 하나 이상의 목표 단어를 식별하도록 구성된다. 하나 이상의 목표 단어의 각각은 하나 이상의 문법 오류 유형의 적어도 하나 에 대응한다. 적어도 하나의 프로세서는, 하나 이상의 목표 단어의 적어도 하나에 대하여, 문법 오류 유형에 대 하여 훈련된 인공 신경망 모델을 이용하여 대응하는 문법 오류 유형에 대한 목표 단어의 분류를 추정하도록 더구성된다. 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어와 목표 단어 후의 적어도 하나의 단어에 적어 도 부분적으로 기초하여 목표 단어의 문맥 벡터를 생성하도록 구성된 2개의 순환 신경망을 포함한다. 모델은 목 표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 문법 오류 유형에 대한 목표 단어의 분류값을 생성하도록 구성된 피드포워드 신경망을 더 포함한다. 적어도 하나의 프로세서는, 목표 단어 및 목표 단어의 추정된 분류에 적어도 부분적으로 기초하여 문장 내의 문법 오류를 검출하도록 더 구성된다. 다른 예에서, 문법 오류 검출 시스템은 메모리 및 메모리에 결합된 적어도 하나의 프로세서를 포함한다. 적어도 하나의 프로세서는, 문법 오류 유형에 대하여 문장 내의 목표 단어의 분류를 추정하기 위한 인공 신경망 모델을 제공하도록 구성된다. 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어와 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 2개의 순환 신경망을 포함한다. 모델은 목표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 목표 단어의 분류값을 출력하도록 구성 된 피드포워드 신경망을 더 포함한다. 적어도 하나의 프로세서는, 훈련 샘플 세트를 획득하도록 더 구성된다. 훈련 샘플 세트 내의 각각의 훈련 샘플은 문법 오류 유형에 대한 목표 단어를 포함하는 문장과 문법 오류 유형 에 대한 목표 단어의 실제 분류를 포함한다. 적어도 하나의 프로세서는, 각각의 훈련 샘플에서의 목표 단어의 실제 분류 및 추정된 분류 사이의 차이에 적어도 부분적으로 기초하여 순환 신경망과 연관된 제1 파라미터 세트 및 피드포워드 신경망과 연관된 제2 파라미터 세트를 공동으로 조정하도록 더 구성된다. 다른 개념은 문법 오류 검출 및 인공 신경망 모델 훈련을 위한 소프트웨어에 관한 것이다. 이 개념에 따라, 소 프트웨어 제품은 적어도 하나의 컴퓨터 판독 가능하고 비일시적인 장치 및 장치가 반송하는 정보를 포함한다. 장치가 반송하는 정보는 요청 또는 동작 파라미터와 연관하는 파라미터에 관한 실행 가능한 명령어일 수 있다. 일례에서, 유형의(tangible) 컴퓨터 판독 가능하고 비일시적인(non-transitory) 장치는 문법 오류 검출을 위하 여 기록된 명령어를 가지며, 명령어는 컴퓨터에 의해 실행될 때 컴퓨터가 일련의 동작을 수행하게 한다. 문장이 수신된다. 문장 내의 하나 이상의 목표 단어는 하나 이상의 문법 오류 유형에 적어도 부분적으로 기초하여 식별 된다. 하나 이상의 목표 단어의 각각은 하나 이상의 문법 오류 유형의 적어도 하나에 대응한다. 하나 이상의 목 표 단어의 적어도 하나에 대하여, 대응하는 문법 오류 유형에 대한 목표 단어의 분류가 문법 오류 유형에 대하 여 훈련된 인공 신경망 모델을 이용하여 추정된다. 모델은, 문장 내의 목표 단어 전의 적어도 하나의 단어와 목 표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 2 개의 순환 신경망을 포함한다. 모델은 목표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 문법 오류 유형에 대한 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망을 더 포함한다. 문장 내의 문법 오류가 목표 단어 및 목표 단어의 추정된 분류에 적어도 부분적으로 기초하여 검출된다. 다른 예에서, 유형의 컴퓨터 판독 가능하고 비일시적인 장치는 인공 신경망 모델 훈련을 위하여 기록된 명령어 를 가지며, 명령어는 컴퓨터에 의해 실행될 때 컴퓨터가 일련의 동작을 수행하게 한다. 문법 오류 유형에 대하 여 문장 내의 목표 단어의 분류를 추정하기 위한 인공 신경망 모델이 제공된다. 모델은 문장 내의 목표 단어 전 의 적어도 하나의 단어와 목표 단어 후의 적어도 하나의 단어에 적어도 부분적으로 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 2개의 순환 신경망을 포함한다. 모델은 목표 단어의 문맥 벡터에 적어도 부분적으로 기초하여 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망을 더 포함한다. 훈련 샘플 세트가 획득된 다. 훈련 샘플 세트 내의 각각의 훈련 샘플은 문법 오류 유형에 대한 목표 단어를 포함하는 문장과 문법 오류 유형에 대한 목표 단어의 실제 분류를 포함한다. 순환 신경망과 연관된 제1 파라미터 세트 및 피드포워드 신경 망과 연관된 제2 파라미터 세트가 각각의 훈련 샘플에서의 목표 단어의 실제 분류 및 추정된 분류 사이의 차이 에 적어도 부분적으로 기초하여 공동으로 훈련된다."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "본 [발명의 내용]은 단지 본 명세서에 설명된 내용에 대한 이해를 제공하기 위하여 일부 실시예들을 예시하는 목적으로만 제공된다. 따라서, 전술한 특징들은 단지 예일 뿐이며, 본 개시 내용에서의 내용의 범위나 사상을 좁히는 것으로 고려되어서는 안 된다. 본 개시 내용의 다른 특징들, 양태들 및 이점들은 이어지는 [발명을 실시 하기 위한 구체적인 내용], [도면] 및 [청구범위]로부터 명백하게 될 것이다."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "다음의 상세한 설명에서, 관련 개시 내용의 완전한 이해를 제공하기 위하여 다양한 특정 상세가 예로서 설명된 다. 그러나, 본 개시 내용이 이러한 상세 없이 실시될 수 있다는 것이 본 기술 분야에서의 통상의 기술자에게 명백하여야 한다. 다른 경우에, 본 개시 내용의 양태들을 불필요하게 흐리게 하는 것을 방지하기 위하여, 잘 알 려진 방법, 절차, 시스템, 컴포넌트 및/또는 회로는 상세 내용 없이 상대적으로 고수준(high-level)으로 설명되 었다. 명세서 및 청구범위 전체에 걸쳐, 용어들은 명시적으로 언급된 의미를 넘어 문맥에서 제안되거나 암시되는 미묘 한 의미를 가질 수 있다. 유사하게, 본 명세서에 사용되는 \"일 실시예/일례에서\"라는 어구는 반드시 동일한 실 시예를 지칭하지는 않으며, 본 명세서에 사용되는 \"다른 실시예/다른 예에서\"라는 어구는 반드시 상이한 실시예 를 지칭하지는 않는다. 예를 들어, 청구된 대상이 전부 또는 일부의 예시적인 실시예들의 조합을 포함하는 것이 의도된다. 일반적으로, 용어는 적어도 부분적으로 문맥에서의 사용으로부터 이해될 수 있다. 예를 들어, 본 명세서에서 사 용되는 \"및\", \"또는\" 또는 \"및/또는\"과 같은 용어는 적어도 부분적으로 이러한 용어들이 사용되는 문맥에 의존 할 수 있는 다양한 의미를 포함할 수 있다. 통상적으로, A, B 또는 C와 같은 목록과 연관되도록 사용되는 경우 의 \"또는\"은 본 명세서에서 베타적 의미로 사용되는 A, B 또는 C뿐만 아니라, 본 명세서에서 포함적 의미로 사 용되는 A, B 및 C를 의미하도록 의도된다. 또한, 본 명세서에서 사용되는 \"하나 이상의\"라는 용어는, 적어도 부 분적으로 문맥에 따라, 단수 의미로의 임의의 특징, 구조 또는 특성의 조합을 설명하는데 사용될 수 있거나, 또 는 복수 의미로의 특징들, 구조들 또는 특성들의 조합을 설명하는데 사용될 수 있다. 유사하게, \"a\", \"an\" 또는\"the\"와 같은 단수 용어, 적어도 부분적으로 문맥에 따라, 단수 사용을 시사하거나 복수 사용을 시사하도록 이 해될 수 있다. 또한, \"~ 기초하여\"라는 용어는 반드시 인자들의 배타적 집합을 시사하도록 의도되지는 않는 것으로 이해될 수 있고, 대신에, 또한, 적어도 부분적으로 문맥에 따라, 반드시 명시적으로 설명될 필요가 없는 추가 인자들의 존재를 허용할 수 있다. 아래에서 상세히 개시되는 바와 같이, 다른 신규한 특징들 중에서, 본 명세서에 개시된 자동화된 GEC 시스템 및 방법은 원시 텍스트(native text) 데이터로부터 훈련될 수 있는 심층 문맥 모델(deep context model)을 이용하 여 문법 오류를 효과적이고 효율적으로 검출 및 정정하는 능력을 제공한다. 일부 실시예에서, 특정 문법 오류 유형에 대하여, 오류 정정 작업은 문법 문맥 표현이 주로 사용 가능한 원시 텍스트 데이터로부터 학습될 수 있 는 분류 문제로서 취급될 수 있다. 전통적인 분류기 방법과 비교하여, 본 명세서에 개시된 시스템 및 방법은 일 반적으로 언어적 지식을 필요로 하지만 모든 문맥 패턴을 커버하지 않을 수 있는 정교한 특징 엔지니어링을 필 요로 하지 않는다. 일부 실시예에서, 피상적인 특징을 이용하는 대신에, 본 명세서에 개시된 시스템 및 방법은 문맥을 표현하기 위한 순환 신경망(recurrent neural network)과 같은 심층 특징을 직접 이용할 수 있다. 일부 실시예에서, 대량의 감독 데이터가 일반적으로 필요하지만 제한적인 크기로 사용 가능한 전통적인 NLP 작업과는 달리, 본 명세서에 개시된 시스템 및 방법은 풍부한 원시 평문 코퍼스(native plain text corpus)를 활용하고 문법 오류를 효율적으로 정정하기 위하여 문맥 표현 및 분류를 종단간(end-to-end) 방식으로 공동으로(jointly) 학습할 수 있다. 추가적인 신규한 특징들은 이어지는 설명에서 부분적으로 설명될 것이며, 부분적으로는 다음에 언급되는 것 및 첨부된 도면을 검토함에 따라 당해 기술 분야에서의 통상의 기술자에게 명백하게 될 것이거나 또는 예들의 생산 또는 동작에 의해 학습될 수 있다. 본 개시 내용의 신규한 특징들은 아래에서 논의되는 상세한 예들에서 설명되 는 방법, 수단 및 조합의 다양한 양태들의 결과 또는 사용에 의해 실현되고 획득될 수 있다. 도 1은 일 실시예에 따른 문법 GEC 시스템을 도시하는 블록도이다. GEC 시스템은 입력 전처리 모듈 , 파싱(parsing) 모듈, 목표 단어 디스패칭(dispatching) 모듈 및 각각이 심층 문맥(deep context)를 이용하여 분류 기반 문법 오류 검출 및 정정을 수행하도록 구성된 복수의 분류 기반 GEC 모듈 을 포함한다. 일부 실시예에서, GEC 시스템은 GEC 시스템의 수행을 더 개선하도록 기계 번역 및 사전 정의 규칙 기반 방법과 같은 다른 GEC 방법을 분류 기반 방법과 결합하기 위하여 파이프라인 아키텍처를 이용하 여 구현될 수 있다. 도 1에 도시된 바와 같이, GEC 시스템은 기계 번역 기반 GEC 모듈, 규칙 기반 GEC 모듈 및 채점(scoring)/정정 모듈을 더 포함할 수 있다. 입력 전처리 모듈은 입력 텍스트를 수신하고 입력 텍스트를 전처리하도록 구성된다. 입력 텍스 트는 적어도 하나의 영어 문장, 예를 들어, 단일 문장, 문단, 글(article) 또는 임의의 텍스트 코퍼스 (text corpus)를 포함할 수 있다. 입력 텍스트는 손글씨(hand writing), 타이핑 또는 복사/붙여넣기를 통 해 직접 수신될 수 있다. 입력 텍스트는, 예를 들어, 음성 인식 또는 화상 인식을 통해, 간접적으로도 수 신될 수 있다. 예를 들어, 임의의 적합한 음성 인식 기술이 음성 입력을 입력 텍스트로 변환하는데 사용될 수 있다. 다른 예에서, 임의의 적합한 광학 문자 인식(optical character recognition(OCR)) 기술이 화상 내에 포함된 텍스트를 입력 텍스트로 변환하는데 사용될 수 있다. 입력 전처리 모듈은 다양한 방식으로 입력 텍스트를 전처리할 수 있다. 일부 실시예에서, 문법 오류 가 일반적으로 특정 문장의 문맥과 연계하여 분석되기 때문에, 입력 전처리 모듈은 각각의 문장이 후속 과 정을 위한 단위로서 취급될 수 있도록 입력 텍스트를 문장으로 분할할 수 있다. 입력 텍스트를 문장 으로 분할하는 것은 문장의 시작 및/또는 끝을 인식함으로써 수행될 수 있다. 예를 들어, 입력 전처리 모듈 은 문장의 끝의 표시자로서 마침표, 세미 콜론, 물음표 또는 느낌표와 같은 소정의 구두점 기호를 검색할 수 있다. 또한, 입력 전처리 모듈은 문장의 시작에 대한 표시자로서 첫 글자가 대문자로 표시된 단어를 검 색할 수 있다. 일부 실시예에서, 입력 전처리 모듈은, 예를 들어, 입력 텍스트 내의 임의의 대문자를 소문자로 변환함으로써, 후속 과정을 용이하게 하기 위하여 입력 텍스트를 소문자화로 바꿀 수 있다. 또한, 일부 실시예에서, 입력 전처리 모듈은 어휘 데이터베이스에 있지 않은 임의의 토큰(token)을 판단하기 위하여 어휘 데이터베이스에 대하여 입력 텍스트 내의 토큰(단어, 구(phrase) 또는 문자열 (text string))을 검사할 수 있다. 일치하지 않은 토큰은 특수 토큰, 예를 들어, 단일 unk 토큰(알려지지 않은 토큰(unknown token))으로서 취급될 수 있다. 어휘 데이터베이스는 GEC 시스템에 의해 처리될 수 있 는 모든 단어를 포함한다. 어휘 데이터베이스에 있지 않은 임의의 단어 또는 다른 토큰은 GEC 시스템(10 0)에 의해 무시되거나 다르게 취급될 수 있다.파싱 모듈은 입력 텍스트의 각각의 문장에서 하나 이상의 목표 단어를 식별하기 위하여 입력 텍스트 를 파싱하도록 구성된다. 통일된 모든 문법 오류를 고려하고 부정확한 텍스트를 정확한 텍스트로 변환하려 고 시도하는 알려진 시스템과는 다르게, GEC 시스템은 아래에서 상세히 설명되는 바와 같이 각각의 특정 문법 오류 유형에 대하여 훈련된 모델을 이용한다. 따라서, 일부 실시예에서, 파싱 모듈은 각각의 목표 단 어가 적어도 하나의 문법 오류 유형에 대응하도록 미리 정의된 문법 오류 유형에 기초하여 각각의 문장에서 텍 스트 토큰으로부터 목표 단어를 식별할 수 있다. 문법 오류 유형은, 관사 오류, 주격 관련 일치(subjective agreement) 오류, 동사 형태 오류, 전치사 오류 및 명사 수 오류를 포함하지만 이에 한정되지 않는다. 문법 오 류 유형이 전술한 예들에 한정되지 않으며 임의의 다른 유형을 포함할 수 있다는 것이 이해되어야 한다. 일부 실시예에서, 파싱 모듈은 각각의 문장을 토큰화하고, GEC 시스템에 알려진 어휘 정보 및 지식을 포함 하는 어휘 데이터베이스와 관련하여 토큰으로부터 목표 단어를 식별할 수 있다. 예를 들어, 주격 관련 일치 오류에 대하여, 파싱 모듈은 비3인칭 단수 현재형 단어 및 3인칭 단수 현재형 단어 맵 관계를 미리 추출할 수 있다. 그 다음, 파싱 모듈은 목표 단어로서 동사의 위치를 찾을 수 있다. 관사 오류에 대하여, 파싱 모듈은 목표 단어로서 명사 및 명사구(명사 단어와 형용사 단어의 조합)의 위치 를 찾을 수 있다. 동사 형태 오류에 대하여, 파싱 모듈은 목표 단어로서 기본형, 동명사 또는 현재 분사, 또는 과거 분사로 있는 동사의 위치를 찾을 수 있다. 전치사 오류에 대하여, 파싱 모듈은 목표 단어로서 전치사의 위치를 찾을 수 있다. 명사 수 오류에 대하여, 파싱 모듈은 목표 단어로서 명사의 위치를 찾을 수 있다. 하나의 단어가 다수의 문법 오류 유형에 대응하는 것으로 파싱 모듈에 의해 식별될 수 있다는 것 이 이해되어야 한다. 예를 들어, 동사는 주격 관련 일치 오류 및 동사 형태 오류에 대하여 목표 단어로서 식별 될 수 있고, 명사 또는 명사구는 관사 오류 및 명사 수 오류에 대하여 목표 단어로서 식별될 수 있다. 또한, 목 표 단어가 명사구와 같은 다수의 단어의 조합인 구를 포함할 수 있다는 것이 이해되어야 한다. 일부 실시예에서, 각각의 문법 오류 유형에 대하여, 파싱 모듈은 각각의 목표 단어의 실제 분류를 결정하 도록 구성될 수 있다. 파싱 모듈은 목표 단어의 실제 분류값으로서 대응하는 문법 오류 유형에 대하여 각 각의 목표 단어에 원 라벨(original label)을 할당할 수 있다. 예를 들어, 주격 관련 일치 오류에 대하여, 동사 의 실제 분류는 3인칭 단수 현재형 또는 기본형이다. 파싱 모듈은 목표 단어에 원 라벨, 예를 들어, 목표 단어가 3인칭 단수 현재형인 경우에 \"1\" 또는 목표 단어가 기본형인 경우에 \"0\"을 목표 단어에 할당할 수 있다. 관사 오류에 대하여, 목표 단어의 실제 분류는 \"a/an\", \"the\" 또는 \"관사 없음\"일 수 있다. 파싱 모듈은 각각의 목표 단어의 실제 분류를 결정하기 위하여 목표 단어(명사 단어 또는 명사구)의 앞에 있는 관사를 검사 할 수 있다. 동사 형태 오류에 관하여, 목표 단어(예를 들어, 동사)의 실제 분류는 \"기본형\", \"동명사 또는 현 재 분사\" 또는 \"과거 분사\"일 수 있다. 전치사 오류에 대하여, 가장 자주 사용되는 전치사가 실제 분류로서 파 싱 모듈에 의해 사용될 수 있다. 일부 실시예에서, 실제 분류는 다음의 11개의 원 라벨을 포함한다: \"about\", \"at\", \"by\", \"for\", \"from\", \"in\", \"of\", \"on\", \"to\", \"until\", \"with\" 및 \"against\". 명사 수 오류 에 관하여, 목표 단어(예를 들어, 명사)의 실제 분류는 단수형 또는 복수형일 수 있다. 일부 실시예에서, 파싱 모듈은 어휘 데이터베이스와 관련된 음성 부분(part of speech(PoS)) 태그에 기초하여 대응하는 문법 오류 유형에 대하여 각각의 목표 단어의 원 라벨을 결정할 수 있다. 목표 단어 디스패칭 모듈은 대응하는 문법 오류 유형에 대하여 각각의 목표 단어를 분류 기반 GEC 모듈 에 디스패치하도록 구성된다. 일부 실시예에서, 각각의 문법 오류 유형에 대하여, ANN 모델은 대응하 는 분류 기반 GEC 모듈에 의해 독립적으로 훈련되어 사용된다. 따라서, 각각의 분류 기반 GEC 모듈은 특정 문법 오류 유형과 연관되고, 동일한 문법 오류 유형에 대하여 목표 단어를 다루도록 구성된다. 예를 들어, (전치사 오류 유형에 대하여) 전치사인 목표 단어에 대하여, 목표 단어는 전치사 오류를 다루는 분류 기반 GEC 모듈에 전치사를 전송할 수 있다. 하나의 단어가 다수의 문법 오류 유형에 대한 목표 단어로서 결정될 수 있기 때문에, 목표 단어 디스패칭 모듈은 동일한 목표 단어를 다수의 분류 기반 GEC 모듈로 전송할 수 있다는 것이 이해되어야 한다. 또한, 일부 실시예에서, GEC 시스템에 의해 각각의 분류 기반 GEC 모듈 에 할당된 리소스는 동일하지 않을 수 있다는 것도 이해되어야 한다. 예를 들어, 소정의 사용자 집단 (cohort) 내에서 또는 특정 사용자에 대하여 각각의 문법 오류 유형이 발생한 빈도에 따라, 목표 단어 디스패칭 모듈은 가장 빈번하게 발생된 문법 오류 유형에 대한 목표 단어를 가장 높은 우선 순위로 디스패치할 수 있다. 큰 크기, 예를 들어, 많은 수의 문장 및/또는 각각의 문장 내의 많은 수의 목표 단어를 갖는 입력 텍스트 에 대하여, 목표 단어 디스패칭 모듈은 대기 시간(latency)을 감소시키기 위하여 각각의 분류 기반 GEC 모듈의 작업 부하의 관점에서 최적 방식으로 각각의 문장 내의 각각의 목표 단어의 처리를 스케줄링할 수 있다.각각의 분류 기반 GEC 모듈은 대응하는 문법 오류 유형에 대하여 훈련된 대응하는 ANN 모델을 포함한 다. 분류 기반 GEC 모듈은 대응하는 ANN 모델을 이용하여 대응하는 문법 오류 유형에 대하여 목표 단 어의 분류를 추정하도록 구성된다. 아래에서 상세하게 설명되는 바와 같이, 일부 실시예에서, ANN 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어 및 목표 단어 후의 적어도 하나에 기초하여 목표 단어의 문맥 벡 터(context vector)를 출력하도록 구성된 2개의 순환 신경망(recurrent neural network)을 포함한다. ANN 모델 은 목표 단어의 문맥 벡터에 기초하여 문법 오류 유형에 대한 목표 단어의 분류값을 출력하도록 구성된 피 드포워드 신경망(feedforward neural network)을 더 포함한다. 분류 기반 GEC 모듈은 목표 단어 및 목표 단어의 추정된 분류에 기초하여 문장에서의 문법 오류를 검출하 도록 더 구성된다. 위에서 설명된 바와 같이, 일부 실시예에서, 각각의 목표 단어의 실제 분류는 파싱 모듈 에 의해 결정될 수 있다. 그 다음, 분류 기반 GEC 모듈은 목표 단어의 추정된 분류를 목표 단어의 실 제 분류와 비교하고, 실제 분류가 목표 단어의 추정된 분류와 일치하지 않을 때 문장 내의 문법 오류를 검출할 수 있다. 예를 들어, 소정의 문법 오류 유형에 대하여, 대응하는 ANN 모델은 목표 단어를 둘러싸는 가변 길이 문맥의 임베딩(embedding) 함수를 학습할 수 있으며, 대응하는 분류 기반 GEC 모듈은 문맥 임베딩을 이용하여 목표 단어의 분류를 예측할 수 있다. 예측된 분류 라벨이 목표 단어의 원 라벨과 상이하다면, 목표 단 어는 오류로서 표시될 수 있고, 예측은 정정으로서 사용될 수 있다. 도 1에 도시된 바와 같이, 일부 실시예에서, 다양한 문법 오류 유형에 대한 문법 오류를 동시에 검출하기 위하 여 다수의 분류 기반 GEC 모듈이 GEC 시스템에서 병렬로 적용될 수 있다. 위에서 설명된 바와 같이, GEC 시스템의 리소스는 각각의 문법 오류 유형의 발생 빈도에 기초하여 상이한 문법 오류 유형에 할당될 수 있다. 예를 들어, 다른 것보다 더 자주 발생하는 문법 오류 유형을 다루기 위하여 더 많은 계산 리소스가 GEC 시스템에 의해 할당될 수 있다. 리소스의 할당은 각각의 분류 기반 GEC 모듈의 작업 부하 및/또 는 빈도 변화의 관점에서 동적으로 조정될 수 있다. 기계 번역 기반 GEC 모듈은 구문 기반 기계 번역, 신경망 기반 기계 번역 등과 같은 통계적 기계 번역에 기초하여 각각의 문장에서 하나 이상의 문법 오류를 검출하도록 구성된다. 일부 실시예에서, 기계 번역 기반 GEC 모듈은 문장에 대하여 확률을 할당하는 언어 서브 모듈과 조건부 확률을 할당하는 번역 서브 모듈을 갖는 모듈을 포함한다. 언어 서브 모듈은 목표 언어에 설정된 단일 언어(monolingual) 훈련 데이터를 이용하여 훈련될 수 있다. 번역 서브 모듈의 파라미터들은 병렬 훈련 데이터 세트, 즉 외국어 문장들과 목표 언어로의 이 들의 대응하는 번역의 세트로부터 추정될 수 있다. GEC 시스템의 파이프라인 아키텍처에서, 기계 번역 기 반 GEC 모듈이 분류 기반 GEC 모듈의 출력에 적용될 수 있거나, 분류 기반 GEC 모듈이 기계 번 역 기반 GEC 모듈에 적용될 수 있다는 것이 이해되어야 한다. 또한, 일부 실시예에서, 기계 번역 기반 GEC 모듈을 파이프라인에 추가함으로써, 기계 번역 기반 GEC 모듈이 능가할 수 있는 소정의 분류 기반 GEC 모듈은 파이프라인에 포함되지 않을 수 있다. 규칙 기반 GEC 모듈은 미리 정의된 규칙에 기초하여 각각의 문장에서 하나 이상의 문법 오류를 검출하도록 구성된다. 파이프라인에서의 규칙 기반 GEC 모듈의 위치가 도 1에 도시된 마지막으로 한정되지 않고 제1 검출 모듈로서 파이프라인의 시작에 또는 분류 기반 GEC 모듈 및 기계 번역 기반 GEC 모듈 사이 있을 수 있다는 것이 이해되어야 한다. 또한, 일부 실시예에서, 구두점, 철자 및 대소문자 오류와 같은 다른 기계적 오류가 규칙 기반 GEC 모듈에 의해 미리 정해진 규칙을 이용하여 검출되고 정정될 수 있다. 채점/정정 모듈은 파이프라인으로부터 수신된 문법 오류 결과에 기초하여 입력 텍스트의 정정된 텍스 트 및/또는 문법 점수를 제공하도록 구성된다. 분류 기반 GEC 모듈을 예로 들면, 추정된 분류가 실제 분류와 일치하지 않아 문법 오류를 갖는 것으로 검출된 각각의 목표 단어에 대하여, 목표 단어의 문법 오류 정 정은 목표 단어의 추정된 분류에 기초하여 채점/정정 모듈에 의해 제공될 수 있다. 입력 텍스트를 평 가하기 위하여, 채점/정정 모듈은 또한 채점 기능을 이용하여 파이프라인으로부터 수신된 문법 오류 결과 에 기초하여 문법 점수를 제공할 수 있다. 일부 실시예에서, 채점 기능은 상이한 유형의 문법 오류가 문법 점수에 대하여 상이한 레벨의 영향을 가질 수 있도록 각각의 문법 오류 유형에 가중치를 할당할 수 있다. 가중치는 문법 오류 결과를 평가하는데 있어서 가중치가 주어진 인자로서 정밀도(precision) 및 리콜(recall)에 할당될 수 있다. 일부 실시예에서, 입력 텍스트를 제공하는 사용자도 채점 기능에 의해 고려될 수 있다. 예를 들어, 가중치는 상이한 사용자에게 대하여 상이할 수 있거나, 또는 사용자의 정보(예를 들어, 모국어, 거 주지, 교육 수준, 과거의 점수, 나이 등)가 채점 기능에 고려될 수 있다. 도 2는 도 1의 GEC 시스템에 의해 수행되는 자동화된 문법 오류 정정의 일례에 대한 도면이다. 도 2에 도 시된 바와 같이, 입력 텍스트는 복수의 문장을 포함하고 사용자 ID의 의해 식별되는 사용자로부터 수신된다. 각각이 대응하는 문법 오류 유형에 대하여 개별적으로 훈련되는 복수의 ANN 모델을 갖는 GEC 시 스템을 통과한 후에, 문법 점수를 갖는 정정된 텍스트가 사용자에게 제공된다. 예를 들어, 입력 텍스 트에서의 문장 \"it will just adding on their misery\"에서, 동사 \"adding\"이 GEC 시스템에 의해 동 사 형태 오류에 대한 목표 단어로서 식별된다. 목표 단어 \"adding\"의 실제 분류는 동명사 또는 현재 분사이다. GEC 시스템은 동사 형태 오류에 대하여 훈련된 ANN 모델을 적용하고 목표 단어 \"adding\"의 분류가 기 본형 \"add\"라고 추정한다. 추정된 분류가 목표 단어 \"adding\"의 실제 분류와 일치하지 않기 때문에, 동사 형태 문법 오류가 GEC 시스템에 의해 검출되고, 이는 사용자의 개인 정보 및/또는 동사 형태 오류 유형에 적용 된 가중치의 관점에서 문법 점수에 영향을 미친다. 또한, 목표 단어 \"adding\"의 추정된 분류는 정정된 텍스트 에서 \"adding\"을 교체하도록 정정 \"add\"를 제공하기 위하여 GEC 시스템에 의해 사용된다. 동사 형태 오류에 대한 동일한 ANN 모델이 입력 텍스트에서 \"dishearten\"과 같은 다른 동사 형태 오류를 검출하 고 \"disheartening\"과 같이 정정하기 위하여 GEC 시스템에 의해 사용된다. 다른 문법 오류 유형에 대한 ANN 모델이 다른 유형의 문법 오류를 검출하기 위하여 GEC 시스템에 의해 사용된다. 예를 들어, 전치 사 오류에 대한 ANN 모델이 입력 텍스트에서 \"for\" 및 \"to\"와 같은 전치사 오류를 검출하고 \"in\" 및 \"on\"과 같이 정정하기 위하여 GEC 시스템에 의해 사용된다. 도 3은 일 실시예에 따른 문법 오류 정정 방법의 일례를 도시하는 순서도이다. 방법은 하드웨어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트웨어(예를 들어, 처리 장치 상에서 실행 되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수행될 수 있다. 모든 단계들이 본 명세서 에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해되어야 한다. 또한, 당해 업계의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거나, 또는 도 3에 도시된 것과 다른 순서로 수행될 수 있다. 방법은 도 1을 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는다. 302 에서, 입력 텍스트가 수신된다. 입력 텍스트는 적어도 하나의 문장을 포함한다. 입력 텍스트는, 예를 들어, 손 글씨, 타이핑 또는 복사/붙여넣기를 통해 직접적으로, 또는, 예를 들어, 음성 인식 또는 화상 인식으로부터 간 접적으로 수신될 수 있다. 304에서, 수신된 입력 텍스트는, 문장들로 분할되는 것과 같이 전처리, 즉 텍스트 토 큰화될 수 있다. 일부 실시예에서, 전처리는 입력 테스트가 소문자로 되도록 대문자를 소문자로 변환하는 것으 로 포함할 수 있다. 일부 실시예에서, 전처리는 어휘 데이터베이스에 있지 않은 입력 텍스트 내의 임의의 토큰을 식별하고 특수 토큰으로서 이를 나타내는 것을 포함할 수 있다. 302 및 304는 GEC 시스템의 입력 전처리 모듈에 의해 수행될 수 있다. 306에서, 전처리된 입력 텍스트는 각각의 문장 내의 하나 이상의 목표 단어를 식별하기 위하여 파싱된다. 목표 단어는 각각의 목표 단어가 적어도 하나의 문법 오류 유형에 대응하도록 문법 오류 유형에 기초하여 텍스트 토 큰으로부터 식별될 수 있다. 문법 오류 유형은 관사 오류, 주격 관련 일치 오류, 동사 형태 오류, 전치사 오류 및 명사 수 오류를 포함하지만 이에 한정되지 않는다. 일부 실시예에서, 대응하는 문법 오류 유형에 대한 각각 의 목표 단어의 실제 분류가 결정된다. 결정은, 예를 들어, PoS 태그 및 문장 내의 텍스트 토큰에 기초하여, 자 동으로 이루어질 수 있다. 일부 실시예에서, 목표 단어 식별 및 실제 분류 결정은 Stanford corenlp 툴과 같은 NLP 툴에 의해 수행될 수 있다. 306은 GEC 시스템의 파싱 모듈에 의해 수행될 수 있다. 308에서, 각각의 목표 단어는 대응하는 분류 기반 GEC 모듈에 디스패치된다. 각각의 분류 기반 GEC 모듈 은, 예를 들어, 원시 훈련 샘플(native training sample)에 관하여 대응하는 문법 오류 유형에 대하여 훈 련된 ANN 모델을 포함한다. 308은 GEC 시스템의 목표 단어 디스패칭 모듈에 의해 수행될 수 있 다. 310에서, 각각의 문장 내의 하나 이상의 문법 오류가 ANN 모델을 이용하여 검출된다. 일부 실시예에서, 각각의 목표 단어에 대하여, 대응하는 문법 오류 유형에 대한 목표 단어의 분류는 대응하는 ANN 모 델을 이용하여 추정될 수 있다. 그 다음, 문법 오류는 목표 단어와 목표 단어의 추정된 분류에 기초하여 검출될 수 있다. 예를 들어, 추정이 원 라벨과 상이하고 확률이 미리 정의된 임계값보다 더 크다면, 문법 오류 가 발견될 것으로 여겨진다. 310은 GEC 시스템의 분류 기반 GEC 모듈에 의해 수행될 수 있다. 312에서, 각각의 문장에서의 하나 이상의 문법 오류가 기계 번역을 이용하여 검출될 수 있다. 312는 GEC 시스템 의 기계 번역 기반 GEC 모듈에 의해 수행될 수 있다. 314에서, 각각의 문장에서의 하나 이상의 문법 오류가 미리 정의된 규칙에 의하여 검출될 수 있다. 314는 GEC 시스템의 규칙 기반 GEC 모듈에 의해 수행될 수 있다. 일부 실시예에서, GEC 시스템의 수행을 더 개선하기 위하여 파이프라인 아키텍처가 임의 의 적합한 기계 번역 및/또는 사전 정의 규칙 기반 방법을 본 명세서에 설명된 분류 기반 방법과 결합하는데 사용될 수 있다. 316에서, 검출된 문법 오류에 대한 정정 및/또는 입력 텍스트의 문법 점수가 제공된다. 일부 실시예에서, 대응 하는 문법 오류 유형에 기초하여 가중치가 목표 단어의 각각의 문법 오류 결과에 적용될 수 있다. 각각의 문장 의 문법 점수는 각각의 문법 오류 결과에 적용된 가중치뿐만 아니라 문법 오류 결과 및 문장 내의 목표 단어에 기초하여 결정된다. 또한, 일부 실시예에서, 문법 점수는 문장이 수신되는 사용자와 연관된 정보에 기초하여 제 공될 수 있다. 검출된 문법 오류에 대한 정정에 관하여, 일부 실시예에서, 대응하는 문법 오류 유형에 대한 목 표 단어의 추정된 분류가 정정을 생성하는데 사용될 수 있다. 정정 및 문법 점수가 반드시 함께 제공될 필요가 없다는 것이 이해되어야 한다. 316은 GEC 시스템의 채점/정정 모듈에 의해 수행될 수 있다. 도 4는 일 실시예에 따른 도 1의 GEC 시스템의 분류 기반 GEC 모듈의 일례를 도시하는 블록도이다. 위에서 설명된 바와 같이, 분류 기반 GEC 모듈은 문장 내의 목표 단어를 수신하고 목표 단어의 대응 하는 문법 오류 유형에 대한 ANN 모델을 이용하여 목표 단어의 분류를 추정하도록 구성된다. 또한, 문장 내의 목표 단어는 목표 단어 라벨링 유닛(예를 들어, 파싱 모듈 내)에 의해 수신된다. 목표 단 어 라벨링 유닛은, 예를 들어, PoS 태그 또는 문장 내의 텍스트 토큰에 기초하여 목표 단어의 실제 분류(예를 들어, 원 라벨)을 결정하도록 구성된다. 분류 기반 GEC 모듈은 목표 단어의 추정된 분류 및 실 제 분류에 기초하여 문법 오류 결과를 제공하도록 더 구성된다. 도 4에 도시된 바와 같이, 분류 기반 GEC 모듈 은 초기 문맥 생성 유닛, 심층 문맥 표현 유닛, 분류 유닛, 어텐션(attention) 유닛 및 분류 비교 유닛을 포함한다. 초기 문맥 생성 유닛은 문장 내의 목표 단어(문맥 단어)를 둘러싸는 단어들에 기초하여 목표 단어의 복수의 초기 문맥 벡터(초기 문맥 매트릭스) 세트를 생성하도록 구성된다. 일부 실시예에서, 초기 문맥 벡터 세 트는 문장 내의 목표 단어 전의 적어도 하나의 단어(포워드 문맥 단어)에 기초하여 생성된 포워드 초기 문 맥 벡터(포워드 초기 문맥 매트릭스) 세트 및 문장 내의 목표 단어 후의 적어도 하나의 단어(백워드 문맥 단어)에 기초하여 생성된 백워드 초기 문맥 벡터(백워드 초기 문맥 매트릭스) 세트를 포함한다. 각각의 초기 문 맥 벡터는 문장 내의 하나의 문맥 단어를 나타낸다. 일부 실시예에서, 초기 문맥 벡터는 원-핫(one-hot) 벡터의 크기(디멘전(dimension))가 어휘 크기(예를 들어, 어휘 데이터베이스에서의)와 동일하도록 원-핫 인코딩에 기초하여 단어를 표현하는 원-핫 벡터일 수 있다. 일부 실시예에서, 초기 문맥 벡터는 문맥 단어의 단어 임베딩 벡터(word embedding vector)와 같은 어휘 크기보다 작은 디멘전을 갖는 저디멘전 벡터일 수 있다. 예를 들어, 단어 임베딩 벡터는 word2vec 또는 Glove와 같지만 이에 한정되지 않는 임의의 적합한 포괄적 단어 임베딩 접근 방식에 의해 생성될 수 있다. 일부 실시예에서, 초기 문맥 생성 유닛은 하나 이상의 초기 문맥 벡터 세트 를 출력하도록 구성된 하나 이상의 순환 신경망을 사용할 수 있다. 초기 문맥 생성 유닛에 의해 사용되는 순환 신경망(들)은 ANN 모델의 일부일 수 있다. 포워드 또는 백워드 초기 문맥 벡터 세트를 생성하기 위하여 사용되는 문맥 단어의 수는 제한되지 않는다는 것 이 이해되어야 한다. 일부 실시예에서, 포워드 초기 문맥 벡터 세트는 문장 내의 목표 단어) 전의 모든 단 어에 기초하여 생성되고, 백워드 초기 문맥 벡터 세트는 문장 내의 목표 단어 후의 모든 단어에 기초하여 생성된다. 각각의 분류 기반 GEC 모듈 및 대응하는 ANN 모델이 특정 문법 오류 유형을 다루고, 상이 한 유형의 문법 오류의 정정이 상이한 단어 거리로부터의 종속성을 필요로 하기 때문에(예를 들어, 전치사는 목 표 단어 근처의 단어에 의해 결정되는 반면, 동사의 상태에는 동사로부터 멀리 있는 주어가 영향을 미칠 수 있 다), 일부 실시예에서, 포워드 또는 백워드 초기 문맥 벡터 세트를 생성하는데 사용되는 문맥 단어의 수(즉, 윈 도우 크기)는 분류 기반 GEC 모듈 및 대응하는 ANN 모델과 연관된 문법 오류 유형에 기초하여 결정될 수 있다. 일부 실시예에서, 초기 문맥 벡터는 목표 단어 자체의 단어 기본형(lemma)에 기초하여 생성될 수 있다. 단어 기 본형은 단어의 기본 형태이다(예를 들어, \"walk\", \"walks\", \"walked\", \"walking\"인 단어들은 모두 동일한 단어 기본형 \"walk\"를 가진다). 예를 들어, 문맥 단어(즉, 문장 내의 목표 단어를 둘러싸는 단어들)에 더하여 명사 수 오류와 연관되는 분류 기반 GEC 모듈 및 대응하는 ANN 모델에 대하여, 목표 단어가 단수 형 태인지 또는 복수 형태인지의 여부가 그 자체에 밀접하게 관련되기 때문에, 목표 명사 단어의 단어 기본형 형태 가 추출 문맥 정보로서 초기 단어 기본형 문맥 벡터의 형태로 도입될 수 있다. 일부 실시예에서, 목표 단어의 단어 기본형의 초기 문맥 벡터는 포워드 초기 문맥 벡터 세트의 일부 또는 백워드 초기 문맥 벡터 세트의 일부 일 수 있다. 일부 알려진 GEC 시스템에서, 언어의 복잡성 때문에 모든 상황을 커버하기 어려운 특징 벡터(feature vector)를 생성하기 위하여 의미 특징(semantic feature)이 설계되어 문장으로부터 수동으로 추출될 필요가 있으며, 이는 언어의 복잡성 때문에 모든 상황을 커버하기는 어렵다. 대조적으로, 문장 내의 목표 단어의 문맥 단어가 초기 문맥 정보(예를 들어, 초기 문맥 벡터 형태)로서 직접 사용되기 때문에, 복잡한 특징 엔지니어링이 본 명 세서에 개시된 분류 기반 GEC 모듈에 의해 요구되지 않으며, 아래에서 상세히 설명되는 바와 같이, 심층 문맥 특징 표현이 종단간 방식으로 분류와 공동으로 학습될 수 있다. 도 5를 참조하면, 본 예에서, 문장은 목표 단어 i를 포함하는 n개의 단어 1 내지 n으로 이루어진다. 목표 단어 i 전의 각각의 단어, 즉 단어 1, 단어 2, ... 또는 단어 i-1에 대하여, 대응하는 초기 문맥 벡터 1, 2 또는 i-1 이 생성된다. 초기 문맥 벡터 1, 2, ... 및 i-1은 이들이 목표 단어 i 전의 단어로부터 생성되고 순방향으로(즉, 문장의 시작, 즉 맨 처음의 단어 1로부터) 이후 스테이지로 공급되기 때문에 \"포워드\" 벡터이다. 목표 단어 i 후의 각각의 단어, 즉 단어 i+1, 단어 i+2, ... 또는 단어 n에 대하여, 대응하는 초기 문맥 벡터 i+1, i+2 또는 n이 생성된다. 초기 문맥 벡터 n, ..., i+2 및 i+1은 이들이 목표 단어 i 후의 단어로 부터 생성되고 역방향으로(즉, 문장의 마지막, 즉 마지막 단어 n으로부터) 이후 스테이지로 공급되기 때문에 \" 백워드\" 벡터이다. 본 예에서, 포워드 초기 문맥 벡터 세트는 단어 임베딩의 디멘전과 동일한 개수의 열(column) 과 목표 단어 i 전의 단어 수와 동일한 개수의 행(row)을 갖는 포워드 초기 문맥 매트릭스로서 표현될 수 있다. 포워드 초기 문 맥 매트릭스에서의 첫 번째 행은 첫 번째 단어 1의 단어 임베딩 벡터일 수 있고, 포워드 초기 문맥 매트릭스에 서의 마지막 행은 목표 단어 i 바로 전의 단어 i-1의 단어 임베딩 벡터일 수 있다. 백워드 초기 문맥 벡터 세트 는 단어 임베딩 디멘전과 동일한 개수의 열과 목표 단어 i 후의 단어 수와 동일한 개수의 행을 갖는 백워드 초 기 문맥 매트릭스로서 표현될 수 있다. 백워드 초기 문맥 매트릭스에서의 첫 번째 행은 마지막 단어 n의 단어 임베딩 벡터일 수 있고, 백워드 초기 문맥 매트릭스에서의 마지막 행은 목표 단어 i 바로 후의 단어 i+1의 단어 임베딩 벡터일 수 있다. 각각의 단어 임베딩 벡터의 디멘전의 수는 적어도 100, 예를 들어, 300일 수 있다. 또 한, 본 예에서, 단어 기본형 초기 문맥 벡터(lem)(예를 들어, 단어 임베딩 벡터)는 목표 단어 i의 단어 기본형 에 기초하여 생성될 수 있다. 도 4를 다시 참조하면, 심층 문맥 표현 유닛은, ANN 모델을 이용하여, 문장 내의 문맥 단어, 예 를 들어, 초기 문맥 생성 유닛에 의해 생성된 포워드 및 백워드 초기 문맥 벡터 세트에 기초하여 목표 단 어의 문맥 벡터를 제공하도록 구성된다. 분류 유닛은, ANN 모델을 이용하여, 문장 내의 목표 단 어의 심층 문맥 표현, 예를 들어, 심층 문맥 표현 유닛에 의해 생성된 문맥 벡터에 기초하여 문법 오류 유 형에 대한 목표 단어의 분류값을 제공하도록 구성된다. 도 6을 참조하면, ANN 모델의 일례의 개략도가 일 실시예에 따라 도시된다. 본 예에서, ANN 모델은 심층 문맥 표현 유닛에 의해 사용될 수 있는 심층 문맥 표현 서브 모델과 분류 유닛에 의해 사 용될 수 있는 분류 서브 모델을 포함한다. 심층 문맥 표현 서브 모델과 분류 서브 모델은 종단 간 방식으로 공동으로 훈련될 수 있다. 심층 문맥 표현 서브 모델은 포워드 순환 신경망과 백워드 순 환 신경망인 2개의 순환 신경망을 포함한다. 각각의 순환 신경망(606 또는 608)은 LSTM(long short-term memory) 신경망, 게이트 순환 유닛(gated recurrent unit(GRU)) 신경망 또는 은닉(hidden) 유닛 사이의 연결이 방향성 사이클(directed cycle)을 형성하는 임의의 다른 적합한 순환 신경망일 수 있다. 순환 신경망(606, 608)은 문장 내의 목표 단어의 문맥 단어로부터 생성된 초기 문맥 벡터에 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된다, 일부 실시예에서, 포워드 순환 신경망은 포워드 초기 문맥 벡터 세트를 수신하고 포워드 초기 문맥 벡터 세트에 기초하여 목표 단어의 포워드 문맥 벡터를 제공하도록 구성된다. 포워드 순환 신경망은 순방향으로 포워드 초기 문맥 벡터 세트를 공급받을 수 있다. 백워드 순 환 신경망은 백워드 초기 문맥 벡터 세트를 수신하고 백워드 초기 문맥 벡터 세트에 기초하여 목표 단어의 백워드 문맥 벡터를 제공하도록 구성된다. 백워드 순환 신경망은 역방향으로 백워드 초기 문맥 벡터 세트 를 공급받을 수 있다. 일부 실시예에서, 포워드 및 백워드 초기 문맥 벡터 세트는 위에서 설명된 바와 같은 단 어 임베딩 벡터일 수 있다. 일부 실시예에서, 목표 단어의 단어 기본형 초기 문맥 벡터가 포워드 순환 신경망 및/또는 백워드 순환 신경망으로 공급되어 포워드 문맥 벡터 및/또는 백워드 문맥 벡터를 생성할 수 있다는 것이 이해되어야 한다. 이제 도 5를 참조하면, 본 예에서, 포워드 순환 신경망은 순방향으로 포워드 초기 문맥 벡터 세트(예를 들어, 포워드 초기 문맥 매트릭스의 형태)를 공급받아 포워드 문맥 벡터 for를 생성한다. 백워드 순환 신경망은 역방 향으로 백워드 초기 문맥 벡터 세트(예를 들어, 백워드 초기 문맥 매트릭스의 형태)를 공급받아 백워드 문맥 벡터 back을 생성한다. 일부 실시예에서, 단어 기본형 초기 문맥 벡터 lem이 포워드 순환 신경망 및/또는 백워드 순환 신경망으로 공급될 수 있다는 것이 이해되어야 한다. 포워드 및 백워드 순환 신경망의 각각 내의 은닉 유 닛의 개수는 적어도 300개, 예를 들어 600개이다. 본 예에서, 그 다음, 목표 단어 i의 심층 문맥 벡터 i가 포워 드 문맥 벡터 for 및 백워드 문맥 벡터 back을 연결(concatenating)하여 생성된다. 심층 문맥 벡터 i는 목표 단 어 i를 둘러싸는 문맥 단어 i 내지 i-1 및 문맥 단어 i+1 내지n(그리고 일부 실시예에서는 목표 단어 i의 단어 기본형)에 기초하여 목표 단어 i의 심층 문맥 정보를 표현한다. 다른 말로 하면, 심층 문맥 벡터 i는 목표 단어 i 주위의 연합 문장 문맥(joint sentential context)의 임베딩으로서 고려될 수 있다. 위에서 설명된 바와 같이, 심층 문맥 벡터 i는 목표 단어 i의 문맥을 표현하기 위한 의미 특징을 수동으로 설계하고 추출하는데 복 잡한 특징 엔지니어링이 필요 없기 때문에 다양한 상황을 다룰 수 있는 포괄적인 표현이다. 도 6을 다시 참조하면, 분류 서브 모델은 목표 단어의 문맥 벡터에 기초하여 문법 오류 유형에 대한 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망을 포함한다. 피드포워드 신경망은 다층 퍼셉 트론(multi-layer perceptron(MLP)) 신경망 또는 은닉 유닛 사이의 연결이 사이클을 형성하지 않는 임의의 다른 적합한 피드포워드 신경망을 포함할 수 있다. 예를 들어, 도 5에 도시된 바와 같이, 심층 문맥 벡터 i가 피드포 워드 신경망에 공급되어 목표 단어 i의 분류값(y)을 생성한다. 다른 문법 오류 유형에 대하여, 분류값(y)은 표 1에 보여진 바와 같이 다른 방식으로 정의될 수 있다. 문법 오류 유형이 표 1에서의 5개의 예에 한정되지 않고, 분류값(y)의 정의가 표 1에 보여진 예에 의해 한정되지 않는다는 것이 이해되어야 한다. 또한, 일부 실시예에서, 분류값(y)이 문법 오류 유형과 연관된 클래스(라벨)에 대한 목표 단어의 확률 분포로서 표현될 수 있다는 것이 이해되어야 한다. 표 1 오류 유형 분류값(y) 관사 0 = a/an, 1 = the, 2 = 없음 전치사 라벨 = 전치사 인덱스 동사 형태 0 = 기본형, 1 = 동명사 또는 현재 분사, 2 = 과거 분사 주격 관련 일치 0 = 비3인칭 단수 현재, 1 = 3인칭 단수 현재 명사 수 0 = 단수, 1 = 복수 일부 실시예에서, 피드포워드 신경망은 문맥 벡터에서의 완전 연결 선형 연산(fully connected linear operation)의 제1 활성화 함수(activation function)를 갖는 제1 층을 포함할 수 있다. 제1 층에서의 제1 활성 화 함수는, 예를 들어, 정류 선형 유닛(rectified linear unit) 활성화 함수 또는 이전 층(들)으로부터의 1배 (one fold) 출력의 함수인 임의의 다른 적합한 활성화 함수일 수 있다. 또한, 피드포워드 신경망은 제1 층 에 연결되고 분류값을 생성하기 위한 제2 활성화 함수를 갖는 제2 층을 포함할 수 있다. 제2 층에서의 제2 활성 화 함수는, 예를 들어, 소프트맥스(softmax) 함수 또는 다계층(multiclass) 분류를 위하여 사용되는 임의의 다 른 적합한 활성화 함수일 수 있다.도 4를 다시 참조하면, 일부 실시예에서, 어텐션 유닛은, ANN 모델(12 0)을 사용하여, 문장 내의 목표 단어 전의 적어도 하나의 단어 및 목표 단어 후의 적어도 하나의 단어에 기초하여 목표 단어의 문맥 가중치 벡터(context weight vector)를 제공하도록 구성된다. 도 7은 일 실시예에 따른 문법 오류 정정을 위한 ANN 모델의 다른 예를 도시하는 개략도이다. 도 6에 도시된 예와 비교하여, 도 7의 ANN 모델은 어텐션 유닛에 의해 사용될 수 있는 어텐션 메커니즘 서브 모델을 더 포함한 다. 그 다음, 문맥 가중치 벡터를 문맥 벡터에 적용함으로써 가중치가 주어진 문맥 벡터가 계산된다. 심층 문맥 표현 서브 모델, 분류 서브 모델 및 어텐션 메커니즘 서브 모델은 종단간 방식으로 공동으로 훈 련될 수 있다. 일부 실시예에서, 어텐션 메커니즘 서브 모델은 목표 단어의 문맥 단어에 기초하여 목표 단 어의 문맥 가중치 벡터를 생성하도록 구성된 피드포워드 신경망을 포함한다. 피드포워드 신경망은 문 장 내의 목표 단어까지의 각각의 문맥 단어 사이의 거리에 기초하여 훈련될 수 있다. 일부 실시예에서, 문맥 가 중치 벡터가 목표 단어까지의 상이한 거리에 따라 문맥 단어의 가중치를 조정할 수 있기 때문에, 초기 문맥 벡 터 세트는 문장 내의 모든 둘러싸는 단어에 기초하여 생성될 수 있고, 문맥 가중치 벡터는 문법적 용례에 영향 을 미치는 문맥 단어에 초점을 맞추도록 가중치가 주어진 문맥 벡터를 튜닝할 수 있다. 도 4를 다시 참조하면, 분류 비교 유닛은 분류 유닛에 의해 제공된 추정된 분류값을 목표 단어 라벨 링 유닛에 의해 제공된 실제 분류값과 비교하여 문법 오류 유형의 임의의 오류의 존재를 검출한다. 실제 분류값이 추정된 분류값과 동일하다면, 문법 오류 유형의 오류가 목표 단어에 대하여 검출되지 않는다. 그렇지 않으면, 문법 오류 유형의 오류가 검출되고, 추정된 분류값이 정정을 제공하기 위하여 사용된다. 예를 들어, 도2에 관하여 위에서 설명된 예에서, 동사 형태 오류에 대한 목표 단어 \"adding\"의 추정된 분류값은 \"0\"(기본형) 인 반면, 목표 단어 \"adding\"의 실제 분류값은 \"1\"(동명사 또는 현재 분사)이다. 따라서, 동사 형태 오류가 검 출되고, 정정은 목표 단어 \"adding\"의 기본형이다. 도 8은 일 실시예에 따른 도 6의 ANN 모델의 일례를 도시하는 상세 개략도이다. 본 예에서, ANN 모델(12 0)은 공동으로 훈련되는 포워드 GRU 신경망, 백워드 GRU 신경망 및 MLP 신경망을 포함한다. 문장 \"I go to school everyday\"에서의 목표 단어 \"go\"에 대하여, 전방 문맥 단어 \"I\"는 왼쪽에서 오른쪽으로(순방향으로) 포 워드 GRU 신경망에 공급되고, 후방 문맥 단어들 \"to school everyday\"는 오른쪽에서 왼쪽으로(역방향으로) 백워 드 GRU 신경망으로 공급된다. 문맥 w1:n을 고려하면, 목표 단어 wi에 대한 문맥 벡터는 수학식 1로서 정의될 수 있다: 수학식 1"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기에서, lGRU는 주어진 문맥에서 왼쪽에서 오른쪽(순방향)으로 단어들을 읽는 GRU이고, rGRU는 주어진 문맥에 서 오른쪽에서 왼쪽(역방향)으로 단어들을 읽는 정반대U이다. l/f는 문맥 단어의 별개의 왼쪽에서 오른쪽/오른 쪽에서 왼쪽 단어 임베딩을 나타낸다. 그 후, 양측의 상호 종속성을 캡처하기 위하여, 연결된(concatenated) 벡 터가 MLP 신경망에 공급된다. MLP 신경망에서의 제2 층에서, 소프트맥스 층이 목표 단어의 분류(예를 들어, 목 표 단어 또는 목표 단어의 상태, 예를 들어, 단수 또는 복수)를 예측하기 위하여 사용될 수 있다: 수학식 2"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기에서, ReLU는 정류 선형 유닛 활성화 함수이고, ReLU(x) = max(0, x)이며, L(x) = W(x)+b는 완전 연결 선 형 연산이다. 본 예에서의 ANN 모델의 최종 출력은 다음과 같다: 수학식 3"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기에서, y는 위에서 설명된 바와 같은 분류값이다. 도 9는 일 실시예에 따른 문장의 문법 오류 정정 방법의 일례를 도시하는 순서도이다. 방법은 하드웨 어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트웨어(예를 들어, 처리 장치 상 에서 실행되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수행될 수 있다. 모든 단계들이 본 명세서에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해되어야 한다. 또한, 당해 업계 의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거나, 또는 도 9에 도시된 것과 다른 순서로 수행될 수 있다. 방법은 도 1 및 4를 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는다. 902에서, 문장이 수신된다. 문장은 입력 텍스트의 일부일 수 있다. 902는 GEC 시스템의 입력 전처 리 모듈에 의해 수행될 수 있다. 904에서, 문장 내의 하나 이상의 목표 단어가 하나 이상의 문법 오류 유 형에 기초하여 식별된다. 각각의 목표 단어는 하나 이상의 문법 오류 유형에 대응한다. 904는 GEC 시스템 의 파싱 모듈에 의해 수행될 수 있다. 906에서, 대응하는 문법 오류 유형에 대한 하나의 목표 단어의 분류 가 문법 오류 유형에 대하여 훈련된 ANN 모델을 이용하여 추정된다. 908에서, 목표 단어 및 목표 단어의 추정된 분류에 기초하여 문법 오류가 검출된다. 검출은 목표 단어의 실제 분류를 목표 단어의 추정된 분류와 비 교함으로써 이루어질 수 있다. 906 및 908은 GEC 시스템의 분류 기반 GEC 모듈에 의해 수행될 수 있 다.910에서, 문장에서 아직 처리되지 않은 더 많은 목표 단어가 있는지 판단된다. 결과가 '예'이면, 방법은 문장 내의 다음 목표 단어를 처리하기 위하여 904로 다시 이동한다. 문장 내의 모든 목표 단어가 처리되면, 912 에서, 문장에 대한 문법 오류 정정이 문법 오류 결과에 기초하여 제공된다. 각각의 목표 단어의 추정된 분류는 문법 오류 정정을 생성하기 위하여 사용될 수 있다. 또한, 문법 점수가 문법 오류 결과에 기초하여 제공될 수 있다. 912는 GEC 시스템의 채점/정정 모듈에 의해 수행될 수 있다. 도 10은 일 실시예에 따른 문법 오류 유형에 대하여 목표 단어를 분류하는 방법의 일례를 도시하는 순서 도이다. 방법은 하드웨어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트웨 어(예를 들어, 처리 장치 상에서 실행되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수행 될 수 있다. 모든 단계들이 본 명세서에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해되 어야 한다. 또한, 당해 업계의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거나, 또는 도 10에 도시된 것과 다른 순서로 수행될 수 있다. 방법은 도 1 및 4를 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는 다. 1002에서, 목표 단어의 문맥 벡터가 문장 내의 문맥 단어에 기초하여 제공된다. 문맥 단어는 문장 내에서 목표 단어를 둘러싸는 임의의 개수의 단어일 수 있다. 일부 실시예에서, 문맥 단어는 목표 단어를 제외한 문장 내의 모든 단어를 포함한다. 일부 실시예에서, 문맥 단어는 또한 목표 단어의 단어 기본형을 포함한다. 문맥 벡 터는 문장으로부터 추출된 의미 특징을 포함하지 않는다. 1002는 분류 기반 GEC 모듈의 심층 문맥 표현 유 닛에 의해 수행될 수 있다. 1004에서, 문맥 가중치 벡터가 문장 내의 문맥 단어에 기초하여 제공된다. 1006에서, 문맥 가중치 벡터가 문맥 벡터에 적용되어 가중치가 주어진 문맥 벡터를 생성한다. 문맥 가중치 벡터는 목표 단어까지의 문맥 단어의 거 리에 기초하여 해당하는 가중치를 문장 내의 각각의 문맥 단어에 적용할 수 있다. 1004 및 1006은 분류 기반 GEC 모듈의 어텐션 유닛에 의해 수행될 수 있다. 1008에서, 문법 오류 유형에 대한 목표 단어의 분류값이 목표 단어의 가중치가 주어진 문맥 벡터에 기초하여 제 공된다. 분류값은 문법 오류 유형과 연관된 다수의 클래스 중 하나를 나타낸다. 분류값은 문법 오류 유형과 연 관된 클래스에 대한 목표 단어의 확률 분포일 수 있다. 1008은 분류 기반 GEC 모듈의 분류 유닛에 의 해 수행될 수 있다. 도 11은 일 실시예에 따른 문법 오류 유형에 대하여 목표 단어를 분류하는 방법의 다른 예를 도시하는 순 서도이다. 방법은 하드웨어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트 웨어(예를 들어, 처리 장치 상에서 실행되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수 행될 수 있다. 모든 단계들이 본 명세서에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해 되어야 한다. 또한, 당해 업계의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거 나, 또는 도 11에 도시된 것과 다른 순서로 수행될 수 있다. 방법은 도 1 및 4를 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는 다. 1102에서, 목표 단어의 문법 오류 유형이, 예를 들어, 미리 정의된 복수의 문법 오류 유형으로부터 결정된 다. 1104에서, 문맥 단어의 윈도우 크기가 문법 오류 유형에 기초하여 결정된다. 윈도우 크기는 문맥 단어로서 고려될 문장 내의 목표 단어 전의 최대 개수의 단어와 목표 단어 전의 최대 개수의 단어를 나타낸다. 윈도우 크 기는 상이한 문법 오류 유형에 대하여 다를 수 있다. 예를 들어, 주격 관련 일치 오류 및 동사 형태 오류에 대 하여, 이러한 2개의 오류 유형이 일반적으로 목표 단어로부터 멀리 있는 문맥 단어로부터의 종속성을 필요로 하 기 때문에, 전체 문장이 문맥으로서 고려될 수 있다. 관사 오류, 전치사 오류 및 명사 수 오류에 대하여, 윈도 우 크기는, 관사 오류에 대한 3, 5 또는 10, 전치사 오류에 대한 3, 5 또는 10, 및 명사 수 오류에 대한 10, 15 또는 20과 같이, 전체 문장보다 더 작을 수 있다. 1106에서, 포워드 단어 임베딩 벡터 세트가 목표 단어 전의 문맥 단어들에 기초하여 생성된다. 각각의 포워드 단어 임베딩 벡터의 디멘전의 수는 적어도 100, 예를 들어 300일 수 있다. 포워드 단어 임베딩 벡터 세트가 생 성되는 순서는 윈도우 크기 내의 첫 번째 단어로부터 목표 단어 바로 전까지(순방향)일 수 있다. 1108에서, 병 렬로, 백워드 단어 임베딩 벡터 세트가 목표 단어 후의 문맥 단어들에 기초하여 생성된다. 각각의 백워드 단어 임베딩 벡터의 디멘전의 수는은 적어도 100, 예를 들어 300일 수 있다. 백워드 단어 임베딩 벡터 세트가 생성되 는 순서는 윈도우 크기 내의 마지막 단어로부터 목표 단어 바로 다음까지(역방향)일 수 있다. 1102, 1104, 1106 및 1108은 분류 기반 GEC 모듈의 초기 문맥 생성 유닛에 의해 수행될 수 있다.1110에서, 포워드 문맥 벡터가 포워드 단어 임베딩 벡터 세트에 기초하여 제공된다. 포워드 단어 임베딩 벡터 세트는 원도우 크기 내의 첫 번째 단어의 포워드 단어 임베딩 벡터로부터 목표 단어 바로 전의 단어의 포워드 단어 임베딩 벡터까지(순방향)의 순서에 따라 순환 신경망에 공급될 수 있다. 1112에서, 병렬로, 백워드 문맥 벡터가 백워드 단어 임베딩 벡터 세트에 기초하여 제공된다. 백워드 단어 임베딩 벡터 세트는 원도우 크기 내의 마지막 단어의 백워드 단어 임베딩 벡터로부터 목표 단어 바로 다음의 단어의 백워드 단어 임베딩 벡터까지(역 방향)의 순서에 따라 다른 순환 신경망에 공급될 수 있다. 1114에서, 문맥 벡터가 포워드 문맥 벡터 및 백워드 문맥 벡터를 연결하여 제공된다. 1110, 1112 및 1114는 분류 기반 GEC 모듈의 심층 문맥 표현 유닛에 의해 수행될 수 있다. 1116에서, 완전 연결 선형 연산이 문맥 벡터에 적용된다. 1118에서, 제1 층의 활성화 함수, 예를 들어, MLP 신 경망의 제1 층의 활성화 함수가 완전 연결 선형 연산의 출력에 적용된다. 활성화 함수는 정류 선형 유닛 활성화 함수일 수 있다. 1120에서, 제2 층의 다른 활성화 함수, 예를 들어, MLP 신경망의 제2 층의 다른 활성화 함수가 제1 층의 활성화 함수의 출력에 적용되어 문법 오류 유형에 대한 목표 단어의 분류값을 생성한다. 문법 오류 유 형에 대한 목표 단어의 다계층 분류가 1116, 1118 및 1120에서 MLP 신경망에 의해 문맥 벡터에 기초하여 수행될 수 있다. 1116, 1118 및 1120는 분류 기반 GEC 모듈의 분류 유닛에 의해 수행될 수 있다. 도 12는 일 실시예에 따른 문법 점수를 제공하는 방법의 일례를 도시하는 순서도이다. 방법은 하드 웨어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트웨어(예를 들어, 처리 장치 상에서 실행되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수행될 수 있다. 모든 단계들이 본 명세서에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해되어야 한다. 또한, 당해 업계 의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거나, 또는 도 12에 도시된 것과 다른 순서로 수행될 수 있다. 방법은 도 1 및 4를 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는 다. 1202에서, 사용자 인자(user factor)가 사용자의 정보에 기초하여 결정된다. 정보는, 예를 들어, 모국어, 거주지, 교육 수준, 나이, 과거의 점수 등을 포함한다. 1204에서, 정밀도 및 리콜의 가중치가 결정된다. 정밀도 와 리콜은 GEC를 위한 주요 평가 기준으로서 조합하여 일반적으로 사용된다. 정밀도(P)와 리콜(R)은 다음과 같 이 정의된다: 수학식 4"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기에서, g는 특정 문법 오류 유형에 대한 2개의 인간 주석의 골드 스탠다드(gold standard)이고, e는 대응하 는 시스템 편집이다. 많은 다른 문법 오류 유형과 동사 형태 오류 유형 사이에 중첩이 있을 수 있고, 따라서, 동사 형태 오류 성능을 계산할 때 g는 모든 문법 오류 유형의 주석에 기초할 수 있다. 정밀도와 리콜 사이의 가 중치는 이들을 평가 기준으로서 함께 조합할 때 조정될 수 있다. 예를 들어, 수학식 5에서 정의되는 F0.5는 정밀 도와 리콜을 함께 조합하는 반면, 정확한 피드백이 일부 실시예에서 커버리지보다 더 중요할 때 정밀도에 2배의 가중치를 할당한다: 수학식 5"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "n이 0 내지 1 사이인 Fn이 다른 예에 적용될 수 있다는 것이 이해되어야 한다. 또한, 일부 실시예에서, 상이한 문법 오류 유형에 대하여 가중치는 다를 수 있다. 1206에서, 채점 함수가 사용자 인자 및 가중치에 기초하여 획득된다. 채점 함수는 파라미터로서 사용자 인자 및 가중치(상이한 문법 오류 유형에 대하여 동일하거나 상이함)를 사용할 수 있다. 1208에서, 문장 내의 각각의 목표 단어의 문법 오류 결과가 수신된다. 1210에서, 문법 점수가 문법 오류 결과 및 채점 함수에 기초하여 제공된 다. 문법 오류 결과는 채점 함수의 변수일 수 있고, 사용자 인자 및 가중치는 채점 함수의 파라미터일 수 있다. 1202, 1204, 1206, 1208 및 1210은 GEC 시스템의 채점/정정 모듈에 의해 수행될 수 있다. 도 13은 일 실시예에 따른 ANN 모델 훈련 시스템을 도시하는 블록도이다. ANN 모델 훈련 시스템은 훈련 알고리즘을 이용하여 목적 함수(objective function)에 기초하여 훈련 샘플 세트에 걸 쳐 특정 문법 오류 유형에 대하여 각각의 ANN 모델을 훈련시키도록 구성된 모델 훈련 모듈을 포함한 다. 일부 실시예에서, 각각의 훈련 샘플은 원시(native) 훈련 샘플일 수 있다. 본 명세서에서 개시된 원 시 훈련 샘플은, 하나 이상의 문법 오류를 갖는 문장을 포함하는 학습자 훈련 샘플에 반대로, 문법 오류가 없는 문장을 포함한다. 맞춤형 훈련을 필요로 하는, 즉 감독되는 훈련 데이터의 크기 및 능력에 의해 제한되는 훈련 샘플(예를 들어, 학습자 훈련 샘플)로서 감독되는 데이터를 이용하는 일부 알려진 GEC 시스템에 비하여, ANN 모 델 훈련 시스템은 ANN 모델을 더욱 효과적이고 효율적으로 훈련시키기 위하여 훈련 샘플로서 풍부한 원시 평문 코퍼스를 활용할 수 있다. 예를 들어, 훈련 샘플은 위키 덤프(wiki dump)로부터 획득될 수 있다. ANN 모델 훈련 시스템을 위한 훈련 샘플이 원시 훈련 샘플에 한정되지 않는다는 것이 이 해되어야 한다. 일부 실시예에서, 소정의 문법 오류 유형에 대하여, ANN 모델 훈련 시스템은 학습자 훈련 샘플을 이용하여 또는 원시 훈련 샘플과 학습자 훈련 샘플의 조합을 이용하여 ANN 모델을 훈련시킬 수 있 다. 도 14는 도 13의 ANN 모델 훈련 시스템에 의해 사용되는 훈련 샘플의 일례에 대한 도면이다. 훈련 샘플은 하나 이상의 문법 오류 유형 1, ..., n과 연관된 문장을 포함한다. 훈련 샘플이 문법 오류가 없는 원시 훈련 샘플일 수 있더라도, 전술한 바와 같이 특정 단어가 예를 들어 이의 PoS 태그에 기초하여 하나 이상의 문 법 오류 유형과 연관되기 때문에, 문장은 여전히 문법 오류 유형과 연관될 수 있다. 예를 들어, 문장이 동사를 포함하는 한, 문장은, 예를 들어, 동사 형태 오류 및 주격 관련 일치 오류와 연관될 수 있다. 하나 이상의 목표 단어 1, ..., m이 각각의 문법 오류 유형과 연관될 수 있다. 예를 들어, 문장 내의 모든 동사가 훈련 샘플에서 동사 형태 오류 또는 주격 관련 일치 오류에 대한 목표 단어이다. 각각의 목표 단어에 대하여, 이는 다음의 2개 의 정보와 더 연관된다: 단어 임베딩 벡터 세트(매트릭스)(x) 및 실제 분류값(y). 단어 임베딩 벡터 세트(x)는 문장 내의 목표 단어의 문맥 단어에 기초하여 생성될 수 있다. 일부 실시예에서, 단어 임베딩 벡터 세트(x)가 원-핫 벡터 세트와 같은 임의의 다른 초기 문맥 벡터 세트일 수 있다는 것이 이해되어야 한다. 위에서 설명된 바와 같이, 실제 분류값(y)은 명사 수 오류에 대하여 단수에 대한 \"0\" 및 복수에 대한 \"1\"과 같은 특정 문법 오 류 유형에 대한 클래스 라벨 중 하나일 수 있다. 따라서, 훈련 샘플은 단어 임베딩 벡터 세트(x) 및 실제 분류 값(y) 쌍을 포함하며, 그 각각은 문장 내의 문법 오류 유형에 대한 목표 단어에 대응한다. 도 13을 다시 참조하면, ANN 모델은 훈련 샘플을 공급받을 때 모델 훈련 모듈에 의해 공동으 로 조정될 수 있는 복수의 파라미터를 포함한다. 모델 훈련 모듈은 훈련 알고리즘을 이용하여 훈련 샘플에 걸쳐 목적 함수를 최소화하기 위하여 ANN 모델의 파라미터들을 공동으로 조정한다. 도 8에 대하여 위에서 설명된 예에서, ANN 모델을 훈련시키기 위한 목적 함수는 다음과 같다: 수학식 6"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, n은 훈련 샘플의 개수이다. 훈련 알고리즘은, 경사 하강(gradient descent) 알고리즘(예를 들어, 스토캐스틱(stochastic) 경사 하강 알고리즘)을 포함하는, 목적 함수의 최소를 찾기 위한 임의의 적합한 반복 최적화 알고리즘일 수 있다. 도 15는 일 실시예에 따른 문법 오류 정정을 위한 ANN 모델 훈련 방법의 일례를 도시하는 순서도이다. 방 법은 하드웨어(예를 들어, 회로, 전용 로직, 프로그래머블 로직, 마이크로 코드 등), 소프트웨어(예를 들 어, 처리 장치 상에서 실행되는 명령어) 또는 이들의 조합을 포함할 수 있는 처리 로직에 의해 수행될 수 있다. 모든 단계들이 본 명세서에 제공된 개시 내용을 수행하는데 필요하지 않을 수 있다는 것이 이해되어야 한다. 또 한, 당해 업계의 통상의 기술자에 의해 이해되는 바와 같이, 단계들의 일부는 동시에 수행되거나, 또는 도 15에 도시된 것과 다른 순서로 수행될 수 있다.방법은 도 13을 참조하여 설명될 것이다. 그러나, 방법은 그 예시적인 실시예에 한정되지 않는다. 1502에서, 문법 오류 유형에 대한 ANN 모델이 제공된다. ANN 모델은 문법 오류 유형에 대하여 문장 내의 목표 단어의 분류를 추정하기 위한 것이다. ANN 모델은, 예를 들어, 도 6 및 7에 도시된 것과 같은, 본 명세서에 개 시된 임의의 ANN 모델일 수 있다. 일부 실시예에서, ANN 모델은 문장 내의 목표 단어 전의 적어도 하나의 단어 및 문장 내의 목표 단어 후의 적어도 하나의 단어에 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 2개 의 순환 신경망을 포함할 수 있다. 일부 실시예에서, 문맥 벡터는 훈련 샘플에서 문장의 의미 특징을 포함하지 않는다. 위에서 설명된 바와 같이, ANN 모델은 포워드 순환 신경망 및 백워드 순환 신경망으로서 파 라미터화될 수 있는 심층 문맥 표현 서브 모델을 포함할 수 있다. 또한, ANN 모델은 목표 단어의 문맥 벡 터에 기초하여 목표 단어의 분류값을 출력하도록 구성된 피드포워드 신경망을 포함할 수 있다. 위에서 설명된 바와 같이, ANN 모델은 피드포워드 신경망으로서 파라미터화될 수 있는 분류 서브 모델을 포함할 수 있다. 1504에서, 훈련 샘플이 획득된다. 각각의 훈련 샘플은 목표 단어를 갖는 문장과 문법 오류 유형에 대한 목표 단 어의 실제 분류를 포함한다. 일부 실시예에서, 훈련 샘플은 포워드 단어 임베딩 벡터 세트와 백워드 단어 임베 딩 벡터 세트를 포함하는 목표 단어의 단어 임베딩 매트릭스를 포함할 수 있다. 각각의 포워드 단어 임베딩 벡 터는 목표 단어 전의 해당하는 문맥 단어에 기초하여 생성되고, 각각의 백워드 단어 임베딩 벡터는 목표 단어 후의 해당하는 문맥 단어에 기초하여 생성된다. 각각의 단어 임베딩 벡터의 디멘전의 수는 적어도 100, 예를 들 어 300일 수 있다. 1506에서, ANN 모델의 파라미터들이, 예를 들어, 종단간 방식으로, 공동으로 조정된다. 일부 실시예에서, 순환 신경망(606, 608)과 연관된 심층 문맥 표현 서브 모델의 제1 파라미터 세트가 각각의 훈련 샘플에서 목표 단어의 실제 분류 및 추정된 분류 사이의 차이에 기초하여 피드포워드 신경망과 연관된 분류 서브 모델 의 제2 파라미터 세트와 함께 공동으로 조정된다. 일부 실시예에서, 포워드 순환 신경망과 연관된 파 라미터는 백워드 순환 신경망과 연관된 파라미터와 분리된다. 또한, 일부 실시예에서, ANN 모델은 피드포 워드 신경망으로서 파라미터화될 수 있는 어텐션 메커니즘 서브 모델을 포함할 수 있다. 피드포워드 신경망과 연관된 어텐션 메커니즘 서브 모델의 파라미터들도 또한 ANN 모델의 다른 파라미터와 함께 공동으로 조정될 수 있다. 일부 실시예에서, ANN 모델의 파라미터들은 훈련 알고리즘을 이용하여 목적 함 수로부터 각각의 훈련 샘플 내의 목표 단어의 실제 분류와 추정된 분류 사이의 차이를 최소화하도록 공동 으로 조정된다. 1502, 1504 및 1506은 ANN 모델 훈련 시스템의 모델 훈련 모듈에 의해 수행될 수 있다. 도 16은 일 실시예에 따른 문법 오류 정정을 위하여 ANN 모델을 훈련시키는 일례를 도시하는 개략도이다. 본 예에서, ANN 모델은 특정 문법 오류 유형에 대하여 훈련 샘플에 걸쳐 훈련된다. 훈련 샘플(130 4)은 원시 텍스트로부터 유래하고 도 1에 대하여 위에서 설명된 바와 같이 전처리 및 파싱될 수 있다. 각각의 훈련 샘플은 문법 오류 유형에 대한 목표 단어를 갖는 문장과 문법 오류 유형에 대한 목표 단어의 실제 분류를 포함한다. 일부 실시예에서, 목표 단어의 단어 임베딩 매트릭스(x)와 목표 단어의 실제 분류값(y)을 포 함하는 쌍이 각각의 훈련 샘플로부터 획득될 수 있다. 단어 임베딩 매트릭스(x)는 목표 단어 전의 문맥 단어에 기초하여 생성된 포워드 단어 임베딩 벡터 세트 및 목표 단어 후의 문맥 단어에 기초하여 생성된 백워드 단어 임베딩 벡터 세트를 포함할 수 있다. 따라서, 훈련 샘플은 복수의 (x, y) 쌍을 포함할 수 있다. 일부 실시예에서, ANN 모델은 복수의 순환 신경망 1 내지 n과 복수의 피드포워드 신경망 1 내지 m을 포함할 수 있다. 신경망(1602, 1604)의 각각은 훈련 알고리즘을 이용하여 목적 함수에 기초하여 훈련 샘플에 걸쳐 훈련될 파라미터 세트와 연관된다. 순환 신경망은 목표 단어의 문맥 단 어에 기초하여 목표 단어의 문맥 벡터를 출력하도록 구성된 포워드 순환 신경망 및 백워드 순환 신경망을 포함 할 수 있다. 일부 실시예에서, 순환 신경망은 목표 단어의 문맥 단어에 기초하여 목표 단어의 단어 임베 딩 매트릭스를 생성하도록 구성된 다른 하나 이상의 순환 신경망을 더 포함할 수 있다. 피드포워드 신경망 은 목표 단어의 문맥 벡터에 기초하여 목표 단어의 분류값(y')을 출력하도록 구성된 피드포워드 신경망을 포함할 수 있다. 또한, 일부 실시예에서, 피드포워드 신경망은 문맥 벡터에 적용될 문맥 가중치 벡터를 출력하도록 구성된 다른 피드포워드 신경망을 포함할 수 있다. 신경망(1602, 1604)은 이들이 종단간 방식으로 공동으로 훈련될 수 있도록 연결될 수 있다. 일부 실시예에서, 문맥 벡터는 훈련 샘플 내의 문장의 의미 특징을 포함하지 않는다. 일부 실시예에서, 각각의 반복에 대하여, 대응하는 훈련 샘플에서의 목표 단어의 단어 임베딩 매트릭스 (x)는 ANN 모델로 공급되어 신경망(1602, 1604)을 통과할 수 있다. 추정된 분류값(y')은 ANN 모델의출력층(예를 들어, 피드포워드 신경망의 일부)으로부터 출력될 수 있다. 대응하는 훈련 샘플에서의 목표 단어의 추정된 분류값(y')과 실제 분류값(y)은 목적 함수로 전송될 수 있고, 추정된 분류값(y')과 실제 분류값(y) 사이의 차이는 훈련 알고리즘을 이용하여 목적 함수에 의해 사용되어 ANN 모델 내의 각각의 신경망(1602, 1604)과 연관된 각각의 파라미터 세트를 공동으로 조정할 수 있다. 각각의 훈 련 샘플에 걸쳐 ANN 모델에서 각각의 신경망(1602, 1604)과 연관된 각각의 파라미터 세트를 반복적 으로 그리고 공동으로 조정함으로써, 추정된 분류값(y')과 실제 분류값(y) 사이의 차이는 점점 더 작아지고, 목 적 함수는 최적화된다. 다양한 실시예들은, 예를 들어, 도 17에 도시된 컴퓨터 시스템과 같은 하나 이상의 컴퓨터 시스템을 이용 하여 구현될 수 있다. 하나 이상의 컴퓨터 시스템은, 예를 들어, 도 3의 방법, 도 9의 방법, 도 10의 방법, 도 11의 방법, 도 12의 방법 및 도 15의 방법을 구현하도록 사용될 수 있다. 예를 들어, 컴퓨터 시스템은, 다양한 실시예들에 따라, 문법 오류를 검출하여 정정하고 그리고/또 는 문법 오류를 검출하여 정정하기 위한 인공 신경망(artificial neural network) 모델을 훈련시킬 수 있다. 컴 퓨터 시스템은 본 명세서에 설명된 기능을 수행할 수 있는 임의의 컴퓨터일 수 있다. 컴퓨터 시스템은 본 명세서에 설명된 기능을 수행할 수 있는 임의의 잘 알려진 컴퓨터일 수 있다. 컴퓨터 시스템은 프로세서와 같은 하나 이상의 프로세서(중앙 처리 유닛 또는 CPU라고도 함)를 포함한다. 프로세서는 통신 인프라스트럭처 또는 버스에 연결된다. 하나 이상의 프로세서는 각각 그래 픽 처리 유닛(graphics processing unit(GPU))일 수 있다. 일 실시예에서, GPU는 수학적으로 집중적인 애플리 케이션을 처리하도록 설계된 전문화된 전자 회로인 프로세서이다. GPU는 컴퓨터 그래픽 애플리케이션, 이미지, 비디오 등에 공통인 수학적으로 집중적인 데이터와 같은 대형 데이터 블록의 병렬 처리에 효율적인 병렬 구조를 가질 수 있다. 또한, 컴퓨터 시스템은 사용자 입/출력 인터페이스(들)를 통해 통신 인프라스트럭처와 통신 하는 모니터, 키보드, 포인팅 장치 등과 같은 사용자 입/출력 장치를 포함한다. 또한, 컴퓨터 시스템은 랜덤 액세스 메모리(random access memory(RAM))와 같은 주 또는 주요 메모리 를 포함한다. 주 메모리는 하나 이상의 레벨의 캐시를 포함할 수 있다. 주 메모리는 그 안에 저장된 제어 로직(즉, 컴퓨터 소프트웨어) 및/또는 데이터를 가진다. 또한, 컴퓨터 시스템은 하나 이상의 보조 저장 장치 또는 메모리를 포함할 수 있다. 보조 메모리는, 예를 들어, 하드 디스크 드라이브 및/또는 리무버블(removable) 저장 장치 또는 드라이브를 포함할 수 있다. 리무버블 저장 드라이 브는 플로피 디스크 드라이브, 자기 테이프 드라이브, 컴팩트 디스크 드라이브, 광 저장 장치, 테이프 백 업 장치 및/또는 임의의 다른 저장 장치/드라이브일 수 있다. 리무버블 저장 드라이브는 리무버블 저장 유닛과 상호 작용할 수 있다. 리무버블 저장 유닛은 그 상에 저장된 컴퓨터 소프트웨어(제어 로직) 및/또는 데이터를 갖는 컴퓨터 사용 가능하거나 또는 판독 가능한 저장 장치를 포함한다. 리무버블 저장 유닛 은 플로피 디스크, 자기 테이프, 컴팩트 디스크, DVD, 광 저장 디스크 및/또는 임의의 다른 컴퓨터 저장 장치일 수 있다. 리무버블 저장 드라이브는 잘 알려진 방식으로 리무버블 저장 유닛으로부터 판독 하고 그리고/또는 리무버블 저장 유닛으로 기록한다. 예시적인 실시예에 따라, 보조 메모리는 컴퓨터 프로그램 및/또는 다른 명령어 및/또는 데이터가 컴퓨터 시스템에 의해 액세스될 수 있게 하도록 다른 수단, 방편 또는 다른 접근 방식을 포함할 수 있다. 이러한 수단, 방편 또는 다른 접근 방식은, 예를 들어, 리무버블 저장 유닛 및 인터페이스를 포함할 수 있 다. 리무버블 저장 유닛 및 인터페이스의 예는, 프로그램 카트리지 및 카트리지 인터페이스(예를 들어, 비디오 게임 장치에서 찾아볼 수 있는 것), 리무버블 메모리 칩(예를 들어, EPROM 또는 PROM) 및 연관된 소켓, 메모리 스틱 및 USB 포트, 메모리 카드 및 연관된 메모리 카드 슬롯 그리고/또는 임의의 다른 리무버블 저장 유닛 및 연관된 인터페이스를 포함할 수 있다. 컴퓨터 시스템은 통신 또는 네트워크 인터페이스를 더 포함할 수 있다. 통신 인터페이스는 컴퓨터 시스템으로 하여금 원격 장치, 원격 네트워크, 원격 엔티티 등(개별적으로 그리고 집합적으로 참 조 번호 1728로 참조됨)의 임의의 조합과 통신하고 상호 작용할 수 있게 한다. 예를 들어, 통신 인터페이스 는 컴퓨터 시스템이 유선 및/또는 무선일 수 있고 LAN, WAN, 인터넷 등의 임의의 조합을 포함할 수 있는 통신 경로를 통해 원격 장치와 통신할 수 있게 한다. 제어 로직 및/또는 데이터는 통신 경로 를 통해 컴퓨터 시스템으로 그리고 컴퓨터 시스템으로부터 전송될 수 있다. 또한, 일 실시예에서, 그 상에 저장된 제어 로직(소프트웨어)을 갖는 유형의(tangible) 컴퓨터 사용 가능하거나 또는 판독 가능한 매체를 포함하는 유형의 제조 장치 또는 물품은 본 명세서에서 컴퓨터 프로그램 제품 또는 프 로그램 저장 장치라 한다. 이것은, 컴퓨터 시스템, 메인 메모리, 보조 메모리 및 리무버블 저장 유닛(1718, 18722)과, 전술한 것의 임의의 조합을 구체화하는 유형의 제조 물품을 포함하지만 이에 한정되 지 않는다. 이러한 제어 로직은, 하나 이상의 데이터 처리 장치(예를 들어, 컴퓨터 시스템)에 의해 실행 될 때, 이러한 데이터 처리 장치가 본 명세서에 설명된 바와 같이 동작하게 한다. 본 개시 내용에 포함된 교시 내용에 기초하여, 도 17에 도시된 것과 다른 데이터 처리 장치, 컴퓨터 시스템 및/ 또는 컴퓨터 아키텍처를 이용하여 본 개시 내용의 실시예들을 형성하고 사용하는 방법은 관련 기술 분야(들)에 서의 통상의 기술자에게 명백할 것이다. 특히, 실시예들은 본 명세서에서 설명된 것이 아닌 소프트웨어, 하드웨 어 및/또는 운영 체제 구현예로 동작할 수 있다."}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "[발명의 내용] 및 [요약서] 부분이 아닌 [발명을 실시하기 위한 구체적인 내용] 부분이 청구범위를 해석하는데"}
{"patent_id": "10-2020-7005087", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "사용되도록 의도된다. [발명의 내용] 및 [요약서] 부분은 발명자(들)에 의해 고려되는 본 개시 내용의 하나 이 상의 예시적인 실시예를 설명하지만 모든 예시적인 실시예를 설명하지는 않을 수 있고, 따라서, 어떠한 방식으 로도 본 개시 내용 또는 첨부된 청구범위를 제한하려고 의도되지 않는다. 본 개시 내용이 예시적인 분야 및 애플리케이션을 위한 예시적인 실시예를 참조하여 본 명세서에서 설명되었지 만, 본 개시 내용이 그에 한정되지 않는다는 것이 이해되어야 한다. 그에 대한 다른 실시예 및 수정이 가능하며, 본 개시 내용의 범위 및 사상 내에 있다. 예를 들어, 그리고 본 문단의 일반성을 제한하지 않으면서, 실시예들은 도면에 도시되거나 본 명세서에서 설명된 소프트웨어, 하드웨어, 펌웨어 및/또는 엔티티에 제한되지 않는다. 또한, 실시예들은(본 명세서에 명시적으로 설명되는지 여부에 관계 없이) 본 명세서에 설명된 예들을 넘는 분야 및 애플리케이션에 대하여 상당히 활용될 수 있다. 실시예들은 본 명세서에서 특정된 기능들의 구현과 이들의 관계를 나타내는 기능 구성 블록들의 도움으로 설명 되었다. 이 기능 구성 블록들의 경계는 설명의 편의를 위하여 본 명세서에서 임의로 정의되었다. 특정된 기능과 관계(또는 이들의 균등물)가 적합하게 수행되는 한 대안적인 경계가 정의될 수 있다. 또한, 대안적인 실시예는 본 명세서에 설명된 것과 다른 순서로 기능 블록, 단계, 동작, 방법 등을 수행할 수 있다. 본 개시 내용의 폭과 범위는 어떠한 전술한 예시적인 실시예들에 의해서도 제한되어서는 안 되며, 이어지는 청 구범위 및 이의 균등물에 따라서만 정의되어야 한다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16 도면17"}
{"patent_id": "10-2020-7005087", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 명세서에 포함되어 명세서의 일부를 형성하는 첨부된 도면은 본 개시 내용을 도시하며, 설명과 함께, 본 개 시 내용의 원리를 설명하고 관련 기술 분야에서의 통상의 기술자가 본 개시 내용을 실시 및 사용할 수 있게 하 는 역할을 한다. 도 1은 일 실시예에 따른 문법 오류 정정(GEC) 시스템을 도시하는 블록도이다;도 2는 도 1의 시스템에 의해 수행되는 자동화된 문법 오류 정정의 일례에 대한 도면이다; 도 3은 일 실시예에 따른 문법 오류 정정 방법의 일례를 도시하는 순서도이다; 도 4는 일 실시예에 따른 도 1의 시스템의 분류 기반 GEC 모듈의 일례를 도시하는 블록도이다; 도 5는 일 실시예에 따른 도 1의 시스템을 이용하여 문장 내의 목표 단어의 ]분류를 제공하는 일례에 대한 도면 이다; 도 6은 일 실시예에 따른 문법 오류 정정을 위한 인공 신경망(artificial neural network(ANN)) 모델의 일례를 도시하는 개략도이다; 도 7은 일 실시예에 따른 문법 오류 정정을 위한 ANN 모델의 다른 예를 도시하는 개략도이다; 도 8은 일 실시예에 따른 도 6의 ANN 모델의 일례를 도시하는 상세 개략도이다; 도 9는 일 실시예에 따른 문장의 문법 오류 정정 방법의 일례를 도시하는 순서도이다; 도 10은 일 실시예에 따른 문법 오류 유형에 대하여 목표 단어를 분류하는 방법의 일례를 도시하는 순서도이다; 도 11은 일 실시예에 따른 문법 오류 유형에 대하여 목표 단어를 분류하는 방법의 다른 예를 도시하는 순서도이 다; 도 12는 일 실시예에 따른 문법 점수를 제공하는 방법의 일례를 도시하는 순서도이다; 도 13은 일 실시예에 따른 ANN 모델 훈련 시스템을 도시하는 블록도이다; 도 14는 도 13의 시스템에 의해 사용되는 훈련 샘플의 일례에 대한 도면이다; 도 15는 일 실시예에 따른 문법 오류 정정을 위한 ANN 모델 훈련 방법의 일례를 도시하는 순서도이다; 도 16은 일 실시예에 따른 문법 오류 정정을 위하여 ANN 모델을 훈련시키는 일례를 도시하는 개략도이다; 그리 고, 도 17은 본 개시 내용에서 설명된 다양한 실시예를 구현하는데 유용한 컴퓨터 시스템이 일례를 도시하는 블록도 이다. 본 개시 내용은 첨부된 도면을 참조하여 설명된다. 도면에서, 일반적으로, 유사한 참조 번호는 동일하거나 기능 적으로 유사한 요소를 나타낸다. 또한, 일반적으로, 참조 번호의 가장 왼쪽의 숫자(들)는 참조 번호가 처음 나 타나는 도면을 식별한다."}
