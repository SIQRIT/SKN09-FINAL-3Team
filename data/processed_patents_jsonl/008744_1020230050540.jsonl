{"patent_id": "10-2023-0050540", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0154221", "출원번호": "10-2023-0050540", "발명의 명칭": "콤팩트 모델 처리 장치 및 방법", "출원인": "성균관대학교산학협력단", "발명자": "공정택"}}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "기하 파라미터 및 공정 파라미터를 입력 값으로 하여 분석 대상에 대한 적어도 하나의 인공신경망을 훈련시키는신경망 학습부; 및상기 신경망 학습부에 의해 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 모델추출부;를 포함하는 콤팩트모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 분석 대상은, 적어도 하나의 채널을 갖는 전계 효과 트랜지스터를 포함하고,상기 공정 파라미터는, 상기 전계 효과 트랜지스터 채널에 이온이 주입되는 깊이인 채널 도핑 깊이를 포함하는콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 전계 효과 트랜지스터는, 나노시트 펫 및 네거티브 커패시턴스 나노시트 펫 중 적어도 하나를 포함할 수있고,상기 기하 파라미터는, 상기 나노시트 펫 및 상기 네거티브 커패시턴스 나노시트 펫 중 적어도 하나의 게이트길이, 나노시트의 폭, 상기 나노시트의 두께, 스페이서의 두께 및 유전체의 두께 중 적어도 하나를 포함하는 콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 인공신경망의 손실함수는, 차단 영역의 실측 값과 예측 값 간의 평균 오차 제곱, 선형 영역의 실측 값과예측 값 간의 평균 오차 제곱 및 포화 영역의 실측 값과 예측 값 간의 평균 오차 제곱의 가중합인 콤팩트 모델처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 인공신경망은, 전류-전압 모델에 대응하는 제1 인공신경망 및 커패시턴스-전압 모델에 대응하는 제2 인공신경망 중 적어도 하나를 포함하는 콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 모델추출부는,훈련된 제1 인공신경망 및 훈련된 제2 인공신경망으로부터 획득된 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화 함수를 기반으로, 상기 훈련된 제1 인공신경망에대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나를결정하는 콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2024-0154221-3-제6항에 있어서,상기 모델추출부는,적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 더 연산하고,상기 훈련된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나와, 상기 적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 조합하여 콤팩트 모델을 생성하는 콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 콤팩트 모델을 기반으로 상기 분석 대상에 대한 시뮬레이션을 수행하는 시뮬레이션부;를 더 포함하는 콤팩트 모델 처리 장치."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "기하 파라미터 및 공정 파라미터를 획득하는 단계; 및상기 기하 파라미터 및 상기 공정 파라미터를 입력 값으로 하여 분석 대상에 대한 적어도 하나의 인공신경망을훈련시켜, 적어도 하나의 훈련된 인공신경망을 획득하는 단계;를 포함하는 콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 분석 대상은, 적어도 하나의 채널을 갖는 전계 효과 트랜지스터를 포함하고,상기 공정 파라미터는, 상기 전계 효과 트랜지스터 채널에 이온이 주입되는 깊이인 채널 도핑 깊이를 포함하는콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 전계 효과 트랜지스터는, 나노시트 펫 및 네거티브 커패시턴스 나노시트 펫 중 적어도 하나를 포함할 수있고,상기 기하 파라미터는, 상기 나노시트 펫 및 상기 네거티브 커패시턴스 나노시트 펫 중 적어도 하나의 게이트길이, 나노시트의 폭, 상기 나노시트의 두께, 스페이서의 두께 및 유전체의 두께 중 적어도 하나를 포함하는 콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 인공신경망의 손실함수는, 차단 영역의 실측 값과 예측 값 간의 평균 오차 제곱, 선형 영역의 실측 값과예측 값 간의 평균 오차 제곱 및 포화 영역의 실측 값과 예측 값 간의 평균 오차 제곱의 가중합인 콤팩트 모델처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 인공신경망은, 전류-전압 모델에 대응하는 제1 인공신경망 및 커패시턴스-전압 모델에 대응하는 제2 인공신경망 중 적어도 하나를 포함하는 콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,공개특허 10-2024-0154221-4-상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계;를 더 포함하는 콤팩트 모델 처리방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계는,훈련된 제1 인공신경망 및 훈련된 제2 인공신경망으로부터 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화 함수를 획득하는 단계; 및상기 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화함수를 기반으로, 상기 훈련된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나를 결정하는 단계;를 포함하는 콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계는,적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 더 연산하는 단계; 및상기 훈련된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나와, 상기 적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 조합하여 콤팩트 모델을 생성하는 단계;를 더 포함하는 콤팩트 모델 처리 방법."}
{"patent_id": "10-2023-0050540", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제9항에 있어서,상기 콤팩트 모델을 기반으로 상기 분석 대상에 대한 시뮬레이션을 수행하는 단계;를 더 포함하는 콤팩트 모델처리 방법."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "콤팩트 모델 처리 장치 및 방법에 관한 것으로, 콤팩트 모델 처리 장치는, 기하 파라미터 및 공정 파라미터를 입 력 값으로 하여 분석 대상에 대한 적어도 하나의 인공신경망을 훈련시키는 신경망 학습부와, 상기 신경망 학습부 에 의해 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 모델추출부를 포함할 수 있다."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "콤팩트 모델 처리 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "집적 회로와 같은 반도체 기반 장치는 설계 및 공정을 통해 수행된다. 그런데, 설계된 반도체 기반 장치가 실제 생산 과정에서 원하는 전기적 특성을 보이지 않고 있거나 기대한 것과 상이하게 동작하는 경우, 공정 과정에서 이를 수정하는 것은 거의 불가능하기 때문에 해당 장치를 처음부터 재설계한 후 다시 생산을 해야만 한다. 이러 한 재설계 과정은 반도체 제작 시간 및 비용을 상당히 증가시킬 수 있으므로, 반도체의 실제 생산 이전에 반도 체에 대한 콤팩트 모델(compact model)을 설계하고 이를 기반으로 해당 반도체의 동작 등에 대한 시뮬레이션(모 의실험)을 진행하는 것이 일반적이다. 현재 이용되는 콤팩트 모델은 공정 이전에 해당 장치의 전기적 특성 등에 대한 시뮬레이션을 수행할 수 있도록 반도체 기반 장치의 각각의 회로 성분들의 전기적 특성이나 동작을 수학적 으로 표현한 것이다. 최근에는 나노 기술 및 미세 공정의 도입에 따른 소자의 미세화 및 반도체 기반 장치의 고집적화에 따라서, 설 계된 반도체 소자나 장치를 적절히 대변할 수 있는 콤팩트 모델의 요청이 더욱 증가하는 추세이다. 그런데, 미 세화 및 고집적화의 진행에 따라 기술, 공정, 재료 또는 구조 등이 변경되면서 설명하기 어려운 물리적 현상이 빈번히 발생하고 있으며, 이에 따라 해당 장치에 대응하는 콤팩트 모델의 개발 및 구현도 상당히 어려워 지고 있다. 따라서, 개발된 차세대 소자에 대응하는 콤팩트 모델은 차세대 소자가 제안된 후 통상적으로 수년 경과된 후에나 소개 및 도입되고 있으며, 이는 반도체 장치의 개발 지연의 주된 원인이 되고 있었다. 한편으로 콤팩트 모델은 모델 파라미터 추출에 고도의 전문성과 대량의 시간을 요구한다. 최근에는 BSIM(Berkeley Short-channelIGFET Model) 모델이 주로 이용되고 있는데, BSIM 모델에는 1000개 이상의 매우 많은 수의 모델 파라미터가 사 용되고 있고, 이는 공정 미세화에 따라 더욱 증가할 것으로 예상되고 있다. 이러한 점은 차세대 소자에 대한 콤 팩트 모델의 개발 및 사용의 어려움을 더욱 가중하고 있다. 또한, 고집적화 및 소자 복잡도의 증가에 따른 콤팩 트 모델의 복잡화는 시뮬레이션의 처리 속도의 저하도 야기하고 있다. 이는 대형 회로에 대한 실험이나 몬테카 를로 시뮬레이션(Monte Carlo simulation) 등과 같이 연산 비용이 큰 시뮬레이션에서 특히 두드러진다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허공보 제10-2022-0048941호(\"인공 신경망을 이용한 트랜지스터 컴팩트 모델링 시스템, 방법 및 컴퓨터 프로그램 제품 \", 출원인: 삼성전자, 공개일자: 2022.04.20) (특허문헌 0002) 미국 등록특허 US 11,176,447호(\"Semiconductor device modeling using input pre-processing and transformed targets for training a deep neural network\", 출원인: Hong Kong Applied Science and Technology, 공개일자: 2019.12.19)"}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "복잡한 반도체 장치와 같은 분석 대상에 부합하면서 높은 정확도와 빠른 속도로 시뮬레이션을 수행할 수 있는 인공신경망 기반의 콤팩트 모델을 생성할 수 있는 콤팩트 모델 처리 장치 및 방법을 제공하는 것을 해결하고자 하는 과제로 한다."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 과제를 해결하기 위하여 콤팩트 모델 처리 장치 및 방법이 제공된다. 콤팩트 모델 처리 장치는, 기하 파라미터 및 공정 파라미터를 입력 값으로 하여 분석 대상에 대한 적어도 하나 의 인공신경망을 훈련시키는 신경망 학습부와, 상기 신경망 학습부에 의해 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 모델추출부를 포함할 수 있다. 상기 분석 대상은, 적어도 하나의 채널을 갖는 전계 효과 트랜지스터를 포함할 수 있고, 상기 공정 파라미터는, 상기 전계 효과 트랜지스터 채널에 이온이 주입되는 깊이인 채널 도핑 깊이를 포함할 수 있다. 상기 전계 효과 트랜지스터는, 나노시트 펫 및 네거티브 커패시턴스 나노시트 펫 중 적어도 하나를 포함할 수 있고, 상기 기하 파라미터는, 상기 나노시트 펫 및 상기 네거티브 커패시턴스 나노시트 펫 중 적어도 하나의 게 이트 길이, 나노시트의 폭, 상기 나노시트의 두께, 스페이서의 두께 및 유전체의 두께 중 적어도 하나를 포함할 수도 있다. 상기 인공신경망의 손실함수는, 차단 영역의 실측 값과 예측 값 간의 평균 오차 제곱, 선형 영역의 실측 값과 예측 값 간의 평균 오차 제곱 및 포화 영역의 실측 값과 예측 값 간의 평균 오차 제곱의 가중합으로 주어질 수 도 있다. 상기 인공신경망은, 전류-전압 모델에 대응하는 제1 인공신경망 및 커패시턴스-전압 모델에 대응하는 제2 인공 신경망 중 적어도 하나를 포함할 수 있다. 상기 모델추출부는, 훈련된 제1 인공신경망 및 훈련된 제2 인공신경망으로부터 획득된 입력층의 입력 값, 은닉 층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화 함수를 기반으로, 상기 훈련 된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모 델 중 적어도 하나를 결정할 수 있다. 상기 모델추출부는, 적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적 어도 하나를 더 연산할 수 있고, 또한 상기 훈련된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나와, 상기 적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 조합하여 콤팩트 모델을 생성할 수도 있다.콤팩트 모델 처리 장치는, 상기 콤팩트 모델을 기반으로 상기 분석 대상에 대한 시뮬레이션을 수행하는 시뮬레 이션부를 더 포함하는 것도 가능하다. 콤팩트 모델 처리 방법은, 기하 파라미터 및 공정 파라미터를 획득하는 단계 및 상기 기하 파라미터 및 상기 공 정 파라미터를 입력 값으로 하여 분석 대상에 대한 적어도 하나의 인공신경망을 훈련시켜, 적어도 하나의 훈련 된 인공신경망을 획득하는 단계를 포함할 수 있다. 상기 분석 대상은, 적어도 하나의 채널을 갖는 전계 효과 트랜지스터를 포함할 수 있고, 상기 공정 파라미터는, 상기 전계 효과 트랜지스터 채널에 이온이 주입되는 깊이인 채널 도핑 깊이를 포함할 수도 있다. 상기 전계 효과 트랜지스터는, 나노시트 펫 및 네거티브 커패시턴스 나노시트 펫 중 적어도 하나를 포함할 수 있으며, 이 경우, 상기 기하 파라미터는, 상기 나노시트 펫 및 상기 네거티브 커패시턴스 나노시트 펫 중 적어 도 하나의 게이트 길이, 나노시트의 폭, 상기 나노시트의 두께, 스페이서의 두께 및 유전체의 두께 중 적어도 하나를 포함할 수도 있다. 상기 인공신경망의 손실함수는, 차단 영역의 실측 값과 예측 값 간의 평균 오차 제곱, 선형 영역의 실측 값과 예측 값 간의 평균 오차 제곱 및 포화 영역의 실측 값과 예측 값 간의 평균 오차 제곱의 가중합일 수도 있다. 상기 인공신경망은, 전류-전압 모델에 대응하는 제1 인공신경망 및 커패시턴스-전압 모델에 대응하는 제2 인공 신경망 중 적어도 하나를 포함할 수도 있다. 콤팩트 모델 처리 방법은, 상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계를 더 포함할 수도 있다. 상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계는, 훈련된 제1 인공신경망 및 훈 련된 제2 인공신경망으로부터 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력 층의 출력 값 및 활성화 함수를 획득하는 단계 및 상기 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화 함수를 기반으로, 상기 훈련된 제1 인공신경망에 대 응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나를 결 정하는 단계를 포함할 수도 있다. 상기 적어도 하나의 훈련된 인공신경망으로부터 콤팩트 모델을 추출하는 단계는, 적어도 하나의 커패시턴스, 적 어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어도 하나를 더 연산하는 단계 및 상기 훈련된 제1 인공신경망에 대응하는 전류-전압 모델 및 상기 훈련된 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적 어도 하나와, 상기 적어도 하나의 커패시턴스, 적어도 하나의 전류, 트랜스컨덕턴스 및 드레인컨덕턴스 중 적어 도 하나를 조합하여 콤팩트 모델을 생성하는 단계를 더 포함하는 것도 가능하다. 콤팩트 모델 처리 방법은, 상기 콤팩트 모델을 기반으로 상기 분석 대상에 대한 시뮬레이션을 수행하는 단계를 더 포함할 수도 있다."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 콤팩트 모델 처리 장치 및 방법에 의하면, 복잡한 분석 대상에 부합하면서 모의시험의 신속한 수행을 높 은 정확도로 가능하게 하는 인공신경망 기반의 콤팩트 모델을 생성할 수 있게 된다. 상술한 콤팩트 모델 처리 장치 및 방법에 의하면, 콤팩트 모델의 생성에 있어서 공정 과정에 관련된 변수 등과 같이 설계자 또는 사용자가 원하는 변수를 용이하게 추가할 수 있게 되어, 콤팩트 모델의 정확성 및 확장성을 개선 및 고도화할 수 있게 된다. 상술한 콤팩트 모델 처리 장치 및 방법에 의하면, 나노시트 펫(NSFET: nanosheet FET)이나 네거티브 커패시턴스 -나노시트 펫(NC-NSFET: negative capacitance FET) 등과 같은 차세대 소자에도 쉽게 활용할 수 있는 콤팩트 모델의 획득이 가능하게 된다. 상술한 콤팩트 모델 처리 장치 및 방법에 의하면, 콤팩트 모델을 상대적으로 경량으로 구축할 수 있어 콤팩트 모델의 생성 및 콤팩트 모델로부터의 파라미터의 도출을 보다 신속하게 처리할 수 있게 된다. 상술한 콤팩트 모델 처리 장치 및 방법에 의하면, 스파이스(SPICE: Simulation Program with Integrated Circuit Emphasis) 등과 같은 시뮬레이터(simulator) 등이 콤팩트 모델에 대응하는 반도체 장치에 대한 시뮬레 이션을 상대적으로 높은 정확도와 빠른 속도로 수행할 수 있게 할 수 있다."}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예들을 참 조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다 양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하고, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구 항의 범주에 의해 정의될 뿐이다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 발명에 대해 구체적으로 설명하기로 한다. 본 발명에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세 히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지 는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에서 사용되는 \"부\", \"모듈\", \"유닛\" 등의 용어는 적어도 하나의 기능 또는 동작을 처리하는 단위를 의미하며, 소프트웨어, FPGA 또는 ASIC과 같은 하드웨어 구성요소, 또는 소프트웨어와 하드웨어의 결합으로 구현될 수 있다. 그렇지만 \"부\", \"모듈\", \"유닛\" 등의 용어가 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. \"부\", \"모듈\", \"유닛\" 등은 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 \"\"부\", \"모듈\", \"유닛\" 등의 용어는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데 이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 및 변수들을 포함한다. \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 구성 요 소들은 용어들에 의해 한정되지는 않는다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로 만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. \"및/또는\" 이라는 용어는 복수의 관련된 항목 들의 조합 또는 복수의 관련된 항목들 중의 어느 하나의 항목을 포함한다. 아래에서는 첨부한 도면을 참고하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략한다. 이하 도 1 내지 도 6을 참조하여 콤팩트 모델 처리 장치의 일 실시예에 대해서 설명하도록 한다. 도 1은 일 실시예에 따른 콤팩트 모델 처리 장치에 대한 개요도이다. 도 1에 도시된 바를 참조하면, 일 실시예에 따른 콤팩트 모델 처리 장치는, 입력부, 통신부, 저장 부, 출력부 및 프로세서를 포함할 수 있다. 여기서, 입력부, 통신부, 저장부, 출력부 및 프로세서 중 적어도 둘은 회로선, 케이블 또는 무선통신네트워크 등을 통해 일방으로 또는 쌍방으 로 데이터나 명령/지시 등을 전달할 수 있게 마련된다. 입력부, 통신부, 저장부 및 출력부 중 적어도 하나는 실시예에 따라 생략되는 것도 가능하다. 입력부는, 콤팩트 모델 처리 장치의 동작에 필요한 데이터, 명령/지시 또는 프로그램(앱, 애플리케이션 또는 소프트웨어로 지칭 가능함) 등을 외부로부터 입력 받을 수 있다. 여기서, 콤팩트 모델 처리 장치의 동 작에 필요한 데이터는, 예를 들어, 분석 대상(도 2a의 30)에 대한 정보 또는 인공신경망(도 3의 80, 도 4의 90 등)에 대한 정보 등을 포함할 수 있다. 분석 대상은, 콤팩트 모델로 구현될 실제적 또는 가상적 대상을 의미할 수 있으며, 실시예에 따라, 반도체 소자나 집적회로 등을 포함할 수 있다. 일 실시예에 따르면, 반도체 소자는 전계 효과 트랜지스터(FET: Field Effect Transistor)를 포함할 수 있다. 전계 효과 트랜지스터는, 소스(일례로 도 2a의 31), 적어도 하나의 채널(일례로 도 2A의 34), 드레인(일례로 도 2A의 32) 및 게이트를 포함하여 구 현될 수 있으며, 예를 들어, 모스펫(MOSFET: Metal-Oxide-Semiconductor FET), 핀펫(FinFET), 게이트올어라운 드(GAAFET: Gate All Around FET), 나노시트 펫(NSFET: NanoSheet FET, 도 2A의 30) 또는 네거티브 커패시턴 스-나노시트 펫(NC-NSFET: Negative Capacitance NSFET) 등을 포함할 수 있다. 그러나, 분석 대상은 이에 한정 되는 것은 아니다. 사용자나 설계자에 따라 이들과 다른 반도체 소자가 분석 대상으로 이용될 수도 있다. 분석 대상에 대한 정보는, 분석될 반도체 소자의 크기(반도체 소자를 이루는 각 부분의 크기 등), 구조 또는 구성에 대한 정보를 포함하거나, 및/또는 분석될 집적회로의 포함된 소자나 이들에 의한 구조 등에 대한 정보 등을 포 함할 수도 있다. 이에 대해선 도 2a 및 도 2b를 참조하여 후술한다. 분석 대상에 대한 정보는 인공신경망(80, 90)의 입력 파라미터(도 4의 70, 도 5의 70-1)로 이용될 수 있다. 인공신경망(80, 90)에 대한 정보는, 예를 들 어, 인공신경망(80, 90)의 크기(노드나 층의 개수 등)를 포함할 수 있다. 그러나, 분석 대상에 대한 정보 및/또 는 인공신경망(80, 90)에 대한 정보 등은 상술한 바에 한정되는 것은 아니며, 설계자나 사용자 등에 따라 추가 로 또는 이들을 대신하여 이들 외의 다른 정보나 파라미터 등을 포함할 수도 있다. 입력부는, 예를 들어, 키보드, 마우스, 태블릿 입력 장치, 터치 스크린, 터치 센서, 마이크로 폰, 데이터 입출력 모듈, 영상 촬영 모 듈, 동작 감지 센서, 감압 센서 또는 근접 센서 등을 이용해 구현될 수 있다. 통신부는 외부의 다른 장치와 유선 또는 무선 통신 네트워크를 통해 통신 가능하게 연결되고, 외부의 다른 장치로부터 콤팩트 모델 처리 장치의 동작에 필요한 데이터, 명령/지시 또는 프로그램 등을 전달받을 수 있 다. 또한, 통신부는 훈련에 따라 획득된 콤팩트 모델이나, 콤팩트 모델로부터 획득된 파라미터(들) 또는 시 뮬레이션 결과 등을 외부의 다른 장치로 전달하는 것도 가능하다. 이에 따라 사용자는 단말기(스마트폰이나 데 스크톱 컴퓨터 등)를 통해 콤팩트 모델 처리 장치에 접근하여 콤팩트 모델 처리 장치에 정보를 제공하 거나, 콤팩트 모델 생성과 관련된 명령을 입력하거나, 또는 콤팩트 모델 처리 장치의 처리 결과를 획득할 수도 있다. 통신부는 랜 카드 또는 무선통신 모듈 등을 이용해 구현 가능하다. 저장부는, 입력부나 통신부를 통해 획득되거나 프로세서의 처리 과정에서 획득된 데이터나 인 공신경망 등을 일시적 또는 비일시적으로 저장할 수 있다. 예를 들어, 저장부는 분석 대상에 대한 정보 또 는 인공신경망에 입력될 파라미터를 저장하거나, 프로세서의 호출에 따라 훈련될 인공신경망 및 파라미터 를 프로세서에 제공하거나, 프로세서에 의해 획득된 콤팩트 모델 또는 해당 콤팩트 모델에 대응 하는 파라미터 등을 전달받아 저장하거나, 프로세서 처리 결과의 전부 또는 일부를 다시 프로세서에 제공할 수도 있다. 또한, 저장부는 콤팩트 모델 처리 장치의 동작 수행을 위한 프로그램을 저장할 수도 있으며, 저장부에 저장된 프로그램은, 프로그래머 등의 설계자에 의해 직접 작성 또는 수정된 후 저장부 에 저장된 것일 수도 있고, 다른 물리적 기록 매체로부터 전달받아 저장된 것일 수도 있으며, 및/또는 유무 선 통신 네트워크를 통해 접속 가능한 전자 소프트웨어 유통망을 통하여 획득 또는 갱신된 것일 수도 있다. 저 장부는, 실시예에 따라, 주기억장치 및 보조기억장치 중 적어도 하나를 이용하여 구현될 수 있다. 출력부는 프로세서의 처리 결과나 저장부에 저장된 데이터 등을 외부로 출력할 수 있다. 예를 들 어, 출력부는 훈련에 의해 획득된 콤팩트 모델이나, 콤팩트 모델에 대응하는 파라미터(들)나, 콤팩트 모델 기반의 시뮬레이션 결과 등을 외부로 시각적 또는 청각적으로 출력할 수도 있다. 출력부는, 일례로, 디스플 레이, 프린터 장치, 스피커 장치, 영상 출력 단자 또는 데이터 입출력 단자 등을 이용하여 구현될 수도 있으나, 이에 한정되는 것은 아니다. 프로세서는, 콤팩트 모델과 관련된 각종 연산 처리나 제어 등의 동작을 수행할 수 있게 마련된다. 예를 들 어, 프로세서는 분석 대상의 생성 동작, 인공신경망(70, 80)의 훈련 동작, 콤팩트 모델의 획득 동작 및 모 의훈련 동작 중 적어도 하나의 동작을 순차적으로 또는 동시에 수행하도록 마련된 것일 수 있다. 보다 구체적으 로 예를 들어, 프로세서는 입력부, 통신부 및 저장부 중 적어도 하나(이하 입력부 등이라 함)의 정보를 기반으로 사용자의 수동 조작에 의해 또는 미리 정의된 바에 따라 자동으로 분석 대상을 생성하거 나, 이들(11, 13, 15)로부터 인공 신경망(80, 90)에 대한 정보를 획득하여 적어도 하나의 인공 신경망(80, 90) 을 구축하거나, 인공 신경망(80, 90)에 입력될 파라미터를 획득하고 이를 기반으로 인공 신경망(80, 90)을 훈련시키거나, 인공 신경망(80, 90)에 대응하는 콤팩트 모델을 생성하거나, 및/또는 콤팩트 모델로부터 추출된 파라미터를 이용하여 시뮬레이션을 수행할 수 있다. 이들 각각의 동작에 대해선 후술하도록 한다. 또한, 실시예 에 따라 프로세서는 저장부에 저장된 프로그램을 실행시켜 콤팩트 모델 처리 장치의 전반적인 동 작을 제어하는 것도 가능하다. 프로세서는, 예를 들어, 중앙 처리 장치(CPU: Central Processing Unit), 그래픽 처리 장치(GPU: Graphic Processing Unit), 마이크로 컨트롤러 유닛(MCU: Micro Controller Unit), 애 플리케이션 프로세서(AP: Application Processor), 전자 제어 유닛(ECU: Electronic Controlling Unit) 또는 상술한 동작 수행이 가능한 기타 전자 장치를 하나 이상 사용하여 구현될 수 있다. 일 실시예에 따른 프로세서는 분석대상획득부, 데이터 전처리부, 신경망 학습부, 모델추출 부 및 시뮬레이션부를 포함할 수 있다. 여기서, 분석대상획득부, 데이터 전처리부, 신경망 학습부, 모델추출부 및 시뮬레이션부 중 적어도 둘은, 실시예에 따라서, 논리적 또는 물리적으 로 구분되는 것일 수 있다. 또한, 분석대상획득부, 데이터 전처리부, 신경망 학습부, 모델추출 부 및 시뮬레이션부 중 적어도 하나는, 설계자나 사용자의 임의적 선택에 따라 생략되는 것도 가능하 다. 예를 들어, 시뮬레이션부는 생략될 수 있으며, 이 경우, 콤팩트 모델 처리 장치는, 모델추출부 가 획득한 모델 또는 이의 파라미터를 통신부나 출력부를 통해 외부로 출력하여 시뮬레이션부 가 마련된 다른 장치로 전달할 수 있으며, 시뮬레이션부가 마련된 다른 장치는 모델 또는 파라미터의 전달에 응하여 전달받은 모델 또는 파라미터를 기반으로 시뮬레이션을 수행할 수도 있다. 도 2a는 분석 대상의 일례에 대한 도면이다. 구체적으로 도 2a는 나노시트 펫에 대한 도면으로, 나노시트 펫의 Y축 기준 단면도이고, 도 2a의 사각형 내의 도면은 A-B 선에 따라 절개된 나노시트 펫의 X축 기준 단면도이다. 분석대상획득부는 사용자나 설계자 등이 제공한 정보를 기반으로 수동 조작에 따라 또는 자동으로 분석 대 상을 획득할 수 있다. 여기서, 분석 대상은, 예를 들어, 도 2a에 도시된 바와 같이, 나노시트 펫을 포함할 수도 있고, 사용자 또는 설계자 등이 제공한 정보는 나노시트 펫에 대한 정보를 포함할 수 있다. 나노시트 펫에 대한 정보는, 예를 들어, 나노시트 펫의 전체적인 구조, 전체적인 구조 내의 각각의 부분, 각각의 부분의 폭, 깊이 또는 넓이, 및/또는 각 부분의 소재 등에 대한 값 등을 하나 이상 포함할 수도 있다. 나노시트 펫의 정보는, 실시예에 따라 입력부, 통신부 및 저장부 중 적어도 하나로부터 회로, 케이블 또 는 무선통신 네트워크를 통해 획득된 것일 수 있다. 나노시트 펫의 정보 중 적어도 하나는, 후술하는 바와 같이 입력 파라미터(70, 70-1)로 이용될 수도 있다. 나노시트 펫에 대한 정보가 획득되면, 분석대상획득부 는 주어진 나노시트 펫에 대한 정보를 조합하여 대응하는 나노시트 펫을 2차원 또는 3차원의 형식 으로 획득할 수 있다. 이하 콤팩트 모델 처리 장치 및 프로세서의 동작을 설명함에 있어서, 나노시트펫이 분석 대상인 실시예를 이용하도록 하나, 이는 예시적인 것이다. 다른 실시예의 경우, 분석 대상은 이 와 상이하게 네거티브 커패시턴스 나노시트 펫(NC-NSFET)일 수도 있고, 또는 동종의 또는 이종의 다른 반도체 소자일 수도 있다. 네거티브 커패시턴스 나노시트 펫이나 다른 반도체 소자가 분석 대상인 경우, 프로세서(10 0)는, 실시예에 따라서, 나노시트 펫이 분석 대상인 경우와 동일하게 동작할 수도 있고, 또는 이와 적어도 부분적으로 상이하게 동작할 수도 있다. 분석대상획득부는 생략되는 것도 가능하다. 도 2a에 도시된 바를 참조하면, 나노시트 펫은, 소스 터미널(31, source, 이하I 소스)와, 드레인 터미널 (32, drain, 이하 드레인)과, 소스와 드레인 사이에서 전자의 전달 통로를 제공하는 채널(34: 34-1, 34-2, 34-3)을 포함할 수 있다(게이트(gate)는 그 도시를 생략함). 소스 및 드레인는 서로 대향하여 배치되되, z축 방향으로 연장 형성되고, x축 및 y축 방향으로 소정의 두께(L_SD)를 갖는 판의 형상을 가질 수 있다. 채널(34: 34-1, 34-2, 34-3)은 소스 및 드레인 사이에 배치 설치되며, 실시예에 따라 하나 이상 마련될 수 있다. 복수의 채널(34: 34-1, 34-2, 34-3)이 마련된 경우, 적어도 두 채널(34-1, 34-2)은 소정 거리(T_sus, 간격)로 이격되어 있을 수 있다. 복수의 채널(34: 34-1, 34- 2, 34-3) 각각 사이의 간격은 모두 동일할 수도 있고, 실시예에 따라, 그 전부 또는 일부가 상이할 수도 있다. 채널(34: 34-1, 34-2, 34-3)은, 소정의 폭(W_sheet, y 방향의 길이) 및 두께(T_sheet, z 방향의 길이)의 나노 시트(34-1A)를 하나 이상 포함할 수 있다. 전자는 나노시트(34-1A)를 통해 소스에서 드레인 방향으로 이동하게 된다. 소스와 적어도 하나의 채널(34: 34-1, 34-2, 34-3) 사이 및 드레인과 적어도 하나의 채널(34: 34-1, 34-2, 34-3) 사이 중 적어도 하나에는, 채널 도핑 구역(36: 36-1, 36-2)이 존재할 수 있다. 채널 도핑 구역 (36: 36-1, 36-2)은, 채널(34: 34-1, 34-2, 34-3)의 길이 방향을 따라, 채널(34: 34-1, 34-2, 34-3)의 소스 방향의 말단 및 드레인 방향의 말단 각각에 형성된 것일 수도 있다. 각 채널 도핑 구역(36: 36-1, 36- 2)의 y축 방향 길이는 채널이 도핑된 깊이(L_CDD, 이하 채널 도핑 깊이)를 나타낸다. 채널 도핑 깊이(L_CDD)는 각각의 채널(34: 34-1, 34-2, 34-3)에 이온이 주입되는 깊이를 의미하며, 통상 이온이 주입되는 깊이가 깊으면 깊을수록, 채널 커패시턴스는 감소하고, 이에 대응하여 각각의 채널(34: 34-1, 34-2, 34-3)을 따라 흐르는 전류 도 감소하게 된다. 그러므로, 채널 도핑 깊이(L_CDD)는 나노시트 펫의 전기적 성능을 결정하는 하나의 변수 가 된다. 채널 도핑 깊이(L_CDD)는 각각의 채널(34: 34-1, 34-2, 34-3)마다 모두 동일할 수도 있고, 일부의 채 널(34: 34-1, 34-2, 34-3)에 대해서는 상이할 수도 있으며, 또는 모든 채널(34: 34-1, 34-2, 34-3)에 대해 상 이할 수도 있다. 상황에 따라, 채널 도핑 깊이(L_CDD)는 스페이서의 두께(L_sp)보다 더 작을 수도 있다. 채널의 나노시트(34-1A)에는 유전체(37, ferroelectric(FE) material)가 더 형성될 수 있다. 유전체는 산화막의 형태로 나노시트(34-1A)를 둘러싸서 마련될 수 있으며, 일례로 나노시트(34-1A)의 상면 및/또는 하면 이나, x축 및/또는 y축 방향의 양측 방향(전후면 및/또는 양 측면 등)에 도포되어 마련될 수 있다. 유전체 는 소정 두께(T_FE, 산화막의 두께를 포함 가능함)로 나노시트(34-1A)에 형성될 수 있다. 만약 분석 대상이 나 노시트 펫이라면, 유전체는 하프늄산화물(HfO2)를 이용하여 구현될 수 있고, 분석 대상이 네거티브 커 패시턴스-나노시트 펫이라면, 유전체는 하프늄산화물(HfO2) 대신에 HfO2-ZrO2 고용체(solid solution)(HZO)를 이용하여 구현될 수도 있다. 소스에는 드레인 방향의 일 측에 소정 두께(L_sp)의 스페이서가 형성될 수 있다. 동일하게 드레인 에도 소스 방향의 일 측에 동일한 또는 상이한 두께(L_sp)의 스페이서가 형성될 수 있다. 각 스페 이서는 절연체를 이용하여 구현될 수 있다. 스페이서 사이에 마련된 공간에는 게이트가 설치될 수 있으 며, y축 방향으로 소정의 폭(L_g, 이하 게이트 길이)을 가질 수 있다. 게이트 길이(L_g)는 각 채널(34: 34-1, 34-2, 34-3)의 길이보다 더 클 수도 있고, 더 작을 수도 있으며, 또는 동일할 수도 있다. 도 2b는 물리/수학적 파라미터 값의 일례를 도시한 도표이다. 상술한 적어도 두 채널(34-1, 34-2) 사이의 z축 방향 간격(T_sus), 나노시트(34-1A)의 폭(W_sheet), 나노시트 (34-1A)의 두께(T_sheet), 채널 도핑 깊이(L_CDD), 유전체의 두께(T_FE, 일례로 산화막의 두께), 소스(3 1)나 드레인의 두께(L_SD) 및 게이트 길이(L_g) 중 적어도 하나는 사용자나 설계자가 입력부 또는 통신 부를 이용하여 입력한 것일 수 있다. 이들 값은, 예를 들어, 도 2b에 도시된 바와 같이 주어질 수 있다. 도 2b에 도시된 바를 참조하면, 게이트 길이(L_g)는 12 nm로, 나노시트(34-1A)의 두께(T_sheet)는 5 nm로, 나노시 트(34-1A)의 폭(W_sheet)은 20 내지 50 nm로, 유전체의 두께(T_FE)로 1.5 nm로, 두 채널(34-1, 34-2) 사이 의 간격(T_sus)은 10 nm로, 소스나 드레인의 두께(L_SD)는 10.5 nm로 주어질 수도 있다. 채널 도핑 깊 이(L_CDD)는, 2 nm, 4 nm 및 6 nm 중 적어도 하나로 정해질 수 있다. 채널(34: 34-1, 34-2, 34-3)의 도핑 값의경우, N은 1e16으로, P는 1e15으로 주어질 수도 있다. 소스나 드레인에 대한 도핑 값과 관하여, N은 6.5e20으로, P는 1e21로 주어질 수도 있다. 한편, 유전체를 이용하는 경우, HfO2-ZrO2 고용체에 대한 각각 의 값(α, β, ε_FE)은, 각각 -6.5e10 cm/F, 8.1e19 cm^5/FC^2 및 33으로 결정될 수도 있다. 그러나, 이들 값 은 예시적인 것으로, 설계자나 사용자는 임의적 선택에 따라 이들 값의 전부 또는 일부를 상술한 바와 상이하게 정의하는 것도 가능하다. 이들 값이 입력되면, 분석대상획득부는 입력된 값을 기반으로 분석 대상, 일례로 나노시트 펫를 가상 적으로 생성하고, 생성 결과를 데이터 전처리부 및 신경망 학습부 중 적어도 하나로 전달할 수 있다. 데이터 전처리부는, 획득된 정보에 대한 데이터 전처리를 수행하여, 인공신경망에 입력될 입력 파라미터 (70, 70-1)를 획득할 수도 있다. 예를 들어, 데이터 전처리부는, 분석대상획득부에 의해 설계된 분석 대상이나, 또는 입력부 등을 사용자로부터 제공받은 분석 대상에 대한 정보에 대해 데이터 전처리를 수행할 수 있다. 여기서, 데이터 전처리는, 예를 들어, 데이터 스케일링을 포함할 수 있으며, 데이터 스케일링은 입력 값에 적용되는 최소-최대 스케일링(Min-Max Scaling) 및 출력 값에 적용되는 로그 스케일링(logarithmic scaling) 중 적어도 하나를 포함할 수도 있다. 데이터 전처리부에 의해 전처리된 값은 신경망 학습부(12 0)로 전달될 수 있다. 데이터 전처리부는 실시예에 따라 생략 가능하다. 도 3은 일 실시예에 따른 제1 인공신경망에 대한 도면이고, 도 4는 일 실시예에 따른 제2 인공신경망에 대한 도 면이다. 신경망 학습부는, 입력부 등에서 전달되거나 또는 데이터 설계부 및 데이터 전처리부 중 적 어도 하나로부터 전달된 데이터를 기반으로, 도 3 및 도 4에 도시된 바와 같은 적어도 하나의 인공 신경망(80, 90, 이하 각각 제1 인공 신경망 및 제2 인공 신경망이라 함)을 훈련시킬 수 있다. 여기서, 적어도 하나의 인공 신경망(80, 90)에 대한 훈련은, 전달된 모든 데이터를 이용하여 수행될 수도 있고, 전달된 모든 데이터 중 일부 의 데이터를 이용하여 수행될 수도 있다. 후자의 경우, 훈련에 이용되지 않은 데이터는 인공 신경망(80, 90)에 대한 검증 데이터로 이용될 수도 있다. 인공 신경망(80, 90)의 훈련은 적어도 일 회 이상 반복 수행될 수 있다. 예를 들어, 인공 신경망(80, 90)의 훈련은 대략 이십만 에포크(epoch)로 수행될 수도 있다. 일 실시예에 의하면, 신경망 학습부는 도 3 및 도 4에 도시된 바와 같이, 두 개의 인공 신경망, 즉 제1 인공 신경망 및 제2 인공 신경망에 대한 훈련을 수행할 수도 있다. 제1 인공 신경망은 후술하는 전류-전압 모 델(I-V model)에 대응하는 것일 수도 있고, 제2 인공 신경망은 커패시턴스-전압 모델(C-V model)에 대응하 는 것일 수 있다. 이들(80, 90)은 사용자나 설계자의 선택에 따라 상호 독립적으로 또는 종속적으로 훈련되되, 상황에 따라 순차적으로 또는 동시에 수행될 수 있다. 제1 인공 신경망 및 제2 인공 신경망 중 적어도 하나의 훈련은, 후술하는 바와 같이 대응하는 입력 파라미터(70, 70-1) 및 출력 값(Ids(x), c_gg(x), c_gs(x), c_gd(x))을 이용하여 수행될 수 있다. 훈련에 이용되는 적어도 하나의 인공 신경망(80, 90)은, 실시예에 따라서, 심층 신경망(DNN: Deep Neural Network), 콘볼루션 신경망(CNN: Convolutional Neural Network), 콘볼 루션 순환 신경망(CRNN: Convolutional Recurrent Neural Network), 다층 퍼셉트론(Multi-layer Perceptron), 심층 신뢰 신경망(DBN: Deep Belief Network) 또는 심층 Q-네트워크(Deep Q-Networks) 등을 포함할 수 있다. 그러나, 신경망 학습부에 의해 훈련 가능한 인공 신경망은 이에 한정되는 것은 아니다. 상술한 적어도 하나의 인공 신경망(80, 90)은, 예를 들어, 하기의 수학식 1과 같이 표현 가능하다. [수학식 1]"}
{"patent_id": "10-2023-0050540", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 Y는 출력층(83, 93)의 값을, W^layer는 특정한 층의 가중치를, X는 입력 층(81, 91)에 입력되는 값(70: 71, 72, 73, 74)을, B^layer는 편향을 의미한다. 한편, 손실 함수(loss function)은 평균 제곱 오차나 교차 엔트로피 오차를 이용하여 구현될 수 있다. 일 실시예에 따르면, 손실 함수는 하기의 수학식 2와 같이 주 어질 수 있다. [수학식 2] 수학식 2에서 y_true.off, y_true.lin 및 y_true.sat는 각각 차단 영역, 선형 영역 및 포화 영역(saturation region)에 대한 실측 값이고, y_pred.off, y_pred.lin 및 y_pred.sat는 각각 차단 영역, 선형 영역 및 포화 영 역(saturation region)에 대한 예측 값이다. α, β 및 γ는 가중치이다. α는 차단 영역에 대한 실측 및 예측 값의 차이를 어느 정도 반영할 것인지를 정하기 위해 이용될 수 있고, β는 선형 영역에 대한 실측 및 예측 값 의 차이를 어느 정도 반영할 것인지를 정하기 위해 이용될 수 있다. 또한, γ는 포화 영역에 대한 실측 및 예측 값의 차이를 어느 정도 반영할 것인지를 정하기 위해 이용될 수 있다. 즉, 손실 함수는 차단 영역의 실측 값 및 예측 값 간의 평균 오차 제곱과, 선형 영역의 실측 값 및 예측 값 간의 평균 오차 제곱과, 포화 영역의 실측 값 및 예측 값 간의 평균 오차 제곱 각각의 가중합으로 주어질 수 있다. 여기서, 가중치 α, β 및 γ는 설계자나 사용자의 선택에 따라 임의적으로 결정될 수 있다. 상황에 따라 이들 가중치(α, β 및 γ) 중 적어도 하나는 0 으로 주어질 수도 있다. 스파이스(SPICE) 등의 시뮬레이터에서는, DC 시뮬레이션, AC 시뮬레이션, 트랜션트 시 뮬레이션(과도 시뮬레이션) 또는 몬테카를로 시뮬레이션을 수행할 수 있다. 이 경우, 아날로그 회로가 해당 시 뮬레이션에서 알맞게 동작하기 위해서는 포화 영역에서의 경사(gradient) 성분에 대한 정확도가 상당히 중요하 다. 다시 말해서, 포화 영역에 대한 높은 정확도가 요구된다. 만약 손실 함수가 수학식 2와 같이 주어졌다면, 포화 영역에 대한 가중치(γ)를 적절히 조절함으로써, 포화 영역에 대한 정확도가 보다 향상될 수 있게 된다. 이에 따라 콤팩트 모델에 대한 시뮬레이션의 전반적인 성능이 더욱 개선될 수 있게 된다. 적어도 하나의 입력 파라미터(70, 70-1)는 제1 인공 신경망 및 제2 인공 신경망 각각에 입력 값으로 이 용될 수 있다. 제1 인공 신경망에서 입력 값으로 이용되는 입력 파라미터 및 제2 인공 신경망에서 입력 값으로 이용되는 입력 파라미터(70-1)는 서로 동일할 수도 있고, 부분적으로 또는 전적으로 상이할 수도 있다. 적어도 하나의 입력 파라미터(70, 70-1)는 기 언급한 바와 같이 입력부 등이나, 데이터 설계부 나, 데이터 전처리부 등으로부터 전달된 것일 수 있다. 일 실시예에 따르면, 제1 및/또는 제2 인공신경망(80, 90)에 대한 입력 파라미터(70, 70-1) 중 적어도 하나는 적어도 하나의 기하 파라미터를 포함할 수 있다. 기하 파라미터는 분석 대상의 구조에 대한 파라미터로, 분석 대상의 적어도 하나의 요소(31, 32, 34, 35, 36 등)의 길이, 두께 또는 폭 등을 포함할 수 있 다. 일 실시예에 의하면, 기하 파라미터는 5개의 키 파라미터(key parameter)를 포함할 수 있다. 여기서, 5 개의 키 파라미터는, 예를 들어, 게이트 길이(L_g), 나노시트(34-1A)의 폭(W_sheet), 나노시트(34-1A)의 두께 (T_sheet), 스페이서의 두께(L_sp) 및 유전체의 두께(T_FE)를 포함할 수 있다. 실시예에 따라서, 기하 파라미터로 오직 이들 5개의 키 파라미터(L_g, W_sheet, T_sheet, L_sp, T_ox)만이 이용될 수도 있다. 기 하 파라미터로 5개의 키 파라미터 만을 사용하여 훈련하는 경우에도, 인공신경망이 98% 이상의 높은 정확도 를 갖게 된다. 따라서, 이들 5개의 키 파라미터(L_g, W_sheet, T_sheet, L_sp, T_ox)만을 기반으로 인공신경망 (80, 90)을 훈련시키는 것은 입력수의 차원을 축약 시키면서도, 높은 정확도의 시뮬레이션 결과를 획득할 수 있 게 되므로, 전체적인 자원 요구량이 감소하고 훈련 시간이 단축되는 효과를 얻을 수 있다. 제1 인공 신경망에 대한 입력 파라미터 및 제2 인공 신경망에 대한 입력 파라미터(70-1) 중 적어도 하나는, 일 실시예에 있어서, 공정 파라미터를 더 포함할 수도 있다. 공정 파라미터는 공정 기반의 변 수(process variable)로, BSIM-CMG와 같은 물리, 수학 방정식 기반의 콤팩트 모델에서는 정의가 불가능하다. 공 정 파라미터를 입력 값으로 이용하는 경우, 반도체 소자에 대한 성능을 보다 빠르고 정확하게 예측할 수 있게 되고, 콤팩트 모델은 확장성을 가질 수 있게 된다. 일 실시예에 따르면, 공정 파라미터는 채널 도 핑 깊이(L_CDD)를 포함할 수 있다. 채널 도핑 깊이(L_CDD)는, 상술한 바와 같이 이온이 주입되는 깊이로, 그 길 이에 따라 채널 커패시턴스 및 전류가 변화하게 된다. 채널 도핑 깊이(L_CDD)는 2, 4, 및 6 중 적어도 하나를 그 값으로 가질 수 있으나, 이에 한정되는 것은 아니다. 설계자나 사용자는 필요에 따라 채널 도핑 깊이(L_CD D)에 대한 값을 조절하여 입력 값으로 이용할 수도 있다. 입력 파라미터(70, 70-1)는, 필요에 따라, 온도를 더 포함할 수도 있다. 온도는 분석 대상, 일례로 나 노시트 펫의 온도를 의미한다. 온도는 대략 영하 40도 내지 영상 125도의 범위 내의 적어도 하나의 값 이 이용될 수 있으며, 예를 들어, 도 2b에 도시된 것처럼, 영하 40도(-40), 영상 27도 및 영상 125도 중 적어도 하나의 값이 온도로 제1 인공 신경망 및 제2 인공 신경망 중 적어도 하나에 입력될 수 있다. 그러나, 온도 역시 이에 한정되는 것은 아니며, 설계자나 사용자에 의해 필요에 따라 그 값이 조절될 수도 있다. 또한, 실시예에 따라서, 입력 파라미터(70, 70-1)는 적어도 하나의 바이어스 전압를 더 포함하는 것도 가능 하다. 바이어스 전압은, 소스와 드레인 사이의 전압(V_ds, 이하 소스-드레인 전압) 및 게이트에 인 가되는 전압(V_gs, 이하 게이트 전압) 중 적어도 하나를 포함할 수 있다. 아날로그 회로는 디지털 회로보다 더많은 전압 바이어스 데이터의를 추가함으로써 더 잘 모델링 될 수 있으므로, 적절한 전압 바이어스의 선택이 요 구된다. 따라서, 소스-드레인 전압 및 게이트 전압 중 적어도 하나는, 예를 들어, 도 2b에 도시된 것처럼 0V 내 지 0.65V 사이의 값을 가질 수 있되, 소스-드레인 전압은, 0.05V, 0.325V 및 0.65V 중 적어도 하나의 값으로 정 의되고, 게이트 전압은 0.05V, 0.25V, 0.325V, 0.5V 및 0.65V 중 적어도 하나의 값으로 정의될 수 있다. 실시 예에 따라, 소스-드레인 전압 및/또는 게이트 전압은 이들과 상이한 값으로 정의되는 것도 가능하다. 전류-전압 모델을 위한 제1 인공 신경망은 출력층의 출력 값으로 전압 인가 시 드레인에 전달되는 전류(Ids(x))를 이용한다. 도 3에 도시된 바를 참조하면, 제1 인공 신경망은 입력층, 은닉층 및 출 력층을 포함할 수 있다. 입력층은 기하 파라미터, 공정 파라미터, 온도 및 바이어스 전압 이 입력되는 층이다. 은닉층은 다수의 은닉 층, 일례로 2개의 은닉 층(82-1, 82-2)을 가질 수 있으며, 각각의 은닉 층(82-1, 82-2)은 수신한 값에 소정의 가중치(W^1, W^2, W^3) 및 편향(B^1, B^2, B^3)을 적용한 후, 결과 값을 다음 층(일례로 다음 은닉층(82-2)이나 출력층)으로 전달할 수 있다. 각 은닉 층(82-1, 82- 2)은, 각각 다른 층(81, 82-1, 82-2, 83)과 연결된 적어도 하나의 노드, 일례로 각 10개의 노드(g_11(x) 내지 g_29(x))를 포함할 수도 있다. 실시예에 따라, 각각의 노드(g_11(x) 내지 g_29(x))는, 활성화 함수로, 하이퍼볼 릭 탄젠트 함수, 계단 함수, 시그모이드 함수 또는 렐루(ReLU) 함수 등을 이용할 수 있다. 출력층은 상술한 바와 같이 게이트 전압 인가 시 전류(Ids(x))를 그 값으로 할 수 있다. 커패시턴스-전압 모델을 위한 제2 인공 신경망은 커패시턴스(c_gg(x), c_gs(x), c_gd(x))를 출력층의 값으로 이용할 수 있다. 구체적으로 도 4에 도시된 바를 참조하면, 제2 인공 신경망은, 일 실시예에 있어서, 입력층, 은닉층 및 출력층을 포함할 수 있다. 입력층은 상술한 것처럼 기하 파라미터 , 공정 파라미터, 온도 및 바이어스 전압이 입력되는 층이다. 은닉층은 수신한 값(일례로 입력 파라미터(70-1))에 소정의 가중치(W'^1, W'^2) 및 편향(B'^1, B'^2)을 적용한 후, 다음 층(일례로 출력층 )으로 전달할 수 있다. 은닉층은 다른 층(91, 93)과 연결된 적어도 하나의 노드, 일례로 10개의 노드 (g'_11(x) 내지 g'_19(x))를 포함할 수 있다. 각 노드(g'_11(x) 내지 g'_19(x))의 활성화 함수는, 예를 들어, 하이퍼볼릭 탄젠트 함수, 시그모이드 함수, 계단 함수 또는 렐루 함수 등을 포함할 수 있다. 출력층은 출력 값으로 적어도 하나의 커패시턴스(c_gg(x), c_gs(x), c_gd(x))를 가질 수 있다. 여기서, 적어도 하나의 커패시 턴스는, 예를 들어, 게이트의 커패시턴스(c_gg(x)), 게이트 및 소스 간의 커패시턴스(c_gs(x)) 및 게이트 및 드레인 간의 커패시턴스(c_gd(x)) 중 적어도 하나를 포함할 수 있다. 실시예에 따라서, 신경망 학습부는, 인공 신경망(80, 90)의 훈련 이전에, 훈련될 인공 신경망(80, 90) 중 적어도 하나를 생성하도록 마련될 수도 있다. 인공 신경망(80, 90)의 생성은 입력부 등에서 전달된 인공 신 경망(80, 90)에 대한 정보를 이용하여 수행될 수 있다. 예를 들어, 신경망 학습부는 입력부 등으로부 터 전달된 제1 인공 신경망에 대한 노드의 개수나 층의 개수를 기반으로 제1 인공 신경망을 생성하고, 입력부 등으로부터 전달된 제2 인공 신경망에 대응하는 노드의 개수나 층의 개수를 기반으로 제2 인공 신경망을 생성할 수 있다. 또한, 신경망 학습부는, 실시예에 따라서, 인공 신경망(80, 90)에 대한 훈련 수행되는 과정에서 또는 인공 신경망(80, 90)에 대한 훈련이 종료된 이후에 훈련된 인공 신경망(80, 90)의 검증을 수행하도록 설계되는 것도 가능하다. 다시 말해서, 신경망 학습부는 인공 신경망(80, 90)의 정확도를 판단할 수도 있다. 만약 검증 결과, 인공 신경망(80, 90)이 적합하지 않은 것으로 판단되면, 신경망 학습부는 인공 신경망을 재생성하거 나, 재훈련시킬 수 있다. 재생성은 기존에 입력된 인공 신경망(80, 90)에 대한 정보가 아닌 다른 정보를 이용하 여 수행될 수도 있으며, 다른 정보는 사용자 등에 의해 새로 입력된 것일 수 있다. 재훈련 시에는 기존의 분석 대상에 대한 정보를 다시 이용할 수도 있고, 또는 사용자로부터 새로 입력 받은 정보를 이용할 수도 있다. 훈련된 인공신경망(80, 90) 그 자체 또는 훈련된 인공신경망(80, 90)에 관한 적어도 하나의 정보(일례로 가중치 나 바이어스 등)는, 모델추출부로 전달될 수 있다. 일 실시예에 따르면, 모델추출부는 훈련된 인공신경망(80, 90)을 이용하여 훈련된 인공신경망(80, 90)에 대응하는 적어도 하나의 콤팩트 모델을 추출 및 획득할 수 있다. 예를 들어, 모델추출부는 훈련된 인공신 경망(80, 90)으로부터 필요한 파라미터를 자동으로 추출하고, 추출된 파라미터를 이용하여 코드를 생성하여, 적 어도 하나의 콤팩트 모델을 생성할 수 있다. 여기서, 콤팩트 모델은 제1 인공신경망에 대응하는 전류-전압 모델 및 제2 인공신경망에 대응하는 커패시턴스-전압 모델 중 적어도 하나를 포함하는 것도 가능하다. 콤팩 트 모델의 생성은, 예를 들어, 베릴로그-A(Verilog-A) 언어를 기반으로 구현될 수도 있으나, 이에 한정되는 것 은 아니다.일 실시예에 따르면, 모델추출부는 벡터화(vectorization) 기법을 이용하여 구현된 것일 수도 있다. 즉, 모델추출부는, 하나의 명령어로 다수의 데이터에 대해 최대한 많은 연산 처리를 수행 가능하도록, 상대적 으로 많은 자원 및 시간을 요구하는 반복 명령어(for 등)이나 조건 명령어(if 등)을 가급적 또는 최대한 배제하 여 설계된 것일 수 있다. 또한, 모델추출부는 콤팩트 모델의 생성을 위해 가중치나 편향을 저장 또는 처리 할 때 배열을 이용하지 않고 변수만을 이용하도록 마련된 것일 수도 있다. 배열은 통상 인덱싱을 이용하므로, 변수보다 접근 및 처리가 지연될 수밖에 없다. 따라서, 배열 대신에 변수를 이용한다면, 메모리 접근 속도나 처 리 속도를 개선하여 전체적인 동작 시간(runtime)을 절감할 수 있게 된다. 한편, 코드 내의 불필요한 변수가 메 모리 공간을 점유하는 것을 방지하기 위해 모델추출부는 변수의 개수도 최소화되어 구현될 수 있다. 이하 도 5 및 도 6을 참조하여, 인공신경망(80, 90)을 기반으로 콤팩트 모델을 획득하는 과정을 보다 구체적으 로 설명한다. 도 5 및 도 6은 각각 일 실시예에 따른 모델설계부에 대한 제1 슈도코드 및 제2 슈도코드로, 각각 베릴로그-A 언어를 기반으로 구현된 프로그래밍 코드의 슈도코드이다. 도 5에 도시된 바를 참조하면, 모델추출부는, 콤팩트 모델을 획득하기 위하여, 먼저 분석 대상(일례로 나 노시트 펫이나 네거티브 커패시턴스 나노시트 펫 등과 같은 반도체 소자) 및 분석 대상의 구조(일례로 소스 , 드레인, 게이트 터미널 및 바디(벌크) 등과 같은 터미널 등)을 결정할 수 있다(라인 1). 분석 대상 및 이의 구조는, 실시예에 따라서, 입력부 등으로부터 전달된 것일 수도 있고, 분석대상획득부로부터 전달 받은 것일 수도 있다. 모델추출부가 이용할 적어도 하나의 변수가 선언된다(라인 2 및 3). 선언되는 변수는, 예를 들어, 훈련된 인공신경망(80, 90)으로부터 추출된 가중치(W), 바이어스(B) 및 은닉층(82-1, 82-2, 92)의 출력 값을 위한 변수 를 포함할 수 있고(제2 라인), 또한 입력 값(X, 일례로 입력 파라미터) 각각에 대한 변수와, 출력 값(Y, 일 례로 전류(Ids(x))나, 커패시턴스(c_gg(x), c_gs(x), c_gd(x)) 등) 각각에 대한 변수를 포함할 수 있다(제3 라 인). 또한, 필요에 따라 반도체 소자(핀펫, 게이트올어라운드, 나노시트 펫 또는 네거티브 커패시턴스-나노시트 펫)의 핀(fin)의 개수를 위한 변수도 더 선언될 수도 있다. 이어서 전류-전압 모델(I-V model) 및 커패시턴스-전압 모델(C-V model)이 연산 될 수 있다(라인 4 및 라인 5). 전류-전압 모델의 연산과 커패시턴스-전압 모델의 연산은 순차적으로 수행될 수도 있고 또는 동시에 수행될 수 도 있다. 전류-전압 모델의 경우, 제1 인공신경망을 기반으로 적어도 하나의 은닉층, 일례로 2개의 은닉층이 생성될 수 있다(라인 4.1 내지 4.4). 먼저 훈련된 제1 인공신경망으로부터 훈련된 제1 인공신경망의 첫번째 층 (82-1)에 대응하는 가중치(W^1), 바이어스(B^1) 및 출력 값(A^1)이 획득되고, 획득된 가중치(W^1), 바이어스 (B^1) 및 출력 값(A^1)을 이용하여 전류-전압 모델의 첫번째 은닉층의 노드가 생성된다(라인 4.1). 전류-전압 모델의 첫번째 은닉층의 노드는, 제1 인공신경망의 첫번째 층(82-1)에 대응할 수 있으며, 상황에 따라 제1 인공신경망의 첫번째 층(82-1)과 대체로 동일할 수도 있다. 이어서 해당 은닉층의 노드에 대해 적어도 하나 의 활성화 함수가 추가된다(라인 4.2). 여기서, 활성화함수는 훈련된 제1 인공신경망에서 이용된 활성화함 수와 동일하게 결정될 수 있으며, 예를 들어, 하이퍼볼릭 탄젠트 함수(tanh(A1))를 포함할 수 있다. 이어서, 훈 련된 제2 인공신경망으로부터 해당 제2 인공신경망의 두번째 층(82-2)의 가중치(W^2), 바이어스(B^2) 및 출력 값(A^2)이 획득되고, 획득된 가중치(W^2), 바이어스(B^2) 및 출력 값(A^2)을 기반으로 전류-전압 모델 의 두번째 은닉층의 노드가 생성될 수 있다(라인 4.3). 마찬가지로 전류-전압 모델의 두번째 은닉층의 노드는, 훈련된 제1 인공신경망의 두번째 층(82-2)에 대응할 수 있다. 이 경우, 두번째 은닉층의 입력 값은 전류-전 압 모델의 첫번째 은닉층의 출력 값(A^1)일 수 있다. 두번째 은닉층의 노드에 대해서도 활성화 함수(일례로 하 이퍼볼릭 탄젠트 함수(tanh(A2)))가 추가될 수 있다(라인 4.4). 또한, 훈련된 제1 인공신경망의 출력층에 대응하여 전류-전압 모델의 출력층도 생성될 수 있다(라인 4.5). 전류-전압 모델의 출력층은 하나의 노드를 포 함할 수 있으며, 해당 노드는 제1 인공신경망의 출력층에 대응하는 가중치(W^3), 바이어스(B^3) 및 출력 값 (Y)를 이용하여 생성될 수 있다. 이와 같이 생성된 전류-전압 모델은 제1 인공 신경망에 대응되며, 대체로 제1 인공 신경망과 동일할 수도 있다. 예를 들어, 전류-전압 모델은 베릴로그-A 언어를 기반으로 변환된 제 1 인공 신경망일 수도 있다. 커패시턴스-전압 모델의 경우, 제2 인공 신경망을 기반으로 소정의 은닉층이 생성된다(라인 5.1 내지 5.2). 이 경우, 훈련된 제2 인공신경망으로부터 훈련된 제2 인공신경망의 은닉층에 대응하는 가중치 (W'^1), 바이어스(B'^1) 및 출력 값(A'^1)이 추출되어 획득되고, 이들 가중치(W'^1), 바이어스(B'^1) 및 출력값(A'^1)을 이용하여 커패시턴스-전압 모델의 은닉층이 생성된다(라인 5.1). 해당 은닉층의 노드에는, 하이퍼볼 릭 탄젠트 함수(tanh(A'1)) 등과 같은 소정의 활성화 함수가 부가될 수 있다(라인 5.2). 부가되는 활성화 함수 는, 제2 인공신경망의 활성화 함수와 동일할 수도 있다. 커패시턴스-전압 모델의 출력층은 훈련된 제2 인공 신경망의 출력층에 대응하여 획득될 수 있다(라인 5.3). 커패시턴스-전압 모델의 출력층의 노드는 제2 인공 신경망의 출력층에 대응하는 가중치(W'^2), 바이어스(B'^2) 및 출력 값(Y_cv)를 이용하여 생성될 수 있다. 커패시턴스-전압 모델의 출력층에 대한 입력 값은 커패시턴스-전압 모델의 은닉층의 출력 값(A'1)을 포함할 수 있다. 상술한 바와 같이 생성된 커패시턴스-전압 모델은 대략 제2 인공 신경망과 동일하거나 대응될 수 있 다. 예를 들어, 커패시턴스-전압 모델은 베릴로그-A 언어로 변환된 제2 인공 신경망일 수도 있다. 도 6에 도시된 바를 참조하면, 적어도 하나의 커패시턴스(C_gg, C_gs, C_gd, C_gb)가 연산 될 수 있다(라인 6). 여기서, 적어도 하나의 커패시턴스(C_gg, C_gs, C_gd, C_gb)는, 예를 들어, 게이트 커패시턴스(C_gg)와, 게이트 및 소스 사이의 커패시턴스(C_gs, 이하 게이트-소스 커패시턴스)와, 게이트 및 드레인 사이의 커패시턴 스(C_gd, 이하 게이트-드레인 커패시턴스)와, 게이트 및 바디 간의 커패시턴스(C_gb, 이하 게이트-바디 커패시 턴스)를 포함할 수도 있다. 보다 상세하게 예를 들어, 게이트 커패시턴스(C_gg)는 핀(fin)의 개수에 제2 인공신 경망의 출력 값 중 어느 하나의 출력 값(Y_CV1, 도 4의 c_gg(x))을 곱하여 결정될 수 있고(라인 6.1), 게이 트 및 소스 간의 커패시턴스(C_gs)는 핀의 개수에 제2 인공신경망의 출력 값 중 다른 하나의 출력 값 (Y_CV2, 도 4의 c_gs(x))을 곱하여 결정될 수 있으며(라인 6.1), 게이트 및 드레인 간의 커패시턴스(C_g d)는 핀의 개수에 제2 인공신경망의 출력 값 중 또 다른 하나의 출력 값(Y_CV3, 도 4의 c_gd(x))을 곱하여 결정될 수 있다(라인 6.3). 또한, 게이트-바디 커패시턴스(C_gb)는 게이트 커패시턴스(C_gg)에 게이트-소스 커 패시턴스(C_gs) 및 게이트-드레인 커패시턴스(C_gd)를 차감하여 획득될 수 있다(라인 6.4). 또한, 적어도 하나의 전류(I_ds, I_gd, I_gs, I_gb)도 연산 될 수 있다(라인 7). 여기서, 적어도 하나의 전류 (I_ds, I_gd, I_gs, I_gb)는, 예를 들어, 소스 및 드레인 사이의 전류(I_ds), 게이트 및 드레인 사이의 전류(I_gd), 게이트 및 소스 사이의 전류(I_gs) 및 게이트 및 바디 간의 전류(I_gb) 중 적어도 하나 를 포함 가능하다. 소스 및 드레인 사이의 전류(I_ds)는, 핀의 개수, 출력 값 및 소스-드레인 간의 전압의 곱보다 작게 주어질 수 있다(라인 7.1). 게이트 및 드레인 사이의 전류(I_gd)는 위의 과정(라 인 6.2)에서 연산된 게이트와 드레인 간의 커패시턴스(C_gd, 양의 값일 수 있음)에 게이트와 드레인 간 의 전압의 시간에 따른 변화량을 곱한 값보다 작게 주어질 수 있다(라인 7.2). 게이트 및 소스 사이의 전류 (I_gs)는 기 연산된 게이트와 소스 간의 커패시턴스(C_gs, 양의 값일 수 있음. 라인 6.3)에 시간에 따른 게 이트와 소스 사이의 전압의 변화량을 곱한 값보다 작게 주어질 수 있다(라인 7.3). 또한, 게이트 및 바디 간의 전류(I_gb)는 게이트와 바디 간의 커패시턴스(C_gs, 양의 값일 수 있음. 라인 6.4)에 게이트와 바디 간의 전압의 시간에 따른 변화량을 곱한 값보다 작게 주어질 수 있다(라인 7.4). 트랜스컨덕턴스(transconductance, gm) 및 드레인컨덕턴스(drain conductance, gds) 중 적어도 하나도 연산될 수 있다(라인 8). 이 경우, 트랜스컨덕턴스(gm)는 소스 및 게이트 간의 전압의 변화에 따른 소스 및 드 레인 사이의 전류의 변화량으로 주어지고(라인 8.1), 드레인컨덕턴스(gds)는 소스 및 드레인 사이 의 전압의 변화에 따른 소스 및 드레인 사이의 전류의 변화량으로 주어질 수 있다(라인 8.2). 상술한 바에 따라 획득된 전류-전압 모델, 커패시턴스-전압 모델 및 적어도 하나의 값(들)(커패시턴스, 전류, 트랜스컨덕턴스 및/또는 드레인컨덕턴스 등)에 의해 제1 및 제2 인공신경망(80, 90)을 기반으로 하는 콤팩트 모 델이 생성된다. 생성된 콤팩트 모델은 상대적으로 변수만을 이용하여 구현되며, 조건 명령 등이 부재하므로, 종 래의 콤팩트 모델에 비해 상대적으로 간단하게 구축된다. 따라서, 콤팩트 모델에 대한 분석이 보다 신속하게 수 행 가능하게 된다. 생성된 콤팩트 모델은 저장부에 저장되거나, 통신부나 출력부를 통해 다른 장치 등으로 전달되거나, 또는 시뮬레이션을 위해 시뮬레이션부로 전달될 수 있다. 시뮬레이션부는 이와 같이 생성된 콤팩트 모델을 이용하여 분석 대상에 대한 시뮬레이션을 수행할 수 있다. 일 실시예에 의하면, 시뮬레이션부는 스파이스를 채용하여 구현된 수도 있으며, 예를 들어, DC 시뮬 레이션, AC 시뮬레이션, 트랜션트 시뮬레이션 및 몬테카를로 시뮬레이션 중 적어도 하나를 수행할 수도 있다. 시뮬레이션부는, 실시예에 따라서, 생략되는 것도 가능하다. 또한, 사용자나 설계자 등의 선택에 따라, 시 뮬레이션 이전에 구멜 대칭 실험(Gummel Symmetry Test)이 더 수행될 수도 있다. 상술한 콤팩트 모델 처리 장치는 상술한 인공신경망의 훈련, 모델링 및 시뮬레이션 중 적어도 하나에 대한 처리를 수행할 수 있도록 특별히 고안된 장치를 이용하여 구현될 수도 있고, 또는 하나 또는 둘 이상의 정보처 리장치를 단독으로 이용하거나 조합 이용함으로써 구현될 수도 있다. 여기서, 하나 또는 둘 이상의 정보처리장치는, 예를 들어, 서버용 하드웨어 장치, 데스크톱 컴퓨터, 랩톱 컴퓨터, 스마트 폰, 태블릿 피씨, 스마트 시계, 휴대용 게임기, 내비게이션 장치, 원격 제어 장치(리모컨), 디지털 텔레비전, 셋 톱 박스, 미디어 스트리 밍 장치, 음향 재생 장치(인공 지능 스피커 등), 가전 기기, 유인 또는 무인 이동체(차량, 이동성 로봇 또는 무 선 모형 차량 등), 유인 또는 무인 비행체(항공기나, 멀티콥터 등), 로봇(가정용, 산업용 또는 군사용 등) 또는 산업용 기계 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 설계자나 사용자 등은, 상황이나 조건에 따라, 이들 정보처리장치 외의 다른 장치(들) 중 적어도 하나를 상술한 콤팩트 모델 처리 장치로 고려하여 채용할 수 있다. 이하 도 7 내지 도 14를 참조하여 상술한 콤팩트 모델 처리 장치의 효과를 설명하도록 한다. 도 7은 네거티브 커패시턴스-나노시트 펫에 대해서, 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래의 BSIM 기반 시뮬레이션 장치 각각의 회로 시뮬레이션 결과를 도시한 도표로, 네거티브 커패시턴스-나노시트 펫의 데이 터를 이용해 인공신경망을 학습하고, 이로부터 콤팩트 모델을 획득한 후, 획득한 콤팩트 모델과 기존의 BSIM에 의한 콤팩트 모델 각각에 대한 시뮬레이션 결과를 비교 도시한 것이다. 해당 시뮬레이션은 디지털 회로로 9-스 테이지 링 오실레이터(RO: Ring Oscillator)를 채용하고, 아날로그 회로로 5 트랜지스터-연산 전달컨덕턴트 증 폭기(5T-OTA: 5 Transistor Operational Transconductance Amplifier)를 채용하여 수행된 것이다. 도 7에 도시된 바를 참조하면, 네거티브 커패시턴스-나노시트 펫과 관련하여 상술한 콤팩트 모델 처리 장치에 의한 시뮬레이션 결과 및 종래의 BSIM 기반 시뮬레이션 장치에 의한 시뮬레이션 결과는, 9-스테이지 링 오실레 이터의 경우, 주기 및 전력 모두 그 차이가 1% 미만으로 그 값이 실질적으로 동일하였고, 5 트랜지스터-연산 전 달컨덕턴트 증폭기의 경우에도, 이득 및 전력은 그 차이가 0.5% 미만으로 거의 차이가 없었다. 다만, 5 트랜지 스터-연산 전달컨덕턴트 증폭기를 채용한 경우, 대역폭에 있어서 2.63% 정도의 차이를 보였다. 도 8은 나노시트 펫에 대해서, 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래의 BSIM 기반 시뮬레이션 장치 각각의 회로 시뮬레이션 결과를 도시한 도표로, 나노시트 펫의 데이터를 이용해 인공신경망을 학습하고, 콤팩트 모델을 획득한 후, 획득한 콤팩트 모델과 기존의 BSIM에 의한 콤팩트 모델 각각의 시뮬레이션 결과를 비교하여 도시한 것이다. 상술한 바와 동일하게 해당 시뮬레이션은 디지털 회로로 9-스테이지 링 오실레이터를 채용하고, 아날로그 회로로 5 트랜지스터-연산 전달컨덕턴트 증폭기를 채용하여 수행된 것이다. 도 8에 도시된 바에 따르면, 나노시트 펫과 관련하여 상술한 콤팩트 모델 처리 장치에 의한 시뮬레이션 결과 및 종래의 BSIM 기반 시뮬레이션 장치에 의한 시뮬레이션 결과는, 9-스테이지 링 오실레이터를 이용한 경우에는, 주기는 그 차이가 0.5%이고, 전력은 그 차이가 0.43%로 양 실험 결과는 실질적으로 동일하였다. 또한, 5 트랜지 스터-연산 전달컨덕턴트 증폭기를 이용한 경우에도, 그 이득 및 전력은 그 차이가 각각 0.29% 및 0.23%으로 극 히 작았다. 다만, 네거티브 커패시턴스-나노시트 펫의 경우와 유사하게, 5 트랜지스터-연산 전달컨덕턴트 증폭 기를 채용한 경우, 대역폭은 약 2.06% 정도의 차이가 존재하였다. 도 9는 다수의 공정 파라미터에 따라 획득된 C-V 곡선의 일례를 도시한 그래프 도면으로, 분석 대상은 네거티브 커패시턴스 나노시트 펫이고, 공정 파라미터는 2nm, 4nm 및 6nm로 주어진 상황 각각으로부터 획득된 C-V 곡선을 도시한 것이다. 도 9에서 적색, 청색 및 흑색 선은 TCAD를 이용한 경우를 나타내고, 원, 삼각형 및 역 삼각형은 상술한 인공신경망을 이용한 경우를 나타낸다. 도 9에 도시된 바를 참조하면, 위의 인공신경망을 이용한 콤팩트 모델을 이용하여 획득된 C-V 곡선은, 공정 파 라미터를 달리한다고 하더라도, TCAD를 이용하였을 때의 C-V 곡선과 그 값이나 기울기 등이 대체로 동일함을 알 수 있다. 이들 간의 오차는 전반적으로 1% 미만으로 나타나, 위의 인공신경망을 이용한 콤팩트 모델은 정확도가 상당히 높음을 알 수 있다. 특히 이러한 인공신경망을 이용한 콤팩트 모델은 종래 기술에 비해 상대적으로 단축 된 시간 내에 생성 가능하므로, 시간 및 경제적인 면에서 우수한 효과를 가짐을 알 수 있다. 도 10은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 링 오실레이터의 트랜션트 시뮬레 이션 결과를 비교 도시한 그래프 도면으로, 여기서 BSIM가 종래 기술로 채용되었다. 도 10에 도시된 바에 의하면, 상술한 콤팩트 모델 처리 장치에 의한 17 스테이지 링 오실레이터 트렌션트 시뮬 레이션에 의해 획득된 시간의 흐름에 따른 출력 전압의 변화는, BSIM에 의한 17 스테이지 링 오실레이터 트렌션 트 시뮬레이션 결과 획득된 시간의 흐름에 대한 출력 전압의 변화와, 나노시트 펫 및 네거티브 커패시턴스 나노 시트 펫 양자 모두에 대해서, 전압의 최대/최소 크기, 전압의 변화 시점 및 변화의 형태 등이 대부분 일치함을 알 수 있다. 다시 말해서, 링 오실레이터 트렌션트 시뮬레이션과 관련해서 상술한 콤팩트 모델 처리 장치는 높 은 정확성을 달성할 수 있음을 알 수 있다.도 11은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 트랜지스터 OTA의 시뮬레이션 결과 를 비교 도시한 그래프 도면으로, 종래 기술로는 BSIM가 이용되었다. 도 11에 도시된 바를 참조하면, 콤팩트 모델 처리 장치 기반의 5 트랜지스터-연산 전달컨덕턴트 증폭기의 시뮬 레이션 결과는, 나노시트 펫 및 네거티브 커패시턴스 나노시트 펫 모두에 대해서, BSIM 기반의 5 트랜지스터-연 산 전달컨덕턴트 증폭기의 시뮬레이션 결과와 대체로 일치함을 알 수 있다. 다시 말해서, 콤팩트 모델 처리 장 치는, 5 트랜지스터-연산 전달컨덕턴트 증폭기의 시뮬레이션과 관련해서도, 그 정확성이 높음을 알 수 있다. 도 12는 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 시뮬레이션 처리 시간의 비교 결과 를 도시한 도표로, 링 오실레이터 스테이지의 개수에 따른 처리 시간의 비교 결과를 도시한 것이다. 도 13은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 시뮬레이션 처리 시간의 비교 결과를 그래프 형식으로 도시한 도면이다. 여기서, 종래 기술로는 BSIM가 이용되었다. 도 13에서 x축은 링 오실레이터 스테이 지의 개수를 의미하고, y축은 시간(초)를 의미한다. 도 12 및 도 13에 도시된 바를 참조하면, 링 오실레이터 스테이지 개수가 3, 5, 7, 9, 11 및 13의 경우, BSIM 기반 시뮬레이션의 처리 시간은 232.89초, 418.32초, 634.36초, 682.1초, 759.57초 및 792.95초로, 링 오실레 이터 스테이지 개수의 증가에 따라 상당히 급격하게 증가함을 알 수 있다. 반면에 상술한 인공신경망 기반의 콤 팩트 모델에 따른 시뮬레이션의 처리 시간은, 링 오실레이터 스테이지 개수가 3, 5, 7, 9, 11 및 13의 경우, 각 각 71.723초, 88.62초, 110.57초, 142.86초, 183.97초 및 225.42초로 BSIM 기반 시뮬레이션의 처리 시간보다 매우 낮을 뿐만 아니라, 그 증가 폭도 BSIM 기반 시뮬레이션의 경우보다 상대적으로 작다. 따라서, 콤팩트 모델 처리 장치는 상술한 바와 같이 높은 정확성을 가지고 있으면서도, 종래 기술에 비해 훨씬 신속하게 시뮬레이션 을 수행할 수 있음을 알 수 있다. 도 14는 채널 도핑 깊이에 따른 온-커런트 분포를 도시한 그래프 도면이다. 도 14에 도시된 바에 의하면, 상술한 인공신경망 기반의 콤팩트 모델을 이용하는 경우에서 채널 도핑 구역의 길 이에 따른 온-커런트의 값은 대체로 반비례 관계에 있으며, TCAD를 이용하는 경우에서의 채널 도핑 구역의 길이 에 따른 온-커런트 값과 실질적으로 일치하고 있다. 따라서, 인공신경망 기반의 콤팩트 모델은 채널 도핑 구역 의 길이에 따른 전류의 변화도 적절히 반영하고 있으며, 이는 분석 대상에 대한 시뮬레이션의 정확성을 더욱 더 향상시키는 효과를 갖는다. 이하 도 15를 참조하여 콤팩트 모델 처리 방법의 일 실시예를 설명하도록 한다. 도 15는 일 실시예에 따른 콤팩트 모델 처리 방법의 흐름도이다. 도 15에 도시된 바에 의하면, 먼저 사용자나 설계자가 분석 대상에 대한 정보를 제공하면, 이에 대응하여 분석 대상이 획득될 수 있다. 분석 대상은, 예를 들어, 모스 펫, 핀펫, 게이트올어라운드, 나노시트 펫 또는 네 거티브 커패시턴스-나노시트 펫 등을 포함할 수 있다. 분석 대상에 대한 정보는, 분석 대상의 구조나 구성에 대 한 정보를 포함할 수 있으며, 예를 들어, 반도체 소자의 전체적인 구조, 각 부분, 각각의 부분의 크기에 대한 수치(폭, 깊이 또는 넓이) 및 각 부분의 소재 중 적어도 하나를 포함할 수 있으나, 이에 한정되지는 않는다. 분 석 대상에 대한 정보가 주어지면, 이를 조합하여 분석 대상이 2차원 또는 3차원의 형식으로 가상적으로 생성된 다. 분석 대상의 획득 과정은, 실시예에 따라서, 생략될 수도 있으며, 이 경우, 사용자나 설계자가 제공한 분석 대상에 대한 정보는, 인공신경망에 대한 입력 파라미터로 이용될 수 있다. 실시예에 따라서, 분석 대상에 대한 정보는 데이터 전처리가 수행된 후, 입력 파라미터로 이용될 수도 있다. 인공신경망의 학습을 위해 데이터 전처리가 수행될 수 있다. 데이터 전처리는 분석 대상에 대한 정보에 대 해 수행될 수도 있다. 이에 따라 인공신경망에 입력될 입력 파라미터가 획득된다. 여기서, 입력 파라미터는 분 석 대상에 대한 정보의 전부 또는 일부를 포함할 수도 있다. 예를 들어, 입력 파라미터는, 기하 파라미터, 공정 파라미터, 온도 및 바이어스 중 적어도 하나를 포함할 수 있다. 여기서, 기하 파라미터는 게이트 길이, 나노시 트의 폭, 나노시트의 두께, 스페이서의 두께 및 유전체의 두께와 같은 5개의 키 파라미터를 포함할 수 있다. 공 정 파라미터는 공정 기반의 변수로, 채널 도핑 깊이 등을 포함할 수 있다. 데이터 전처리는, 예를 들어, 최소- 최대 스케일링 또는 로그 스케일링 등과 같은 데이터 스케일링을 포함할 수 있다. 데이터 전처리 과정은, 실시예에 따라서, 생략될 수도 있으며, 이 경우, 사용자나 설계자가 직접 제공한 분석 대상에 대한 정보가 그대 로 입력 파라미터로 이용될 수도 있고, 상술한 바와 같이 정보 조합에 따라 획득된 분석 대상의 정보가 입력 파 라미터로 이용될 수도 있다.입력 파라미터를 기반으로 인공 신경망에 대한 훈련이 수행될 수 있다. 여기서, 훈련되는 인공 신경망은, 예를 들어, 전류-전압 모델에 대응하는 제1 인공 신경망과, 커패시턴스-전압 모델에 대응하는 제2 인공 신경망 을 포함할 수 있다. 제1 인공 신경망은 입력층, 은닉층 및 출력층을 포함하되, 은닉층은 순차적으로 연결된 2개 의 은닉층을 포함할 수 있다. 각 은닉 층은, 각각 다른 층과 연결된 적어도 하나의 노드, 일례로 각 10개의 노 드를 포함할 수도 있다. 입력층의 입력 값으로는, 상술한 입력 파라미터(기하 파라미터, 공정 파라미터, 온도 및/또는 바이어스)가 이용될 수 있고, 출력층의 출력 값으로는 전압 인가 시 드레인에 전달되는 전류가 이용될 수도 있다. 제2 인공 신경망은 입력층, 은닉층 및 출력층을 포함할 수 있다. 제2 인공 신경망의 입력층의 입력 값은 상술한 입력 파라미터(기하 파라미터, 공정 파라미터, 온도 및/또는 바이어스 등)를 포함할 수 있고, 출력 층의 출력 값은 커패시턴스(게이트 커패시턴스, 게이트-소스 커패시턴스 및/또는 게이트-드레인 커패시턴스 등)를 포함할 수 있다. 제1 및 제2 인공신경망의 각 노드의 활성화 함수로는, 하이퍼볼릭 탄젠트 함수, 시그모 이드 함수, 계단 함수 및 렐루 함수 중 적어도 하나를 채용 가능하다. 제1 인공 신경망 및 제2 인공 신경망은 실시예에 따라서 동일 또는 동종의 인공 신경망을 이용하여 구현될 수도 있고, 또는 상이한 인공 신경망을 이용 하여 구현될 수도 있다. 여기서, 인공 신경망은, 일례로, 심층 신경망, 콘볼루션 신경망, 콘볼루션 순환 신경망, 다층 퍼셉트론, 심층 신뢰 신경망 및 심층 Q-네트워크 중 어느 하나를 포함 가능하나, 이에 한정되는 것은 아니다. 한편, 상술한 제1 및 제2 인공 신경망 중 적어도 하나는 상술한 수학식 1으로 표현 가능하며, 이 들 중 적어도 하나에 관한 손실 함수는 수학식 2와 같이 차단 영역, 선형 영역 및 포화 영역 각각에 대한 실측 값 및 예측 값의 오차에 대한 제곱 평균의 가중합으로 주어질 수도 있다. 훈련된 인공신경망을 이용하여 콤팩트 모델이 추출 및 획득될 수 있다. 예를 들어, 콤팩트 모델은 훈련이 종료된 인공신경망으로부터 필요한 파라미터를 자동으로 추출하고, 추출된 파라미터를 이용하여 베릴로그-A 언 어를 이용하여 코드를 생성함으로써 획득될 수도 있다. 콤팩트 모델의 추출 및 획득은, 벡터화 기법을 이용하여 조건 명령어 등을 최대한 배제하여 수행될 수 있으며, 배열 없이 변수만을 이용하여 수행될 수도 있다. 일 실시예에 따르면, 콤팩트 모델이 추출 및 획득을 위해 먼저 분석 대상이 결정되고, 콤팩트 모델 생성을 위한 적어도 하나의 변수가 선언 및 생성된다. 선언 및 생성되는 변수는, 예를 들어, 인공신경망으로부터 추출된 값 (각 인공신경망의 가중치, 바이어스 및 각 은닉층의 출력 값)에 대한 변수, 입력 값을 위한 변수, 출력 값을 위 한 변수 및 핀의 개수에 대한 변수 중 적어도 하나를 포함할 수도 있다. 이어서, 훈련된 인공신경망으로부터 획 득된 입력층의 입력 값, 은닉층의 가중치, 은닉층의 바이어스, 은닉층의 출력 값, 출력층의 출력 값 및 활성화 함수를 기반으로 전류-전압 모델과 커패시턴스-전압 모델이 각각 생성된다. 또한, 적어도 하나의 커패시턴스(게 이트 커패시턴스, 게이트-소스 커패시턴스, 게이트-드레인 커패시턴스 및/또는 게이트-바디 커패시턴스 등)와, 적어도 하나의 전류(소스 및 드레인 사이의 전류, 게이트 및 드레인 사이의 전류, 게이트 및 소스 사이의 전류, 및/또는 게이트 및 바디 간의 전류 등)와, 트랜스컨덕턴스와, 드레인컨덕턴스 등이 더 연산될 수도 있다. 콤팩 트 모델은 전류-전압 모델과, 커패시턴스-전압 모델과, 연산된 값(적어도 하나의 커패시턴스, 적어도 하나의 전 류, 트랜스컨덕턴스 및/또는 드레인컨덕턴스 등)(들)을 기반으로 추출 생성되며, 이들을 조합하여 생성될 수 있 다. 콤팩트 모델이 추출 및 생성되면, 실시예에 따라서 이를 기반으로 분석 대상에 대한 시뮬레이션이 수행될 수 있 다. 시뮬레이션은 스파이스 기반 시뮬레이션을 포함할 수 있다. 스파이스 기반 시뮬레이션은, 예를 들어, DC 시뮬레이션, AC 시뮬레이션, 트랜션트 시뮬레이션 및 몬테카를로 시뮬레이션 중 적어도 하나를 포함할 수도 있다. 분석 대상에 대한 시뮬레이션 수행 과정은 생략되는 것도 가능하다. 실시예에 따라서, 분석 대상에 대한 시뮬레이션 이전에 또는 동시에, 구멜 대칭 실험이 콤팩트 모델과 관련하여 더 수행되는 것도 가능하다. 상술한 실시예에 따른 콤팩트 모델 처리 방법은, 컴퓨터 장치에 의해 구동될 수 있는 프로그램의 형태로 구현될 수 있다. 프로그램은, 명령어, 라이브러리, 데이터 파일 및/또는 데이터 구조 등을 단독으로 또는 조합하여 포 함할 수 있으며, 기계어 코드나 고급 언어 코드를 이용하여 설계 및 제작된 것일 수 있다. 프로그램은 상술한 방법을 구현하기 위하여 특별히 설계된 것일 수도 있고, 컴퓨터 소프트웨어 분야에서 통상의 기술자에게 기 공 지되어 사용 가능한 각종 함수나 정의를 이용하여 구현된 것일 수도 있다. 또한, 여기서, 컴퓨터 장치는, 프로 그램의 기능을 실현 가능하게 하는 프로세서나 메모리 등을 포함하여 구현된 것일 수 있으며, 필요에 따라 통신 장치를 더 포함할 수도 있다. 상술한 콤팩트 모델 처리 방법을 구현하기 위한 프로그램은, 컴퓨터 등의 장치에 의해 판독 가능한 기록 매체에 기록될 수 있다. 컴퓨터에 의해 판독 가능한 기록 매체는, 예를 들어, 롬, 램, SD카드 또는 플래시 메모리(일례로 솔리드 스테이트 드라이브(SSD) 등)와 같은 반도체 저장 매체나, 하드 디스 크 또는 플로피 디스크 등과 같은 자기 디스크 저장 매체나, 콤팩트 디스크 또는 디브이디 등과 같은 광 기록 매체나, 또는 플롭티컬 디스크 등과 같은 자기-광 기록 매체 등과 같이 컴퓨터 등의 장치의 호출에 따라 실행되는 하나 이상의 프로그램을 일시적 또는 비일시적으로 저장 가능한 적어도 한 종류의 물리적 저장 매체를 포함 할 수 있다. 이상 콤팩트 모델 처리 장치 및 콤팩트 모델 처리 방법의 여러 실시예에 대해 설명하였으나, 콤팩트 모델 처리 장치 또는 콤팩트 모델 처리 방법은 오직 상술한 실시예에 한정되는 것은 아니다. 해당 기술 분야에서 통상의 지식을 가진 자가 상술한 실시예를 기초로 수정 및 변형하여 구현할 수 있는 다른 다양한 장치나 방법 역시 상 술한 콤팩트 모델 처리 장치 또는 콤팩트 모델 처리 방법의 일 실시예가 될 수 있다. 예를 들어, 설명된 방법 (들)이 설명된 바와 다른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성 요소(들)가 설명된 바와 다른 형태로 결합, 연결 또는 조합되거나 다른 구성 요소 또는 균등물 등에 의하여 대치 또는 치환 되더라도, 상술한 콤팩트 모델 처리 장치 및/또는 콤팩트 모델 처리 방법의 일 실시예가 될 수 있다. 본 발명의 실시예들과 관련된 기술 분야에서 통상의 지식을 가진 자는 상기 기재의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로, 개시된 방법들은 한정적인 관점이 아닌 설명적 관점에서 고려되어야 한다. 본 발명의 범위는 발명의 상세한 설명이 아닌 특허청구 범위에 나타나며, 그와 동등한 범위 내에 있는 모든 차이점은 본 발명의 범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2023-0050540", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 콤팩트 모델 처리 장치에 대한 개요도이다. 도 2a는 분석 대상의 일례에 대한 도면이다. 도 2b는 물리/수학적 파라미터 값의 일례를 도시한 도표이다. 도 3은 일 실시예에 따른 제1 인공신경망에 대한 도면이다. 도 4는 일 실시예에 따른 제2 인공신경망에 대한 도면이다. 도 5는 일 실시예에 따른 모델설계부에 대한 제1 슈도코드(pseudo code)이다. 도 6은 일 실시예에 따른 모델설계부에 대한 제2 슈도코드이다. 도 7은 네거티브 커패시턴스-나노시트 펫에 대해서, 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래의 BSIM 기반 시뮬레이션 장치 각각의 회로 시뮬레이션 결과를 도시한 도표이다. 도 8은 나노시트 펫에 대해서, 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래의 BSIM 기반 시뮬레이션 장치 각각의 회로 시뮬레이션 결과를 도시한 도표이다. 도 9는 다수의 공정 파라미터에 따라 획득된 C-V 곡선의 일례를 도시한 그래프 도면이다. 도 10은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 링 오실레이터(ring oscillator)의 트랜션트(transient) 시뮬레이션 결과를 비교 도시한 그래프 도면이다. 도 11은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 트랜지스터 OTA의 시뮬레이션 결과 를 비교 도시한 그래프 도면이다. 도 12는 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 시뮬레이션 처리 시간의 비교 결과 를 도시한 도표이다. 도 13은 일 실시예에 따른 콤팩트 모델 처리 장치 및 종래 기술 각각에 의한 시뮬레이션 처리 시간의 비교 결과 를 도시한 그래프 도면이다. 도 14는 채널 도핑 깊이에 따른 온-커런트 분포를 도시한 그래프 도면이다. 도 15는 일 실시예에 따른 콤팩트 모델 처리 방법의 흐름도이다."}
