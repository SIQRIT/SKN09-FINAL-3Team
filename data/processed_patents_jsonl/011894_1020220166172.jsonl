{"patent_id": "10-2022-0166172", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0083206", "출원번호": "10-2022-0166172", "발명의 명칭": "콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법", "출원인": "한신대학교 산학협력단", "발명자": "강민구"}}
{"patent_id": "10-2022-0166172", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "OTT 시청자의 생체인식에 의해 FIDO 기반으로 사용자 프로파일을 구성하는 제 1 단계; OTT 시청자의 음성으로부터 음성인식을 수행하여 음성 데이터에 포함된 음성신호 톤과 템포 특징 중 하나이상에의해 OTT 시청자의 감정을 분류하는 제 2 단계; OTT 시청자의 얼굴 촬영 영상으로부터 표정인식을 수행하여 OTT 시청자의 감정을 분류하는 제 3 단계; 상기 사용자 프로파일 기반으로 협업 필터링 모델을 적용하는 제 4 단계; 상기 음성인식 기반의 감정분류 결과, 상기 표정인식 기반의 감정분류 결과, 상기 협업 필터링 모델의 출력을결합하여 앙상블 추천 모델에 의해 콘텐츠 추천을 수행하는 제 5 단계; 상기 추천된 콘텐츠의 재생 중에 OTT 시청자 반응을 획득하는 제 6 단계; 상기 획득한 OTT 시청자 반응을 이용하여 상기 앙상블 추천 모델을 재학습하는 제 7 단계; 를 포함하여 구성되는 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법."}
{"patent_id": "10-2022-0166172", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 제 3 단계는, OTT 시청자의 얼굴 촬영 영상에서 눈과 입을 식별하는 단계;OTT 시청자의 얼굴 촬영 영상에서 얼굴 윤곽 특징점을 식별하는 단계; OTT 시청자의 얼굴 촬영 영상에서 얼굴 측면 요소를 식별하는 단계; OTT 시청자의 얼굴 촬영 영상의 여러 이미지에 대해 얼굴 스케일을 정규화하여 얼굴두상 영역을 식별하는 단계; 상기 얼굴두상 영역에서 조도의 특징에 따른 자연적 및 인공적 영상인식 기반의 감정분류를 수행하는 단계; 를 포함하여 구성되는 것을 특징으로 하는 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법."}
{"patent_id": "10-2022-0166172", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서, 상기 제 5 단계에서 상기 앙상블 추천 모델은 상기 음성인식 기반 또는 상기 표정인식 기반의 감정분류 결과에대응하여 바이노럴비트가 포함되어 있는 콘텐츠를 선택하여 추천하도록 구성되는 것을 특징으로 하는 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법."}
{"patent_id": "10-2022-0166172", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서, 상기 제 4 단계에서 상기 협업 필터링 모델은 아이템 협업필터링(IBCF) 알고리즘 또는 유저 기반 협업필터링(UBCF) 알고리즘으로 동작하도록 구성되는 것을 특징으로 하는 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법. 공개특허 10-2024-0083206-3-청구항 5 컴퓨터에 청구항 1 내지 4 중 어느 하나의 항에 따른 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리방법을 실행시키기 위하여 저장매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 반응형 표정모델 기반으로 OTT 시청자의 표정인식을 수행하고 이를 통해 한국형 감정분류 영상 데이터 베이스를 구축하며 이 감정분류 영상 데이터베이스에 의해 OTT 콘텐츠에 대한 시청자의 감정반응을 식별해내어 콘텐츠 사용성 평가에 활용하는 감정반응 FIDO 거래인증 처리 기술에 관한 것이다. 본 발명에 따르면 OTT 콘텐츠 (뒷면에 계속)"}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 OTT 콘텐츠에 대한 시청자의 감정반응을 식별하고 시청자 감정반응에 기초하여 콘텐츠 사용성을 평가 한 후에 해당 시청자에 대한 추천 콘텐츠를 콘텐츠 데이터베이스로부터 선별하여 제공하는 FIDO 거래인증 처리 기술에 관한 것이다. 특히, 본 발명은 반응형 표정모델 기반으로 OTT 시청자의 표정인식을 수행하고 이를 통해 한국형 감정분류 영상 데이터베이스를 구축하며 이 감정분류 영상 데이터베이스에 의해 OTT 콘텐츠에 대한 시청자의 감정반응을 식별 해내어 콘텐츠 사용성 평가에 활용하는 감정반응 FIDO 거래인증 처리 기술에 관한 것이다."}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근들어 다양한 매체를 통해 콘텐츠가 제공되고 있다. [도 1]은 일반적인 OTT(Over-The-Top) 환경에서 콘텐츠 서비스를 제공하는 구성도이고, [도 2]는 일반적인 XR 디바이스에 대한 콘텐츠 서비스를 제공하는 구성도이다. [도 1]을 참조하면, OTT 디바이스는 네트워크를 통해 콘텐츠 서버로부터 콘텐츠를 수신하여 가입자에 게 재생 출력한다. 이때, OTT 디바이스는 OTT 단말, 스마트폰, IP 셋톱박스 등으로 구현될 수 있다. 서비 스 서버는 OTT 서비스를 전체적으로 관리하는 장치로서 OTT 서비스 가입자의 정보를 가입자 데이터베이스 에 저장 관리한다. 콘텐츠 서버는 가입자에 대해 OTT 환경에서 콘텐츠 스트리밍을 제공하는 장치로서 콘텐츠 데이터를 콘텐츠 데이터베이스에 관리한다. 콘텐츠 제공은 라이브 채널(Live channels)일 수도 있 고 VOD(Video-On-Demand)일 수도 있다. [도 2]를 참조하면, 사용자가 XR 디바이스를 통해 콘텐츠를 이용하고 있다. 확장현실(XR; eXtended Reality)은 일반적으로 가상현실(VR), 증강현실(AR), 혼합현실(MR)을 총칭하는 용어이다. 가상현실(VR; Virtual Reality)은 컴퓨터그래픽(CG)으로 가상의 세계를 생성하여 표시하는 기술이고, 증강현실(AR; Augmented Reality)은 현실 세계의 영상에 컴퓨터그래픽 콘텐츠를 추가하여 표시하는 기술이고, 혼합현실(MR; Mixed Reality)은 현실 세계의 영상에 컴퓨터그래픽 콘텐츠를 독립적으로 결합 또는 믹싱하는 기술이다. 이러한 확장 현실(XR; eXtended Reality) 기술은 게임, 콘서트, 실습교육, 대테러 작전, 화재 진압훈련 등 활용도가 높다. OTT 디바이스나 XR 디바이스를 통해 콘텐츠가 사용자에게 제공되는데, 사용자가 이러한 콘텐츠를 사 용하면서 만족하는지 혹은 불만족하는지에 대해서는 제대로 평가하지 못한다. OTT 콘텐츠나 XR 콘텐츠(이하, 간단히 'OTT 콘텐츠'라 함)의 시장이 폭발적으로 늘어나고 있고 그에 따라 서비 스 제공자 간의 경쟁이 격화되어 가고 있다는 점을 감안하면 OTT 콘텐츠의 사용성을 체계적이고 객관적으로 평 가할 필요성이 있다. 이를 통해, OTT 콘텐츠의 품질을 평가하고 부족한 부분을 개선할 수 있을 것이다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허 10-1312446호 \"사용자의 행위 로그를 이용한 모바일 어플리케이션의 사용성 분석 장치 및 방법\" (특허문헌 0002) 대한민국 등록특허 10-1560760호 \"어플리케이션 사용성 측정 서비스 제공 시스템\" (특허문헌 0003) 대한민국 등록특허 10-1889704호 \"크라우드 소싱 기반 앱 UI 사용성 시나리오 분석 방법 및 장 치\" (특허문헌 0004) 대한민국 등록특허 10-2233857호 \"가상현실 장치의 사용으로 인한 사용자의 어지럼증을 정량화"}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "하여 평가하는 방법 및 장치\" 발명의 내용"}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 OTT 콘텐츠에 대한 시청자의 감정반응을 식별하고 시청자 감정반응에 기초하여 콘텐츠 사용성 을 평가한 후에 해당 시청자에 대한 추천 콘텐츠를 콘텐츠 데이터베이스로부터 선별하여 제공하는 FIDO 거래인 증 처리 기술을 제공하는 것이다. 특히, 본 발명의 목적은 반응형 표정모델 기반으로 OTT 시청자의 표정인식을 수행하고 이를 통해 한국형 감정분 류 영상 데이터베이스를 구축하며 이 감정분류 영상 데이터베이스에 의해 OTT 콘텐츠에 대한 시청자의 감정반응 을 식별해내어 콘텐츠 사용성 평가에 활용하는 감정반응 FIDO 거래인증 처리 기술을 제공하는 것이다. 본 발명의 해결 과제는 이 사항에 제한되지 않으며 본 명세서의 기재로부터 다른 해결 과제가 이해될 수 있다."}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기의 목적을 달성하기 위하여 본 발명에 따른 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 방법 은, OTT 시청자의 생체인식에 의해 FIDO 기반으로 사용자 프로파일을 구성하는 제 1 단계; OTT 시청자의 음성으 로부터 음성인식을 수행하여 음성 데이터에 포함된 음성신호 톤과 템포 특징 중 하나이상에 의해 OTT 시청자의 감정을 분류하는 제 2 단계; OTT 시청자의 얼굴 촬영 영상으로부터 표정인식을 수행하여 OTT 시청자의 감정을 분류하는 제 3 단계; 사용자 프로파일 기반으로 협업 필터링 모델을 적용하는 제 4 단계; 음성인식 기반의 감정 분류 결과, 표정인식 기반의 감정분류 결과, 협업 필터링 모델의 출력을 결합하여 앙상블 추천 모델에 의해 콘 텐츠 추천을 수행하는 제 5 단계; 그 추천된 콘텐츠의 재생 중에 OTT 시청자 반응을 획득하는 제 6 단계; 그 획 득한 OTT 시청자 반응을 이용하여 앙상블 추천 모델을 재학습하는 제 7 단계;를 포함하여 구성될 수 있다. 본 발명에서 제 3 단계는, OTT 시청자의 얼굴 촬영 영상에서 눈과 입을 식별하는 단계; OTT 시청자의 얼굴 촬영 영상에서 얼굴 윤곽 특징점을 식별하는 단계; OTT 시청자의 얼굴 촬영 영상에서 얼굴 측면 요소를 식별하는 단 계; OTT 시청자의 얼굴 촬영 영상의 여러 이미지에 대해 얼굴 스케일을 정규화하여 얼굴두상 영역을 식별하는 단계; 얼굴두상 영역에서 조도의 특징에 따른 자연적 및 인공적 영상인식 기반의 감정분류를 수행하는 단계;를 포함하여 구성될 수 있다. 이때, 제 5 단계에서 앙상블 추천 모델은 음성인식 기반 또는 표정인식 기반의 감정분류 결과에 대응하여 바이 노럴비트가 포함되어 있는 콘텐츠를 선택하여 추천하도록 구성될 수 있다. 또한, 제 4 단계에서 협업 필터링 모델은 아이템 협업필터링(IBCF) 알고리즘 또는 유저 기반 협업필터링(UBCF) 알고리즘으로 동작하도록 구성될 수 있다. 한편, 본 발명에 따른 컴퓨터프로그램은 컴퓨터에 이상과 같은 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래 인증 처리 방법을 실행시키기 위하여 비휘발성 저장매체에 저장된 것이다."}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 OTT 콘텐츠에 대한 시청자의 감정반응에 기초하여 콘텐츠 사용성을 평가할 수 있는 장점이 있 다. 또한, 본 발명에 따르면 OTT 시청자의 표정인식을 수행하여 한국형 감정분류 영상 데이터베이스를 구축하고 이 에 기초하여 콘텐츠 추천을 실행한 후에 그 추천 콘텐츠에 대한 사용자 감정반응을 수집하여 평가함으로써 OTT 콘텐츠 추천 모델의 성능을 개선할 수 있는 장점이 있다. 또한, 본 발명에 따르면 FIDO 생체인증 기반으로 콘텐츠 거래를 인증하므로 현장 기반의 비대면 콘텐츠 사용성 평가를 달성할 수 있는 장점이 있다. 또한, 본 발명은 차량의 자율주행 환경에서 OTT 콘텐츠 거래인증 플랫폼에 양호하게 적용될 수 있다."}
{"patent_id": "10-2022-0166172", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 도면을 참조하여 본 발명을 상세하게 설명한다. 본 발명을 설명함에 있어서 종래기술과 중복되는 부분에 대해서는 자세한 설명을 생략할 수 있다. [도 3]은 본 발명에서 카메라 영상에 의한 표정인식 개념도이다. [도 3] (a)는 광 흐름 분석 기반으로 표정 특 징을 추출하는 두가지 예시도이고, [도 3] (b)는 CNN 모델 기반으로 표정인식 프로세스를 구성한 도면이다. OTT 콘텐츠의 시청자 얼굴을 카메라로 촬영한 후에 그 얼굴 영상을 분석하여 표정인식(facial expression recognition)을 수행할 수 있다. 이때, 후술하는 바와 같이 자연적 고려요소와 인공적 고려요소를 분류할 수 있 다. 이를 통해 표정인식에 의한 한국형 감정분류 영상 데이터베이스를 구축할 수 있으며, 감정 유형별 영상 데 이터베이스 스키마를 설계할 수 있다. [도 4]는 본 발명에서 표정인식 과정에서 얼굴두상 영역에서 조도 특징분석의 개념도이다. [도 4] (a)는 촬영 영상에서 눈과 입을 식별한 예시도이고, [도 4] (b)는 촬영 영상에서 얼굴 윤곽 특징점(facial features)을 식 별한 예시도이고, [도 4] (c)는 촬영 영상에서 얼굴 측면 요소(side facial components)를 식별한 예시도이고, [도 4] (d)는 촬영 영상의 여러 이미지에 대해 얼굴 스케일(face scale)을 정규화(normalization)하는 예시도이 다. 이상의 과정을 통해 촬영영상으로부터 얼굴두상 영역을 식별한다. 시청자의 얼굴 부분을 촬영한 영상을 처리할 때에 얼굴두상 영역에서 조도의 특징에 따른 자연적 및 인공적 영 상인식 기반의 감정분류를 수행할 수 있다. 동일인의 얼굴이지만 얼굴 좌우의 조도 차로 인한 흑백 명함에 차이 가 발생하게 되고, 이를 기초로 시청자의 두상과 얼굴을 인식할 수 있으며, 이 영역을 분석하면 시청자의 성별 과 연령 등을 인식할 수 있다. 시청자 얼굴을 검출 및 인식하고 성별 및 나이에 대한 인식 탐지 결과를 얻을 수 있으며, 이러한 요소를 고려하 여 [도 3] (b)에 제시된 신경망에 의해 OTT 시청자의 반응형 표정인식을 수행할 수 있다. 이때, 신경망 모델을 머신러닝시키기 위한 훈련 데이터셋으로는 오픈 데이터셋을 이용하는 것이 바람직하다. 오픈 데이터셋으로는 Denver Intensity of Spontaneous Facial Action(DISFA) 데이터셋, Extended Conh-Kanade(CK+) 데이터셋 등을 들 수 있다. [도 5]는 본 발명에서 반응형 표정인식을 위한 앙상블 모델의 개념도이다. OTT 시청자의 얼굴 촬영 영상으로부 터 표정인식을 수행할 수 있으며, 그 표정인식 결과의 과적합 방지 및 모델 안정성을 확보하기 위한 앙상블 학 습(ensemble learning) 기법(예: 배깅, 부스팅)을 수행한다. [도 6]은 본 발명에서 OTT 음향에 대한 바이노럴비트(Binaural beats) 음향효과를 위한 반응형 콘텐츠 추천 서 비스 모델의 개념도이다. 일반적으로 바이노럴비트(Binaural beats)는 뇌파를 인위적으로 조절하는 기술이다. 예를 들어, 한쪽 귀에 300Hz의 소리, 다른쪽 귀에 310Hz의 소리를 들려주면 뇌가 10Hz의 파동으로 받아들여 뇌 파가 10Hz로 조절된다. 양쪽 귀에 서로 다른 주파수의 소리를 들려주는 콘텐츠 재생이 필요하며, OTT 시청자의 감정 상태를 고려하여 필요시에는 바이노럴비트가 포함되어 있는 콘텐츠를 선택하여 추천할 수 있다. 이때, 바 이노럴비트가 포함된 콘텐츠를 재생하는 동안에 시청자의 감정변화를 모니터링함으로써 해당 콘텐츠의 감정이완 효과를 평가할 수 있다. 본 발명에서는 [도 6]에서와 같이 음성인식 및 표정인식을 활용한 앙상블 기반의 콘텐츠 추천을 수행한다. 이때, 음성인식 모델을 대체하기 위한 음성-표정 변환(STF: Speech-to-Face) 과정을 채택할 수도 있다. 이때, 음성인식 및 표정인식을 활용한 앙상블 기반으로 이루어진 추천 콘텐츠가 재생되는 동안에 표정인식 기반 또는 시청시간이나 평점 등에 의한 시청자 반응을 획득할 수 있으며, 이로부터 콘텐츠 추천을 위한 앙상블 추천 모델 의 재학습을 수행할 수 있다. [도 7]과 [도 8]은 디지털 인식 콘텐츠 제공을 위한 감정유형별 자연적 고려요소 및 인공적 고려요소의 분류표 를 나타낸 것이다. [도 7]은 본 발명에서 감정요소에 대한 자연적 및 인공적 효과 결합 모델의 예시도이고, [도 8]은 본 발명에서 영상효과 및 바이노럴비트 음향효과 요소의 예시도이다. [도 7]은 인간 감정의 3개 주요요소(기쁨, 즐거움, 온유함)별로 자연적 효과 및 인공적 효과를 결합하는 예를 나타낸 것이다. [도 8] (a)는 자연적 요소(natural)에 대한 영상효과와 음향효과(바이노럴비트 효과)를 예시한 것이고, [도 8] (b)는 인공적 요소(artificial)에 대한 영상효과와 음향효과(바이노럴비트 효과)를 예시한 것이다. [도 9]는 본 발명에서 음성 데이터의 감정 톤(주파수)에 대한 감정분류 구성도이다. [도 6]에 전술한 바와 같이, OTT 시청자의 음성으로부터 감정을 식별하고 이를 콘텐츠 추천에 활용할 수 있는데, [도 9]는 사람 음성의 감정 톤(주파수)에 기초하여 감정 분류를 수행하는 구성의 일 예이다. [도 9]의 구성은 2017년 추계학술발표대회 논문집 제24권 제2호의 \"발화 음성을 기반으로 한 감정분석 시스템\"에 따른 것 이다. [도 10]은 본 발명에서 OTT 반응형 앙상블 기반 콘텐츠 추천 서비스 모델의 개념도이다. OTT 시청자의 얼굴 촬영 영상으로부터 시청자의 감정 반응을 식별하기 위한 표정인식 모델에 XAI(Explainable Artificial Intelligence) 기법(예: LIME, SHAP, LRP)을 적용할 수 있다. LIME(Local Interpretable Model-agnostic Explanations)은 특정 인스턴스에 대한 국소적 대리 모형(local surrogate model)을 생성하고 모델을 설명하는 기법이다. 이미지 전처리 및 특징 추출 기법(예: Gabor 필터, 광 흐름 분석, Active Shape Model), 표정 분류 모델(예: k-NN, Support Vector Machine, AdaBoost) 등을 이용할 수 있다. SHAP(Shapley Additive Explanation)은 특징별 Shapley 값을 계산하여 특징 중요도를 전역적(globa l)으로 설명하는 기법이다. 또한, LRP(Layer-wise Relevance Propagation)는 신경망 계열 모델의 출력 값을 타 당성 점수(relevance score)로 분해하여 출력단에서 입력단으로 역전파하는 알고리즘으로 최종적으로 계산되는 특징별 타당성 점수를 통해 기여도를 평가하는 기법이다. 그리고, [도 3] (b)를 참조하여 전술한 바와 같이 딥러닝 기반으로 표정인식을 수행할 수 있는데, 합성곱 신경 망(CNN: Convolutional Neural Network) 모델, 심층신뢰망(Deep Belief Network) 모델, 적대적 생성망(GAN: Generative Advasarial Network) 모델 등이 표정인식에 사용될 수 있다. [도 10]에서 사용자 반응형 앙상블 추천 모델에 아이템 협업필터링(IBCF: Item-based Collaborative Filtering) 알고리즘 또는 유저 기반 협업필터링(UBCF: User-based Collaborative Filtering) 알고리즘으로 동 작하는 협업 필터링 모델을 이용할 수 있다. 이때, 음성인식 모델은 음성신호 톤이나 템포 특징에 기초하여 OTT 시청자의 감정을 분류하는 구성이다. 예를 들면, The Transactions of the Korean Institute of Electrical Engineers Vol. 65, No. 1, pp. 116~121, 2016 \"IoT를 위한 음성신호 기반의 톤, 템포 특징벡터를 이용한 감정인식\" 논문의 모델을 채택할 수 있다. 또한, 표정인식 모델은 시청자의 얼굴 영상을 분석하여 그 표정에서 감정을 읽어내는 구성이다. 예를 들면, KAIST 바이오 및 뇌공학과 정기훈 이도훈 교수 연구팀이 개발한 인공지능 기반의 표정인식 기술을 채택할 수 있 다. 이 기술에서는 근적외선 기반의 라이트필드 카메라를 이용해 획득한 얼굴 표정별 거리 지도와 얼굴의 특징을 담은 입체정보를 인공지능(AI)으로 분석함으로써 표정 인식을 매우 정확하게 수행할 수 있다. 본 발명에서는 [도 10]에서와 같이 음성인식 모델의 출력과 표정인식 모델의 출력을 결합하여 앙상블 기반의 콘 텐츠 추천을 수행한다. 이때, 사용자 프로파일에 의한 협업 필터링을 이용하는 것이 바람직하다. 이때, 앙상블 추천 모델에 의해 이루어진 추천 콘텐츠가 재생되는 동안에 표정인식 기반 또는 시청시간이나 평점 등에 의한 시청자 반응을 획득할 수 있으며, 이로부터 콘텐츠 추천을 위한 앙상블 추천 모델의 재학습을 수행할 수 있다. 사용자 프로파일 구성은 사용자의 생체인식에 의한 FIDO 기반으로 이루어질 수 있다. [도 11]은 본 발명에 적용 가능한 얼굴표정 분석에 의한 감정인식의 예시도로서, KAIST 바이오 및 뇌공학과 정 기훈 이도훈 교수 연구팀이 개발한 인공지능 기반의 표정인식 기술을 나타낸 것이다. 이때, 라이트필드 카메라 의 조명으로 강한 근적외선을 쏘는 '수직 공진형 표면 발광 레이저(VCSEL)'와 근적외선 대역 필터를 적용해 3차 원 재구성의 정확도 향상할 수 있다. 그리고, 얼굴 정면을 기준으로 0도, 30도, 60도 각도에서 기존보다 최대 54%까지 영상 재구성 오류를 최소화할 수 있고, 가시광선 및 근적외선 영역의 흡수용 광 흡수층을 미세렌즈 사 이에 배치함으로써 광학 크로스토크를 낮출 수 있다. [도 12]는 본 발명에서 FIDO 거래인증 과정을 개념적으로 나타내는 도면이고, [도 13]은 본 발명에서 FIDO 거래 인증을 위한 인증모듈의 구성을 개념적으로 나타내는 블록도이다. 일반적으로 FIDO(Fast IDentity Online) 기술 은 온라인 환경에서 지문인식, 홍채인식 등과 같은 생체인식 기술을 활용하여 단말인증을 수행하는 기술이다. 본 발명에서는 XR 디바이스와 개인인증단말 간의 사용자 개인 인증 및 디지털 자산의 거래에 대해 FIDO 거래인증을 연계한다. FIDO의 구성요소에는 FIDO 서버, FIDO 클라이언트, ASM(Authenticator Specific Module), 인증기(authenticator)가 있다. 본 발명에서는 FIDO 거래인증 서버가 FIDO 서버의 연할을 수행 하고 개인인증단말이 FIDO 클라이언트의 역할을 수행한다. ASM은 FIDO 클라이언트의 요청을 인증기에 전달 하고, 여기서 생성된 응답 값을 FIDO 클라이언트로 전달한다. 오픈 ID 지원 그룹(Relying Party, RP)란 그룹 간의 신뢰를 뜻하는데 오픈 아이디를 지원하는 장치들의 그룹이 라는 의미이다. 예를 들면, FIDO는 사용자가 이용하는 서버와 상호 RP 관계이기도 하다. 그리고, FIDO 클라이언 트 응용서버는 RP 서버라고 부르고, 응용 서비스 클라이언트는 RP 클라이언트라고 부른다. FIDO 거래인증 서버 가 보내는 요청 메시지가 RP 서버에 전달되면, 네트워크를 통해 RP 클라이언트를 거쳐 FIDO 클라이언트에 도달한다. 한편, [도 7]과 [도 8]은 웹 인증 방식으로 이루어져 있다. '웹 인증'은 인터넷 사용자들이 여러 웹 사이트 및 디바이스에서 안전하게 인증할 수 있는 방법으로서 웹 브라우저 및 그와 연계된 웹 플랫폼 인프라에 표준화된 API 정의를 다운로드받아 인증 기능을 제공한다. [도 14]는 본 발명에 따른 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 프로세스를 나타내는 순서 도이다. 먼저, OTT 시청자의 생체인식에 의해 FIDO 기반으로 사용자 프로파일을 구성한다(S100). 다음으로, OTT 시청자의 음성으로부터 음성인식을 수행하여 음성 데이터에 포함된 음성신호 톤과 템포 특징 중 하나이상에 의해 OTT 시청자의 감정을 분류한다(S110). 다음으로, OTT 시청자의 얼굴 촬영 영상으로부터 표정인식을 수행하여 OTT 시청자의 감정을 분류한다(S120). 이 때, 단계 (S120)은, OTT 시청자의 얼굴 촬영 영상에서 눈과 입을 식별하는 과정과, OTT 시청자의 얼굴 촬영 영 상에서 얼굴 윤곽 특징점을 식별하는 과정과, OTT 시청자의 얼굴 촬영 영상에서 얼굴 측면 요소를 식별하는 과 정과, OTT 시청자의 얼굴 촬영 영상의 여러 이미지에 대해 얼굴 스케일을 정규화하여 얼굴두상 영역을 식별하는 과정과, 얼굴두상 영역에서 조도의 특징에 따른 자연적 및 인공적 영상인식 기반의 감정분류를 수행하는 과정으 로 구성될 수 있다. 다음으로, 사용자 프로파일 기반으로 협업 필터링 모델을 적용한다(S130). 이때, 협업 필터링 모델은 아이템 협 업필터링(IBCF) 알고리즘 또는 유저 기반 협업필터링(UBCF) 알고리즘으로 동작하도록 구성될 수 있다. 다음으로, 음성인식 기반의 감정분류 결과, 표정인식 기반의 감정분류 결과, 협업 필터링 모델의 출력을 결합하 여 앙상블 추천 모델에 의해 콘텐츠 추천을 수행한다(S140). 이때, 앙상블 추천 모델은 음성인식 기반 또는 표 정인식 기반의 감정분류 결과에 대응하여 바이노럴비트가 포함되어 있는 콘텐츠를 선택하여 추천하도록 구성될 수 있다. 다음으로, 그 추천된 콘텐츠의 재생 중에 OTT 시청자 반응을 획득하고(S150), 그 획득한 OTT 시청자 반응을 이 용하여 앙상블 추천 모델을 재학습한다(S160). 이를 통해, 앙상블 추천 모델을 고도화할 수 있고, 이러한 과정 을 반복함에 따라 한국형 감정분류 영상 데이터베이스가 구축된다. 한편, 본 발명은 컴퓨터가 읽을 수 있는 비휘발성 기록매체에 컴퓨터가 읽을 수 있는 코드의 형태로 구현되는 것이 가능하다. 이러한 비휘발성 기록매체로는 다양한 형태의 스토리지 장치가 존재하는데 예컨대 하드디스크, SSD, CD-ROM, NAS, 자기테이프, 웹디스크, 클라우드 디스크 등이 있다. 또한, 본 발명은 하드웨어와 결합되어 특정의 절차를 실행시키기 위하여 매체에 저장된 컴퓨터프로그램의 형태로 구현될 수도 있다."}
{"patent_id": "10-2022-0166172", "section": "도면", "subsection": "도면설명", "item": 1, "content": "[도 1]은 일반적인 OTT 환경에서 콘텐츠 서비스를 제공하는 구성도. [도 2]는 일반적인 XR 디바이스에 대한 콘텐츠 서비스를 제공하는 구성도. [도 3]은 본 발명에서 카메라 영상에 의한 표정인식 개념도. [도 4]는 본 발명에서 얼굴두상 영역에서 조도 특징분석의 개념도. [도 5]는 본 발명에서 반응형 표정인식을 위한 앙상블 모델의 개념도. [도 6]은 본 발명에서 바이노럴비트 음향효과를 위한 반응형 콘텐츠 추천 서비스 모델의 개념도. [도 7]은 본 발명에서 감정요소에 대한 자연적 및 인공적 효과 결합 모델의 예시도. [도 8]은 본 발명에서 영상효과 및 바이노럴비트 음향효과 요소의 예시도. [도 9]는 본 발명에서 음성 데이터의 감정 톤에 대한 감정분류 구성도. [도 10]은 본 발명에서 OTT 반응형 앙상블 기반 콘텐츠 추천 서비스 모델의 개념도. [도 11]은 본 발명에 적용 가능한 얼굴표정 분석에 의한 감정인식의 예시도. [도 12]는 본 발명에서 FIDO 거래인증 과정을 개념적으로 나타내는 도면. [도 13]은 본 발명에서 FIDO 거래인증을 위한 인증모듈의 구성을 개념적으로 나타내는 블록도. [도 14]는 본 발명에 따른 콘텐츠 사용성 평가에 의한 감정반응 FIDO 거래인증 처리 프로세스를 나타내는 순서 도."}
