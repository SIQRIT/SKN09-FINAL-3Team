{"patent_id": "10-2023-0077448", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0028917", "출원번호": "10-2023-0077448", "발명의 명칭": "인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "김홍숙"}}
{"patent_id": "10-2023-0077448", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "데이터셋 내 이미지들에 존재하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계;상기 너비 최대값 및 높이 최대값을 이용하여 이미지 분할 시 스트라이드(stride)를 산출하는 단계;상기 스트라이드에 기반하여 원본 이미지를 복수의 서브 이미지로 분할하는 단계; 및상기 서브 이미지를 신경망 모델에 입력하여 인공지능 태스크를 수행하는 단계;를 포함하고,상기 스트라이드는 상기 신경망 모델의 입력 사이즈보다 작은 것을 특징으로 하는 중첩 분할 이미지 생성 방법."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법은 데이터셋 내 이미지들 에 존재하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계; 상기 너비 최대값 및 높이 최대값을 이용하여 이미지 분할 시 스트라이드(stride)를 산출하는 단계; 상기 스트라이드에 기반하여 원본 이미지를 복수 의 서브 이미지로 분할하는 단계; 및 상기 서브 이미지를 신경망 모델에 입력하여 인공지능 태스크를 수행하는 단계를 포함한다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 기존 딥러닝 방법으로 학습하거나 추론이 어려운 고해상도 이미지에서의 소형 오브젝트의 탐지를 위 한 딥러닝 모델의 학습 및 추론을 위한 이미지 분할 방법에 관한 것이다. 구체적으로, 본 발명은 데이터 셋 내 바운드 박스의 크기 정보에 기반하여 중첩 분할 이미지를 생성하는 기술에 관한 것이다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능의 한 분야인 시각 지능은 컴퓨터를 통하여 인간의 시각 처리 능력을 구현하는 기술이다. 시각 지능 분 야에서의 작업은 크게 이미지 분류(classification), 오브젝트 탐지(object detection), 인스턴스 세그멘테이션 (instance segmentation)으로 구분할 수 있다. 도 1은 시각 지능 작업의 종류를 나타내는 도면이다. 이미지 분류는 도 1에서의 좌측 첫번째와 같이 주어진 입 력 이미지 내의 1개의 오브젝트의 종류를 구분하는 작업이다. 오브젝트 탐지는 도 1에서의 좌측 2번째와 같이 오브젝트 분류에 추가하여 오브젝트의 위치 정보인 바운딩 박스(bounding box) 정보까지 찾아내는 오브젝트 지 역화(object localization)을 통하여 입력 이미지 내에서의 2개 이상의 오브젝트에 대하여 각각의 오브젝트에 대한 분류와 오브젝트들의 위치까지 찾아내는 작업이다. 마지막으로 인스턴스 세그멘테이션은 도 1의 4번째 그 림과 같이 입력 이미지 내에서 다수의 오브젝트들에 대하여 분류 및 오브젝트의 외곽선 모양까지 찾아내는 작업 이다. 오브젝트의 위치 정보인 bounding box는 도 2에서와 같이 오브젝트를 감싸는 최소 크기의 직사각형의 좌 상단 시작점의 좌표(x, y)와 너비(width) 및 높이(height)의 4개의 값으로 표현된다. 도 2는 바운딩 박스의 개 념을 설명하기 위한 도면이다. 작업의 난이도라 할 수 있는 딥러닝 모델의 복잡도 및 계산량은 이미지 분류, 오브젝트 탐지, 인스턴스 세그멘 테이션 순서로 증가한다. 시각 지능 작업을 수행하는 기존 기술의 경우 객체의 크기에 따라 성능이 달라지며, 작은 사이즈의 객체에 대하여는 검출 정확도가 떨어지는 문제가 존재하며, 이와 같은 문제를 해결할 수 있는 시 각 지능 기술에 대한 필요성이 절실히 대두된다. 선행기술문헌 특허문헌"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "(특허문헌 0001) 국내 공개특허공보 제2012-0070126호(발명의 명칭: 객체 영상 획득 장치 및 방법) 발명의 내용"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 고해상도 이미지에서의 작은 크기의 오브젝트 탐지를 위하여 원본 이미지를 모델에서 요구하 는 크기의 작은 이미지들로 중첩 분할한 이미지들을 생성하는 것이다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법은 데이터셋 내 이미지들에 존재하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계; 상기 너 비 최대값 및 높이 최대값을 이용하여 이미지 분할 시 스트라이드(stride)를 산출하는 단계; 상기 스트라이드에 기반하여 원본 이미지를 복수의 서브 이미지로 분할하는 단계; 및 상기 서브 이미지를 신경망 모델에 입력하여 인공지능 태스크를 수행하는 단계를 포함한다. 이때, 상기 스트라이드는 상기 신경망 모델의 입력 사이즈보다 작을 수 있다. 이때, 상기 이미지 분할 시 스트라이드(stride)는 상기 신경망 모델의 입력 너비 값과 높이 값에서 각각 상기 너비 최대값 및 높이 최대값을 뺀 값에 상응할 수 있다. 이때, 상기 방법은 상기 탐지한 객체의 바운딩 박스의 좌표 정보를 원본 이미지에 상응하도록 후처리하는 단계 를 더 포함할 수 있다. 이때, 상기 원본 이미지를 복수의 서브 이미지로 분할하는 단계는 상기 원본 이미지를 상기 신경망 모델의 입력 너비 값과 높이 값의 배수에 상응하도록 제로 패딩을 수행할 수 있다. 이때, 상기 인공지능 태스크가 인스턴스 세그멘테이션에 상응하면, 객체 마스크 정보에 좌표를 원본 이미지에 상응하도록 후처리할 수 있다. 또한, 상기한 목적을 달성하기 위한 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미 지 생성 장치는 하나 이상의 프로세서; 및 상기 하나 이상의 프로세서에 의해 실행되는 적어도 하나 이상의 프 로그램을 저장하는 실행메모리를 포함하고, 상기 적어도 하나 이상의 프로그램은 데이터셋 내 이미지들에 존재 하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계; 상기 너비 최대값 및 높이 최대값을 이용하 여 이미지 분할 시 스트라이드(stride)를 산출하는 단계; 상기 스트라이드에 기반하여 원본 이미지를 복수의 서 브 이미지로 분할하는 단계; 및 상기 서브 이미지를 신경망 모델에 입력하여 인공지능 태스크를 수행하는 단 계;의 수행을 위한 명령어들을 포함한다. 이때, 상기 스트라이드는 상기 신경망 모델의 입력 사이즈보다 작을 수 있다. 이때, 상기 이미지 분할 시 스트라이드(stride)는 상기 신경망 모델의 입력 너비 값과 높이 값에서 각각 상기 너비 최대값 및 높이 최대값을 뺀 값에 상응할 수 있다. 이때, 상기 적어도 하나 이상의 프로그램은 상기 탐지한 객체의 바운딩 박스의 좌표 정보를 원본 이미지에 상응 하도록 후처리하는 단계의 수행을 위한 명령어를 더 포함할 수 있다. 이때, 상기 원본 이미지를 복수의 서브 이미지로 분할하는 단계는 상기 원본 이미지를 상기 신경망 모델의 입력 너비 값과 높이 값의 배수에 상응하도록 제로 패딩을 수행할 수 있다. 이때, 상기 인공지능 태스크가 인스턴스 세그멘테이션에 상응하면, 객체 마스크 정보에 좌표를 원본 이미지에 상응하도록 후처리할 수 있다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면, 고해상도 이미지에서의 작은 크기의 오브젝트 탐지를 위하여 원본 이미지를 모델에서 요구하 는 크기의 작은 이미지들로 중첩 분할한 이미지들을 생성할 수 있다. 또한, 본 발명에 따르면, 고해상도 이미지 내에 존재하는 작은 오브젝트를 탐지할 수 있다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다. 본 명세서에서, \"A 또는 B\", \"A 및 B 중 적어도 하나\", \"A 또는 B 중 적어도 하나\", \"A, B 또는 C\", \"A, B 및 C 중 적어도 하나\", 및 \"A, B, 또는 C 중 적어도 하나\"와 같은 문구들 각각은 그 문구들 중 해당하는 문구에 함 께 나열된 항목들 중 어느 하나, 또는 그들의 모든 가능한 조합을 포함할 수 있다."}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 실시예들을 상세히 설명하기로 하며, 도면을 참조하여 설명할 때 동일 하거나 대응하는 구성 요소는 동일한 도면 부호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 오브젝트 탐지 및 인스턴스 세그멘테이션 기술은 자율 주행차와 같은 산업용 시각 지능 응용에서 다른 차량, 교 통 신호, 도로 표지판 및 보행자의 탐지 등에서 필수적인 기술이다. 오브젝트 탐지에 많이 사용되는 R-CNN계열 또는 YOLO 계열의 딥러닝 기반 오브젝트 탐지기들은 중간 크기 또는 큰 크기의 오브젝트를 잘 탐지하지만, 작은 크기의 오브젝트 탐지에는 성능이 좋지 않다. 대부분의 딥러닝 기반 오브젝트 탐지기들은 오브젝트 위치 및 오브젝트 분류 태스크에 입력으로 사용되는 특징 (feature)들을 추출할 때 CNN(Convolutional Neural Network) 기반의 특징 추출기(feature extractor)를 백본 (backbone)으로 사용한다. CNN 기반의 특징 추출기에서는 특징(feature)의 크기가 레이어가 깊어질수록 반씩 줄 어들게 되므로, 작은 오브젝트의 경우 feature 자체가 마지막 레이어에서는 사라질 수도 있다. 도 3은 오브젝트 탐지용 딥러닝 모델 PP-YOLO의 구조를 나타낸다. 도 3을 참조하면, PP-YOLO 구조는 컨볼루션 레이어에서의 pixel aggregation을 통하여 특징 맵(feature map)들 을 만드는 것을 알 수 있다. 하기 [수학식 1]은 YOLO 모델에서 사용하는 손실 함수를 나타낸다. 오브젝트 탐지용 모델의 마지막 레이어에서 는 손실 함수에 기반하여 모델의 파라미터들을 학습하는데, 이러한 손실 함수는 모델을 통한 예측값과 실제값 (ground truth)의 차이에 기반하여 특징(feature)들 간에 sum하는 형태로 정의되어 있다. [수학식 1]"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "따라서 ground truth box의 크기가 크지 않은 경우, 트레이닝 과정에서 loss function에서 기여하는 바가 아주 작거나 없어져서 작은 크기의 오브젝트 탐지가 불가능하게 된다. 예를 들어, 오브젝트 탐지용 딥러닝 모델의 히든 레이어에서 512x512의 원본 이미지는 30x30 크기의 특징 맵 (feature map)으로 작아질 수 있다. 이러한 경우 원본 이미지 내에 포함된 작은 크기의 오브젝트에 대한 특징 (feature)은 사라질 수도 있어서 작은 크기의 오브젝트가 탐지되지 않을 수 있다. 이에 대한 해결책으로 작은 크기의 오브젝트 탐지를 위해서 고해상도 이미지를 사용하는 방법을 생각할 수 있다. 그러나 고해상도 이미지를 입력으로 하는 모델을 트레이닝에 많은 시간과 GPU 메모리도 비례하여 증가한 다. 모델 크기도 증가하므로 추론 시간 역시 증가하게 된다. 더 큰 문제점은 고해상도 이미지 자체를 그대로 사 용하는 경우, 모델에 필요한 GPU 메모리가 부족하여 모델 트레이닝 자체가 불가능할 수도 있다. 카메라 및 영상 기술이 발전함에 따라 딥러닝 기반 오브젝트 탐지기의 입력 해상도를 초과하는 많은 고해상도 이미지들이 존재한다. 예를 들어, FullHD 영상의 경우, 1920x1080 해상도를 가지며, 4K영상의 경우에는 3840x2160 해상도를 제공하며, 8K 영상의 경우에는 7680x4320해상도를 가지고 있다. 의료 영상에 사용하는 DICOM (Digital Imaging and Communication in Medicine) 표준을 따르는 영상 이미지의 경우, 12bit gray- scaled 2,000 ~ 3,500 x 2,000 ~ 3,500의 해상도를 가지고 있다. CNN 기반의 딥러닝 모델들은 대개 224x224 또는 448x448 크기 등의 작은 해상도의 입력 이미지를 가정하고 설계 되었다. 도 4는 오브젝트 탐지용 딥러닝 모델 중 최상위 성능을 보이는 이른바 SOTA(State of The Art) 모델인 YOLOv3, EfficientDet 및 YOLOv4에서의 성능을 비교한 표이다. Size 열의 값이 해당 딥러닝 모델에서 가정하는 입력 이미지의 크기로 416x416, 523x523, 608x608등의 크기가 사용된 것을 볼 수 있다. 최근에는 GAP(Global Average Pooling)을 사용하여 입력 이미지의 크기에 제한이 없기는 하지만, 보통 1024x1024 해상도 이상의 이미지를 트레이닝하거나 추론에 사용하지는 않는다. 1024x1024 해상도의 컬러 이미지 의 경우, 이미지 한 장에 1024x1024x3 = 3,145,728 byte = 3Mbyte의 대용량 입력이기 때문이다. 입력 이미지의크기에 비례하여 CNN 기반 딥러닝 모델의 크기도 커지므로, GPU 메모리가 딥러닝 모델 차체가 사용하는 메모리 크기 + 1장의 대용량 이미지를 감당할 수 없게 된다. 한 장의 대용량 이미지를 감당할 수 있는 크기의 GPU 메모 리가 지원된다 하더라도 batch 기반의 트레이닝은 불가능하게 된다. 따라서, 오브젝트 탐지용 딥러닝 모델에서 가정하는 해상도로 입력 이미지를 줄여서 사용해야 하지만, 이 경우, 문제가 발생할 수 있다. 예를 들어, 3000x3000 해상도의 DICOM 포맷 의료 영상 이미지에 대하여 딥러닝 모델의 입력 크기인 224x224 해상도로 줄여버린다면, 딥러닝 모델을 통하여 탐지할 병변의 이미지 상에서의 특징 정보 가 줄어들거나 심한 경우 사라질 수도 있어서, 시각지능 응용에서 요구하는 성능을 얻을 수 없게 될 수 있다. 이하, 본 발명은 고해상도 이미지에서의 작은 크기의 오브젝트 탐지를 위하여 원본 이미지를 모델에서 요구하는 크기의 작은 이미지들로 중첩 분할한 이미지들을 통하여 트레이닝을 하고, 추론 시에도 입력 이미지를 트레이닝 시에 사용한 해상도에 맞게 중첩 분할한 이미지를 대상으로 추론한 결과를 다시 합쳐서 원본 이미지에서의 오브 젝트 탐지 결과를 복원하는 장치 및 방법을 고안한다. 예를 들어, 2048x2048 크기의 원본 이미지에서 30x30크기의 오브젝트의 경우, 원본 이미지를 512x512크기의 4개 의 타일로 분할하면, 오브젝트는 512x512 타일 이미지에서도 동일한 크기(30x30)을 유지한다. 원본 이미지에서 의 오브젝트의 면적의 비율과 타일 이미지에서의 오브젝트의 면적의 비율을 계산하면 아래와 같다. 원본 이미지에서의 오브젝트의 면적의 비율 = (30x30) / (2048x2048) = 0.0002145 = 0.0214% 타일 이미지에서의 오브젝트의 면적의 비율 = (30x30) / (512x512) 0.0034332 = 0.3433% 따라서, 초고해상도 원본 이미지의 경우에도, Yolo v4에서 사용하는 입력 이미지인 416x416 크기의 타일들로 분 할하여 딥러닝 모델을 학습할 수 있다. 추론 시에도 추론용 입력 이미지를 416x416 크기의 타일 이미지로 분할 하여 추론한 후, 타일 이미지상에서의 추론 결과들을 역으로 결합하여 초고해상도 이미지상에서의 오브젝트 탐 지를 수행할 수 있다. 원본 이미지가 416의 배수가 아닌 경우에는 416의 배수 중 가장 작은 배수가 되도록 원본 이미지에 zero padding을 통하여 크기를 일치 시킨 후 추론한다. 예를 들어 원본 이미지가 5616x3744인 경우, 너비의 경우 5616/416 = 13.5, 높이의 경우 3744/416 = 9이므로, 너비는 13.5를 반올림함 14를 사용하여 새로운 너비를 계 산하면 416x14=5824이므로, 원본 이미지 5616x374에 너비에 zero-padding을 통하여 5824x3744로 만든다. 이러한 타일로 분할한 이미지를 통한 트레이닝 및 추론 방법들은 작은 크기의 오브젝트 탐지에서 유용하게 사용 되지만, 다음과 같은 문제점이 있다. 타일 경계선에 걸친 오브젝트의 경우, 타일 분할에 따라 오브젝트도 분할되어, 트레이닝시 불완전한 정보를 제 공하므로, 타일 경계선에 거치는 오브젝트가 많은 경우, 트레이닝용 데이터셋 자체의 품질 저하의 문제가 발생 한다. 도 5는 타일 분할 이미지 생성의 일 예이다. 예를 들어, 트레이닝 데이터셋에 포함된 이미지의 크기가 WOI×HOI=1024×768이고, 딥러닝 모델에서 가정하는 입 력 이미지의 크기가 WTI×HTI=416×416인 경우, 도 5에서와 같이 6개의 타입 분할 이미지가 가능하지만, X로 표 시된 타일 이미지 경계선 영역에 존재하는 오브젝트의 경우, 트레이닝시 오브젝트의 일부분만이 포함되는 문제 가 발생한다. 이러한 문제점은 해결하기 위하여 본 발명에서는 트레이닝 데이터셋 전처리 과정을 통하여 중첩된 타일 이미지 들을 만들고 중첩된 타일 이미지들을 대상으로 딥러닝 모델의 학습 및 추론하는 방법 및 장치를 고안한다. 본 발명에서는 설명의 편의를 위하여, 오브젝트 탐지용 딥러닝 모델을 대상으로 예시하였으나, 인스턴스 세그멘 테이션용 딥러닝 모델에도 본 발명의 사상을 동일한 방식으로 적용할 수 있다. 도 6은 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법을 나타낸 흐름도 이다. 도 6을 참조하면, 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법은 데 이터셋 내 이미지들에 존재하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계(S110), 상기 너비 최대값 및 높이 최대값을 이용하여 이미지 분할 시 스트라이드(stride)를 산출하는 단계(S120), 상기 스트라이 드에 기반하여 원본 이미지를 복수의 서브 이미지로 분할하는 단계(S130) 및 상기 서브 이미지를 신경망 모델에입력하여 인공지능 태스크를 수행하는 단계(S140)를 포함한다. 이때, 상기 스트라이드는 상기 신경망 모델의 입력 사이즈보다 작을 수 있다. 이때, 상기 이미지 분할 시 스트라이드(stride)는 상기 신경망 모델의 입력 너비 값과 높이 값에서 각각 상기 너비 최대값 및 높이 최대값을 뺀 값에 상응할 수 있다. 이때, 도 6에는 도시되지 않았지만, 상기 방법은 상기 탐지한 객체의 바운딩 박스의 좌표 정보를 원본 이미지에 상응하도록 후처리하는 단계를 더 포함할 수 있다. 이때, 상기 원본 이미지를 복수의 서브 이미지로 분할하는 단계(S140)는 상기 원본 이미지를 상기 신경망 모델 의 입력 너비 값과 높이 값의 배수에 상응하도록 제로 패딩을 수행할 수 있다. 이때, 상기 인공지능 태스크가 인스턴스 세그멘테이션에 상응하면, 객체 마스크 정보에 좌표를 원본 이미지에 상응하도록 후처리할 수 있다. 도 7은 본 발명의 일 실시예에 따른 시각 지능 추론의 전체 흐름을 나타낸다. 도 7을 참조하면, 본 발명의 일 실시예에 따른 시각 지능 추론 과정은 딥러닝 모델의 학습을 위한 트레이닝 데 이터셋 전처리 단계(S610), 전처리된 트레이닝 데이터셋을 사용한 딥러닝 모델의 학습 단계(S620), 학습된 모델 을 사용한 추론을 위한 테스트 데이터셋 전처리 단계(S630), 학습된 모델을 사용한 추론 단계(S640) 및 추론 결 과의 후처리 단계(S650)를 포함할 수 있다. 도 8은 본 발명의 일 실시예에 따른 트레이닝 데이터셋 전처리 단계를 상세히 나타낸 흐름도이다. 먼저, 본 발명의 일 실시예에 따른 트레이닝 데이터셋 전처리 단계를 설명하기 위해 하기 [표 1]과 같이 기호들 을 정의할 수 있다. 표 1 기호 설명 WOI 트레이닝 데이터셋에 포함된 원본 고해상도 이미지(original image)의 너비(width) HOI 트레이닝 데이터셋에 포함된 원본 고해상도 이미지(original image)의 너비(height) WBB_MAX 트레이닝 데이터셋 전체에서의 오브젝트 바운딩 박스의 너비(width)의 최대값 HBB_MAX 트레이닝 데이터셋 전체에서의 오브젝트 바운딩 박스의 높이(height)의 최대값 xOPIi 원본 이미지 좌상단 기준, i 번째 중첩 분할 이미지의 좌상단 x 좌표값 yOPIi 원본 이미지 좌상단 기준, i 번째 중첩 분할 이미지의 좌상단 y 좌표값 xBBj 원본 이미지 좌상단 기준, j 번째 오브젝트의 바운딩 박스의 좌상단 x 좌표값 yBBj 원본 이미지 좌상단 기준, j 번째 오브젝트의 바운딩 박스의 좌상단 y 좌표값 WBBj 원본 이미지 좌상단 기준, j 번째 오브젝트의 바운딩 박스의 너비(width) HBBj 원본 이미지 좌상단 기준, j 번째 오브젝트의 바운딩 박스의 높이(height) Cj j 번째 오브젝트의 클래스 레이블 Maskj 원본 이미지 좌상단 기준, j 번째 오브젝트의 인스턴스 마스크 정보 WTI 중첩 분할 이미지의 너비 - 오브젝트 탐지용 딥러닝 모델에서 가정하는 입력 이미지 의 너비 HTI 중첩 분할 이미지의 높이- 오브젝트 탐지용 딥러닝 모델에서 가정하는 입력 이미지 의 높이 SW 원본 이미지를 중첩 분할 시 사용할 너비 방향(X축)의 이동 폭(stride) SH 원본 이미지를 중첩 분할 시 사용할 높이 방향(Y축)의 이동 폭(stride) 이하, 도 8 및 표 1을 참고하여 본 발명의 일 실시예에 따른 트레이닝 데이터셋 전처리 단계를 상세히 설명한다.- 중첩 분할 이미지 생성을 위한 WBB_MAX 및 HBB_MAX 계산(S611) 원본 이미지에서 중첩 분할되는 타일 이미지를 생성하는 과정에서 탐지해야 할 오브젝트의 영역이 중첩 불할 되 는 타일 이미지중 하나에는 반드시 전체 크기가 포함될 수 있도록, 하기 [수학식 2]와 같이 트레이닝 데이터셋 에 포함된 모든 이미지들의 레이블링 정보에서 바운딩 박스(BB_BOX)의 너비와 높이 정보에서 각각 너비의 최대 값과 높이의 최대값을 계산한다. [수학식 2]"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 2에서 n은 트레이닝 데이터셋 내의 전체 이미지들에 포함된 오브젝트의 전체 개수를 의미한다. - 중첩 분할 이미지 생성을 위한 SW 및 SH 계산(S613) 원본 이미지의 중첩 분할을 위하여 원본 이미지에서 X축 및 Y축 스트라이드(stride)를 하기의 수학식 3과 같이 계산한다. [수학식 3]"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "- 중첩 분할 이미지 생성(S615) 단계(S611) 및 단계(S613)에서 트레이닝 데이터셋에 포함된 이미지들의 레이블링 정보로부터 계산한 SW 및 SH를 사용하여 중첩 분할 이미지를 생성한다. 도 9는 본 발명의 일 실시예에 따른 이미지 분할 방법에 따른 이미지 분할의 일 예이다. 예를 들어, 트레이닝 데이터셋에 포함된 이미지의 크기가 WOI×HOI=1024×768 이고, 트레이닝 데이터셋에 포함된 이미지들로부터 계산된 오브젝트의 바운딩 박스의 최대 크기가 각각 WBB_MAX = 30, HBB_MAX = 30이고, 딥러닝 모델에 서 가정하는 입력 이미지의 크기가 WTI × HTI = 416 × 416 인 경우, 중첩 분할 이미지 생성을 위한 스트라이드 는 각각 SW = WTI - WBB_MAX = 416 - 30 = 386, SH = HTI-HBB_MAX = 416-30 = 386이 되므로, 스트라이드를 적용하여 원본 이미지를 중첩 분할하면 도 9와 같이 중첩 분할 이미지가 6개가 생성된다. - 중첩 분할 이미지용 오브젝트 레이블링 정보 수정(S617) 원본 입력 이미지에서 오브젝트 레이블링 정보 Bounding Box 정보(x, y, width, height)에서 Bounding Box의 좌상단 좌표를 나타내는 (x, y)는 원본 입력 이미지의 좌상단 원점 기준의 좌표이므로, 중첩 분할된 이미지에 맞게 보정이 필요할 수 있다. 먼저 현재 고려중인 중첩 분할된 이미지 내에 해당 오브젝트의 완전 포함(fully contained)여부를 판별하고, 완 전히 포함되는 오브젝트들을 대상으로 아래와 같이 오브젝트 레이블링 정보 중 Bounding Box 정보(x, y, width, height) 중 Bounding Box의 좌상단 좌표를 나타내는 (x, y)를 하기의 수학식 4와 같이 변환할 수 있다.[수학식 4]"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "오브젝트의 Bounding Box 정보(x, y, width, height) 중 Bounding Box의 너비와 높이는 중첩 분할된 이미지 내 에서도 유지되므로, 별도의 처리가 필요하지 않을 수 있다. 또한 오브젝트의 분류값(Cj)도 중첩 분할 이미지에서 그대로 유지되므로 별도의 처리가 필요 없다. 인스턴스 세그멘테이션용 딥러닝 모델의 경우에는 오브젝트의 인스턴스 마스크 정보 Maskj에도 Bounding Box의 좌상단 좌표를 나타내는 (x, y)에서의 좌표변환과 같은 과정을 적용하여 마스크 정보를 변환해주어야 한다. 현재 고려중인 중첩 분할 이미지 기준으로, 완전히 포함되지 않은 오브젝트의 경우, 현재 고려중인 중첩 분할 이미지에는 포함이 되지는 않지만, 다른 1개 이상의 중첩 분할 이미지에는 반드시 포함이 되도록 중첩 분할 이 미지 생성 기법이 설계되어 있으므로, 현재 고려중인 중첩 분할 이미지에 배제된 오브젝트는 다른 중첩 분할 이 미지에서 처리되도록 보장될 수 있다. 전처리된 트레이닝 데이터셋을 사용한 딥러닝 모델의 학습 단계(S620)는 딥러닝 모델의 학습을 위한 트레이닝 데이터셋 전처리 과정을 통하여 생성된 트레이닝 데이터셋을 사용하여 딥러닝 모델을 학습한다. 도 10은 테스트 데이터셋 전처리 단계의 세부 단계를 나타낸 도면이다. 학습된 모델을 사용한 추론을 위한 테스트 데이터셋 전처리 단계(S630)에서, 전처리된 트레이닝 데이터셋을 사 용하여 학습된 모델을 사용하여 추론하는 경우, 추론용 데이터셋에도 도 10에서와 같이, 중첩 분할 이미지 생성 에서 제시한 방법과 동일한 전처리 과정을 적용하여 중첩 분할된 이미지들로 구성된 테스트 데이터셋을 생성한 다. 테스트 데이터셋 전처리 과정에서 중첩 분할 이미지 생성을 위해 필요한 스트라이드 SW 및 SH의 값은 트레이닝 데이터 셋의 중첩 분할 이미지 생성을 위해 사용한 스트라이드의 값과 동일한 값을 사용할 수 있다. 학습된 모델을 사용한 추론 단계(S640)는 테스트 데이터 셋 전처리 과정을 통하여 생성된 중첩 분할된 이미지들 로 구성된 테스트 데이터셋을 입력으로 학습된 딥러닝 모델에서 추론을 수행한다. 도 11은 추론 결과의 후처리 단계의 세부 단계를 나타낸 도면이다. 추론된 결과는 중첩 분할된 이미지들을 대상으로 한 결과이므로, 원본 이미지를 대상으로 한 추론 결과를 위하 여 후처리가 필요하다. 중첩 분할된 이미지들로 구성된 추론용 데이터셋에는 탐지할 오브젝트의 Bounding Box 정보 및 분류 정보가 포 함되어 있지 않으며, 추론의 결과로 탐지된 오브젝트별로 Bounding Box 정보 및 분류 정보가 리턴된다. 추론된 Box Bounding Box 정보(x, y, width, height)에서 Bounding Box의 좌상단 좌표를 나타내는 (x, y)정보 는 중첩 분할된 이미지의 좌상단 원점 기준의 좌표값이므로, 원본 입력 이미지의 좌상단 원점 기준의 좌표로 보 정해주는 후처리가 필요하다. 후처리 과정은 하기의 수학식 5와 같을 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0077448", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "이때, 오브젝트의 Bounding Box 정보(x, y, width, height)중 Bounding Box의 너비와 높이는 중첩 분할된 이미 지 내에서도 유지되므로, 별도의 후처리가 필요 없다. 또한, 오브젝트의 분류값(Cj)도 중첩 분할 이미지에서 그 대로 유지되므로 별도의 후처리가 필요 없다. 도 12는 실시예에 따른 컴퓨터 시스템의 구성을 나타낸 도면이다. 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 장치는 컴퓨터로 읽을 수 있는 기록매체와 같은 컴퓨터 시스템에서 구현될 수 있다. 컴퓨터 시스템은 버스를 통하여 서로 통신하는 하나 이상의 프로세서, 메모리, 사용자 인터페이스 입력 장치, 사용자 인터페이스 출력 장치 및 스토리지를 포함할 수 있다. 또한, 컴퓨터 시스템은 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있다. 프로세서 는 중앙 처리 장치 또는 메모리나 스토리지에 저장된 프로그램 또는 프로세싱 인스트럭션들 을 실행하는 반도체 장치일 수 있다. 메모리 및 스토리지는 휘발성 매체, 비휘발성 매체, 분리형 매체, 비분리형 매체, 통신 매체, 또는 정보 전달 매체 중에서 적어도 하나 이상을 포함하는 저장 매체일 수 있 다. 예를 들어, 메모리는 ROM이나 RAM을 포함할 수 있다. 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 장치는 하나 이상의 프로세서 및 상기 하나 이상의 프로세서에 의해 실행되는 적어도 하나 이상의 프로그램을 저장하는 실행메모리 를 포함하고, 상기 적어도 하나 이상의 프로그램은 데이터셋 내 이미지들에 존재하는 바운딩 박스의 너비 최대값 및 높이 최대값을 산출하는 단계; 상기 너비 최대값 및 높이 최대값을 이용하여 이미지 분할 시 스트라 이드(stride)를 산출하는 단계; 상기 스트라이드에 기반하여 원본 이미지를 복수의 서브 이미지로 분할하는 단 계; 및 상기 서브 이미지를 신경망 모델에 입력하여 인공지능 태스크를 수행하는 단계;의 수행을 위한 명령어들 을 포함한다. 이때, 상기 스트라이드는 상기 신경망 모델의 입력 사이즈보다 작을 수 있다. 이때, 상기 이미지 분할 시 스트라이드(stride)는 상기 신경망 모델의 입력 너비 값과 높이 값에서 각각 상기 너비 최대값 및 높이 최대값을 뺀 값에 상응할 수 있다. 이때, 상기 적어도 하나 이상의 프로그램은 상기 탐지한 객체의 바운딩 박스의 좌표 정보를 원본 이미지에 상응 하도록 후처리하는 단계의 수행을 위한 명령어를 더 포함할 수 있다. 이때, 상기 원본 이미지를 복수의 서브 이미지로 분할하는 단계는 상기 원본 이미지를 상기 신경망 모델의 입력 너비 값과 높이 값의 배수에 상응하도록 제로 패딩을 수행할 수 있다. 이때, 상기 인공지능 태스크가 인스턴스 세그멘테이션에 상응하면, 객체 마스크 정보에 좌표를 원본 이미지에 상응하도록 후처리할 수 있다. 본 발명에서 설명하는 특정 실행들은 실시예들로서, 어떠한 방법으로도 본 발명의 범위를 한정하는 것은 아니다. 명세서의 간결함을 위하여, 종래 전자적인 구성들, 제어시스템들, 소프트웨어, 상기 시스템들의 다른 기능적인 측면들의 기재는 생략될 수 있다. 또한, 도면에 도시된 구성 요소들 간의 선들의 연결 또는 연결 부재 들은 기능적인 연결 및/또는 물리적 또는 회로적 연결들을 예시적으로 나타낸 것으로서, 실제 장치에서는 대체 가능하거나 추가의 다양한 기능적인 연결, 물리적인 연결, 또는 회로 연결들로서 나타내어질 수 있다. 또한, “ 필수적인”, “중요하게” 등과 같이 구체적인 언급이 없다면 본 발명의 적용을 위하여 반드시 필요한 구성 요 소가 아닐 수 있다. 따라서, 본 발명의 사상은 상기 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청구범위뿐만 아 니라 이 특허청구범위와 균등한 또는 이로부터 등가적으로 변경된 모든 범위는 본 발명의 사상의 범주에 속한다 고 할 것이다."}
{"patent_id": "10-2023-0077448", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 시각 지능 작업의 종류를 나타내는 도면이다. 도 2는 바운딩 박스의 개념을 설명하기 위한 도면이다. 도 3은 오브젝트 탐지용 딥러닝 모델 PP-YOLO의 구조를 나타낸다. 도 4는 객체 탐지 네트워크의 성능을 나타낸 표이다. 도 5는 타일 분할 이미지 생성의 일 예이다. 도 6은 본 발명의 일 실시예에 따른 인공지능 태스크 수행을 위한 중첩 분할 이미지 생성 방법을 나타낸 흐름도 이다. 도 7은 본 발명의 일 실시예에 따른 시각 지능 추론의 전체 흐름을 나타낸다. 도 8은 본 발명의 일 실시예에 따른 트레이닝 데이터셋 전처리 단계를 상세히 나타낸 흐름도이다. 도 9는 본 발명의 일 실시예에 따른 이미지 분할 방법에 따른 이미지 분할의 일 예이다. 도 10은 테스트 데이터셋 전처리 단계의 세부 단계를 나타낸 도면이다. 도 11은 추론 결과의 후처리 단계의 세부 단계를 나타낸 도면이다. 도 12는 실시예에 따른 컴퓨터 시스템의 구성을 나타낸 도면이다."}
