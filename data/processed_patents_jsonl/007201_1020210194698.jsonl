{"patent_id": "10-2021-0194698", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0103664", "출원번호": "10-2021-0194698", "발명의 명칭": "감정 지표와 집중 지표를 바탕으로 제공되는 아바타를 이용하여 사용자 및 참여자 간 상호 작", "출원인": "주식회사 마블러스", "발명자": "한규선"}}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨팅 장치에 의해 수행되는 감정 및 집중상태의 변화에 기반한 비대면 화상 회의 방법에 있어서,외부 전자장치로부터 이미지 데이터를 수신하는 단계;수신한 이미지 데이터에서 얼굴 데이터를 검출하는 단계;상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계;상기 얼굴 데이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 감정 지표를 획득하는 단계; 상기 얼굴 데이터를 바탕으로 집중 지표를 획득하는 단계; 및상기 얼굴 데이터, 감정 지표, 및 집중 지표를 바탕으로 가상 아바타를 제공하는 단계를 포함하는 비대면 화상회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 집중 지표를 획득하는 단계는 광혈류량 변화량 기반의 rPPG 모델에 입력하여 집중 지표를 획득하는 단계인것을 특징으로 하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 rPPG 모델은 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 도출하고 보통, 집중, 몰입 단계별로집중 상태를 판별하고, 각각의 확률값을 산출하는 것을 특징으로 하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 감정 인식 모델은 긍정, 부정, 중립 중 어느 하나의 감정 유형을 판별하고 각각의 확률값을 산출하는 제1감정 인식 모델을 포함하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 집중 지표와 상기 감정 지표를 바탕으로 학습 지수를 산출하는 단계;를 더 포함하는 것을 특징으로 하는비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 가상 아바타의 표현 양상을 실시간으로 운영자에게 전달하고, 피드백을 제공하는 단계;를 더 포함하는 것공개특허 10-2023-0103664-3-을 특징으로 하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 가상 아바타를 제공하는 단계는,인공지능 이미지 생성 기술 및 인공지능 음성 합성 기술을 활용하여 사용자 맞품형 아바타를 제공하는 단계인것을 특징으로 하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 가상 아바타를 제공하는 단계는, GAN(Generative Adversarial Networks)를 활용하여 제공하는 단계인 것을 특징으로 하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항에 있어서,상기 감정 인식 모델은 즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함 중 어느 하나의 감정 유형을 판별하고 각각의 확률값을 산출하는 제2 감정 인식 모델을 포함하는 비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서,상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계는,1차적으로 얼굴 영역을 검출하고,얼굴 영역 검출에 실패한 경우 2차적으로 사물 인식 모델을 바탕으로 출석 여부를 판별하는 것을 특징으로 하는비대면 화상 회의 방법."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "전자 장치에 있어서,메모리, 송수신기 및 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는, 제1 항 내지 제8 항 중 어느 한 항에 따른 비대면 화상 회의 방법을 수행하도록 구성된, 전자 장치."}
{"patent_id": "10-2021-0194698", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제1 항 내지 제10 항 중 어느 한 항에 따른 비대면 화상 회의 방법을 전자 장치를 통해 수행하도록 구성되며,컴퓨터 판독 가능한 저장 매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 컴퓨팅 장치에 의해 수행되는 컴퓨팅 장치에 의해 수행되는 감정 및 집중상태의 변화에 기반한 비대면 화상 회의 방법은, 외부 전자장치로부터 이미지 데이터를 수신하는 단계, 수신한 이미지 데이터에서 얼굴 데이터 를 검출하는 단계, 상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계, 상기 얼굴 데이터를 표정 인식 기 술 기반의 감정 인식 모델에 입력하여 감정 지표를 획득하는 단계, 상기 얼굴 데이터를 바탕으로 집중 지표를 획 득하는 단계, 및 상기 얼굴데이터, 감정 지표, 및 집중 지표를 바탕으로 가상 아바타를 제공하는 단계를 포함한 다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "감정 지표와 집중 지표를 바탕으로 한 비대면 화상 회의 장치, 방법, 및 프로그램에 관한 것이다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "1980년대 PC 혁명과 1990년대 인터넷이 등장으로 IT 디바이스와 네트워크는 급속도로 발전을 거듭하게 된다. 이 런 과정 중에 원격 회의와 e-Learning이라는 온라인 기반의 새로운 서비스들이 생겨났다. 이후 ICT의 비약적인 발전으로 이와 관련된 많은 기술과 시스템, 그리고 플랫폼들이 생겨난다. 그러면서 화상 교육 및 회의 시스템은 접근성, 편의성, 효율성 등 많은 장점을 바탕으로 학교와 기업뿐만 아니라 일상 생활 속에 자연스럽게 자리잡게 된다. 특히, 작년부터 코로나19로 인해 화상 교육과 회의는 많은 사람들에게 필수적인 서비스가 되었다. 하지만 기존 화상 교육 및 회의 시스템은 비디오나 음성을 제어할 수 있어서 비디오나 음성 기능을 끄거나 사용 하더라도 액션이 없으면 해당 사용자의 상태를 확인할 수 없었다. 그래서 참여자들 간의 상호작용이 이뤄지지 않고 보통 교강사 또는 회의 주최자가 일방적으로 진행되는 경우가 많았다. 이에 아바타와 AI 기술을 사용하여 사용자↔아바타↔참여자간 상호작용할 수 있는 방법과 이를 통해 화상 교육 및 회의 시스템을 효율적으로 이용 할 수 있는 기술의 개발이 요구된다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 전술한 문제점을 해결하기 위하여 감정 지표와 집중 지표를 바탕으로 한 비대면 화상 회의 장치, 방 법, 및 프로그램에 관한 것을 제공하고자 한다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 감정 및 집중상태의 변화에 기반한 비대면 화상 회의 방법은, 외부 전자장치로부터 이미지 데이터를 수신하는 단계; 수신한 이미지 데이터에서 얼굴 데이터를 검출하는 단계; 상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계; 상기 얼굴 데이터를 표정 인식 기술 기반의 감정 인 식 모델에 입력하여 감정 지표를 획득하는 단계; 상기 얼굴 데이터를 바탕으로 집중 지표를 획득하는 단계; 및 상기 얼굴데이터, 감정 지표, 및 집중 지표를 바탕으로 가상 아바타를 제공하는 단계;를 포함한다. 상기 집중 지표를 획득하는 단계는 광혈류량 변화량 기반의 rPPG 모델에 입력하여 집중 지표를 획득하는 단계일 수 있다. 상기 rPPG 모델은 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 도출하고 보통, 집중, 몰입 단계별로 집중 상태를 판별하고, 각각의 확률값을 산출할 수 있다. 상기 감정 인식 모델은 긍정, 부정, 중립 중 어느 하나의 감정 유형을 판별하고 각각의 확률값을 산출하는 제1 감정 인식 모델을 포함할 수 있다. 상기 집중 지표와 상기 감정 지표를 바탕으로 학습 지수를 산출하는 단계;를 더 포함할 수 있다. 상기 가상 아바타의 표현 양상을 실시간으로 운영자에게 전달하고, 피드백을 제공하는 단계;를 더 포함할 수 있 다. 상기 가상 아바타를 제공하는 단계는, 인공지능 이미지 생성 기술 및 인공지능 음성 합성 기술을 활용하여 사용자 맞품형 아바타를 제공하는 단계일 수 있다. 상기 가상 아바타를 제공하는 단계는, GAN(Generative Adversarial Networks)를 활용하여 제공하는 단계일 수 있다. 상기 감정 인식 모델은 즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함 중 어느 하나의 감정 유형을 판별하 고 각각의 확률값을 산출하는 제2 감정 인식 모델을 포함할 수 있다. 상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계는, 1차적으로 얼굴 영역을 검출하고, 얼굴 영역 검출에 실패한 경우 2차적으로 사물 인식 모델을 바탕으로 출석 여부를 판별할 수 있다. 전자 장치에 있어서, 메모리, 송수신기 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는, 전술한 비대면 화상 회의 방법을 수행하도록 구성된, 전자 장치. 전술한 비대면 화상 회의 방법을 전자 장치를 통해 수행하도록 구성되며, 컴퓨터 판독 가능한 저장 매체에 기록 된 컴퓨터 프로그램이 개시된다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 이미지 인식 기술을 사용하여 다음과 같이 3가지 세부 유형의 감정 지표를 측정할 수 있다. 제1 감정 상태 지표 및 제2 감정 상태 지표는 AI 딥러닝 기술 기반의 얼굴 표정 인식 기술을 사용하고, 집중 상태 지표는 얼굴 이미지의 광혈류량 변화량을 분석하는 rPPG 기술을 사용한다. 측정된 감정 및 집중 지표를 바탕으로 학습지수를 산출할 수 있다. 감정 상태, 집중상태 및 얼굴 인식 기술을 바탕으로, 사용자와 아바타, 아바타와 참여자의 상호작용을 증진시키 기 위한 참여자 가상 아바타를 제공할 수 있다. 화상 교육/회의가 진행 중일 때 사용자의 감정(긍정, 부정, 중립), 집중(보통, 집중, 몰입), 자리 이탈 상태를 AI 감정 및 사물 인식 기술을 이용하여 실시간으로 인식하고, 인식 결과를 토대로 역시 AI 생성 기술을 이용하 여 사용자의 아바타에 표현할 수 있다. 이를 통해 강사 또는 참여자들은 아바타가 표현하는 모습을 보고 해당 사용자의 상태를 실시간으로 파악할 수 있고, 적절한 피드백을 제공할 수 있다. 그리고 시스템에서 사용자의 상 태에 따라 적절한 알람을 설정하여 송출할 수 있다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과는 이상에서 언급된 것들에 한정되지 않으며, 언급되지 아니한 다른 효과들은 아래의 기재로부터"}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "당해 기술분야에 있어서의 통상의 지식을 가진 자가 명확하게 이해할 수 있을 것이다."}
{"patent_id": "10-2021-0194698", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부한 도면을 참고로 하여 본 발명의 실시 예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다.도 1은 본 발명의 다양한 실시 예들에 따른 통신 시스템을 도시한다. 도 1을 참고하면, 본 발명의 다양한 실시 예들에 따른 통신 시스템은 전자 장치, 유/무선 통신 네트워크 , 서버를 포함한다. 서버는 이미지 데이터를 유/무선통신 네트워크를 통해 사용자의 전자 장치로 부터 획득하고, 감정상태 및 집중상태를 도출한 뒤 인식 결과를 토대로 역시 AI 생성 기술을 이용 하여 사용자의 아바타를 생성하고, 해당 정보를 바탕으로 한 비대면 화상 회의 정보를 UX/UI로 구현하여 유/무 선통신 네트워크를 통해 사용자의 전자 장치에 다시 송신한다. 전자 장치는, 유/무선 통신 네트워크를 통하여 서버의 요청에 따라 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 촬영하여 송신한다. 전자 장치는 퍼스널 컴퓨터, 셀룰러 폰, 스마트 폰 및 태블릿 컴퓨터 등과 같이, 정보를 저장할 수 있는 메모리, 정보의 송수신을 수행할 수 있는 송수신부, 정보의 연산을 수행할 수 있는 적어도 하나의 프로세서를 포함하는 전자 장치일 수 있다. 전자 장치 의 종류는 한정되지 않는다. 유/무선 통신 네트워크는, 전자 장치 및 서버가 서로 신호 및 데이터를 송수신할 수 있는 통신 경로를 제공한다. 유/무선 통신 네트워크는 특정한 통신 프로토콜에 따른 통신 방식에 한정되지 않으며, 구현 예에 따라 적절한 통신 방식이 사용될 수 있다. 예를 들어, 인터넷 프로토콜(IP) 기초의 시스템으로 구성 되는 경우 유/무선 통신 네트워크는 유무선 인터넷망으로 구현될 수 있으며, 전자 장치 및 서버(13 0)가 이동 통신 단말로서 구현되는 경우 유/무선 통신 네트워크는 셀룰러 네트워크 또는 WLAN(wireless local area network) 네트워크와 같은 무선망으로 구현될 수 있다. 서버는, 유/무선 통신 네트워크를 통하여 전자 장치로부터 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 수신한다. 서버는 정보를 저장할 수 있는 메모리, 정보의 송수신을 수행할 수 있는 송수신부, 정보의 연산을 수행할 수 있는 적어도 하나의 프로세서를 포함하는 전자 장치일 수 있다. 도 2는 본 발명의 다양한 실시 예들에 따른 전자 장치의 구성에 대한 블록도를 도시한다. 도 2를 참고하면, 본 발명의 다양한 실시 예들에 따른 전자 장치는 메모리, 송수신부 및 프로세 서를 포함한다. 메모리는 휘발성 메모리, 비휘발성 메모리 또는 휘발성 메모리와 비휘발성 메모리의 조합으로 구성될 수 있다. 그리고, 메모리는 프로세서의 요청에 따라 저장된 데이터를 제공할 수 있다. 송수신부는, 프로세서와 연결되고 신호를 전송 및/또는 수신한다. 송수신부의 전부 또는 일부는 송신기(transmitter), 수신기(receiver), 또는 송수신기(transceiver)로 지칭될 수 있다. 송수신기는 유 선 접속 시스템 및 무선 접속 시스템들인 IEEE(institute of electrical and electronics engineers) 802.xx 시스템, IEEE Wi-Fi 시스템, 3GPP(3rd generation partnership project) 시스템, 3GPP LTE(long term evolution) 시스템, 3GPP 5G NR(new radio) 시스템, 3GPP2 시스템, 블루투스(bluetooth) 등 다양한 무선 통신 규격 중 적어도 하나를 지원할 수 있다. 프로세서는, 본 발명에서 제안한 절차 및/또는 방법들을 구현하도록 구성될 수 있다. 프로세서는 생 체 정보의 기계 학습 분석에 기반하여 컨텐츠를 제공하기 위한 전자 장치의 전반적인 동작들을 제어한다. 예를 들어, 프로세서는 송수신부를 통해 정보 등을 전송 또는 수신한다. 또한, 프로세서는 메모 리에 데이터를 기록하고, 읽는다. 프로세서는 적어도 하나의 프로세서(processor)를 포함할 수 있다. 도 3은 본 발명의 다양한 실시 예들에 따른 서버의 구성에 대한 블록도를 도시한다. 도 3을 참고하면, 본 발명의 다양한 실시 예들에 따른 서버는 메모리, 송수신부 및 프로세서 를 포함한다. 서버는 전자 장치의 일종일 수 있다. 서버는 유/무선 통신 네트워크를 통하여 전자 장치로부터 사용자의 학습 상태에 대한 얼굴 및 자세 정보를 포함하는 이미지 데이터를 수신한다. 서버는 수신한 이미지 데이터를 수신한 이미지 데이터를 Mat 데이터로 변환하고, Mat 데이터를 감정인식 SDK 모듈에 입력하여 감정 및 집중상태 값으로 변환하고, 감정 상태 및 집중상태 값을 제어 로직에 삽입하고, 상기 제어 로직에 연동되는 챗봇 메시지 UI를 호출한다. 메모리는, 송수신부와 연결되고 통신을 통해 수신한 정보 등을 저장할 수 있다. 또한, 메모리는, 프로세서와 연결되고 프로세서의 동작을 위한 기본 프로그램, 응용 프로그램, 설정정보, 프로세서의 연산에 의하여 생성된 정보 등의 데이터를 저장할 수 있다. 메모리는 휘발성 메모 리, 비휘발성 메모리 또는 휘발성 메모리와 비휘발성 메모리의 조합으로 구성될 수 있다. 그리고, 메모리 는 프로세서의 요청에 따라 저장된 데이터를 제공할 수 있다. 송수신부는, 프로세서와 연결되고 신호를 전송 및/또는 수신한다. 송수신부의 전부 또는 일부는 송신기(transmitter), 수신기(receiver), 또는 송수신기(transceiver)로 지칭될 수 있다. 송수신기는 유 선 접속 시스템 및 무선 접속 시스템들인 IEEE(institute of electrical and electronics engineers) 802.xx 시스템, IEEE Wi-Fi 시스템, 3GPP(3rd generation partnership project) 시스템, 3GPP LTE(long term evolution) 시스템, 3GPP 5G NR(new radio) 시스템, 3GPP2 시스템, 블루투스(bluetooth) 등 다양한 무선 통신 규격 중 적어도 하나를 지원할 수 있다. 프로세서는, 본 발명에서 제안한 절차 및/또는 방법들을 구현하도록 구성될 수 있다. 프로세서는 이 미지 데이터로부터 감정인식 SDK 모듈에 입력하여 감정 및 집중상태 값으로 변환하고, 감정상태 및 집중상태 값 을 입력값으로 하는 제어 로직 등 서버의 전반적인 동작들을 제어한다. 예를 들어, 프로세서는 송수 신부를 통해 정보 등을 전송 또는 수신한다. 또한, 프로세서는 메모리에 데이터를 기록하고, 읽 는다. 프로세서는 적어도 하나의 프로세서(processor)를 포함할 수 있다. 도 4는 본 발명의 다양한 실시 예들에 따른 순서도를 도시한다. 도4를 참조하면, 본 발명에 따른 비대면 화상 회의 제공 방법은, 외부 전자장치로부터 이미지 데이터를 수신하는 단계(S100), 수신한 이미지 데이터에서 얼굴 데이터를 검출하는 단계(S200), 상기 얼굴 데이터를 바탕으로 출석 여부를 판별하는 단계(S300), 상기 얼굴 데 이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 감정 지표를 획득하는 단계(S400), 상기 얼굴 데이터 를 바탕으로 집중 지표를 획득하는 단계(S500) 및 상기 얼굴 데이터, 감정 지표, 및 집중 지표를 바탕으로 가상 아바타를 제공하는 단계(S600)을 포함한다. 단계(S100)은, 사용자의 전자장치으로부터 촬영되거나 실시간으로 전송되는 얼굴을 포함하는 이미지 데이 터를 수신하는 단계이다. 본 발명에 따른 교육 방법은 사용자의 얼굴표정이 포함된 이미지 데이터를 바탕으로, 얼굴표정 기반 감정 및 집 중상태를 도출하고 해당 감정 및 집중상태를 바탕으로 학습지수를 도출하고, 비대면 화상 회의 시에 UI로 도시 하는 가상 아바타를 사용자의 전자장치로 제공함으로써 비대면 상황하에서의 회의 참여자의 반응에 대응할 수 있도록 하여 피드백을 효율화 하는 것에 있다. 이에 따라, 이미지 데이터는 적어도 복수개의 시간 시점의 얼굴 표정이 확인될 수 있는 해상도 및 화질을 가지 는 데이터일 것이 요구될 수 있다. 전술한 서버는 유/무선 통신 네트워크를 통해 전자장치로부 터 데이터를 수신할 수 있다. 단계(S200)은, 수신한 이미지 데이터에서 사용자의 얼굴 표정을 추출하여 얼굴표정 기반 감정 및 집중상태를 도 출하기 위한 데이터 셋을 추출하는 단계이다. 본 단계(S200)에 의한 데이터 셋 추출은 특정 실시예에 한정되는 것은 아니며 하기와 같은 추출 방법이 일 예시로 제시된다. 예를 들어 본 단계(S200)는 OpenCV 모듈 등을 통하여 수신한 이미지 데이터를 가공할 수 있다. 예를 들어, 본 단계에서는 이미지 데이터를 OpenCV 영상처리 라이브러리에서 다루는 데이터 형식 Mat(matrix) 형태로 변환할 수 있다. OpenCV는 오픈소스 컴퓨터 비전 C라이브러리의 약자로, 실시간 이미지 프로세싱에 활용되는 언어를 나 타낸다. 단계(S300)는, 사용자 디바이스의 일반 RGB 카메라로 사용자의 이미지를 입력으로 받아 얼굴 영역을 검출하고, 이를 바탕으로 하여 참여자의 출석 여부를 판별하는 단계이다. 구체적으로는, 얼굴 영역에서 특징들을 추출하고 이를 미리 저장해 놓은 사용자의 얼굴 이미지와 비교하여 일치 정도를 예측할 수 있다. 본 단계(S300)는 1차적으로, 사용자 디바이스의 일반 RGB 카메라로 사용자의 이미지를 입력으로 받아 얼굴 영역 을 검출하고, 검출되는 경우 출석한 것으로 판단할 수 있다. 만약. 얼굴 영역 검출에 실패하는 경우, 2처적으로 사물 인식 모델이 실행되어 카메라 안에 사람이 있는지 여부를 판단할 수 있다. 사물 인식 모델은 얼굴 뿐만 아 니라 손, 발, 머리, 어깨 등 신체 일부만 보여도 사람으로 인식할 수 있다. 본 단계(S300)는 딥러닝 기반 AI 사물 인식 기술 및 얼굴 인식 기술을 사용할 수 있다. 상기 얼굴 데이터를 표정 인식 기술 기반의 감정 인식 모델에 입력하여 감정 지표를 획득하는 단계(S400)는, 제 1 감정 인식 모델은 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수행하고 3가지 감정 유형(긍정, 부정, 중립) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감 정지수가 도출될 수 있다. 본 단계(S400)는. 제2 감정 인식 모델를 통해 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수행하고 7가지 감정 유형(즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감정지수가 도출될 수 있다. 제1 감정 인식 모델과 제2 감정 인식 모델은 택일적으로 적용되거나 양 모델이 상호보완적으로 적용되어 보다 정밀한 감정지수의 산출 이 가능할 수 있다. 상기 얼굴 데이터를 바탕으로 집중 지표를 획득하는 단계(S500)는. 집중도 인식 모델을 통해, 입력 이미지로부 터 얼굴 검출 후 심박수와 심박변이도를 도출할 수 있다. 도출된 심박변이도 데이터에 rPPG(원격 광혈류측정) 기술을 적용함으로써 심박수와 심박변이도를 측정하고 분석할 수 있다. 집중도 인식 모델은 심박변이도의 분석 결과를 토대로 3단계 집중 상태(보통→집중 →몰입) 각각의 확률값이 산출하여 이것을 집중 상태 지표를 도출할 수 있다. 집중 상태 지표를 바탕으로 집중 지수를 도출할 수 있다. 집중 상태 지표는 학습지수를 도출하기 위한 기초 수치로 활용될 수 있다. 단계(S600)은, 상기 얼굴 데이터, 감정 지표, 및 집중 지표를 바탕으로 가상 아바타를 제공할 수 있다. 화상 교 육/회의가 진행 중일 때 사용자의 감정(긍정, 부정, 중립), 집중(보통, 집중, 몰입), 자리 이탈 상태를 AI 감성 및 사물 인식 기술을 이용하여 실시간으로 인식하고, 인식 결과를 토대로 역시 AI 생성 기술을 이용하여 사용자 의 아바타에 표현할 수 있다. 본 단계(S600)는, 인공지능 이미지 생성 기술 및 인공지능 음성 합성 기술을 활용하여 사용자 맞품형 아바타를 제공하는 단계일 수 있다. 아바타의 생성 및 표현에 있어서는 전 단계에서 도출된 감정지수, 집중도, 학습지수를 GAN(Generative Adversarial Networks)를 활용하여 사용자의 아바타에 표현할 수 있다. 사용자의 아바타에 각 지수를 반영하는 구체적인 예시는 하기와 같다. 첫째로 감정지수의 표현에 있어서, 감정지수 0~100을 3가지 감정 유형에 따라 구간으로 나눌수 있다(0~29: 부정 / 30~69: 중립 / 70~100: 긍정). AI 감성 인식 모델에 의해 산출된 감정지수에 따라 AI 이미지 생성 기술을 사 용하여 해당 감정지수에 적합한 얼굴 이미지를 생성한다. 예를 들어, 감정지수가 75이면 긍정 감정 상태이므로 웃는 표정의 얼굴 이미지를 생성한다. GAN은 빅데이터로 학습된 모델이기 때문에 감정지수가 같더라도 단일한 표정의 이미지가 아닌 다양한 표정의 이미지 생성이 가능할 수 있다. 그래서 서로 다른 사용자의 감정지수가 같 더라도 다른 표정의 이미지로 생성될 수 있는 것이다. 이렇게 생성된 이미지를 사용자의 3D 아바타에 매핑하여 최종적으로 아바타가 해당 표정을 지어 감정을 표현하도록 한다. 둘째로, 집중지수 표현: 집중지수 0~100을 3단계 집중 유형에 따라 구간으로 나눌 수 있다 (0~39: 보통 / 40~79: 집중 / 80~100: 몰입). AI 감성 인식 모델에 의해 산출된 집중지수에 따라 사용자의 아바타에 후광 효과 를 적용한다. ○ 보통: 후광 효과 없음. ○ 집중: 후광 효과 약하게 적용. ○ 몰입: 후광 효과 강하게 적용. 셋째로, 학습(업무)지수 표현: 학습지수 0~100을 3가지 학습(업무) 몰입 유형에 따라 구간으로 나눌수 있다 (0~29: 하 / 30~69: 중 / 70~100: 상). AI 감성 인식 모델에 의해 산출된 학습(업무)지수에 따라 AI 이미지 생 성 기술을 사용하여 해당 학습(업무)지수에 적합한 포즈 이미지를 생성할 수 있다. 아래는 유형별 포즈 예시이 다. ○ 하: 지루해하는 모습(하품, 엎드림, 턱을 굄 등) ○ 중: 평범한 모습(가만히 있음) ○ 상: 몰두하는 모습(필기하고 있음) 마지막으로, 자리 이탈 표현: AI 사물(사람) 인식 모델에 의해 자리 이탈로 판단되면 사용자의 아바타를 투명 처리하고 아바타 윤곽선을 점선으로 표시한다. 다시 사람이 인식되면 아바타를 표시한다.추가적으로 본 발명은, 감정인식 도출 모델은 전술한 제1 감정 인식 모델, 제2 감정 인식 모델, 집중도 인식 모 델에서 도출된 정량적 지표 값을 바탕으로 학습지수를 도출할 수 있다. 학습지수는 사용자의 학습 상태를 정량 적으로 파악하기 위한 지표를 의미하며, 학생 데이터 및 학습 상황에서 발생하는 데이터, 예를 들어, 학습 시간, 질문 수, 자리이탈 상태 등을 추가적으로 활용하여 보다 정밀한 산출이 가능할 수 있다. 추가적으로 감정 지수와 학습 지표 등은 데이터베이스로 구축될 수 있다. 데이터베이스는 감정 지수와 학습 지 표를 시계열 형태로 저장하여 종합적인 분석이 가능할 수 있다. 추가적으로 본 발명은 상기 가상 아바타의 표현 양상을 실시간으로 운영자에게 전달하고, 피드백을 제공하는 단 계를 더 포함할 수 있다. 도 5는 비대면 화상 교육의 프로세스를 예시적으로 도시한 도면이다. 도 5를 참조하면, 참여자가 화상 교육에 입장했을 때, 우선적으로 출석 인식 모듈이 작동하여 참석 여부를 판별할 수 있다. 회의 진행 중에는 감성지수, 학습지수, 집중지수, 자리이탈 상태를 실시간으로 판별하여 인식하고 아바타에 반영할 수 있다. 강사 및 운영진 은 UI로 도시된 아바타의 표시 상태를 바탕으로 사용자의 집중 상태 등을 즉각적으로 확인할 수 있으며 용이한 상호작용을 수행할 수 있다. 회의 종료시 사용자의 감정, 집중, 자리 이탈 상태에 대한 데이터와 감정지수, 집중지수, 학습(업무)지수 데이 터들이 데이터베이스에 저장된다. 이 데이터들을 그래프화 해서 주요 정보가 포함된 AI 리포트를 생성하고 교육 /회의 종료 시 바로 확인할 수 있도록 한다. 그래프의 예시는 도 12에 도시된다. 도 6은 본 발명에 따른 인공지능 로직이 감정, 자세, 집중에 대한 결과값을 도출하는 일 예를 도시한다. 도 6을 참조하면, 제어로직은사용자의 전자장치에서 제공된 이미지 데이터로부터 학습자의 얼굴을 인식한 뒤, 제1 뎁스 로 감성(감정) 판단, 얼굴 감지, 집중 상태 판단으로 구분할 수 있다. 전술한 감성인식 SDK 모듈에서 도출된 학 습자 감성상태 값 및 집중상태 값은 각기 감성 판단부, 집중 상태 판단부로 할당되고, 기초적인 얼굴 감지 데이 터는 얼굴 감지부에 대응될 수 있다. 감성 판단부에 대응되는 제2 뎁스 에서는 감성 판단부에 입력된 결과를 바탕으로 7가지 종류의 학습자 감성상태 중 어느하나를 도출할 수 있다. 이러한 감성 판단부는 딥러닝, 인공지능 등의 머신러닝(기계학습) 모듈을 통해 해당 결과를 도출할 수 있으며 특정 기술에 한정되지 않는다. 도 7은 본 발명에 따른 감정 및 집중 상태 도출 모델의 작동 프로세스를 구체적으로 나타낸 일 예를 도시한다. 도 7을 참조하면, 본 발명에 따른 감성인식 도출 모델은 다음의 세가지 세부 모델로 구성될 수 있다. 제1 감정 인식 모델은 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수 행하고 3가지 감정 유형(긍정, 부정, 중립) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감정지수가 도출될 수 있다. 제2 감정 인식 모델은 입력 받은 이미지로부터 얼굴 검출 후 전술한 ai 딥러닝 모듈을 적용하여 표정 분석을 수 행하고 7가지 감정 유형(즐거움, 놀람, 슬픔, 화남, 두려움, 불쾌함, 덤덤함) 각각의 확률값을 산출할 수 있다. 상기 확률값을 바탕으로 정량적 감정지수가 도출될 수 있다. 제1 감정 인식 모델과 제2 감정 인식 모델은 택일 적으로 적용되거나 양 모델이 상호보완적으로 적용되어 보다 정밀한 감정지수의 산출이 가능할 수 있다. 집중도 인식 모델은, 입력 이미지로부터 얼굴 검출 후 심박수와 심박변이도를 도출할 수 있다. 도출된 심박변이 도 데이터에 rPPG(원격 광혈류측정) 기술을 적용함으로써 심박수와 심박변이도를 측정하고 분석할 수 있다. 집 중도 인식 모델은 심박변이도의 분석 결과를 토대로 3단계 집중 상태(보통→집중 →몰입) 각각의 확률값이 산출 하여 이것을 집중 상태 지표를 도출할 수 있다. 집중 상태 지표를 바탕으로 집중 지수를 도출할 수 있다. 집중 상태 지표는 학습지수를 도출하기 위한 기초 수치로 활용될 수 있다. 최종적으로 감성인식 도출 모델은 전술한 제1 감정 인식 모델, 제2 감정 인식 모델, 집중도 인식 모델에서 도출 된 정량적 지표 값을 바탕으로 학습지수를 도출할 수 있다. 학습지수는 사용자의 학습 상태를 정량적으로 파악 하기 위한 지표를 의미하며, 학생 데이터 및 학습 상황에서 발생하는 데이터, 예를 들어, 학습 시간, 질문 수, 자리이탈 상태 등을 추가적으로 활용하여 보다 정밀한 산출이 가능할 수 있다. 도 8는 내지 도 10은 본 발명에서 따른 비대면 화상 방법이 사용자에게 제공되는 디스플레잉 화면의 일 예를 도 시한 도면이다. 하드웨어를 이용하여 본 발명의 실시 예를 구현하는 경우에는, 본 발명을 수행하도록 구성된 ASICs(application specific integrated circuits) 또는 DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays) 등이 본 발명의 프 로세서에 구비될 수 있다. 한편, 상술한 방법은, 컴퓨터에서 실행될 수 있는 프로그램으로 작성 가능하고, 컴퓨터 판독 가능 매체를 이용 하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 방법에서 사용된 데이 터의 구조는 컴퓨터 판독 가능한 저장 매체에 여러 수단을 통하여 기록될 수 있다. 본 발명의 다양한 방법들을 수행하기 위한 실행 가능한 컴퓨터 코드를 포함하는 저장 디바이스를 설명하기 위해 사용될 수 있는 프로그램 저장 디바이스들은, 반송파(carrier waves)나 신호들과 같이 일시적인 대상들은 포함하는 것으로 이해되지는 않 아야 한다. 상기 컴퓨터 판독 가능한 저장 매체는 마그네틱 저장매체(예를 들면, 롬, 플로피 디스크, 하드 디스 크 등), 광학적 판독 매체(예를 들면, 시디롬, DVD 등)와 같은 저장 매체를 포함한다. 이상에서 설명된 실시 예들은 본 발명의 구성요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성요소 또 는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성요소 또는 특징은 다른 구 성요소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명의 실시 예를 구성하는 것도 가능하다. 발명의 실시 예들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시 예의 일부 구성이나 특징은 다른 실시 예에 포함될 수 있고, 또는 다른 실시 예의 대응하는 구성 또 는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시 예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다. 본 발명이 본 발명의 기술적 사상 및 본질적인 특징을 벗어나지 않고 다른 형태로 구체화될 수 있음은 본 발명 이 속한 분야 통상의 기술자에게 명백할 것이다. 따라서, 상기 실시 예는 제한적인 것이 아니라 예시적인 모든 관점에서 고려되어야 한다. 본 발명의 권리범위는 첨부된 청구항의 합리적 해석 및 본 발명의 균등한 범위 내 가능한 모든 변화에 의하여 결정되어야 한다."}
{"patent_id": "10-2021-0194698", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 다양한 실시 예들에 따른 통신 시스템을 도시한다. 도 2는 본 발명의 다양한 실시 예들에 따른 감정 및 집중상태의 변화에 기반한 화상 전자 장치의 구성에 대한 블록도를 도시한다. 도 3은 본 발명의 다양한 실시 예들에 따른 서버의 구성에 대한 블록도를 도시한다. 도 4는 본 발명의 다양한 실시 예들에 따른 순서도를 도시한다. 도 5는 비대면 화상 교육의 프로세스를 예시적으로 도시한 도면이다. 도 6은 본 발명에 따른 인공지능 로직이 감정, 자세, 집중에 대한 결과값을 도출하는 일 예를 도시한다. 도 7은 본 발명에 따른 감정 및 집중 상태 도출 모델의 작동 프로세스를 구체적으로 나타낸 일 예를 도시한다. 도 8는 내지 도 10은 본 발명에서 따른 비대면 화상 방법이 사용자에게 제공되는 디스플레잉 화면의 일 예를 도 시한 도면이다."}
