{"patent_id": "10-2021-0002356", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0100206", "출원번호": "10-2021-0002356", "발명의 명칭": "비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 및 방법", "출원인": "계명대학교 산학협력단", "발명자": "이종하"}}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "감정인식 시스템(100)으로서,얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델(111)을 저장하고, 상기 제1 모델(111)을이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하는 제1 감정 인식부(110);비접촉식으로 측정된 ECG 데이터로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델(121)을 저장하고, 상기 제2 모델(121)을 이용해 상기 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력하는 제2 감정인식부(120); 및상기 제1 예측 감정과 상기 제2 예측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력하는 최종감정 인식부(130)를 포함하며,비접촉식으로 촬영된 상기 얼굴 이미지 및 비접촉식으로 측정된 상기 ECG 데이터를 통해 최종 예측 감정을 출력하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 저장하는 데이터베이스부(140)를 더 포함하는 것을특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 제1 감정 인식부(110)는, 상기 데이터베이스부(140)에 저장된 얼굴 이미지와 감정 레이블 쌍을 이용해 학습된 상기 제1 모델(111)을 저장하고,상기 제2 감정 인식부(120)는, 상기 데이터베이스부(140)에 저장된 ECG 데이터와 감정 레이블 쌍을 이용해 학습된 상기 제2 모델(121)을 저장하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 제1 모델(111)은,2D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제2 모델(121)은,1D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트공개특허 10-2022-0100206-3-워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 최종 감정 인식부(130)는,상기 제1 예측 감정과 제2 예측 감정을 예측확률 보팅 방식으로 결합하는 것을 특징으로 하는, 비접촉식 측정데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 제1 예측 감정과 상기 제2 예측 감정이 서로 다르면, 상기 제2 예측 감정에 기반하여 거짓 감정임을 추정하는 거짓 감정 추정부(150)를 더 포함하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 제1 예측 감정, 제2 예측 감정 및 최종 예측 감정은,행복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포를 포함하는 군에서 선택된 어느 하나인 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템(100)."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "컴퓨터로 구현되는 감정인식 시스템(100)에 의해 각 단계가 수행되는 감정인식 방법으로서,(1) 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델(111) 및 비접촉식으로 측정된 ECG 데이터로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델(121)을 저장하는 단계;(2) 상기 제1 모델(111)을 이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하고, 상기 제2 모델(121)을 이용해 상기 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력하는 단계; 및(3) 상기 제1 예측 감정과 상기 제2 예측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력하는 단계를 포함하며,비접촉식으로 촬영된 상기 얼굴 이미지 및 비접촉식으로 측정된 상기 ECG 데이터를 통해 최종 예측 감정을 출력하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 단계 (1) 이전에는,(0) 감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 데이터베이스부(140)에 저장하는 단계를 더 포함하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 단계 (1)에서는,상기 데이터베이스부(140)에 저장된 얼굴 이미지와 감정 레이블 쌍을 이용해 학습된 상기 제1 모델(111)을 저장공개특허 10-2022-0100206-4-하고, 상기 데이터베이스부(140)에 저장된 ECG 데이터와 감정 레이블 쌍을 이용해 학습된 상기 제2 모델(121)을저장하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서, 상기 제1 모델(111)은,2D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서, 상기 제2 모델(121)은,1D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제9항에 있어서, 상기 단계 (3)에서는,상기 제1 예측 감정과 제2 예측 감정을 예측확률 보팅 방식으로 결합하는 것을 특징으로 하는, 비접촉식 측정데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서, 상기 단계 (3) 이후에는,(4) 상기 제1 예측 감정과 상기 제2 예측 감정이 서로 다르면, 상기 제2 예측 감정에 기반하여 거짓 감정임을추정하는 단계를 더 포함하는 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항에 있어서, 상기 제1 예측 감정, 제2 예측 감정 및 최종 예측 감정은,행복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포를 포함하는 군에서 선택된 어느 하나인 것을 특징으로 하는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템에 관한 것으로서, 보 다 구체적으로는 감정인식 시스템으로서, 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델 을 저장하고, 상기 제1 모델을 이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하는 제1 (뒷면에 계속)"}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 감정인식 시스템 및 방법에 관한 것으로서, 보다 구체적으로는 비접촉식 측정 데이터를 통한 감정 예 측을 위한 인공지능 기반 감정인식 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "감정인식 기술이란, 인간의 감정을 측정하여 이를 분석함으로써 제품 개발, 환경 설계, 건강상태 예측 등에 적 용하여 인간의 삶의 질적 향상을 도모하는 기술로, 인간의 특성을 파악하려는 생체측정기술, 인간의 오감 센서 및 감정 처리 기술, 감정 디자인 기술, 마이크로 가공 기술, 및 사용성 평가나 가상현실 기술 등 인간의 삶과관련이 있는 기술이다. 한편, 질병 예방을 위한 생체신호 모니터링은 필수적이다. 생체신호를 이용하여 감정 상태 및 건강상태를 예측 하는 방법이 다양하게 연구되어 왔다. 특히, 기존에는 얼굴인식을 이용해 감정 상태를 예측하는 방법으로, 형 태학이나 텍스처 기반 특징들을 얼굴 이미지에서 추출하여 국소 이진화 패턴, 국소방향적 패턴, Gabor wavelet 등을 이용하여 얼굴을 인식하는 방법이 있다. 최근에는 컴퓨터 비전 분야에서 자동 객체 특징 추출과 분류 문 제에 CNN이나 RNN을 이용한 딥러닝 방식이 성공적으로 적용되었는바, 이러한 딥러닝 방식을 얼굴인식에도 활용 할 수 있다. 최근에는, 비접촉(Untact 또는 non-contact) 방식이 필수화됨에 따라 사용자와의 접촉 없이 생체신호를 습득하 여 감정 상태 및 건강상태를 예측하는 시스템의 개발이 필요한 실정이다. 한편, 본 발명과 관련된 선행기술로서, 등록특허 제10-2147052호(발명의 명칭: 얼굴 영상 기반의 감정 인식 시 스템 및 방법, 등록일자: 2020년 08월 17일) 등이 개시된 바 있다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 기존에 제안된 방법들의 상기와 같은 문제점들을 해결하기 위해 제안된 것으로서, 얼굴 이미지와 생 체신호 중 하나인 ECG 데이터를 이용해 딥러닝 알고리즘 기반으로 감정을 예측함으로써, 비접촉 방식으로 수집 할 수 있는 데이터만으로 정확하게 감정을 예측할 수 있으며, 감정 상태뿐만 아니라 건강상태 예측에도 활용할 수 있는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 및 방법을 제공하는 것을 그 목적으로 한다. 또한, 본 발명은, 얼굴 이미지를 이용해 감정을 분류하는 제1 모델과 ECG 데이터로 감정을 분류하는 제2 모델의 예측 감정을 보팅 방식으로 결합해 최종 예측 감정을 출력함으로써, 얼굴 이미지와 ECG 데이터 각각을 사용할 때보다 평균적으로 정확도 높은 감정 예측을 할 수 있으며, 두 모델의 예측 감정이 서로 다르면 ECG 데이터 기 반의 예측 감정에 기반하여 거짓 감정임을 추정함으로써, 표정을 감추거나 얼굴에 감정이 잘 드러나지 않는 사 람의 감정도 정확하게 예측할 수 있는, 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 및 방법을 제공하는 것을 또 다른 목적으로 한다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 목적을 달성하기 위한 본 발명의 특징에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템은, 감정인식 시스템으로서, 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델을 저장하고, 상기 제1 모델을 이용해 사 람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하는 제1 감정 인식부; 비접촉식으로 측정된 ECG 데이터로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델을 저장하고, 상기 제 2 모델을 이용해 상기 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력하는 제2 감정 인식부; 및 상기 제1 예측 감정과 상기 제2 예측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력하는 최종 감정 인식부를 포함하며, 비접촉식으로 촬영된 상기 얼굴 이미지 및 비접촉식으로 측정된 상기 ECG 데이터를 통해 최종 예측 감정을 출력 하는 것을 그 구성상의 특징으로 한다.바람직하게는, 감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 저장하는 데이터베이스부를 더 포함할 수 있다. 더욱 바람직하게는, 상기 제1 감정 인식부는, 상기 데이터베이스부에 저장된 얼굴 이미지와 감정 레이블 쌍을 이용해 학습된 상기 제1 모델을 저장하고, 상기 제2 감정 인식부는, 상기 데이터베이스부에 저장된 ECG 데이터와 감정 레이블 쌍을 이용해 학습된 상기 제 2 모델을 저장할 수 있다. 바람직하게는, 상기 제1 모델은, 2D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트 워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것일 수 있다. 바람직하게는, 상기 제2 모델은, 1D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트 워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것일 수 있다. 바람직하게는, 상기 최종 감정 인식부는, 상기 제1 예측 감정과 제2 예측 감정을 예측확률 보팅 방식으로 결합할 수 있다. 바람직하게는, 상기 제1 예측 감정과 상기 제2 예측 감정이 서로 다르면, 상기 제2 예측 감정에 기반하여 거짓 감정임을 추정 하는 거짓 감정 추정부를 더 포함할 수 있다. 바람직하게는, 상기 제1 예측 감정, 제2 예측 감정 및 최종 예측 감정은, 행복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포를 포함하는 군에서 선택된 어느 하나일 수 있다. 또한, 상기한 목적을 달성하기 위한 본 발명의 특징에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인 공지능 기반 감정인식 방법은, 컴퓨터로 구현되는 감정인식 시스템에 의해 각 단계가 수행되는 감정인식 방법으로서, 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델 및 비접촉식으로 측정된 ECG 데이터 로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델을 저장하는 단계; 상기 제1 모델을 이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하고, 상기 제2 모 델을 이용해 상기 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력하는 단계; 및 상기 제1 예측 감정과 상기 제2 예측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력하는 단 계를 포함하며, 비접촉식으로 촬영된 상기 얼굴 이미지 및 비접촉식으로 측정된 상기 ECG 데이터를 통해 최종 예측 감정을 출력 하는 것을 그 구성상의 특징으로 한다.바람직하게는, 상기 단계 이전에는, 감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 데이터베이스부에 저장하는 단계를 더 포함할 수 있다. 더욱 바람직하게는, 상기 단계 에서는, 상기 데이터베이스부에 저장된 얼굴 이미지와 감정 레이블 쌍을 이용해 학습된 상기 제1 모델을 저장하고, 상기 데이터베이스부에 저장된 ECG 데이터와 감정 레이블 쌍을 이용해 학습된 상기 제2 모델을 저장할 수 있다. 바람직하게는, 상기 제1 모델은, 2D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트 워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것일 수 있다. 바람직하게는, 상기 제2 모델은, 1D 콘볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트 워크(Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것일 수 있다. 바람직하게는, 상기 단계 에서는, 상기 제1 예측 감정과 제2 예측 감정을 예측확률 보팅 방식으로 결합할 수 있다. 바람직하게는, 상기 단계 이후에는, 상기 제1 예측 감정과 상기 제2 예측 감정이 서로 다르면, 상기 제2 예측 감정에 기반하여 거짓 감정임을 추정하는 단계를 더 포함할 수 있다. 바람직하게는, 상기 제1 예측 감정, 제2 예측 감정 및 최종 예측 감정은, 행복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포를 포함하는 군에서 선택된 어느 하나일 수 있다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에서 제안하고 있는 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 및 방법에 따르면, 얼굴 이미지와 생체신호 중 하나인 ECG 데이터를 이용해 딥러닝 알고리즘 기반으로 감정을 예측 함으로써, 비접촉 방식으로 수집할 수 있는 데이터만으로 정확하게 감정을 예측할 수 있으며, 감정 상태뿐만 아 니라 건강상태 예측에도 활용할 수 있다. 또한, 본 발명에서 제안하고 있는 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스 템 및 방법에 따르면, 얼굴 이미지를 이용해 감정을 분류하는 제1 모델과 ECG 데이터로 감정을 분류하는 제2 모 델의 예측 감정을 보팅 방식으로 결합해 최종 예측 감정을 출력함으로써, 얼굴 이미지와 ECG 데이터 각각을 사 용할 때보다 평균적으로 정확도 높은 감정 예측을 할 수 있으며, 두 모델의 예측 감정이 서로 다르면 ECG 데이 터 기반의 예측 감정에 기반하여 거짓 감정임을 추정함으로써, 표정을 감추거나 얼굴에 감정이 잘 드러나지 않 는 사람의 감정도 정확하게 예측할 수 있다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자가 본 발명을 용이하게 실 시할 수 있도록 바람직한 실시예를 상세히 설명한다. 다만, 본 발명의 바람직한 실시예를 상세하게 설명함에 있어, 관련된 공지 기능 또는 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단 되는 경우에는 그 상세한 설명을 생략한다. 또한, 유사한 기능 및 작용을 하는 부분에 대해서는 도면 전체에 걸쳐 동일한 부호를 사용한다. 덧붙여, 명세서 전체에서, 어떤 부분이 다른 부분과 ‘연결’ 되어 있다고 할 때, 이는 ‘직접적으로 연결’ 되 어 있는 경우뿐만 아니라, 그 중간에 다른 소자를 사이에 두고 ‘간접적으로 연결’ 되어 있는 경우도 포함한다. 또한, 어떤 구성요소를 ‘포함’ 한다는 것은, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제 외하는 것이 아니라 다른 구성요소를 더 포함할 수 있다는 것을 의미한다. 도 1은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템을 포함하는 전체 시스템 구성을 도시한 도면이다. 도 1에 도시된 바와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템은, 비접촉식 측정 데 이터로서 카메라에서 촬영되는 얼굴 이미지와 비접촉식 ECG 측정 장치에서 측정되는 ECG 데이터를 이용 하며, 딥러닝 알고리즘 기반으로 사람의 감정을 예측할 수 있다. 즉, 본 발명에서는, 감정인식을 위해 얼굴 이미지와 ECG 데이터를 사용하는데, 그중에서 얼굴 이미지는 카메라 에 의해 촬영되며, 여기서 카메라는 CCTV 카메라, 웹캠, 휴대전화 카메라 등 다양할 수 있다. 카메라 는 비접촉식으로 촬영되므로, 얼굴 이미지는 비접촉식 측정 데이터에 해당한다. 한편, ECG 데이터는 비접촉식 ECG 측정 장치에 의해 비접촉식으로 측정될 수 있다. ECG(electrocardiogram, 심전도)는 심장 활동에 의해 발생하는 전압 신호로서, ECG 신호의 파형을 분석하면 심 장의 이상 유무를 알 수 있으며 심전도 신호의 R파 간격을 통하여 사람의 심박동을 계산할 수 있다. 또한, 심박동은 자율 신경계에 의해 조절되는 것이라 감정이나 흥분 등의 다양한 상태를 진단하는 것이 가능하다. 그런 데, 종래의 심전도 검사는 환자가 병원에 내방하여 진행되는 것이어서, 환자가 검사를 진행할 당시 이상이 있는 경우에만 정확한 결과를 판단할 수 있었다. 그러나, 심장의 이상 발생은 언제 발생할지 예측할 수 없으므로, 병원에 내방하여 실시하는 검사는 문제가 발생할 때의 원인에 대하여 정확한 결과를 기대하기는 어렵다는 문제 점이 있다. 그에 따라, 심전도 측정이 병원뿐 아니라 환자가 쉽게 찾을 수 있는 장소에서 상시로 이루어져야 할 필요가 대두되었다. 그리고, 종래의 심전도 측정 방법은 은/염화은 등으로 이루어진 금속 전극을 환자의 몸 에 직접 부착하여 측정하는 방법이므로, 번거로울 뿐 아니라 환자에게 불쾌감을 야기할 수 있다. 이러한 필요 에 따라 최근에는, 기술의 발전으로 인해 환자의 몸에 직접적인 전극의 부착 없이 의자, 좌석 등에서도 비접촉 식으로 심전도 신호를 측정하는 것이 가능해졌다. 따라서 ECG 데이터를 더 쉽고 상시로 측정할 수 있게 되었다. 도 2는 비접촉식 ECG 측정 장치를 예를 들어 도시한 도면이다. 도 2에 도시된 바와 같이, 의자, 배낭, 침 대 등 사람의 등이 닿는 부분에 ECG 측정 센서를 구비한 비접촉식 ECG 측정 장치를 이용해 비접촉식으로 심 전도 데이터를 수집할 수 있다. 다만, 본 발명에서 비접촉식이란 사람의 신체에 직접적인 접촉이 없다는 좁은 의미일 수 있으나, 의료진 등 사람과 사람이 서로 대면할 필요가 없다는 넓은 의미일 수도 있다. 따라서 도 2 의 마지막에 도시된 바와 같이 스마트 워치와 같은 개인용 건강 디바이스도 넓은 의미의 비접촉식 ECG 측정 장 치의 역할을 할 수 있다. 도 3은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템이 비접촉식 측정 데이터를 수집하는 모습을 예를 들어 도시한 도면이다. 도 3에 도시된 바와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 은, 의자에 앉아있는 사람의 비접촉식 측정 데이터를 수집할 수 있는데, 카메라를 통해 비접촉식으로 촬영된 얼굴 이미지와 의자 형태로 구현된 비접촉식 ECG 측정 장치를 통해 비접촉식으로 측정된 ECG 데이터 를 동시에 수집할 수 있다. 한편, 감정인식 시스템은, 네트워크를 통해 카메라와 비접촉식 ECG 측정 장치로부터 데이터를 수 신하여 감정인식에 사용할 수 있다. 여기서, 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신 망(Wide Area Network; WAN) 또는 부가가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이 동 통신망(mobile radio communication network), 위성 통신망, 블루투스(Bluetooth), Wibro(Wireless Broadband Internet), HSDPA(High Speed Downlink Packet Access), LTE(Long Term Evolution), 5G(5th Generation Mobile Telecommunication) 등과 같은 모든 종류의 무선 네트워크로 구현될 수 있다. 또한, 감정인식 시스템은 컴퓨터 등 전자 장치로 구현될 수 있다. 보다 구체적으로 전자 장치는 스마트폰, 태블릿(tablet) PC(personal computer), 이동 전화기, 영상 전화기, 전자책 리더기, 데스크탑 (desktop) PC, 랩탑(laptop) PC, 넷북(netbook) 컴퓨터, 워크스테이션(workstation), 서버(server), PDA(personal digital assistant), 미디어 박스, 게임 콘솔, 전자사전 또는 웨어러블 장치(wearable device) 중 적어도 하나를 포함할 수 있다. 다양한 실시예들에서, 전자 장치는 전술한 기기들에 한정되지는 않으며, 전 술한 다양한 장치 중 둘 이상의 조합일 수 있다. 도 4는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템의 구성을 도시한 도면이다. 도 4에 도시된 바와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데 이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템은, 제1 감정 인식부, 제2 감정 인식부 및 최종 감정 인식부를 포함하여 구성될 수 있으며, 데이터베이스부 및 거짓 감정 추정부(15 0)를 더 포함하여 구성될 수 있다. 도 5는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템의 감정인식 과정을 도식화하여 나타낸 도면이다. 이하에서는, 도 4 및 도 5를 참조하여 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템의 각 구 성에 대해 상세히 설명하도록 한다. 제1 감정 인식부는, 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델을 저장하 고, 제1 모델을 이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력할 수 있다. 여기 서, 제1 모델은 신경망 기반의 딥러닝 모델일 수 있으며, 얼굴 이미지를 미리 정해진 개수의 클래스의 감 정으로 분류하도록 학습된 것일 수 있다. 보다 구체적으로, 도 5에 도시된 바와 같이, 제1 모델은, 2D 콘 볼루셔널 레이어(Convolutional layer) 및 풀링 레이어(Pooling layer)를 포함하는 콘볼루셔널 뉴럴 네트워크 (Convolutional Neural Network, CNN) 모델을 기반으로 학습된 것으로, 덴스 레이어(Dense layer), 출력 레이 어(Output layer)를 더 포함하여 구성될 수 있다. 제2 감정 인식부는, 비접촉식으로 측정된 ECG 데이터로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델을 저장하고, 제2 모델을 이용해 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력 할 수 있다. 여기서, 제2 모델은 신경망 기반의 딥러닝 모델일 수 있으며, 1D 콘볼루셔널 레이어 및 풀링 레이어를 포함하는 콘볼루셔널 뉴럴 네트워크 모델을 기반으로 학습된 것으로, 덴스 레이어, 출력 레이어를 더 포함하여 구성될 수 있다. 실시예에 따라서는, ECG 데이터와 같은 시퀀셜 데이터에 적용할 수 있도록 디자인된 RNN(Recurrent Neural Network), LSTM(Long short-term memory) RNN 등을 기반으로 학습하여 제2 모델을 생성할 수도 있다. 도 6은 인공지능 모델의 구조를 개략적으로 도시한 도면이다. 도 6에 도시된 바와 같이, 제1 모델 및 제2 모델은 입력층, 은닉층, 출력층으로 구성되는 다중 퍼셉트론 기반의 신경망 구조일 수 있다. 즉, 제1 모 델 및 제2 모델은, 대량의 학습 데이터를 이용해 제1 예측 감정 및 제2 예측 감정을 각각 출력하도록 생성될 수 있다. 최종 감정 인식부는, 제1 예측 감정과 제2 예측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력할 수 있다. 보다 구체적으로, 최종 감정 인식부는, 제1 예측 감정과 제2 예측 감정을 예측확률 보팅 방식으로 결합할 수 있다. 여기서, 예측확률 보팅은, 둘 이상의 딥러닝 모델을 결합한 앙상블 모델을 생성하는 방법으로써, 소프트 보팅을 의미할 수 있다. 소프트 보팅은, 하드 보팅과 다르게 각 레이블의 예측확률의 평균으로 최종 분류를 진행하는 방법이다. 예측확 률의 평균으로 분류하기 때문에 다수결 방식으로 투표하는 하드 보팅과 결과가 다르게 나올 수 있다. 종래 소 프트 보팅을 이용한 앙상블에서는, 같은 학습 데이터를 이용해 복수의 서로 다른 모델을 학습시키고, 복수 모델 들의 예측확률을 토대로 최종 결과를 출력한다. 반면에, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서는, 비접촉식으로 촬영된 얼굴 이미지 및 비접촉식으로 측정된 ECG 데이터로 각각 예측한 결과를 앙상블하여 최종 예측 감정을 출력할 수 있다. 즉, 최종 감정 인식부는, 전혀 다른 종류의 데이터를 학습 데이터로 사용해 학습된 제1 모델과 제2 모델의 예측 결과를 소프트 보팅으로 결합하는 방식을 사용 할 수 있다. 데이터베이스부는, 감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 저장할 수 있다. 여기서, 얼굴 이미지와 ECG 데이터는 도 3에 도시된 바와 같이, 카메라 및 비접촉식 ECG 측정 장치에서 측정된 데이터를 수신한 것으로, 전처리 과정에 의해 각각 감정 클래스가 레이블 된 것일 수 있다. 여기서, 감정 레이 블은, 행복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포의 7개 클래스로 레이블 될 수 있다.제1 감정 인식부는, 데이터베이스부에 저장된 얼굴 이미지와 감정 레이블 쌍을 이용해 학습된 제1 모 델을 저장하고, 제2 감정 인식부는, 데이터베이스부에 저장된 ECG 데이터와 감정 레이블 쌍을 이용해 학습된 제2 모델을 저장할 수 있다. 도 7은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 제1 모델 및 제2 모델을 학습하는 과정을 설명하기 위해 도시한 도면이다. 도 7에 도시된 바와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템에서는, 얼굴 이미지, ECG 데이터, 감정 레이블이 한 세트의 데이터로 구성되어, 제1 모델 의 학습 시에는 얼굴 이미지와 감정 레이블, 제2 모델의 학습 시에는 ECG 데이터와 감정 레이블을 각 각 학습 데이터로 사용할 수 있다. 실시예에 따라서, 데이터베이스부는, 제1 모델의 학습을 위한 감정 클래스가 레이블 된 얼굴 이미지, 제2 모델의 학습을 위한 감정 클래스가 레이블 된 ECG 데이터를 각각 별도의 학습 데이터셋으로 저장하고 학습에 각각 사용할 수 있다. 예를 들어, 도 3에 도시된 바와 같이, 특정 사람으로부터 얼굴 이미지와 ECG 데 이터를 모두 수집할 수도 있으나, 별개로 수집되어 데이터베이스화될 수 있으므로, 별도의 데이터를 이용해 제1 모델 및 제2 모델을 각각 학습할 수 있다. 다만, 전술한 바와 같은 내용은 학습 과정에 대한 것으로 서, 예측 시에는 특정 사람의 감정을 인식해야 하므로, 도 3에 도시된 바와 같이 특정 사람에 대해 동시에 수집 된 얼굴 이미지와 ECG 데이터를 이용해 감정을 인식할 수 있다. 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 에 의한 감정인식 실험으로서, 얼굴 이미지로 된 학습 데이터를 이용해 제1 모델을 생성하고 검증하 였다. 딥러닝 학습을 위한 데이터베이스는 FER 2013으로 7개 감정으로 분류된 48×48 그레이 스케일 이미지 총 35,685 장이며, 각 이미지는 행복, 중립, 슬픔, 분노, 놀라움, 혐오와 공포의 총 7개 클래스의 감정으로 레이블 되어 있다. 총 35,685장 중에서, 학습 데이터셋으로 28,709장, 검증 데이터셋으로 3,589장을 사용하였다. 학습 데이터셋을 이용해 도 5에 도시된 바와 같은 CNN을 학습시켜 제1 모델을 생성하였다. 학습 데이터를 증가시키고 모델의 일반화 성능을 높이기 위해 매 학습마다 좌우대칭, 밝기 변화 10%, 셔플의 데이터 증강(Data augmentation) 방법을 적용해 과적합(over fitting)을 방지하였다. 파라미터 학습을 위한 최적화는 adam optimization 기법을 사용하였고, 초기 학습률은 0.0001로 설정하였다. 타이탄 XP GPU 4대를 병렬로 하여 미니 배치 256으로 총 500번의 학습을 수행하였으며, 매 에폭마다 검증 데이터로 모델의 성능을 검증하였다. 모델의 성능을 측정하기 위한 검증은 전체 데이터 개수 중 올바르게 예측한 개수를 계산하여 Accuracy로 분류 정확도를 측정하였다. 검증 시 이전 검증보다 Accuracy가 높은 구간에서 모델과 가중치 파라미터를 저장하였다. 테스트 시 모델의 Accuracy가 가장 높은 구간의 파라미터를 사용하여 실제 테스트에 사용하였다. 딥러닝 개발 프레임 워크로는 pytorch 1.5.1을 사용하였다. 학습에 의해 생성된 제1 모델의 성능을 실험하기 위한 테스트 데 이터로는, 학습 및 검증에 사용되지 않은 나머지 3,589장을 사용하였다. 도 8은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 제1 모델의 감정인식 정확도를 나타낸 도면이다. 도 8에는, 얼굴 이미지 데이터만을 사용 하여 학습한 CNN 모델(제1 모델)의 성능을 측정한 결과와 데이터 증강(data augmentation) 방법을 함께 적 용하여 모델의 성능을 측정한 결과를 나타낸다. 즉, 데이터 증강 방법을 적용함으로써 제1 모델을 학습하 는데 더 다양하고 많은 데이터를 사용할 수 있으므로, 성능 향상에 도움이 된다는 것을 확인할 수 있다.전술한 바와 같이 학습된 제1 모델 및 제2 모델과 최초 감정 인식부를 이용해, 감정인식이 필요한 특 정 사람의 감정을 예측해 최종 예측 감정을 출력할 수 있다. 도 9는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 최종 예측 감정을 출력하는 과정을 설명하기 위해 도시한 도면이다. 도 5 및 도 9에 도시된 바 와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템에서는, 도 3에 도시된 바와 같이 특정 사람에 대해 동시에 수집된 얼굴 이미지와 ECG 데이터를 이 용해 감정을 인식하는데, 제1 감정 인식부는 얼굴 이미지를 제1 모델에 입력해 제1 예측 감정을 출력 하고, 제2 감정 인식부는 비접촉식 ECG 데이터를 제2 모델에 입력해 제2 예측 감정을 출력할 수 있다. 최종 감정 인식부는 제1 예측 감정과 제2 예측 감정을 이용해 예측확률 보팅 방식으로 결합해 앙상 블 결과인 최종 예측 감정을 출력할 수 있다. 여기서, 제1 예측 감정, 제2 예측 감정 및 최종 예측 감정은, 행 복, 중립, 슬픔, 분노, 놀라움, 혐오 및 공포를 포함하는 군에서 선택된 어느 하나일 수 있다. 기존 CNN 기반의 감정인식은 얼굴 이미지에 기반하여 표정을 인식하기 때문에 실제 감정이 아닌 얼굴 표정에 따 라 간접적으로 감정을 인식한다. 그러나 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템에서는, ECG 생체신호를 직접적으로 분석하여 감정을 예측하기 때문에 얼굴 표정만으로 연출된 거짓 감정인 상황에서의 감정인식 가능성을 보여준다. 거짓 감정 추정부는, 제1 예측 감정과 제2 예측 감정이 서로 다르면, 제2 예측 감정에 기반하여 거짓 감정 임을 추정할 수 있다. 즉, 거짓 감정 추정부를 포함함으로써, 얼굴 이미지에 따른 예측 결과와 ECG 데이 터에 따른 예측 결과가 다르면, ECG 데이터 분석 결과에 기반하여 거짓된 감정이라고 예측할 수 있다. 도 10은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방 법의 흐름을 도시한 도면이다. 도 10에 도시된 바와 같이, 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방법은, 컴퓨터로 구현되는 감정인식 시스템에 의해 각 단 계가 수행되는 감정인식 방법으로서, 얼굴 이미지로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제1 모델 및 ECG 데이터로부터 감정을 인식하는 딥러닝 알고리즘 기반의 제2 모델을 저장하는 단계(S110), 제 1 모델을 이용해 사람의 얼굴 이미지로부터 감정을 분류해 제1 예측 감정을 출력하고, 제2 모델을 이 용해 사람의 ECG 데이터로부터 감정을 분류해 제2 예측 감정을 출력하는 단계(S120) 및 제1 예측 감정과 제2 예 측 감정을 보팅(Voting) 방식으로 결합해, 최종 예측 감정을 출력하는 단계(S130)를 포함하여 구현될 수 있으며, 감정인식을 위한 얼굴 이미지, ECG 데이터 및 감정 레이블을 데이터베이스부에 저장하는 단계 (S10), 및 제1 예측 감정과 제2 예측 감정이 서로 다르면, 제2 예측 감정에 기반하여 거짓 감정임을 추정하는 단계(S140)를 더 포함하여 구현될 수 있다. 각각의 단계들과 관련된 상세한 내용들은, 앞서 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템과 관련하여 충분히 설명되었으므로, 상세한 설명은 생략하기 로 한다. 전술한 바와 같이, 본 발명에서 제안하고 있는 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스템 및 방법에 따르면, 얼굴 이미지와 생체신호 중 하나인 ECG 데이터를 이용해 딥러닝 알고 리즘 기반으로 감정을 예측함으로써, 비접촉 방식으로 수집할 수 있는 데이터만으로 정확하게 감정을 예측할 수 있으며, 감정 상태뿐만 아니라 건강상태 예측에도 활용할 수 있다.또한, 본 발명에서 제안하고 있는 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시스 템 및 방법에 따르면, 얼굴 이미지를 이용해 감정을 분류하는 제1 모델과 ECG 데이터로 감정을 분류 하는 제2 모델의 예측 감정을 보팅 방식으로 결합해 최종 예측 감정을 출력함으로써, 얼굴 이미지와 ECG 데이터 각각을 사용할 때보다 평균적으로 정확도 높은 감정 예측을 할 수 있으며, 두 모델의 예측 감정이 서로 다르면 ECG 데이터 기반의 예측 감정에 기반하여 거짓 감정임을 추정함으로써, 표정을 감추거나 얼굴에 감정이 잘 드러나지 않는 사람의 감정도 정확하게 예측할 수 있다. 한편, 본 발명은 다양한 통신 단말기로 구현되는 동작을 수행하기 위한 프로그램 명령을 포함하는 컴퓨터에서 판독 가능한 매체를 포함할 수 있다. 예를 들어, 컴퓨터에서 판독 가능한 매체는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD_ROM, DVD와 같은 광기록 매체(optical media), 플롭티 컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media) 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 이와 같은 컴퓨터에서 판독 가능한 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합 하여 포함할 수 있다. 이때, 컴퓨터에서 판독 가능한 매체에 기록되는 프로그램 명령은 본 발명을 구현하기 위 하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 예를 들어, 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다."}
{"patent_id": "10-2021-0002356", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상 설명한 본 발명은 본 발명이 속한 기술분야에서 통상의 지식을 가진 자에 의하여 다양한 변형이나 응용이 가능하며, 본 발명에 따른 기술적 사상의 범위는 아래의 특허청구범위에 의하여 정해져야 할 것이다."}
{"patent_id": "10-2021-0002356", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템을 포함하는 전체 시스템 구성을 도시한 도면. 도 2는 비접촉식 ECG 측정 장치를 예를 들어 도시한 도면. 도 3은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템이 비접촉식 측정 데이터를 수집하는 모습을 예를 들어 도시한 도면. 도 4는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템의 구성을 도시한 도면. 도 5는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템의 감정인식 과정을 도식화하여 나타낸 도면. 도 6은 인공지능 모델의 구조를 개략적으로 도시한 도면. 도 7은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 제1 모델 및 제2 모델을 학습하는 과정을 설명하기 위해 도시한 도면. 도 8은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 제1 모델의 감정인식 정확도를 나타낸 도면. 도 9는 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 시 스템에서, 최종 예측 감정을 출력하는 과정을 설명하기 위해 도시한 도면. 도 10은 본 발명의 일실시예에 따른 비접촉식 측정 데이터를 통한 감정 예측을 위한 인공지능 기반 감정인식 방 법의 흐름을 도시한 도면."}
