{"patent_id": "10-2023-0126110", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0042966", "출원번호": "10-2023-0126110", "발명의 명칭": "트랜스포머 기반의 요약문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그", "출원인": "인천대학교 산학협력단", "발명자": "신유현"}}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사전 설정된 복수의 훈련용 텍스트들 - 상기 복수의 훈련용 텍스트들 각각은, 하나 이상의 문장으로 구성된 텍스트임 - 과, 상기 복수의 훈련용 텍스트들 각각에 대응되는 정답 레이블로서 사전 지정된 서로 다른 요약문이저장되어 있는 훈련 데이터 저장부;상기 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈련용 텍스트를 선정하여, 상기 선정된 훈련용 텍스트와 그에 대응되는 요약문을 기초로, 트랜스포머(Transformer) 기반의 신경망모델을 학습시키는 과정을, 상기 복수의 훈련용 텍스트들 각각에 대해 반복 수행함으로써, 요약문 생성 모델을생성하는 모델 생성부; 및상기 요약문 생성 모델의 생성이 완료된 이후에, 사용자에 의해, 제1 텍스트가 입력으로 인가되면서 요약문 생성 명령이 인가되면, 상기 요약문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성하는 요약문 생성부를 포함하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 모델 생성부는상기 복수의 훈련용 텍스트들 중 어느 하나인 제1 훈련용 텍스트가 선정됨에 따라, 상기 제1 훈련용 텍스트에대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, n(n은 2이상의 자연수임)개의서로 다른 토크나이저(Tokenizer)들 - 상기 n개의 토크나이저들 각각은, n개의 서로 다른 BERT(BidirectionalEncoder Representations from Transformers) 기반의 사전 훈련된 언어 모델(Pre-trained Language Model:PLM)들을 기초로 구성된 토크나이저임 - 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를 생성하는 토큰 생성부;상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를, n개의 인코더들 - 상기 n개의 인코더들 각각은, 트랜스포머 모델을 구성하는 인코더를 의미함 - 각각에 하나씩 입력으로 인가함으로써, n개의 인코더 출력들을 생성하는 인코더 출력부; 및상기 n개의 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 예측 요약문을 생성한 후, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행하는 학습 수행부를 포함하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 학습 수행부는상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들을 서로 연접(Concatenation)하여 하나의 단일인코더 출력으로 구성한 후, 상기 단일 인코더 출력을, 트랜스포머 모델을 구성하는 하나의 단일 디코더에 입력시퀀스로서 인가하되, 상기 단일 디코더가, 상기 단일 인코더 출력을 기반으로 인코더-디코더 어텐션(Attention)을 수행하도록 처리함으로써, 상기 단일 디코더로부터 산출되는 출력을 상기 예측 요약문으로 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는요약문 간의 오차가 최소가 되도록 학습을 수행하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2025-0042966-3-제2항에 있어서,상기 학습 수행부는상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들 각각을, n개의 디코더들 - 상기 n개의 디코더들 각각은, 트랜스포머 모델을 구성하는 디코더를 의미함 - 각각에 하나씩 입력 시퀀스로서 인가하되, 상기 n개의 디코더들 각각이, 각 디코더에 인가되는 인코더 출력을 기반으로 인코더-디코더 어텐션을 수행하도록 처리함으로써, 상기 n개의 디코더들 각각에 대한 출력을 산출한 후, 상기 n개의 디코더들 각각에 대한 출력을 하나의디코더 출력으로 결합함으로써, 상기 예측 요약문을 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 n개의 서로 다른 BERT 기반의 PLM들은, M-BERT, KoBERT, HanBERT 및 KorBERT 기반의 PLM인 것을 특징으로하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,상기 요약문 생성부는상기 요약문 생성 모델의 생성이 완료된 이후에, 상기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서상기 요약문 생성 명령이 인가되면, 상기 제1 텍스트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를 생성하고, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 상기 n개의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1인코더 출력들을 생성한 후, 상기 n개의 제1 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 상기 제1 텍스트에 대응되는 요약문을 생성하는 것을 특징으로 하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 모델 생성부를 통해, 상기 요약문 생성 모델의 생성이 완료된 이후에, 상기 사용자로부터 상기 요약문 생성 모델의 이용을 위한 비밀번호 설정 명령이 인가되면, p(p는 2이상의 자연수임)자리수의 제1 번호와 q(q는 2이상의 자연수임)자리수의 제2 번호를 랜덤하게 생성하고, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로(modulo)-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을성분으로 갖는 벡터 간의 합성곱을 수행하여 결과 벡터를 산출한 후, 상기 결과 벡터의 해밍 무게를 사전 설정된 해시 함수에 입력으로 인가하였을 때 산출되는 해시 값을, 사용자 인증 값으로 지정하여, 사전 지정된 클라우드 저장 서버에 저장하는 저장 처리부;상기 제1 번호와 상기 제2 번호를 서로 연접한 번호를, 상기 요약문 생성 모델의 이용을 위한 제1 비밀번호로서생성한 후, 상기 제1 비밀번호를 화면 상에 표시하면서, 상기 제1 비밀번호를 숙지할 것을 지시하는 메시지를화면 상에 표시하는 표시부;상기 제1 비밀번호와 상기 메시지가 화면 상에 표시된 이후, 상기 사용자로부터 상기 요약문 생성 모델의 이용을 위한 인증 요청 명령이 인가되면, 화면 상에, 인증을 위한 비밀번호를 입력할 것을 지시하는 메시지를 표시한 후, 상기 사용자에 의해, 상기 제1 비밀번호가 입력되면, 상기 제1 비밀번호로부터 상기 제1 번호와 상기 제2 번호를 분리한 후, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수행하여 상기 결과 벡터를 산출한 다음, 상기 결과 벡터의 해밍 무게를 상기 해시 함수에 인가하여 상기 해시 값을 연산한후, 상기 해시 값이 상기 클라우드 저장 서버에 저장되어 있는 상기 사용자 인증 값과 일치하는지 확인하여, 서로 일치하는 것으로 확인되면, 상기 사용자에 대한 인증을 완료 처리하는 인증 완료부; 및상기 사용자에 대한 인증이 완료 처리되면, 화면 상에, 인증이 완료되었음을 지시하는 메시지를 표시함과 동시공개특허 10-2025-0042966-4-에, 상기 요약문 생성 모델에 입력으로 인가될 텍스트를 입력할 것을 지시하는 안내 메시지를 표시하는 안내 메시지 표시부를 더 포함하는 전자 장치."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "사전 설정된 복수의 훈련용 텍스트들 - 상기 복수의 훈련용 텍스트들 각각은, 하나 이상의 문장으로 구성된 텍스트임 - 과, 상기 복수의 훈련용 텍스트들 각각에 대응되는 정답 레이블로서 사전 지정된 서로 다른 요약문이저장되어 있는 훈련 데이터 저장부를 유지하는 단계;상기 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈련용 텍스트를 선정하여, 상기 선정된 훈련용 텍스트와 그에 대응되는 요약문을 기초로, 트랜스포머(Transformer) 기반의 신경망모델을 학습시키는 과정을, 상기 복수의 훈련용 텍스트들 각각에 대해 반복 수행함으로써, 요약문 생성 모델을생성하는 단계; 및상기 요약문 생성 모델의 생성이 완료된 이후에, 사용자에 의해, 제1 텍스트가 입력으로 인가되면서 요약문 생성 명령이 인가되면, 상기 요약문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성하는 단계를 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 요약문 생성 모델을 생성하는 단계는상기 복수의 훈련용 텍스트들 중 어느 하나인 제1 훈련용 텍스트가 선정됨에 따라, 상기 제1 훈련용 텍스트에대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, n(n은 2이상의 자연수임)개의서로 다른 토크나이저(Tokenizer)들 - 상기 n개의 토크나이저들 각각은, n개의 서로 다른 BERT(BidirectionalEncoder Representations from Transformers) 기반의 사전 훈련된 언어 모델(Pre-trained Language Model:PLM)들을 기초로 구성된 토크나이저임 - 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를 생성하는 단계;상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를, n개의 인코더들 - 상기 n개의 인코더들 각각은, 트랜스포머 모델을 구성하는 인코더를 의미함 - 각각에 하나씩 입력으로 인가함으로써, n개의 인코더 출력들을 생성하는 단계; 및상기 n개의 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 예측 요약문을 생성한 후, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행하는 단계를 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 학습을 수행하는 단계는상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들을 서로 연접(Concatenation)하여 하나의 단일인코더 출력으로 구성한 후, 상기 단일 인코더 출력을, 트랜스포머 모델을 구성하는 하나의 단일 디코더에 입력시퀀스로서 인가하되, 상기 단일 디코더가, 상기 단일 인코더 출력을 기반으로 인코더-디코더 어텐션(Attention)을 수행하도록 처리함으로써, 상기 단일 디코더로부터 산출되는 출력을 상기 예측 요약문으로 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는요약문 간의 오차가 최소가 되도록 학습을 수행하는 것을 특징으로 하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,공개특허 10-2025-0042966-5-상기 학습을 수행하는 단계는상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들 각각을, n개의 디코더들 - 상기 n개의 디코더들 각각은, 트랜스포머 모델을 구성하는 디코더를 의미함 - 각각에 하나씩 입력 시퀀스로서 인가하되, 상기 n개의 디코더들 각각이, 각 디코더에 인가되는 인코더 출력을 기반으로 인코더-디코더 어텐션을 수행하도록 처리함으로써, 상기 n개의 디코더들 각각에 대한 출력을 산출한 후, 상기 n개의 디코더들 각각에 대한 출력을 하나의디코더 출력으로 결합함으로써, 상기 예측 요약문을 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행하는 것을 특징으로 하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,상기 n개의 서로 다른 BERT 기반의 PLM들은, M-BERT, KoBERT, HanBERT 및 KorBERT 기반의 PLM인 것을 특징으로하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,상기 요약문을 생성하는 단계는상기 요약문 생성 모델의 생성이 완료된 이후에, 상기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서상기 요약문 생성 명령이 인가되면, 상기 제1 텍스트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를 생성하고, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 상기 n개의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1인코더 출력들을 생성한 후, 상기 n개의 제1 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 상기 제1 텍스트에 대응되는 요약문을 생성하는 것을 특징으로 하는 전자 장치의 동작방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 요약문 생성 모델을 생성하는 단계를 통해, 상기 요약문 생성 모델의 생성이 완료된 이후에, 상기 사용자로부터 상기 요약문 생성 모델의 이용을 위한 비밀번호 설정 명령이 인가되면, p(p는 2이상의 자연수임)자리수의 제1 번호와 q(q는 2이상의 자연수임)자리수의 제2 번호를 랜덤하게 생성하고, 상기 제1 번호를 구성하는 각자리 숫자를 모듈로(modulo)-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수행하여 결과 벡터를 산출한 후, 상기 결과 벡터의 해밍 무게를 사전 설정된 해시 함수에 입력으로 인가하였을 때 산출되는 해시 값을, 사용자 인증 값으로지정하여, 사전 지정된 클라우드 저장 서버에 저장하는 단계;상기 제1 번호와 상기 제2 번호를 서로 연접한 번호를, 상기 요약문 생성 모델의 이용을 위한 제1 비밀번호로서생성한 후, 상기 제1 비밀번호를 화면 상에 표시하면서, 상기 제1 비밀번호를 숙지할 것을 지시하는 메시지를화면 상에 표시하는 단계;상기 제1 비밀번호와 상기 메시지가 화면 상에 표시된 이후, 상기 사용자로부터 상기 요약문 생성 모델의 이용을 위한 인증 요청 명령이 인가되면, 화면 상에, 인증을 위한 비밀번호를 입력할 것을 지시하는 메시지를 표시한 후, 상기 사용자에 의해, 상기 제1 비밀번호가 입력되면, 상기 제1 비밀번호로부터 상기 제1 번호와 상기 제2 번호를 분리한 후, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수행하여 상기 결과 벡터를 산출한 다음, 상기 결과 벡터의 해밍 무게를 상기 해시 함수에 인가하여 상기 해시 값을 연산한후, 상기 해시 값이 상기 클라우드 저장 서버에 저장되어 있는 상기 사용자 인증 값과 일치하는지 확인하여, 서로 일치하는 것으로 확인되면, 상기 사용자에 대한 인증을 완료 처리하는 단계; 및상기 사용자에 대한 인증이 완료 처리되면, 화면 상에, 인증이 완료되었음을 지시하는 메시지를 표시함과 동시에, 상기 요약문 생성 모델에 입력으로 인가될 텍스트를 입력할 것을 지시하는 안내 메시지를 표시하는 단계공개특허 10-2025-0042966-6-를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제8항 내지 제14항 중 어느 한 항의 방법을 컴퓨터와의 결합을 통해 실행시키기 위한 컴퓨터 프로그램을 기록한컴퓨터 판독 가능 기록 매체."}
{"patent_id": "10-2023-0126110", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제8항 내지 제14항 중 어느 한 항의 방법을 컴퓨터와의 결합을 통해 실행시키기 위한 저장매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그 동작 방법 요 약"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "요약", "paragraph": 2, "content": "본 발명은 트랜스포머 기반의 요약문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그 동작 방법"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "요약", "paragraph": 3, "content": "을 제시함으로써, 사용자가 보다 간편하게 소정의 텍스트에 대한 요약문을 획득할 수 있도록 지원할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 트랜스포머 기반의 요약문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그 동작 방 법에 대한 것이다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 인터넷 등의 발달로 인해, 많은 양의 정보가 유통됨에 따라, 다수의 문장들로 구성된 텍스트로부터 주요"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "핵심 정보만을 간추린 요약문을 자동으로 생성할 수 있는 기술에 대한 수요가 증가하고 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "기존의 요약문 생성 기법은, 텍스트에서의 등장 빈도수가 많은 단어들을 주요 단어로 선택하고, 이 주요 단어들"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "을 조합하는 방식이 주로 사용되었지만, 이러한 방식은, 문맥을 고려하지 않은 요약문 생성 기법이라는 점에서, 정확도가 다소 떨어지는 문제가 있었다. 이와 관련해서, 최근에는 일부의 샘플 데이터를 기초로 소정의 결과를 판단하기 위한 학습 모델을 만들 수 있는 기계학습 기반의 인공지능 기술이 자주 활용되고 있고, 이러한 인공지능 기술은 자연어 처리 분야에서도 널리"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "활용되고 있다는 점에서, 요약문 자동 생성 기능에 대해서도, 이러한 인공지능 기술의 활용을 고려할 수 있다. 특히, 최근 제안된 트랜스포머(Transformer) 모델은, 소정의 입력 시퀀스가 인가되면, 어텐션(Attention) 메커 니즘을 통해, 시퀀스 내에서의 각 데이터의 관계를 고려하여, 소정의 출력 시퀀스를 산출하는 모델로서, 문장에 서의 문맥을 고려한 학습 모델을 만들 수 있다는 점에서, 언어 모델 구성에 자주 사용되고 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "따라서, 소정의 텍스트로부터 요약문을 자동으로 생성하는 모델을 구성하는 데에 있어서도, 이 트랜스포머 모델 을 이용하는 것을 고려할 수 있다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Ashish Vaswani et al., \"Attention Is All You Need\", arXiv:1706.03762, 2017. (비특허문헌 0002) Jacob Devlin et al., \"BERT: Pre-training of deep bidirectional transformers for language understanding\", In arXiv:1810.04805, 2019."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 트랜스포머 기반의 요약문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그 동작 방"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "법을 제시함으로써, 사용자가 보다 간편하게 소정의 텍스트에 대한 요약문을 획득할 수 있도록 지원하고자 한다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일실시예에 따른 전자 장치는, 사전 설정된 복수의 훈련용 텍스트들 - 상기 복수의 훈련용 텍스트들 각각은, 하나 이상의 문장으로 구성된 텍스트임 - 과, 상기 복수의 훈련용 텍스트들 각각에 대응되는 정답 레이"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "블로서 사전 지정된 서로 다른 요약문이 저장되어 있는 훈련 데이터 저장부, 상기 훈련 데이터 저장부에 저장되 어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈련용 텍스트를 선정하여, 상기 선정된 훈련용 텍스트와"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "그에 대응되는 요약문을 기초로, 트랜스포머(Transformer) 기반의 신경망 모델을 학습시키는 과정을, 상기 복수"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "의 훈련용 텍스트들 각각에 대해 반복 수행함으로써, 요약문 생성 모델을 생성하는 모델 생성부 및 상기 요약문"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "생성 모델의 생성이 완료된 이후에, 사용자에 의해, 제1 텍스트가 입력으로 인가되면서 요약문 생성 명령이 인"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "가되면, 상기 요약문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성하는 요약문 생성부를 포함한 다. 또한, 본 발명의 일실시예에 따른 전자 장치의 동작 방법은, 사전 설정된 복수의 훈련용 텍스트들 - 상기 복수 의 훈련용 텍스트들 각각은, 하나 이상의 문장으로 구성된 텍스트임 - 과, 상기 복수의 훈련용 텍스트들 각각에"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 7, "content": "대응되는 정답 레이블로서 사전 지정된 서로 다른 요약문이 저장되어 있는 훈련 데이터 저장부를 유지하는 단계, 상기 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈련용 텍스트를"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 8, "content": "선정하여, 상기 선정된 훈련용 텍스트와 그에 대응되는 요약문을 기초로, 트랜스포머 기반의 신경망 모델을 학"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 9, "content": "습시키는 과정을, 상기 복수의 훈련용 텍스트들 각각에 대해 반복 수행함으로써, 요약문 생성 모델을 생성하는"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 10, "content": "단계 및 상기 요약문 생성 모델의 생성이 완료된 이후에, 사용자에 의해, 제1 텍스트가 입력으로 인가되면서 요"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 11, "content": "약문 생성 명령이 인가되면, 상기 요약문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성하는 단계 를 포함한다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 트랜스포머 기반의 요약문 생성 모델에 기초하여 요약문을 생성할 수 있는 전자 장치 및 그 동작 방"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "법을 제시함으로써, 사용자가 보다 간편하게 소정의 텍스트에 대한 요약문을 획득할 수 있도록 지원할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 본 발명에 따른 실시예들을 첨부된 도면을 참조하여 상세하게 설명하기로 한다. 이러한 설명은 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구 성요소에 대해 사용하였으며, 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 본 명세서 상"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "에서 사용되는 모든 용어들은 본 발명이 속하는 기술분야에서 통상의 지식을 가진 사람에 의해 일반적으로 이해 되는 것과 동일한 의미를 가지고 있다. 본 문서에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구 성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있다는 것을 의미한다. 또한, 본 발명의 다양한 실시예들에 있어서, 각 구성요소들, 기능 블록들 또는 수단들은 하나 또는 그 이상의 하부 구성요소로 구성될 수 있고, 각 구성요소들이 수행하는 전기, 전자, 기계적 기능들은 전자회로, 집적회로, ASIC(Application Specific Integrated Circuit) 등 공지된 다양한 소자들 또는 기계적 요소들로 구현될 수 있으며, 각각 별개로 구현되거나 2 이상이 하나로 통합되어 구현될 수도 있다. 한편, 첨부된 블록도의 블록들이나 흐름도의 단계들은 범용 컴퓨터, 특수용 컴퓨터, 휴대용 노트북 컴퓨터, 네 트워크 컴퓨터 등 데이터 프로세싱이 가능한 장비의 프로세서나 메모리에 탑재되어 지정된 기능들을 수행하는 컴퓨터 프로그램 명령들(instructions)을 의미하는 것으로 해석될 수 있다. 이들 컴퓨터 프로그램 명령들은 컴 퓨터 장치에 구비된 메모리 또는 컴퓨터에서 판독 가능한 메모리에 저장될 수 있기 때문에, 블록도의 블록들 또 는 흐름도의 단계들에서 설명된 기능들은 이를 수행하는 명령 수단을 내포하는 제조물로 생산될 수도 있다. 아 울러, 각 블록 또는 각 단계는 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 명령들을 포 함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 가능한 실시예들에서는 블록들또는 단계들에서 언급된 기능들이 정해진 순서와 달리 실행되는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 또는 단계들은 실질적으로 동시에 수행되거나, 역순으로 수행될 수 있으며, 경우 에 따라 일부 블록들 또는 단계들이 생략된 채로 수행될 수도 있다. 도 1은 본 발명의 일실시예에 따른 전자 장치의 구조를 도시한 도면이다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 3, "content": "도 1을 참조하면, 본 발명에 따른 전자 장치는 훈련 데이터 저장부, 모델 생성부 및 요약문 생 성부를 포함한다. 훈련 데이터 저장부에는 사전 설정된 복수의 훈련용 텍스트들과, 상기 복수의 훈련용 텍스트들 각각에 대"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "응되는 정답 레이블로서 사전 지정된 서로 다른 요약문이 저장되어 있다. 여기서, 상기 복수의 훈련용 텍스트들 각각은, 하나 이상의 문장으로 구성된 텍스트를 의미하고, 상기 복수의"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "훈련용 텍스트들 각각에 대응되는 요약문은, 각 훈련용 텍스트를 요약한 요약문을 의미한다. 이와 관련해서, 훈련 데이터 저장부에는 하기의 표 1과 같이 데이터가 저장되어 있을 수 있다. 표 1"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "복수의 훈련용 텍스트들 요약문"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "훈련용 텍스트 1 요약문 1"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "훈련용 텍스트 2 요약문 2"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "훈련용 텍스트 3 요약문 3"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "훈련용 텍스트 4 요약문 4 ... ... 모델 생성부는 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "련용 텍스트를 선정하여, 상기 선정된 훈련용 텍스트와 그에 대응되는 요약문을 기초로, 트랜스포머 (Transformer) 기반의 신경망 모델을 학습시키는 과정을, 상기 복수의 훈련용 텍스트들 각각에 대해 반복 수행"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "함으로써, 요약문 생성 모델을 생성한다. 여기서, 트랜스포머 모델이란, 시퀀스-투-시퀀스 모델로서, 소정의 입력 시퀀스가 인가되면, 해당 입력 시퀀스 에 대응되는 출력 시퀀스를 출력으로 산출하는 모델을 의미한다. 이때, 트랜스포머 모델은 인코더와 디코더로 구성되는데, 인코더와 디코더에서 각각 셀프 어텐션(Self Attention)이 수행되도록 구성되고, 마지막으로 디코 더에서, 인코더와 디코더 간의 인코더-디코더 어텐션이 수행되도록 구성된다. 이때, 본 발명의 일실시예에 따르면, 모델 생성부는 토큰 생성부, 인코더 출력부 및 학습 수행 부를 포함할 수 있다. 토큰 생성부는 상기 복수의 훈련용 텍스트들 중 어느 하나인 제1 훈련용 텍스트가 선정됨에 따라, 상기 제 1 훈련용 텍스트에 대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, n(n은 2이 상의 자연수임)개의 서로 다른 토크나이저(Tokenizer)들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n 개의 토크나이저들 각각에 대응되는 토큰 시퀀스를 생성한다. 여기서, 상기 n개의 토크나이저들 각각은, n개의 서로 다른 BERT(Bidirectional Encoder Representations from Transformers) 기반의 사전 훈련된 언어 모델(Pre-trained Language Model: PLM)들을 기초로 구성된 토크나이 저를 의미한다. 관련해서, BERT 모델이란, 자연어 처리 모델로서, 소정의 문장이 입력되면, 이 문장을 단어 단위로 구분하여 토 큰화시켜, 소정의 입력 시퀀스를 생성하고, 이 입력 시퀀스를, layer normalization, multihead self- attention, dropout, feed forward networks로 구성된, 소정 개수의 BERT 레이어들에 연쇄적으로 인가하여 출 력 시퀀스를 생성해내는 모델을 의미한다. 이때, 본 발명의 일실시예에 따르면, 상기 n개의 서로 다른 BERT 기반의 PLM들은, M-BERT, KoBERT, HanBERT 및 KorBERT 기반의 PLM일 수 있다. 여기서, M-BERT는 Multilingual BERT로서, 영어뿐 아니라 다수의 언어로 구성된 데이터를 함께 학습시킨 언어 모델이고, KoBERT, HanBERT, KorBERT는 한국어 문장으로 이루어진 말뭉치를 기 초로 학습된 언어 모델을 의미한다. 이로 인해, 토큰 생성부는 상기 제1 훈련용 텍스트에 대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, M-BERT, KoBERT, HanBERT 및 KorBERT 기반의 PLM에 따른 토크나이저들 각각 에 통과시켜 토큰 임베딩을 수행함으로써, M-BERT에 대응되는 토큰 시퀀스, KoBERT에 대응되는 토큰 시퀀스, HanBERT에 대응되는 토큰 시퀀스 및 KorBERT에 대응되는 토큰 시퀀스를 생성할 수 있다. 이렇게, 토큰 생성부를 통해, 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스가 생성되면, 인코더 출력부는 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를, n개의 인코더들(여기서, 상기 n개의 인코더들 각각은, 트랜스포머 모델을 구성하는 인코더를 의미함) 각각에 하나씩 입력으로 인가함으로써, n개의 인코더 출력들을 생성한다. 그리고, 학습 수행부는 상기 n개의 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "산출함으로써, 예측 요약문을 생성한 후, 상기 예측 요약문과, 훈련 데이터 저장부에 저장되어 있는 상기"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행한다. 이때, 본 발명의 일실시예에 따르면, 학습 수행부는 상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인 코더 출력들을 서로 연접(Concatenation)하여 하나의 단일 인코더 출력으로 구성한 후, 상기 단일 인코더 출력 을, 트랜스포머 모델을 구성하는 하나의 단일 디코더에 입력 시퀀스로서 인가하되, 상기 단일 디코더가, 상기 단일 인코더 출력을 기반으로 인코더-디코더 어텐션(Attention)을 수행하도록 처리함으로써, 상기 단일 디코더"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "로부터 산출되는 출력을 상기 예측 요약문으로 생성한 다음, 상기 예측 요약문과, 훈련 데이터 저장부에"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행할 수 있다. 이와 관련해서, 도 2를 참조하여, 학습 수행부의 동작을 예를 들어 설명하면 다음과 같다. 우선, 토큰 생성부는 상기 복수의 훈련용 텍스트들 중 어느 하나인 상기 제1 훈련용 텍스트에 대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, 도면부호 211에 도시된 그림과 같이, n 개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 도면후보 212에 도시된 그림과 같은, 각 토크 나이저에 대응되는 토큰 시퀀스를 생성할 수 있다. 그 이후, 인코더 출력부는, 도면부호 213에 도시된 그림과 같이, 상기 n개의 토크나이저들 각각에 대응되 는 토큰 시퀀스를, n개의 인코더들 각각에 하나씩 입력으로 인가하여, 각 인코더를 통해서 산출되는 인코더 출 력을 획득함으로써, 총 n개의 인코더 출력들을 생성할 수 있다. 그러면, 학습 수행부는, 도면부호 214에 도시된 그림과 같이, 상기 n개의 인코더 출력들을 서로 연접 (Concatenation)하여 하나의 단일 인코더 출력으로 구성할 수 있다. 그러고 나서, 학습 수행부는, 도면부호 215에 도시된 그림과 같이, 상기 단일 인코더 출력을, 트랜스포머 모델을 구성하는 하나의 단일 디코더에 입력 시퀀스로서 인가함으로써, 상기 단일 디코더로부터 산출되는 출력"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "을 예측 요약문으로 생성할 수 있다. 이때, 상기 단일 디코더는, 트랜스포머 모델을 구성하는 디코더에 해당되기 때문에, 상기 단일 디코더에, 상기 단일 인코더 출력이 입력 시퀀스로서 인가되면, 해당 입력 시퀀스에 대해 포지셔널 임베딩, 셀프-어텐션, 잔차 연결, 인코더-디코더 어텐션, 잔차 연결, 피드-포워드, 잔차 연결을 수행하여 소정의 시퀀스 형태의 출력을 산 출하게 된다. 이때, 상기 단일 디코더에는, 도면부호 214와 같은 상기 n개의 인코더 출력들이 서로 연접되어 생성된 단일 인코더 출력이 입력 시퀀스로서 인가되는 것이기 때문에, 상기 단일 디코더는, 인코더-디코더 어텐 션을 수행할 때, 상기 n개의 인코더 출력들이 서로 연접되어 생성된 상기 단일 인코더 출력을 기반으로 인코더- 디코더 어텐션을 수행하게 된다. 이렇게, 상기 단일 디코더를 통해, 출력이 산출되면, 학습 수행부는 상기 단일 디코더로부터 산출되는 출"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "력을 예측 요약문으로 생성한 다음, 상기 예측 요약문과, 훈련 데이터 저장부에 저장되어 있는 상기 제1"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "이때, 본 발명의 일실시예에 따르면, 학습 수행부는 상기 단일 디코더를 통해 산출되는 상기 예측 요약문"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "과 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 BLEU(Bilingual Evaluation Understudy Score) 스코어가"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "최대가 되도록 학습을 수행할 수 있다. 아니면, 학습 수행부는 상기 예측 요약문과 상기 제1 훈련용 텍스"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "트에 대응되는 요약문 간의 BLEU 스코어에 따른 오차를 표상하는 손실 값이 최소가 되도록 역전파(Backpropagation) 처리를 수행하는 방법으로, 학습을 수행할 수 있다. 한편, 본 발명의 다른 일실시예에 따르면, 학습 수행부는, 앞서 설명한 실시예와 달리, 인코더 출력부 를 통해 상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들 각각을, n개의 디코더들(여기서, 상기 n개의 디코더들 각각은, 트랜스포머 모델을 구성하는 디코더를 의미함) 각각에 하나씩 입력 시퀀스로서 인가하되, 상기 n개의 디코더들 각각이, 각 디코더에 인가되는 인코더 출력을 기반으로 인코더 -디코더 어텐션을 수행하도록 처리함으로써, 상기 n개의 디코더들 각각에 대한 출력을 산출한 후, 상기 n개의"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "디코더들 각각에 대한 출력을 하나의 디코더 출력으로 결합함으로써, 상기 예측 요약문을 생성한 다음, 상기 예"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "측 요약문과, 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차 가 최소가 되도록 학습을 수행할 수 있다. 이와 관련해서, 도 3을 참조하여, 학습 수행부와 관련된 또 다른 실시예를 상세히 설명하면 다음과 같다. 우선, 토큰 생성부는 상기 복수의 훈련용 텍스트들 중 어느 하나인 상기 제1 훈련용 텍스트에 대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, 도면부호 311에 도시된 그림과 같이, n 개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 도면후보 312에 도시된 그림과 같은, 각 토크 나이저에 대응되는 토큰 시퀀스를 생성할 수 있다. 그 이후, 인코더 출력부는, 도면부호 313에 도시된 그림과 같이, 상기 n개의 토크나이저들 각각에 대응되 는 토큰 시퀀스를, n개의 인코더들 각각에 하나씩 입력으로 인가하여, 각 인코더를 통해서 산출되는 인코더 출 력을 획득함으로써, 총 n개의 인코더 출력들을 생성할 수 있다. 그러면, 학습 수행부는, 도면부호 314에 도시된 그림과 같이, 상기 n개의 인코더 출력들 각각을, n개의 디 코더들 각각에 하나씩 입력 시퀀스로서 인가할 수 있다. 이때, 상기 n개의 디코더들 각각은, 트랜스포머 모델을 구성하는 디코더에 해당되기 때문에, 상기 n개의 디코더 들 각각에, 상기 n개의 인코더 출력들 각각이 하나씩 입력 시퀀스로서 인가되면, 각 디코더는, 해당 입력 시퀀 스에 대해 포지셔널 임베딩, 셀프-어텐션, 잔차 연결, 인코더-디코더 어텐션, 잔차 연결, 피드-포워드, 잔차 연 결을 수행하여 소정의 시퀀스 형태의 출력을 산출하게 된다. 이때, 상기 n개의 디코더들 각각에는, 상기 n개의 인코더 출력들 각각이 하나씩 인가되는 것이기 때문에, 각 디코더는, 인코더-디코더 어텐션을 수행할 때, 각 디 코더에 인가되는 인코더 출력을 기반으로 인코더-디코더 어텐션을 수행하게 된다. 이렇게, 상기 n개의 디코더들 각각에 대한 출력이 산출되면, 학습 수행부는, 상기 n개의 디코더들 각각에"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 26, "content": "대한 출력을 하나의 디코더 출력으로 결합함으로써, 상기 예측 요약문을 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "그러고 나서, 학습 수행부는 상기 예측 요약문과, 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "이때, 본 발명의 일실시예에 따르면, 학습 수행부는 상기 예측 요약문과 상기 제1 훈련용 텍스트에 대응되"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "는 요약문 간의 BLEU 스코어가 최대가 되도록 학습을 수행할 수 있다. 아니면, 학습 수행부는 상기 예측"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "요약문과 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 BLEU 스코어에 따른 오차를 표상하는 손실 값이 최소 가 되도록 역전파 처리를 수행하는 방법으로, 학습을 수행할 수 있다. 지금까지 설명한 방식에 따라, 모델 생성부는 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "용 텍스트들과 그에 대응되는 요약문을 기초로 한 기계학습을 반복 수행함으로써, 텍스트로부터 요약문을 생성"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "하기 위한, 요약문 생성 모델을 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "이렇게, 모델 생성부를 통해, 상기 요약문 생성 모델의 생성이 완료된 이후에, 사용자에 의해, 전자 장치"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 35, "content": "상에, 제1 텍스트가 입력으로 인가되면서 요약문 생성 명령이 인가되면, 요약문 생성부는 상기 요약"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 36, "content": "문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성한다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 37, "content": "이때, 본 발명의 일실시예에 따르면, 요약문 생성부는 상기 요약문 생성 모델의 생성이 완료된 이후에, 상"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 38, "content": "기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서 상기 요약문 생성 명령이 인가되면, 상기 제1 텍스 트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를 생성하고, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 상기 n개 의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1 인코더 출력들을 생성한 후, 상기 n개의 제1 인코더"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 39, "content": "출력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 상기 제1 텍스트에 대응되는 요약문을 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 40, "content": "구체적으로, 상기 요약문 생성 모델의 구조가 도 2와 같은 구조로 되어 있다고 하는 경우, 요약문 생성부"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 41, "content": "는, 상기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서 상기 요약문 생성 명령이 인가되면, 우선, 도 면부호 211에 도시된 그림과 같이, 상기 제1 텍스트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 도면부호 212에 도시된 그림과 같은 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스 를 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 42, "content": "그러고 나서, 요약문 생성부는 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 도면부호 213에 도시된 그림과 같이, 상기 n개의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1 인코더 출력들을 생성한 후, 도면부호 214에 도시된 그림과 같이, 상기 n개의 제1 인코더 출력들을 연접하여 단일 인코더 출력으 로 구성한 다음, 도면부호 215에 도시된 그림과 같이, 상기 단일 인코더 출력을 트랜스포머 모델을 구성하는 디 코더에 입력 시퀀스로서 인가함으로써, 상기 디코더를 통해서 산출되는 출력을, 상기 제1 텍스트에 대응되는 요 약문으로 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 43, "content": "반면에, 상기 요약문 생성 모델의 구조가 도 3과 같은 구조로 되어 있다고 하는 경우, 요약문 생성부는,"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 44, "content": "상기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서 상기 요약문 생성 명령이 인가되면, 우선, 도면부 호 311에 도시된 그림과 같이, 상기 제1 텍스트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수 행함으로써, 도면부호 312에 도시된 그림과 같은 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 45, "content": "그러고 나서, 요약문 생성부는 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 도면부호 313에 도시된 그림과 같이, 상기 n개의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1 인코더 출력들을 생성한 후, 도면부호 314에 도시된 그림과 같이, 상기 n개의 제1 인코더 출력들 각각을, n개의 디코더들 각각에 하나씩 입력 시퀀스로서 인가하였을 때의 출력들을 하나의 디코더 출력으로 결합함으로써, 상기 제1 텍스트에"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 46, "content": "대응되는 요약문을 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 47, "content": "본 발명의 일실시예에 따르면, 전자 장치는, 소정의 인증된 사용자만이, 학습이 완료된 상기 요약문 생성"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 48, "content": "모델을 이용하여, 소정의 텍스트에 대한 요약문을 획득할 수 있도록 하는, 사용자 인증과 관련된 구성을 더 포 함할 수 있다. 이와 관련해서, 본 발명의 일실시예에 따르면, 전자 장치는 저장 처리부, 표시부, 인증 완료부 및 안내 메시지 표시부를 더 포함할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 49, "content": "저장 처리부는, 모델 생성부를 통해, 상기 요약문 생성 모델의 생성이 완료된 이후에, 상기 사용자로"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 50, "content": "부터 상기 요약문 생성 모델의 이용을 위한 비밀번호 설정 명령이 인가되면, p(p는 2이상의 자연수임)자리수의 제1 번호와 q(q는 2이상의 자연수임)자리수의 제2 번호를 랜덤하게 생성하고, 상기 제1 번호를 구성하는 각 자 리 숫자를 모듈로(modulo)-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈 로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수행하여 결과 벡터를 산출한 후, 상기 결과 벡터의 해밍 무게를 사전 설정된 해시 함수에 입력으로 인가하였을 때 산출되는 해시 값을, 사용자 인증 값으로 지정하여, 사전 지정된 클라우드 저장 서버에 저장한다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 51, "content": "표시부는 상기 제1 번호와 상기 제2 번호를 서로 연접한 번호를, 상기 요약문 생성 모델의 이용을 위한 제 1 비밀번호로서 생성한 후, 상기 제1 비밀번호를 화면 상에 표시하면서, 상기 제1 비밀번호를 숙지할 것을 지시 하는 메시지를 화면 상에 표시한다. 예컨대, p를 3, q를 5라고 하고, 상기 제1 번호가 '213', 상기 제2 번호가 '45678'과 같이 생성되었다고 하는 경우, 저장 처리부는 '213'을 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터인 '[0 1 0]'과, '45678'을 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터인 '[0 1 0 1 0]' 간의 합 성곱을 수행하여 '[0 0 1 0 1 0 0]'라는 결과 벡터를 생성한 후, 상기 결과 벡터의 해밍 무게인 '2'를 상기 해 시 함수에 입력으로 인가하였을 때 산출되는 해시 값을, 사용자 인증 값으로 지정하여, 사전 지정된 클라우드 저장 서버에 저장할 수 있다. 그러고 나서, 표시부는 상기 제1 번호인 '213'과 상기 제2 번호인 '45678'을 서로 연접한 번호인"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 52, "content": "'21345678'을, 상기 요약문 생성 모델의 이용을 위한 제1 비밀번호로서 생성한 후, 상기 제1 비밀번호를 화면 상에 표시하면서, 상기 제1 비밀번호를 숙지할 것을 지시하는 메시지를 화면 상에 표시할 수 있다. 이를 통해,상기 사용자는 상기 제1 비밀번호인 '21345678'을 숙지해 둘 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 53, "content": "인증 완료부는 상기 제1 비밀번호와 상기 메시지가 화면 상에 표시된 이후, 상기 사용자로부터 상기 요약 문 생성 모델의 이용을 위한 인증 요청 명령이 인가되면, 화면 상에, 인증을 위한 비밀번호를 입력할 것을 지시 하는 메시지를 표시한 후, 상기 사용자에 의해, 상기 제1 비밀번호가 입력되면, 상기 제1 비밀번호로부터 상기 제1 번호와 상기 제2 번호를 분리한 후, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분 으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합 성곱을 수행하여 상기 결과 벡터를 산출한 다음, 상기 결과 벡터의 해밍 무게를 상기 해시 함수에 인가하여 상 기 해시 값을 연산한 후, 상기 해시 값이 클라우드 저장 서버에 저장되어 있는 상기 사용자 인증 값과 일치 하는지 확인하여, 서로 일치하는 것으로 확인되면, 상기 사용자에 대한 인증을 완료 처리한다. 관련해서, 전술한 예와 같이, '21345678'라는 상기 제1 비밀번호와 상기 메시지가 화면 상에 표시된 이후, 상기"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 54, "content": "사용자로부터 상기 요약문 생성 모델의 이용을 위한 인증 요청 명령이 전자 장치에 입력으로 인가되면, 인 증 완료부는 상기 제1 비밀번호로부터 '213'이라는 제1 번호와 '45678'이라고 하는 제2 번호를 분리한 후, '213'을 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터인 '[0 1 0]'과, '45678'을 구성하 는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터인 '[0 1 0 1 0]' 간의 합성곱을 수행하여 '[0 0 1 0 1 0 0]'라는 결과 벡터를 산출할 수 있다. 그러고 나서, 인증 완료부는 '[0 0 1 0 1 0 0]'라는 결과 벡터의 해밍 무게인 '2'를 상기 해시 함수에 입력으로 인가함으로써, 해시 값을 연산한 후, 상기 해시 값이 클 라우드 저장 서버에 저장되어 있는 상기 사용자 인증 값과 일치하는지 확인하여, 서로 일치하는 것으로 확 인되면, 상기 사용자에 대한 인증을 완료 처리할 수 있다. 안내 메시지 표시부는 상기 사용자에 대한 인증이 완료 처리되면, 화면 상에, 인증이 완료되었음을 지시하"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 55, "content": "는 메시지를 표시함과 동시에, 상기 요약문 생성 모델에 입력으로 인가될 텍스트를 입력할 것을 지시하는 안내 메시지를 표시한다. 이를 통해, 상기 사용자는 상기 안내 메시지를 확인한 후, 소정의 텍스트를 전자 장치 상에 인가할 수 있"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 56, "content": "고, 이때, 요약문 생성부는, 상기 사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면, 상기 요약문 생"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 57, "content": "성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생성할 수 있다. 도 4는 본 발명의 일실시예에 따른 전자 장치의 동작 방법을 도시한 순서도이다. 단계(S410)에서는, 사전 설정된 복수의 훈련용 텍스트들(상기 복수의 훈련용 텍스트들 각각은, 하나 이상의 문 장으로 구성된 텍스트임)과, 상기 복수의 훈련용 텍스트들 각각에 대응되는 정답 레이블로서 사전 지정된 서로"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 58, "content": "다른 요약문이 저장되어 있는 훈련 데이터 저장부를 유지한다. 단계(S420)에서는, 상기 훈련 데이터 저장부에 저장되어 있는 상기 복수의 훈련용 텍스트들 중 어느 하나의 훈"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 59, "content": "련용 텍스트를 선정하여, 상기 선정된 훈련용 텍스트와 그에 대응되는 요약문을 기초로, 트랜스포머 기반의 신"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 60, "content": "경망 모델을 학습시키는 과정을, 상기 복수의 훈련용 텍스트들 각각에 대해 반복 수행함으로써, 요약문 생성 모 델을 생성한다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 61, "content": "단계(S430)에서는, 상기 요약문 생성 모델의 생성이 완료된 이후에, 사용자에 의해, 제1 텍스트가 입력으로 인"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 62, "content": "가되면서 요약문 생성 명령이 인가되면, 상기 요약문 생성 모델을 기초로, 상기 제1 텍스트에 대한 요약문을 생 성한다. 이때, 본 발명의 일실시예에 따르면, 단계(S420)에서는, 상기 복수의 훈련용 텍스트들 중 어느 하나인 제1 훈련 용 텍스트가 선정됨에 따라, 상기 제1 훈련용 텍스트에 대한 학습 과정이 수행되어야 하는 순서가 되는 경우, 상기 제1 훈련용 텍스트를, n(n은 2이상의 자연수임)개의 서로 다른 토크나이저들(상기 n개의 토크나이저들 각 각은, n개의 서로 다른 BERT 기반의 사전 훈련된 언어 모델(Pre-trained Language Model: PLM)들을 기초로 구 성된 토크나이저임) 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를 생성하는 단계, 상기 n개의 토크나이저들 각각에 대응되는 토큰 시퀀스를, n개의 인코더들(상기 n개의 인코더들 각각은, 트랜스포머 모델을 구성하는 인코더를 의미함) 각각에 하나씩 입력으로 인가함으로써, n개의 인코더 출력들을 생성하는 단계 및 상기 n개의 인코더 출력들을 기초로, 트랜스포머 모델을 구성하는 디"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 63, "content": "코더의 출력을 산출함으로써, 예측 요약문을 생성한 후, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 64, "content": "되어 있는 상기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행하는 단계를 포함 할 수 있다. 이때, 본 발명의 일실시예에 따르면, 상기 학습을 수행하는 단계는, 상기 n개의 인코더 출력들이 생성되면, 상 기 n개의 인코더 출력들을 서로 연접하여 하나의 단일 인코더 출력으로 구성한 후, 상기 단일 인코더 출력을, 트랜스포머 모델을 구성하는 하나의 단일 디코더에 입력 시퀀스로서 인가하되, 상기 단일 디코더가, 상기 단일 인코더 출력을 기반으로 인코더-디코더 어텐션을 수행하도록 처리함으로써, 상기 단일 디코더로부터 산출되는"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 65, "content": "출력을 상기 예측 요약문으로 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 66, "content": "기 제1 훈련용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행할 수 있다. 또한, 본 발명의 다른 일실시예에 따르면, 상기 학습을 수행하는 단계는, 상기 n개의 인코더 출력들이 생성되면, 상기 n개의 인코더 출력들 각각을, n개의 디코더들(상기 n개의 디코더들 각각은, 트랜스포머 모델을 구성하는 디코더를 의미함)각각에 하나씩 입력 시퀀스로서 인가하되, 상기 n개의 디코더들 각각이, 각 디코더에 인가되는 인코더 출력을 기반으로 인코더-디코더 어텐션을 수행하도록 처리함으로써, 상기 n개의 디코더들 각각 에 대한 출력을 산출한 후, 상기 n개의 디코더들 각각에 대한 출력을 하나의 디코더 출력으로 결합함으로써, 상"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 67, "content": "기 예측 요약문을 생성한 다음, 상기 예측 요약문과, 상기 훈련 데이터 저장부에 저장되어 있는 상기 제1 훈련"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 68, "content": "용 텍스트에 대응되는 요약문 간의 오차가 최소가 되도록 학습을 수행할 수 있다. 또한, 본 발명의 일실시예에 따르면, 상기 n개의 서로 다른 BERT 기반의 PLM들은, M-BERT, KoBERT, HanBERT 및 KorBERT 기반의 PLM일 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 69, "content": "또한, 본 발명의 일실시예에 따르면, 단계(S430)에서는, 상기 요약문 생성 모델의 생성이 완료된 이후에, 상기"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 70, "content": "사용자에 의해, 상기 제1 텍스트가 입력으로 인가되면서 상기 요약문 생성 명령이 인가되면, 상기 제1 텍스트를 상기 n개의 토크나이저들 각각에 통과시켜 토큰 임베딩을 수행함으로써, 상기 n개의 토크나이저들 각각에 대응 되는 제1 토큰 시퀀스를 생성하고, 상기 n개의 토크나이저들 각각에 대응되는 제1 토큰 시퀀스를, 상기 n개의 인코더들 각각에 하나씩 입력으로 인가하여, n개의 제1 인코더 출력들을 생성한 후, 상기 n개의 제1 인코더 출"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 71, "content": "력들을 기초로, 트랜스포머 모델을 구성하는 디코더의 출력을 산출함으로써, 상기 제1 텍스트에 대응되는 요약 문을 생성할 수 있다."}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 72, "content": "또한, 본 발명의 일실시예에 따르면, 상기 전자 장치의 동작 방법은, 단계(S420)을 통해, 상기 요약문 생성 모"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 73, "content": "델의 생성이 완료된 이후에, 상기 사용자로부터 상기 요약문 생성 모델의 이용을 위한 비밀번호 설정 명령이 인 가되면, p(p는 2이상의 자연수임)자리수의 제1 번호와 q(q는 2이상의 자연수임)자리수의 제2 번호를 랜덤하게 생성하고, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로(modulo)-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수행하여 결과 벡터를 산출한 후, 상기 결과 벡터의 해밍 무게를 사전 설정된 해시 함수에 입력으로 인가하였을 때 산출되는 해시 값을, 사용자 인증 값으로 지정하여, 사전 지정된 클라우드 저장 서버에 저장하는 단계, 상기 제1 번호와"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 74, "content": "상기 제2 번호를 서로 연접한 번호를, 상기 요약문 생성 모델의 이용을 위한 제1 비밀번호로서 생성한 후, 상기 제1 비밀번호를 화면 상에 표시하면서, 상기 제1 비밀번호를 숙지할 것을 지시하는 메시지를 화면 상에 표시하"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 75, "content": "는 단계, 상기 제1 비밀번호와 상기 메시지가 화면 상에 표시된 이후, 상기 사용자로부터 상기 요약문 생성 모 델의 이용을 위한 인증 요청 명령이 인가되면, 화면 상에, 인증을 위한 비밀번호를 입력할 것을 지시하는 메시 지를 표시한 후, 상기 사용자에 의해, 상기 제1 비밀번호가 입력되면, 상기 제1 비밀번호로부터 상기 제1 번호 와 상기 제2 번호를 분리한 후, 상기 제1 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터와 상기 제2 번호를 구성하는 각 자리 숫자를 모듈로-2 연산한 값을 성분으로 갖는 벡터 간의 합성곱을 수 행하여 상기 결과 벡터를 산출한 다음, 상기 결과 벡터의 해밍 무게를 상기 해시 함수에 인가하여 상기 해시 값 을 연산한 후, 상기 해시 값이 상기 클라우드 저장 서버에 저장되어 있는 상기 사용자 인증 값과 일치하는지 확 인하여, 서로 일치하는 것으로 확인되면, 상기 사용자에 대한 인증을 완료 처리하는 단계 및 상기 사용자에 대"}
{"patent_id": "10-2023-0126110", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 76, "content": "한 인증이 완료 처리되면, 화면 상에, 인증이 완료되었음을 지시하는 메시지를 표시함과 동시에, 상기 요약문 생성 모델에 입력으로 인가될 텍스트를 입력할 것을 지시하는 안내 메시지를 표시하는 단계를 더 포함할 수 있 다. 이상, 도 4를 참조하여 본 발명의 일실시예에 따른 전자 장치의 동작 방법에 대해 설명하였다. 여기서, 본 발 명의 일실시예에 따른 전자 장치의 동작 방법은 도 1 내지 도 3을 이용하여 설명한 전자 장치의 동작에 대 한 구성과 대응될 수 있으므로, 이에 대한 보다 상세한 설명은 생략하기로 한다. 본 발명의 일실시예에 따른 전자 장치의 동작 방법은 컴퓨터와의 결합을 통해 실행시키기 위한 저장매체에 저장 된 컴퓨터 프로그램으로 구현될 수 있다. 또한, 본 발명의 일실시예에 따른 전자 장치의 동작 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로 그램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그 램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크 (floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같 은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실 행될 수 있는 고급 언어 코드를 포함한다. 이상과 같이 본 발명에서는 구체적인 구성 요소 등과 같은 특정 사항들과 한정된 실시예 및 도면에 의해 설명되 었으나 이는 본 발명의 보다 전반적인 이해를 돕기 위해서 제공된 것일 뿐, 본 발명은 상기의 실시예에 한정되 는 것은 아니며, 본 발명이 속하는 분야에서 통상적인 지식을 가진 자라면 이러한 기재로부터 다양한 수정 및 변형이 가능하다. 따라서, 본 발명의 사상은 설명된 실시예에 국한되어 정해져서는 아니되며, 후술하는 특허청구범위뿐 아니라 이 특허청구범위와 균등하거나 등가적 변형이 있는 모든 것들은 본 발명 사상의 범주에 속한다고 할 것이다."}
{"patent_id": "10-2023-0126110", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일실시예에 따른 전자 장치의 구조를 도시한 도면이다. 도 2 내지 도 3은 본 발명의 일실시예에 따른 전자 장치의 동작을 설명하기 위한 도면이다. 도 4는 본 발명의 일실시예에 따른 전자 장치의 동작 방법을 도시한 순서도이다."}
