{"patent_id": "10-2019-0144026", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0057358", "출원번호": "10-2019-0144026", "발명의 명칭": "제스처 인식 방법 및 이를 수행하는 제스처 인식 장치", "출원인": "주식회사 에스오에스랩", "발명자": "이용이"}}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방법으로서,깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계;싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 특정 핸드 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과 관련된 제1 클래스 값이 라벨링된 학습용 데이터 셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 특정 핸드 포스처를 검출함으로써, 트리거링 이벤트의 발생을 모니터링하는 단계; 및상기 트리거링 이벤트가 발생하면, 제스처 인식 모드를 실행하는 단계;를 포함하되,상기 제스처 인식 모드는,연속되는 이미지들을 입력받는 3D CNN 모델의 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처가 검출된이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계; 및상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계; 를 포함하는제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 학습용 데이터셋은 상기 특정 핸드 포스처와 상이한 핸드 포스처를 포함하는 깊이 이미지에 상기 제1 클래스 값과 상이한 제2 클래스 값이 라벨링된 데이터셋을 포함하는제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서,미리 설정된 시간이 경과하면 상기 제스처 인식 모드를 종료하는 단계;를 포함하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 타켓 컴포넌트의 동작이 수행되는 상기 핸드 제스처의 종류가 식별되는 경우 상기 제스처 인식 모드를 종료하는 단계;를 포함하는,공개특허 10-2021-0057358-3-제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 RNN 모델은 LSTM 모델인 것을 특징으로 하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 타겟 컴포넌트에서 수행되는 상기 타겟 동작은 상기 핸드 무브먼트에 대응되는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항에 있어서,상기 제스처는 상기 타겟 컴포넌트 종류에 따라 미리 정해진 포스처 및 상기 타겟 동작의 속성에 대응하는 이동속도를 갖는 무브먼트를 포함하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방법으로서,깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계;연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 태깅된 학습용 데이터셋으로 학습된 제1 인공 신경망을 제공하는 단계;상기 제1 인공 신경망의 상기 3D CNN 모델이 상기 RNN 모델과 연결되는 상기 맥스 풀링 레이어 또는 상기 풀리커넥티드 레이어를 입력 레이어로 하고, 특정 핸드 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과관련된 제1 클래스 값이 라벨링되고 상기 특정 핸드 포스처와 상이한 핸드 포스처를 포함하는 깊이 이미지에 상기 제1 클래스 값과 상이한 제2 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 제공하는단계;상기 영상이 상기 제1 인공 신경망에 입력됨에 따라 도출되는 상기 맥스 풀링 레이어 또는 상기 풀리 커넥티드레이어의 노드값으로부터, 상기 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처를 검출함으로써, 트리거링이벤트의 발생을 모니터링하는 단계;상기 트리거링 이벤트가 발생하면, 상기 제1 인공 신경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지프레임의 후속 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계; 및상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계; 를 포함하는공개특허 10-2021-0057358-4-제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 트리거링 이벤트가 발생하고 미리 설정된 시간이 경과하면 상기 핸드 제스처의 종류를 식별하는 단계를 종료하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8 항에 있어서,상기 RNN 모델은 LSTM 모델인 것을 특징으로 하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8 항에 있어서,상기 타겟 컴포넌트에서 수행되는 상기 타겟 동작은 상기 핸드 무브먼트에 대응되는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8 항에 있어서,상기 제스처는 상기 타겟 컴포넌트 종류에 따라 미리 정해진 포스처 및 상기 타겟 동작의 속성에 대응하는 이동속도를 갖는 무브먼트를 포함하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방법으로서,깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계;싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동승자를 지시하는 클래스 값이라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터상기 핸드 제스처의 수행 주체를 판단하는 단계;연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 영상의 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계; 및상기 핸드 제스처의 수행 주체에 따라 상기 핸드 제스처의 종류에 대응하는 타겟 동작을 상기 차량의 컴포넌트공개특허 10-2021-0057358-5-중 타겟 컴포넌트가 수행하도록 제어하는 단계; 를 포함하는제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 타겟 컴포넌트 및 상기 타겟 동작은 매칭 테이블에 기초하여 설정되되,상기 매칭 테이블은 상기 운전자의 제스처와 그에 대응되는 컴포넌트 및 동작을 포함하는 제1 매칭 테이블 및상기 동승자의 제스처와 그에 대응되는 컴포넌트 및 동작을 포함하는 제2 매팅 테이블을 포함하는,제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방법으로서,깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계;싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동승자를 지시하는 클래스 값이라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터상기 핸드 제스처의 수행 주체를 판단하는 단계;상기 핸드 제스처의 수행 주체가 운전자인 경우 수행되며, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터상기 핸드 제스처의 종류를 식별하는 단계; 및상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계; 를 포함하는제스처 인식 방법."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 관찰 영역(FoV: Field of View)에 대한 제스처 인식을 수행하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치로, 카메라에 의해 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입 력받는 2D CNN 모델로 제공되는 인공 신경망을 이용하여 트리거링 이벤트의 발생을 모니터링하는 단계, 상기 트 리거링 이벤트가 발생하면 제스처 인식 모드를 실행하는 단계를 포함하되, 상기 제스처 인식 모드는 연속되는 이 미지들을 입력받는 3D CNN 모델의 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되 는 제2 인공 신경망을 이용하여 상기 특정 제스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 제스처의 종류를 식별하는 단계 및 상기 제스처의 종류에 기초하여 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동 작을 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법 및 이를 수행하는 인식 장치에 관한 발명이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치에 관한 것으로, 보다 상세하게는 관찰 영역을 촬영하여 획득한 이미지 또는 영상을 이용하여 관찰 영역에서 수행된 제스처를 인식하고 인식된 제스처에 대응 하는 명령을 수행하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치에 관한 것이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사용자의 편의를 위해 별도의 조작 없이 장치나 장비를 제어하는 기술의 대표적인 예로 음성 인식 기술 및 제스 처 인식 기술을 들 수 있다. 이 중 제스처 인식 기술 분야는 사용자의 신체 일부를 이용하여 근거리 또는 원거 리에서 장치나 장비를 제어하여 원하는 기능을 수행하는 것을 목적으로 하며, 그 과정에서 제스처의 인식 정확 도 및 인식 속도를 향상시키는 데에 연구가 집중되고 있다. 최근 이미지 또는 영상을 분석하는 데에 있어 갖는 기계 학습 모델 또는 인공 지능 기술이 정확도 및 속도 측면 에서 강점을 가지면서 대세적인 분석 기술로 자리잡고 있으며, 이러한 기계 학습 모델 또는 인공 지능은 이미지 또는 영상 분석 기술과 밀접한 관련이 있는 제스처 인식 기술 분야에서 역시 각광 받고 있다.다만 종래에는 센서에 의해 획득된 제스처를 포함한 이미지 또는 영상 전체에 대해 기계 학습 모델 또는 인공 지능을 이용하여 제스처 인식을 수행함으로써, 여전히 인식 속도가 느리고 제스처 인식 시스템이 효율적이지 못 한 한계점을 가지고 있다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명이 해결하고자 하는 일 과제는, 제스처 정보를 포함하는 이미지 또는 영상의 적어도 일부를 분석하여 제 스처를 인식하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 연속되는 이미지 데이터를 이용하여 제스처를 인식하는 데에 있어 트리 거링 이벤트를 포함하는 이미지의 후속 이미지들로부터 제스처를 인식하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 제스처 수행 주체를 구분하고 제스처 수행 주체에 따라 제스처 인식 결 과를 다르게 설정하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 제스처가 포함된 이미지 또는 영상을 분석하는 과정에서 도출되는 중간 데이터에 대해 트리거링 이벤트 감지 및/또는 제스처 수행 주체 판단을 수행하는 제스처 인식 방법 및 이를 수 행하는 제스처 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 일 과제는, 트리거링 이벤트 감지 및/또는 제스처 수행 주체 감지를 위해 제스처 인 식에 이용되는 인공 신경망과 다른 모델의 인공 신경망을 이용하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치를 제공하는 것이다. 본 발명이 해결하고자 하는 과제가 상술한 과제로 제한되는 것은 아니며, 언급되지 아니한 과제들은 본 명세서"}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 2, "content": "및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 명세서의 일 양상에 따르면, 본 명세서의 일 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자 의 핸드 제스처를 인식하는 방법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸 드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 특정 핸드 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과 관련된 제1 클래스 값이 라벨링된 학습 용 데이터 셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 특정 핸 드 포스처를 검출함으로써, 트리거링 이벤트의 발생을 모니터링하는 단계 및 상기 트리거링 이벤트가 발생하면, 제스처 인식 모드를 실행하는 단계를 포함하되, 상기 제스처 인식 모드는 연속되는 이미지들을 입력받는 3D CNN 모델의 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋 으로 학습된 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임 들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포 넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될 수 있 다. 본 명세서의 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥 티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 태깅된 학습용 데이터셋으로 학습된 제1 인공 신경망을 제공하 는 단계, 상기 제1 인공 신경망의 상기 3D CNN 모델이 상기 RNN 모델과 연결되는 상기 맥스 풀링 레이어 또는 상기 풀리 커넥티드 레이어를 입력 레이어로 하고, 특정 핸드 포스처를 포함하는 깊이 이미지에 트리거링 이벤 트의 발생과 관련된 제1 클래스 값이 라벨링되고 상기 특정 핸드 포스처와 상이한 핸드 포스처를 포함하는 깊이 이미지에 상기 제1 클래스 값과 상이한 제2 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망 을 제공하는 단계, 상기 영상이 상기 제1 인공 신경망에 입력됨에 따라 도출되는 상기 맥스 풀링 레이어 또는상기 풀리 커넥티드 레이어의 노드값으로부터, 상기 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처를 검출 함으로써, 트리거링 이벤트의 발생을 모니터링하는 단계, 상기 트리거링 이벤트가 발생하면, 상기 제1 인공 신 경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스 처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될 수 있다. 본 발명의 또 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링 되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동 승자를 지시하는 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 핸드 제스처의 수행 주체를 판단하는 단계, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸 드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학 습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 영상의 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 수행 주체에 따라 상기 핸드 제스처의 종류에 대응하는 타겟 동작 을 상기 차량의 컴포넌트 중 타겟 컴포넌트가 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될 수 있다. 본 발명의 또 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링 되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동 승자를 지시하는 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 핸드 제스처의 수행 주체를 판단하는 단계, 상기 핸드 제스처의 수행 주체 가 운전자인 경우 수행되며, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티 드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상 기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하 여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 핸드 제스처 인식 방법이 제공될 수 있다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 발명의 과제의 해결 수단이 상술한 해결 수단들로 제한되는 것은 아니며, 언급되지 아니한 해결 수단들은 본"}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "명세서 및 첨부된 도면으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 의하면, 제스처가 포함된 이미지 또는 영상 중 필요한 부분에 대해 제스처 인식을 수행함으로써 불필 요한 데이터를 처리하는 것을 방지하여 제스처 인식 시스템의 효율 및 강건성이 향상될 수 있다.본 발명에 의하 면, 제스처 수행 주체에 대한 판단을 통해 제스처 인식 결과를 달리함으로써 제스처 인식 알고리즘의 다양성을 확보할 수 있다. 본 발명에 의하면, 트리거링 이벤트 감지 및/또는 제스처 수행 주체 판단에 각각 적합한 인공 신경망을 이용함 으로써 제스처 인식 시스템의 전체 처리 속도가 향상될 수 있다. 본 발명에 의하면, 제스처 인식 과정 중에 도출된 데이터를 이용하여 트리거링 이벤트 감지 및/또는 제스처 수 행 주체 판단함으로써 새로운 알고리즘을 도입하는 것보다 처리하는 데이터량이 감소하여 상대적으로 적은 데이 터를 이용하여 제스처 인식 알고리즘의 다양성을 확보할 수 있다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "본 발명의 효과가 상술한 효과들로 제한되는 것은 아니며, 언급되지 아니한 효과들은 본 명세서 및 첨부된 도면"}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 3, "content": "으로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확히 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 상술한 목적, 특징들 및 장점은 첨부된 도면과 관련된 다음의 상세한 설명을 통해 보다 분명해질 것 이다. 다만, 본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예들을 가질 수 있는 바, 이하에서는 특정 실시예들을 도면에 예시하고 이를 상세히 설명하고자 한다. 본 명세서에 기재된 실시예는 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명의 사상을 명 확히 설명하기 위한 것이므로, 본 발명이 본 명세서에 기재된 실시예에 의해 한정되는 것은 아니며, 본 발명의 범위는 본 발명의 사상을 벗어나지 아니하는 수정예 또는 변형예를 포함하는 것으로 해석되어야 한다. 본 명세서에 첨부된 도면은 본 발명을 용이하게 설명하기 위한 것으로 도면에 도시된 형상은 본 발명의 이해를 돕기 위하여 필요에 따라 과장되어 표시된 것일 수 있으므로 본 발명이 도면에 의해 한정되는 것은 아니다. 본 발명과 관련된 공지 기능 혹은 구성에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 본 명세서의 설명 과정에서 이용되는 숫자(예를 들어, 제1, 제2 등)는 하나의 구성요소를 다른 구성요소와 구분하기 위한 식별기호에 불과하다. 또한, 이하의 설명에서 사용되는 구성요소에 대한 접미사 \"유닛\", \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 본 명세서의 일 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방법 으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬 영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 특정 핸드 포스처를 포함하는 깊 이 이미지에 트리거링 이벤트의 발생과 관련된 제1 클래스 값이 라벨링된 학습용 데이터 셋으로 학습된 제1 인 공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 특정 핸드 포스처를 검출함으로써, 트리 거링 이벤트의 발생을 모니터링하는 단계 및 상기 트리거링 이벤트가 발생하면, 제스처 인식 모드를 실행하는 단계를 포함하되, 상기 제스처 인식 모드는 연속되는 이미지들을 입력받는 3D CNN 모델의 풀링 레이어 또는 풀 리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미 지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될 수 있다. 여기서, 상기 학습용 데이터셋은 상기 특정 핸드 포스처와 상이한 핸드 포스처를 포함하는 깊이 이미지에 상기 제1 클래스 값과 상이한 제2 클래스 값이 라벨링된 데이터셋을 포함할 수 있다. 또 여기서, 미리 설정된 시간이 경과하면 상기 제스처 인식 모드를 종료하는 단계가 더 포함될 수 있다. 또 여기서, 상기 타켓 컴포넌트의 동작이 수행되는 상기 핸드 제스처의 종류가 식별되는 경우 상기 제스처 인식 모드를 종료하는 단계가 더 포함될 수 있다. 또 여기서, 상기 RNN 모델은 LSTM 모델일 수 있다. 또 여기서, 상기 타겟 컴포넌트에서 수행되는 상기 타겟 동작은 상기 핸드 무브먼트에 대응될 수 있다. 또 여기서, 상기 제스처는 상기 타겟 컴포넌트 종류에 따라 미리 정해진 포스처 및 상기 타겟 동작의 속성에 대 응하는 이동 속도를 갖는 무브먼트를 포함할 수 있다. 본 명세서의 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥 티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 태깅된 학습용 데이터셋으로 학습된 제1 인공 신경망을 제공하 는 단계, 상기 제1 인공 신경망의 상기 3D CNN 모델이 상기 RNN 모델과 연결되는 상기 맥스 풀링 레이어 또는 상기 풀리 커넥티드 레이어를 입력 레이어로 하고, 특정 핸드 포스처를 포함하는 깊이 이미지에 트리거링 이벤 트의 발생과 관련된 제1 클래스 값이 라벨링되고 상기 특정 핸드 포스처와 상이한 핸드 포스처를 포함하는 깊이 이미지에 상기 제1 클래스 값과 상이한 제2 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망 을 제공하는 단계, 상기 영상이 상기 제1 인공 신경망에 입력됨에 따라 도출되는 상기 맥스 풀링 레이어 또는 상기 풀리 커넥티드 레이어의 노드값으로부터, 상기 제2 인공 신경망을 이용하여 상기 특정 핸드 포스처를 검출 함으로써, 트리거링 이벤트의 발생을 모니터링하는 단계, 상기 트리거링 이벤트가 발생하면, 상기 제1 인공 신 경망을 이용하여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스 처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될 수 있다. 여기서, 상기 트리거링 이벤트가 발생하고 미리 설정된 시간이 경과하면 상기 핸드 제스처의 종류를 식별하는 단계를 종료할 수 있다. 또 여기서, 상기 RNN 모델은 LSTM 모델일 수 있다. 또 여기서, 상기 타겟 컴포넌트에서 수행되는 상기 타겟 동작은 상기 핸드 무브먼트에 대응될 수 있다. 또 여기서, 상기 제스처는 상기 타겟 컴포넌트 종류에 따라 미리 정해진 포스처 및 상기 타겟 동작의 속성에 대 응하는 이동 속도를 갖는 무브먼트를 포함할 수 있다. 본 발명의 또 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링 되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동 승자를 지시하는 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 핸드 제스처의 수행 주체를 판단하는 단계, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸 드 제스처를 포함하는 연속되는 깊이 이미지들에 상기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학 습용 데이터셋으로 학습된 제2 인공 신경망을 이용하여 상기 영상의 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 수행 주체에 따라 상기 핸드 제스처의 종류에 대응하는 타겟 명령 을 상기 차량의 컴포넌트 중 타겟 컴포넌트가 수행하도록 제어하는 단계를 포함하는 제스처 인식 방법이 제공될수 있다. 여기서, 상기 타겟 컴포넌트 및 상기 타겟 동작은 매칭 테이블에 기초하여 설정되되, 상기 매칭 테이블은 상기 운전자의 제스처와 그에 대응되는 컴포넌트 및 동작을 포함하는 제1 매칭 테이블 및 상기 동승자의 제스처와 그 에 대응되는 컴포넌트 및 동작을 포함하는 제2 매팅 테이블을 포함할 수 있다. 본 발명의 또 다른 양상에 따르면, 차량의 컴포넌트를 제어하기 위한 차량 탑승자의 핸드 제스처를 인식하는 방 법으로서, 깊이 카메라에 의해 핸드 포스처와 핸드 무브먼트로 정의되는 상기 핸드 제스처가 수행되는 공간을 촬영한 영상을 획득하는 단계, 싱글 이미지를 입력받는 2D CNN 모델로 제공되고, 상기 차량 탑승자 중 운전자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 운전자를 지시하는 클래스 값이 라벨링 되고 상기 차량 탑승자 중 동승자에 의해 수행된 핸드 제스처의 핸드 포스처를 포함하는 깊이 이미지에 상기 동 승자를 지시하는 클래스 값이 라벨링된 학습용 데이터셋으로 학습된 제1 인공 신경망을 이용하여 상기 영상에 포함된 이미지 프레임으로부터 상기 핸드 제스처의 수행 주체를 판단하는 단계, 상기 핸드 제스처의 수행 주체 가 운전자인 경우 수행되며, 연속되는 이미지들을 입력받는 3D CNN 모델의 맥스 풀링 레이어 또는 풀리 커넥티 드 레이어에 RNN 모델이 연결되는 형태로 제공되고, 상기 핸드 제스처를 포함하는 연속되는 깊이 이미지들에 상 기 핸드 제스처의 종류를 지시하는 클래스값이 라벨링된 학습용 데이터셋으로 학습된 제2 인공 신경망을 이용하 여 상기 특정 핸드 포스처가 검출된 이미지 프레임의 후속 이미지 프레임들로부터 상기 핸드 제스처의 종류를 식별하는 단계 및 상기 핸드 제스처의 종류에 기초하여 상기 차량의 컴포넌트 중 타겟 컴포넌트가 타겟 동작을 수행하도록 제어하는 단계를 포함하는 핸드 제스처 인식 방법이 제공될 수 있다. 본 명세서는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치에 관한 것으로, 보다 상세하게는 관찰 영역 을 촬영하여 획득한 이미지 또는 영상을 이용하여 관찰 영역에서 수행된 제스처를 인식하고 인식된 제스처에 대 응하는 명령을 수행하는 제스처 인식 방법 및 이를 수행하는 제스처 인식 장치에 관한 것이다. 여기서, 제스처(gesture)는 일반적으로 사용자의 신체 일부분의 형태 또는 동작을 의미할 수 있다. 이 때, 이용 되는 사용자의 신체 부위에 따라 제스처는 핸드 제스처(hand gesture), 풋 제스처(foot gesture) 또는 포인팅 제스처(pointing gesture) 등을 포함할 수 있다. 또한, 제스처는 사용자의 신체가 일정 시간 움직이지 않는 포 스처(posture), 특정한 경로를 따라 움직이는 무브먼트(movement) 및 포스처와 무브먼트의 조합을 포함할 수 있 다. 예를 들어, 핸드 제스처는 핸드 포스처 및 핸드 무브먼트를 포함할 수 있다. 제스처에 관한 보다 상세한 설 명은 후술하도록 한다. 여기서, 관찰 영역(FoV: Field of View)은 제스처가 인식될 수 있는 영역으로, 센서 또는 장비 등에 의해 이미 지 또는 영상 데이터가 획득되는 영역을 의미할 수 있다. 관찰 영역에 대한 구체적인 설명은 후술하도록 한다. 여기서, 제스처 인식(gesture recognition)은 사용자의 제스처에 대한 이미지 또는 영상 데이터를 분석하여 컴 포넌트의 기능을 수행하는 것을 의미할 수 있다. 여기서, 컴포넌트는 그 컴포넌트가 속하는 장치, 장비 또는 건 물 등에서 특정 기능을 수행할 수 있다. 예를 들어, 차량 내 컴포넌트는 차량 내 냉난방기 제어, 미디어 제어, 창문, 차문 등의 기계적 구조 제어 등을 수행할 수 있다. 또 다른 예로, 가전 제품 컴포넌트는 가전 제품의 각 종 기능 제어 등을 수행할 수 있다. 한편, 제스처 인식은 사용자의 제스처를 감지하여 해당 제스처를 특정 기준 에 따라 분류하여 의미를 부여하는 제스처 이미지 분석을 의미할 수도 있다. 이하에서는, 도 1을 참조하여 본 명세서의 일 실시예에 따라 제스처 인식을 수행하기 위한 제스처 인식 시스템 에 대하여 서술하도록 한다. 도 1은 본 명세서의 일 실시예에 따른 제스처 인식 시스템을 도시한 도면이다. 도 1을 참조하면, 제스처 감지 유닛을 통해 관찰 영역에 대한 데이터가 획득되고, 컴포넌트 제어 유닛은 관찰 영역에 대한 데이터 를 분석하여 컴포넌트를 제어함으로써 제스처 인식을 수행하되, 제스처 인식 과정에서 서버가 이용될 수 있다. 제스처 감지 유닛은 관찰 영역에 대한 데이터를 획득할 수 있다. 예를 들어, 제스처 감지 유닛은 라이다(LiDAR: Light Detection And Ranging 또는 Laser Imaging, Detection And Ranging) 장치, 레이더 (RaDAR: Radio Detection And Ranging) 장치, 깊이 카메라 및 RGB 카메라 중 적어도 하나를 포함할 수 있다. 여기서, 관찰 영역에 대한 데이터는 관찰 영역에 전파, 광 등을 조사하여 획득한 비행 시간(ToF: Time of Flight) 데이터, 깊이(depth) 데이터, 세기(intensity) 데이터, 2차원 또는 3차원 이미지 데이터 및 영상 데이 터 중 적어도 하나를 포함할 수 있다. 관찰 영역에 대한 데이터는 일정 시간 동안 획득된 데이터의 집합을 의미할 수 있다. 예를 들어, 제스처 감지 유닛은 주기적으로 일정 시간 동안 관찰 영역에 대한 데이터를 획득하도록 설정될 수 있고, 이 때 획득되 는 관찰 영역에 대한 데이터는 관찰 영역에 대응되며 연속되는 이미지 프레임들의 집합인 영상 데이터를 포함할 수 있다. 제스처 감지 유닛은 컴포넌트 제어 유닛과 유/무선으로 통신할 수 있다. 제스처 감지 유닛은 컴포넌트 제어 유닛으로 관찰 영역에 대한 데이터를 제공할 수 있다. 제스처 감지 유닛은 서버에 관찰 영역에 대한 데이터를 제공할 수 있다. 제스처 감지 유닛은 컴포넌트 제어 유닛 및 서버 중 적어도 하나와 통신하기 위해 통신 모듈 을 포함할 수 있다. 컴포넌트 제어 유닛은 관찰 영역에 대한 제스처 인식을 수행할 수 있다. 구체적으로, 컴포넌트 제어 유닛 은 제스처 감지 유닛으로부터 획득한 관찰 영역에 대한 데이터를 가공하여 이미지 또는 영상 데이 터를 획득할 수 있다. 이 때, 컴포넌트 제어 유닛이 제스처 감지 유닛으로부터 이미지 데이터 또는 영상 데이터를 직접 획득할 수도 있다. 컴포넌트 제어 유닛은 관찰 영역에 대한 데이터를 분석하여 컴포넌트를 제어하는 명령을 생성함으로써 제 스처 인식을 수행할 수 있다. 구체적으로, 컴포넌트 제어 유닛은 제스처 감지 유닛이 사용자의 제 스처를 촬영한 데이터를 이용하여 이미지 또는 영상 데이터를 획득하고, 획득한 이미지 또는 영상 데이터를 분 석하여 사용자의 제스처를 판별하고 판별된 제스처에 대응하여 컴포넌트를 제어할 수 있다. 컴포넌트 제어 유닛은 서버와 통신할 수 있다. 구체적으로, 컴포넌트 제어 유닛은 서버 로부터 컴포넌트 제어 명령을 수신하여 컴포넌트를 제어할 수도 있다. 서버는 제스처 감지 유닛 또는 컴포넌트 제어 유닛을 대신하여 제스처 인식을 수행할 수 있 다. 구체적으로, 서버는 제스처 감지 유닛으로부터 관찰 영역에 대한 데이터를 획득하여 컴포넌트 제어 명령어를 생성하고 컴포넌트 제어 유닛에 생성한 컴포넌트 제어 명령어를 제공하거나 컴포넌트 각각 에 제공할 수 있다. 서버는 제스처 감지 유닛 및 컴포넌트 제어 유닛과 통신할 수 있다. 구체적으로, 서버(160 0)는 제스처 감지 유닛으로부터 관찰 영역에 대한 데이터를 획득할 수 있다. 또한, 서버는 제스처 감지 유닛으로부터 획득한 관찰 영역에 대한 데이터를 분석하여 컴포넌트 제어 명령어를 생성할 수 있다. 제스처 인식 과정에서 서버가 이용됨에 따라 제스처 감지 유닛 및 컴포넌트 제어 유닛에 의 한 제스처 인식 속도가 보다 향상될 수 있다. 이상에서 설명한 제스처 인식 시스템은 물리적으로 단일한 장치로 제공되거나 복수의 장치로 제공될 수 있 다. 예를 들어, 제스처 감지 유닛과 컴포넌트 제어 유닛은 물리적으로 통합된 단일한 제스처 인식 장치로 제공될 수 있다. 또 다른 예로, 제스처 감지 유닛과 컴포넌트 제어 유닛은 분리된 복수의 장치로 제공될 수 있다. 이 때, 관찰 영역에 대한 데이터 생성, 가공 및 처리와 관찰 영역에 대한 제스처 인식은 제스처 감지 유닛 , 컴포넌트 제어 유닛 및 서버 중 적어도 어느 하나에서 수행될 수 있다. 예를 들어, 제스처 감지 유닛에서 관찰 영역에 대한 데이터를 가공하여 관찰 영역에 대한 이미지 또는 영상을 컴포넌트 제어 유닛에 제공하고, 컴포넌트 제어 유닛은 획득한 이미지 또는 영상 데이터를 이용하여 관찰 영역에 대한 제스처 인식을 수행할 수 있다. 또 다른 예로, 제스처 감지 유닛은 관찰 영역에 대한 데이터를 가공 및 이용하여 제스처 인식을 수행할 수 있으며 컴포넌트 제어 유닛을 통해 컴포넌트를 제어할 수 있다. 또 다른 예로, 서버는 제스처 감지 유닛으로부터 관찰 영역에 대한 데이터를 획득하여 제스처 인식을 수행할 수 있다. 이하에서는, 도 2를 참조하여 본 명세서의 일 실시예에 따른 컴포넌트 제어 유닛에 대하여 서술한다. 도 2는 본 명세서의 일 실시예에 따른 컴포넌트 제어 유닛에 관한 블록도이다. 도 2를 참조하면, 컴포넌 트 제어 유닛은 제어 모듈, 컴포넌트, 입력 모듈, 출력 모듈, 통신 모듈 및 메모리를 포함할 수 있다. 통신 모듈은 외부 기기와 통신을 수행할 수 있다. 컴포넌트 제어 유닛은 통신 모듈을 통해 제스처 감지 유닛이나 서버와 데이터 송수신을 할 수 있다. 여기서, 통신 모듈은 유/무선 방 식을 포함할 수 있다. 메모리는 각종 정보를 저장할 수 있다. 메모리에는 컴포넌트 제어 유닛을 구동하기 위한 운 용 프로그램이나 컴포넌트 제어 유닛의 각 구성을 동작시키기 위한 프로그램을 비롯해 컴포넌트 제어 유 닛의 동작에 필요한 각종 데이터가 임시적으로 또는 반영구적으로 저장될 수 있다. 예를 들어, 메모리 에는 관찰 영역에 대한 데이터를 가공 및 처리하기 위한 프로그램 및 데이터 분석을 위한 인공 신경망이 저장될 수 있다. 메모리는 컴포넌트 제어 유닛에 내장되는 형태나 탈부착 가능한 형태로 제공될 수 있다. 입력 모듈은 사용자로부터 사용자 입력을 수신할 수 있다. 사용자 입력은 키 입력, 터치 입력, 음성 입력 을 비롯한 다양한 형태로 이루어 질 수 있다. 입력 모듈은 전통적인 형태의 키패드나 물리적 버튼, 키보 드, 마우스는 물론, 사용자의 터치를 감지하는 터치 센서 및 그 외의 다양한 형태의 사용자 입력을 감지하거나 입력 받는 다양한 형태의 입력 수단을 모두 포함하는 포괄적인 개념이다. 출력 모듈은 각종 정보를 출력해 사용자에게 이를 제공할 수 있다. 출력 모듈은 영상을 출력하는 디스플레이, 소리를 출력하는 스피커, 진동을 발생시키는 햅틱 장치 및 그 외의 다양한 형태의 출력 수단을 모 두 포함하는 포괄적인 개념이다. 제어 모듈은 컴포넌트 제어 유닛의 전반적인 동작을 제어할 수 있다. 예를 들어, 제어 모듈 은 메모리로부터 데이터 가공 및 분석을 위한 프로그램을 로딩하여 제스처 감지 유닛으로부터 획득 한 데이터를 가공 및 분석하고 그 결과를 출력 모듈을 통해 사용자에게 제공하거나 해당 컴포넌트 의 기능을 수행하도록 제어 신호를 생성할 수 있다. 컴포넌트는 미리 설정된 기능이나 동작을 수행하는 장비, 장치, 모듈, 기기 또는 기구 등을 포함할 수 있 다. 컴포넌트는 설정된 기능이나 동작에 따라 제1 내지 제n 컴포넌트를 포함할 수 있다. 컴포넌트 는 제어 모듈에 의해 각각 설정된 기능이나 동작을 수행할 수 있다. 이를 위해 각각의 컴포넌트는 제어 모듈과 유/무선 통신할 수 있다. 후술하듯이 컴포넌트는 인식되는 제스처에 따라 제어될 수 있다. 한편, 컴포넌트 중 적어도 일부는 컴포넌트 제어 유닛으로부터 물리적으로 분리되어 제공될 수 있 다. 예를들어, 컴포넌트는 컴포넌트 제어 유닛으로부터 분리되어 제공되되, 각각 별도의 제어 모듈 을 가지고, 컴포넌트 제어 유닛으로부터 제어 신호를 획득하여 동작할 수도 있다. 또는, 컴포넌트 제어 유닛은 하나의 컴포넌트를 갖되 복수 개로 제공될 수 있다. 컴포넌트 제어 유닛은 별도의 전원부를 가지거나 유선 혹은 무선으로 외부로부터 전원을 공급받을 수 있 으며 전원부를 제어하는 스위치를 별도로 가질 수 있다. 컴포넌트 제어 유닛은 제스처 인식을 수행하기 위해 빅데이터(big data), 기계 학습 모델(machine learning model), 인공 지능(artificial intelligence) 또는 인공 신경망(ANN: Artificial Neural Network) 등 의 기술을 이용할 수 있다. 예를 들어, 컴포넌트 제어 유닛은 기계 학습된 프로그램을 구동하여 관찰 영 역에 대한 제스처 인식을 수행할 수 있다. 컴포넌트 제어 유닛에서 제스처 인식이 수행되는 예들에 대한 보다 구체적인 설명은 후술하도록 한다. 이하에서는, 도 3을 참조하여 본 명세서의 일 실시예에 따라 제스처 인식 시스템이 활용되는 방법에 대해 서술한다. 도 3은 본 명세서의 일 실시예에 따른 차량에 탑재된 제스처 인식 시스템에 관한 도면이다. 도 3을 참조하 면, 제스처 인식 시스템은 차량 내 관찰 영역에 대한 제스처 인식을 수행할 수 있다. 제스처 감지 유닛은 차량 내부에 탑재되어 관찰 영역에 대한 데이터를 획득할 수 있다. 구체적으로, 제스처 감지 유닛은 차량 천장 또는 차량 내 리어 뷰 미러(rear view mirror)에 탑재되어 차량 내부를 관 찰 영역으로 하여 제스처 인식을 위한 데이터를 획득할 수 있다. 관찰 영역은 다양한 형상으로 형성될 수 있다. 예를 들어, 관찰 영역은 제스처 감지 유닛으로 부터 일정 거리 이격된 가상 평면 또는 가상 곡면을 포함할 수 있다. 또 다른 예로, 관찰 영역은 제스처 감지 유닛 으로부터 제1 거리 이격된 면과 제2 거리 이격된 면 사이의 가상 공간을 포함할 수 있다. 또 다른 예로, 관찰 영역은 제스처 감지 유닛으로부터 미리 설정된 수평 각도와 미리 설정된 수직 각도 및 이격 거리에 의해 정의되는 임의의 면 또는 공간을 포함할 수 있다. 차량 내에서 관찰 영역은 다양한 범위로 설정될 수 있다. 예를 들어, 관찰 영역은 차량 내 탑승자의 제스처를 인식하기 위해 기어를 중심으로 형성되거나 차량 내 운전석과 조수석 사이 공간에 형성될 수 있다. 또 다른 예를 들어, 관찰 영역은 차량 내 컴포넌트로부터 일정 거리 이내에 형성될 수도 있다. 또 다른 예를 들어, 관찰 영역은 차량 내 각 좌석을 중심으로 형성되거나 차량 내부 전체에 대해 형성될 수도 있다. 이상에서는 차량 내부에서 관찰 영역이 형성되는 것으로 서술하였으나, 본 발명의 사상이 이에 한정되는 것은 아니며 관찰 영역은 차량 외부, 핸드폰과 같은 단말기로부터 일정 거리 이내 등 제스처가 인식될 수 있는 공간에 형성될 수 있음은 물론이다. 컴포넌트 제어 유닛은 다양한 형태로 차량 내 탑재될 수 있다. 예를 들어, 컴포넌트 제어 유닛은 대시 보드(dash board)에 내장되어 차량 내 서로 다른 위치에 배치된 각각의 컴포넌트를 유/무선 방식으 로 제어할 수 있다. 또 다른 예를 들어, 차량의 각 컴포넌트 마다 컴포넌트 제어 유닛이 제공될 수 있다. 또 다른 예로, 서버에서 컴포넌트를 제어하여 컴포넌트 제어 유닛은 일부 구성이 생략 된 상태로 차량 내 탑재될 수도 있다. 이하에서는, 도 4 내지 도 6을 참조하여 본 명세서의 일 실시예에 따른 제스처 인식 시스템에서 이용되는 인공 신경망에 대하여 서술한다. 제스처 인식 시스템에서 제스처 인식을 수행하기 위해 인공 신경망이 이용될 수 있다. 구체적으로, 컴포넌 트 제어 유닛은 제스처 감지 유닛으로부터 획득한 관찰 영역에서의 사용자의 제스처에 대한 데이터 및 인공 신경망을 이용하여 사용자의 제스처를 판별하고 그 결과를 기초로 컴포넌트를 제어할 수 있다. 인공 신경망의 대표적인 예로는 데이터를 입력받는 입력 레이어(input layer), 결과를 출력하는 출력 레이어 (output layer) 및 입력 레이어와 출력 레이어 사이에서 데이터를 처리하는 히든 레이어(hidden layer)를 포함 하는 딥 러닝(deep learning) 계열의 인공 신경망이 있다. 인공 신경망의 세부적인 예시들로는, 심층 신경망 (DNN: Deep Neural Network), 합성곱 신경망(CNN: Convolution Neural Network) 및 순환 신경망(Recurrent Neural Network) 등이 있으며, 본 명세서에서 인공 신경망은 상술된 인공 신경망, 그 외의 다양한 형태의 인공 신경망 및 이들이 조합된 형태의 인공 신경망을 모두 포함하는 포괄적인 의미로 해석되어야 하며, 반드시 딥 러 닝 계열이어야만 하는 것도 아니다. 도 4는 본 명세서의 일 실시예에 따른 인공 신경망을 학습시키는 방법에 관한 도면이다. 도 4를 참조하면, 인공 신경망은 라벨링 데이터를 학습 데이터로 이용하여 학습하며, 학습 과정에서 출력 데이 터와 라벨링 데이터의 오차가 고려될 수 있다. 여기서, 라벨링 데이터(labeling data)는 입력 값과 출력 값을 매칭시켜 놓은 학습용 데이터셋(data set)을 의 미할 수 있다. 구체적으로, 라벨링 데이터는 특정 목적을 가지고 입력 값으로부터 특정 출력 값을 도출하고자 할 때, 해당 입력 값과 희망하는 출력 값 또는 해당 입력 값과 인과 관계가 있는 출력 값을 서로 매칭시킨 데이 터를 포함할 수 있다. 보다 구체적으로, 제스처 인식 시스템에서 라벨링 데이터는 관찰 영역에 대한 데이터와 해당 관찰 영역에 포함된 제스처 의미를 임의로 판별한 결과 데이터를 서로 매칭 시킨 데이터를 포함할 수 있다. 다른 예로, 라벨링 데이터는 관찰 영역에 대한 이미지 또는 영상 데이터와 해당 이미지 또는 영상 내 제스처의 종류 또는 특징 등을 임의로 판단한 결과 데이터를 서로 매칭 시킨 데이터를 포함할 수 있다. 한편, 라벨링 데이터는 제스처 인식 시스템을 위해 직접 생성 또는 가공될 수도 있으나 외부의 공개된 데 이터베이스로부터 획득될 수도 있다. 인공 신경망은 라벨링 데이터로부터 학습 데이터를 입력 받아 출력 데이터를 생성하고 출력 데이터와 라벨링 데 이터를 비교한 오차 역전파를 이용하여 학습될 수 있다. 구체적으로, 인공 신경망은 입력 데이터로부터 출력 데 이터를 생성하기 위해 복수의 계수(가중치, weight)를 갖는 적어도 하나의 히든 레이어를 포함하고, 오차 역전 파를 이용하여 계수들을 변경함으로써 학습될 수 있다. 도 5는 본 명세서의 일 실시예에 따른 인공 신경망의 구조를 도시한 도면이다. 도 5를 참조하면, 도 4에서 서술 한 바와 같이 학습된 인공 신경망은 입력 데이터를 입력 레이어의 입력 값으로 하여 출력 레이어를 통해 결과 값을 산출할 수 있다. 구체적으로, 본 명세서의 일 실시예에 따르면 제스처 인식 시스템에서 인공 신경망은 제스처에 대한 이미 지 또는 영상 값을 입력 레이어의 입력 값으로 제공 받아 출력 레이어를 통해 제스처 인식 결과를 출력 값으로 획득할 수 있다. 제스처 인식 결과는 인공 신경망의 구현 형태에 따라 다양하게 도출될 수 있다. 예를 들어, 인공 신경망이 이진 분류기(binary classification) 형태로 제스처 인식 결과를 출력하는 경우에는 출력 레이어에 하나 또는 두 개의 출력 노드가 포함될 수 있다. 이진 분류기 형태로 결과 값을 출력하는 인공 신경망은 주로 제스처의 주체가 특정 인물에 해당하는지 여부를 판단할 수 있다. 또 다른 예를 들어, 인공 신경망이 다중 분류기(multi classification) 형태로 제스처 인식 결과를 출력하는 경 우에는 출력 레이어는 복수 개의 출력 노드를 포함할 수 있다. 다중 분류기 형태로 결과값을 출력하는 인공 신 경망은 주로 제스처가 어떤 종류의 제스처에 해당하는지 여부를 판단할 수 있다. 한편, 다시 도 5를 참조하면, 제스처 인식 시스템에서 제스처 인식을 수행하기 위해 순환 신경망이 제공될 수 있다. 순환 신경망은 적어도 하나의 히든 레이어를 포함하되, 적어도 하나의 히든 레이어는 출력 값을 다시 입력 값이 되는 구조로 구현될 수 있다. 인공 신경망에 입력되는 입력 데이터가 순차적이거나 연속성을 갖는 경우 인공 신 경망은 순환 신경망으로 구현될 수 있다. 순환 신경망의 히든 레이어가 순환하는 구조로 구현됨으로써 순환 신 경망은 입력 데이터의 순서를 고려하여 학습될 수 있고 연속성을 갖거나 순차적인 데이터를 분석하는 데에 정확 도가 높아지는 등의 효과가 발생할 수 있다. 도 6은 본 명세서의 일 실시예에 따른 연속된 이미지를 분석하기 위한 인공 신경망의 구조를 도시한 도면이다. 도 6을 참조하면, 제스처 인식 시스템에서 제스처 인식을 수행하기 위해 합성곱 신경망 및 순환 신경망이 제공될 수 있다. 합성곱 신경망은 일반적으로 합성곱 레이어(convolution layer), 풀링 레이어(pooling layer) 및 전연결 레이어 (FC layer: Fully Connected layer)를 포함할 수 있다. 합성곱 신경망은 입력 데이터 내 인접 데이터 사이의 특징을 고려하여 출력 데이터를 생성하도록 학습될 수 있다. 구체적으로, 합성곱 신경망은 이미지 데이터를 입 력 데이터로 하여 합성곱 레이어 및 풀링 레이어를 통해 입력 데이터로부터 특징 값들을 추출하고 전연결 레이 어를 통해 특징 값들을 연결하며 소프트맥스 레이어(softmax layer)를 이용하여 결과 값을 출력할 수 있다. 제스처 인식 시스템에서 제스처 인식이 수행되기 위해 합성곱 신경망과 순환 신경망이 결합되어 제공될 수 있다. 다시 도 6을 참조하면, 합성곱 신경망에서 입력 데이터로부터 추출된 특징 값들이 전연결 레이어에서 서 로 연결되고 순환 신경망은 전연결 레이어로부터 입력 데이터를 받아 출력 데이터를 생성할 수 있다. 여기서, 순환 신경망은 LSTM(Long-Short-Term Memory) 블록을 포함할 수 있다. LSTM 블록은 일반적인 순환 신 경망에서 기억하거나 저장하고자 하는 데이터의 범위를 제한할 수 있다. 제스처 인식 시스템에 있어서 상술한 합성곱 신경망과 순환 신경망의 조합이 제공됨으로써 이미지 또는 영 상 데이터로 제공되는 포스처 및 무브먼트를 포함하는 제스처가 보다 효율적이고 정확하게 인식될 수 있다. 상술한 인공 신경망의 구조 또는 인공 신경망을 포함하는 알고리즘이 제스처 인식 시스템에서 활용되는 방 법에 대해서는 추후에 보다 구체적으로 서술하도록 한다. 이하에서는 도 7 및 도 8을 참조하여 본 명세서의 일 실시예에 따른 제스처가 인식되는 방법에 대하여 서술한다. 제스처 인식 방법에 대해 설명하기에 앞서 제스처 인식 시스템에서 인식하고자 하는 제스처의 종류에 대해 먼저 서술하도록 한다. 제스처는 사용자의 움직임이 포함되지 않은 포스처와 사용자의 움직임이 포함되는 무브먼트를 포함할 수 있다. 예를 들어, 사용자가 관찰 영역에 대해 손 또는 발과 같은 신체의 일부를 위치하고 움직이지 않는 경우 제 스처 인식 결과 포스처로 구분될 수 있다. 또 다른 예를 들어, 사용자가 관찰 영역 내에서 손 또는 발 등 의 신체의 일부를 특정 경로를 따라 움직이는 경우 제스처 인식 결과 무브먼트로 구분될 수 있다. 포스처는 제스처 인식 시스템에서 제스처를 인식하기 위한 트리거링 이벤트(triggering event) 감지 또는 제스처 수행 주체 판별에 이용될 수 있다. 예를 들어, 제스처 인식 시스템은 컴포넌트 제어를 위한 제스처를 인식하기에 앞서 미리 설정된 포스처를 트리거링 이벤트로 검출하고, 트리거링 이벤트가 검출되면 제 스처 윈도우(gesture window)를 실행하여 제스처 감지를 시작할 수 있다. 또 다른 예를 들어, 제스처 인식 시스 템은 차량 내 관찰 영역에서 감지된 포스처가 운전자의 포스처인지 동승자의 포스처인지 여부를 판단 할 수 있다. 여기서, 제스처 윈도우는 제스처 인식 시스템에서 사용자의 제스처를 인식하기 위한 구간 또는 시간을 의 미할 수 있다. 예를 들어, 트리거링 이벤트가 발생한 후 미리 설정된 시간 동안 무브먼트와 같은 연속성을 갖는 제스처를 감지하고 인식하는 프로그램이 실행됨으로써 제스처 윈도우가 실행될 수 있다. 컴포넌트 제어 유닛은 제스처 윈도우를 실행함으로써 제스처 인식 모드로 진입할 수 있다. 이와 같이 컴 포넌트 제어 유닛이 상시 제스처 인식 모드에 있지 않고 트리거링 이벤트가 발생한 경우에 제스처 인식 모드로 진입함으로써 컴포넌트 제어 유닛에서 불필요한 데이터 처리가 방지되고 컴포넌트 제어 유닛 에서 이루어지는 제스처 인식 프로세스의 효율 및 강건성이 향상될 수 있다. 여기서, 제스처 인식 모드는 컴포넌트를 제어하기 위한 제스처를 인식하기 위해 제스처 인식 시스템(10 0)이 진입하는 모드를 의미할 수 있다. 이 때, 제스처 인식 시스템이 제스처 인식 모드에 진입할지 여부를 판단하기 위해 트리거링 이벤트 발생 여부가 이용될 수 있다. 제스처 인식 시스템은 제스처 인식 모드 진 입 후 제스처 인식을 위한 프로그램을 실행할 수 있으며, 제스처 인식 모드 진입 전에는 트리거링 이벤트 감지 를 위한 프로그램을 실행할 수 있다. 보다 구체적으로, 제스처 인식 시스템은 제스처 인식 모드 진입 전 후 다른 프로그램 또는 다른 인공 신경망으로 구현된 프로그램을 이용할 수 있다. 제스처 윈도우는 오프닝 제스처(opening gesture) 및 클로징 제스처(closing gesture)에 의해 시작과 끝 시점이 설정될 수 있다. 예를 들어, 상술한 포스처에 의해 트리거링 이벤트가 검출된 시점에 제스처 윈도우가 실행되고 미리 설정된 클로징 제스처가 감지되면 제스처 윈도우가 중단되고 제스처 인식 모드가 종료될 수 있다. 여기서, 오프닝 제스처 및 클로징 제스처는 같은 인공 신경망 또는 다른 인공 신경망에 의해 감지될 수 있다. 또는, 제스처 윈도우는 제스처 및 미리 설정된 시간에 의해 중단 시점이 설정될 수 있다. 또는, 제스처 윈도우는 미리 설정된 제스처 인식 개수에 의해 중단 시점이 설정될 수 있다. 예를 들어, 제스처 윈도우가 실행 된 후 컴포넌트를 제어하는 제스처가 인식된 경우 제스처 윈도우가 중단될 수 있다. 또는, 제스처 윈도우가 실행된 상태에서 미리 설정된 시간 동안 제스처 인식 시스템에 의해 어떠한 제스처 도 인식되지 않는 경우 제스처 윈도우는 중단되고 제스처 인식 모드가 종료될 수 있다. 무브먼트는 제스처 인식 시스템에서 컴포넌트를 제어하는 데에 이용될 수 있다. 구체적으로, 상술한 포스처에 의해 트리거링 이벤트가 검출되어 제스처 윈도우가 실행되고, 컴포넌트 제어 유닛은 제스처 윈 도우가 실행되는 동안 사용자에 의해 수행된 무브먼트를 인식하여 해당 무브먼트에 대응되는 컴포넌트의 기능을 수행하거나 동작시킬 수 있다. 한편, 본 발명에서 상술한 바와 같이 포스처와 무브먼트가 트리거링 이벤트 검출이나 제스처 수행 주체 판단, 컴포넌트에 이용되는 것으로 한정되는 것은 아니다. 예를 들어, 무브먼트가 제스처 윈도우를 실행하는 트 리거링 이벤트 발생 여부를 판단하는 데에 이용될 수 있으며 포스처 역시 컴포넌트를 제어 하기 위해 인 식되는 제스처로 이용될 수 있음은 물론이다. 다시 말해, 트리거링 이벤트 검출, 제스처 수행 주체 판단 및 타 겟 컴포넌트 설정 또는 타겟 동작 설정이나 수행과 같은 컴포넌트 제어 등에 포스처, 무브먼트 또는 이들 의 조합이 이용될 수도 있다.도 7은 본 명세서의 일 실시예에 따른 제스처 인식 방법을 도시한 순서도이다. 도 8은 본 명세서의 일 실시예에 따른 제스처와 컴포넌트 매칭을 나타내는 표에 관한 도면이다. 도 7을 참조하면, 제스처 인식 방법은 관찰 영역에 대한 이미지를 획득하는 단계(S1100), 트리거링 이벤트 검출 단계(S1200), 제스처 윈도우 실행 단계(S1300), 제스처 인식 수행 단계(S1500), 타겟 컴포넌트 및 타겟 동 작 결정 단계(S1500) 및 타겟 동작 수행 단계(S1600)를 포함할 수 있다. 이하에서는 상술한 각 단계들에 대해서 보다 구체적으로 설명한다. 제스처 감지 유닛은 관찰 영역에 대한 이미지를 획득할 수 있다(S1100). 여기서, 이미지는 관찰 영 역내 평면 좌표에 대응되는 픽셀 값들의 집합인 2D 이미지 및 관찰 영역내 공간 좌표에 대응되는 픽 셀 값들의 집합인 3D 이미지를 포함할 수 있다. 예를 들어, 제스처 감지 유닛은 일정한 주기로 관찰 영역 을 촬영하여 2D 이미지 데이터를 획득하여 컴포넌트 제어 유닛에 제공할 수 있다. 또 다른 예로, 제 스처 감지 유닛은 관찰 영역에 대해 조사한 광을 수신하여 관찰 영역을 구성하는 포인트들 각 각에 대한 획득한 비행 시간(ToF: Time of Flight) 데이터를 컴포넌트 제어 유닛에 제공하고, 컴포넌트 제어 유닛은 획득한 비행 시간 데이터를 기초로 포인트 클라우드(point cloud) 또는 깊이 이미지(depth map)를 획득할 수 있다. 컴포넌트 제어 유닛은 획득한 이미지를 기초로 트리거링 이벤트가 발생하였는 여부를 판단할 수 있다 (S1200). 구체적으로, 컴포넌트 제어 유닛은 획득한 이미지를 인공 신경망의 입력 데이터로 하여 이미지 에 포함된 제스처를 형상이나 의미를 분석하고, 그 결과에 기초하여 트리거링 이벤트가 발생하였는지 여부를 판 단할 수 있다. 컴포넌트 제어 유닛은 트리거링 이벤트가 발생하면 제스처 윈도우를 실행할 수 있다(S1300). 다만, 컴포 넌트 제어 유닛은 제스처를 포함하는 트리거링 이벤트 외에 물리적 버튼 입력 또는 음성 입력 등의 다른 수단을 이용하여 제스처 윈도우를 실행할 수도 있다. 한편, 제스처 인식 시스템에서 주기적으로 관찰 영역에 대한 데이터를 획득하여 제스처 인식을 수행 하는 경우, 트리거링 이벤트 검출 단계(S1200) 및 제스처 윈도우 실행 단계(S1300)는 생략될 수 있다. 제스처 감지 유닛 및 컴포넌트 제어 유닛은 제스처 윈도우 실행 중에 제스처 인식을 수행할 수 있 다(S1500). 구체적으로, 제스처 윈도우가 실행되면 제스처 감지 유닛은 주기적으로 관찰 영역에 대 한 데이터를 획득하여 컴포넌트 제어 유닛에 제공하고, 컴포넌트 제어 유닛은 획득한 데이터를 기 초로 관찰 영역에서 수행되는 제스처의 형상이나 종류를 판단할 수 있다. 컴포넌트 제어 유닛은 제스처 인식 결과에 기초하여 타겟 컴포넌트 및 타겟 동작을 결정할 수 있다 (S1500). 제스처의 종류 또는 유형에 따라 타겟 컴포넌트 및 타겟 동작이 설정될 수 있다. 이를 위해, 제스처 종류, 타겟 컴포넌트 및 타겟 동작을 포함하는 매칭 테이블이 제공될 수 있다. 예를 들어, 도 8을 참조하면, 검지를 펴고 좌에서 우로 이동하거나 위 아래로 왕복 운동하는 제스처의 경우 '핸드폰'이 타겟 컴포넌트가 되고 '착신 또는 무시'가 타겟 동작으로 결정될 수 있다. 또 다른 예로, 검지를 펴고 정지한 제스처의 경우 '미디어 플레이어'가 타겟 컴포넌트가 되고 '재생 또는 일시 정지'가 타겟 동작으로 설정될 수 있다. 또 다른 예로, 검지를 펴고 시 계 방향 또는 반시계 방향으로 원을 그리는 제스처의 경우 '미디어 플레이어'가 타겟 컴포넌트가 되고 '음량 조 절'이 타겟 동작으로 설정될 수 있다. 본 발명에서 제스처와 컴포넌트 매칭이 도 8에 도시된 매칭 테이블에 한정되는 것은 아니며, 매칭 테이블 은 제스처의 수행 주체에 따라 복수의 매칭 테이블이 제공될 수 있으며, 사용자가 임의로 제스처와 컴포넌트 를 매칭시키는 등의 프로그램 설계도 가능함은 물론이다. 여기서, 제스처의 종류 또는 유형은 제스처가 포함하는 무브먼트의 경로, 방향, 속도와 같은 무브먼트의 특성에 기초하여 식별될 수 있으며, 무브먼트의 특성에 따라 타겟 컴포넌트 및 타겟 동작의 속성이 설정될 수도 있다. 예를 들어, 다시 도 8을 참조하면, 검지를 펴고 시계 방향 또는 반시계 방향으로 원을 그리는 무브먼트를 포함 하는 제스처에 의해 '미디어 플레이어'가 타겟 컴포넌트로 설정되고, '음량 조절'이 타겟 동작으로 설정되는 경 우, 무브먼트의 속도가 증가하면 미디어 플레이어의 음량이 커지는 속도가 증가하고 무브먼트의 방향에 따라 미 디어 플레이어의 음량이 감소하거나 증가할 수 있다.한편, 컴포넌트 제어 유닛은 제스처 인식 외에 물리적 버튼이나 음성 인식 등을 통해 타겟 컴포넌트를 설 정할 수 있고, 제스처 인식을 통해 타겟 동작을 수행할 수도 있다. 컴포넌트 제어 유닛은 인식된 제스처에 매칭된 타겟 동작에 기초하여 타겟 컴포넌트를 동작시킬 수 있다 (S1600). 이하에서는 도 9 및 도 10을 참조하여 본 명세서의 일 실시예에 따른 트리거링 이벤트 감지 방법에 대해서 서술 한다. 도 9 및 도 10은 본 명세서의 일 실시예에 따른 트리거링 이벤트를 감지하는 네트워크를 도시한 순서도이다. 트리거링 이벤트 감지는 제스처 인식 시스템에서 제스처 인식을 위해 이용되는 인공 신경망의 적어도 일부 에서 수행될 수 있다. 도 9를 참조하면, 제스처 인식 시스템에서 이용되는 제스처 인식 네트워크는 3차원 합성곱 신경망(3D CNN), 전연결 레이어, 순환 신경망을 포함하며 트리거링 이벤트 감지를 위해 2차원 합성곱 신 경망(2D CNN)이 더 포함될 수 있다. 이하에서는 설명의 편의를 위해 제스처 인식 시스템에서 깊이 이미지 를 이용하고, 제스처 인식 네트워크가 상술한 구성으로 구현되는 것으로 서술하지만 본 발명의 사상이 이에 한 정되는 것은 아니며 깊이 이미지가 아닌 관찰 영역에 대한 데이터가 이용될 수 있고, 제스처 인식 네트워 크는 상술한 구성 외에 다른 종류의 인공 신경망으로 구성되거나 그 구성 순서가 변경될 수 있음을 미리 밝혀둔 다. 또한, 이하에서 서술하는 깊이 이미지는 경우에 따라 관찰 영역에 대한 단일(single) 이미지이거나 관찰 영역에 대한 연속된 이미지들의 집합을 의미할 수 있다. 예를 들어, 깊이 이미지가 2차원 합성곱 신경망 또는 심층 신경망에 입력되는 경우 단일 이미지로 인식될 수 있으며 깊이 이미지가 3차원 합성곱 신경망에 입력 되는 경우 깊이 이미지는 특정 시간 동안 연속되는 이미지들의 집합을 의미할 수 있다. 다시 도 9를 참조하면, 제스처 인식 네트워크는 사용자 제스처에 대한 깊이 이미지를 획득하여 3차원 합성곱 신 경망을 이용하여 깊이 이미지의 특징 값을 추출하고, 전연결 레이어를 이용하여 연결한 깊이 이미지의 특징 값 들을 순환 신경망에 입력 데이터로 입력하여 제스처 종류를 판단하는 출력 데이터를 획득하고, 판단된 제스처 종류에 기초하여 컴포넌트를 제어하는 명령어를 생성할 수 있다. 여기서, 다시 도 9를 참조하면 제스처 인식 시스템은 깊이 이미지를 분석하기 위해 3차원 합성곱 신경망에 입력하기에 앞서 깊이 이미지를 2차원 합성곱 신경망에 입력하여 트리거링 이벤트를 감지하고, 트리거링 이벤트 가 감지된 경우 3차원 합성곱 신경망을 이용하여 제스처 인식을 수행할 수 있다. 여기서, 2차원 합성곱 신경망은 특정 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과 관련된 제1 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 여기서, 2차원 합성곱 신경망을 학습시키기 위한 라벨링 데이터는 제1 클래스 값이 라벨링된 포스처와는 다른 특정 포스처를 포함하는 깊이 이미지에 제2 클래스 값이 라벨링된 라벨링 데이터를 더 포함할 수 있다. 한편, 2차원 합성곱 신경망을 대신하여 이미지 분석 프로그램이 이용될 수도 있다. 예를 들어, 제스처 인식 시 스템은 이미지와 레퍼런스(reference) 데이터를 비교하여 이미지가 포함하는 제스처가 트리거링을 위한 특 정 포스처인지 여부를 판단할 수도 있다. 여기서, 3차원 합성곱 인공 신경망 또는 순환 신경망은 제스처를 포함하는 연속되는 깊이 이미지들에 제스처의 종류를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 제스처 인식 시스템은 도 9에 도시된 제스처 인식 네트워크를 이용하여 관찰 영역에 대한 연속적인 깊이 이미지를 획득하고 트리거링 이벤트를 감지한 시점 이후에 제스처 인식 모드로 진입하여 트리거링 이벤트 가 감지된 깊이 이미지 이후에 입력되는 깊이 이미지들에 대해 제스처 인식을 수행할 수 있다. 이와 같이 제스처 인식 시스템은 사용자의 제스처를 인식하는 데에 있어서 트리거링 이벤트를 검출한 경우 에만 제스처 인식을 수행하여 처리해야하는 데이터를 선택함으로써 제스처 인식 프로세스의 강건성 및 효율을 향상시킬 수 있고, 깊이 이미지로부터 트리거링 이벤트를 감지하는 데에 3차원 합성곱 신경망보다 상대적으로 연산량이 적은 2차원 합성곱 신경망을 이용함으로써 데이터 처리 속도를 향상시킬 수 있다. 한편, 도 10을 참조하면 제스처 인식 네트워크는 3차원 합성곱 신경망, 전연결 레이어 및 순환 신경망을 포함하 되, 전연결 레이어를 입력 데이터로 하여 트리거링 이벤트를 감지하는 심층 신경망을 포함할 수 있다. 심층 신경망은 3차원 합성곱 신경망에 의해 깊이 이미지로부터 추출된 특징 값들에 기초하여 깊이 이미지가 특 정 포스처 또는 무브먼트를 포함하는지 여부를 판단함으로써 트리거링 이벤트를 감지할 수 있다. 여기서, 심층 신경망은 특정 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과 관련된 제1 클래스 값 이 라벨링되고 다른 특정 포스처를 포함하는 깊이 이미지에 제2 클래스 값이 라벨링된 라벨링 데이터를 이용하 여 학습된 인공 신경망을 포함할 수 있다. 여기서, 3차원 합성곱 신경망 또는 순환 신경망은 제스처를 포함하는 연속되는 깊이 이미지들에 제스처의 종류 를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 제스처 인식 시스템은 도 10에 도시된 제스처 인식 네트워크를 이용하여 순환 신경망에서 연속된 깊이 이 미지를 분석하기 앞서 트리거링 이벤트를 감지하여 순환 신경망의 동작 여부를 결정할 수 있다. 이와 같이 제스처 인식 시스템은 순환 신경망을 이용하여 연속적인 깊이 이미지들을 분석하기에 앞서 트리 거링 이벤트를 감지하여 불필요한 데이터 처리 과정을 생략할 수 있고 3차원 합성곱 신경망에 의해 깊이 이미지 로부터 추출된 특징 값들을 이용하여 트리거링 이벤트를 감지하므로 부가적인 데이터 가공을 생략할 수 있으며, 순환 신경망 보다 상대적으로 연산 속도가 빠른 심층 신경망을 이용함으로써 결과적으로 보다 신속하게 제스처 인식을 수행할 수 있다. 이하에서는 도 11 및 도 12를 참조하여 본 명세서의 일 실시예에 따른 제스처 수행 주체를 판단하는 방법에 대 해 서술한다. 도 11은 본 명세서의 일 실시예에 따른 차량 내 제스처 수행 주체를 도시한 도면이다. 도 12는 본 명세서의 일 실시예에 따른 제스처 수행 주체를 고려하여 제스처와 컴포넌트 매칭을 나타내는 표에 관한 도면이다. 제스처의 수행 주체는 복수일 수 있다. 예를 들어, 도 11을 참조하면 차량 내 제스처는 운전석에 탑승한 운전자 와 보조석에 탑승한 동승자에 의해 수행될 수 있다. 또 다른 예로, 차량 내 제스처는 앞좌석인 운전석 및 보조 석뿐만 아니라 뒷좌석에 탑승한 동승자에 의해 수행될 수도 있다. 제스처 인식 시스템은 제스처의 수행 주체에 따라 제스처 인식을 수행하거나 수행하지 않을 수 있다. 또는, 제스처 인식 시스템은 제스처의 수행 주체에 따라 다른 컴포넌트를 제어하거나 같은 컴포넌트 의 다른 기능을 수행하거나 동작 시킬 수 있다. 제스처 인식 시스템은 제스처 수행 주체에 따른 제스처 인식을 위해 제스처 수행 주체를 고려하여 제스처 와 타겟 컴포넌트 및 타겟 동작을 매칭 시킨 매칭 테이블이 이용될 수 있다. 예를 들어, 도 12를 참조하면 검지 를 편 손을 일정 각도 구부린 상태에서 좌측 또는 우측으로 움직이는 핸드 제스처는 제스처의 주체와 상관 없이 '라디오'가 타겟 컴포넌트로 설정되고 '채널 조절'이 타겟 동작으로 설정될 수 있다. 또 다른 예로, 엄지를 위 로 편 핸드 제스처는 '오픈'이 타겟 동작으로 설정되되, 제스처 수행 주체가 운전자인 경우 '운전석 창문'이 타 겟 컴포넌트로 설정되고 제스처 수행 주체가 동승자인 경우 '보조석 창문'이 타겟 컴포넌트로 설정될 수 있다. 이와 같은 매칭 테이블은 운전자의 제스처, 그와 매칭되는 타겟 컴포넌트 및 타겟 동작으로 구성된 적어도 하나 의 매칭 데이터를 포함하는 제1 매칭 테이블 및 동승자의 제스처, 그와 매칭되는 타겟 컴포넌트 및 타겟 동작으 로 구성된 적어도 하나의 매칭 데이터를 포함하는 제2 매팅 테이블을 포함할 수 있다. 본 발명에서 제스처 수행 주체를 고려한 제스처와 컴포넌트 매칭이 도 12에 도시된 표에 한정되는 것은 아니며, 사용자가 임의로 제스처와 컴포넌트를 매칭시키는 등의 프로그램 설계도 가능함은 물론이다. 한편, 컴포넌트 제어 유닛은 제스처 인식 외에 물리적 버튼이나 음성 인식 등을 통해 타겟 컴포넌트를 설 정할 수 있고, 제스처 인식을 통해 타겟 동작을 수행할 수도 있다. 이하에서는 도 13 내지 도 16을 참조하여 본 명세서의 일 실시예에 따른 제스처 수행 주체 감지 방법에 대해서 서술한다.도 13 내지 도 16은 본 명세서의 일 실시예에 따른 제스처 수행 주체를 인식하는 네트워크를 도시한 순서도이다. 도 13 내지 도 16에 도시된 제스처 수행 주체를 인식하는 네트워크의 구성은 별도의 언급이 없는 경우 앞서 도 9에서 서술한 내용이 동일하게 적용될 수 있다. 제스처 수행 주체 감지는 제스처 인식 시스템에서 제스처 인식을 위해 이용되는 인공 신경망의 적어도 일 부에서 수행될 수 있다. 도 13을 참조하면, 제스처 인식 시스템에서 이용되는 제스처 인식 네트워크는 3차 원 합성곱 신경망, 전연결 레이어, 순환 신경망을 포함하며 제스처 수행 주체 감지를 위해 2차원 합성곱 신경망 이 더 포함될 수 있다. 다시 도 13을 참조하면, 제스처 인식 시스템은 깊이 이미지를 입력 데이터로 하는 2차원 합성곱 신경망을 이용하여 깊이 이미지에 포함된 제스처의 수행 주체를 판단하고, 그 결과에 기초하여 컴포넌트를 제어하 는 명령어를 생성하지 않을 수 있다. 한편, 제스처 인식 시스템은 깊이 이미지를 이용하여 제스처의 수행 주체를 판단하고 제스처 수행 주체가 컴포넌트 제어 자격이 없는 경우 제스처 인식 모드로 진입하지 않을 수도 있다. 여기서, 2차원 합성곱 신경망은 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 제1 주체를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 예를 들어, 2 차원 합성곱 신경망은 차량 탑승자 중 운전자에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 운전자 를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 또는, 2차원 합성곱 신경망은 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 제1 주체를 지시하는 클래스 값이 라벨링되고, 제2 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 제2 주 체를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 예를 들 어, 2차원 합성곱 신경망은 차량 탑승자 중 운전자에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 운전자를 지시하는 클래스 값이 라벨링되고 차량 탑승자 중 동승자에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 동승자를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함 할 수 있다. 한편, 2차원 합성곱 신경망을 대신하여 이미지 분석 알고리즘이 이용될 수도 있다. 예를 들어, 제스처 인식 시 스템은 이미지와 레퍼런스(reference) 데이터를 비교하여 이미지가 포함하는 제스처가 운전자의 제스처인 지 여부를 판단할 수도 있다. 도 14를 참조하면 제스처 인식 시스템은 제스처 수행 주체에 따라 다른 명령어를 생성할 수 있다. 예를 들 어, 제스처 인식 시스템은 깊이 이미지를 획득하고 2차원 합성곱 신경망을 이용하여 깊이 이미지에 포함된 제스처의 수행 주체를 판단하고 판단된 제스처 수행 주체 및 제스처 수행 주체가 고려된 제스처 및 컴포넌트 매칭 테이블 및 깊이 이미지들을 분석한 제스처 인식 결과에 기초하여 컴포넌트를 제어하는 명령 어를 생성할 수 있다. 여기서, 제스처 인식 네트워크에서 수행되는 제스처 인식 및 제스처 수행 주체 판단은 동시에 또는 순차적으로 수행될 수 있다. 한편, 도 15 및 도 16을 참조하면 제스처 인식 네트워크는 전연결 레이어를 입력 데이터로 하여 제스처 수행 주 체를 감지하는 심층 신경망을 포함할 수 있다. 여기서, 심층 신경망은 3차원 합성곱 신경망에 의해 깊이 이미지로부터 추출된 특징 값들로부터 깊이 이미지가 특정 포스처 또는 무브먼트를 포함하는지 여부에 기초하여 제스처 수행 주체를 감지할 수 있다. 여기서, 심층 신경망은 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 제1 주체를 지시하 는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 예를 들어, 심층 신 경망은 운전자에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 운전자를 지시하는 클래스 값이 라벨 링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 한편, 다시 도 16을 참조하면 심층 신경망은 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지 에 제1 주체를 지시하는 클래스 값이 라벨링되고 제2 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이 미지에 제2 주체를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수있다. 예를 들어, 심층 신경망은 차량 탑승자 중 운전자에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미 지에 운전자를 지시하는 클래스 값이 라벨링되고 차량 탑승자 중 동승자에 의해 수행된 제스처의 포스처를 포함 하는 깊이 이미지에 동승자를 지시하는 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 제스처 인식 시스템은 도 13 내지 도 16에 도시된 제스처 인식 네트워크를 이용하여 컴포넌트 제어 를 위한 명령어를 생성함에 있어 제스처 수행 주체를 고려할 수 있다. 나아가 제스처 수행 주체가 고려된 컴포넌트 제어는 제스처 인식 시스템에서 복수의 제스처 수행 주 체를 감지하는 경우에도 각 제스처에 대응되는 타겟 컴포넌트가 설정되고 타겟 동작이 수행될 수 있어 복수의 사용자가 차량을 이용하며 동시에 제스처를 수행하는 경우에도 제스처 인식 기능을 제공할 수 있다. 이와 같이 본 발명에서 제스처 인식 시스템은 제스처 수행 주체를 고려하여 제스처 인식을 수행하고, 보다 복잡하고 다양한 제스처 인식 시스템의 구현이 가능해져 제스처 인식 기술 분야에서 다양하게 활용될 수 있다. 이하에서는 도 17 내지 도 22를 참조하여 상술한 트리거링 이벤트를 감지하는 네트워크 및 제스처 수행 주체를 감지하는 네트워크를 혼합한 하이브리드 네트워크에 대하여 서술한다. 도 17 내지 도 22은 본 명세서의 일 실시예에 따른 트리거링 이벤트 감지 및 제스처 수행 주체를 인식하는 네트 워크를 도시한 순서도이다. 도 17 내지 도 22에 도시된 하이브리드 네트워크의 구성은 별도의 언급이 없는 경우 앞서 도 9에서 서술한 내용 이 동일하게 적용될 수 있다. 도 17을 참조하면, 제스처 인식 시스템에서 이용되는 하이브리드 네트워크는 깊이 이미지 분석을 위한 3차 원 합성곱 신경망 및 순환 신경망, 트리거링 이벤트 감지를 위한 2차원 합성곱 신경망 및 제스처 수행 주체 감 지를 위한 심층 신경망을 포함할 수 있다. 보다 구체적으로, 제스처 인식 시스템은 2차원 합성곱 신경망을 이용하여 사용자의 제스처가 포함된 연속 된 깊이 이미지 중 적어도 일부에 대해 트리거링 이벤트를 감지하고, 트리거링 이벤트가 감지되는 경우 연속된 깊이 이미지에 대해 3차원 합성곱 신경망 및 순환 신경망을 이용하여 제스처 종류를 구분하되, 전연결 레이어에 서 추출된 특징 값들을 입력으로 하는 심층 신경망을 이용하여 제스처의 수행 주체를 판단하고, 제스처의 수행 주체가 운전자인 경우에만 컴포넌트 제어를 위한 명령어를 생성할 수 있다. 이 때, 2차원 합성곱 신경망은 도 9에서 서술한 바와 같이 특정 포스처에 대한 라벨링 데이터를 이용하여 학습 된 인공 신경망을 포함하고, 3차원 합성곱 신경망 또는 순환 신경망은 제스처 종류와 관련된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함하며, 심층 신경망은 도 15에서 서술한 바와 같이 제스처 수행 주체에 관련 된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 한편, 도 18을 참조하면 전연결 레이어에서 추출된 특징 값들을 입력으로 하는 심층 신경망은 도 16에서 서술한 바와 같이 제스처 수행 주체를 구분하도록 학습된 인공 신경망을 포함하고, 이로써 제스처 인식 시스템은 제스처 수행 주체를 고려하여 제스처와 타겟 컴포넌트 및 타겟 동작을 매칭시켜 놓은 매칭 테이블 및 인공 신경 망에 의해 판단된 제스처 종류를 이용하여 명령어를 생성할 수 있다. 이와 같이 제스처 인식을 수행함에 있어서 트리거링 이벤트를 감지하고 제스처 수행 주체를 판단하거나 구분함 으로써 제스처 인식 시스템이 다양하게 구현되어 활용도가 높아지고 제스처 인식 알고리즘 또는 프로세스 의 강건성 및 정확도가 향상되며, 별도의 데이터 처리 방법을 이용하는 것보다 제스처 인식 속도도 향상될 수 있다. 도 19를 참조하면, 제스처 인식 시스템에서 이용되는 하이브리드 네트워크는 깊이 이미지 분석을 위한 2차 원 합성곱 신경망, 트리거링 이벤트 감지를 위한 제1 심층 신경망, 제스처 수행 주체 감지를 위한 제2 심층 신 경망을 포함할 수 있다. 보다 구체적으로, 제스처 인식 시스템은 2차원 합성곱 신경망과 제1 심층 신경망을 이용하여 깊이 이미지 로부터 트리거링 이벤트 발생 여부를 판단할 수 있고 트리거링 이벤트가 감지된 경우 3차원 합성곱 신경망 및 순환 신경망을 이용하여 깊이 이미지에 대한 제스처 인식을 수행할 수 있고, 2차원 합성곱 신경망과 제2 심층신경망을 이용하여 제스처 수행 주체를 판단하여 제스처 수행 주체가 특정 주체, 예를 들어 운전자인 경우 컴포 넌트를 제어하는 명령어를 생성할 수 있다. 이 때, 제1 심층 신경망은 특정 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생과 관련된 클래스 값 이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있고, 제2 심층 신경망은 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 제1 주체를 지시하는 클래스 값이 라벨링된 라벨링 데 이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 한편, 도 20을 참조하면 제2 심층 신경망은 도 16에서 서술한 바와 같이 제스처 수행 주체를 구분하도록 학습된 인공 신경망을 포함하고, 이로써 제스처 인식 시스템은 제스처 수행 주체를 고려하여 제스처와 타겟 컴포 넌트 및 타겟 동작을 매칭시켜 놓은 매칭 테이블 및 인공 신경망에 의해 판단된 제스처 종류를 이용하여 명령어 를 생성할 수 있다. 도 21을 참조하면, 제스처 인식 시스템에서 이용되는 하이브리드 네트워크는 깊이 이미지로부터 트리거링 이벤트를 감지하고 제스처 수행 주체를 감지하는 2차원 합성곱 신경망 및 소프트맥스 레이어를 포함할 수 있다. 여기서, 소프트맥스 레이어는 깊이 이미지에 포함된 제스처가 트리거링 이벤트에 해당하는지 여부 및 깊이 이미 지에 포함된 제스처의 수행 주체를 판단할 수 있다. 소프트맥스 레이어는 2차원 합성곱 신경망에 포함될 수 있 다. 보다 구체적으로, 2차원 합성곱 신경망은 특정 포스처 및 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 트리거링 이벤트의 발생 및 제1 주체를 지시하는 제1 클래스 값이 라벨링 되고, 특정 포스처를 포함하지 않고 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 트리거링 이벤트가 발생되지 않음 및 제1 주체를 지시하는 제2 클래스 값이 라벨링 되고, 특정 포스처를 포함하되 제1 주체에 의해 수행된 제스처의 포스처를 포함하지 않는 깊이 이미지에 트리거링 이벤트의 발생 및 제1 주체가 아님을 지시하는 제3 클래스 값이 라벨링 되고, 특정 포스처 및 제1 주체에 의해 수행된 제스처의 포스처를 모두 포함하지 않는 깊이 이미지에 트리거링 이벤트의 미발생 및 제1 주체가 아님을 지시하는 제4 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 제스처 인식 시스템은 2차원 합성곱 신경망에 의해 도출된 제스처 인식 결과에 기초하여 트리거링 이벤트 가 발생하고 제스처 수행 주체가 특정 주체인 경우 컴포넌트를 제어하는 명령어를 생성할 수 있다. 또는, 제스처 인식 시스템은 2차원 합성곱 신경망에 의해 도출된 제스처 인식 결과에 기초하여 트리거링 이벤트가 감지되면 3차원 합성곱 신경망 및 순환 신경망을 이용하여 제스처 인식을 수행하고, 제스처 수행 주체 가 특정 주체인 경우 컴포넌트를 제어하는 명령어를 생성할 수 있다. 한편, 2차원 합성곱 신경망은 트리거링 이벤트 및 특정 주체 구분과 관련된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수도 있다. 이 때, 제스처 인식 시스템은 제스처 수행 주체와 타겟 컴포넌트 및 타 겟 동작을 매칭시킨 제스처 매칭 테이블을 고려하여 컴포넌트 제어를 위한 명령어를 생성할 수 있다. 도 22를 참조하면 제스처 인식 시스템에서 이용되는 하이브리드 네트워크는 깊이 전연결 레이어로부터 입 력 데이터를 받아 깊이 이미지로부터 트리거링 이벤트를 감지하고 제스처 수행 주체를 감지하는 심층 신경망 및 소프트맥스 레이어를 포함할 수 있다. 여기서, 소프스맥스 레이어는 심층 신경망에 포함될 수 있다. 보다 구체적으로, 심층 신경망은 특정 포스처 및 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이 미지에 트리거링 이벤트의 발생 및 제1 주체를 지시하는 제1 클래스 값이 라벨링 되고, 특정 포스처를 포함하지 않고 제1 주체에 의해 수행된 제스처의 포스처를 포함하는 깊이 이미지에 트리거링 이벤트가 발생되지 않음 및 제1 주체를 지시하는 제2 클래스 값이 라벨링 되고, 특정 포스처를 포함하되 제1 주체에 의해 수행된 제스처의 포스처를 포함하지 않는 깊이 이미지에 트리거링 이벤트의 발생 및 제1 주체가 아님을 지시하는 제3 클래스 값 이 라벨링 되고, 특정 포스처 및 제1 주체에 의해 수행된 제스처의 포스처를 모두 포함하지 않는 깊이 이미지에 트리거링 이벤트의 미발생 및 제1 주체가 아님을 지시하는 제4 클래스 값이 라벨링된 라벨링 데이터를 이용하여 학습된 인공 신경망을 포함할 수 있다. 제스처 인식 시스템은 심층 신경망에 의해 도출된 제스처 인식 결과에 기초하여 트리거링 이벤트가 발생하 고 제스처 수행 주체가 특정 주체인 경우 컴포넌트를 제어하는 명령어를 생성할 수 있다. 한편, 심층 신경망은 트리거링 이벤트 및 특정 주체 구분과 관련된 라벨링 데이터를 이용하여 학습된 인공 신경 망을 포함할 수도 있다. 이 때, 제스처 인식 시스템은 제스처 수행 주체와 타겟 컴포넌트 및 타겟 동작을매칭시킨 제스처 매칭 테이블을 고려하여 컴포넌트 제어를 위한 명령어를 생성할 수 있다. 도 19 내지 도 22에서 서술한 바와 같이 제스처 인식을 수행함에 있어서 공통의 인공 신경망을 이용하거나 다중 분류기 형태의 인공 신경망을 이용하여 트리거링 이벤트를 감지하고 제스처 수행 주체를 판단하거나 구분하는 경우 하이브리드 네트워크의 구조가 단순해짐으로써 제스처 인식 시스템에서 수행되는 일련의 제스처 인식 프로세스 또는 알고리즘의 정확도 및 강건성이 향상되고 나아가 제스처 인식 속도가 향상될 수 있다. 실시예에 따른 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그램 명령 형태로 구현되어 컴퓨터 판 독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명령, 데이터 파일, 데이터 구조 등 을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 실시예를 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수도 있다. 컴퓨터 판 독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체 (magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도 록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 실시예의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도 록 구성될 수 있으며, 그 역도 마찬가지이다."}
{"patent_id": "10-2019-0144026", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이상과 같이 실시예들이 비록 한정된 실시예와 도면에 의해 설명되었으나, 해당 기술분야에서 통상의 지식을 가 진 자라면 상기의 기재로부터 다양한 수정 및 변형이 가능하다. 예를 들어, 설명된 기술들이 설명된 방법과 다 른 순서로 수행되거나, 및/또는 설명된 시스템, 구조, 장치, 회로 등의 구성요소들이 설명된 방법과 다른 형태 로 결합 또는 조합되거나, 다른 구성요소 또는 균등물에 의하여 대치되거나 치환되더라도 적절한 결과가 달성될 수 있다. 그러므로, 다른 구현들, 다른 실시예들 및 특허청구범위와 균등한 것들도 후술하는 특허청구범위의 범위에 속한 다."}
{"patent_id": "10-2019-0144026", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 명세서의 일 실시예에 따른 제스처 인식 시스템을 도시한 도면이다. 도 2는 본 명세서의 일 실시예에 따른 컴포넌트 제어 유닛에 관한 블록도이다. 도 3은 본 명세서의 일 실시예에 따른 차량에 탑재된 제스처 인식 시스템에 관한 도면이다. 도 4는 본 명세서의 일 실시예에 따른 인공 신경망을 학습시키는 방법에 관한 도면이다. 도 5는 본 명세서의 일 실시예에 따른 인공 신경망의 구조를 도시한 도면이다. 도 6은 본 명세서의 일 실시예에 따른 연속된 이미지를 분석하기 위한 인공 신경망의 구조를 도시한 도면이다. 도 7는 본 명세서의 일 실시예에 따른 제스처 인식 방법을 도시한 순서도이다. 도 8은 본 명세서의 일 실시예에 따른 제스처와 컴포넌트 매칭을 나타내는 표에 관한 도면이다. 도 9 및 도 10은 본 명세서의 일 실시예에 따른 트리거링 이벤트를 감지하는 네트워크를 도시한 순서도이다. 도 11은 본 명세서의 일 실시예에 따른 차량 내 제스처 수행 주체를 도시한 도면이다. 도 12은 본 명세서의 일 실시예에 따른 제스처 수행 주체를 고려하여 제스처와 컴포넌트 매칭을 나타내는 표에 관한 도면이다. 도 13 내지 도 16은 본 명세서의 일 실시예에 따른 제스처 수행 주체를 인식하는 네트워크를 도시한 순서도이다. 도 17 내지 도 22은 본 명세서의 일 실시예에 따른 트리거링 이벤트 감지 및 제스처 수행 주체를 인식하는 네트 워크를 도시한 순서도이다."}
