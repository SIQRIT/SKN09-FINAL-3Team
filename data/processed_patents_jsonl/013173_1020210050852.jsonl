{"patent_id": "10-2021-0050852", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0005384", "출원번호": "10-2021-0050852", "발명의 명칭": "시맨틱 표현 모델을 트레이닝 하는 방법, 장치, 전자 기기 및 컴퓨터 기록 매체", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "왕, 슈오환"}}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시맨틱 표현 모델을 트레이닝 하는 방법에 있어서,제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하는 단계;상기 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 상기 대상 트레이닝층에 대해초기화를 실행하며, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝종료 조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이닝을 실행하는 단계;트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하고, 상기 대상 트레이닝층이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이닝을 각각 실행하는 단계; 및모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는 단계;를 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 시맨틱 표현 모델은 트랜스포머(Transformer) 모델을 포함하는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 방법."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서,상기 제2 언어 종류의 트레이닝 코퍼스는 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스크(mask)에 대응하는 문자를 포함하고;상기 제1 시맨틱 표현 모델의 각 계층에 대해 트레이닝을 실행할 경우, 트레이닝 목표는 상기 최상층이 마스크(mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것인 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 방법."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 또는 제2항에 있어서,상기 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제2 시맨틱 표현 모델로 설정하는 단계;상기 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행할 경우, 상기 제2언어 종류의 트레이닝 코퍼스 대응하는 제1 언어 종류의 평행 코퍼스를 상기 제2 시맨틱 표현 모델에 입력하는단계; 및상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행하는 단계;를 더 포함하는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 방법.공개특허 10-2022-0005384-3-청구항 5 제4항에 있어서,상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행하는 단계는,상기 제1 시맨틱 표현 모델의 출력 결과 및 상기 제2 시맨틱 표현 모델의 출력 결과를 정렬 모델에 입력하는 단계; 및상기 정렬 모델이 상기 제1 시맨틱 표현 모델의 출력 결과를 이용하여 상기 제2 시맨틱 표현 모델의 출력 결과에 대해 주의력 메커니즘(attention mechanism)의 처리를 실행하고, 상기 주의력 메커니즘의 처리 결과에 대해매핑을 실행하여, 제2 언어 종류의 트레이닝 코퍼스 중의 문자에 대한 예측 결과를 얻는 단계를 포함하는 것을특징으로 하는,시맨틱 표현 모델 트레이닝 방법."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스크(mask)에 대응하는 문자를 포함하면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 마스크(mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것이고;상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류의 마스크(mask)가 없는 텍스트이면, 트레이닝 목표는상기 제2 언어 종류의 트레이닝 코퍼스 중의 각 문자에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 각 문자와일치하게 되는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 방법."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "시맨틱 표현 모델을 트레이닝 하는 장치에 있어서,제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하기 위한 제1 취득 유닛;상기 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기화를 실행하며, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하고; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하고, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2언어 종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이닝을 각각 실행하며; 모든 계층의트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻기 위한 트레이닝 유닛;을 구비하는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 시맨틱 표현 모델은 트랜스포머(Transformer) 모델을 포함하는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항 또는 제8항에 있어서,상기 제2 언어 종류의 트레이닝 코퍼스는 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스공개특허 10-2022-0005384-4-크(mask)에 대응하는 문자를 포함하고;상기 트레이닝 유닛이 제1 시맨틱 표현 모델의 각 계층에 대해 트레이닝을 실행할 경우, 트레이닝 목표는 상기최상층이 마스크(mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것인 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항 또는 제8항에 있어서,상기 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제2 시맨틱 표현 모델로 설정하기 위한 제2 취득 유닛;을 더 구비하고;상기 트레이닝 유닛은 또한 상기 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행할 경우, 상기 제2 언어 종류의 트레이닝 코퍼스 대응하는 제1 언어 종류의 평행 코퍼스를 상기 제2시맨틱 표현 모델에 입력하고; 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행하는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 트레이닝 유닛은 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행할 때, 구체적으로,상기 제1 시맨틱 표현 모델의 출력 결과 및 상기 제2 시맨틱 표현 모델의 출력 결과를 정렬 모델에 입력하고;상기 정렬 모델이 상기 제1 시맨틱 표현 모델의 출력 결과를 이용하여 상기 제2 시맨틱 표현 모델의 출력 결과에 대해 주의력 메커니즘(attention mechanism)의 처리를 실행하고, 상기 주의력 메커니즘의 처리 결과에 대해매핑을 실행하여, 제2 언어 종류의 트레이닝 코퍼스 중의 문자에 대한 예측 결과를 얻는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스크(mask)에 대응하는 문자를 포함하면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 마스크(mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것이고;상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류의 마스크(mask)가 없는 텍스트이면, 트레이닝 목표는상기 제2 언어 종류의 트레이닝 코퍼스 중의 각 문자에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 각 문자와일치하게 되는 것을 특징으로 하는,시맨틱 표현 모델 트레이닝 장치."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "전자 기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제6항 중 어느 한공개특허 10-2022-0005384-5-항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는,전자 기기."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는것을 특징으로 하는,기록 매체."}
{"patent_id": "10-2021-0050852", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는,컴퓨터 프로그램."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 시맨틱 표현 모델을 트레이닝 하는 방법, 장치, 전자 기기 및 컴퓨터 기록 매체를 개시하는 바, 인공 지능 중의 자연 언어 처리"}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것이다. 구체적으로 실현 방안은, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하는 단계; 상기 제1 시맨틱 표현 모델의 최하층 및 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2022-0005384 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기화를 실행하며, 기타 계층의 모델 매개 변 수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대 상 트레이닝층에 대해 트레이닝을 실행하는 단계; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대 상 트레이닝층에 추가하며, 각각에 대해, 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하 면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층 에 대해 트레이닝을 실행하는 단계; 및 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는 단계를 포함한다. 본 발명은 코스트를 절감할 수 있고, 또한 트레이닝 효율이 더욱 높다. CPC특허분류 G06N 3/08 (2013.01) 발명자 오우양, 쑤안 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층 순, 위 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층우, 화 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 바이두 캠퍼스, 넘버 10, 2층 왕, 하이펑 중국, 베이징 100085, 하이뎬 디스트릭트, 샹디 1 0번가, 넘버 10, 바이두 캠퍼스, 2층명 세 서 청구범위 청구항 1 시맨틱 표현 모델을 트레이닝 하는 방법에 있어서, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하는 단계; 상기 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 상기 대상 트레이닝층에 대해 초기화를 실행하며, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이 닝을 실행하는 단계; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하고, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이닝을 각각 실행하는 단계; 및 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는 단계; 를 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법. 청구항 2 제1항에 있어서, 상기 시맨틱 표현 모델은 트랜스포머(Transformer) 모델을 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법. 청구항 3 제1항 또는 제2항에 있어서, 상기 제2 언어 종류의 트레이닝 코퍼스는 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스 크(mask)에 대응하는 문자를 포함하고; 상기 제1 시맨틱 표현 모델의 각 계층에 대해 트레이닝을 실행할 경우, 트레이닝 목표는 상기 최상층이 마스크 (mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것인 것 을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법. 청구항 4 제1항 또는 제2항에 있어서, 상기 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제2 시맨틱 표현 모델로 설정하는 단계; 상기 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행할 경우, 상기 제2 언어 종류의 트레이닝 코퍼스 대응하는 제1 언어 종류의 평행 코퍼스를 상기 제2 시맨틱 표현 모델에 입력하는 단계; 및 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행하는 단 계; 를 더 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법.청구항 5 제4항에 있어서, 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행하는 단 계는, 상기 제1 시맨틱 표현 모델의 출력 결과 및 상기 제2 시맨틱 표현 모델의 출력 결과를 정렬 모델에 입력하는 단 계; 및 상기 정렬 모델이 상기 제1 시맨틱 표현 모델의 출력 결과를 이용하여 상기 제2 시맨틱 표현 모델의 출력 결과 에 대해 주의력 메커니즘(attention mechanism)의 처리를 실행하고, 상기 주의력 메커니즘의 처리 결과에 대해 매핑을 실행하여, 제2 언어 종류의 트레이닝 코퍼스 중의 문자에 대한 예측 결과를 얻는 단계를 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법. 청구항 6 제5항에 있어서, 상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스 크(mask)에 대응하는 문자를 포함하면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 마스크 (mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것이고; 상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류의 마스크(mask)가 없는 텍스트이면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 각 문자에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 각 문자와 일치하게 되는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 방법. 청구항 7 시맨틱 표현 모델을 트레이닝 하는 장치에 있어서, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하기 위한 제1 취 득 유닛; 상기 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기 화를 실행하며, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하 고; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하고, 상기 대상 트레이 닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 상기 대상 트레이닝층에 대해 트레이닝을 각각 실행하며; 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻기 위한 트레이닝 유닛; 을 구비하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 8 제7항에 있어서, 상기 시맨틱 표현 모델은 트랜스포머(Transformer) 모델을 포함하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 9 제7항 또는 제8항에 있어서, 상기 제2 언어 종류의 트레이닝 코퍼스는 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스크(mask)에 대응하는 문자를 포함하고; 상기 트레이닝 유닛이 제1 시맨틱 표현 모델의 각 계층에 대해 트레이닝을 실행할 경우, 트레이닝 목표는 상기 최상층이 마스크(mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하 게 되는 것인 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 10 제7항 또는 제8항에 있어서, 상기 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제2 시맨틱 표현 모델로 설정하기 위한 제 2 취득 유닛; 을 더 구비하고; 상기 트레이닝 유닛은 또한 상기 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이 닝을 실행할 경우, 상기 제2 언어 종류의 트레이닝 코퍼스 대응하는 제1 언어 종류의 평행 코퍼스를 상기 제2 시맨틱 표현 모델에 입력하고; 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결 과에 대해 정렬을 실행하는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 11 제10항에 있어서, 상기 트레이닝 유닛은 상기 제2 시맨틱 표현 모델의 출력 결과와 상기 제1 시맨틱 표현 모델의 출력 결과에 대 해 정렬을 실행할 때, 구체적으로, 상기 제1 시맨틱 표현 모델의 출력 결과 및 상기 제2 시맨틱 표현 모델의 출력 결과를 정렬 모델에 입력하고; 상기 정렬 모델이 상기 제1 시맨틱 표현 모델의 출력 결과를 이용하여 상기 제2 시맨틱 표현 모델의 출력 결과 에 대해 주의력 메커니즘(attention mechanism)의 처리를 실행하고, 상기 주의력 메커니즘의 처리 결과에 대해 매핑을 실행하여, 제2 언어 종류의 트레이닝 코퍼스 중의 문자에 대한 예측 결과를 얻는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 12 제10항에 있어서, 상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 상기 마스 크(mask)에 대응하는 문자를 포함하면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 마스크 (mask)에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 마스크(mask)에 대응하는 문자와 일치하게 되는 것이고; 상기 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류의 마스크(mask)가 없는 텍스트이면, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 각 문자에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 각 문자와 일치하게 되는 것을 특징으로 하는, 시맨틱 표현 모델 트레이닝 장치. 청구항 13 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 제1항 내지 제6항 중 어느 한항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 전자 기기. 청구항 14 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하는 것을 특징으로 하는, 기록 매체. 청구항 15 비 일시적 컴퓨터 판독 가능 기록 매체에 기록되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램은 상기 컴퓨터로 하여금 제1항 내지 제6항 중 어느 한 항에 기재된 방법을 수행하도록 하 는 것을 특징으로 하는, 컴퓨터 프로그램. 발명의 설명 기 술 분 야 본 발명은 컴퓨터 응용 기술 분야에 관한 것인 바, 특히 인공 지능 기술에 관한 것이다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근에 BERT（Bidirectional Encoder Representation from Transformer, 트랜스포머(transformer)로부터의 양 방향 인코더 표현） 모델을 대표로 하는 사전 트레이닝 모델이 NLP（Natural Language Processing, 자연 언어 처리） 태스크의 효과를 대폭적으로 향상시켰다. 하지만 현재 주류 시맨틱 표현 모델은 영어, 중국어, 프랑스어, 독일어 등 자주 노출되는 언어에 집중되어 있다. 그러나 세계에는 수천 종류의 언어가 있으며, 대부 분은 영어 등 자주 노출되는 언어와 비교하여 코퍼스(corpus)가 상대적으로 적은 바, 우리는 이러한 언어를 리 소스가 적은 언어라 부른다. 사전 트레이닝 모델의 트레이닝은 대량의 계산 리소스가 필요하기에, 코스트가 매 우 높은 바, 각각의 모델의 코스트가 수십만 심지어 수백만에 달한다. 따라서, 각각의 언어에 대해 모두 충분한 코퍼스를 구축하여 트레이닝을 실행하기 어렵다. 또한 예를 들면 체코어 등 코퍼스가 매우 희소한 언어 종류의 경우, 심지어 충분히 많은 코퍼스를 수집하여 트레이닝을 실행하기도 어렵다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "이에 감안하여, 본 발명은 코퍼스가 희소한 언어 종류에 대해 시맨틱 표현 모델을 트레이닝 하는 방법, 장치, 디바이스 및 컴퓨터 기록 매체를 제공한다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "제1 측면에 있어서, 본 발명은 시맨틱 표현 모델을 트레이닝 하는 방법을 제공하는 바, 당해 방법은, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하는 단계; 상기 제1 시맨틱 표현 모델의 최하층 및 최하층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기 화를 실행하며, 상기 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하 는 단계; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하며, 각각에 대해, 대상 트 레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하는 단계; 및 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는 단계를 포함한다. 제2 측면에 있어서, 본 발명은 시맨틱 표현 모델을 트레이닝 하는 장치를 더 제공하는 바, 당해 장치는, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정하기 위한 제1 취 득 유닛; 상기 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기 화를 실행하며, 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까지 제2 언 어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하고; 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하며, 각각에 대해, 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어 종류의 트레 이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하며; 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻기 위한 트레이닝 유닛을 구비한다. 제3 측면에 있어서, 본 발명은 전자 기기를 제공하는 바, 당해 전자 기기는, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 연결된 메모리를 구비하며, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기록되어 있으며, 상기 명령이 상기 적어도 하나의 프로세서에 의해 수행되어 상기 적어도 하나의 프로세서로 하여금 상기 방법을 실행하도록 한다. 제4 측면에 있어서, 본 발명은 컴퓨터 명령이 기록되어 있는 비 일시적 컴퓨터 판독 가능 기록 매체를 더 제공 하는 바, 상기 컴퓨터 명령은 상기 컴퓨터로 하여금 상기 방법을 실행하도록 한다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 기존의 언어 종류의 트레이닝된 시맨틱 표현 모델을 충분히 이용하여, 각 계층을 순차적으로 번역 트 레이닝하여 다른 일 언어 종류의 시맨틱 표현 모델을 얻는다. 코퍼스가 희소한 언어 종류의 경우, 트레이닝 샘 플을 수집하는데 필요한 코스트를 현저히 줄였으며, 또한 트레이닝 효율이 더욱 높다. 상기 선택적인 방식이 가지는 기타 효과는 아래에서 구체적인 실시예를 참조하여 설명하기로 한다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 도면을 참조하여 본 발명의 시범적인 실시예를 설명하는 바, 본 발명에 대한 이해를 돕기 위해 여기에는 본 발명 실시예의 다양한 세부 사항이 포함되며, 이러한 세부 사항을 단지 시범적인 것으로 간주해야 할 것이다. 따라서, 당업자는 본 발명의 범위 및 정신을 벗어나지 않는 전제 하에서, 여기서 설명되는 실시예에 대 해 다양한 변경 및 수정을 수행할 수 있음을 인식해야 한다. 마찬가지로, 명확성 및 간결성을 위하여 이하의 설 명에서는 잘 알려진 기능 및 구조의 설명을 생략하였다. 본 발명의 핵심 사상은, 이미 충분히 트레이닝 된 제1 언어 종류의 시맨틱 표현 모델을 이용하여, 제2 언어 종 류의 시맨틱 표현 모델의 트레이닝을 지원하는 것이다. 설명의 편의 및 이해를 돕기 위하여, 후속의 실시예에관한 예에서 제1 언어 종류가 영어이고, 제2 언어 종류가 중국어인 예를 들어 설명하나, 본 발명은 한정되지 않 는 바, 임의의 언어 종류에 적용할 수 있다. 또한, 본 발명에 언급된 시맨틱 표현 모델은 RNN（Recurrent Neural Network, 순환 신경망）, CNN（ Convolutional Neural Networks, 컨볼루션 신경망）, 트랜스포머(Transformer) 모델, 등을 채용할 수 있다. 전 형적인 시맨틱 표현 모델로서, 후속의 실시예에 언급된 예에서 모두 트랜스포머 모델인 예를 들어 설명하는 바, 기타 모델의 실현 원리는 이와 유사하다. 실시예 1: 도 1은 본 발명은 실시예 1에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 방법 플로우 차트이며, 본 방법 의 수행 주체는 시맨틱 표현 모델을 트레이닝 하는 장치이며, 당해 장치는 컴퓨터 시스템/서버에 위치한 응용 프로그램이거나, 컴퓨터 시스템/서버에 위치한 응용 프로그램 중의 플러그인 또는 소프트웨어 개발 키트（ Software Development Kit, SDK） 등 기능 유닛일 수도 있다. 도 1에 도시된 바와 같이, 당해 방법은 이하의 단계를 포함할 수 있다. 101에 있어서, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정한 다. 영어가 제1 언어 종류인 예를 들면, 영어가 국제 통용 언어이기에, 일반적으로 영어에 대한 코퍼스가 매우 많으 며, 따라서 영어를 채용하면 Transformer 모델과 같은 시맨틱 표현 모델을 매우 쉽게 트레이닝하여 얻을 수 있 다. 본 단계에 있어서, 이미 트레이닝된 영어의 트랜스포머 모델을 제1 시맨틱 표현 모델로 설정함으로써, 후속 의 번역 트레이닝을 실행할 때 중국어 트랜스포머 모델의 트레이닝을 지원한다. 102에 있어서, 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이닝층에 대해 초기화를 실행하며, 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달할 때까 지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행한다. 이해의 편의를 위하여, 우선 본 발명에서 채용한 트레이닝 코퍼스를 간단히 소개한다. 트랜스포머 모델의 경우, 일반적으로 채용되는 코퍼스는 mask（마스크）가 있는 텍스트 및 마스크(mask)에 대응하는 문자를 포함한다. 예 를 들면, 하나의 트레이닝 코퍼스는 \" 〔mask〕 (내가 하나의 사과를〔mask〕)”이며, 여기서 〔mask 〕에 대응하는 문자는 \" (먹었다)”이다. 다른 하나의 트레이닝 코퍼스는 \" 〔mask〕， 〔 mask〕 (내가 아주 긴 〔mask〕을 달려서야 겨우 너를 〔mask〕)”이며, 여기서 2개의 〔mask〕에 대응하는 문자는 각각 \" (길)” 및 \" (찾았다)”이다. 트랜스포머의 작용은 바로 트레이닝 코퍼스 중의 마스크(mask) 에 대응하는 문자를 예측하는 것이며, 가능한 한 예측되는 결과가 기대치（트레이닝 코퍼스 중의 마스크(mask) 에 대응하는 문자）와 일치하도록 하는 것이다. 트랜스포머 모델의 경우, 도 2에 도시된 바와 같이, 다층 구조를 가진다. 여기서, 최하층은 임베딩 계층으로서, 일반적으로 Embedding Layer로 표현하며, 트레이닝 코퍼스 중의 각 문자의 벡터 표현을 확정하기 위하여 이용된 다. 최상층은 완전 연결 계층으로서, 일반적으로 Task Layer로 표현하며, 트랜스포머의 중간의 각각의 계층의 처리를 거친 후의 벡터 표현에 대해 매핑을 실행함으로써, 트레이닝 코퍼스 중의 마스크(mask)에 대한 콘텐츠 예측을 얻기 위하여 이용된다. 최상층 및 최하층 사이에는 복수의 계층이 더 포함되는 바, 일반적으로 트랜스포 머 블록(Transformer Block)으로 표현하며. 각각의 계층의 트랜스포머 블록은 입력된 각 문자의 벡터 표현에 대 해 주의력（Attention） 메커니즘 처리를 실행하여, 전역 벡터(global vector) 표현으로 개변한다. 각각의 계층 의 트랜스포머 블록은 주의력을 실행할 때 바로 위의 계층의 전역 벡터 표현을 참고하게 된다. 각 계층의 트랜 스포머 블록의 구체적인 작업 메커니즘에 대해 여기서 상세히 설명하지 않는다. 본 발명은 실시예의 도 2에서는, 3개의 계층을 가지는 트랜스포머 블록의 예를 들었다. 일반적으로 트랜스포머 모델의 최하층은 문자의(literal) 논리 처리에 더 많은 주의를 기울이며, 최상층은 의미 의(semantic) 논리 처리에 더 많은 주의를 기울인다. 최상층의 시맨틱 논리는 서로 다른 언어에 있어서 일관성 이 더 강하다. 이러한 가정을 바탕으로, 본 발명은 실시예에서는 각 계층에 대해 실행 순차적으로 트레이닝하는 방식을 채용하는 바, 우선 최하층 및 최상층을 트레이닝 하고, 이어서 최하층 및 최상층을 결합하여 중간의 각 계층에 대해 각각 트레이닝을 실행한다. 도 2에 도시된 （a） 단계와 같이, 영어의 트랜스포머 모델 중의 Embedding Layer 및 Task Layer을 대상 트레이 닝층으로 설정하고 초기화를 실행하며, 즉 모델의 매개 변수에 대해 초기화를 실행한다. 기타 계층 즉 각 트랜 스포머 블록의 매개 변수를 그대로 유지하는 바, 즉 각 트랜스포머 블록의 매개 변수를 영어의 전에 트레이닝 하여 얻은 모델 매개 변수로 유지한다. 이어서, 중국어의 트레이닝 코퍼스를 입력하여 대상 트레이닝층인 Embedding Layer 및 Task Layer에 대해 트레이닝을 실행한다. 대상 트레이닝층에 대해 트레이닝을 실행할 때마다, 트레이닝 목표는 모두 Task Layer 계층 마스크에 대한 예측 결과가 기대치와 일치하게 되는 것이다. 다시 말하면, 당해 트레이닝 목표에 따라 손실 함수를 구축할 수 있으 며, 손실 함수의 값을 이용하여 대상 트레이닝층의 모델 매개 변수에 대해 최적화를 실행할 수 있다. 항상 대상 트레이닝층에 대한 트레이닝 종료 조건은, Loss가 점차 수렴되는 것 또는 반복 횟수가 소정의 임계 값에 달한 것 등일 수 있다. 다시 말하면, （a） 단계에 있어서, 중국어의 트레이닝 코퍼스를 이용하여 Embedding Layer 및 Task Layer에 대 해 트레이닝을 실행하는 과정에서, loss가 점차 수렴되거나, 반복 횟수가 소정의 임계 값에 달할 때까지, Loss 를 이용하여 반복적으로 실행하여, Embedding Layer 및 Task Layer의 매개 변수를 점차 최적화시킨다. 103에 있어서, 트레이닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하며, 각각에 대해, 대상 트레이닝층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행한다. 도 2에 도시된 （b） 단계와 같이, 아래서부터 위로 우선 트랜스포머 블록1을 대상 트레이닝층에 추가하며, 이 때의 대상 트레이닝층은 Embedding Layer, Task Layer 및 트랜스포머 블록1을 포함한다. 여기서 Embedding Layer, Task Layer의 현재의 매개 변수는 102의 트레이닝을 거친 후의 매개 변수이고, 트랜스포머 블록1의 매개 변수는 영어의 트랜스포머 모델 중의 트랜스포머 블록1의 매개 변수이다. 트랜스포머 블록2 및 트랜스포머 블록 3의 매개 변수를 그대로 유지하는 정황 하에서, Embedding Layer, Task Layer 및 트랜스포머 블록1을 트레이닝 한다. 중국어의 트레이닝 코퍼스를 이용하여 Embedding Layer, Task Layer 및 트랜스포머 블록1에 대해 트레이 닝을 실행하는 과정에서, loss가 점차 수렴되거나, 반복 횟수가 소정의 임계 값에 달할 때까지, Loss를 이용하 여 반복적으로 실행하여, Embedding Layer, Task Layer 및 트랜스포머 블록1의 매개 변수를 점차 최적화시킨다. 도 2에 도시된 （c） 단계와 같이, 아래서부터 위로 트랜스포머 블록2를 대상 트레이닝층에 추가하며, 이때의 대상 트레이닝층은 Embedding Layer, Task Layer, 트랜스포머 블록1 및 트랜스포머 블록2를 포함한다. 트랜스포 머 블록3의 매개 변수를 그대로 유지하는 정황 하에서, Embedding Layer, Task Layer, 트랜스포머 블록1 및 트 랜스포머 블록2를 트레이닝한다. 중국어의 트레이닝 코퍼스를 이용하여 Embedding Layer, Task Layer, 트랜스포 머 블록1 및 트랜스포머 블록2에 대해 트레이닝을 실행하는 과정에서, loss가 점차 수렴되거나, 반복 횟수가 소 정의 임계 값에 달할 때까지, Loss를 이용하여 반복적으로 실행하여, Embedding Layer, Task Layer, 트랜스포머 블록1 및 트랜스포머 블록2의 매개 변수를 점차 최적화시킨다. 도 2에 도시된 （d） 단계와 같이, 아래서부터 위로 트랜스포머 블록3을 대상 트레이닝층에 추가하며, 이때의 대상 트레이닝층은 Embedding Layer, Task Layer, 트랜스포머 블록1, 트랜스포머 블록2 및 트랜스포머 블록3을 포함한다. 중국어의 트레이닝 코퍼스를 이용하여 Embedding Layer, Task Layer, 트랜스포머 블록1, 트랜스포머 블록2 및 트랜스포머 블록3에 대해 트레이닝을 실행하는 과정에서, loss가 점차 수렴되거나, 반복 횟수가 소정 의 임계 값에 달할 때까지, Loss를 이용하여 반복적으로 실행하여, Embedding Layer, Task Layer, 트랜스포머 블록1, 트랜스포머 블록2 및 트랜스포머 블록3의 매개 변수를 점차 최적화시킨다. 상기 과정으로부터 알 수 있듯이, 실제 상 중간의 각 계층에 대해서는 영어의 각 트랜스포머 블록 계층을 이용 하여 핫 스타트(hot start)를 실행하여 중국어의 각 트랜스포머 블록 계층을 트레이닝한다. 또한, 중간의 각 계 층의 경우, 상기 아래서부터 위로 한 층 한 층씩 트레이닝을 실행하는 방식 이외에, 중간 계층의 수가 상대적으 로 많을 경우, 아래서부터 위로 두 층 두 층씩 트레이닝하거나, 순차적으로 더 많은 계층 수를 트레이닝할 수 있다. 104에 있어서, 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는다. 도 2에 도시된 （d） 단계의 트레이닝이 완료된 후, 중국어의 트랜스포머 모델을 얻으므로, 트레이닝 된 영어의 트랜스포머 모델로부터 점차 번역 트레이닝 하여 중국어의 트랜스포머 모델을 얻는 것을 실현할 수 있다. 본 실시예에서는 단일 코퍼스 즉 중국어 코퍼스를 이용하여 영어의 트랜스포머 모델로부터 번역을 실행하는 방 식을 채용하여, 중국어의 트랜스포머 모델에 대해 트레이닝을 실행한다. 사용 가능한 이중 언어의 평행 코퍼스 가 있을 경우, 트레이닝 코스트를 더 한층 줄이고, 트레이닝 효과를 향상시킬 수 있다. 이 경우 실시예 2에 도시된 방법을 채용하여 트레이닝을 실행할 수 있다. 실시예 2: 본 실시예에 있어서, 실시예 1의 기초 상에서, 또한 제1 언어 종류를 이용하여 트레이닝 하여 얻은 시맨틱 표현 모델을 취득하여, 제2 시맨틱 표현 모델로 설정한다. 여기서 제1 시맨틱 표현 모델은 기초로서 한 층씩 번역 트 레이닝을 실행하며, 제2 시맨틱 표현 모델은 제2 언어 종류의 시맨틱 표현 모델을 트레이닝 하는 과정에서, 제2 시맨틱 표현 모델이 출력한 제1 언어 종류의 결과와 제1 시맨틱 표현 모델이 출력한 결과를 이용하여 정렬 처리 를 실행한다. 이 경우, 하나의 정렬 모델을 별도로 추가하여 제1 시맨틱 표현 모델의 번역 트레이닝을 지원할 필요가 있으며, 당해 정렬 모델은 상기 정렬 처리를 실행하기 위한 것이다. 도 2 중의 （a） 단계의 트레이닝을 예로 들면, 도 3에 도시된 바와 같이, 중국어와 영어의 평행 코퍼스 중의 영어 트레이닝 코퍼스를 사전에 트레이닝 하여 얻은 영어의 트랜스포머 모델에 입력하고, Task Layer 계층이 출 력한 영어 결과를 정렬 모델에 입력한다. 이와 동시에, 영어 트레이닝 코퍼스에 대응하는 중국어 트레이닝 코퍼 스를 （a） 단계에 대응하는 트레이닝 과정에서의 중국어의 트랜스포머 모델에 입력하고, Task Layer 계층이 출 력한 중국어 결과도 정렬 모델에 입력한다. 정렬 모델이 트레이닝 중의 중국어의 트랜스포머 모델의 출력 결과 를 이용하여 영어의 트랜스포머 모델의 출력 결과에 대해 주의력 메커니즘의 처리를 실행한 후, 주의력 처리 결 과에 대해 매핑을 실행하여, 중국어 트레이닝 코퍼스 중의 마스크에 대한 예측 결과를 얻는다. 마찬가지로, 트 레이닝 목표는 마스크에 대한 예측 결과가 트레이닝 코퍼스 중의 기대치 문자와 일치하게 되는 것이다. 정렬 모 델의 예측 결과를 이용하여 Loss를 구축하여, Loss의 값을 이용하여 트레이닝 중의 중국어의 트랜스포머 모델 매개 변수 （즉 대상 트레이닝층의 모델 매개 변수）에 대해 최적화를 실행하는 동시에, 정렬 모델의 모델 매개 변수도 최적화된다. 정렬 모델이 주의력 처리를 실행할 경우, 중국어의 트랜스포머 모델이 출력한 문자 표현이 이고, 영어의 트랜 스포머 모델이 출력한 문자 표현이 라고 가정한다. 와 의 내적이 로 표현되며, 를 이용하여 에 대해 가중치 처리를 실행한다."}
{"patent_id": "10-2021-0050852", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, n은 영어의 트랜스포머 모델이 출력한 문자의 합계이다. 이어서, 주의력 처리 후 얻은 각 으로 구성된 벡터를 이용하여 완전 연결 계층 （Softmax）의 매핑을 실행하 여, 중국어 트레이닝 코퍼스 중의 마스크 값을 예측한다. 기타 단계의 트레이닝 과정은 이와 유사한 바, 마찬가지로 영어의 트랜스포머 모델의 출력 결과를 이용하여 정 렬 처리를 실행하는 바, 여기서 반복적으로 설명하지 않는다. 일 예에 있어서, 이러한 일 그룹의 평행 코퍼스가 존재한다고 가정한다. 영어 ：I ate an apple. 중국어 ： 〔mask〕 . 여기서 마스크(mask)의 기대 문자는 \" ”이다. 도 4에 도시된 바와 같이, 중국어 코퍼스 및 각 문자의 위치 식별자 （도면에서 \" \"의 위치 식별자가 “0\"이고, 〔mask〕의 위치 식별자가 1이며, ...）를 트레이닝 과정에서의 중국어 트랜스포머 모델에 입력한다. 평행된 영어 코퍼스 및 각 문자의 위치 식별자 （도면에서 \"I\"의 위치 식별자가 0이고, \"ate\"의 위치 식별자가 1이며, ...）를 이미 트레이닝 된 영어 트랜스포머 모델에 입력한다. 영어 트랜스포머 모델이 출력한 각 영어의 문자 및 중국어 트랜스포머 모델이 출력한 각 중국어의 문자를 모두 정렬 모델에 출력하며, 정렬 모델이 중국어 트랜스포머 모델의 출력 결과를 이용하여 영어 트랜스포머 모델의 출력 결과에 대해 주의력을 실행한 후, 주의 력을 실행하여 얻은 결과를 Softmax 매핑을 거친 후, 중국어의 예측된 각 중국어의 문자를 얻는다. 당해 중국어 의 예측된 문자와 중국어 코퍼스의 기대 문자를 이용하여 loss를 확정한 후, 한 층씩 트레이닝 한 중국어 트랜스포머 중의 대상 트레이닝층의 모델 매개 변수 및 갱신 정렬 모델의 모델 매개 변수를 갱신한다. 또한, 상기 실시예 2에 있어서, 이중 언어 평행 코퍼스를 채용하면, 채용한 트레이닝 데이터에 대해 마스크를 실행하지 않을 수 있다. 예를 들면, 이러한 일 그룹의 평행 코퍼스가 존재한다고 가정한다. 영어 ：I ate an apple. 중국어 ： . 정렬 모델이 주의력 처리를 실행하는 과정과 실시예 2에서 설명한 것과 동일한 바, Softmax를 거친 후, 마찬가 지로 중국어 트레이닝 코퍼스 중의 각 문자를 예측하여 얻을 수 있다. 당해 중국어의 예측된 문자와 중국어 코 퍼스의 기대 문자를 이용하여 loss를 확정한 후, 한 층씩 트레이닝 한 중국어 트랜스포머 중의 대상 트레이닝층 의 모델 매개 변수 및 갱신 정렬 모델의 모델 매개 변수를 갱신한다. 실시예 2에 의해 제공되는 방식에 따르면, 이중 언어 평행 코퍼스를 충분히 이용하였고, 리소스가 많은 언어 종 류의 코퍼스를 충분히 이용하여 트레이닝 코스트를 더 한층 절감하였고, 리소스가 적은 언어 종류의 시맨틱 표 현 모델의 트레이닝 효과를 향상시켰다. 이상은 본 발명에 의해 제공되는 방법에 대한 상세한 설명이며, 이하 실시예에 결합시켜 본 발명에 의해 제공되 는 장치를 상세히 설명한다. 실시예 3: 도 5는 본 발명의 실시예 3에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 장치의 구조도이다. 도 5에 도 시된 바와 같이, 당해 장치는 제1 취득 유닛 및 트레이닝 유닛을 구비하며, 또한 제2 취득 유닛을 더 구비할 수 있다. 여기서 각 구성 유닛의 주요 기능은 아래와 같다. 제1 취득 유닛은, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제1 시맨틱 표현 모델로 설정한다. 트레이닝 유닛은, 제1 시맨틱 표현 모델의 최하층 및 최상층을 대상 트레이닝층으로 설정하고, 대상 트레이 닝층에 대해 초기화를 실행하며, 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 도달 할 때까지 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하고; 트레이 닝 되지 않은 각 계층을 아래서부터 위로 순차적으로 대상 트레이닝층에 추가하며, 각각에 대해, 대상 트레이닝 층 이외의 기타 계층의 모델 매개 변수를 그대로 유지하면서, 트레이닝 종료 조건에 각각 도달할 때까지 제2 언 어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행하며; 모든 계층의 트레이닝이 모두 종료된 후, 제2 언어 종류에 대한 시맨틱 표현 모델을 얻는다. 여기서, 최하층은 임베딩(embedding) 계층이고, 최상층은 완전 연결 계층이며. 시맨틱 표현 모델은 CNN, RNN, 트랜스포머(Transformer) 모델 등일 수 있다. 제2 언어 종류의 트레이닝 코퍼스는 제2 언어 종류를 사용하는 마스크(mask)가 있는 텍스트 및 마스크에 대응하 는 문자를 포함한다. 트레이닝 유닛은 제1 시맨틱 표현 모델의 각 계층에 대해 트레이닝을 실행할 경우, 트레이닝 목표는 최하층 이 마스크에 대한 예측 결과가 트레이닝 코퍼스 중의 마스크에 대응하는 문자와 일치하게 되는 것이다. 대상 트레이닝층에 대해 트레이닝을 실행할 때마다, 트레이닝 목표는 모두 최상층이 마스크에 대한 예측 결과가 기대치와 일치하게 되는 것이다. 다시 말하면, 당해 트레이닝 목표에 따라 손실 함수를 구축할 수 있으며, 손실 함수의 값을 이용하여 대상 트레이닝층의 모델 매개 변수에 대해 최적화를 실행할 수 있다. 항상 대상 트레이닝 층에 대한 트레이닝 종료 조건은, Loss가 점차 수렴되는 것 또는 반복 횟수가 소정의 임계 값에 달한 것 등일 수 있다. 사용 가능한 이중 언어의 평행 코퍼스가 있을 경우, 트레이닝 코스트를 더 한층 줄이고, 트레이닝 효과를 향상 시킬 수 있다. 이 경우, 제2 취득 유닛은, 제1 언어 종류에 의해 트레이닝된 시맨틱 표현 모델을 취득하여 제2 시맨틱 표현 모델로 설정한다. 트레이닝 유닛은, 제2 언어 종류의 트레이닝 코퍼스를 이용하여 대상 트레이닝층에 대해 트레이닝을 실행할 경우, 제2 언어 종류의 트레이닝 코퍼스 대응하는 제1 언어 종류의 평행 코퍼스를 제2 시맨틱 표현 모델에 입력하고; 제2 시맨틱 표현 모델의 출력 결과와 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행한다. 구체적으로, 트레이닝 유닛은, 제2 시맨틱 표현 모델의 출력 결과와 제1 시맨틱 표현 모델의 출력 결과에 대해 정렬을 실행할 경우, 구체적으로, 제1 시맨틱 표현 모델의 출력 결과 및 제2 시맨틱 표현 모델의 출력 결과를 정렬 모델에 입력하고; 정렬 모델이 제1 시맨틱 표현 모델의 출력 결과를 이용하여 제2 시맨틱 표현 모델의 출력 결과에 대해 주의력 메커니즘(attention mechanism)의 처리를 실행하고, 주의력 메커니즘의 처리 결과에 대해 매핑을 실행하여, 제2 언어 종류의 트레이닝 코퍼스 중의 문자에 대한 예측 결과를 얻을 수 있다. 평행 코퍼스 중의 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류를 사용하는 마스크가 있는 텍스트 및 상기 마스크에 대응하는 문자를 포함할 경우, 트레이닝 목표는 제2 언어 종류의 트레이닝 코퍼스 중의 마스크에 대한 코퍼스 결과가 트레이닝 코퍼스 중의 마스크에 대응하는 문자와 일치하게 되는 것이다. 평행 코퍼스 중의 제2 언어 종류의 트레이닝 코퍼스가 제2 언어 종류의 마스크가 없는 텍스트일 경우, 트레이닝 목표는 상기 제2 언어 종류의 트레이닝 코퍼스 중의 각 문자에 대한 예측 결과가 상기 트레이닝 코퍼스 중의 각 문자와 일치하게 하는 것이다. 본 발명의 실시예에 따르면, 본 발명은 전자 기기 및 판독 가능 기록 매체를 더 제공한다. 도 6은 본 발명에 따른 실시예의 시맨틱 표현 모델을 트레이닝 하는 방법을 실현하는 전자 기기의 블럭도이다. 전자 기기는 예를 들면 랩톱 컴퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인 디지털 보조기, 서버, 블레이드 서 버, 대형 컴퓨터 및 기타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타낸다. 전자 기기는 또한 예 를 들면 개인 디지털 처리기, 셀폰, 스마트 전화, 웨어러블 기기 및 기타 유사한 계산 장치와 같은 다양한 형태 의 모바일 장치를 나타낼 수 있다. 본 명세서에 나타낸 구성 요소, 이들의 연결과 관계 및 이들의 기능은 단지 예일뿐이며, 본 명세서에서 설명하거나 및/또는 요구하는 본 발명의 실현을 한정하려는 것이 아니다. 도 6에 도시된 바와 같이, 당해 전자 기기는 하나 또는 복수의 프로세서, 메모리 및 각 구성 요소를 연결하기 위한 인터페이스를 구비하며, 당해 인터페이스는 고속 인터페이스 및 저속 인터페이스를 포함한다. 각 구성 요소는 서로 다른 버스를 통해 상호 연결되며, 공통 마더 보드에 설치되거나 또는 수요에 따라 기타 방식 으로 설치된다. 프로세서 전자 기기 내에서 수행되는 명령에 대해 처리를 실행할 수 있으며, 메모리 내에 기억 되어 외부 입력/출력 장치（예를 들면 인터페이스에 연결된 디스플레이 기기） 상에 GUI의 그래픽 정보를 표시 하기 위한 명령을 포함한다. 기타 실시 방식에 있어서, 필요할 경우, 복수의 프로세서 및/또는 복수의 버스와 복수의 메모리를 함께 사용할 수 있다. 마찬가지로, 복수의 전자 기기를 연결할 수 있으며, 각 기기는 부분적인 필요한 조작 （예를 들면, 서버 어레이, 일 그룹의 블레이드 서버, 또는 다중 프로세서 시스템）을 제공한다. 도 6에서는 하나의 프로세서의 예를 들었다. 메모리는 본 발명에 의해 제공되는 비 일시적 컴퓨터 판독 가능 기억 매체이다. 여기서, 상기 메모리에는 적어도 하나의 프로세서에 의해 수행 가능한 명령이 기억되어 있으며, 상기 적어도 하나의 프로세서로 하여금 본 발명에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 방법을 수행하도록 한다. 본 발명의 비 일시적 컴 퓨터 판독 가능 기억 매체는 컴퓨터 명령을 기억하며, 당해 컴퓨터 명령은 컴퓨터로 하여금 본 발명에 의해 제 공되는 시맨틱 표현 모델을 트레이닝 하는 방법을 수행하도록 한다. 메모리는 일종의 비 일시적 컴퓨터 판독 가능 기억 매체로서, 비 일시적 소프트웨어 프로그램을 기억하는 데 사용될 수 있는 바, 예를 들면 비 일시적 컴퓨터 수행 가능 프로그램 및 모듈, 본 발명 실시예 중의 시맨틱 표현 모델을 트레이닝 하는 방법 대응하는 프로그램 명령/모듈을 기억하는데 사용될 수 있다. 프로세서는 메모리 내에 기억된 비 일시적 소프트웨어 프로그램, 명령 및 모듈을 운행함으로써, 서버의 다양한 기능 응용 및 데이터 처리를 수행하는 바, 즉 상술한 방법 실시예 중의 시맨틱 표현 모델을 트레이닝 하는 방법을 실 현한다. 메모리는 프로그램 기억 영역 및 데이터 기억 영역을 포함할 수 있으며, 여기서, 프로그램 기억 영역은 운 영 체제 및 적어도 하나의 기능에 필요한 응용 프로그램을 기억할 수 있고, 데이터 기억 영역은 시맨틱 표현 모 델을 트레이닝 하는 방법을 실현하는 전자 기기의 사용을 통해 생성된 데이터 등을 기억할 수 있다. 또한, 메모 리는 고속 랜덤 액세스 메모리를 포함할 수 있고, 비 일시적 메모리를 더 포함할 수 있는 바, 예를 들면 적어도 하나의 자기 디스크 저장 장치, 플래시 장치, 또는 기타 비 일시적 고체 저장 장치를 포함할 수 있다. 일부 실시예에 있어서, 메모리는 선택적으로 프로세서에 대해 원격 설치한 메모리를 포함할 수 있으며, 이러한 원격 메모리는 네트워크를 통해 시맨틱 표현 모델을 트레이닝 하는 방법을 실현하는 전자 기기에 연 결될 수 있다. 상술한 네트워크의 실예는 인터넷, 기업 인트라 넷, 근거리 통신망, 이동 통신 네트워크 및 이들 의 조합을 포함하나 이에 한정되지 않는다. 시맨틱 표현 모델을 트레이닝 하는 방법을 실현하는 전자 기기는 입력 장치 및 출력 장치를 더 포함 할 수 있다. 프로세서, 메모리, 입력 장치 및 출력 장치는 버스 또는 기타 방식을 통해 연 결될 수 있으며, 도 6에서는 버스를 통해 연결하는 예를 들었다. 입력 장치는 입력된 디지털 또는 문자 정보를 수신하고, 또한 시맨틱 표현 모델을 트레이닝 하는 방법을 실현하는 전자 기기의 사용자 설정 및 기능 제어에 관한 키 신호 입력을 생성할 수 있다. 예를 들면 터치 스크 린, 키패드, 마우스, 트랙 패드, 터치 패드, 포인팅 스틱, 하나 또는 복수의 마우스 버튼, 트랙볼, 조이스틱 등 입력 장치를 포함할 수 있다. 출력 장치은 디스플레이 기기, 보조 조명 장치（예를 들면 LED） 및 촉각 피 드백 장치（예를 들면 진동 모터） 등을 포함할 수 있다. 당해 디스플레이 기기는 액정 디스플레이（LCD）, 발 광 다이오드（LED） 디스플레이 및 등 플라즈마 디스플레이를 포함할 수 있으나 이에 한정되지 않는다. 일부 실 시 방식에 있어서, 디스플레이 기기는 터치 스크린일 수 있다. 여기서 설명하는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 전용 ASIC （전용 집적 회로）, 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및/또는 이들의 조합에서 실현될 수 있다. 이러한 다양한 실시예는 하나 또는 복수의 컴퓨터 프로그램에서 실시되고, 당해 하나 또는 복수의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그램 가능 시스템 상에서 수행 및/또는 해석될 수 있으 며, 당해 프로그램 가능 프로세서는 전용 또는 일반 프로그램 가능 프로세서일 수 있고, 기억 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신할 수 있으며, 또한 데이터 및 명령 을 당해 기억 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 이러한 계산 프로그램（, 소프트웨어, 소프트웨어 응용 또는 코드로도 불림）은 프로그램 가능 프로세서의 기계 명령을 포함하며, 또한 고급 과정 및/또는 객체 지향 프로그래밍 언어 및/또는 어셈블리/기계 언어를 이용하여 이러한 계산 프로그램을 실시할 수 있다. 본 명세서에서 사용되는 \"기계 판독 가능 매체” 및 \"컴퓨터 판독 가 능 매체”와 같은 용어는, 기계 명령 및/또는 데이터를 프로그램 가능 프로세서의 임의의 컴퓨터 프로그램 제품, 기기 및/또는 장치（예를 들면, 자기 디스크, 광 디스크, 메모리, 프로그램 가능 논리 장치（PLD））에 제공하기 위한 것을 의미하며, 기계 판독 가능 신호로서의 기계 명령을 수신하는 기계 판독 가능 매체를 포함한 다. \"기계 판독 가능 신호”와 같은 용어는 기계 명령 및/또는 데이터를 프로그램 가능 프로세서에 제공하기 위 한 임의의 신호를 의미한다. 유저와의 대화를 제공하기 위하여, 컴퓨터 상에서 여기서 설명하는 시스템 및 기술을 실시할 수 있으며, 당해 컴퓨터는 유저에게 정보를 표시하기 위한 디스플레이 장치（예를 들면 CRT（음극선관） 또는 LCD（액정 디스플 레이） 모니터） 및 키보드와 포인팅 장치（예를 들면, 마우스 또는 트랙볼）를 구비할 수 있으며, 유저는 당해 키보드 및 당해 포인팅 장치를 통해 입력을 컴퓨터에 제공할 수 있다. 기타 유형의 장치는 또한 유저와의 대화 를 제공하는데 사용될 수 있다. 예를 들면, 유저에 제공하는 피드백은 임의의 형태의 감각 피드백（예를 들면, 시각적 피드백, 청각적 피드백, 또는 촉각 피드백）일 수 있으며, 또한 임의의 형태（음향 입력, 음성 입력 또 는 촉각 입력을 포함함）를 통해 유저로부터의 입력을 수신할 수 있다. 여기서 설명하는 시스템 및 기술을 백엔드 구성 요소를 포함하는 계산 시스템（예를 들면 데이터 서버）, 또는 미들웨어 구성 요소를 포함하는 계산 시스템（예를 들면 응용 서버）, 또는 프런트엔드 구성 요소를 포함하는 계산 시스템（예를 들면 그래픽 유저 인터페이스 또는 웹 브라우저를 구비하는 유저 컴퓨터인 바, 유저는 당해 그래픽 유저 인터페이스 또는 당해 웹 브라우저를 통해 여기서 설명하는 시스템 및 기술의 실시 방식과 대화함 ）, 또는 이러한 백엔드 구성 요소, 미들웨어 구성 요소, 또는 프런트엔드 구성 요소의 임의의 조합을 포함하는 계산 시스템에서 실시할 수 있다. 임의의 형태 또는 매체의 디지털 데이터 통신（예를 들면, 통신 네트워크）을 통해 시스템의 구성 요소를 상호 연결할 수 있다. 통신 네트워크의 예는 근거리 통신망（LAN）, 광역 통신망（ WAN） 및 인터넷을 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있 고, 또한 일반적으로 통신 네트워크를 통해 대화를 실행한다. 해당되는 컴퓨터 상에서 운행되고, 또한 클라이언 트 - 서버 관계를 갖는 컴퓨터 프로그램을 통해 클라이언트와 서버의 관계를 발생시킬 수 있다. 상기에 나타낸 다양한 형태의 흐름을 이용하여 단계를 재정열, 증가 또는 삭제할 수 있음을 이해해야 한다. 예 를 들면, 본 발명에 기재된 각 단계는 병열로 수행되거나 또는 차례로 수행되거나 또는 다른 순서로 수행될 수 있으며, 본 발명이 개시하는 기술 방안이 원하는 결과를 실현할 수 있는 한, 본 명세서는 이에 대해 한정하지 않는다. 상술한 구체적인 실시 방식은 본 발명의 보호 범위를 한정하지 않는다. 당업자는 설계 요건 및 기타 요인에 따 라 다양한 수정, 조합, 서브 조합 및 대체를 실행할 수 있음을 이해해야 한다. 본 발명의 정신 및 원칙 내에서 이루어진 임의의 수정 동등한 대체 및 개선 등은 모두 본 발명의 보호 범위 내에 포함되어야 한다."}
{"patent_id": "10-2021-0050852", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도면은 본 방안을 더 잘 이해하도록 하기 위한 것이며, 본 발명에 대한 한정을 이루지 않는다. 도 1은 본 발명의 실시예 1에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 방법의 플로우 차트이다. 도 2는 본 발명의 실시예 1에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 각 단계의 모식도이다. 도 3은 본 발명의 실시예 2에 의해 제공되는 평행 코퍼스를 이용하여 모델 트레이닝을 실행하는 모식도이다. 도 4는 본 발명의 실시예 2에 의해 제공되는 모델을 정렬하는 작업 원리의 일 실예시도이다. 도 5는 본 발명의 실시예 3에 의해 제공되는 시맨틱 표현 모델을 트레이닝 하는 장치의 구조도이다. 도 6은 본 발명의 실시예를 실현하기 위한 전자 기기의 블럭도이다."}
