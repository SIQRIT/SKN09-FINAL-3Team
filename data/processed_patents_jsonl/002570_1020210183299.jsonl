{"patent_id": "10-2021-0183299", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0094283", "출원번호": "10-2021-0183299", "발명의 명칭": "추천 알고리즘 학습 가속을 위한 파이프라이닝의 컴퓨팅 시스템 및 그의 방법", "출원인": "한국과학기술원", "발명자": "유민수"}}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "추천 알고리즘 학습 가속을 위한 컴퓨팅 시스템에 있어서, 둘 이상의 이기종의 기억 장치들; 및상기 기억 장치들에 임베딩 파라미터들을 저장하고, 상기 임베딩 파라미터들을 사용하여, 각 학습 이터레이션(iteration)에서 추천 알고리즘을 학습하도록 구성되는 프로세서를 포함하고,상기 프로세서는,상기 기억 장치들 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱하도록 구성되는,컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 기억 장치들은,고용량-저대역폭 기억 장치, 및 저용량-고대역폭 기억 장치를 포함하고,상기 프로세서는,상기 저용량-고대역폭 기억 장치에, 상기 접근될 임베딩 파라미터들을 캐싱하도록 구성되는,컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 프로세서는,미래의 입력 데이터 정보를 활용하여, 상기 접근될 임베딩 파라미터들과 나머지 임베딩 파라미터들을 특정하고, 상기 기억 장치들 중 하나에 대해, 상기 나머지 임베딩 파라미터들을 퇴거(evict)함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치(prefetch)하도록 구성되는,컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서, 상기 프로세서는,상기 나머지 임베딩 파라미터들을 퇴거함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치하는 과정과 현재의 이터레이션에서 상기 추천 알고리즘을 학습하는 과정을 파이프라인하여 병행적으로 실행하도록 구성되는,공개특허 10-2023-0094283-3-컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서, 상기 프로세서는, 상기 나머지 임베딩 파라미터들을 퇴거함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치하는 과정과 상기추천 알고리즘을 학습하는 과정을 파이프라인하여 병행적으로 실행하여도 데이터 의존성을 위배하지 않도록, 상기 나머지 임베딩 파라미터들을 빅팀(victim)들로 선택하도록 구성되는,컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 기억 장치들은,동일한 노드에 배치되거나, 상이한 노드들에 각각 배치되는,컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 기억 장치들의 각각은, 메모리(memory), 스토리지(storage), 또는 스토리지 클래스 메모리(storage class memory) 중 하나인, 컴퓨팅 시스템."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "컴퓨팅 시스템의 추천 알고리즘 학습 가속을 위한 방법에 있어서,둘 이상의 이기종의 기억 장치들에 임베딩 파라미터들을 저장하는 단계; 및상기 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에서 추천 알고리즘을 학습하는 단계를 포함하고,상기 추천 알고리즘을 학습하는 단계는,상기 기억 장치들 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱하는,방법."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 8 항에 있어서,상기 추천 알고리즘을 학습하는 단계는,미래의 입력 데이터 정보를 활용하여, 상기 접근될 임베딩 파라미터들과 나머지 임베딩 파라미터들을 특정하는공개특허 10-2023-0094283-4-단계; 및상기 기억 장치들 중 하나에 대해, 상기 나머지 임베딩 파라미터들을 퇴거함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치하는 단계를 포함하는,방법."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 추천 알고리즘을 학습하는 단계는,현재의 이터레이션에서 상기 추천 알고리즘을 학습하는 단계를 더 포함하고, 상기 나머지 임베딩 파라미터들을 퇴거함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치하는 단계와 상기현재의 이터레이션에서 상기 추천 알고리즘을 학습하는 단계는,파이프라인되어 병행적으로 실행되는,방법."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서, 상기 나머지 임베딩 파라미터들은,상기 나머지 임베딩 파라미터들을 퇴거함과 동시에 상기 접근될 임베딩 파라미터들을 프리페치하는 단계와 상기현재의 이터레이션에서 상기 추천 알고리즘을 학습하는 단계가 파이프라인되어 병행적으로 실행되어도 데이터의존성을 위배하지 않도록, 빅팀들로 선택되는,방법."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 8 항에 있어서,상기 기억 장치들의 각각은, 메모리, 스토리지, 또는 스토리지 클래스 메모리 중 하나인, 방법."}
{"patent_id": "10-2021-0183299", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "추천 알고리즘 학습 가속을 위한 방법을 컴퓨팅 시스템에 실행시키기 위한 하나 이상의 프로그램을 저장하는 비-일시적인 컴퓨터 판독 가능한 기록 매체에 있어서,상기 방법은, 둘 이상의 이기종의 기억 장치들에 임베딩 파라미터들을 저장하는 단계; 및상기 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에서 추천 알고리즘을 학습하는 단계공개특허 10-2023-0094283-5-를 포함하고,상기 추천 알고리즘을 학습하는 단계는,상기 기억 장치들 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱하는,비-일시적인 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 추천 알고리즘 학습 가속을 위한 파이프라이닝의 컴퓨팅 시스템 및 그의 방법에 관한 것으로, 컴퓨팅 시스템은 둘 이상의 이기종의 기억 장치들에 임베딩 파라미터들을 저장하고, 임베딩 파라미터들을 사용하여, 각 학습 이터레이션(iteration)에서 추천 알고리즘을 학습하도록 구성된다. 구체적으로, 컴퓨팅 시스템은 미래의 입 력 데이터 정보를 활용하여, 접근될 임베딩 파라미터들과 나머지 임베딩 파라미터들을 특정하고, 기억 장치들 중 하나에 대해, 나머지 임베딩 파라미터들을 퇴거(evict)함과 동시에 접근될 임베딩 파라미터들을 프리페치 (prefetch)한다. 그리고, 컴퓨팅 시스템은 퇴거/프리페치 과정과 현재의 이터레이션에서 상기 추천 알고리즘을 학습하는 과정을 파이프라인하여 병행적으로 실행한다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 딥러닝 기반의 추천 모델의 학습 과정을 가속하기 위한 것으로, 보다 구체적으로는 추천 알고리즘 학 습 가속을 위한 파이프라이닝의 컴퓨팅 시스템 및 그의 방법에 관한 것이다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "온라인 서비스 제공자는 개인 맞춤형 추천을 위해 딥러닝을 이용한 추천 모델을 구축한다. 추천 모델은 온라인 서비스의 핵심요소 중 하나로, 친구, 컨텐츠, 영화, 상품, 광고 등에 대한 개인 맞춤형 추천을 제공하는 데에 사용된다. 추천 모델의 성능이 온라인 서비스의 품질에 지대한 영향을 미침과 동시에 온라인 광고의 효율성을 좌우하는 관계로 온라인 서비스의 수익과 직결된다. 이러한 이유로 대규모 온라인 서비스를 운영하는 구글, 아 마존, 페이스북 등의 IT 기업에서는 개인 맞춤형 추천 성능의 향상을 위해 크고 복잡한 딥러닝 모델이 사용되며, 이러한 모델의 크기는 점차 증가하고 있다. 페이스북에서 최근 공개한 논문에 따르면, 추천 모델의 학습은 페이스북 데이터 센터에서 수행되는 인공지능 학습 수요의 50% 이상을 차지하며, 이를 효과적으로 가속 하기 위한 연구의 필요성을 강조하였다. 딥러닝 기반의 추천 모델의 경우 임베딩 레이어(Embedding layer)를 사용하여 사용자 및 추천의 대상이 되는 객 체(영화, 상품, 광고 등)에 대한 정보를 학습한다. 따라서 추천을 제공하는 서비스의 규모에 비례하여 추천 모 델의 크기가 증가하는 특성을 가지게 되며, 대규모 온라인 서비스를 위한 추천 모델의 경우 그 크기가 수십 GB ~ 수 TB에 달한다고 알려져 있다. 이러한 대규모 추천 모델의 경우 그 크기가 단일 그래픽프로세서(GPU), 텐서 프로세서(TPU) 등의 인공지능 가속기 하드웨어가 제공하는 메모리의 범위를 초과하게 된다. 따라서 이러한 대형 추천 모델의 학습 및 추론이 가능한 시스템은 다음과 같은 두가지로 분류된다. 첫 번째는 다수의 가속기 장치에 모델을 분산하여 저장 및 학습하는 방법이다. 두 번째는 이기종 시스템을 사용하는 방법으로, 높은 메모리 용량 을 요구하는 임베딩 레이어의 경우 고용량-저대역폭 메모리를 사용하는 중앙처리장치(CPU) 메모리를 이용하여 학습하고, 그 외의 부분은 저용량-고대역폭 메모리를 사용하는 GPU를 이용하여 학습하는 방법이다. 각각의 방법 은 장단점이 존재하며, 두 방법 모두 활발하게 연구 및 사용되고 있는 방법이다. 본 발명은 이기종 시스템을 이 용하여 학습하는 경우를 가정하고 있으며, 해당 시스템에서의 학습을 가속할 수 있는 소프트웨어 기법에 해당한 다. 이기종 시스템을 이용하여 추천 모델을 학습하는 경우에 이를 최적화하기 위한 다음과 같은 선행 연구가 존재한 다. 선행 연구에서는 추천 모델 학습의 다음과 같은 두 가지 특징들을 활용한다. 첫 번째 특징은, 임베딩 레이 어의 경우 학습 이터레이션(iteration)에서 전체 모델 파라미터가 사용되는 것이 아닌 극히 일부의 파라미터씩 학습되는 특징을 가진다는 것이다. 두 번째 특징은, 모든 임베딩 파라미터가 균등하게 접근되는 것이 아니라 임 베딩 파라미터는 서로 다른 접근 빈도를 가지며, 소수의 파라미터가 많은 양의 접근을 야기한다는 것이다. 위와 같은 두 가지 특징들을 이용하여 자주 접근되는 소수의 임베딩 파라미터의 경우 높은 대역폭을 제공하는 GPU 메 모리에서 학습하는 기법이 제안된 바 있다. 이와 같은 선행 연구들에서는 양자화(quantization) 등의 방법을 이 용하여 필요한 메모리 용량을 낮추는 방법을 제안하였으며, 해당 연구들에서 제안한 방법으로는 유의미한 학습 속도 향상을 얻을 수 없으며 원인은 다음과 같다. GPU 메모리의 경우 용량의 제약이 존재하여, 소수의 자주 접 근되는 임베딩만 저장(캐싱) 가능하다. 그러나, 학습 이터레이션(iteration)에서는 해당 임베딩뿐만 아니라, CPU에 존재하는 임베딩에 대한 접근이 동 시에 일어나게 되며, CPU에 존재하는 임베딩을 처리하는 과정이 임계 경로(critical path)에 놓이게 되어 향상 가능한 성능이 제한된다. 확률적 그라디언트하강법(Stochastic gradient decent; SGD) 알고리즘의 연산 과정은 전진 전파, 역전파, 모델 업데이트 과정 순으로 진행되며, 학습 과정에는 데이터 의존성이 존재하여 해당 이터 레이션의 업데이트가 완료되어야 다음 이터레이션의 정전파를 수행할 수 있다. 이와 같은 이터레이션 단위의 데"}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "이터 의존성에 의해 각 이터레이션은 원자적(atomic)으로 실행되는 것이 보통이다. 발명의 내용"}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 목적은 대규모 추천 모델 학습을 가속하기 위한 소프트웨어 기법을 제공하여 학습 속도 가속 및 이를 통한 에너지 소모량 감소를 달성하는 데 있다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시는 추천 알고리즘 학습 가속을 위한 파이프라이닝의 컴퓨팅 시스템 및 그의 방법을 제공한다. 본 개시에 따른 컴퓨팅 시스템은, 둘 이상의 이기종의 기억 장치들, 및 상기 기억 장치들에 임베딩 파라미터들 을 저장하고, 상기 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에서 추천 알고리즘을 학습하도록 구성되 는 프로세서를 포함하고, 상기 프로세서는, 상기 기억 장치들 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱하도록 구성된다. 본 개시에 따른 컴퓨팅 시스템의 방법은, 둘 이상의 이기종의 기억 장치들에 임베딩 파라미터들을 저장하는 단 계, 및 상기 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에서 추천 알고리즘을 학습하는 단계를 포함하 고, 상기 추천 알고리즘을 학습하는 단계는, 상기 기억 장치들 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱한다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 기존 방법에서 병목을 야기하였던 부분의 연산을 단순화하고 병행적 실행을 가능하게 함으로 써 학습 시간 및 에너지 소모량을 비약적으로 개선할 수 있으며, 기존 학습 방법과 기능적으로 동등함을 보장하 기 때문에 동일한 정확도를 달성할 수 있음이 보장된다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서는 추천 모델 학습의 다음과 같은 세 가지 특징들을 이용한다. 첫 번째 특징은, 임베딩 레이어의 학 습의 경우 단일 이터레이션(iteration)에서는 소수의 임베딩 파라미터만을 학습하며, 따라서 이터레이션간 데이 터 의존성이 임베딩 파라미터 전체가 아닌, 해당 이터레이션에서 접근된 임베딩에만 발생한다는 것이다. 두 번 째 특징은, 임의의 이터레이션에서 접근될 임베딩은 입력 데이터(input data)에 의해 결정된다는 것이다. 세 번 째 특징은, 입력 데이터는 데이터셋(dataset)에서 불러오는 정보로 학습 과정 중 불변하며 미리 불러오는 것이 가능하다는 것이다. 따라서, 입력 데이터를 미리 불러오는 과정을 통해 미래의 이터레이션에서 접근 및 업데이 트될 임베딩 파라미터를 명확하게 특정할 수 있음을 알 수 있다. 본 개시의 구성의 핵심적인 부분은 다음과 같다. 본 개시는 임베딩 파라미터를 이기종 메모리 시스템을 사용하 여 저장 및 학습하는 경우를 가정한다. 상기 이기종 메모리 시스템은 지연시간/대역폭/용량 등의 특성에 대해 트레이드오프가 존재하는 둘 이상의 메모리를 사용하여 구성된 시스템을 이용하여 임베딩을 저장하고자 하는 경 우라면 모두 가능하다. 상기 이기종 메모리 시스템은 다음과 같은 실시예들이 존재할 수 있다. 1) 가속기(GPU 등)의 고대역폭 메모리와 CPU 메모리를 사용하는 경우, 2) CPU 메모리와 원격 노드에 존재하는 메모리를 사용하 는 경우(분산형 메모리(disaggregated memory)를 사용하는 경우), 3) CPU 메모리와 SSD/DISK와 같은 스토리지 를 사용하는 경우, 및 4) 인텔(Intel)의 사파이어 래피즈(Sapphire Rapids)와 같이 CPU에서 고대역폭 HBM과 고 용량의 DRAM을 모두 사용하는 경우 등이 있으며, 위의 실시예들을 조합하여 3단계 이상의 메모리 계층을 구성한 시스템에서도 본 개시가 적용 가능하다. 이하에서는 설명의 편의성을 위해 가속기(GPU 등)의 고대역폭 메모리와 CPU의 고용량 메모리를 사용하여 시스템 을 구성한 경우를 가정하여 설명한다. 해당 시스템에서 가속기 메모리는 자주 접근되는 임베딩 파라미터를 캐싱 하는 역할을 수행한다. 상기 캐시는 동적으로 관리되며, 미래의 입력 데이터 정보를 활용하여 다가오는 이터레 이션에 사용되지 않는 임베딩 파라미터를 퇴거(evict)함과 동시에 접근될 모든 임베딩 파라미터를 프리페치 (prefetching)한다. 본 개시는 상기 퇴거/프리페치 과정을 모델 학습 과정과 동시에 병행적으로 수행하여도 데 이터 의존성을 위배하지 않는 빅팀(victim) 선택 알고리즘을 포함한다. 본 발명에서 제안하는 방법을 사용하면 상기 과정들을 파이프라인(pipeline)하여 병행적으로 실행하는 방법을 통해 학습 시간을 비약적으로 단축할 수 있으며, 기존 학습 방법과 기능적으로 동등(functionally equivalent)함을 보장하기 때문에 동일한 정확도를 달 성할 수 있음이 보장된다. 본 개시의 특징 및 효과는 다음과 같이 정리된다. 본 개시는 새로운 하드웨어의 추가 없이 소프트웨어의 변경만 으로 구현이 가능하여 실용적이며 적용 가능성이 높다. 기존 방법과 기능적으로 동등하기 때문에, 학습하고자 하는 모델의 정확도를 떨어뜨리지 않으며, 동일한 수의 이터레이션 수행 시 동일 정확도 달성이 가능하다. 학습 속도 가속을 가능하게 하며, 이를 통해 학습을 완료하는 데에 소모되는 에너지 소모량을 크게 감소시킬 수 있다. 본 개시는 특정 추천 모델이 아닌, 임베딩 레이어를 사용하는 모든 추천 모델에 범용적으로 적용 가능하 다. 본 개시는 GPU 메모리 - CPU 메모리로 나누어진 이기종 메모리 시스템에서의 학습뿐만 아니라, 둘 이상의 지연시간/대역폭/용량 등의 트레이드오프가 있는 저장장치를 사용하여 이기종 메모리 시스템을 구성하는 모든 경우에 대해 적용하여 임베딩 학습을 가속하는 것이 가능하다. 이하에서, 본 개시와 관련되는 사항들이 첨부된 도면을 참조하여 구체적으로 설명된다. 배경 및 관련 연구 권장 모델 및 임베딩 계층 도 1은 임베딩 계층과 다층 퍼셉트론(multi-layer perceptrons; MLP)을 사용하는 DNN 계층의 두 가지 주요 구 성 요소로 구성된 일반적인 DNN 기반 RecSys 모델을 보여준다. RecSys는 일반적으로 특정 이벤트의 확률(예: 전 자상거래 구매자가 추천 제품을 구매할 가능성)을 예측하는 문제로 공식화되므로 RecSys의 정확도는 다양한 입 력 특징의 고유한 특성이 어떻게 포착되는지에 따라 달라진다. 임베딩은 이산 값, 범주형 특징을 연속 실수 벡 터에 투영하는 것이다. 예를 들어, 전자 상거래 서비스에서 판매되는 다른 제품 항목은 이산 ID(즉, 각 프로젝 트 항목에 차별화를 위한 고유 ID가 할당됨)에서 연속적이고 실제 가치가 있는 벡터 표현으로 투영된다. 이러한 프로세스는 임베딩 계층을 사용하여 임베딩을 학습함으로써 이루어진다. 특정 특징에 속하는 고유한 항목의 수 가 일반적으로 수백만에서 수십억에 달하기 때문에(즉, 온라인 비디오 스트리밍 서비스에서 사용 가능한 비디오 또는 전자 상거래에서 판매되는 제품의 수에 비례하는 척도), 모든 임베딩을 저장하는 임베딩 테이블은 수십 GB 의 용량에 이른다. 의미론적 표현을 캡처하는 데 유용한 여러 범주형 특징(예: 사용자 ID, 항목 ID 등)이 있을 수 있기 때문에, RecSys당 수십 개의 임베딩 테이블을 사용할 수 있으며, 전체 모델이 수백 GB에서 일정한 TB의메모리 용량을 소비하도록 한다. 맞춤형 권장 사항에 대한 학습 파이프라인 전진 전파(forward propagation). 도 2의 (a)는 RecSys에서 임베딩 계층을 학습하는 전진 전파 동안 수행된 주 요 연산을 보여준다. 임베딩 테이블을 사용하여 이산 범주형 특징을 임베딩 수집 작업을 통해 해당 벡터 표현에 매핑한다. 즉, 희소 특징 ID 그룹을 활용하여 임베딩 테이블을 인덱싱하고 해당 임베딩을 읽는다. 임베딩 수집 에 사용되는 ID 그룹은 임베딩 테이블 내의 연속 행을 반드시 가리킬 필요는 없으므로 임베딩 수집 작업은 메모 리 대역폭이 제한된 속성을 나타내는 매우 희소하고 불규칙한 메모리 액세스 패턴을 나타낸다. 수집된 임베딩은 요소별 연산을 통해 서로 결합되므로 단일 벡터로 감소한다. 임베딩 감소는 각 테이블에 따라 수행되며 감소된 임베딩은 피처 상호 작용 단계를 통해 하위 MLP 계층(연속 입력 특징을 중간 특징 벡터로 변환하는 역할)의 출 력과 결합된다. 특징 상호작용의 출력은 최상위 MLP 계층에 의해 처리되며, 최종 클릭률(click through rate; CTR)을 예측하기 위해 소프트맥스 함수를 사용하여 처리된다(예: 최종 사용자가 권장 항목을 클릭할 확률). 그 런 다음 MLP 계층의 역전파 단계는 임베딩 계층으로 다시 라우팅되는 그라디언트(gradient) 벡터 집합을 도출한 다. 역전파를 위한 그라디언트의 수는 순방향 전파 동안 감소된 임베딩의 수와 동일하다(예: 도 2의 (a)에서는 2 개, 즉 G[0], G[1]). 역전파(backpropagation). RecSys 학습의 흥미로운 속성은 임베딩 테이블 내부에 저장된 임베딩이 모델 업데이 트의 대상이라는 것이다. 즉, 임베딩 테이블이 학습 과정 중에 읽고 기록된다. 구체적으로, 전진 전파 및 역전 파의 단일 이터레이션 동안, 전진 전파 중에 수집된 임베딩이 역전파 중에 학습의 대상이 될 것이다. 이 프로세 스는 도 2의 (b)에 설명되어 있으며, 여기서 백엔드(backend) MLP 계층에서 역전달된 두 개의 그라디언트는 그 라디언트 분산 연산을 사용하여 임베딩 테이블 내의 여러 행(즉, 전진 전파 중에 수집된 위치 조회)을 업데이트 하는 데 사용된다. 예를 들어, 도 2에서, 첫 번째 배치에 대한 임베딩 테이블의 행 0과 4와 두 번째 배치에 대 한 행 0, 2, 5는 전진 전파(도 2의 (a)) 동안 수집되었으므로, 이러한 위치는 역전파(도 2의 (b)) 동안 그라디 언트 분산 업데이트를 대상으로 한다. 주어진 임베딩은 다른 배치에 걸쳐 여러 번 판독될 수 있기 때문에(예: 0 행의 E[0]는 첫 번째 배치(batch)와 두 번째 배치에 대해 두 번 조회됨), 모델 업데이트에 사용할 최종 그라디 언트를 도출할 때 다수의 역전파 그라디언트를 고려해야 한다. 이것은 메모리 대역폭 제한 작업인 일련의 그라 디언트 복제 및 병합에 의해 처리된다. RecSys 학습을 위한 시스템 아키텍처 전술된 바와 같이, 최첨단 RecSys 모델은 최대 몇 TB의 메모리 사용량을 발생시킬 수 있는 대형 임베딩 테이블 을 사용한다. 학습은 처리량 관련 작업이기 때문에 하이엔드 GPU 또는 TPU는 3D 스택 DRAM과 같이 대역폭에 최 적화되었지만 용량이 제한된 메모리 솔루션(예: HBM)을 사용한다. 이러한 메모리 솔루션은 수십 GB의 스토리지 만 제공되기 때문에 GPU 로컬 메모리 내에 방대한 임베딩 테이블을 저장하기가 매우 어렵다. 결과적으로, RecSys 훈련(training)을 위한 시스템 아키텍처는 일반적으로 하이브리드 CPU-GPU 시스템(예: 페이스북의 시온 (Zion) 시스템, 바이두(Baidu)의 AI박스(AIBox))을 채택한다. 이러한 시스템 설계 지점에서 용량 최적화된 CPU DIMM은 CPU가 메모리 집약적인 임베딩 계층의 학습을 처리하는 반면 GPU는 연산 집약적인 DNN 계층의 학습을 수 행한다. 전술된 바와 같이, 임베딩 계층의 전진 및 역전파의 주요 컴퓨팅 기본 요소는 메모리 대역폭이 매우 제 한적이기 때문에, 느린 CPU 메모리에서 실행할 경우 RecSys 학습에서 상당한 성능 병목 현상이 발생한다. 동기 부여 임베딩 계층 학습의 집약성 도 3은 다양한 실제 RecSys 데이터셋에 포함된 테이블 항목의 (정렬된) 액세스 수를 보여준다. 임베딩 테이블 액세스는 일반적으로 테이블 항목의 작은 하위 집합이 매우 높은 액세스 빈도를 받는 반면 나머지 항목은 적은 수의 액세스만 받는 멱함수 분포를 나타낸다. 이러한 롱테일(long tail) 현상은 일반적으로 RecSys 사용자 중 상당수가 가장 인기 있는 품목의 작은 부분에 관심을 갖는 경우이기 때문에 놀라운 일이 아니다. 집약성의 크기 는 RecSys 모델이 배치되는 위치에 따라 크게 달라지지만, 본 문서에서는 일반적으로 RecSys 데이터셋이 실제로 롱테일을 가지는 멱함수 분포를 나타낸다는 것을 관찰한다. 안타깝게도, 기본 하이브리드 CPU-GPU는 임베딩 계층의 고유한 인접성 특성을 활용하도록 설계되지 않았다. 도 4의 (a)는 기본 CPU-GPU 하에서의 전진 전파 및 역전파의 주요 단계를 보여준다. 도시된 바와 같이, 메모리 대 역폭 제한 임베딩 수집 및 그라디언트 분산(gradient scatters) 작업은 모두 느린 CPU 메모리 시스템을 통해 수 행된다. 따라서 엔드 투 엔드 교육 시간은 계층 임베딩 CPU 측 교육에서 주로 병목 현상을 일으켜 학습 시간의상당 부분을 소비한다(도 5 참조). RecSys용 소프트웨어 관리 임베딩 캐시 테이블 액세스 임베딩의 멱함수 분포를 고려할 때, 이러한 인접성을 활용하는 효과적인 메커니즘은 CPU 메모리 로의 임베딩 수집/그라디언트 분산 트래픽을 줄이는 데 도움이 될 수 있는 소형 GPU 임베딩 캐시(고속 GPU DRAM 에서 관리)를 통합하는 것이다. 도 4의 (b)는 소프트웨어 관리 GPU 임베딩 캐시로 강화된 CPU-GPU 시스템에서의 RecSys 학습 개요를 제공한다. 이전의 연구에서 제안한 정적 캐시 아키텍처를 가정한다. 여기서 전체 RecSys 학 습 과정 동안 제거되지 않고 GPU DRAM에 캐싱하기 위해 가장 자주 액세스하는 임베딩(즉, 도 3의 최상위 N개 테 이블 항목)을 선택한다. 도 5는 정규화된 엔드 투 엔드 학습 시간을 학습의 주요 단계가 실행되는 위치에 따라 세분화한 것이다. 이 실험에서 몇 가지 중요한 관찰을 했다. 이는, 정적 GPU 임베딩 캐시는 메모리 대역폭 제한 임베딩 계층 학습에 소요되는 시간을 눈에 띄게 줄인다는 것이다. 이는 GPU 임베딩 캐시에서 발생하는 핫 임베 딩 때문에 CPU 메모리에서 서비스되는 임베딩 테이블 액세스 수를 필터링할 수 있기 때문이다. 안타깝게도 CPU 메모리에 대한 임베딩 캐시 수집 및 그라디언트 분산 작업을 처리하는 데 걸리는 시간은 여전히 무시할 수 없을 정도로 크며, 이는 평균 학습 시간의 77%에서 94%에 이른다. 특히, 미스(누락) ID에 해당하는 임베딩 계층의 역 전파(도 4의 (b)의 가장 오른쪽 두 검은 단계)는 느린 CPU 메모리를 통해 메모리 대역폭 제한 그라디언트 중복/ 합병/분산을 유발하여 심각한 지연 오버헤드를 유발한다. 평균적으로 정적 GPU 임베딩 캐시는 12%(높은 집약성 데이터셋)에서 91%(낮은 집약성 데이터셋) 캐시 미스(miss)율이 발생하며, 이 모두가 CPU 임베딩 테이블에서 서 비스되어야 한다. 더 큰 GPU 임베딩 캐시가 잠재적으로 CPU 메모리에 대한 트래픽의 대부분을 받아들이고 CPU-GPU의 성능이 \"GPU 전용\" 시스템의 성능에 도달할 수 있다고 주장할 수 있다(즉, 고대역 GPU 메모리는 임베딩 테이블을 모두 저장 하여 임베딩 GPU 메모리 속도로 학습시킬 수 있다). 도 6은 CPU 임베딩 테이블의 많은 부분을 캐시함에 따라 GPU 임베딩 캐시 히트(hit)율이 향상되는 모습을 보여준다. Criteo와 같은 데이터셋의 경우, 핫 임베딩 테이블 항목의 소수는 매우 높은 집약성을 나타낸다(도 6의 (d) 참조). 결과적으로 임베딩 캐시가 크면 CPU 메모리 트 래픽을 더 많이 받아들일 수 있다. 반대로, 낮은 집약성을 가진 데이터셋의 경우(도 6의 (a) 참조) 90%보다 큰 캐시 히트율을 달성하려면 임베딩 테이블 항목의 65% 이상이 GPU 메모리에 캐시되어야 한다. 최첨단 RecSys 모 델은 TB 규모의 메모리 용량을 요구하기 때문에, 수십 GB의 \"부적격한\" GPU 메모리를 고려할 때 이렇게 높은 비 율의 임베딩 테이블을 캐싱하는 것은 추구할 수 없는 설계 지점이다. 본 개시의 목표: GPU 메모리 속도로 임베딩 계층 학습 본 개시의 특성화 루트는 RecSys 학습의 주요 제한 사항으로 메모리 대역폭을 제한했다. 고대역 GPU 메모리 내 부에 가장 자주 액세스하는 핫 임베딩을 캐싱하면 희소 임베딩 수집 및 그라디언트 분산의 성능 저하를 완화할 수 있지만 GPU 메모리 용량이 제한되어 임베딩 테이블의 활성 작업 집합을 완전히 캡처할 수 없다. 이는 GPU 임 베딩 캐시를 여전히 CPU 메모리에서 미스 임베딩을 가져오는 지연 시간 오버헤드로 인해 어려움을 겪게 하며, 이는 RecSys 학습의 중요한 경로에 위치하고 성능을 저하시킨다. 본 개시의 핵심 목표는 임베딩 계층 학습이 시 작될 때 GPU 메모리 내에서 활성 작업 집합을 \"항상\" 사용할 수 있도록 현재뿐만 아니라 미래의 임베딩 테이블 액세스를 지능적으로 저장할 수 있는 GPU 임베딩 캐시를 개발하는 것이다. 이를 통해 임베딩 계층 학습이 처리 량이 높은 GPU 메모리의 속도로 수행되어 엔드 투 엔드 RecSys 학습의 성능을 획기적으로 향상시킬 수 있다. ScratchPipe 아키텍처 및 설계 이하에서는 ScratchPipe 아키텍처와 설계에 대해 자세히 설명한다. 먼저 ScratchPipe 이면의 설계 목적에 대해 자세히 설명하고 제안 동기를 부여하는 주요 관찰 사항을 제시한다. 그런 다음 본 개시가 혁신하는 중요한 연구 과제를 지적하기 위해 활용하는 순수한 허수아비(straw-man) 아키텍처에 대해 설명한다. 마지막으로 ScratchPipe 아키텍처 및 구현 세부 정보가 설명된다. 설계 개요 설계 목적. ScratchPipe는 임베딩 수집 및 그라디언트 분산을 서비스하기 위한 고속 스크래치패드(scratch- pad)로 고대역 GPU 메모리를 관리하는 소프트웨어 런타임 시스템이다. GPU 스크래치패드는 임베딩 학습 절차가 시작되기 직전에 CPU→GPU에서 필요한 임베딩을 주의 깊게(그리고 체계적으로) 가져오도록 설계되어 항상 히트 하는 임베딩 캐시 역할을 한다. 사실상 ScratchPipe의 설계 목적은 GPU 메모리 속도로 임베딩 계층 학습을 가능 하게 하는 \"GPU 전용\" 시스템(하이브리드 CPU-GPU를 통해 설계됨)으로 착각하게 하는 것이다. 주요 관찰 사항. 기존 캐시는 캐시 삽입/교체 정책이 과거 기록을 기반으로 향후 메모리 액세스에서 발생할 수 있는 일에 대한 최선의 추측을 기반으로 하는 사후 대응 정책이기 때문에 전술된 목표(즉, 항상 히트하는 캐 시)를 달성할 수 없다. 본 개시에서 추구하는 \"최적\" 캐시 설계는 곧 액세스 시기와 몇 개의 데이터 요소가 액 세스될지 정확히 파악하고, 이 정보를 활용하여 온디맨드(on-demand) 캐시 액세스가 발생하기 직전에 필요한 데 이터를 캐쉬로 사전 예방적으로 \"프리페치\"하여 캐시 히트이 될 수 있도록 해야 한다. 본 개시의 주요 관찰은 임베딩 계층 학습과 관련하여 실제로 향후 임베딩 테이블 액세스가 언제 얼마나 발생할 지 정확하게 알 수 있다는 것이다. 보다 구체적으로, 임베딩 수집 및 그라디언트 분산(도 2)에 사용되는 희소 특징 ID는 이미 학습 데이터셋의 일부로 기록되었다. 그 이유는 희소 특징 ID에 해당하는 임베딩이 모델 업데이 트의 주요 대상이 될 것이기 때문에 학습 데이터셋은 임베딩 테이블, 즉 임베딩 수집 및 그라디언트 분산을 위 해 참조할 메모리 위치에 대한 인덱싱 정보를 포함한다. 다시 말해, 학습 데이터셋은 현재뿐만 아니라 향후 학 습 반복을 위해 임베딩 테이블 내에서(로) 읽을(쓸) 행을 정확하게 제공하도록 설계되어 있다. ScratchPipe의 참신함은 GPU 메모리를 통해 임베딩 수집 및 그라디언트 분산을 \"항상\" 완벽하게 서비스할 수 있 는 GPU 임베딩 캐시 아키텍처를 개발하기 위해 이러한 정보를 활용한다는 것이다. 도 7은 ScratchPipe 설계 시 본 개시의 주요 접근 방식을 보여준다. 기존 GPU 임베딩 캐시는 미스 임베딩을 CPU 임베딩 테이블에서 GPU로 반 응적으로 가져와 대기 시간 오버헤드를 발생시켜야 하는 캐시 미스를 자주 호출한다. 미스 임베딩이 GPU 메모리 에 복사되면 GPU 메모리 속도로 RecSys 학습을 시작할 수 있다. ScratchPipe에서는 CPU 메모리에서 곧 출시될 임베딩을 사전 추출하여 GPU에 복사하기 위해 학습 데이터셋을 미리 조사한다. CPU→GPU 임베딩 프리페치의 대 기 시간 오버헤드를 숨기기 위해 ScratchPipe는 학습 데이터셋에서 희소 특징 ID의 여러 미니 배치들(mini- batches) 값을 수집하며 동시에 \"파이프라인\" 실행을 통해 여러 학습 이터레이션의 다른 단계를 처리한다. 도 7 에 도시된 바와 같이, CPU→GPU에서 프리페치된 임베딩을 복사하는 지연 시간을 숨기면 모든 파이프라인 \"주기 (cycle)\"마다 단일 RecSys 훈련 이터레이션을 완료할 수 있다(도 7의 빨간색 단계). 이러한 파이프라인 설계의 주요 과제는 미니 배치들의 임베딩 테이블 액세스를 동시에 실행하는 것이 잠재적으로 서로의 GPU 임베딩 캐시 조회를 방해하여 RecSys 학습 알고리즘의 올바른 실행을 방해할 수 있다는 것이다. 결과적으로, 연구 과제는 여 러 입력 미니 배치가 동시에 실행 중일 때 프로그램 실행의 정확성을 위반하지 않고 GPU 임베딩 캐시를 지능적 으로 관리하는 방법에 있다. 이하에서는 ScratchPipe를 위한 순수한 허수아비 아키텍처를 제시하며, 도 7의 비 전을 실현하기 위한 결점과 한계를 논의하여 본 개시에서 다루는 중요한 연구 과제에 동기를 부여한다. ScratchPipe의 Straw-man 아키텍처 정적 캐시가 아닌 동적 임베딩 캐시의 필요성. 도 4의 (b)에서 가정한 정적 임베딩 캐시는 항상 제거되지 않는 상위 N개의 핫 임베딩에 의해 점유된다. ScratchPipe는 현재 및 향후 미니 배치에 대해 어떤 임베딩 테이블 액 세스가 발생할지 동적으로 결정하고 이 정보를 활용하여, 1) CPU 메모리에서 임베딩(현재 GPU 임베딩 캐시 내부 에 미스)을 사전 예방적으로 프리페치하고, 2) 사전에 GPU 임베딩 캐시로 전송할 수 있는 기능을 필요로 한다. 온디맨드 임베딩 수집 작업이 발생한다. 이러한 동적 임베딩 캐시를 설계하려면 다음 기능이 필요하다: 1) 희소 특징 ID 중 히트 또는 미스를 결정하는 기능, 2) CPU 임베딩 테이블에서 미스 임베딩을 수집하기 위해 해당 정 보를 활용하는 기능, 및 3) GPU 임베딩 캐시에서 제거하기 위해 (미스 ID의 수와 동일한) 여러 임베딩을 선택하 는 기능. CPU에서 수집된 미스 임베딩이 GPU 임베딩 캐시 내에 있는 제거된 항목의 슬롯에 삽입될 수 있도록 하 기 위함이다. 도 8에서 볼 수 있듯이 동적 GPU 임베딩 캐시를 사용하는 본 개시의 허수아비 아키텍처는 RecSys 학습을 위해 다음 단계를 거친다. 1. [쿼리(Query)] 단계: CPU→GPU에서 현재 학습 반복에 대한 희소 특징 ID를 복사하고 GPU 임베딩 캐시를 쿼리 하여 히트/미스 ID를 확인한다. 2. [수집(Collect)] 단계: 미스 ID를 사용하여 CPU 임베딩 테이블에서 해당 임베딩을 수집한다. 동시에 GPU는, 1) 제거된 임베딩을 CPU의 임베딩 테이블에 다시 쓸 수 있도록 GPU 임베딩 캐시에서 동일한 수의 빅팀 임베딩을 수집하며, 2) CPU에서 수집된 미스 임베딩을 제거 슬롯에 삽입할 수 있다. 후술되겠지만, GPU 임베딩 캐시에서 제거된 임베딩을 CPU 메모리에 다시 써야 하는 이유는 GPU 임베딩 캐시가 학습된 임베딩의 최신 버전을 보유하 고 있기 때문이다. 3. [교환(Exchange)] 단계: 미스 임베딩은 CPU→GPU에서 복사하는 동시에 PCIe를 통해 GPU→CPU에서 제거된 임 베딩도 복사한다. 4. [삽입(Insert)] 단계: CPU↔GPU 교환 임베딩을 사용하여 CPU 임베딩 테이블을 제거된(GPU 임베딩 캐시) 임베 딩으로 업데이트하고 GPU는 미스 임베딩을 GPU 임베딩 캐시로 채운다.[삽입] 단계에 도달하면 GPU 임베딩 캐시는 이제 임베딩 계층의 전진 전파 및 역전파를 거치는 데 필요한 전체 임베딩 집합을 보유한다. 구체적으로, [전진 임베딩(Embedding Forwar)d] 단계는 GPU 임베딩 캐시(모두 히트될 것임)에서 필요한 임베딩을 모두 수집하고 나머지 MLP 전진 전자 및 역전파를 거친다. 최종 그라디언트가 준비 되면 [파라미터 업데이트(Parameter Update)] 단계는 GPU 임베딩 캐시의 해당 행을 업데이트된 모델 값으로 덮 어쓴다. 결과적으로, (동적) GPU 임베딩 캐시에 삽입된 모든 임베딩은 학습 대상이기 때문에 임베딩 캐시에서 제거된 모든 항목은 주 CPU 임베딩 테이블에 다시 기록되어야 한다. 허수아비 아키텍처의 한계. 허수아비 아키텍처의 중요한 문제는 [캐시 관리(Cache Management)] 단계가 RecSys 학습의 중요한 경로에 있다는 것이다. 도 7을 참조하여 설명된 바와 같이, 본 개시의 목표는 파이프라인 실행을 통해 여러 학습 미니 배치들을 수집하고 동시에 여러 학습 이터레이션의 다른 단계를 처리하는 것이다. 이는 [쿼리→수집] 단계에서 수집된 임베딩이 GPU에 프리페치되는 효과를 가질 수 있게 한다. 안타깝게도, 허수아비 아키텍처의 여러 단계는 GPU 임베딩 캐시와 CPU 임베딩 테이블 모두에서 여러 RAW(read-after-write) 데이터 의 존성을 발생시키며, 이로 인해 허수아비는 도 8의 여러 단계를 동시에 실행할 수 없게 될 수 있다. 특히 GPU 임 베딩 캐시는 다음과 같은 한 쌍의 단계들 동안 RAW 의존성의 소스가 된다. · [파라미터 업데이트] (W) → [전진 임베딩] (R) (도 8의 RAW-①): 임베딩 캐시를 업데이트하기 위해 학습된 임베딩 값 쓰기 → 전진 전파를 위해 임베딩 캐시에서 임베딩 읽기. · [파라미터 업데이트] (W) → [수집] (R) (RAW-②): 임베딩 캐시를 업데이트 하기 위해 학습된 임베딩 값 쓰 기 → 임베딩 캐시로부터 빅팀 임베딩 읽기. · [삽입] (W) → [수집] (R) (RAW-③): 미스 임베딩을 임베딩 캐시에 쓰기 → 임베딩 캐시에서 빅팀 임베딩 읽 기. 마찬가지로 CPU 임베딩 테이블은 [삽입] (W)(즉, CPU 임베딩 테이블에 제거된 임베딩 쓰기) 및 [수집] (R) (즉, CPU 임베딩 테이블에서 미스 임베딩 읽기), 도 8의 RAW-④ 동안 RAW 의존성의 또 다른 소스가 된다. 이러한 모 든 RAW 의존성은 서로 다른 학습 반복이 순차적으로 실행될 때 복잡함 없이 자연스럽게 해결된다. 그러나 허수 아비 아키텍처를 파이프라인하려고 할 때 RAW 의존성은 파이프라인 내부에서 데이터 위험을 유발하고 RecSys 학 습의 올바른 실행을 방해한다(도 9). \"파이프라인\" ScratchPipe 아키텍처 ScratchPipe의 \"파이프라인\" 버전인 ScratchPipe의 최종 제안은 원칙적인 설계 접근 방식으로 허수아비의 모든 단점을 전체적으로 해결한다. 설명의 명확성을 위해, 우리는 이제 RAW 의존성(① 내지 ④)이 발생하는 한 쌍의 단계들을 RAW 의존성 단계라고 부른다. 파이프라인 ScratchPipe는 6-단계 파이프라인(도 10)에 [계획(Plan)] 단계(허수아비의 [쿼리] 대신)를 통합하며 다음과 같은 기능을 제공한다. 1. [계획] 단계는 특히 [수집] 및 [삽입] 단계에서 남은 파이프라인 단계에서 수집하고 채울(CPU 임베딩 테이블 에서 GPU 임베딩 캐시로) 임베딩 및 퇴거하고 후기입(write-back)할(GPU 임베딩 캐시에서 CPU 임베딩 테이블로) 임베딩을 미리 계획하는 주요 제어 장치 역할을 한다. 2. 허수아비의 [쿼리] 단계 동안 수행되는 모든 절차는 [계획]에서 수행된다. 즉, CPU→GPU에서 현재 미니 프로 세서에 대한 희소 특징 ID를 복사하고 GPU 임베딩 캐시를 쿼리하여 히트/미스 ID를 결정한다. ScratchPipe의 [계획] 단계에서는 전반적으로 파이프라인 내부에서 발생하는 RAW 위험을 제거하여 ScratchPipe 의 프리페치 지연 시간을 효과적으로 숨겨 RecSys 학습에 문제를 일으키지 않도록 하는 것이 목표이다. \"희소(sparse)\" 임베딩 테이블의 RAW 종속성. 임베딩 테이블 액세스의 고유한 특성은 매우 희소하다는 점이다. 즉, 단일 훈련 반복은 임베딩 테이블의 수백만 개에서 수십억 개 항목 내에서 수만 개의 행을 터치하는 것뿐이 라는 점을 기억한다. RAW 데이터 종속성 관점에서 이러한 속성의 함축은 다음과 같다. GPU 임베딩 캐시와 CPU 임베딩 테이블에 대한 읽기/쓰기에 사용되는 행 ID가 RAW 종속성 단계에서 겹치지 않는 한, 본 개시는 효과적인 데이터 의존성을 완전히 제거할 수 있으며, 따라서 발생으로부터 모든 데이터 위험을 해결할 수 있다. 키는 GPU 임베딩 캐시 또는 CPU 임베딩 테이블의 \"읽기\" 작업에 사용되는 행 ID 집합이 RAW 종속 단계에서 \"쓰기\" 작업에 사용되는 행 ID 집합과 일치하지 않는지 확인하는 것이다. 이하에서는 파이프라인 RecSys 교육에서 모든 RAW 의 존성을 제거하기 위한 원칙적인 접근 방식을 자세히 설명한다.RAW-① 제거. [전진 임베딩]에 사용된 희소 특징 ID가 [파라미터 업데이트]에 사용된 ID와 정확히 일치하기 때 문에 RAW 데이터 종속성은 제거할 수 없으며 모든 상황에서 준수해야 하는 기본적인 ID이다. 도 10에서 보듯이 ScratchPipe는 의존성을 존중하여 ScratchPipe의 파이프라인 실행 내에서 [모델 훈련]의 네 단계를 모두 단일 단계([훈련] 단계로 표시)로 실행하여 이러한 RAW 의존성을 처리한다. RAW-②/③ 제거. 데이터 위험 야기로부터 RAW-②/③ 방지를 위해 ScratchPipe는 쓰기(예: [파라미터 업데이트] 또는 [삽입] (W)를 통해 업데이트하도록 예약된 GPU 임베딩 캐시 항목이 임베딩 캐시에서 읽혀져 CPU 임베딩 테 이블에 다시 기록되는 빅팀으로 너무 일찍 선택되지 않도록 보장해야 한다. ScratchPipe는 다음 메커니즘을 통 해 RAW-②/③ 의존성을 모두 처리한다. [계획]이 GPU 임베딩 캐시에서 제거할 빅팀을 선택할 때 ScratchPipe는 이전 세 번의 훈련 반복 동안 사용된 입력 희소 특징 ID 집합(즉, [훈련] 단계와 [수집] 단계 사이의 거리)를 빅팀으로 간주하지 않는다. 정상 상태에서는 총 6개의 훈련 미니 배치들(6 개 집합의 입력 희소 특징 ID들에 해 당)가 동시에 처리되지만 다른 단계에서 실행된다. 입력 미니 배치가 [계획] 단계에 들어가면 ScratchPipe 컨트 롤러는 이전의 3개의 입력 미니 배치들을 검사하고([계획] 단계의 관점에서 [수집-교환-삽입] 단계에서 실행) 캐시 제거 후보에서 모든 입력 희소 특징 ID를 제외한다. 이를 통해 [계획]에서 현재 실행 중인 입력 미니 배치 는 RAW 종속 단계에서 실행되는 이전 입력 미니 배치에 의해 업데이트((W))될 임베딩 중 하나를 제거(즉, 읽기 (R))하지 않고 효과적으로 위험을 제거할 수 있다. RAW-④ 제거. RAW-②/③에 대한 위해성 차단은 RAW 의존성의 \"읽기\" 부분을 제어하는 문제이다(즉, GPU 임베딩 캐시에 대한 \"쓰기\"는 [계획] 단계에서 제어할 수 없는 이전 입력 미니 배치에 의해 수행되므로 ScratchPipe 컨 트롤러가 RAW를 방지하기 위해 행 ID를 호출하는 잠재적 위험을 적절히 배제한다). RAW-④를 제거하는 것은 RAW 의존성의 소스가 CPU 임베딩 테이블에 있기 때문에 반대이다. 여기서 CPU 임베딩 테이블에 대한 \"쓰기\"는 GPU 임베딩 캐시 대상을 선택하는 [계획] 단계의 인공물이다. 제거된 항목의 임베딩 테이블에 대한 라이트백은 [삽 입] 단계에서 발생하므로 RAW-④에 대한 RAW 위험을 방지하는 것은 RAW 의존성의 \"쓰기\" 부분을 제어하고 향후 입력 미니 배치가 현재 선택한 제거 후보와 충돌하지 않도록 하는 문제이다(즉, [삽입]에서 실행되는 미니 배치 의 관점에서, RAW 의존 [수집] 단계에서 실행되는 미니 배치는 미래 입력이다). ScratchPipe는 [계획]이 GPU 임 베딩 캐시에서 제거할 빅팀을 선택할 때 ScratchPipe는 다음 두 번의 훈련 반복 동안 사용되는 입력 희소 특징 ID 집합(즉, [삽입]와 [수집] 사이의 거리)을 빅팀으로 간주하지 않는다. 전체 종합. 도 10과 같이 ScratchPipe는 잠재적인 RAW 위험을 체계적으로 평가하여 슬라이딩 윈도우 기반 [계획] 단계를 통해 위험을 제거한다. 입력 미니 배치가 [계획] 단계에 들어가면, 희소 특징 ID의 다음 2개(미 래 윈도우) 미니 배치뿐만 아니라 이전 3개(과거 윈도우) 미니 배치가 검사된다. [계획] 제어부(control unit) 는 과거/미래 윈도우에 속하는 희소 특징 ID의 상위 집합을 생성함으로써 상위 집합에 포함된 ID를 캐시 제거 후보에서 배제하여 파이프라인의 후속 단계에서 RAW 위험이 발생하는 것을 방지한다. 구현 이제, 도 11을 예시로 사용하여 ScratchPipe의 구현 세부 사항에 대해 설명한다. GPU 스크래치패드 설계. 도시된 바와 같이 ScratchPipe의 GPU 임베딩 캐시(즉, GPU 스크래치패드)는 1) 캐시 임 베딩 벡터(스토리지(Storage) 표시)를 저장하는 데이터 어레이, 및 2) 캐시 쿼리 결과를 히트 또는 미스로 반환 하는 히트-맵(Hit-Map)이라 불리는 (키(key), 값(value)) 저장소를 사용하여 구현된다. 즉 (키, 값) 저장소는 ID(키) 및 인덱스를 사용하여 스토리지 어레이(값) 내에 캐시된 임베딩을 찾는다. 지정된 미니 배치가 [계획] 단계에 들어갈 때마다 GPU 스크래치패드의 히트-맵을 쿼리하여 히트/미스를 도출하고, 그 결과는 CPU 임베딩 테 이블에서 수집할 임베딩과 GPU 스크래치패드(있는 경우)에서 제거할 항목을 예약하는데 사용된다. GPU 스크래치 패드 설계의 고유한 특성은 히트-맵 및 스토리지의 상태가 (목적적으로) 비동기식 및 지연 방식으로 업데이트된 다는 것이다. 도 11에 표시된 것처럼, [계획]에서 새로운 미니 배치가 처리될 때마다 히트-맵의 상태가 업데이 트되는 반면, 스토리지 어레이는 미니 배치가 히트-맵 미스로 [삽입]에 진입할 때 업데이트된다. 이는 ScratchPipe의 파이프라인 설계의 인공물로, 향후 스토리지 어레이 \"4\" 주기의 임베딩 캐싱 상태(즉, [훈련]과 [계획] 사이의 거리)를 항상 반영하도록 히트-맵의 상태를 나타낸다. 예를 들어, 스토리지 어레이가 여전히 비 어 있더라도 ID 3010/7089의 두 번째 미니 배치는 두 번째 주기 동안 히트-맵을 쿼리할 때 미스/히트로 반환된 다(도 11의 (b)). 히트-맵과 스토리지 상태 사이의 이러한 불일치는 의도적인 것인데, 1) 스토리지 어레이가 [삽입] 단계에 도달하지 않았기 때문에 첫 번째 미니배치의 ID 7089/2021 쿼리로 업데이트되지 않아야 하며, 2) 두 번째 미니 배치는 GPU 스크래치패드의 정확한 캐싱 상태를 확인할 수 있어야 하고, 이는 현재 [계획] 단계임 에도 불구하고 CPU에서 프리페치할 임베딩 집합을 정확하게 결정할 수 있어야 한다(즉, 첫 번째 미니 배치가 훈련을 완료했다고 가정한 상태). RAW 위험을 제거하기 위해 마스크를 유지. ScratchPipe는 [계획] 단계에서 현재 슬라이딩 윈도우([계획]를 중심 으로) 아래에 있는 이전 3 개(과거 윈도우) 및 다음 2 개(미래 윈도우) 미니 배치의 임베딩 테이블 조회 ID를 검사하도록 하여 이러한 캐시된 위치가 GPU 스크래치패드에서 제거되지 않도록 함으로써 RAW 위험을 제거한다. 도 11과 같이, 본 개시는 비트 마스크(bitmask)(비트 마스크 내의 비트 수는 Storage 어레이의 캐시 가능한 슬 롯 수와 동일함)인 홀드(Hold) 마스크라는 데이터 구조를 사용한다. [계획] 단계는 스크래치패드의 스토리지 어 레이 내에서 현재 미니 배치가 [훈련] 단계에서 활용할 위치를 지정하는 데 사용한다. 홀드 마스크는 슬라이딩 윈도우 기반 작동을 수용하기 위해 원형 큐를 사용하여 설계되었으며, 슬라이딩 윈도우가 효과적일 때 제거 대 상이 되지 않아야 하는 스토리지의 과거 3개, 현재 1개, 및 미래 2개 미니 배치의 캐시 위치를 추적하는 6개의 비트 마스크 집합을 저장한다. [계획] 단계의 제어 장치가 GPU 스크래치패드 제거를 위한 공격 대상을 선택해야 할 경우 홀드 마스크를 검사한다. 공격 대상 후보는 '0' 값으로 설정된 홀드 마스크의 비트 위치에 해당하는 스 토리지 어레이 위치로 선택된다. 즉, GPU 스크래치패드 내부에 이 위치를 유지하도록 요청하는 현재 슬라이딩 윈도우에 속하는 희소 특징 ID 중 어느 것도 제거가 허용되지 않는다. 예를 들어 스토리지 어레이 내의 세 번째 요소로 저장된 E는 네 번째 주기 이후 홀드 마스크의 해당 위치가 \"000\"이므로 다섯 번째 사이클에서 제 거 대상으로 지정된다(도 11의 (d), (e)). ScratchPipe가 0이 아닌 값으로 설정된 홀드 마스크 내의 비트 위치가 스크래치패드 제거의 대상이 되지 않도록 하려면 스토리지 어레이가 현재 슬라이딩 윈도우에 해당하는 6개의 미니 배치의 처리 내에서 GPU 스크래치패드 의 최악의 사용을 수용할 수 있을 만큼 충분히 커야 한다. 전술된 내용에 따라, 본 개시의 다양한 실시예들에 따른 컴퓨팅 시스템 및 그의 방법이 다음과 같이 구현 될 수 있다. 도 12는 본 개시에 따른 컴퓨팅 시스템을 도시하는 도면이다. 도 12를 참조하면, 컴퓨팅 시스템은 입력 모듈, 출력 모듈, 둘 이상의 이기종의 기억 장치들 (130, 140), 또는 프로세서 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 컴퓨팅 시스템의 구 성 요소들 중 적어도 하나가 생략될 수 있으며, 적어도 하나의 다른 구성 요소가 추가될 수 있다. 어떤 실시예 에서, 컴퓨팅 시스템의 구성 요소들 중 적어도 두 개가 하나의 통합된 회로로 구현될 수 있다. 어떤 실시 예에서, 컴퓨팅 시스템의 구성 요소들은 동일한 노드에 배치될 수 있으며, 상이한 노드들에 각각 배치될 수도 있다. 입력 모듈은 컴퓨팅 시스템의 적어도 하나의 구성 요소에 사용될 신호를 입력할 수 있다. 입력 모듈 은, 사용자가 컴퓨팅 시스템에 직접적으로 신호를 입력하도록 구성되는 입력 장치, 주변의 변화를 감 지하여 신호를 발생하도록 구성되는 센서 장치, 또는 외부 기기로부터 신호를 수신하도록 구성되는 수신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 입력 장치는 마이크로폰(microphone), 마우스(mouse) 또는 키보 드(keyboard) 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 입력 장치는 터치를 감지하도록 설정된 터치 회로(touch circuitry) 또는 터치에 의해 발생되는 힘의 세기를 측정하도록 설정된 센서 회로 중 적어도 하나를 포함할 수 있다. 출력 모듈은 컴퓨팅 시스템의 외부로 정보를 출력할 수 있다. 출력 모듈은, 정보를 시각적으로 출력하도록 구성되는 표시 장치, 정보를 오디오 신호로 출력할 수 있는 오디오 출력 장치, 또는 정보를 무선으 로 송신할 수 있는 송신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 표시 장치는 디스플레이, 홀로그램 장치 또는 프로젝터 중 적어도 하나를 포함할 수 있다. 일 예로, 표시 장치는 입력 모듈의 터치 회로 또는 센서 회로 중 적어도 하나와 조립되어, 터치 스크린으로 구현될 수 있다. 예를 들면, 오디오 출력 장치는 스피 커 또는 리시버 중 적어도 하나를 포함할 수 있다. 일 실시예에 따르면, 수신 장치와 송신 장치는 통신 모듈로 구현될 수 있다. 통신 모듈은 컴퓨팅 시스템에 서 외부 기기와 통신을 수행할 수 있다. 통신 모듈은 컴퓨팅 시스템와 외부 기기 간 통신 채널을 수립하고, 통신 채널을 통해, 외부 기기와 통신을 수행할 수 있다. 여기서, 외부 기기는 차량, 위성, 기지국, 서버 또는 다른 컴퓨팅 시스템 중 적어도 하나를 포함할 수 있다. 통신 모듈은 유선 통신 모듈 또는 무선 통신 모듈 중 적어도 하나를 포함할 수 있다. 유선 통신 모듈은 외부 기기와 유선으로 연결되어, 유선으로 통신할 수 있다. 무선 통신 모듈은 근거리 통신 모듈 또는 원거리 통신 모듈 중 적어도 하나를 포함할 수 있다. 근거리 통 신 모듈은 외부 기기와 근거리 통신 방식으로 통신할 수 있다. 예를 들면, 근거리 통신 방식은, 블루투스(Bluetooth), 와이파이 다이렉트(WiFi direct), 또는 적외선 통신(IrDA; infrared data association) 중 적어 도 하나를 포함할 수 있다. 원거리 통신 모듈은 외부 기기와 원거리 통신 방식으로 통신할 수 있다. 여기서, 원 거리 통신 모듈은 네트워크를 통해 외부 기기와 통신할 수 있다. 예를 들면, 네트워크는 셀룰러 네트워크, 인터 넷, 또는 LAN(local area network)이나 WAN(wide area network)과 같은 컴퓨팅 네트워크 중 적어도 하나를 포 함할 수 있다. 이기종의 기억 장치들(130, 140)은 컴퓨팅 시스템의 적어도 하나의 구성 요소에 의해 사용되는 다양한 데 이터를 저장할 수 있다. 여기서, 기억 장치들(130, 140)은 동일한 노드에 배치되거나, 상이한 노드들에 각각 배 치될 수 있다. 이 때, 기억 장치들(130, 140)에는 지연시간/대역폭/용량 등의 특성에 대해 트레이드오프가 존재 할 수 있다. 예를 들면, 기억 장치들(130, 140)은 고용량-저대역폭 기억 장치, 및 저용량-고대역폭 기억 장치를 포함할 수 있다. 이 때, 기억 장치들(130, 140)의 각각은 메모리(memory), 스토리지(storage), 또 는 스토리지 클래스 메모리(storage class memory) 중 하나일 수 있다. 프로세서는 기억 장치들(130, 140)의 프로그램을 실행하여, 컴퓨팅 시스템의 적어도 하나의 구성 요 소를 제어할 수 있다. 이를 통해, 프로세서는 데이터 처리 또는 연산을 수행할 수 있다. 프로세서는 기억 장치들(130, 140)에 임베딩 파라미터들을 저장하고, 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에 서 추천 알고리즘을 학습하도록 구성될 수 있다. 이 때, 프로세서는 기억 장치들(130, 140) 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미 터들을 캐싱하도록 구성될 수 있다. 여기서, 프로세서는 저용량-고대역폭 기억 장치에, 접근될 임베 딩 파라미터들을 캐싱할 수 있다. 구체적으로, 프로세서는, 미래의 입력 데이터 정보를 활용하여, 접근될 임베딩 파라미터들과 나머지 임베딩 파라미터들을 특정하고, 기억 장치들(130, 140) 중 하나에 대해, 나머지 임 베딩 파라미터들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프리페치하도록 구성될 수 있다. 그리고, 프로 세서는, 나머지 임베딩 파라미터들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프리페치하는 과정과 현재의 이터레이션에서 추천 알고리즘을 학습하는 과정을 파이프라인하여 병행적으로 실행하도록 구성될 수 있 다. 이를 위해, 프로세서는, 나머지 임베딩 파라미터들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프 리페치하는 과정과 추천 알고리즘을 학습하는 과정을 파이프라인하여 병행적으로 실행하여도 데이터 의존성을 위배하지 않도록, 나머지 임베딩 파라미터들을 빅팀들로 선택하도록 구성될 수 있다. 도 13은 본 개시에 따른 컴퓨팅 시스템의 방법을 도시하는 도면이다. 도 13을 참조하면, 컴퓨팅 시스템의 방법은, 둘 이상의 이기종의 기억 장치들(130, 140)에 임베딩 파라미 터들을 저장하는 단계, 및 임베딩 파라미터들을 사용하여, 각 학습 이터레이션에서 추천 알고리즘을 학습 하는 단계를 포함할 수 있다. 추천 알고리즘을 학습하는 단계는, 기억 장치들(130, 140) 중 하나에, 미래의 학습 이터레이션에서 접근될 임베딩 파라미터들을 캐싱할 수 있다. 추천 알고리즘을 학습하는 단계는, 미래의 입력 데이터 정보를 활용하여, 접근될 임베딩 파라미터들과 나 머지 임베딩 파라미터들을 특정하는 단계, 및 기억 장치들(130, 140) 중 하나에 대해, 나머지 임베딩 파라미터 들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프리페치하는 단계를 포함할 수 있다. 추천 알고리즘을 학습 하는 단계는, 현재의 이터레이션에서 추천 알고리즘을 학습하는 단계를 더 포함할 수 있다. 나머지 임베딩 파라미터들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프리페치하는 단계와 현재의 이터레이션에서 추천 알고리즘을 학습하는 단계는, 파이프라인되어 병행적으로 실행될 수 있다. 나머지 임베딩 파라미터들은, 나머지 임베딩 파라미터들을 퇴거함과 동시에 접근될 임베딩 파라미터들을 프리페치하는 단계와 현재의 이터레이션에서 추천 알고리즘을 학습하는 단계가 파이프라인되어 병행적으로 실행되어도 데이터 의존성을 위배하지 않도록, 빅 팀들로 선택될 수 있다. 본 개시는 대규모 추천 모델의 학습을 필요로 하는 분야에 적용 가능하다. 더 나아가 대형 임베딩 레이어를 사 용하는 모든 알고리즘 학습 과정에 적용가능 하다. 이에 대한 예시로는 언어 모델, 그래프 신경망 알고리즘이 있을 수 있다. 추천 모델의 성능은 온라인 광고 수익과 직결되기 때문에 온라인 플랫폼 운영 회사에서 매우 중요하게 여겨진다. 이와 같은 추천 모델의 경우 연구 개발을 위한 학습 수요뿐만 아니라 새로운 데이터를 이용하여 지속 적으로 모델을 학습하는 과정이 필요하여 학습 과정의 가속 및 에너지 효율성 증대를 위한 기술 수요가 매우 크 다. 본 개시의 경우 소프트웨어 구현만으로 유의미한 학습 가속 및 에너지 효율성 증대를 달성할 수 있는 기술로, 시장에서의 수요가 충분할 것으로 기대된다. 본 개시에서 제안하는 기법은 온라인 플랫폼 기업의 학습 프레임워크에 구현될 수 있을 뿐만 아니라, 현재 매우 중요한 인공지능 모델로 여겨지는 추천 모델의 학습을 효과적으로 가속할 수 있는 기법을 제공하는 만큼 텐서플 로우(TensorFlow), 파이토치(PyTorch)와 같은 머신러닝 프레임워크 단계에서 구현이 이루어질 가능성 또한 존재 한다. 구체적으로는 머신러닝 프레임워크에서 제공하는 임배딩 레이어 모듈의 백엔드 구현으로 해당 아이디어가 구현되는 것이 가능하다. 본 개시는 소프트웨어 기술에 해당하는 발명으로 특허 권한 판매 혹은 특허 사용료로 인한 수익을 기대할 수 있 을 것으로 생각된다. 상술한 방법은 컴퓨터에서 실행하기 위해 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램으로 제공될 수 있다. 매체는 컴퓨터로 실행 가능 한 프로그램을 계속 저장하거나, 실행 또는 다운로드를 위해 임시 저장하 는 것일 수도 있다. 또한, 매체는 단일 또는 수개 하드웨어가 결합된 형태의 다양한 기록 수단 또는 저장수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트워크 상에 분산 존재하는 것일 수 도 있다. 매체의 예시로는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD 와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등을 포함하여 프로그램 명령어가 저장되도록 구성된 것이 있을 수 있다. 또한, 다른 매체 의 예시로, 애플리케이션을 유통하는 앱 스토어나 기타 다양한 소프트웨어를 공급 내지 유통하는 사이트, 서버 등에서 관리하는 기록매체 내지 저장매체도 들 수 있다. 본 개시의 방법, 동작 또는 기법들은 다양한 수단에 의해 구현될 수도 있다. 예를 들어, 이러한 기법들은 하드 웨어, 펌웨어, 소프트웨어, 또는 이들의 조합으로 구현될 수도 있다. 본원의 개시와 연계하여 설명된 다양한 예 시적인 논리적 블록들, 모듈들, 회로들, 및 알고리즘 단계들은 전자 하드웨어, 컴퓨터 소프트웨어, 또는 양자의 조합들로 구현될 수도 있음을 통상의 기술자들은 이해할 것이다. 하드웨어 및 소프트웨어의 이러한 상호 대체를 명확하게 설명하기 위해, 다양한 예시적인 구성요소들, 블록들, 모듈들, 회로들, 및 단계들이 그들의 기능적 관 점에서 일반적으로 위에서 설명되었다. 그러한 기능이 하드웨어로서 구현되는지 또는 소프트웨어로서 구현되는 지의 여부는, 특정 애플리케이션 및 전체 시스템에 부과되는 설계 요구사항들에 따라 달라진다. 통상의 기술자 들은 각각의 특정 애플리케이션을 위해 다양한 방식들로 설명된 기능을 구현할 수도 있으나, 그러한 구현들은 본 개시의 범위로부터 벗어나게 하는 것으로 해석되어서는 안된다. 하드웨어 구현에서, 기법들을 수행하는 데 이용되는 프로세싱 유닛들은, 하나 이상의 ASIC들, DSP들, 디지털 신 호 프로세싱 디바이스들(digital signal processing devices; DSPD들), 프로그램가능 논리 디바이스들 (programmable logic devices; PLD들), 필드 프로그램가능 게이트 어레이들(field programmable gate arrays; FPGA들), 프로세서들, 제어기들, 마이크로제어기들, 마이크로프로세서들, 전자 디바이스들, 본 개시에 설명된 기능들을 수행하도록 설계된 다른 전자 유닛들, 컴퓨터, 또는 이들의 조합 내에서 구현될 수도 있다. 따라서, 본 개시와 연계하여 설명된 다양한 예시적인 논리 블록들, 모듈들, 및 회로들은 범용 프로세서, DSP, ASIC, FPGA나 다른 프로그램 가능 논리 디바이스, 이산 게이트나 트랜지스터 로직, 이산 하드웨어 컴포넌트들, 또는 본원에 설명된 기능들을 수행하도록 설계된 것들의 임의의 조합으로 구현되거나 수행될 수도 있다. 범용 프로세서는 마이크로프로세서일 수도 있지만, 대안으로, 프로세서는 임의의 종래의 프로세서, 제어기, 마이크로 제어기, 또는 상태 머신일 수도 있다. 프로세서는 또한, 컴퓨팅 디바이스들의 조합, 예를 들면, DSP와 마이크로 프로세서, 복수의 마이크로프로세서들, DSP 코어와 연계한 하나 이상의 마이크로프로세서들, 또는 임의의 다른 구성의 조합으로서 구현될 수도 있다. 펌웨어 및/또는 소프트웨어 구현에 있어서, 기법들은 랜덤 액세스 메모리(random access memory; RAM), 판독 전 용 메모리(read-only memory; ROM), 비휘발성 RAM(non-volatile random access memory; NVRAM), PROM(programmable read-only memory), EPROM(erasable programmable read-only memory), EEPROM(electrically erasable PROM), 플래시 메모리, 컴팩트 디스크(compact disc; CD), 자기 또는 광학 데이 터 스토리지 디바이스 등과 같은 컴퓨터 판독가능 매체 상에 저장된 명령들로서 구현될 수도 있다. 명령들은 하 나 이상의 프로세서들에 의해 실행 가능할 수도 있고, 프로세서(들)로 하여금 본 개시에 설명된 기능의 특정 양 태들을 수행하게 할 수도 있다. 이상 설명된 실시예들이 하나 이상의 독립형 컴퓨터 시스템에서 현재 개시된 주제의 양태들을 활용하는 것으로 기술되었으나, 본 개시는 이에 한정되 지 않고, 네트워크나 분산 컴퓨팅 환경과 같은 임의의 컴퓨팅 환경과 연 계하여 구현될 수도 있다. 또 나아가, 본 개시에서 주제의 양상들은 복수의 프로세싱 칩들이나 장치들에서 구현 될 수도 있고, 스토리지는 복수의 장치들에 걸쳐 유사하게 영향을 받게 될 수도 있다. 이러한 장치들은 PC들, 네트워크 서버들, 및 휴대용 장치들을 포함할 수도 있다."}
{"patent_id": "10-2021-0183299", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "본 개시가 일부 실시예들과 관련하여 설명되었지만, 본 개시의 발명이 속하는 기술분야의 통상의 기술자가 이해 할 수 있는 본 개시의 범위를 벗어나지 않는 범위에서 다양한 변형 및 변경이 이루어질 수 있다. 또한, 그러한 변형 및 변경은 본 명세서에 첨부된 청구범위 내에 속하는 것으로 생각되어야 한다."}
{"patent_id": "10-2021-0183299", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일반적인 DNN 기반 RecSys 모델을 보여준다. 도 2의 (a)는 RecSys에서 임베딩 계층을 학습하는 전진 전파 동안 수행된 주요 연산을 보여준다. 도 2의 (b)는 RecSys에서 임베딩 계층을 학습하는 역전파 동안 수행된 주요 연산을 보여준다. 도 3은 다양한 실제 RecSys 데이터셋에 포함된 테이블 항목의 (정렬된) 액세스 수를 보여준다. 도 4의 (a)는 기본 CPU-GPU 하에서의 전진 전파 및 역전파의 주요 단계를 보여준다. 도 4의 (b)는 소프트웨어 관리 GPU 임베딩 캐시로 강화된 CPU-GPU 시스템에서의 RecSys 학습 개요를 제공한다. 도 5는 정규화된 엔드 투 엔드 학습 시간을 학습의 주요 단계가 실행되는 위치에 따라 세분화한 것이다. 도 6은 CPU 임베딩 테이블의 많은 부분을 캐시함에 따라 GPU 임베딩 캐시 히트(hit)율이 향상되는 모습을 보여 준다. 도 7은 ScratchPipe 설계 시 본 개시의 주요 접근 방식을 보여준다. 도 8은 RecSys 학습의 허수아비 아키텍처의 단일 이터레이션 동안 수행되는 단계들을 보여준다. 도 9는 허수아비 아키텍처를 파이프라인하려고 할 때의 RAW 의존성을 보여준다. 도 10은 파이프라인 ScratchPipe 아키텍처를 보여준다. 도 11은 ScratchPipe의 구현 세부 사항을 보여준다. 도 12는 본 개시에 따른 컴퓨팅 시스템을 도시하는 도면이다. 도 13은 본 개시에 따른 컴퓨팅 시스템의 방법을 도시하는 도면이다."}
