{"patent_id": "10-2020-0036629", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0120218", "출원번호": "10-2020-0036629", "발명의 명칭": "에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템", "출원인": "한양대학교 에리카산학협력단", "발명자": "홍승호"}}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "생산 계획 정보 및 에너지 가격 정보를 수집하는 수집부; 및상기 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기위한 인공 신경망을 강화 학습하는 학습부를 포함하고,상기 인공 신경망은,상기 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크(actor network) 및 상기 복수의에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크(critic network)를 기반으로 동작하고,상기 학습부는,상기 인공 신경망에 대한 강화 학습을 수행하기 위해, N개의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 기초로 상기 액터 네트워크의 파라미터 및 상기 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 각 학습 샘플은,상기 각 학습 샘플에 대응하는 타임 슬롯인 타깃 타임 슬롯에서 상기 복수의 에너지 소비 장치의 상태, 상기 타깃 타임 슬롯에서 상기 복수의 에너지 소비 장치에 수행될 액션, 상기 타깃 타임 슬롯에서 상기 액션에 따른 보상 및 상기 타깃 타임 슬롯의 후속 타임 슬롯에서 상기 복수의 에너지 소비 장치의 상태에 대한 정보를 포함하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 학습부는,상기 N개의 학습 샘플을 설정된 학습 버퍼에 저장하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 학습부는,상기 액터 네트워크의 파라미터 및 상기 크리틱 네트워크의 파라미터를 업데이트하기 위해, 상기 학습 버퍼에저장된 상기 N개의 학습 샘플을 생성된 시간의 역순으로 추출하면서 상기 액터 네트워크의 파라미터에 대한 그라디언트(gradient) 및 상기 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트하는 동작을 반복하는에너지 관리 장치.공개특허 10-2021-0120218-3-청구항 5 제 4 항에 있어서,상기 학습부는,상기 N개의 학습 샘플 중 추출된 타깃 학습 샘플에 대해, 상기 타깃 학습 샘플을 기초로 하여, 누적 할인 보상값을 업데이트하고 예상 상태 보상 값을 계산하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서,상기 학습부는,상기 누적 할인 보상과 상기 예상 상태 보상의 차이를 기초로 하여, 상기 액터 네트워크의 파라미터에 대한 그라디언트 및 상기 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 학습부는,상기 액터 네트워크의 파라미터에 대한 그라디언트를 업데이트할 때, 상기 액터 네트워크에 대한 엔트로피 정규화 요소를 추가하는 에너지 관리 장치."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "에너지 공급원과 연결된 제1 파워 라인 및 복수의 에너지 소비 장치와 연결된 제2 파워 라인 사이의 에너지 전송 정보를 기록하는 스마트 미터;상기 에너지 공급원과 상기 복수의 에너지 소비 장치 간의 메시지 교환 및 상기 복수의 에너지 소비 장치 간의메시지 교환을 제어하는 게이트웨이; 및생산 계획 정보 및 에너지 가격 정보를 수집하고, 상기 생산 계획 정보 및 에너지 가격 정보를 기초로 상기 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습하는 에너지 관리 장치를 포함하고,상기 인공 신경망은,상기 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 상기 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크를 기반으로 동작하고,상기 에너지 관리 장치는,상기 인공 신경망에 대한 강화 학습을 수행하기 위해, N개의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 기초로 상기 액터 네트워크의 파라미터 및 상기 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 시스템."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "생산 계획 정보 및 에너지 가격 정보를 수집하는 수집 단계; 및상기 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기위한 인공 신경망을 강화 학습하는 학습 단계를 포함하고,공개특허 10-2021-0120218-4-상기 인공 신경망은,상기 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 상기 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크를 기반으로 동작하고,상기 학습 단계는,상기 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2 이상의 자연수)의 타임 슬롯 각각에 대응하는N개의 학습 샘플을 생성하고 상기 N개의 학습 샘플을 기초로 상기 액터 네트워크의 파라미터 및 상기 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,상기 각 학습 샘플은,상기 각 학습 샘플에 대응하는 타임 슬롯인 타깃 타임 슬롯에서 상기 복수의 에너지 소비 장치의 상태, 상기 타깃 타임 슬롯에서 상기 복수의 에너지 소비 장치에 수행될 액션, 상기 타깃 타임 슬롯에서 상기 액션에 따른 보상 및 상기 타깃 타임 슬롯의 후속 타임 슬롯에서 상기 복수의 에너지 소비 장치의 상태에 대한 정보를 포함하는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서,상기 N개의 학습 샘플은 설정된 학습 버퍼에 저장되는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,상기 학습 단계는,상기 액터 네트워크의 파라미터 및 상기 크리틱 네트워크의 파라미터를 업데이트하기 위해, 상기 학습 버퍼에저장된 상기 N개의 학습 샘플을 생성된 시간의 역순으로 추출하면서 상기 액터 네트워크의 파라미터에 대한 그라디언트 및 상기 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트하는 동작을 반복하는 에너지 관리방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 12 항에 있어서,상기 학습 단계는,상기 N개의 학습 샘플 중 추출된 타깃 학습 샘플에 대해, 상기 타깃 학습 샘플을 기초로 하여, 누적 할인 보상값을 업데이트하고 예상 상태 보상 값을 계산하는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 학습 단계는,상기 누적 할인 보상과 상기 예상 상태 보상의 차이를 기초로 하여, 상기 액터 네트워크의 파라미터에 대한 그공개특허 10-2021-0120218-5-라디언트 및 상기 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트하는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서,상기 학습 단계는,상기 액터 네트워크의 파라미터에 대한 그라디언트를 업데이트할 때, 상기 액터 네트워크에 대한 엔트로피 정규화 요소를 추가하는 에너지 관리 방법."}
{"patent_id": "10-2020-0036629", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항 내지 제15항 중 어느 한 항의 에너지 관리 방법을 구현하기 위한 프로그램이 기록된 컴퓨터로 판독 가능한 기록 매체."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 실시예들은 생산 계획 정보 및 에너지 가격 정보를 수집하고 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습하는 에너지 관 리 장치, 에너지 관리 방법 및 에너지 관리 시스템을 제공한다. 인공 신경망은 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리 틱 네트워크를 기반으로 동작하며, N개(N은 2 이상의 자연수)의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터가 반복적으로 업데이트되는 방식으로 학습될 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명의 실시예들은 에너지 관리 장치 및 그 방법 및 에너지 관리 시스템에 관한 것이다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "산업 발전에 따라 에너지 수요가 증가하면서, 에너지를 소비하는 다양한 주체들의 수요 반응(demand response, DR)을 관리하는 것이 중요한 문제로 대두되고 있다. 이를 위해 다양한 분야에 대해 수요 반응을 관리하기 위한 에너지 관리 장치, 방법 및 시스템을 연구하는 노력이 활발하게 이루어지고 있다. 그러나 가정이나 상업 시설에 대한 에너지 관리 장치, 방법 및 시스템에 대한 연구가 활발한 반면, 산업 시설에 대한 에너지 관리 장치, 방법 및 시스템에 대한 연구는 미진한 실정이다. 산업 시설에서 에너지를 소비하는 산 업 설비들의 특성은 산업 시설의 분야에 따라 다양할 뿐 아니라 산업 시설은 에너지 소비를 줄이는 경우에도 지 정된 생산량을 보장해야 할 필요가 있다. 그런데, 기존의 에너지 관리 장치, 방법 및 시스템은 다양한 특성을 가진 산업 설비들의 특성 및 상호 작용을 고려하지 못하며 에너지 소비로 인해 발생하는 비용만 고려하고 생산 량에 대한 요소는 고려하고 있지 않다는 문제가 있기 때문이다. 따라서, 다양한 분야의 산업 시설에 적용될 수 있고, 일정 수준 이상의 생산량을 보장하면서도 에너지 소비로 인해 발생하는 비용을 최소화할 수 있도록 하는 에너지 관리 장치, 방법 및 시스템이 필요한 실정이다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "전술한 배경에서 본 발명의 실시예들은 다양한 분야의 산업 시설에 적용 가능한 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템을 제공할 수 있다. 또한 본 발명의 실시예들은 일정 수준 이상의 생산량을 보장하면서도 에너지 소비로 인해 발생하는 비용을 최소 화할 수 있도록 하는 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템을 제공할 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 측면에서, 본 발명의 실시예들은 생산 계획 정보 및 에너지 가격 정보를 수집하는 수집부 및 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습하는 학습부를 포함하고, 인공 신경망은 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액 터 네트워크(actor network) 및 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워 크(critic network)를 기반으로 동작하고, 학습부는 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2이상의 자연수)의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 상기 크 리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 장치를 제공할 수 있다. 다른 측면에서, 본 발명의 실시예들은 에너지 공급원과 연결된 제1 파워 라인 및 복수의 에너지 소비 장치와 연 결된 제2 파워 라인 사이의 에너지 전송 정보를 기록하는 스마트 미터, 에너지 공급원과 복수의 에너지 소비 장 치 간의 메시지 교환 및 상기 복수의 에너지 소비 장치 간의 메시지 교환을 제어하는 게이트웨이 및 생산 계획 정보 및 에너지 가격 정보를 수집하고, 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치 의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습하는 에너지 관리 장치를 포함하고, 인공 신경 망은 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 복수의 에너지 소비 장치에 의 해 수행된 액션의 결과를 판단하는 크리틱 네트워크를 기반으로 동작하고, 에너지 관리 장치는 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2 이상의 자연수)의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 시스템을 제공할 수 있다. 또 다른 측면에서, 본 발명의 실시예들은 생산 계획 정보 및 에너지 가격 정보를 수집하는 수집 단계 및 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습하는 학습 단계를 포함하고, 인공 신경망은 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크를 기반으로 동작하고, 학습 단계는 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2 이상의 자연수)의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 생성하고 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복하는 에너지 관리 방법을 제공할 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들에 따르면 다양한 분야의 산업 시설에 적용 가능한 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템을 제공할 수 있다. 또한 본 발명의 실시예들에 따르면, 일정 수준 이상의 생산량을 보장하면서도 에너지 소비로 인해 발생하는 비 용을 최소화할 수 있도록 하는 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템을 제공할 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 통해 상세하게 설명한다. 각 도면의 구성요소들에 참조부호 를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구 체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 또한, 본 개시의 구성 요소를 설명하는 데 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본 질이나 차례 또는 순서 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\" 된다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구 성 요소 사이에 또 다른 구성 요소가 \"연결\", \"결합\" 또는 \"접속\"될 수도 있다고 이해되어야 할 것이다. 본 발명의 실시예들에 있어서, 에너지의 형태는 전기 에너지, 열 에너지, 빛 에너지 등일 수 있다. 이하, 본 발 명의 실시예들에서는 에너지의 형태가 전기 에너지인 경우에 대해 주로 설명하나 에너지의 형태는 이에 한정되 지 않는다. 이하에서는, 관련 도면을 참조하여 본 발명의 실시예들에 따른 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템에 대하여 설명하기로 한다. 도 1은 본 발명의 실시예들에 따른 에너지 관리 장치의 블록도이다. 도 1을 참조하면, 에너지 관리 장치는 수집부 및 학습부를 포함할 수 있다. 수집부는 생산 계획 정보 및 에너지 가격 정보를 수집할 수 있다. 생산 계획 정보는 타깃이 되는 복수의 에너지 소비 장치를 이용하여 제품을 얼마만큼 생산할지에 대한 계획을 나타낸 정보이다. 그리고 에너지 가격 정보는 복수의 에너지 소비 장치가 에너지를 사용할 경우 지불해야 하는 가격을 나타낸 정보로서 에너지 소비로 인해 발생하는 비용을 계산하는데 사용된다. 학습부는 수집부에서 수집한 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치 의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습(RL, Reinforcement Learning)할 수 있다. 복수의 에너지 소비 장치는 산업 설비 내에서 제품을 생산하기 위해 에너지를 소비하는 장치를 의미하며, 산업 설비는 복수의 에너지 소비 장치를 사용하여 제품을 생산할 수 있다. 일 예로 철강 분말 제조(SPM, Steel Powder Manufactoring) 설비는 액체 상태의 철로부터 철강 분말을 생성하기 위해서 아토마이저(atomizer), 디하이드레이터(dehydrator), 드라이어(dryer), 크러셔(crusher), 클래시파이어 (classifier), 마그네틱 세퍼레이터(magnetic separator), 리덕션 퍼니스(reduction furnace), 블렌더 (blender) 등의 에너지 소비 장치를 사용할 수 있다. 본 발명의 실시예들에서, 학습부에 의해 강화 학습되는 인공 신경망은 복수의 에너지 소비 장치에 의해 수 행될 액션을 결정하는 액터 네트워크(actor network) 및 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크(critic network)를 기반으로 동작할 수 있다. 이하 도 2에서 인공 신경망의 구조에 대해 설명한다. 학습부는 전술한 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2 이상의 자연수)의 타임 슬롯 각각에 대응하는 N개의 학습 샘플을 생성하고 생성된 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하는 동작을 반복할 수 있다. 액터 네트워크의 파라미터 및 크리틱 네트 워크의 파라미터는 최초에 랜덤한 값으로 초기화된 후, 학습부에 의해 업데이트되는 과정을 통해 최적화될 수 있다. 이때, 학습부는 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하는 동작을 설정된 횟수만큼 반복하거나 또는 설정된 조건이 만족될 때까지 반복할 수 있다. 학습부가 학습 샘플을 생성하는 동작 및 생성된 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하는 동작의 일 예는 이하 도 3 내지 도 8을 통해 설명한다. 도 2는 본 발명의 실시예들에 따른 인공 신경망의 개략적 구조를 나타낸 도면이다. 본 발명의 실시예들에 따른 인공 신경망은 상태(state), 액션(action)과 보상(reward)과의 연관 관계를 나타내 는 마르코프 결정 프로세스(MDP, Markov Decision Process)에 기반한다. 인공 신경망은 학습을 통해 타깃이 되는 시스템인 환경(environment)과 상호작용(interact)하면서, 환경을 제어 하기 위한 정책(policy) 를 최적화할 수 있다. 구체적으로, 인공 신경망은 환경의 상태(state) 를 입력받은 후 정책 에 따라 액션(action) 를 결정할 수 있다. 환경은 액션 를 실행한 후 그 결과에 따라 보상(reward)을 결정하여 인공 신경망에 피드백하고, 인공 신 경망은 피드백된 보상에 따라 정책 를 변경할 수 있다. 그리고 환경은 액션 을 실행한 후 변경된 상태를 다시 인공 신경망에 입력할 수 있다. 이와 같이 환경과 인공 신경망 간에 서로 상태, 액션, 보상에 대한 정보를 주고 받으면서 서로 상호 작용하는 과정을 통해, 인공 신경망은 환경을 제어하기 위한 정책을 최적화할 수 있다. 이때, 상태를 입력받아 정책에 따 라 액션을 결정하는 네트워크가 액터 네트워크이고 액션의 결과를 판단하는 네트워크가 크리틱 네트워크이다. 도 3은 본 발명의 실시예들에 따른 학습부의 동작의 일 예를 나타낸 흐름도이다. 도 3을 참조하면, 학습부는 먼저 타임 슬롯을 지시하기 위한 인덱스 t의 값을 t로 설정할 수 있다(S310). 인덱스 t에 의해 지시되는 타임 슬롯 t는 일 예로 하루 중에서 특정한 시간을 지시할 수 있다. 만약 t=3이면 타 임 슬롯 t는 오전 3시를 지시하고 t=13이면 타임 슬롯 t는 오후 1시를 지시하고 t=18이면 타임 슬롯 t는 오후 6 시를 지시할 수 있다. 그리고 학습부는 t의 값이 생성할 학습 샘플의 개수인 N보다 작거나 같은지를 판단한다(S320). 예를 들어 학습부가 하루 동안의 학습 샘플을 1시간 간격의 타임 슬롯에 따라 추출하려는 경우 하루는 24시간이므로 N=24로 결정될 수 있다. 만약 t의 값이 N보다 작으면(S320-Y), 학습부는 타임 슬롯 t에 대응하는 학습 샘플을 생성하고(S330), t의 값을 1만큼 증가시킨 후(S340)에 다시 S320 단계로 진입할 수 있다. 반면 t의 값이 N보다 크면(S320-N), 이는 N개의 학습 샘플이 생성되었다는 것을 의미하므로 학습부는 학습 샘플을 더 이상 생성하지 않고, 생성된 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워 크의 파라미터를 업데이트할 수 있다(S350). 이하, S330 단계에서 학습부가 학습 샘플을 생성하는 동작의 일 예를 도 4에서 구체적으로 설명한다. 도 4는 본 발명의 실시예들에 따른 학습부가 학습 샘플을 생성하는 동작의 일 예를 나타낸 도면이다. 학습부는 타깃 타임 슬롯 에서 복수의 에너지 소비 장치의 상태 를 인공 신경망에 입력하여 복수의 에너 지 소비 장치에 수행될 액션 를 추출해 낼 수 있다. 그리고 복수의 에너지 소비 장치에 대해 액션 가 수행 되면 액션 에 따른 보상 와 타깃 타임 슬롯 의 후속 타임 슬롯 에서의 복수의 에너지 소비 장치의 상 태 가 결정된다. 학습부는 전술한 을 타깃 타임 슬롯 에 대한 학습 샘플로 설정할 수 있다. 따라서 각 학습 샘플은 타깃 타임 슬롯 에서 복수의 에너지 소비 장치의 상태 , 복수의 에너지 소비 장치에 수행될 액 션 , 액션 에 따른 보상 및 타깃 타임 슬롯 의 후속 타임 슬롯 에서의 복수의 에너지 소비 장치의 상태 를 포함할 수 있다. 도 5는 본 발명의 실시예들에 따른 학습부가 학습 샘플을 학습 버퍼에 저장하는 동작의 일 예를 나타낸 도면이 다. 학습부는 타임 슬롯 1~N 각각에 대응하는 N개의 학습 샘플 , , , ... , , 을 학습 버퍼에 저장할 수 있다. 이때, 학습 버퍼는 일 예로 에너지 관리 장치 내부 또는 외부에 위치하는 저장 장치(e.g. HDD, SSD, SRAM, DRAM, NAND FLASH)를 통해 구현될 수 있다. 이하, 학습부가 학습 버퍼에 저장된 N개의 학습 샘플을 추출하여 액터 네트워크 및 크리틱 네트워크의 파 라미터를 업데이트하는 동작을 설명한다.도 6은 본 발명의 실시예들에 따른 학습부가 학습 버퍼에 저장된 학습 샘플을 추출하는 동작의 일 예를 나타낸 도면이다. 도 6을 참조하면, 학습부는 학습 버퍼에 저장된 N개의 학습 샘플을 생성된 시간의 역순으로 추출할 수 있 다. 즉, 학습부는 가장 나중에 생성된 학습 샘플 을 먼저 추출하고, 이후 , ... , , , 의 순서로 학습 샘플을 추출할 수 있다. 학습부는 학습 버퍼에 저장된 N개의 학습 샘플을 생성된 시간의 역순으로 추출하면서, 각 학습 샘플 별로 액터 네트워크의 파라미터에 대한 그라디언트(gradient) 및 크리틱 네트워크의 파라미터에 대한 그라디언트 (gradient)를 업데이트하는 동작을 반복할 수 있다. 액터 네트워크의 파라미터에 대한 그라디언트는 액터 네트워크의 파라미터를 업데이트하기 위해 사용되고, 크리 틱 네트워크의 파라미터에 대한 그라디언트는 크리틱 네트워크의 파라미터를 업데이트하기 위해 사용된다. 도 7은 본 발명의 실시예들에 따른 학습부가 액터 네트워크 및 크리틱 네트워크의 파라미터를 업데이트하는 동 작의 일 예를 나타낸 흐름도이다. 학습부는 먼저 타임 슬롯을 지시하기 위한 인덱스 t의 값을 N으로 설정할 수 있다(S710). 이는 학습 버퍼 에 저장된 N개의 학습 샘플을 생성된 시간의 역순으로 추출하기 위함이다. 학습부는 인덱스 t의 값이 1보다 크거나 같은지 판단한다(S720). 만약 t의 값이 1보다 크거나 같으면 (S720-Y), 학습부는 타임 슬롯 t에 대응하는 학습 샘플인 타깃 학습 샘플을 학습 버퍼로부터 추출할 수 있 다(S730). 그리고 학습부는 S730 단계에서 추출된 타깃 학습 샘플을 기초로 하여 누적 할인 보상 값을 업데이트하고 예상 상태 보상 값을 계산할 수 있다(S740). 인덱스 t에 대한 누적 할인 보상 값 는 다음과 같이 계산될 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 는 할인 팩터(discount factor)로서 의 값은 0 < < 1 로 결정될 수 있다. 인덱스 t에 대한 예상 상태 보상 값 는 다음과 같이 결정될 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 는 보상 값을 결정하기 위한 크리틱 네트워크를 의미하고 는 크리틱 네트워크의 파라미터를 의미한다. 학습부는 S730 단계에서 추출된 타깃 학습 샘플과 S740 단계에서 계산된 누적 할인 보상 값 및 예상 상태 보상 값을 기초로 액터 네트워크의 파라미터에 대한 그라디언트 및 크리틱 네트워크의 파라미터에 대한 그라디 언트를 업데이트할 수 있다(S750). 학습부가 액터 네트워크의 파라미터에 대한 그라디언트 및 크리틱 네트 워크의 파라미터에 대한 그라디언트를 업데이트하는 구체적인 동작의 일 예는 이하 도 8에서 설명한다. 학습부는 액터 네트워크의 파라미터에 대한 그라디언트 및 크리틱 네트워크의 파라미터에 대한 그라디언트 를 업데이트한 이후 t의 값을 1만큼 감소시키고(S760), 다시 S720 단계를 수행할 수 있다. 반면 t의 값이 1보다 작다면(S720-Y), 이는 학습부가 학습 버퍼에 저장된 N개의 학습 샘플을 생성된 시간 의 역순으로 모두 추출했다는 것을 의미한다. 따라서, 학습부는 업데이트된 액터 네트워크의 파라미터에 대한 그라디언트 및 크리틱 네트워크의 파라미터에 대한 그라디언트를 기초로 하여 액트 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트할 수 있다(S770). 도 8은 본 발명의 실시예들에 따른 학습부가 액터 네트워크 및 크리틱 네트워크의 파라미터에 대한 그라디언트 를 계산하는 동작의 일 예를 나타낸 흐름도이다.도 8을 참조하면, 학습부는 먼저 타임 슬롯 t에서 누적 할인 보상 값 과 예상 상태 보상 값 의 차이 를 계산할 수 있다(S810). 이때, 가 양수이면 현재 정책으로 인한 보상값이 예상보다 좋다는 것을 의미하고, 가 음수이면 현재 정책으로 인한 보상값이 예상보다 나쁘다는 것을 의미한다. 학습부는 액터 네트워크의 파라미터에 대한 그라디언트를 계산할 수 있다(S820). 액터 네트워크의 파라미터를 라고 정의하면, 타임 슬롯 t에서 액터 네트워크의 파라미터에 대한 그라디언트"}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "는 다음과 같이 계산될 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이때, 는 액터 네트워크를 의미하며, 는 액터 네트워크 에 대한 엔트로 피 정규화 요소 값을 의미한다. 이때 는 데이터 집합의 혼잡도를 나타내는 엔트로피 함수이고 는 설정된 탐색 팩터(exploration factor) 값이다. 엔트로피 함수 및 탐색 팩터의 값은 미리 설정된 값일 수 있다. 학습부는 크리틱 네트워크의 파라미터에 대한 그라디언트를 계산할 수 있다(S830). 크리틱 네트워크이 파라미터를 라고 정의하면, 타임 슬롯 t에서 크리틱 네트워크의 파라미터에 대한 그라디언 트 는 편미분을 이용하여 다음과 같이 결정될 수 있다."}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "한편, 학습부는 액터 네트워크의 파라미터에 대한 그라디언트를 크리틱 네트워크의 파라미터에 대한 그라 디언트보다 먼저 계산할 수도 있으나, 크리틱 네트워크의 파라미터에 대한 그라디언트를 액터 네트워크의 파라 미터에 대한 그라디언트보다 먼저 계산할 수도 있으며, 액터 네트워크의 파라미터에 대한 그라디언트와 크리틱 네트워크의 파라미터에 대한 그라디언트를 동시에 계산할 수도 있다. 도 9는 본 발명의 실시예들에 따른 누적 보상의 변화 패턴의 일 예를 나타낸 그래프이다. 도 9를 참조하면, 인공 신경망에 대한 강화 학습이 최초로 실행되는 단계에서는 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터가 최적화되지 않아 누적 보상 값이 낮다. 강화 학습이 반복되면서 누적 보상의 값 은 증가하지만 누적 보상이 증가하는 기울기는 점차 감소하여 일정 횟수 이상이 되면 누적 보상의 값은 특정한 최대값에 수렴된다. 도 10은 본 발명의 실시예들에 따른 에너지 관리 시스템의 블록도이다. 도 10을 참조하면, 에너지 관리 시스템은 스마트 미터, 게이트웨이 및 전술한 에너지 관리 장치를 포 함할 수 있다. 스마트 미터는 에너지를 공급하는 에너지 공급원(e.g. 스마트 그리드)과 연결된 제1 파워 라인 및 에너지 공급 원으로부터 공급받은 에너지를 소비하는 복수의 에너지 소비 장치와 연결된 제2 파워 라인 사이의 에너지 전송 정보를 기록할 수 있다. 게이트웨이는 에너지 공급원과 복수의 에너지 소비 장치 간의 메시지 교환 및 복수의 에너지 소비 장치 간의 메 시지 교환을 제어할 수 있다. 이때, 메시지는 일 예로 에너지 가격 정보를 포함할 수 있다. 이때, 에너지 공급원과 복수의 에너지 소비 장치 간의 메시지 교환을 위해서, 에너지 공급원과 게이트웨이 간에 사용되는 제1 프로토콜 및 게이트웨이와 복수의 에너지 소비 장치 간에 사용되는 제2 프로토콜은 현재 또는 장 래의 유무선 통신 기술 중에서 결정될 수 있다. 일 예로 제1 프로토콜은 광역 통신망(WAN), LTE 또는 5G 무선 통신일 수 있다. 일 예로 제2 프로토콜은 근거리 통신망(LAN), 블루투스, WiFi, ZigBee 등의 유무선 프로토콜일 수 있다. 그리고 복수의 에너지 소비 장치 간의 메시지 교환을 위해서 복수의 에너지 소비 장치 간에 사용되는 제3 프로 토콜은 제2 프로토콜과 동일하거나 또는 상이하게 결정될 수 있다. 에너지 관리 장치는 도 1에서 전술한 바와 같이 생산 계획 정보 및 에너지 가격 정보를 수집하고, 생산 계 획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습할 수 있다. 도 11은 본 발명의 실시예들에 따른 에너지 관리 방법을 나타낸 흐름도이다. 도 11을 참조하면, 에너지 관리 방법은 수집 단계(S1110) 및 학습 단계(S1120)를 포함할 수 있다. 수집 단계(S1110)는 생산 계획 정보 및 에너지 가격 정보를 수집할 수 있다. 학습 단계(S1120)는 수집 단계(S1110)에서 수집된 생산 계획 정보 및 에너지 가격 정보를 기초로 복수의 에너지 소비 장치의 타깃 에너지 소비량을 결정하기 위한 인공 신경망을 강화 학습할 수 있다. 이때, 인공 신경망은 복수의 에너지 소비 장치에 의해 수행될 액션을 결정하는 액터 네트워크 및 복수의 에너지 소비 장치에 의해 수행된 액션의 결과를 판단하는 크리틱 네트워크를 기반으로 동작할 수 있다. 학습 단계(S1120)는 인공 신경망에 대한 강화 학습을 수행하기 위해, N개(N은 2 이상의 자연수)의 타임 슬롯 각 각에 대응하는 N개의 학습 샘플을 기초로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트 하는 동작을 반복할 수 있다. 이때, 일 예로 각 학습 샘플은 각 학습 샘플에 대응하는 타임 슬롯인 타깃 타임 슬롯에서 복수의 에너지 소비 장치의 상태, 타깃 타임 슬롯에서 복수의 에너지 소비 장치에 수행될 액션, 타깃 타임 슬롯에서 액션에 따른 보 상 및 타깃 타임 슬롯의 후속 타임 슬롯에서 상기 복수의 에너지 소비 장치의 상태에 대한 정보를 포함할 수 있 다. N개의 학습 샘플은 설정된 학습 버퍼에 저장될 수 있다. 학습 단계(S1120)는 일 예로 액터 네트워크의 파라미터 및 크리틱 네트워크의 파라미터를 업데이트하기 위해, 학습 버퍼에 저장된 N개의 학습 샘플을 생성된 시간의 역순으로 추출하면서 액터 네트워크의 파라미터에 대한 그라디언트 및 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트하는 동작을 반복할 수 있다. 이때, 학습 단계(S1120)는 일 예로 N개의 학습 샘플 중 추출된 타깃 학습 샘플에 대해, 상기 타깃 학습 샘플을 기초로 하여, 누적 할인 보상 값을 업데이트하고 예상 상태 보상 값을 계산할 수 있다. 그리고 학습 단계(S1120)는 누적 할인 보상과 예상 상태 보상의 차이를 기초로 하여, 액터 네트워크의 파라미터 에 대한 그라디언트 및 크리틱 네트워크의 파라미터에 대한 그라디언트를 업데이트할 수 있다. 이때, 학습 단계 (S1120)는 액터 네트워크의 파라미터에 대한 그라디언트를 업데이트할 때, 액터 네트워크에 대한 엔트로피 정규 화 요소를 추가할 수 있다. 한편, 전술한 에너지 관리 방법은 전술한 에너지 관리 장치에 의해 실행될 수 있다. 전술한 에너지 관리 장치, 에너지 관리 방법 및 에너지 관리 시스템은 인공 신경망을 이용한 강 화 학습을 통해 에너지 소비 장치의 타깃 에너지 소비량을 결정하므로, 다양한 분야의 산업 시설 각각에 대한 최적화된 에너지 소비 정책을 미리 설계하지 않아도 반복된 강화 학습을 통해 각 산업 시설 별 최적화를 수행할 수 있다. 따라서, 다양한 분야의 산업 시설에 적용 가능하다. 또한 에너지 관리 장치에서 사용하는 인공 신경망은 에너지 소비로 인한 결과값, 즉 제품 생산량을 반영한 강화 학습을 수행하므로, 일정 수준 이상의 생산량을 보장하면서도 에너지 소비로 인해 발생하는 비용을 최소화 할 수 있다. 전술한 에너지 관리 장치는, 프로세서, 메모리, 사용자 입력장치, 프레젠테이션 장치 중 적어도 일부를 포 함하는 컴퓨팅 장치에 의해 구현될 수 있다. 메모리는, 프로세서에 의해 실행되면 특정 태스크를 수행할 있도록 코딩되어 있는 컴퓨터-판독가능 소프트웨어, 애플리케이션, 프로그램 모듈, 루틴, 인스트럭션(instructions), 및/또는 데이터 등을 저장하는 매체이다. 프로세서는 메모리에 저장되어 있는 컴퓨터-판독가능 소프트웨어, 애 플리케이션, 프로그램 모듈, 루틴, 인스트럭션, 및/또는 데이터 등을 판독하여 실행할 수 있다. 사용자 입력장 치는 사용자로 하여금 프로세서에게 특정 태스크를 실행하도록 하는 명령을 입력하거나 특정 태스크의 실행에 필요한 데이터를 입력하도록 하는 수단일 수 있다. 사용자 입력장치는 물리적인 또는 가상적인 키보드나 키패드, 키버튼, 마우스, 조이스틱, 트랙볼, 터치-민감형 입력수단, 또는 마이크로폰 등을 포함할 수 있다. 프 레젠테이션 장치는 디스플레이, 프린터, 스피커, 또는 진동장치 등을 포함할 수 있다. 컴퓨팅 장치는 스마트폰, 태블릿, 랩탑, 데스크탑, 서버, 클라이언트 등의 다양한 장치를 포함할 수 있다. 컴퓨 팅 장치는 하나의 단일한 스탠드-얼론 장치일 수도 있고, 통신망을 통해 서로 협력하는 다수의 컴퓨팅 장치들로 이루어진 분산형 환경에서 동작하는 다수의 컴퓨팅 장치를 포함할 수 있다. 또한 전술한 에너지 관리 방법은, 프로세서를 구비하고, 또한 프로세서에 의해 실행되면 딥 러닝 모델을 활용한 영상 진단 방법을 수행할 수 있도록 코딩된 컴퓨터 판독가능 소프트웨어, 애플리케이션, 프로그램 모듈, 루틴, 인스트럭션, 및/또는 데이터 구조 등을 저장한 메모리를 구비하는 컴퓨팅 장치에 의해 실행될 수 있다. 상술한 본 실시예들은 다양한 수단을 통해 구현될 수 있다. 예를 들어, 본 실시예들은 하드웨어, 펌웨어 (firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 실시예들에 따른 딥 러닝 모델을 활용한 영상 진단 방법은 하나 또는 그 이상 의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 컨트롤러, 마이크로 컨트롤러 또는 마이크로 프로세서 등에 의해 구현될 수 있다. 예를 들어 실시예들에 따른 에너지 관리 방법은 심층 신경망의 뉴런(neuron)과 시냅스(synapse)가 반도체 소자 들로 구현된 인공지능 반도체 장치를 이용하여 구현될 수 있다. 이때 반도체 소자는 현재 사용하는 반도체 소자 들, 예를 들어 SRAM이나 DRAM, NAND 등일 수도 있고, 차세대 반도체 소자들, RRAM이나 STT MRAM, PRAM 등일 수 도 있고, 이들의 조합일 수도 있다. 실시예들에 따른 에너지 관리 방법을 인공지능 반도체 장치를 이용하여 구현할 때, 딥 러닝 모델을 소프트웨어 로 학습한 결과(가중치)를 어레이로 배치된 시냅스 모방소자에 전사하거나 인공지능 반도체 장치에서 학습을 진 행할 수도 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 실시예들에 따른 에너지 관리 방법은 이상에서 설명된 기능 또는 동작들을 수행하는 장치, 절차 또는 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리 유닛에 저장 되어 프로세서에 의해 구동될 수 있다. 메모리 유닛은 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 프로세서와 데이터를 주고 받을 수 있다. 또한, 위에서 설명한 \"시스템\", \"프로세서\", \"컨트롤러\", \"컴포넌트\", \"모듈\", \"인터페이스\", \"모델\", 또는 \"유 닛\" 등의 용어는 일반적으로 컴퓨터 관련 엔티티 하드웨어, 하드웨어와 소프트웨어의 조합, 소프트웨어 또는 실 행 중인 소프트웨어를 의미할 수 있다. 예를 들어, 전술한 구성요소는 프로세서에 의해서 구동되는 프로세스, 프로세서, 컨트롤러, 제어 프로세서, 개체, 실행 스레드, 프로그램 및/또는 컴퓨터일 수 있지만 이에 국한되지 않는다. 예를 들어, 컨트롤러 또는 프로세서에서 실행 중인 애플리케이션과 컨트롤러 또는 프로세서가 모두 구 성 요소가 될 수 있다. 하나 이상의 구성 요소가 프로세스 및/또는 실행 스레드 내에 있을 수 있으며, 구성 요 소들은 하나의 장치(예: 시스템, 컴퓨팅 디바이스 등)에 위치하거나 둘 이상의 장치에 분산되어 위치할 수 있다. 한편, 또 다른 실시예는 전술한 에너지 관리 방법을 수행하는, 컴퓨터 기록매체에 저장되는 컴퓨터 프로그램을 제공한다. 또한 또 다른 실시예는 전술한 에너지 관리 방법을 실현시키기 위한 프로그램을 기록한 컴퓨터로 읽 을 수 있는 기록매체를 제공한다. 기록매체에 기록된 프로그램은 컴퓨터에서 읽히어 설치되고 실행됨으로써 전술한 단계들을 실행할 수 있다. 이와 같이, 컴퓨터가 기록매체에 기록된 프로그램을 읽어 들여 프로그램으로 구현된 기능들을 실행시키기 위하 여, 전술한 프로그램은 컴퓨터의 프로세서(CPU)가 컴퓨터의 장치 인터페이스(Interface)를 통해 읽힐 수 있는 C, C++, JAVA, 기계어 등의 컴퓨터 언어로 코드화된 코드(Code)를 포함할 수 있다. 이러한 코드는 전술한 기능들을 정의한 함수 등과 관련된 기능적인 코드(Function Code)를 포함할 수 있고, 전 술한 기능들을 컴퓨터의 프로세서가 소정의 절차대로 실행시키는데 필요한 실행 절차 관련 제어 코드를 포함할 수도 있다. 또한, 이러한 코드는 전술한 기능들을 컴퓨터의 프로세서가 실행시키는데 필요한 추가 정보나 미디어가 컴퓨터 의 내부 또는 외부 메모리의 어느 위치(주소 번지)에서 참조 되어야 하는지에 대한 메모리 참조 관련 코드를 더 포함할 수 있다. 또한, 컴퓨터의 프로세서가 전술한 기능들을 실행시키기 위하여 원격(Remote)에 있는 어떠한 다른 컴퓨터나 서 버 등과 통신이 필요한 경우, 코드는 컴퓨터의 프로세서가 컴퓨터의 통신 모듈을 이용하여 원격(Remote)에 있는어떠한 다른 컴퓨터나 서버 등과 어떻게 통신해야만 하는지, 통신 시 어떠한 정보나 미디어를 송수신해야 하는 지 등에 대한 통신 관련 코드를 더 포함할 수도 있다. 이상에서 전술한 바와 같은 프로그램을 기록한 컴퓨터로 읽힐 수 있는 기록매체는, 일 예로, ROM, RAM, CD-ROM, 자기 테이프, 플로피디스크, 광 미디어 저장장치 등이 있으며, 또한 캐리어 웨이브(예를 들어, 인터넷을 통한 전송)의 형태로 구현되는 것도 포함할 수 있다. 또한 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 그리고, 본 발명을 구현하기 위한 기능적인(Functional) 프로그램과 이와 관련된 코드 및 코드 세그먼트 등은,"}
{"patent_id": "10-2020-0036629", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "기록매체를 읽어서 프로그램을 실행시키는 컴퓨터의 시스템 환경 등을 고려하여, 본 발명이 속하는 기술분야의 프로그래머들에 의해 용이하게 추론되거나 변경될 수도 있다. 도 10를 통해 설명된 에너지 관리 방법은, 컴퓨터에 의해 실행되는 애플리케이션이나 프로그램 모듈과 같은 컴 퓨터에 의해 실행 가능한 명령어를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임 의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 전술한 에너지 관리 방법은, 단말기에 기본적으로 설치된 애플리케이션(이는 단말기에 기본적으로 탑재된 플랫 폼이나 운영체제 등에 포함된 프로그램을 포함할 수 있다)에 의해 실행될 수 있고, 사용자가 애플리케이션 스토 어 서버, 애플리케이션 또는 해당 서비스와 관련된 웹 서버 등의 애플리케이션 제공 서버를 통해 마스터 단말기 에 직접 설치한 애플리케이션(즉, 프로그램)에 의해 실행될 수도 있다. 이러한 의미에서, 전술한 에너지 관리 방법은 단말기에 기본적으로 설치되거나 사용자에 의해 직접 설치된 애플리케이션(즉, 프로그램)으로 구현되고 단말기에 등의 컴퓨터로 읽을 수 있는 기록매체에 기록될 수 있다. 이상에서, 본 개시의 실시예를 구성하는 모든 구성 요소들이 하나로 결합되거나 결합되어 동작하는 것으로 설명 되었다고 해서, 본 개시는 반드시 이러한 실시예에 한정되는 것은 아니다. 즉, 본 개시의 목적 범위 안에서라면, 그 모든 구성 요소들이 하나 이상으로 선택적으로 결합하여 동작할 수도 있다. 또한, 그 모든 구성 요소들이 각각 하나의 독립적인 하드웨어로 구현될 수 있지만, 각 구성 요소들의 그 일부 또는 전부가 선택적으 로 조합되어 하나 또는 복수 개의 하드웨어에서 조합된 일부 또는 전부의 기능을 수행하는 프로그램 모듈을 갖 는 컴퓨터 프로그램으로서 구현될 수도 있다. 그 컴퓨터 프로그램을 구성하는 코드들 및 코드 세그먼트들은 본 개시의 기술 분야의 당업자에 의해 용이하게 추론될 수 있을 것이다. 이러한 컴퓨터 프로그램은 컴퓨터가 읽을 수 있는 저장매체(Computer Readable Media)에 저장되어 컴퓨터에 의하여 읽혀지고 실행됨으로써, 본 개시의 실 시예를 구현할 수 있다. 컴퓨터 프로그램의 저장매체로서는 자기 기록매체, 광 기록매체, 등이 포함될 수 있다. 또한, 이상에서 기재된 \"포함하다\", \"구성하다\" 또는 \"가지다\" 등의 용어는, 특별히 반대되는 기재가 없는 한, 해당 구성 요소가 내재될 수 있음을 의미하는 것이므로, 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요 소를 더 포함할 수 있는 것으로 해석되어야 한다. 기술적이거나 과학적인 용어를 포함한 모든 용어들은, 다르게 정의되지 않는 한, 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 사전에 정의된 용어와 같이 일반적으로 사용되는 용어들은 관련 기술의 문맥 상의 의미 와 일치하는 것으로 해석되어야 하며, 본 개시에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 이상의 설명은 본 개시의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 개시가 속하는 기술 분야에 서 통상의 지식을 가진 자라면 본 개시의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변형이 가 능할 것이다. 따라서, 본 개시에 개시된 실시예들은 본 개시의 기술 사상을 한정하기 위한 것이 아니라 설명하 기 위한 것이고, 이러한 실시예에 의하여 본 개시의 기술 사상의 범위가 한정되는 것은 아니다. 본 개시의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 개시의 권리범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2020-0036629", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예들에 따른 에너지 관리 장치의 블록도이다. 도 2는 본 발명의 실시예들에 따른 인공 신경망의 개략적 구조를 나타낸 도면이다. 도 3은 본 발명의 실시예들에 따른 학습부의 동작의 일 예를 나타낸 흐름도이다. 도 4는 본 발명의 실시예들에 따른 학습부가 학습 샘플을 생성하는 동작의 일 예를 나타낸 도면이다. 도 5는 본 발명의 실시예들에 따른 학습부가 학습 샘플을 학습 버퍼에 저장하는 동작의 일 예를 나타낸 도면이 다. 도 6은 본 발명의 실시예들에 따른 학습부가 학습 버퍼에 저장된 학습 샘플을 추출하는 동작의 일 예를 나타낸 도면이다. 도 7은 본 발명의 실시예들에 따른 학습부가 액터 네트워크 및 크리틱 네트워크의 파라미터를 업데이트하는 동 작의 일 예를 나타낸 흐름도이다. 도 8은 본 발명의 실시예들에 따른 학습부가 액터 네트워크 및 크리틱 네트워크의 파라미터에 대한 그라디언트 를 계산하는 동작의 일 예를 나타낸 흐름도이다. 도 9는 본 발명의 실시예들에 따른 누적 보상의 변화 패턴의 일 예를 나타낸 그래프이다. 도 10은 본 발명의 실시예들에 따른 에너지 관리 시스템의 블록도이다. 도 11은 본 발명의 실시예들에 따른 에너지 관리 방법을 나타낸 흐름도이다."}
