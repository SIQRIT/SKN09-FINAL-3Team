{"patent_id": "10-2017-0053681", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2018-0119939", "출원번호": "10-2017-0053681", "발명의 명칭": "신경망 다운사이징 방법 및 시스템", "출원인": "주식회사 노타", "발명자": "김태호"}}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "신경망 다운사이징 방법에 있어서,신경망 다운사이징을 위해 큰 신경망을 제 1 트레이닝 데이터를 사용함으로써 트레이닝하는 단계로서, 상기 큰신경망은 k개의 히든 레이어를 포함하며 상기 k개의 히든 레이어 각각은 M_k개의 노드들을 포함하고, 상기 제 1트레이닝 데이터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 트레이닝하는 데이터 세트로서, 상기태스크에 대한 제 1 입력 데이터 및 상기 제 1 입력 데이터에 대한 정답을 나타내는 제 1 라벨 데이터를 포함하는, 상기 단계;작은 신경망으로의 지식 전달을 위한 제 2 트레이닝 데이터를 생성하는 단계; 및상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하는 단계로서, 상기 작은 신경망은 k개의 히든 레이어를 포함하며, k개의 히든 레이어 각각은 상기 M_k보다 작은 N_k개의 노드를 포함하는, 상기 단계를 포함하는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 큰 신경망을 상기 제 1 트레이닝 데이터를 사용함으로써 트레이닝하는 단계는,상기 복수의 히든 레이어 각각의 상기 M_k개의 노드들 중 상기 N_k개의 노드들을 선택적으로 사용함으로써 상기입력 데이터를 프로세싱하는 단계,상기 입력 데이터에 대한 프로세싱 결과인 제 1 출력 데이터를 출력하는 단계; 및상기 제 1 출력 데이터와 상기 라벨 데이터를 비교하여 상기 큰 신경망의 파라미터를 조정하는 단계를포함하는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,상기 제 2 트레이닝 데이터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 훈련하는 데이터 세트로서,상기 태스크에 대한 제 2 입력 데이터 및 상기 입력 데이터에 대한 정답을 나타내는 제 2 라벨 데이터를 포함하는 복수의 데이터를 포함하고,상기 제 2 트레이닝 데이터의 상기 제 2 입력 데이터는 상기 제 1 트레이닝 데이터의 제 1 입력 데이터에 해당하고, 상기 제 2 트레이닝 데이터의 제 2 라벨 데이터는 상기 큰 신경망에서 출력된 상기 제 1 출력 데이터에해당하는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 복수의 히든 레이어에서 사용되는 상기 N_k개의 노드는 상기 히든 레이어 및/또는 상기 데이터 쌍 각각 대해 랜덤하게 선택되는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 3 항에 있어서,상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하는 단계는,상기 제 2 입력 데이터를 프로세싱하여 제 2 출력 데이터를 출력하는 단계; 및공개특허 10-2018-0119939-3-상기 제 2 출력 데이터와 상기 제 2 라벨 데이터를 비교하여 상기 작은 신경망의 파라미터를 조정하는 단계를포함하는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항에 있어서,상기 제 1 라벨 데이터는 원 핫 인코딩된(one hot encoded) 데이터로서, 정답 여부를 나타내는 복수의 바이너리값들을 포함하고, 상기 제 2 라벨 데이터는 정답일 확률을 나타내는 복수의 확률 값들을 포함하는, 신경망 다운사이징 방법."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "신경망 다운사이징을 수행하는 컴퓨팅 시스템에 있어서,데이터를 저장하는 메모리; 및 상기 메모리와 연결되어 신경망의 동작 및 신경망 다운사이징을 수행하는 프로세서를 포함하며, 상기 프로세서는, 신경망 다운사이징을 위해 큰 신경망을 제 1 트레이닝 데이터를 사용함으로써 트레이닝하고, 작은 신경망으로의지식 전달을 위한 제 2 트레이닝 데이터를 생성하고, 상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하며, 상기 큰 신경망은 k개의 히든 레이어를 포함하며 상기 k개의 히든 레이어 각각은 M_k개의 노드들을 포함하고,상기 제 1 트레이닝 데이터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 트레이닝하는 데이터 세트로서, 상기 태스크에 대한 제 1 입력 데이터 및 상기 제 1 입력 데이터에 대한 정답을 나타내는 제 1 라벨 데이터를 포함하고,상기 작은 신경망은 k개의 히든 레이어를 포함하며, k개의 히든 레이어 각각은 상기 M_k보다 작은 N_k개의 노드를 포함하는, 컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서,상기 제 1 트레이닝 데이터를 사용한 상기 큰 신경망의 트레이닝은,상기 복수의 히든 레이어 각각의 상기 M_k개의 노드들 중 상기 N_k개의 노드들을 선택적으로 사용함으로써 상기입력 데이터를 프로세싱하고,상기 입력 데이터에 대한 프로세싱 결과인 제 1 출력 데이터를 출력하고, 상기 제 1 출력 데이터와 상기 라벨 데이터를 비교하여 상기 큰 신경망의 파라미터를 조정함으로써 수행되는,컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 9 항에 있어서,상기 제 2 트레이닝 데이터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 훈련하는 데이터 세트로서,상기 태스크에 대한 제 2 입력 데이터 및 상기 입력 데이터에 대한 정답을 나타내는 제 2 라벨 데이터를 포함하는 복수의 데이터를 포함하고,상기 제 2 트레이닝 데이터의 상기 제 2 입력 데이터는 상기 제 1 트레이닝 데이터의 제 1 입력 데이터에 해당하고, 상기 제 2 트레이닝 데이터의 제 2 라벨 데이터는 상기 큰 신경망에서 출력된 상기 제 1 출력 데이터에해당하는, 컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서,공개특허 10-2018-0119939-4-상기 복수의 히든 레이어에서 사용되는 상기 N_k개의 노드는 상기 히든 레이어 및/또는 상기 데이터 쌍 각각 대해 랜덤하게 선택되는, 컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 9 항에 있어서,상기 제 2 트레이닝 데이터를 사용한 상기 작은 신경망의 트레이닝은, 상기 제 2 입력 데이터를 프로세싱하여 제 2 출력 데이터를 출력하고, 및상기 제 2 출력 데이터와 상기 제 2 라벨 데이터를 비교하여 상기 작은 신경망의 파라미터를 조정함으로써 수행되는, 컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 9 항에 있어서,상기 제 1 라벨 데이터는 원-핫 인코딩된 데이터로서, 정답 여부를 나타내는 복수의 바이너리 값들을 포함하고,상기 제 2 라벨 데이터는 정답일 확률을 나타내는 복수의 확률 값들을 포함하는, 컴퓨팅 시스템."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "신경망 다운사이징 방법이 개시된다. 본 발명의 실시예에 따른 신경망 다운사이징 방법은, 신경망 다운사이징을 위해 큰 신경망을 제 1 트레이닝 데이터를 사용함으로써 트레이닝하는 단계로서, 작은 신경망으로의 지식 전달을 위한 제 2 트레이닝 데이터를 생성하는 단계; 및 상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하는 단계를 포함한다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 신경망의 다운사이징 방법으로서, 특히 신경망의 다운사이징을 위해 최적화된 방법으로 큰 신경망을 트레이닝하고, 큰 신경망의 트레이닝 결과를 사용하여 작은 신경망을 트레이닝함으로써 특정 태스트 수행을 위 한 신경망을 다운사이징하는 방법에 대한 것이다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "신경망은 인간의 뇌에서 뉴런들이 연결된 것처럼 구성한 네트워크를 말한다. 수학적 모델로서의 뉴런이 상호 연 결되어 네트워크를 형성하며, 생물학적 신경망과 구별하여 인공 신경망이라고도 지칭할 수 있다. 최근 인공지능의 적용 범위가 증가함에 따라서 인공지능 구현을 위한 알고리즘 중 하나인 신경망 또한 많은 기 술 분야에 적용되고 있다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "신경망은 매우 복잡한 연산을 수행하여 고난이도의 태스크를 수행할 수 있다. 다만, 신경망의 구현 자체가 복잡 한 구조 및 알고리즘을 필요로 하므로 일정 이상의 메모리와 컴퓨팅 파워를 필요로 한다. 신경망의 사용이 보급 됨에 따라서, 개인화된 데이터 처리를 위해 신경망의 사이즈를 줄이는 방법이 제안된다. 신경망의 사이즈를 줄이면서 성능 열화는 최소화하는 것이 중요하다. 사이즈를 줄인 신경망의 성능 향상을 위한 트레이닝 방법이 요구된다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 기술적 과제를 해결하기 위하여, 본 발명의 실시예에 따른 신경망 다운사이징 방법은, 신경망 다운사이 징을 위해 큰 신경망을 제 1 트레이닝 데이터를 사용함으로써 트레이닝하는 단계로서, 상기 큰 신경망은 k개의 히든 레이어를 포함하며 상기 k개의 히든 레이어 각각은 M_k개의 노드들을 포함하고, 상기 제 1 트레이닝 데이 터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 트레이닝하는 데이터 세트로서, 상기 태스크에 대한 제 1 입력 데이터 및 상기 제 1 입력 데이터에 대한 정답을 나타내는 제 1 라벨 데이터를 포함하는, 상기 단계; 작은 신경망으로의 지식 전달을 위한 제 2 트레이닝 데이터를 생성하는 단계; 및 상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하는 단계로서, 상기 작은 신경망은 k개의 히든 레이어를 포함하며, k 개의 히든 레이어 각각은 상기 M_k보다 작은 N_k개의 노드를 포함하는, 상기 단계를 포함한다. 본 발명의 실시예에 따른 신경망 다운사이징 방법에 있어서, 상기 큰 신경망을 상기 제 1 트레이닝 데이터를 사 용함으로써 트레이닝하는 단계는, 상기 복수의 히든 레이어 각각의 상기 M_k개의 노드들 중 상기 N_k개의 노드 들을 선택적으로 사용함으로써 상기 입력 데이터를 프로세싱하는 단계, 상기 입력 데이터에 대한 프로세싱 결과 인 제 1 출력 데이터를 출력하는 단계; 및 상기 제 1 출력 데이터와 상기 라벨 데이터를 비교하여 상기 큰 신경 망의 파라미터를 조정하는 단계를 포함한다. 본 발명의 실시예에 따른 신경망 다운사이징 방법에 있어서, 상기 제 2 트레이닝 데이터는 신경망을 특정 태스 크에 대한 답을 제공할 수 있도록 훈련하는 데이터 세트로서, 상기 태스크에 대한 제 2 입력 데이터 및 상기 입 력 데이터에 대한 정답을 나타내는 제 2 라벨 데이터를 포함하는 복수의 데이터를 포함하고, 상기 제 2 트레이 닝 데이터의 상기 제 2 입력 데이터는 상기 제 1 트레이닝 데이터의 제 1 입력 데이터에 해당하고, 상기 제 2 트레이닝 데이터의 제 2 라벨 데이터는 상기 큰 신경망에서 출력된 상기 제 1 출력 데이터에 해당한다. 본 발명의 실시예에 따른 신경망 다운사이징 방법에 있어서, 상기 복수의 히든 레이어에서 사용되는 상기 N_k개 의 노드는 상기 히든 레이어 및/또는 상기 데이터 쌍 각각 대해 랜덤하게 선택된다. 본 발명의 실시예에 따른 신경망 다운사이징 방법에 있어서, 상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하는 단계는, 상기 제 2 입력 데이터를 프로세싱하여 제 2 출력 데이터를 출력하는 단계; 및 상기 제 2 출력 데이터와 상기 제 2 라벨 데이터를 비교하여 상기 작은 신경망의 파라미터를 조정하는 단계 를 포함한다. 본 발명의 실시예에 따른 신경망 다운사이징 방법에 있어서, 상기 제 1 라벨 데이터는 원 핫 인코딩된(one hot encoded) 데이터로서, 정답 여부를 나타내는 복수의 바이너리 값들을 포함하고, 상기 제 2 라벨 데이터는 정답 일 확률을 나타내는 복수의 확률 값들을 포함한다. 또한, 상술한 기술적 과제를 해결하기 위한 본 발명의 실시예에 따른 신경망 다운사이징을 수행하는 컴퓨팅 시 스템은, 데이터를 저장하는 메모리; 및 상기 메모리와 연결되어 신경망의 동작 및 신경망 다운사이징을 수행하 는 프로세서를 포함하며, 상기 프로세서는, 신경망 다운사이징을 위해 큰 신경망을 제 1 트레이닝 데이터를 사 용함으로써 트레이닝하고, 작은 신경망으로의 지식 전달을 위한 제 2 트레이닝 데이터를 생성하고, 상기 작은 신경망을 상기 제 2 트레이닝 데이터를 사용함으로써 트레이닝하며, 상기 큰 신경망은 k개의 히든 레이어를 포 함하며 상기 k개의 히든 레이어 각각은 M_k개의 노드들을 포함하고, 상기 제 1 트레이닝 데이터는 신경망을 특 정 태스크에 대한 답을 제공할 수 있도록 트레이닝하는 데이터 세트로서, 상기 태스크에 대한 제 1 입력 데이터 및 상기 제 1 입력 데이터에 대한 정답을 나타내는 제 1 라벨 데이터를 포함하고, 상기 작은 신경망은 k개의 히 든 레이어를 포함하며, k개의 히든 레이어 각각은 상기 M_k보다 작은 N_k개의 노드를 포함한다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따르면 시스템은 히든 레이어의 노드들의 수를 감소시킴으로써 신경망의 사이즈를 감소시킬 수 있다. 그리고 시스템은 작은 신경망에게 큰(large) 신경망에서 미리 트레이닝된 지식을 전달해줌으로써 빠르고 효율적 으로 작은 신경망을 트레이닝하여 태스크 수행을 준비시킬 수 있다. 본 발명은 큰 신경망을 트레이닝하는 과정 에서, 목표 사이즈의 신경망을 최적화된 방법으로 트레이닝함으로써, 큰 신경망의 트레이닝에 따른 지식이 작은 신경망에게 효과적으로 적용되도록 할 수 있다. 따라서 본 발명은 태스크 수행을 위한 신경망 사이즈를 감소시 키면서, 사이즈 감소에 따른 성능 열화를 최소화할 수 있다. 그리고 본 발명의 시스템은 사이즈 감소된 신경망 을 효율적으로 그리고 좋은 성능을 갖도록 트레이닝시킬 수 있다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "이하의 명세서에서 본 발명의 효과가 구성과 함께 추가로 설명될 수 있다."}
{"patent_id": "10-2017-0053681", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 바람직한 실시예에 대해 구체적으로 설명하며, 그 예는 첨부된 도면에 나타낸다. 첨부된 도면을 참조 한 아래의 상세한 설명은 본 발명의 실시예에 따라 구현될 수 있는 실시예만을 나타내기보다는 본 발명의 바람 직한 실시예를 설명하기 위한 것이다. 다음의 상세한 설명은 본 발명에 대한 철저한 이해를 제공하기 위해 세부 사항을 포함하지만, 본 발명이 이러한 세부 사항을 모두 필요로 하는 것은 아니다. 본 발명은 이하에서 설명되 는 실시예들은 각각 따로 사용되어야 하는 것은 아니다. 복수의 실시예 또는 모든 실시예들이 함께 사용될 수 있으며, 특정 실시예들은 조합으로서 사용될 수도 있다. 본 발명에서 사용되는 대부분의 용어는 해당 분야에서 널리 사용되는 일반적인 것들에서 선택되지만, 일부 용어 는 출원인에 의해 임의로 선택되며 그 의미는 필요에 따라 다음 설명에서 자세히 서술한다. 따라서 본 발명은 용어의 단순한 명칭이나 의미가 아닌 용어의 의도된 의미에 근거하여 이해되어야 한다. 본 발명은 심층신경망(Deep Neural Network)에 대한 것으로, 심층신경망은 본 명세서에서 신경망으로 약칭할 수 도 있다. 신경망은 인간의 뇌에서 뉴런들이 연결된 것처럼 구성한 네트워크를 말한다. 수학적 모델로서의 뉴런이 상호 연 결되어 네트워크를 형성하며, 생물학적 신경망과 구별하여 인공 신경망이라고도 지칭할 수 있다. 본 발명에서, 컴퓨팅 시스템이 신경망을 구현하고, 데이터 연산을 수행할 수 있다. 컴퓨팅 시스템은 컴퓨터, 핸 드폰, 스마트허브, 노트북, IoT 디바이스 등 데이터 프로세싱이 가능한 임의의 기기의 컴퓨팅 프로세싱 부분을 지칭할 수 있다. 본 명세서에서 컴퓨팅 시스템은 시스템으로 약칭할 수 있다. 컴퓨팅 시스템은 저장 장치에 저 장된 애플리케이션과 같은 데이터를 수행함으로써 본 발명을 수행할 수 있다. 도 1은 본 발명의 실시예에 따른 신경망 모델 및 각 노드의 동작을 나타낸다. 도 1(a)는 본 발명의 실시예에 따른 신경망을 나타낸다. 신경망은 뉴런들을 포함하며, 도 1(a)에서 각각의 원들 이 뉴런들을 나타낸다. 그리고 복수의 뉴런들의 집합을 레이어라고 한다. 뉴런들은 이하에서 노드라고 지칭할 수 있다. 입력 데이터를 받는 레이어를 입력 레이어, 신경망의 연산 결과가 출력되는 레이어를 출력 레이어라고 지칭한다. 도 1(b)는 신경망에서 뉴런의 동작을 나타낸다. 뉴런은 입력(i_0, i_1)에 가중치(w_0, w_1)를 적용하고, 활성함 수(f(x)) 연산을 수행하여 출력을 제공한다. 활성 함수는 실제 신경 세포 처럼 일정 수준 이상의 자극이 주어졌 을 때 값이 급격히 커지는 함수가 사용될 수 있다. 선형 함수를 사용하면 분석 능력이 감소할 수 있어, 비선형 함수가 사용될 수 있다. 실시예로서, 활성 함수는 시그모이드(sigmoid), tenh, 렐루(relu) 등의 함수가 사용될 수 있다. 활성 함수는 입력 신호의 총합을 그대로 사용하지 않고, 출력 신호로 변환한다. 즉 활성 함수는 가중 치가 적용된 입력들에 대해 활성화 여부를 결정한다. 신경망은 특정 태스크의 수행을 위해 학습한다. 즉 시스템이 특정 태스크의 수행을 위해 신경망을 구성하고, 신 경망을 트레이닝할 수 있다. 예를 들면, 사진을 보고 개인지 고양이인지를 알아내는 태스크가 수행될 수 있다.시스템은 복수의 사진들과 복수의 사진에 대한 정답 데이터(개 또는 고양이)를 사용함으로써 입력 데이터 연산 (프로세싱), 출력 데이터와 정답 데이터의 비교 및 가중치의 조정 동작을 반복하여 수행한다. 이렇게 트레이닝 된 신경망은 태스크를 수행할 준비가 된다. 본 명세서에서, 신경망의 학습/트레이닝을 위한 입력 및 정답을 포 함하는 데이터를 데이터 세트 또는 트레이닝 데이터라고 지칭할 수 있다. 도 2는 본 발명의 실시예에 따른 신경망을 나타낸다. 도 2(a)는 본 발명의 실시예에 따른 신경망을, 도 1(b)는 신경망 모델링을 위한 데이터 세트를 나타낸다. 도 1에서, 신경망은 입력 레이어(input), 출력 레이어(output) 및 히든 레이어들(layer1~layer3)을 포함한다. 입력 레이어(input)는 들어온 신호/데이터를 다음 레이어로 전달할 수 있다. 출력 레이어의 노드들의 출력이 신 경망의 최종 결과값에 해당할 수 있다. 입력 레이어와 출력 레이어 사이의 적어도 하나의 레이어를 히든 레이어 라고 지칭한다. 특히 히든 레이어가 복수인 신경망을 심층 신경망이라고 부를 수도 있으며, 본 발명에서는 심층 신경망을 실시예로 설명한다. 신경망은 학습 또는 트레이닝에 의해 모델링될 수 있다. 신경망이 모델링되면, 그 신경망은 시스템이 원하는 태 스크를 수행할 준비가 된 것으로 볼 수 있다. 즉 모델링된 신경망은 입력에 대해 추론된 출력을 제공할 수 있다. 신경망은 목표 태스크를 수행하기 위해 모델링될 수 있다. 목표 태스크를 달성하도록 신경망을 트레이닝하기 위 해 데이터 세트가 준비될 수 있으며, 이러한 데이터 세트를 트레이닝 데이터라고 지칭할 수 있다. 데이터 세트 는 복수의 데이터 쌍(Data Pair 1, Data Pair 2, Data Pair3..)을 포함하며, 데이터 쌍은 입력 데이터와 라벨 데이터를 포함한다. 라벨 데이터는 원 핫 인코딩된(one hot encoded) 데이터로서 하나의 정답을 나타낼 수 있다. 실시예로서, 라벨 데이터는 원 핫 인코딩된 데이터로서, 복수의 바이너리 값들을 포함할 수 있다. 트레이닝 데이터는 복수의 데이터 쌍을 포함할 수 있다. 다만, 데이터의 분류에 따라서 트레이닝 데이터는 입력 데이터와 라벨 데이터를 포함할 수도 있다. 입력 데이터와 라벨 데이터는 서로 매칭될 수 있다. 즉 n번째 입력 데이터에 대한 정답이 n번째 라벨 데이터에 해당할 수 있다. 신경망의 히든 레이어는 노드들을 포함하며, 노드들은 가중치(weight)를 갖는다. 노드들의 가중치는 트레이닝 단계에서 학습을 통해 결정/조정될 수 있다. 신경망은 라벨 데이터와 출력 데이터의 차이가 적어지도록 각 노드 들의 가중치들을 조정할 수 있다. 노드들에 대한 적어도 하나의 가중치를 신경망의 파라미터로 지칭할 수도 있 다. 즉, 시스템/신경망은 각각의 입력 데이터, 출력 데이터 및 라벨 데이터에 대해 신경망의 파라미터 조정을 수행할 수 있으며, 이러한 파라미터 조정은 트레이닝 데이터에 포함된 입력 데이터, 출력 데이터 및 라벨 데이 터의 수 만큼 반복 수행될 수 있다. 도 3은 본 발명의 실시예에 따른 신경망의 학습 과정을 나타낸다. 도 3의 실시예에서, 시스템은 신경망 모델에 데이터 세트를 입력하여 신경망 모델을 트레이닝한다. 즉, 신경망 에 (1,2,0,9,5,4,7,3)이 입력되고, 신경망은 (0.1, 0.2, 0.6, 0.1)을 출력한다. 그리고 시스템/신경망은 출력 과 입력 데이터에 대한 라벨 데이터(0,0,1,0)을 비교한다. 시스템/신경망은 출력 데이터(0.1, 0.2, 0.6, 0.1)과 라벨 데이터(0,0,1,0)의 차이가 줄어들도록 각 레이어들의 파라미터들을 조정할 수 있다. 신경망은 입력되는 트레이닝 데이터에 대해 연산, 출력과 라벨의 비교 및 파라미터 조정의 과정을 반복적으로 수행한다. 모든 트레이닝 데이터에 대해 학습이 완료되면, 이 신경망이 특정 태스크를 위해 모델링된 것으로 볼 수 있다. 다만, 신경망을 구현하는 컴퓨팅 환경에 따라서 구현 가능한 신경망의 사이즈가 다를 수 있다. 히든 레이어의 수가 많고 히든 레이어가 포함하는 노드들의 수가 많을수록 일반적으로 신경망은 더 잘 모델링될 수 있다. 더 잘 모델링된 신경망은 특정 태스크에 대해 정답을 제공할 확률이 더 높다. 그러나 많은 히든 레이어 및 많은 노 드들의 수는 강한 컴퓨팅 파워 및 저장 공간을 필요로 할 수 있다. 따라서 환경에 따라서 학습된 신경망 모델을 저 작은 신경망으로 이전(transfer)할 필요가 발생할 수 있다. 본 명세서에서, 신경망 모델 이전을 위해 학습되 는 오리지널 신경망 모델을 큰 신경망 모델 또는 선생(teacher) 신경망 모델로 지칭하고, 신경망 모델이 이전되 는 대상 신경망 모델을 작은 신경망 모델 또는 학생(student) 신경망 모델로 지칭할 수 있다. 신경망 모델은 신경망 또는 모델로 약칭할 수도 있다. 이하에서는 하나의 신경망에서 학습된 모델을 다른 신경망으로 이전하는 방법에 대해 설명한다. 도 4 내지 도 6은 본 발명의 일 실시예에 따른 신경망 모델 이전(transfer) 방법을 나타낸다. 도 4는 본 발명의 실시예에 따른 신경망 모델 이전을 위한 선생 모델과 학생 모델을 나타낸다. 도 4에서, 모델 A는 도 2에서 설명한 바와 같이 트레이닝 데이터를 사용하여 트레이닝된 모델을 나타내고, 모델 B는 아직 학습이 되지 않은 모델을 나타낸다. 도 4에서는 레이어의 수가 같은 두 개의 모델을 도시하였지만, 모 델 A와 모델 B는 서로 다른 수의 레이어들 또는 서로 다른 수의 노드들을 포함할 수 있다. 도 5는 본 발명의 실시예에 따른 선생 모델을 사용한 트레이닝 데이터 생성 방법을 나타낸다. 상술한 바와 같이 신경망 모델은 입력 데이터와 라벨 데이터를 포함하는 데이터 쌍들 즉 트레이닝 데이터에 의 해 학습된다. 즉 도 5에서처럼, 입력 데이터가 (1,2,0,9,5,4,7,3)인 경우 모델 A의 출력 데이터는 (0.1, 0.2, 0.6, 0.1)가 될 수 있다. 이 경우 시스템은 출력 데이터 (0.1, 0.2, 0.6, 0.1)를 입력 데이터 (1,2,0,9,5,4,7,3)에 대한 라벨로 간주하여 새로운 트레이닝 데이터를 생성할 수 있다. 도 5에서 나타낸 과정을 반복하여, 시스템은 복수의 입력 데이터에 대해, 복수의 출력 데이터를 라벨 데이터로 간주하여 복수의 데이터 쌍을 갖는 새로운 트레이닝 데이터를 생성할 수 있다. 도 6은 본 발명의 실시예에 따른 새로운 트레이닝 데이터를 사용하여 대상 신경망을 트레이닝하는 방법을 나타 낸다. 도 6에서, 시스템은 도 5에서 생성한 새로운 트레이닝 데이터를 사용함으로써 대상 모델을 트레이닝한다. 즉, 대상 모델을 학습시킬 때, 같은 태스크를 수행하는 오리지널 신경망 모델에 의해 생성된 출력 데이터를 라벨 데 이터로 사용하는 것이다. 도 6에서, 대상 신경망 모델은 오리지널 신경망 모델에서 학습의 결과로 출력된 출력 데이터(0.1, 0.2, 0.6, 0.1)를 사용하여 학습한다. 즉 오리지널 신경망 모델의 학습 결과에 해당하는 출력 데이터를 라벨로 사용하므로, 대상 신경망 모델은 더 많은 정보를 사용하여 최적화된 학습을 수행할 수 있다. 도 6에서와 같이 생성된 새로운 트레이닝 데이터의 라벨 데이터는 소프트 디시전 밸류들(soft decision values) 또는 소프트 값들을 포함할 수 있다. 소프트 디시전 밸류 또는 소프트 밸류는 출력 데이터의 각 값들이 정답일 가능성을 나타내는 확률값이 될 수 있다. 즉, 새로운 트레이닝 데이터는 정답이 될 수 있는 확률을 나타내는 복 수의 소프트 값들을 포함할 수 있다. 도 6에서, 첫번째 데이터 쌍에 대해, 출력 데이터는 각각 첫번째 값이 1일 확률(0.3), 두번째 값이 1일 확률 (0.2), 세번째 값이 1일 확률(0.4), 네번째 값이 1일 확률(0.1)을 포함할 수 있다. 시스템은 이 출력 데이터를 라벨 데이터(0.1, 0.2, 0.6, 0.1)과 비교하여 신경망의 파라미터를 조정할 수 있다. 이러한 트레이닝 데이터는 모델 A의 트레이닝에 의해 학습된 지식(knowledge)를 포함한다. 즉, 모델 A의 트레이 닝의 결과(지식)을 모델 B가 트레이닝 데이터로 사용하게 되는 것이다. 따라서 모델 B는 라벨값을 이용하는 경 우에 비해 더 효율적으로, 그리고 더 잘 트레이닝될 수 있다. 특히 태스크를 이한 데이터 중 커먼 영역의 데이 터를 학습하고, 그 지식을 대상 신경망으로 이전하면 대상 신경망의 트레이닝 부담을 더 줄일 수도 있다. 학습된 오리지널 모델의 출력 데이터는 각 클래스를 어떻게 구분할 수 있는지, 각 클래스를 어떤 기준으로 구분 해야 잘 구분할 수 있는지의 정보를 갖는다. 이러한 지식 전달(knowledge transfer) 방법을 사용함으로써 대상 모델은 더 빠르고 더 정확하게 학습할 수 있다. 예를 들면, 신경망 모델의 태스크가 사진의 강아지/고양이 구분일 수 있다. 일반적인 학습 방법에 있어서, 신경 망 모델은 모든 강아지 사진에 대해 [강아지, 고양이]=[1,0] 이라는 라벨 데이터를 사용하여 학습한다. 그리고 출력 데이터는 [0.7, 0.3]이나, 태스크의 결과로는 강아지라는 정답을 출력할 수 있다. 지식 전달 방법은 소프 트 값을 포함하는 오리지널 신경망의 출력 데이터를 라벨 데이터로 사용함으로써, 대상 네트워크를 더 빠르고 정확하게 트레이닝할 수 있는 것이다. 실시예로서, 더 큰 오리지널 신경망을 학습하여 지식을 축적하고, 이 지 식을 활용하여 더 작은 대상 신경망을 트레이닝함으로써 대상 신경망의 성능을 향상시킬 수 있다.노드는 줄지만 더 많은 정보를 갖는 데이터로 대상 모델을 학습시켜서 성능을 더 높일 수 있다. 도 7은 본 발명의 다른 실시예에 따른 신경망 모델 트레이닝 방법을 나타낸다. 도 7은 신경망 모델을 최적화하는 방법을 나타낸다. 즉, 상술한 신경망 학습 방법에 추가로, 레이어들에 대해 추가적인 동작이 부가된다. 도 7의 실시예에서, 신경망 학습 시 레이어들 마다 적어도 하나의 노드들이 연산에 서 제외된다. 도 7은 각 히든 레이어의 노드가 9개인 신경망을 나타낸다. 도 7의 예에서, 레이어 2에서는 1, 3, 4, 9 번째 노드들이 연산에서 제외된다. 레이어 3에서는 3, 5, 6, 7 번 째 노드들이 연산에서 제외되고, 레이어 4에서는 2, 4, 7, 8 번째 노드들이 연산에서 제외된다. 연산에서 제외 되는 노드들은 확률적으로 결정될 수 있다. 연산에서 제외되는 노드들은 랜덤하게 결정될 수도 있다. 학습을 계 속하며 파라미터/웨이트를 수정하는 과정에서도 연산에서 제외된 노드들은 무시될 수 있다. 다만, 학습 이후 신 경망이 동작할 때는 모든 노드가 사용될 수도 있다. 학습 과정에서 특정 확률로 노드들을 제외시키면, 하나의 데이터를 학습할 때마다 비슷한 형태의 다른 모델들을 사용하여 학습하는 것과 비슷한 결과를 가져온다. 즉 일부 노드들을 공유하는 서로 다른 복수의 모델들을 학습 시키는 것과 유사한 동작이 될 수 있다. 따라서 신경망 동작 시에 모든 노드들을 사용하면, 훈련된 복수의 모델 들의 결과를 합친 것과 같아 신경망의 정답 추론 성능이 매우 향상될 수 있다. 이 실시예에 따르면 하나의 모델 로만 학습하는 경우에 비해 결과가 트레이닝 세트에 기초하여 편향되는 것을 방지할 수 있고, 따라서 신경망의 성능이 더욱 향상될 수 있다. 도 8은 본 발명의 실시예에 따른 신경망 다운사이징 방법을 나타낸다. 최근 신경망은 모바일 기기, IoT(Internet on Things) 기기 등 다양한 전자 기기에서 사용되고 있다. 신경망을 트레이닝하는 과정 및 추론(infer)하는 과정은 신경망 모델의 사이즈가 증가함에 따라서 더 많은 연산을 필요로 하게 되었다. 따라서 신경망의 트레이닝 및 추론 연산은 외부 서버에서 수행될 수 있다. 다만, 이런 경우 개인 정보가 외부 서버로 전송되어야 하여 개인 정보 유출 위험이 증가하고, 서버 운용/통신 비용이 발생할 수 있다. 따라서 개별 기기에서 신경망을 활용하기 위해 신경망의 성능은 최대한 적게 감소키면서 모델의 사이즈를 줄이 는 신경망 모델 다운사이즈 방법이 필요하다. 이를 위해 상술한 도 3~5의 신경망 모델 이전을 위한 지식 이전 방법을 사용할 수 있다. 그러나 상술한 지식 이 전 방법은 더 많은 정보를 갖는 트레이닝 데이터로 대상 신경망을 학습시켜 성능을 개선할 수는 있지만, 네트워 크 사이즈의 차이로 인한 성능 열화를 피하기 어렵다. 즉 큰 신경망과 작은 신경망의 사이즈 차이가 크면 큰 신 경망이 전달한 지식을 작은 신경망이 표현하기가 어려워지기 때문이다. 또는 사이즈의 차이로 인해 큰 신경망에 서 전달하는 지식이 오히려 작은 신경망의 학습 성능을 저해할 수도 있다. 따라서 신경망의 사이즈를 줄이면서 성능 열화를 최소화기는 방법이 필요하다. 본 발명은 도 8과 같이 작은 신경망이 학습하기 좋은 데이터 세트를 생성할 수 있도록 큰 신경망을 트레이닝하 는 방법을 제안한다. 즉 본 발명은 작은 신경망이 학습할 수 있는 데이터 세트를 생성하고, 생성된 데이터 세트 로 다운사이징된 작은 신경망을 트레이닝하는 방법에 대해 설명한다. 이하에서 큰 신경망은 히든 레이어의 노드 들의 수가 9이고 작은 신경망은 히든 레이어의 노드들의 수가 3인 실시예로 설명하나, 본 발명의 사상이 이러한 실시예에 제한되지 않는다. 도 9는 본 발명의 실시예에 따른 큰 신경망의 학습 방법을 나타낸다. 본 발명은 큰 신경망을 트레이닝하는 때 다운사이징하려고 하는 작은 신경망의 노드수에 따라서 노드들의 수를 줄여서 트레이닝하는 방법을 제안한다. 도 9에서, 데이터 세트을 학습하는 경우, 큰 신경망의 히든 레이어의 노 드들의 수는 3개로 감소된다. 이 때 데이터 세트에 대해 기 결정된 3개의 노드를 사용하는 대신, 연산마다 다른 조합인 3개의 노드가 사용될 수 있다. 즉 신경망 트레이닝을 위한 각각의 데이터 연산시에 랜덤하게 노드들이 선택되고, 히든 레이어별로 랜덤하게 노드들이 선택될 수 있다. 도 9에서, 큰 신경망은 히든 레이어의 연산 시 9개 중 3개의 노드들만을 사용하여 연산을 수행한다. 히든 레이 어의 연산에서 사용되는 3개의 노드들은 고정되지 않는다. 임의의 확률로 랜덤하게 선택된 임의의 3개의 노드들이 각 히든 레이어에서 데이터 연산에 사용될 수 있다. 도 8에서 첫번째 데이터(1,2,0,9,5,4,7,3)가 입력되면 첫번째 히든 레이어는 1, 4, 8번째 노드들을 사용하여 연산을 수행하고, 두번째 히든 레이어는 1, 5, 9번째 노 드들을 사용하여 연산을 수행하여 (0.1, 0.2, 0.6, 0.1)의 출력 데이터를 출력한다. 시스템은 출력 데이터를 라 벨 데이터(0,0,1,0)과 비교하여 신경망의 파라미터/가중치를 조정할 수 있다. 그리고 두번째 데이터(2, 6, 2, 8, 7, 4, 7, 2)가 입력되면 히든 레이어들 각각은 첫번째 데이터의 경우와 같거 나 다른 랜덤하게 결정된 3개의 노드들을 사용하여 출력 데이터를 출력한다. 그리고 2번째 데이터에 대한 라벨 데이터와 출력 데이터를 비교하여 신경망의 각 히든 레이어의 각 노드들에 대한 가중치 또는 파라미터를 조정할 수 있다. 큰 신경망은 전체 데이터 세트에 대해 이런 과정을 반복함으로써 학습을 종료한다. 실시예로서, 각 히든레이어의 노드들의 수가 다를 수 있다. 이 경우 본 발명은 큰 신경망의 히든 레이어의 노드 들에 대해, 작은 신경망의 히든 레이어의 노드들의 수를 랜덤하게 선택할 수 있다. 즉 작은 신경망의 제 2 히든 레이어의 노드 수가 3인 경우, 큰 신경망은 제 2 히든 레이어의 노드 3개를 사용하여 트레이닝을 수행한다. 도 10은 본 발명의 실시예에 따른 작은 신경망의 트레이닝을 위한 새로운 데이터 세트를 생성하는 방법을 나타 낸다. 큰 신경망은 데이터 세트에 대한 학습 결과 즉 출력 데이터를 라벨 데이터로 하는 새로운 데이터 세트를 생성한 다. 이렇게 생성된 데이터 세트를 다운사이징을 위한 트레이닝 데이터/데이터 세트라고 지칭할 수도 있다. 다운 사이징 용 데이터 세트는 큰 신경망이 오리지널 데이터 세트(또는 로(Raw) 데이터 세트)를 입력받고, 학습하면 서 출력한 데이터를 라벨 데이터로 갖는다. 즉 큰 신경망이 오리지널 데이터 세트를 학습하여 출력하는 출력 데 이터를 각 입력 데이터에 대한 라벨 데이터로 갖는 데이터 세트가 된다. 도 9에서 설명한 바와 같이 큰 신경망은 히든 레이어 당 3개의 노드를 사용함으로써, 입력 데이터 (1,2,0,9,5,4,7,3)를 연산하여 출력 데이터 (0.1, 0.2, 0.6, 0.1)를 출력한다. 그리고 이렇게 출력된 출력 데 이터를 라벨 데이터로 치환함으로써 작은 신경망의 훈련을 위한 데이터 세트가 생성된다. 도 9 및 도 10에서 데 이터 세트가 x개의 입력 데이터와 라벨 데이터의 쌍을 포함하는 경우, 다운사이징 용 데이터 세트는 x개의 입력 데이터와 라벨 데이터의 쌍을 포함한다. 그리고 다운 사이징 용 데이터 세트의 x개의 라벨 데이터는 큰 신경망 의 학습 결과인 출력 데이터로서, 소프트 값들을 포함하는 데이터이다. 새로운 트레이닝 데이터 생성에 대해서는 도 4 내지 도 6과 관련하여 상술한 설명이 적용된다. 도 11은 본 발명의 실시예에 따른 작은 신경망의 학습을 나타낸다. 도 11에서, 각각의 히든 레이어의 노드가 3개인 작은 신경망이 학습을 시작한다. 작은 신경망은 도 9 내지 도 10의 과정에서 생성된 다운사이징 용 트레이닝 데이터를 사용하여 학습을 시작한다. 도 11의 실시예에서, 제 1 입력 데이터(1, 2, 0, 9, 5, 4, 7, 3)이 작은 신경망에 입력되고, 작은 신경망은 (0.2, 0.1, 0.5, 0.2)의 출력 데이터를 출력한다. 그리고 작은 신경망은 트레이닝용 라벨 데이터(0.2, 0.1, 0.5, 0.2)를 사용하여 파라미터/웨이트를 조정한다. 트레이닝용 데이터 세트에 대해 신경망은 학습을 수행하고, 학습이 끝나면 태스크를 수행할 준비가 완료된다. 도 12는 본 발명의 실시예에 따른 신경망 다운사이징 방법을 나타낸다. 시스템은 신경망 다운사이징을 위해 큰 신경망을 트레이닝한다(S12010). 시스템은 제 1 트레이닝 데이터를 사용하여 큰 신경망을 트레이닝한다. 큰 신경망은 k개의 히든 레이어를 포함 하고, k개의 히든 레이어 각각은 M_k 개의 노드들을 포함할 수 있다. 제 1 트레이닝 데이터는 신경망이 특정 태 스크에 대한 답을 제공할 수 있도록 신경망을 트레이닝하는 데이터 세트이다. 제 1 트레이닝 데이터는 제 1 입 력 데이터 및 제 1 라벨 데이터를 포함할 수 있다. 시스템은 작은 신경망으로의 지식 전달을 위한 제 2 트레이닝 데이터를 생성할 수 있다(S12020). 상기 제 2 트레이닝 데이터는 신경망을 특정 태스크에 대한 답을 제공할 수 있도록 훈련하는 데이터 세트이다. 제 2 트레이닝 데이터는 태스크에 대한 제 2 입력 데이터 및 제 2 입력 데이터에 대한 정답을 나타내는 제 2 라벨 데이터를 포함할 수 있다. 시스템은 제 2 트레이닝 데이터를 사용함으로써 작은 신경망을 트레이닝할 수 있다(S12030). 작은 신경망은 k개의 히든 레이어를 포함하고, k개의 히든 레이어 각각은 M_k 보다 작은 N_k개의 노드를 포함할 수 있다. 시스템은 큰 신경망 트레이밍 전에 작은 신경망의 구조를 구성 또는 결정할 수 있다. 즉 시스템은 목 표로 하는 작은 신경망의 레이어별 노드수에 따라서 큰 레이어의 노드수를 줄여서 학습함으로써, 작은 신경망에 최적화된 트레이닝 결과를 전달할 수 있다. 큰 신경망을 트레이닝하는 단계는, 복수의 히든 레이어 각각의 M_k 개의 노드들 중 N_k 개의 노드들을 선택적으 로 사용함으로써 제 1 입력 데이터를 프로세싱하는 단계, 입력 데이터를 프로세싱한 제 1 출력 데이터를 출력하 는 단계 및 제 1 출력 데이터와 라벨 데이터를 비교하여 신경망의 파라미터를 조정하는 단계를 포함할 수 있다. 이러한 프로세싱, 출력, 조정 단계는 트레이닝 데이터의 각 입력데이터 및 각 라벨 데이터에 대해 반복적으로 수행될 수 있다. N_k개의 노드는 히든 레이어 및/또는 데이터 쌍 각각에 대해 랜덤하게 선택/결정될 수 있다. M_k는 자연수이고, N_k는 M_k 미만 또는 이하인 자연수가 될 수 있다. k, M_k, N_k는 모두 자연수가 될 수 있다. 도 8 내지 도 11에서, k=2, M_k=9, N_k=3인 실시예로서 설명하였다. 즉 M_1=M_2=9, N_1=N_2=3인 실시예를 설명하였으나, M_1과 M_2가 동일하지 않을 수 있고, N_1과 N_2가 동일하 지 않을 수도 있다. 본 발명은 N_1이 M_1 미만 또는 이하이고, N_2가 M_2 미만 또는 이하이면 필요한 조건이 만 족된다. 제 2 트레이닝 데이터 생성 단계는 제 1 트레이닝 데이터의 라벨 데이터를 제 1 출력 데이터로 교환함으로써 수 행된다. 즉 제 2 트레이닝 데이터에 대해, 제 2 입력 데이터는 제 1 트레이닝 데이터의 제 1 입력 데이터에 해 당하고, 제 2 라벨 데이터는 큰 신경망에서 출력된 제 1 출력 데이터에 해당한다. 작은 신경망을 트레이닝하는 단계는, 제 2 입력 데이터를 프로세싱하여 제 2 출력 데이터를 출력하는 단계 및 제 2 출력 데이터와 제 2 라벨 데이터를 비교하여 작은 신경망의 파라미터를 조정하는 단계를 포함한다. 신경망 다운 사이징을 위해 큰 신경망을 트레이닝하고(S12010), 작은 신경망으로 지식 전달을 위하 트레이닝 데 이터를 생성(S12020)하는 단계는 도 1 내지 도 10에서, 작은 신경망을 트레이닝하는 단게는 도 11에서 설명하였 으며, 각 단계들에 상술한 설명이 적용될 수 있다. 제 1 라벨 데이터는 원 핫 인코딩된 데이터로서, 정답 여부를 나타내는 복수의 바이너리 값들을 포함할 수 있다. 제 2 라벨 데이터는 정답일 확률을 나타내는 복수의 확률값들을 포함할 수 있다. 본 발명에 따르면, 시스템은 성능 열화를 최소화하면서 신경망을 다운사이징할 수 있다. 또한, 작은 신경망을 완전히 새롭게 학습시키는 대신, 커먼 데이터에 대해 학습된 신경망을 사용함으로써 추가 학습의 양을 줄일 수 있다. 즉 신경망 모델의 이전을 통해 개인화를 위한 신경망 모델링의 학습 양을 현저하게 줄일 수도 있음. 실시 예에 따라서, 시스템은 커먼 데이터에 대해서는 상술한 방법에 따라서 신경망 다운사이징을 수행하고, 개인 정 보에 대해서만 작은 신경망을 추가적으로 트레이닝할 수도 있다. 도 13은 본 발명의 실시예에 따른 신경망 다운사이징 시스템을 나타낸다. 신경망 다운사이징 시스템은 컴퓨팅 시스템으로서, 임의의 전자 기기에 포함될 수 있다. 메모리는 프로세서와 연결되어, 프로세서를 구동하기 위한 다양한 정보를 저장한다. 메모 리는 프로세서의 내부에 포함되거나 또는 프로세서의 외부에 설치되어 프로세서와 공지의 수단에 의해 연결될 수 있다. 메모리는 휘발성 및 비휘발성 메모리를 통칭한다. 본 발명에서 메 모리는 신경망, 신경망에 대한 트레이닝 정보, 신경망 구현 및 트레이닝을 위한 애플리케시션과 같은 데 이터를 저장할 수 있다. 프로세서는 메모리과 연결되어 본 발명에 따른 신경망, 신경망의 트레이닝 방법, 신경망의 다운 사이징 방법을 수행할 수 있다. 상술한 본 발명의 다양한 실시예에 따른 시스템의 동작을 구현하는 모듈, 데이터, 프로그램 또는 소프트웨어 중 적어도 하나가 메모리에 저장되고, 프로세서에 의하 여 실행될 수 있다. 프로세싱 유닛은 본 발명의 방법을 수행하기 위한 애플리케이션/소프트웨어를 구동 함으로써, 본 발명의 방법을 실시할 수 있다.통신 유닛은 시스템의 외부 기기와 유선 통신 또는 무선 통신을 수행할 수 있다. 통신 유닛은 디 바이스의 구성에 따라서 구비되지 않을 수도 있다. 또한, 통신 유닛은 복수의 통신 칩셋들로 구성될 수 도 있다. 통신 유닛은 통신 모듈을 구비하고, 3G, 4G(LTE), 5G, WIFI, 블루투스, NFC 등 다양한 통신 프로토콜에 기초하여 통신을 수행할 수 있다. 시스템 또는 시스템의 프로세서는 도 1 내지 도 12에서 상술한 본 발명의 신경망 다운사이징 방 법을 수행할 수 있다. 이상에서 설명된 실시예들은 본 발명의 구성요소들과 특징들이 소정 형태로 결합된 것들이다. 각 구성요소 또는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려되어야 한다. 각 구성요소 또는 특징은 다른 구성요 소나 특징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명 의 실시예를 구성하는 것도 가능하다. 본 발명의 실시예들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시예의 일부 구성이나 특징은 다른 실시예에 포함될 수 있고, 또는 다른 실시예의 대응하는 구성 또는 특징과 교체될 수 있다. 특허청구범위에서 명시적인 인용 관계가 있지 않은 청구항들을 결합하여 실시예를 구성하거나 출원 후의 보정에 의해 새로운 청구항으로 포함시킬 수 있음은 자명하다. 본 발명에 따른 실시예는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결 합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하 는 모듈, 절차, 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리에 저장되어 프로세서에 의해 구동 될 수 있다. 상기 메모리는 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 상기 프로세서와 데이터를 주고 받을 수 있다. 본 발명은 본 발명의 필수적 특징을 벗어나지 않는 범위에서 다른 특정한 형태로 구체화될 수 있음은 당업자에 게 자명하다. 따라서, 상술한 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니 되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2017-0053681", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시예에 따른 신경망 모델 및 각 노드의 동작을 나타낸다. 도 2는 본 발명의 실시예에 따른 신경망을 나타낸다. 도 3은 본 발명의 실시예에 따른 신경망의 학습 과정을 나타낸다. 도 4는 본 발명의 실시예에 따른 신경망 모델 이전을 위한 선생 모델과 학생 모델을 나타낸다. 도 5는 본 발명의 실시예에 따른 선생 모델을 사용한 트레이닝 데이터 생성 방법을 나타낸다. 도 6은 본 발명의 실시예에 따른 새로운 트레이닝 데이터를 사용하여 대상 신경망을 트레이닝하는 방법을 나타낸다. 도 7은 본 발명의 다른 실시예에 따른 신경망 모델 트레이닝 방법을 나타낸다. 도 8은 본 발명의 실시예에 따른 신경망 다운사이징 방법을 나타낸다. 도 9는 본 발명의 실시예에 따른 큰 신경망의 학습 방법을 나타낸다. 도 10은 본 발명의 실시예에 따른 작은 신경망의 트레이닝을 위한 새로운 데이터 세트를 생성하는 방법을 나타 낸다. 도 11은 본 발명의 실시예에 따른 작은 신경망의 학습을 나타낸다. 도 12는 본 발명의 실시예에 따른 신경망 다운사이징 방법을 나타낸다. 도 13은 본 발명의 실시예에 따른 신경망 다운사이징 시스템을 나타낸다."}
