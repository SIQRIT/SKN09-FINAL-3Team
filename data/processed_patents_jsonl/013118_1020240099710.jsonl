{"patent_id": "10-2024-0099710", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0056095", "출원번호": "10-2024-0099710", "발명의 명칭": "증강 현실 장치 및 그 동작 방법", "출원인": "삼성전자주식회사", "발명자": "신승학"}}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "증강 현실 장치(10)가 가상 영상을 출력하는 방법에 있어서,영상 촬영 장치(20)를 통해 촬영된 영상 데이터를 획득하는 단계;상기 영상 데이터의 촬영 시점의 상기 영상 촬영 장치(20)의 포즈 정보를 획득하는 단계;상기 영상 촬영 장치(20)의 포즈 정보에 기초하여 상기 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정하는 단계; 및상기 결정된 위치 및 각도에 기초하여 상기 가상 영상이 표시된 뷰잉 윈도우를 출력하는 단계를 포함하는,방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 영상 촬영 장치(20)의 포즈 정보는 상기 영상 데이터의 촬영 시점에서 영상 촬영 장치(20)의 높이 정보 및상기 영상 데이터의 촬영 시점에서 영상 촬영 장치(20)의 3차원 공간 각도 정보를 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 또는 제2항에 있어서,상기 영상 촬영 장치(20)의 포즈 정보를 획득하는 단계는, 상기 영상 촬영 장치(20)로부터 상기 포즈 정보를 수신하는 단계를 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 또는 제2항에 있어서,상기 영상 촬영 장치(20)의 포즈 정보를 획득하는 단계는, 상기 영상 데이터 또는 상기 영상 촬영 장치(20)로부터 수신한 센서 정보 중 적어도 하나에 기초하여 상기 포즈 정보를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 영상 촬영 장치(20)의 포즈 정보를 획득하는 단계는,상기 영상 데이터에 대응되는 뎁스 맵을 획득하는 단계;상기 뎁스 맵에 기초하여 상기 영상 데이터의 촬영 시점에서의 상기 영상 촬영 장치(20)의 지면으로부터 높이및 3차원 공간 상의 각도를 획득하는 단계; 및상기 영상 촬영 장치(20)의 지면으로부터 높이 및 3차원 공간 상의 각도에 기초하여 상기 영상 촬영 장치(20)의포즈 정보를 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 증강 현실 장치(10)의 포즈 정보를 획득하는 단계; 및상기 증강 현실 장치(10)의 포즈 정보에 추가로 기초하여 상기 뷰잉 윈도우의 위치 및 각도를 결정하는 단계를더 포함하는, 방법.공개특허 10-2025-0056095-3-청구항 7 제1항 내지 제6항 중 어느 한 항에 있어서,상기 가상 영상이 표시된 뷰잉 윈도우를 출력하는 단계는,상기 결정된 뷰잉 윈도우의 위치 및 각도에 기초하여 디스플레이 상 뷰잉 윈도우를 출력하는 단계; 및상기 출력된 뷰잉 윈도우에 상기 영상 데이터에 대응되는 3차원 가상 영상을 렌더링하는 단계를 포함하는,방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항에 있어서,상기 영상 데이터에 대응되는 상기 영상 촬영 장치(20)의 모션 정보를 획득하는 단계; 및상기 영상 촬영 장치(20)의 모션 정보에 기초하여 상기 뷰잉 윈도우의 위치 및 각도를 변경하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 영상 촬영 장치(20)의 모션 정보는 시간에 따른 상기 영상 촬영 장치(20)의 포즈의 변화에 대한 정보를 포함하는, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항 및 제9항 중 어느 한 항에 있어서,상기 영상 촬영 장치(20)의 모션 정보에 기초하여 상기 뷰잉 윈도우의 위치 및 각도를 변경하는 단계는, 대응되는 상기 가상 영상의 프레임 마다 수행되는 것인, 방법."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "가상 영상을 출력하는 증강 현실 장치(10)에 있어서,디스플레이부(162);적어도 하나의 명령어(instruction)를 포함하는 프로그램을 저장하는 메모리(150); 및적어도 하나의 프로세서(140)를 포함하고,상기 적어도 하나의 프로세서(140)는,영상 촬영 장치(20)를 통해 촬영된 영상 데이터를 획득하고,상기 영상 데이터의 촬영 시점의 상기 영상 촬영 장치(20)의 포즈 정보를 획득하고,상기 영상 촬영 장치(20)의 포즈 정보에 기초하여 상기 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정하고,상기 결정된 위치 및 각도에 기초하여 상기 가상 영상이 표시된 뷰잉 윈도우를 출력하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 영상 촬영 장치(20)의 포즈 정보는 상기 영상 데이터의 촬영 시점에서 영상 촬영 장치(20)의 높이 정보 및상기 영상 데이터의 촬영 시점에서 영상 촬영 장치(20)의 3차원 공간 각도 정보를 포함하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2025-0056095-4-제11항 또는 제12항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 영상 촬영 장치(20)로부터 상기 포즈 정보를 수신하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항 또는 제12항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 영상 데이터 또는 상기 영상 촬영 장치(20)로부터 수신한 센서 정보 중 적어도 하나에 기초하여 상기 포즈정보를 획득하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 영상 데이터에 대응되는 뎁스 맵을 획득하고,상기 뎁스 맵에 기초하여 상기 영상 데이터의 촬영 시점에서의 상기 영상 촬영 장치(20)의 지면으로부터 높이및 3차원 공간 상의 각도를 획득하고,상기 영상 촬영 장치(20)의 지면으로부터 높이 및 3차원 공간 상의 각도에 기초하여 상기 영상 촬영 장치(20)의포즈 정보를 획득하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제15항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 증강 현실 장치(10)의 포즈 정보를 획득하고,상기 증강 현실 장치(10)의 포즈 정보에 추가로 기초하여 상기 뷰잉 윈도우의 위치 및 각도를 결정하는, 증강현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항 내지 제16항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 결정된 뷰잉 윈도우의 위치 및 각도에 기초하여 디스플레이 상 뷰잉 윈도우를 출력하고,상기 출력된 뷰잉 윈도우에 상기 영상 데이터에 대응되는 3차원 가상 영상을 렌더링하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항 내지 제17항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서(140)는,상기 영상 데이터에 대응되는 상기 영상 촬영 장치(20)의 모션 정보를 획득하고,상기 영상 촬영 장치(20)의 모션 정보에 기초하여 상기 뷰잉 윈도우의 위치 및 각도를 변경하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 영상 촬영 장치(20)의 모션 정보는 시간에 따른 상기 영상 촬영 장치(20)의 포즈의 변화에 대한 정보를 포공개특허 10-2025-0056095-5-함하고,상기 적어도 하나의 프로세서(140)는, 상기 가상 영상의 프레임 마다 상기 영상 촬영 장치(20)의 모션 정보에기초하여 상기 뷰잉 윈도우의 위치 및 각도를 변경하는, 증강 현실 장치(10)."}
{"patent_id": "10-2024-0099710", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 증강 현실 장치가 영상 촬영 장치를 통해 촬영된 영상을 뷰잉 윈도우를 통해 가상 영상으로 출력하는 방법을 제공한다. 방법은 영상 촬영 장치를 통해 촬영된 영상 데이터를 획득하는 단계를 포함할 수 있다. 방법은 영상 데이터의 촬영 시점의 영상 촬영 장치의 포즈 정보를 획득하는 단계를 포함할 수 있다. 방법은 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정 하는 단계를 포함할 수 있다. 방법은 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력하 는 단계를 포함할 수 있다."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 증강 현실 장치 및 그 동작 방법에 관한 것으로, 증강 현실 장치가 가상 영상을 표시하기 위한 뷰잉 윈도우의 위치 및 각도를 결정하고 그에 기초하여 가상 영상을 뷰잉 윈도우를 통해 출력하는 방법 및 그 증강 현실 장치에 관한 것이다."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "증강 현실(Augmented Reality, AR) 기술은 현실의 환경에 가상 사물이나 정보를 합성하여, 가상 사물이나 정보 가 현실의 물리적 환경에 존재하는 사물처럼 보이도록 하는 기술이다. 현대의 컴퓨팅 및 디스플레이 기술은 증 강 현실 경험을 위한 시스템의 개발을 가능하게 하였는데, 증강 현실 경험에서는, 디지털적으로 재 생성된 이미 지 또는 그 일부가, 이들이 현실인 것처럼 생각되거나, 또는 현실로서 인식될 수 있는 방식으로 사용자에게 제 시될 수 있다. 증강 현실 기술을 통해 현실 세계(Real-world)의 물리적 환경 공간이나 현실 객체(real world object) 상에 가 상 이미지를 오버레이(overlay)하여 함께 보여줄 수 있다. 증강 현실 기술에 대한 관심이 높아짐에 따라, 증강 현실을 구현하는 다양한 기술들에 대한 개발이 활발하게 이루어지고 있다. 증강 현실 기술을 활용한 증강 현실 장치(예를 들어, 스마트 안경(Smart Glass))가 정보 검색, 길 안내, 멀리 떨어진 사용자들 간의 화상 회의, 카 메라 촬영과 같이 일상 생활에서 유용하게 사용되고 있다. 증강 현실 장치는 사용자 시점의 데이터 획득이 가능한 일상 생활 기기로서, 특히, 스마트 안경 등에서는 현실 세계의 물리적 환경에 대한 이미지 상에 가상 영상을 오버레이하여 표시할 수 있다. 현실 공간의 객체와 가상 객체가 공존하는 증강 현실 환경에서는 사용자의 멀미를 저감시키거나 전체적인 영상 의 이질감을 감소시키기 위해 증강 현실 영상과 현실 세계 이미지 간에 동기화가 필요하다."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면(aspect)은 증강 현실 장치가 가상 영상을 출력하는 방법을 제공한다. 증강 현실 장치의 동작 방법은, 영상 촬영 장치를 통해 촬영된 영상 데이터를 획득하는 단계를 포함할 수 있다. 증강 현실 장치의 동작 방법은, 영상 데이터의 촬영 시점의 영상 촬영 장치의 포즈 정보를 획득하는 단계를 포함할 수 있다. 증강 현실 장치의 동작 방법은, 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰 잉 윈도우의 위치 및 각도를 결정하는 단계를 포함할 수 있다. 증강 현실 장치의 동작 방법은, 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력하는 단계를 포함할 수 있다. 본 개시의 일 측면(aspect)은 가상 영상을 출력하는 증강 현실 장치를 제공한다. 증강 현실 장치는 디스플레이 부를 포함할 수 있다. 증강 현실 장치는 적어도 하나의 명령어를 포함하는 프로그램을 저장하는 메모리를 포함 할 수 있다. 증강 현실 장치는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는, 영상 촬 영 장치를 통해 촬영된 영상 데이터를 획득하고, 영상 데이터의 촬영 시점의 영상 촬영 장치의 포즈 정보를 획 득하고, 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정하고, 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력할 수 있다. 본 개시의 일 측면(aspect)은 컴퓨터로 읽을 수 있는 저장 매체를 포함하는 컴퓨터 프로그램 제품(computer program product)을 제공한다. 저장 매체는 개시된 방법의 실시예들 중에서 적어도 하나를 증강 현실 장치에서실행시키기 위한, 증강 현실 장치에 의해 판독 가능한 명령어들이 저장된 것일 수 있다."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시예를 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시의 실시예들에서 사용되는 용어는 본 개시의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용 어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라 질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 실시예의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 명세서에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 개시 전체에서 어떤 부분이 어떤 구성 요소를 “포함”한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있음을 의미한다. 또한, 본 명세서에 기재된 “...부”, “...모듈” 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 명세서 전체에서, 어떤 부분이 다른 부분과 “연결”되어 있다고 할 때, 이는 “직접적으로 연결”되어 있는 경 우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 “전기적으로 연결”되어 있는 경우도 포함한다. 또한 어떤 부분이 어떤 구성 요소를 “포함”한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외 하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본 명세서에서 사용된 표현 “~하도록 구성된(또는 설정된)(configured to)”은 상황에 따라, 예를 들면, “~에 적합한(suitable for)”, “~하는 능력을 가지는(having the capacity to)”, “~하도록 설계된(designed to) ”, “~하도록 변경된(adapted to)”, “~하도록 만들어진(made to)”, 또는 “~를 할 수 있는(capable of)”과 바꾸어 사용될 수 있다. 용어 “~하도록 구성된(또는 설정된)”은 하드웨어적으로 “특별히 설계된 (specifically designed to)” 것만을 반드시 의미하지 않을 수 있다. 대신, 어떤 상황에서는, “~하도록 구성 된 시스템”이라는 표현은, 그 시스템이 다른 장치 또는 부품들과 함께 “~할 수 있는” 것을 의미할 수 있다. 예를 들면, 문구 “A, B, 및 C를 수행하도록 구성된(또는 설정된) 프로세서”는 해당 동작을 수행하기 위한 전 용 프로세서(예: 임베디드 프로세서), 또는 메모리에 저장된 하나 이상의 소프트웨어 프로그램들을 실행함으로 써, 해당 동작들을 수행할 수 있는 범용 프로세서(generic-purpose processor)(예: CPU 또는 application processor)를 의미할 수 있다. 본 개시에 따른 ‘인공지능(Artificial Intelligence, AI)’과 관련된 기능은 프로세서와 메모리를 통해 동작된 다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저 장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하 드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습(training)을 통해 만들어진 것을 특징으로 한다. 여기서, 학습 을 통해 만들어진다는 것은, 기본 인공지능 모델(또는, 딥러닝 모델)이 학습 알고리즘에 의하여 다수의 학습 데 이터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인 공지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어 질 수도 있고, 별도의 서버 및/또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. ‘인공지능 모델(또는, 딥러닝 모델)’은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 수정될 수 있다. 인공 신경망은 심층 신 경망(Deep Neural Network, DNN)를 포함할 수 있으며, 예를 들어, 컨볼루션 신경망(Convolutional Neural Network, CNN), 순환 신경망(Recurrent Neural Network, RNN), 제한된 볼츠만 머신(Restricted Boltzmann Machine, RBM), 심층 신뢰망(Deep Belief Network, DBN), 양방향 순환 신경망(Bidirectional Recurrent Deep Neural Network, BRDNN), 또는 심층 Q-네트워크(Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. 본 개시에서, ‘증강 현실(Augmented Reality)’은 현실 세계(Real-world)의 물리적 환경 공간 내에 가상 이미 지를 함께 보여주거나 현실 객체와 가상 이미지를 함께 보여주는 것을 나타낸다. 본 개시에서, ‘증강 현실 장치’ 는 증강 현실(augmented reality, AR)을 표현할 수 있는 장치로서, 현실에 존 재하는 물리적 대상체(physical object) 및 가상 대상체(virtual object)를 포함하는 이미지를 표시할 수 있다. 증강 현실 장치는 예를 들어, 사용자가 안면부(顔面部)에 착용하는 안경 형상의 증강 현실 글래스(Augmented Reality Glasses) 뿐만 아니라, 두부(頭部)에 착용하는 헤드 마운트 디스플레이 장치 (Head Mounted Display, HMD, Apparatus)나, 증강 현실 헬멧(Augmented Reality Helmet) 등일 수 있다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른 증강 현실 장치가 영상 데이터에 대응되는 가상 영상(VI)을 표시하기 위한 뷰잉 윈도우(viewing window)(W)의 위치 및 각도를 결정하고 그에 기초하여 가상 영상(VI)을 출력하는 방 법을 설명하기 위한 도면이다. 증강 현실 장치는 증강 현실을 표현할 수 있는 장치로서, 예를 들어, 사용자가 안면부(顔面部)에 착용하는 안경 형상의 증강 현실 글래스(Augmented Reality Glasses)로 구성될 수 있다. 증강 현실 장치의 구성 요소에 대해서는 후술할 도 8 및 도 9에서 보다 상세하게 설명하기로 한다. 일 실시예에서, 증강 현실 장치는 영상 촬영 장치를 통해 촬영된 영상을 가상 영상으로 출력할 수 있다. 영상 촬영 장치는 외부 장치로서 카메라, 스마트폰, 태블릿 PC 등 영상의 촬영이 가능한 다양한 장치 를 포함할 수 있다. 영상 촬영 장치는 증강 현실 장치와 다른 별도의 전자 장치에 포함된 카메라일 수있다. 일 실시예에서, 영상 촬영 장치는 증강 현실 장치에 포함될 수도 있다. 예를 들어, 증강 현실 장 치는 내장된 카메라 모듈을 통해 촬영한 영상을 이후 가상 영상으로 디스플레이 할 수도 있다. 영상 촬영 장치를 통해 촬영된 영상에는 촬영 객체(O)가 포함될 수 있다. 촬영 객체에 대한 영상은 가상 영상(VI) 의 형태로 증강 현실 장치에서 표시될 수 있다. 본 개시에서, '뷰잉 윈도우(viewing window)(W)'는 증강 현실 장치의 디스플레이 영역 중, 기 촬영된 영상 데이터가 출력되는 영역을 나타낼 수 있다. VR(가상 현실, virtual reality) 또는 AR(증강 현실, augmented reality) 장치에서 영상 촬영 장치(VR 또는 AR 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카메라 모듈)를 통해 기 촬영된 영상을 감상할 때, 사용자의 시야 정면에 스크린 (예를 들어, 3차원 공간 각도가 0도인 뷰잉 윈도우(W))을 배치하여 영 상을 재생하는 경우를 고려할 수 있다. 이 경우, 기 촬영된 영상이 장치를 착용한 사용자의 시선 방향과 다른 각도로 촬영된 경우, 영상을 시청하는 사용자의 시선 방향과 영상이 촬영된 방향 간의 차이로 인해 영상이 기울 어져 보일 수 있다. 사용자의 시선 방향과 영상이 촬영된 방향의 차이로 인한 기울기는 3차원 영상 감상 시 사 용자가 이질감을 더욱 심하게 느끼도록 한다. 일 실시예에서, 영상이 촬영되던 중 영상 촬영 장치의 움직임이 발생할 수 있다. 이 경우, 사용자의 시야 정면에 고정된 스크린 (예를 들어, 3차원 공간 각도가 0도인 뷰잉 윈도우(W))을 배치하여 영상을 재생하면 영상 을 시청하는 사용자의 어지러움이 유발될 수 있다. 본 개시의 일 실시예에서는 영상 촬영 장치가 영상을 촬영하는 시점에서의 촬영 방향과 사용자가 해당 영상 을 감상하는 시야 방향이 일치할 수 있도록, 증강 현실 장치가 기 촬영된 영상이 재생되는 영역(예를 들어, 뷰잉 윈도우(W))을 촬영 시점에서의 영상 촬영 장치 포즈(pose)에 기초하여 결정할 수 있다. 예를 들어, 증 강 현실 장치는 영상 데이터가 재생되는 영역인 뷰잉 윈도우(W)의 디스플레이 상 위치 및 각도(뷰잉 윈도우 를 통해 영상이 프로젝션(projection) 될 각도)를 영상 촬영 시점에서의 영상 촬영 장치의 포즈에 기초하여 결 정할 수 있다. 증강 현실 장치는 영상 데이터를 획득할 수 있다. 영상 데이터가 외부 영상 촬영 장치를 통해 촬영된 경우, 증강 현실 장치는 외부 영상 촬영 장치로부터 영상 데이터를 수신할 수 있다. 영상 데이터가 증 강 현실 장치에 내장된 카메라 모듈을 통해 촬영된 경우, 증강 현실 장치는 내부 메모리에서 영상 데이 터에 접근할 수 있다. 이후, 증강 현실 장치는 획득한 영상 데이터를 뷰잉 윈도우(W)를 통해 출력하기 위해, 영상 데이터의 촬영 시점에서의 영상 촬영 장치(내장 카메라(카메라 모듈) 또는 카메라를 포함하는 외부 장치)의 포즈 정보를 획득할 수 있다. 영상 촬영 장치의 포즈 정보는 영상 데이터의 촬영 시점에서, 영상 촬영 장치의 지면 에 대한 높이 정보, 영상 촬영 장치의 촬영 객체(O)에 대한 높이 정보, 또는 영상 촬영 장치의 3차원 공간 각도 정보 중 적어도 하나를 포함할 수 있다. 3차원 공간 각도 정보는 영상 촬영 장치의 촬영 방향의 요(yaw) 값, 피치(pitch) 값 및 롤(roll) 값을 나타낼 수 있으며, 예를 들어, 오일러 각(Euler angle) 또는 쿼 터니언(Quaternion)으로 표현될 수 있다. 일 실시예에서, 증강 현실 장치가 영상 촬영 장치의 포즈 정보를 획득하는 동작은, 외부 영상 촬영 장 치로부터 영상 데이터의 촬영 시점에서의 포즈 정보를 수신하는 것일 수 있다. 예를 들어, 영상 촬영을 위 한 카메라를 포함하는 외부 영상 촬영 장치는 카메라 이외에도 관성 센서(inertial measurement unit, IMU) 또는 깊이 센서(depth camera) 중 적어도 하나를 포함할 수 있다. 카메라를 포함하는 영상 촬영 장치 는 센서를 통해 촬영 시점에서의 카메라의 포즈 정보를 획득할 수 있고, 획득된 포즈 정보를 증강 현실 장치 로 전송할 수 있다. 포즈 정보는 영상 데이터와 함께 메타데이터의 형태로 증강 현실 장치로 전송될 수 도 있고, 별도의 메시지를 통해 증강 현실 장치로 전송될 수 있다. 영상 데이터가 증강 현실 장치에 내장된 카메라 모듈을 통해 촬영된 경우, 증강 현실 장치는 내부 메모 리에서 영상 데이터의 촬영 시점에서의 포즈 정보에 접근할 수 있다. 포즈 정보는 영상 데이터와 함께 메타데이 터의 형태로 메모리에 저장되어 있을 수 있다. 일 실시예에서, 증강 현실 장치가 영상 촬영 장치의 포즈 정보를 획득하는 동작은, 획득한 영상 데이터 또는 외부 영상 촬영 장치로부터 수신한 센서 정보 중 적어도 하나에 기초하여 증강 현실 장치가 직접 포즈 정보를 계산하는 것일 수 있다. 예를 들어, 증강 현실 장치는 획득한 영상 데이터에 대해 이미지 분석을 수행하여 해당 영상 데이터를 촬영 한 영상 촬영 장치의 포즈 정보를 산출할 수 있다. 이 경우, 증강 현실 장치는 영상 데이터에 대응되는 뎁스 맵을 획득하고, 뎁스 맵에 기초하여 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 지면으로부터의 높이 및 3차원 공간 상의 각도를 획득할 수 있다. 예를 들어, 증강 현실 장치는 영상 촬영 장치로부터 관성 센서 또는 뎁스 센서 중 적어도 하나의 센서 정보를 수신할 수 있고, 수신된 센서 정보를 이용하여 해당 영상 데이터를 촬영한 영상 촬영 장치의 포즈 정보를 산출할 수 있다. 센서 정보는 영상 촬영 장치로부터 영상 데이터와 함께 메타데이터의 형태로 수신 될 수도 있고, 영상 데이터와 별개의 메시지를 통해 수신될 수도 있다. 증강 현실 장치는 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되 는 뷰잉 윈도우의 위치 및 각도를 결정할 수 있다. 일 실시예에서 뷰잉 윈도우의 위치 및 각도는 대응되는 영상 데이터가 촬영된 시점에서의 영상 촬영 장치의 촬영 방향과 뷰잉 윈도우에 표시되는 가상 영상을 시청하는 사용자의 시선 방향(시청 방향)이 일치하도록 결정될 수 있다. 영상 촬영 장치의 촬영 방향은 영상 촬영을 위한 카메라 모듈의 시야(field of view, FOV) 중심 방향을 나타낼 수 있고, 사용자의 시청 방향은 영상을 시청 하는 사용자의 시선 방향을 나타낼 수 있다. 일 실시예에서, 뷰잉 윈도우(W)의 수직 방향(노말, normal)이 카메 라 모듈의 촬영 방향과 일치(또는, 정반대)할 경우, 카메라 모듈의 촬영 방향과 사용자의 시청 방향이 일치할 수 있다. 카메라 모듈의 촬영 각도와 대응되도록 뷰잉 윈도우를 배치할 경우 재생되는 컨텐츠와 실세계 사이의 이질감을 줄일 수 있다. 일 실시예에서, 뷰잉 윈도우의 위치 및 각도를 결정하는 동작에는 영상 촬영 장치의 포즈 정보에 추가로 증 강 현실 장치 자체의 포즈 정보가 이용될 수 있다. 일 실시예에서, 증강 현실 장치를 착용한 사용자는 정면 을 바라보고 있지 않거나, 재생되는 영상을 감상하는 도중 움직일 수 있다. 이 경우, 영상 촬영 장치(또는, 카메라 모듈)의 촬영 방향과 뷰잉 윈도우를 통해 재생되는 화면을 시청하는 사용자의 시선 방향이 일치하도록 하기 위해서는, 영상 촬영 장치의 포즈뿐만 아니라 증강 현실 장치를 착용한 사용자의 헤드 포즈 정보 를 이용할 필요가 있다. 일 실시예에서, 증강 현실 장치는 영상 데이터에 대응되는 영상 촬영 장치의 모션(motion) 정보를 획득 하고, 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경할 수 있다. 일 실시예에 서, 영상 촬영 장치는 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영할 수 있다. 영상 촬영 장치의 모션 정보는 시간에 따른 영상 촬영 장치의 포즈의 변화에 대한 정보를 포함할 수 있 다. 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경함으로써, 가상 영상 내에 포함되는 컨텐츠(객체)를 공간 좌표계 상에서 고정시킬 수 있다. 또한, 영상 촬영 장치의 모션 정보에 기초 하여 뷰잉 윈도우의 위치 및 각도를 시간에 따라 변경함으로써, 영상의 촬영 당시의 영상 촬영 장치 모션 그대로 뷰잉 윈도우를 이동시키면서 가상 영상을 재생할 수 있고, 이에 따라 영상 촬영 당시의 모션을 현장감 있게 사용자에게 제공할 수 있다. 일 실시예에서, 증강 현실 장치가 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도 를 변경하는 동작은, 대응되는 가상 영상의 프레임(frame)마다 수행되는 것일 수 있다. 증강 현실 장치는 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력할 수 있다. 일 실시예에서, 증강 현실 장치는 결정된 뷰잉 윈도우의 위치 및 각도에 기초하여 디스플레이 상 뷰잉 윈도 우를 출력하고, 출력된 뷰잉 윈도우에 영상 데이터에 대응되는 3차원 가상 영상을 렌더링할 수 있다. 일 실시예에서, 증강 현실 장치는 뷰잉 윈도우를 통해 출력되는 3차원 가상 영상의 3차원 좌표계 상 원점과 뷰잉 윈도우 밖의 영역을 통해 사용자에게 제공되는 현실 세계 영상의 3차원 좌표계 상 원점을 일치시킬 수 있 다. 예를 들어, 뷰잉 윈도우를 통해 디스플레이되는 가상 영상의 지면 레벨이 뷰잉 윈도우 밖의 영역을 통해 디 스플레이되는 현실 세계 영상의 지면 레벨과 동일한 높이를 가지도록 영상이 출력될 수 있다. 증강 현실 장치 가 뷰잉 윈도우 영역에서 렌더링되는 3차원 가상 영상의 3D 축척(scale)을 실제(또는, 뷰잉 윈도우 밖의 영 역에서 렌더링되는 현실 세계 영상의 3D 축척)와 같도록 함으로써 현실감 있는 가상 영상의 재생이 가능하고, 증강 현실 장치를 통해 가상 영상을 시청하는 사용자에게 영상의 촬영 당시의 카메라 모션을 간접적으로 체 험할 수 있도록 할 수 있다. 이와 같이, 본 개시의 일 실시예에 따르면, 영상의 촬영 당시의 영상 촬영 장치 (또는, 카메라 모듈)의 포 즈를 기반으로 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치를 조정함으로써, 영상 시청 시 사용자의 현 장감을 극대화할 수 있고, 컨텐츠와 실세계 사이의 이질감을 경감하여 가상 영상을 시청하는 사용자의 어지러움을 경감할 수 있다. 도 2는 본 개시의 일 실시예에 따른 증강 현실 장치가 동작하는 방법의 흐름도이다. 단계 210에서 증강 현실 장치는 영상 데이터를 획득한다. 영상 데이터는 영상 촬영 장치를 통해 촬영될 수 있다. 영상 촬영 장치는 증강 현실 장치와 다른 별도의 전자 장치로 구성되거나, 증강 현실 장치에 내장된 카메 라 모듈일 수 있다. 일 실시예에서, 영상 데이터가 별도의 영상 촬영 장치를 통해 촬영된 경우, 증강 현실 장치는 영상 촬영 장치로 부터 무선 통신 또는 유선 통신을 통해 영상 데이터를 수신할 수 있다. 영상 데이터가 증강 현실 장치에 내장된 카메라 모듈을 통해 촬영된 경우, 증강 현실 장치는 내부 메모리에 저장된 영상 데이터에 접근하여 영상 데이터 를 획득할 수 있다. 단계 220에서 증강 현실 장치는 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 포즈 정보를 획득한다. 일 실 시예에서, 영상 촬영 장치는 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영할 수 있다. 영상 촬영 장치가 영상 데이터를 움직이면서 촬영하는 경우, 증강 현실 장치는 영상 데이터 촬영 시의 영상 촬 영 장치의 모션 정보를 획득할 수 있다. 모션 정보는 시간에 따른 영상 촬영 장치의 포즈 변화에 대한 정보를 포함할 수 있다. 증강 현실 장치는 획득한 영상 데이터를 뷰잉 윈도우를 통해 출력하기 위해, 영상 데이터의 촬영 시점에서의 영 상 촬영 장치(내장 카메라 모듈, 별도의 영상 촬영 장치, 또는 외부 카메라 모듈)의 포즈 정보를 획득할 수 있 다. 영상 촬영 장치의 포즈 정보는 영상 데이터의 촬영 시점에서, 영상 촬영 장치의 지면에 대한 높이 정보, 영 상 촬영 장치의 특정 촬영 객체에 대한 높이 정보, 또는 영상 촬영 장치의 3차원 공간 각도 정보 중 적어도 하 나를 포함할 수 있다. 3차원 공간 각도 정보는 영상 촬영 장치의 촬영 방향의 요(yaw) 값, 피치(pitch) 값 및 롤(roll) 값을 나타낼 수 있으며, 오일러 각(Euler angle) 또는 쿼터니언(Quaternion)으로 표현될 수 있다. 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 포즈 정보는 외부 장치로부터 수신되거나 증강 현실 장치가 직접 산출하여 획득할 수 있다. 예를 들어, 증강 현실 장치는 획득한 영상 데이터에 대해 이미지 분석(예를 들 어, 이미지의 뎁스 맵 계산)을 수행하여 해당 영상 데이터를 촬영한 영상 촬영 장치의 포즈 정보를 산출할 수 있다. 예를 들어, 영상 데이터가 별도의 영상 촬영 장치 (또는, 외부 카메라 모듈)를 통해 촬영된 경우, 촬영 시점에 서의 영상 촬영 장치의 포즈 정보는 해당 외부 카메라 모듈을 포함하는 다른 전자 장치로부터 수신될 수 있다. 포즈 정보는 영상 데이터와 함께 메타데이터의 형태로 수신될 수도 있고, 별도의 메시지를 통해 수신될 수도 있 다. 또는, 증강 현실 장치는 해당 외부 카메라 모듈을 포함하는 다른 전자 장치로부터 센서 정보 등 포즈 정보 를 산출하기 위한 정보(예를 들어, 관성 센서 정보 또는 깊이 센서 정보)를 수신한 후, 수신된 정보 및 영상 데 이터에 기초하여 영상 촬영 장치의 포즈 정보를 계산할 수 있다. 포즈 정보를 산출하기 위한 정보는 영상 데이 터와 함께 메타데이터의 형태로 수신될 수도 있고, 별도의 메시지를 통해 수신될 수도 있다. 예를 들어, 영상 데이터가 증강 현실 장치에 내장된 카메라 모듈을 통해 촬영된 경우, 증강 현실 장치는 내부 메모리에서 영상 데이터의 촬영 시점에서의 포즈 정보에 접근하거나, 센서 정보 등 포즈 정보를 산출하기 위한 정보 및 영상 데이터에 기초하여 카메라 모의 포즈 정보를 계산할 수 있다. 포즈 정보 또는 포즈 정보를 산출하 기 위한 정보는 영상 데이터와 함께 메타데이터의 형태로 증강 현실 장치의 메모리에 저장되어 있을 수 있다. 단계 230에서 증강 현실 장치는 영상 촬영 장치의 포즈 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 결정한 다. 뷰잉 윈도우는 영상 데이터가 재생되는 영역, 즉, 영상 데이터에 대응되는 가상 영상이 표시되는 디스플레 이 상의 영역을 나타낼 수 있다. 일 실시예에서, 뷰잉 윈도우의 위치 및 3차원 공간 각도는 영상 촬영 장치가 영상을 촬영하는 시점에서의 촬영 방향과 사용자가 해당 영상을 가상 영상을 통해 감상하는 시선 방향이 일치하도록 결정될 수 있다. 예를 들어, 뷰잉 윈도우의 3차원 공간 각도는 뷰잉 윈도우의 수직 방향(노말 방향)이 영상 촬영 장치의 촬영 방향과 일치하 도록 결정될 수 있다. 영상 촬영 장치의 촬영 각도와 대응되도록 뷰잉 윈도우를 배치할 경우, 촬영된 컨텐츠와 실세계 사이의 이질감을 줄일 수 있다. 일 실시예에서, 증강 현실 장치가 뷰잉 윈도우의 위치 및 각도를 결정하는 동작에는 카메라의 포즈 정보에 추가 로 증강 현실 장치 자체의 포즈 정보가 이용될 수 있다. 예를 들어, 증강 현실 장치를 착용한 사용자가 정면을 바라보고 있지 않거나 영상을 감상하는 도중 움직이는 경우, 영상 촬영 장치의 촬영 방향과 사용자의 시선 방향을 일치시키기 위해서는, 영상 촬영 장치의 촬영 시 포즈뿐만 아니라 증강 현실 장치를 착용한 사용자의 헤드 포즈 정보를 이용할 수 있다. 일 실시예에서, 증강 현실 장치는 영상 촬영 장치의 영상 데이터 촬영 시의 모션 정보를 획득하고, 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 시간에 따라 변경할 수 있다. 일 실시예에서, 영상 촬영 장치는 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영할 수 있다. 영상 촬영 장치 의 모션 정보는 시간에 따른 영상 촬영 장치의 포즈 변화에 대한 정보를 포함할 수 있다. 증강 현실 장치는 모 션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경함으로써, 가상 영상 내에 포함되는 컨텐츠(객체)를 공 간 좌표계 상에서 고정시킬 수 있다. 또한, 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 시간에 따라 변경함으로써, 영상의 촬영 당시의 영상 촬영 장치의 모션 그대로 뷰잉 윈도우를 이동시키면서 가상 영상을 재 생할 수 있고, 이에 따라 영상 촬영 당시의 모션을 현장감 있게 사용자에게 제공할 수 있다. 일 실시예에서, 증강 현실 장치가 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 시간 에 따라 변경하는 동작은, 대응되는 가상 영상의 프레임(frame)마다 수행되는 것일 수 있다. 단계 240에서 증강 현실 장치는 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력한다. 증강 현실 장치는 단계 230을 통해 결정된 뷰잉 윈도우의 위치 및 각도에 기초하여 디스플레이 상 뷰잉 윈도우 를 출력하고, 출력된 뷰잉 윈도우 영역에 단계 210에서 획득한 영상 데이터에 대응되는 3차원 가상 영상을 렌더 링할 수 있다. 즉, 영상 데이터는 뷰잉 윈도우를 통해 출력될 수 있다. 일 실시예에서, 증강 현실 장치는 뷰잉 윈도우를 통해 출력되는 3차원 가상 영상의 3차원 좌표계 상의 원점과 뷰잉 윈도우 밖의 영역을 통해 사용자에게 제공되는 현실 세계 영상의 3차원 좌표계 상의 원점을 일치시킬 수 있다. 예를 들어, 뷰잉 윈도우를 통해 디스플레이되는 가상 영상의 지면 레벨이 뷰잉 윈도우 밖의 영역을 통해 디스플레이되는 현실 세계 영상의 지면 레벨과 동일한 높이를 가지도록 영상이 출력될 수 있다. 뷰잉 윈도우를 통해 출력되는 영상 데이터의 3차원 좌표계 상의 원점과 뷰잉 윈도우 밖의 디스플레이 영역을 통해 사용자에게 제공되는 현실 세계 영상의 3차원 좌표계 상의 원점을 일치시키는 동작에 대해서는 후술할 도 6을 통해 보다 자 세히 설명하도록 한다. 증강 현실 장치가 디스플레이 상의 뷰잉 윈도우 영역에서 렌더링되는 3차원 가상 영상의 3D 축척(scale)을 실제 (또는, 뷰잉 윈도우 밖의 영역에서 렌더링되는 현실 세계 영상의 3D 축척)와 같도록 함으로써 현실감 있는 가상 영상의 재생이 가능하고, 증강 현실 장치를 통해 영상 데이터를 시청하는 사용자에게 영상 데이터의 촬영 당시 영상 촬영 장치의 모션을 간접적으로 체험할 수 있도록 할 수 있다. 도 3은 본 개시의 일 실시예에 따른 영상 촬영 장치를 통해 영상 데이터를 촬영하는 동작을 설명하기 위한 도면이다. 일 실시예에서, 영상 데이터는 영상 촬영 장치를 통해 촬영될 수 있다. 영상 촬영 장치를 통해 촬영된 영상은 2차원 또는 3차원 영상일 수 있다. 영상 촬영 장치는 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영할 수 있다. 영상 촬영 장치는 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 포즈 정보를 계산할 수 있다. 영상 촬영 장치의 포즈 정보는 영상 데이터의 촬영 시점에서, 영상 촬영 장치의 지면에 대한 높이 정보, 영 상 촬영 장치의 촬영 객체에 대한 높이 정보, 또는 영상 촬영 장치의 3차원 공간 각도 정보 중 적어도 하나를 포함할 수 있다. 3차원 공간 각도 정보는 영상 촬영 장치의 촬영 방향의 요(yaw) 값, 피치(pitch) 값 및 롤(roll) 값을 나타낼 수 있으며, 예를 들어, 오일러 각(Euler angle) 또는 쿼터니언(Quaternion)으로 표현될 수 있다. 영상 촬영 장치가 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영하는 경우, 영상 촬영 장치의 모션 정보는 시간에 따른 영상 촬영 장치의 포즈의 변화에 대한 정보를 포함할 수 있다. 예를 들어, 모션 정보는 영상 촬영 장치의 가상 영상의 프레임 단위의 영상 촬영 장치의 포즈 정보의 변화를 나타낼 수 있다. 카메라 모듈을 포함하는 전자 장치는 카메라 이외에도 관성 센서(inertial measurement unit, IMU) 또는 깊이 센서(depth camera) 중 적어도 하나를 포함할 수 있다. 카메라 모듈을 포함하는 전자 장치는 센서를 통해 촬영 시점에서의 카메라 모듈의 포즈 정보(또는, 모션 정보)를 획득할 수 있다. 카메라 모듈의 포즈 정보는 영상 데 이터 또는 센서 정보 중 적어도 하나에 기초하여 계산될 수 있다. 예를 들어, 카메라 모듈을 포함하는 전자 장 치는 촬영한 영상 데이터에 대해 이미지 분석을 수행하여 해당 영상 데이터를 촬영한 카메라 모듈의 포즈 정보를 산출할 수 있다. 이 경우, 전자 장치는 영상 데이터에 대응되는 뎁스 맵을 획득하고, 뎁스 맵에 기초하여 영 상 데이터의 촬영 시점에서의 카메라 모듈의 지면으로부터의 높이 및 3차원 공간 상의 각도를 획득할 수 있다. 카메라 모듈의 포즈 정보(또는, 모션 정보)는 영상 데이터와 함께 메타데이터의 형태로 전자 장치의 메모리에 저장될 수 있다. 영상이 촬영되던 중 영상 촬영 장치의 움직임이 발생하는 경우, 사용자의 시야 정면에 고정된 스크린 (예를 들어, 3차원 공간 각도가 0도인 뷰잉 윈도우(W))을 배치하여 영상을 재생하면 영상을 시청하는 사용자의 어지러 움이 유발될 수 있다. 따라서, 영상 데이터의 촬영 중 영상 촬영 장치의 움직임이 있는 경우, 해당 영상 데 이터를 재생하는 증강 현실 장치에서 영상 촬영 장치의 움직임을 고려하여 영상을 표시하도록 함으로써, 영 상 촬영 당시의 영상 촬영 장치의 모션 그대로 뷰잉 윈도우를 이동시키면서 가상 영상을 재생하여 영상 촬영 당 시의 모션을 현장감 있게 사용자에게 제공할 수 있도록 한다. 도 4는 본 개시의 일 실시예에 따른 증강 현실 장치를 착용한 사용자가 시선을 정면으로 고정하였을 때 뷰잉 윈 도우(W1, W2, W3)를 통해 가상 영상을 출력하는 동작을 설명하기 위한 도면이다. 도 4는 전술한 도 3에서 촬영된 영상을 증강 현실 장치가 재생하는 예시를 도시한다. 도 4의 경우, 증강 현실 장치를 착용한 사용자가 시선을 움직이지 않고 정면 방향으로 고정한 경우를 도시한다. 도 4를 참조하면 증강 현실 장치의 디스플레이 상 일부 영역에 영상을 재생하기 위한 뷰잉 윈도우(W1, W2, W3)가 표시될 수 있다. 증강 현실 장치는 영상 데이터에 대응되는 카메라 모듈(예를 들어, 증강 현실 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카메라 모듈)의 모션 정보를 획득하고, 카메라 모듈의 모션 정보에 기초 하여 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도를 변경할 수 있다. 카메라 모듈을 포함하는 장치가 영상 데이터 를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영하였을 때, 카메라 모듈의 모션 정보는 시간에 따른 카메라 모듈의 포즈의 변화에 대한 정보를 포함할 수 있다. 카메라 모듈의 모션 정보에 기초하여 뷰잉 윈도우 (W1, W2, W3)의 위치 및 각도를 변경함으로써, 가상 영상 내에 포함되는 정적인 컨텐츠(객체(VI))를 공간 좌표 계 상에서 고정시킬 수 있다. 또한, 카메라 모듈의 모션 정보에 기초하여 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도를 변경함으로써, 영상의 촬영 당시의 카메라 모션 그대로 뷰잉 윈도우(W1, W2, W3)를 이동시키면서 가상 영상을 재생할 수 있고, 이에 따라 영상 촬영 당시의 모션을 현장감 있게 사용자에게 제공할 수 있다. 도 4를 참조하면, 영상 데이터에 대응되는 카메라 모듈의 모션 정보에 기초하여 시간에 따라 영상 데이터가 표 시되는 뷰잉 윈도우의 위치 및 각도가 변경될 수 있다. 예를 들어, 도 3에서 영상의 촬영 시 영상 촬영 장치 가 장애물 달리기를 하는 객체를 따라 좌측에서 우측으로 이동하는 경우, 촬영된 영상 데이터를 출력하는 과정에서, 시간 t1에서 뷰잉 윈도우(W1)는 증강 현실 장치의 디스플레이 상 좌측에 위치할 수 있고, 시간 t2에 서 뷰잉 윈도우(W2)는 증강 현실 장치의 디스플레이 상 가운데 위치할 수 있고, 시간 t3에서 뷰잉 윈도우(W3)는 증강 현실 장치의 디스플레이 상 우측에 위치할 수 있다. 본 개시의 일 실시예에 따르면, 영상 데이터의 촬영 시부터 영상 내에서 동적인 객체는 증강 현실 장치를 통해 재생되는 영상 데이터 내에서도 동적으로 움직이게 표시되지만, 영상 내에서 고정인(정적인) 객체는 증강 현실 장치를 통해 재생되는 영상 데이터 내에서도 고정적으로 표현될 수 있다. 예를 들어, 장애물 달리기를 하는 사 람을 찍은 영상을 증강 현실 장치를 통해 재생할 경우, 실제 촬영 과정 중 정적인 객체, 예를 들어, 바닥에 고 정된 장애물(VI)은 재생되는 가상 영상 내에서도 공간 좌표계 상 고정될 수 있다. 즉, 도 3에서 촬영한 영상의 경우, 영상 촬영 시 영상 촬영 장치가 이동하였으므로, 카메라 모듈의 FOV 영 역이 촬영된 영상 데이터 자체에서는 장애물(VI)의 위치가 동적으로 표현될 것이다. 카메라 모듈의 촬영 방향을 기준으로 장애물(VI)은 동적이라고 할 수 있다. 그러나, 증강 현실 장치에서 영상을 재생할 때, 영상이 표시되 는 뷰잉 윈도우(W1, W2, W3)를 카메라 모듈의 모션 정보에 기초하여 이동시킴으로써, 실제로 정적인 객체 (장애 물(VI))를 공간 좌표계 상 고정된 것과 같이 표현할 수 있다. 도 4의 실시예에서 증강 현실 장치를 착용한 사용 자의 시선이 정면으로 고정되어 있으므로 (증강 현실 장치의 포즈가 공간 좌표계 상 고정되어 있으므로) 실제 정적인 객체(장애물(VI), 트랙 선 잔디 등)은 증강 현실 장치의 디스플레이 상 고정된 것처럼 표현될 수 있다. 도 5는 본 개시의 일 실시예에 따른 증강 현실 장치를 착용한 사용자가 시선을 움직일 때 뷰잉 윈도우를 통 해 가상 영상을 출력하는 동작을 설명하기 위한 도면이다. 도 5는 전술한 도 3에서 촬영된 영상을 증강 현실 장치가 재생하는 예시를 도시한다. 도 5의 경우, 증강 현 실 장치를 착용한 사용자가 시선을 영상 내의 동적 객체를 따라 움직이는 경우를 도시한다. 증강 현실 장치 를 착용한 사용자의 시선 방향 변화는 증강 현실 장치의 포즈 변화로 나타날 수 있다. 도 5를 참조하면증강 현실 장치의 디스플레이 상 일부 영역에 영상을 재생하기 위한 뷰잉 윈도우가 표시될 수 있다. 증강 현실 장치는 영상 데이터에 대응되는 카메라 모듈(예를 들어, 증강 현실 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카메라 모듈)의 모션 정보를 획득하고, 카메라 모듈의 모션 정보 및 증강 현실 장치의 포즈 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경할 수 있다. 카메라 모듈을 포 함하는 장치가 영상 데이터를 정지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영하였을 때, 카메라 모듈의 모션 정보는 시간에 따른 카메라 모듈의 포즈의 변화에 대한 정보를 포함할 수 있다. 카메라 모듈의 모션 정보 및 증강 현실 장치의 포즈 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경함으로써, 가상 영상 내에 포함되는 정적인 컨텐츠를 공간 좌표계 상에서 고정시킬 수 있다. 카메라 모듈의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 결정함으로써, 영상의 촬영 당시의 카메라 모션을 반영하여 가상 영상을 재생할 수 있고, 증강 현실 장치의 포즈 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 결정함으로써 더욱 현실감 있는 가상 영상을 사용자에게 제공할 수 있다. 도 5를 참조하면, 영상 데이터에 대응되는 카메라 모듈의 모션 정보 및 증강 현실 장치의 포즈 정보에 기초 하여 시간에 따라 영상 데이터가 표시되는 뷰잉 윈도우의 위치 및 각도가 변경될 수 있다. 예를 들어, 도 3에서 영상의 촬영 시 영상 촬영 장치가 장애물 달리기를 하는 객체를 따라 좌측에서 우측으 로 이동할 수 있다. 도 5를 참조하면, 촬영된 영상 데이터를 출력하는 과정에서, 시간 t1에서 증강 현실 장치 를 착용한 사용자의 시선 방향은 정면을 향할 수 있고(θ1=0), 시간 t2에서 증강 현실 장치를 착용한 사용자의 시선 방향은 정면보다 우측을 향할 수 있고(θ2), 시간 t3에서 증강 현실 장치를 착용한 사용자의 시선 방향은 보다 더 우측을 향할 수 있다(θ3). 도 5를 참조하면, 사용자의 시선은 시간에 따라 우측으로 이동하나, 영상 데이터가 출력되는 뷰잉 윈도우의 위 치는 디스플레이 상 변화가 없을 수 있다. 이 경우, 시간에 따른 사용자의 시선 방향의 변화가 영상 데이터 촬 영 시 카메라 모듈의 촬영 방향의 움직임과 일치하는 경우를 나타낼 수 있다. 예를 들어, 영상 데이터의 시작 시점(t1=0)에서의 카메라 모듈의 촬영 방향이 정면이고, (t2-t1) 시점에서의 카메라 모듈의 촬영 방향이 θ2이 고, (t3-t1) 시점에서의 카메라 모듈의 촬영 방향이 θ3일 수 있다. 다시 말해, 증강 현실 장치를 착용한 사용자가 카메라 모듈의 움직임과 동일한 방향 및 속도로 시선을 움직일 경우, 영상 데이터가 출력되는 뷰잉 윈 도우는 증강 현실 장치의 디스플레이 상 정면에 고정된 것과 같이 보일 수 있다. 본 개시의 일 실시예에 따르면, 영상 데이터의 촬영 시부터 영상 내에서 동적인 객체는 증강 현실 장치를 통해 재생되는 영상 데이터 내에서도 동적으로 움직이게 표시되지만, 영상 내에서 고정인(정적인) 객체는 증강 현실 장치를 통해 재생되는 영상 데이터 내에서도 고정적으로 표현될 수 있다. 예를 들어, 장애물 달리기를 하는 사 람을 찍은 영상을 증강 현실 장치를 통해 재생할 경우, 실제 촬영 과정 중 정적인 객체, 예를 들어, 바닥에 고 정된 장애물(VI)은 재생되는 가상 영상 내에서도 공간 좌표계 상 고정될 수 있다. 즉, 도 3에서 촬영한 영상의 경우, 영상 촬영 시 영상 촬영 장치가 이동 하였으므로, 카메라 모듈의 FOV 영역이 촬영된 영상 데이터 자체에서는 장애물(VI)의 위치가 동적으로 표현될 것이다. 증강 현실 장치에서 영상을 재생할 때, 영상을 시청 중인 사용자가 시선을 영상 내의 '장애물 달리기 중인 사람'에 맞추어 이동시키는 경우 (사용자의 시선이 영상 촬영 장치의 촬영 방향과 동일하게 움직이는 경우), 뷰잉 윈도우가 디스플레이 상 중앙 에 고정되고, 촬영된 영상 데이터가 그대로 보여질 수 있다. 일 실시예에서, 영상이 표시되는 뷰잉 윈도우를 카메라 모듈의 모션 정보에 기초하여 이동시킴으로써, 실제로 정적인 객체를 공간 좌표계 상 고정된 것과 같이 표현할 수 있다. 예를 들어, 실제 정적인 객체 (장애물, 트랙 선, 잔디 등)은 공간 좌표계 상 고정된 것과 같이 표현될 수 있다. 또한, 영상이 표시되는 뷰잉 윈도우를 증강 현실 장치의 포즈 정보에 기초하여 이동시킴으로써, 더욱 현실감 있는 가상 영상을 사용자에게 제공할 수 있다. 도 5의 실시예에서 증강 현실 장치를 착용한 사용자가 시선을 변화시킴에 따라 뷰잉 윈도우의 위치가 변함으로써, 영상 데이터 내의 '가상 공간'을 뷰잉 윈도우를 통해 바라보는 것처럼 표현될 수 있다. 도 6은 본 개시의 일 실시예에 따른 증강 현실 장치가 뷰잉 윈도우(W)의 위치 및 각도를 결정하는 동작을 설명 하기 위한 도면이다. VR 또는 AR 장치에서 영상 촬영 장치(VR 또는 AR 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포 함된 외부 카메라 모듈)를 통해 기 촬영된 영상을 감상할 때, 사용자의 시야 정면에 스크린 (예를 들어, 3차원 공간 각도가 0도인 뷰잉 윈도우(W))을 배치하여 영상을 재생하는 경우를 고려할 수 있다. 이 경우, 기 촬영된 영상이 장치를 착용한 사용자의 시선 방향과 다른 각도로 촬영된 경우, 영상을 시청하는 사용자의 시선 방향과영상이 촬영된 방향 간의 차이로 인해 영상이 기울어져 보일 수 있다. 사용자의 시선 방향과 영상이 촬영된 방 향의 차이로 인한 기울기는 3차원 영상 감상 시 사용자가 이질감을 더욱 심하게 느끼도록 한다. 본 개시의 일 실시예에 따른 증강 현실 장치는, 영상 촬영 장치가 영상 데이터를 촬영하는 시점에서의 촬영 방 향과 사용자가 해당 영상을 감상하는 시야 방향이 일치할 수 있도록, 영상 데이터가 재생되는 영역인 뷰잉 윈도 우(W)의 디스플레이 상 위치 및 영상 프로젝션 각도를 영상 촬영 시점에서의 영상 촬영 장치의 포즈에 기초하여 결정할 수 있다. 일 실시예에서, 뷰잉 윈도우(W)의 디스플레이 상 위치 및 각도의 결정에는 증강 현실 장치의 포즈 정보가 추가로 고려될 수 있다. 일 실시예에서, 증강 현실 장치는 뷰잉 윈도우(W)를 통해 출력되는 3차원 가상 영상(VI)의 3차원 좌표계 상 원 점과 뷰잉 윈도우(W) 밖의 영역을 통해 사용자에게 제공되는 현실 세계 영상(RI)의 3차원 좌표계 상 원점을 일 치시킬 수 있다. 예를 들어, 뷰잉 윈도우(W)를 통해 디스플레이되는 영상(VI)의 지면 레벨이 뷰잉 윈도우(W) 밖 의 영역을 통해 디스플레이되는 현실 세계 영상(RI)의 지면 레벨과 동일한 높이를 가지도록 영상이 출력될 수 있다. 예를 들어, 도 6을 참조하면, 현실 세계 영상(RI) 내에 포함된 책상의 면이 뷰잉 윈도우(W)에 프로젝션되어 디 스플레이되는 3차원 가상 영상(VI) 내에 포함된 책상의 면과 같은 각도를 갖고 평행하도록 뷰잉 윈도우(W)의 디 스플레이 상 위치 또는 3차원 공간 각도 중 적어도 하나가 결정될 수 있다. 현실 세계 영상(RI) 내의 책상의 면 과 3차원 가상 영상(VI) 내의 책상의 면의 각도 및 레벨을 결정하는 동작에는 영상 데이터에 대한 이미지 분석 기법이 이용될 수 있다. 이미지 분석 기법에서는 영상 데이터로부터 뎁스 맵을 획득하고, 뎁스 맵에 기초하여 객체 인식 및 객체와 연관된 정보를 획득할 수 있다. 증강 현실 장치가 뷰잉 윈도우(W) 영역에서 렌더링되는 3차원 가상 영상(VI)의 3D 축척(scale)을 실제(또는, 뷰 잉 윈도우(W) 밖의 영역에서 렌더링되는 현실 세계 영상(RI)의 3D 축척)와 같도록 함으로써 현실감 있는 가상 영상의 재생이 가능하고, 증강 현실 장치를 통해 가상 영상(VI)을 시청하는 사용자에게 영상(VI)의 촬영 당시의 영상 촬영 장치의 모션을 간접적으로 체험할 수 있도록 할 수 있다. 이와 같이, 본 개시의 일 실시예에 따르면, 영상의 촬영 당시의 영상 촬영 장치의 포즈 및 증강 현실 장치의 포 즈 정보를 기반으로 대응되는 가상 영상(VI)이 표시되는 뷰잉 윈도우(W)의 위치 또는 3차원 공간 각도를 조정함 으로써, 영상 시청 시 사용자의 현장감을 극대화할 수 있고, 컨텐츠와 실세계 사이의 이질감을 경감하여 가상 영상을 시청하는 사용자의 어지러움을 경감할 수 있다. 도 7은 본 개시의 일 실시예에 따른 증강 현실 장치가 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도를 변경하는 단계를 설명하기 위한 도면이다. 도 7의 경우, 증강 현실 장치를 착용한 사용자가 시선을 움직이지 않고 정면 방향으로 고정한 경우를 도시한다. 도 7을 참조하면 증강 현실 장치의 디스플레이 상 일부 영역에 영상을 재생하기 위한 뷰잉 윈도우(W1, W2, W3) 가 표시될 수 있다. 증강 현실 장치는 영상 데이터에 대응되는 영상 촬영 장치의 모션 정보를 획득하고, 영상 촬영 장치의 모션 정 보에 기초하여 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도를 변경할 수 있다. 영상 촬영 장치가 영상 데이터를 정 지된 상태에서 촬영하는 것이 아니라 움직이면서 촬영하였을 때, 영상 촬영 장치의 모션 정보는 시간에 따른 영 상 촬영 장치의 포즈의 변화에 대한 정보를 포함할 수 있다. 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도 우(W1, W2, W3)의 위치 및 각도를 변경함으로써, 가상 영상 내에 포함되는 정적인 컨텐츠를 공간 좌표계 상에서 고정시킬 수 있다. 또한, 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도를 변경함으로써, 영상의 촬영 당시의 영상 촬영 장치의 모션 그대로 뷰잉 윈도우(W1, W2, W3)를 이동시키면서 가 상 영상을 재생할 수 있고, 이에 따라 영상 촬영 당시의 모션을 현장감 있게 사용자에게 제공할 수 있다. 도 7을 참조하면, 영상 데이터에 대응되는 영상 촬영 장치의 모션 정보에 기초하여 시간(t1, t2, t3)에 따라 영 상 데이터가 표시되는 뷰잉 윈도우(W1, W2, W3)의 위치 및 각도가 변경될 수 있다. 예를 들어, 뷰잉 윈도우(W1, W2, W3)를 통해 표시되는 영상 데이터는 사무실을 좌측에서 우측으로 이동하며 찍은 파노라마 영상일 수 있다. 영상 데이터를 출력하는 과정에서, 시간 t1에서 뷰잉 윈도우(W1)는 증강 현실 장치의 디스플레이 상 중심보다 좌측에 위치할 수 있고, 시간 t2에서 뷰잉 윈도우(W2)는 증강 현실 장치의 디스플레이 상 중심보다 우측에 위치 할 수 있고, 시간 t3에서 뷰잉 윈도우(W3)는 증강 현실 장치의 디스플레이 상 더욱 우측에 위치할 수 있다. 본 개시의 일 실시예에 따르면, 영상 데이터의 촬영 시부터 영상 내에서 고정인(정적인) 객체는 증강 현실 장치 를 통해 재생되는 영상 데이터 내에서도 고정적으로 표현될 수 있다. 예를 들어, 사무실을 촬영한 파노라마 영상을 증강 현실 장치를 통해 재생할 경우, 실제 촬영 과정 중 정적인 객체(예를 들어, 책상, 화분, 컴퓨터 등) 는 재생되는 가상 영상 내에서도 공간 좌표계 상 고정될 수 있다. 또한, 뷰잉 윈도우(W1, W2, W3)를 통해 출력되는 3차원 가상 영상의 3차원 좌표계 상 원점과 뷰잉 윈도우(W1, W2, W3) 밖의 영역을 통해 사용자에게 제공되는 현실 세계 영상(RI)의 3차원 좌표계 상 원점은 일치할 수 있다. 예를 들어, 도 7을 참조하면, 현실 세계 영상(RI) 내에 포함된 책상의 면이 뷰잉 윈도우(W1, W2, W3)에 프로젝 션되어 디스플레이되는 3차원 가상 영상 내에 포함된 적어도 하나의 책상의 면과 같은 각도를 갖고 평행하도록 뷰잉 윈도우(W1, W2, W3)의 디스플레이 상 위치 또는 3차원 공간 각도 중 적어도 하나가 결정될 수 있다. 현실 세계 영상(RI) 내의 책상의 면과 3차원 가상 영상 내의 책상의 면의 각도 및 레벨을 결정하는 동작에는 영상 데 이터에 대한 이미지 분석 기법이 이용될 수 있다. 이미지 분석 기법에서는 영상 데이터로부터 뎁스 맵을 획득하 고, 뎁스 맵에 기초하여 객체 인식 및 객체와 연관된 정보를 획득할 수 있다. 증강 현실 장치가 뷰잉 윈도우(W1, W2, W3) 영역에서 렌더링되는 3차원 가상 영상의 3D 축척(scale)을 실제(또 는, 뷰잉 윈도우(W1, W2, W3) 밖의 영역에서 렌더링되는 현실 세계 영상(RI)의 3D 축척)와 같도록 함으로써 현 실감 있는 가상 영상의 재생이 가능하다. 즉, 움직이는 영상 촬영 장치가 촬영한 영상을 증강 현실 장치에서 재생할 때, 영상이 표시되는 뷰잉 윈도우 (W1, W2, W3)를 영상 촬영 장치의 모션 정보에 기초하여 이동시킴으로써, 실제로 정적인 객체를 공간 좌표계 상 고정된 것과 같이 표현할 수 있다. 도 7의 실시예에서 증강 현실 장치를 착용한 사용자의 시선이 정면으로 고정 되어 있으므로 (증강 현실 장치의 포즈가 공간 좌표계 상 고정되어 있으므로) 실제 정적인 객체들은 증강 현실 장치의 디스플레이 상 고정된 것처럼 표현될 수 있다. 도 8은 본 개시의 일 실시예에 따른 증강 현실 장치의 블록도이다. 도 8을 참조하면, 증강 현실 장치는 통신 인터페이스, 카메라 모듈, 센서, 프로세서, 메모리, 및 출력 인터페이스를 포함할 수 있다. 통신 인터페이스, 카메라 모듈, 센서 , 프로세서, 메모리, 및 출력 인터페이스는 각각 전기적 및/또는 물리적으로 서로 연결될 수 있다. 한편, 증강 현실 장치의 구성 요소가 도 8에 도시된 바와 같이 한정되는 것은 아니다. 도 8에 도 시된 구성 요소보다 많은 구성 요소들에 의해 증강 현실 장치가 구현될 수도 있고, 도 8에 도시된 구성 요 소보다 적은 구성 요소에 의해 증강 현실 장치가 구현될 수도 있다. 본 개시의 일 실시예에서, 증강 현실 장치는 사용자의 두부에 착용하는 증강 현실 글래스로 구현될 수 있고, 이 경우 증강 현실 장치는 통신 인터페이스, 카메라 모듈, 센서, 프로세서, 및 출력 인터페이스에 구동 전력을 공급하는 전원 공급부(예를 들어, 배터리)를 더 포함할 수 있다. 일 실시 예에서, 증강 현실 장치는 스피커를 포함하지 않을 수도 있다. 통신 인터페이스는 유선 또는 무선 통신 네트워크를 통해 서버 또는 외부 디바이스(예를 들어, 관심 디바 이스)와 데이터를 송수신하도록 구성된다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜(Wireless LAN), 와이파이(Wi-Fi), 블루투스(Bluetooth), 지그비(zigbee), WFD(Wi-Fi Direct), 적외선 통신(IrDA, infrared Data Association), BLE(Bluetooth Low Energy), NFC(Near Field Communication), 와이브로 (Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Allicance, WiGig) 및 RF 통신을 포함하 는 데이터 통신 방식 중 적어도 하나를 이용하여 서버 또는 외부 디바이스와 데이터 통신을 수행할 수 있다. 그 러나, 이에 한정되는 것은 아니고, 증강 현실 장치가 스마트 글래스와 같은 웨어러블 디바이스로 구현되는 경우, 통신 인터페이스는 CDMA, WCDMA, 3G, 4G(LTE), 5G Sub 6, 및/또는 밀리미터 파(mmWave)를 이용한 통신 방식과 같은 이동 통신 규격을 따르는 네트워크를 통하여 서버 또는 외부 디바이스와 데이터 송수신할 수 도 있다. 일 실시예에서, 통신 인터페이스는 블루투스 또는 와이파이 다이렉트 등 근거리 통신 방식을 통해 적어도 하나의 외부 객체(예를 들어, 별도의 영상 촬영 장치)와 연결되고, 외부 객체와 다양한 정보를 송수신할 수도 있다. 카메라 모듈은 현실 공간을 촬영함으로써 2차원(2D) 또는 3차원(3D) 이미지 데이터를 획득하도록 구성된다. 카메라 모듈은 증강 현실 장치에 장착될 수 있도록 소형 폼 팩터(form factor)로 구현되고, 저전력을 소비하는 경량 RGB 카메라일 수 있다. 그러나, 이에 한정되는 것은 아니고, 본 개시의 일 실시예에서 카메라 모듈은 깊이 추정 기능을 포함하는 RGB-depth 카메라, 동적 비전 센서 카메라(Dynamic VisionSensor camera), 스테레오 어안 카메라, 그레이 스케일 카메라, 또는 적외선 카메라 등 공지의 모든 종류의 카 메라로 구현될 수 있다. 일 실시예에서, 증강 현실 장치는 복수의 카메라 모듈을 포함할 수 있고, 복 수의 카메라 모듈 중 적어도 일부는 사용자를 향하는 방향으로 배치되고, 사용자의 얼굴을 촬영하도록 구 성될 수도 있다. 카메라 모듈은 렌즈 모듈, 이미지 센서, 및 이미지 프로세싱 모듈을 포함할 수 있다. 카메라 모듈은 이미지 센서(예를 들어, CMOS 또는 CCD)에 의해 현실 장면에 관한 정지 이미지(still image) 또는 동영상 (video)을 획득할 수 있다. 동영상은 카메라 모듈을 통해 현실 영역을 촬영함으로써 실시간으로 획득되는 복수의 이미지 프레임을 포함할 수 있다. 이미지 프로세싱 모듈은 이미지 센서를 통해 획득된 단일 이미지 프레 임으로 구성된 정지 이미지 또는 복수의 이미지 프레임으로 구성된 동영상 데이터를 인코딩하여 프로세서 에 전달할 수 있다. 본 개시의 일 실시예에 따른 현실 장면 이미지의 촬영은, 증강 현실 장치에 구비된 카메라 모듈(예를 들어, 이미지 센서 및 렌즈를 포함하는 카메라)를 제어하여 렌즈를 통해 형성된 광학 상(optical image)을 전기 신호로 변환하여 이미지를 획득하는 증강 현실 장치의 동작을 포함할 수 있다. 예를 들어, 하나 이상의 프 로세서는 증강 현실 장치에 구비된 카메라 모듈을 제어하여 증강 현실 장치의 주변을 촬영하 여 하나 이상의 프레임을 포함하는 이미지(예를 들어, 촬영 이미지)를 획득할 수 있다. 여기서, 이미지는, 라이 브 뷰(live-view) 이미지를 포함할 수 있다. 센서는 현실 공간, 위치, 상황, 또는 사용자 정보를 검출하도록 구성된 센서들을 포함할 수 있다. 본 개시 의 일 실시예에서, 센서는 시선 추적 센서, IMU(Inertial Measurement Unit) 센서, GPS(Global Positioning System) 센서, BLE(Bluetooth Low Energy) 센서, UWB(Ultra Wide Broadband) 센서 또는 그 밖에 다양한 신호를 센싱할 수 있는 센서를 포함할 수 있으나, 이에 제한되는 것은 아니다. 프로세서는 메모리에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행할 수 있다. 프 로세서는 산술, 로직 및 입출력 연산과 이미지 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 도 8에는 프로세서가 한 개인 것처럼 도시 되었으나, 이에 한정되는 것은 아니며 프로세서는 하나 이상의 복수 개의 요소들로 구성될 수 있다. 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지능 전용 프로세서일 수 있다. 프로세서는, 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 프로세서가 인공지능 전용 프로세서인 경우, 인공 지능 전용 프로세서는 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 메모리는 예를 들어, 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미 디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 또는 광 디스크 중 적어도 하나의 타입의 저장매체로 구성될 수 있다. 메모리에는 증강 현실 장치가 동작하기 위한 기능 및/또는 동작과 관련된 명령어들(instructions)이 저장될 수 있다. 일 실시예에서, 메모리에는 프로세서가 판독할 수 있는 명령어들, 알고리즘 (algorithm), 데이터 구조, 프로그램 코드(program code), 및 애플리케이션 프로그램(application program) 중 적어도 하나가 저장될 수 있다. 메모리에 저장되는 명령어들, 알고리즘, 데이터 구조, 및 프로그램 코드는 예를 들어, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 프로세서는 메모리에 저장된 명령어들 또는 프로그램 코드들을 실행할 수 있고, 증강 현실 장치 의 전반적인 동작을 제어할 수 있다. 프로세서는 본 개시의 일 실시예에 따른 동작들을 수행할 수 있다. 예를 들어, 프로세서는 메모리에 저장된 프로그램들을 실행함으로써, 통신 인터페이스, 카메라 모듈, 센서, 및 출력 인터페이스를 전반적으로 제어할 수 있다. 프로세서는 산술, 로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성요소로 구성될 수 있 다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific IntegratedCircuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), 및 FPGAs(Field Programmable Gate Arrays) 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 일 실시예에서, 프로세서는 메모리에 저장된 하나 이상의 명령어들을 실행함으로써, 영상 촬영 장치 (예를 들어, 증강 현실 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카메라 모듈)를 통해 촬영된 영상 데이터를 획득하고, 해당 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 포즈 정 보를 획득하고, 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈 도우의 위치 및 각도를 결정하고, 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력할 수 있다. 출력 인터페이스는 프로세서의 제어에 의해 영상 촬영 장치를 통해 촬영된 영상 데이터를 뷰잉 윈도 우를 통한 증강 현실 영상 (가상 영상)으로 출력하거나, 또는 음향 신호를 출력하도록 구성될 수 있다. 출력 인 터페이스는 디스플레이부 및 스피커를 포함할 수 있다. 디스플레이부는 예를 들어, 액정 디스플레이(liquid crystal display), 박막 트랜지스터 액정 디스플레이 (thin film transistor-liquid crystal display), 유기 발광 다이오드(organic light-emitting diode), 플렉시 블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기 영동 디스플레이(electrophoretic display) 중에서 적어도 하나로 구성될 수 있다. 일 실시예에서, 증강 현실 장치가 증강 현실 글라스로 구성되는 경우, 디스플레이부는 렌즈 광학계로 구성되고, 웨이브가이드(waveguide) 및 광학 엔진을 포함할 수 있다. 광학 엔진은 텍스트, 아이콘, 또는 가상 이미지 등으로 구성된 증강 현실 영상의 광을 생성하고, 광을 웨이브가이드에 투사(project)하는 프로젝터 (projector)로 구성될 수 있다. 광학 엔진은 예를 들어, 화상 패널, 조명 광학계, 투사 광학계 등을 포함할 수 있다. 일 실시예에서, 광학 엔진은 증강 현실 글래스의 프레임 또는 안경 다리에 배치될 수 있다. 스피커는 음향 신호를 출력하도록 구성된다. 일 실시예에서, 스피커는 프로세서의 제어에 의해 음성 메시지 또는 알림음을 출력할 수 있다. 도 9는 본 개시의 일 실시예에 따른 글래스 타입의 증강 현실 장치를 도시한 도면이다. 도 9를 참조하면, 영상 촬영 장치를 통해 촬영된 영상 데이터를 가상 영상으로 출력할 수 있는 증강 현실 장치 가 도시 된다. 증강 현실 장치는 증강 현실(AR) 관련 서비스를 제공할 수 있는 장치로서, 일반적으로 사용자가 안면부에 착용하는 안경 형상의 AR 안경(AR glasses), 두부에 착용하는 헤드 마운트 디스플레이(head mounted display, HMD), 가상 현실 헤드셋(virtual reality headset, VRH), 또는 AR 헬멧(AR helmet) 등을 포 함할 수 있다. 헤드 마운트형 장치의 경우, 사용자의 눈 앞에 디스플레이를 배치함으로써, 사용자에게 초대형 화면을 제공할 수 있고, 사용자의 움직임에 따라 화면이 움직이므로 사실적인 가상 세계를 제공할 수 있다. 일 실시예에서, 사용자는 시각적 확장 현실 콘텐트를 표시할 수 있는, 증강 현실 장치를 착용할 수 있다. 증강 현실 장치는 오디오 확장 현실 콘텐트를 사용자에게 제공할 수 있는 오디오 모듈을 포함할 수 있다. 일 실시예에서, 증강 현실 장치는 환경의 이미지 및 비디오를 캡쳐할 수 있는 하나 이상의 카메라를 포함 할 수 있다. 증강 현실 장치는 사용자의 수렴 거리(vergence distance)를 결정하기 위해 시선 추적(eye tracking) 시스템을 포함할 수 있다. 일 실시예에서, 증강 현실 장치는 경량의 머리 착용형 디스플레이 (HMD)(예를 들어, 고글(goggles), 안경, 바이저(visor) 등)를 포함할 수 있다. 일 실시예에서, 증강 현실 장치 는 경량의 휴대용 표시 장치 또는 하나 이상의 레이저 투사 안경(예를 들어, 사용자에게 이미지 또는 깊이 콘텐트를 투사 및 표시하기 위해 사용자의 망막 상에 저전력 레이저(low-powered laser)를 투사할 수 있는 안경)과 같은 장치를 포함할 수 있다. 일 실시예에서, 증강 현실 장치는 사용자의 시야(field of view, FOV)로 판단되는 영역에 적어도 하나의 가상 객체가 겹쳐 보이도록 출력하는 AR 서비스를 제공할 수 있다. 예를 들어, 사용자의 시야로 판단되는 영역 은 증강 현실 장치를 착용한 사용자가 증강 현실 장치를 통해 인지할 수 있다고 판단되는 영역으로, 증강 현실 장치의 디스플레이의 전체 또는 적어도 일부를 포함하는 영역일 수 있다. 일 실시예에서, 증강 현실 장치는 사용자의 양안 각각에 대응하는 복수개의 투명 부재(920, 930)를 포함할 수 있다. 일 실시예에서, 증강 현실 장치는 디스플레이 모듈, 카메라, 오디오 출력부, 및 지지부(921, 922)를 포함할 수 있다.카메라는 사용자의 시야에 대응되는 영상을 촬영하거나 객체와의 거리를 측정할 수 있다. 카메라는 전술한 도 8 의 카메라 모듈에 대응될 수 있다. 일 실시예에서, 카메라는 헤드 트래킹(head tracking) 및 공간 인식을 위해 사용될 수 있다. 또한, 카메라는 사용자의 움직임을 인식할 수도 있다. 일 실시예에서, 카메라는 사용자의 시야에 대응되는 영상, 즉 현실 장면 이미지를 획득하고, 객체를 검출하거나, 공간 인식을 위해 사용되는 카메라 이외에 ‘ET(eye tracking) 카메라’를 더 포함할 수 도 있다. 일 실시예에서, ET 카메라는 사용자의 눈동자를 검출하고 추적하는 용도로 사용될 수 있다. ET 카메라는 증강 현실 장치에 투영되는 가상 영상의 중심이, 증강 현실 장치를 착용하는 사용자의 눈동자가 응시하는 방향에 따라 위치하도록 조절하기 위한 용도로 사용될 수 있다. 예를 들어, ET 카메라 에는 눈동자(pupil)를 검출하고 빠른 눈동자 움직임을 지연 없이 추적할 수 있도록 GS(Global shutter) 카메라 가 이용될 수 있다. ET 카메라는 좌안용 카메라(912-1) 및 우안용 카메라(912-2)를 별도로 포함할 수도 있 다. 일 실시예에서 디스플레이 모듈은 제1 디스플레이 및 제2 디스플레이를 포함할 수 있다. 디스플 레이 모듈은 전술한 도 8의 디스플레이부에 대응될 수 있다. 디스플레이 모듈을 통해 출력되는 가상 객체는 영상 촬영 장치를 통해 촬영된 영상 데이터가 표시되는 뷰잉 윈도우를 포함할 수 있다. 일 실시예에서, 디스플레이(920, 930)는 투명 부재에 집광 렌즈 또는 웨이브가이드(waveguide, 도파관 또는 도 파로)를 포함할 수 있다. 예를 들어, 투명 부재는 글래스 플레이트, 플라스틱 플레이트, 또는 폴리머로 형성될 수 있고, 완전히 투명하거나 반투명하게 제작될 수 있다. 일 실시예에서, 투명 부재는 증강 현실 장치를 착용한 사용자의 우안에 대면하는 제1 투명 부재 및 사용자의 좌안에 대면하는 제2 투명 부재를 포함 할 수 있다. 디스플레이가 투명인 경우, 사용자의 눈과 대면하는 위치에 배치되어 화면을 표시할 수 있다. 웨이브가이드는 디스플레이의 광원에서 생성된 빛을 사용자의 눈으로 전달할 수 있다. 예를 들어, 웨이브가이드 는 투명 부재(920, 930)의 일부에 적어도 부분적으로 위치할 수 있다. 일 실시예에 따르면, 디스플레이에서 방 출된 광은 웨이브가이드의 일단으로 입사될 수 있고, 입사된 광이 웨이브가이드(waveguide) 내에서 내부 전반사 를 통해 사용자 눈으로 전달될 수 있다. 웨이브가이드는 유리, 플라스틱, 또는 폴리머등 투명한 소재로 제작될 수 있으며, 내부 또는 외부의 일 표면에 형성된 나노 패턴, 예를 들어, 다각형 또는 곡면 형상의 격자 구조 (grating structure)를 포함할 수 있다. 일 실시예에서, 입광된 광은 나노 패턴에 의해 웨이브가이드 내부에서 전파 또는 반사되어 사용자의 눈으로 제공될 수 있다. 일 실시예에서, 웨이브가이드(waveguide)는 적어도 하나 의 회절요소(예를 들어, DOE(diffractive optical element), HOE(holographic optical element)) 또는 반사요 소(예를 들어, 거울) 중 적어도 하나를 포함할 수 있다. 일 실시예에서, 웨이브가이드는 적어도 하나의 회절요 소 또는 반사요소를 이용하여 광원부로부터 방출된 디스플레이 광을 사용자의 눈으로 유도할 수 있다. 일 실시예에서, 디스플레이(920, 930)는 디스플레이 패널 또는 렌즈(예를 들어, 글라스)를 포함할 수 있다. 예 를 들어, 디스플레이 패널은 유리 또는 플라스틱과 같은 투명한 재질을 포함할 수 있다. 일 실시예에서, 디스플 레이는 투명 소자로 구성될 수 있고, 사용자가 디스플레이를 투과하여, 디스플레이 후면의 실제 공간을 인지할 수도 있다. 디스플레이는 사용자에게 실제 공간의 적어도 일부에 뷰잉 윈도우 등의 가상 객체가 덧붙여진 것으 로 보여지도록 투명 소자의 적어도 일부 영역에 가상 객체를 표시할 수 있다. 일 실시예에서, 지지부(921, 922)는 증강 현실 장치의 각 구성요소에 전기적 신호를 전달하기 위한 인쇄 회로 기판(printed circuit board, PCB)(931-1, 931-2), 오디오 신호를 출력하기 위한 스피커(932-1, 932-2), 또는 전력을 공급하기 위한 배터리(933-1, 933-2)를 포함할 수 있다. 오디오 신호를 출력하기 위한 스피커(932- 1, 932-2)는 전술한 도 8의 스피커에 대응될 수 있다. 예를 들어, 안경 타입의 증강 현실 장치에서, 지지부(921, 922)는 안경 다리부에 배치될 수 있다. 지지부(921, 922)는 증강 현실 장치의 본체부에 결합 하기 위한 힌지부(940-1, 940-2)를 포함할 수 있다. 스피커(932-1, 932-2)는 사용자의 좌측 귀에 오디오 신호를 전달하기 위한 제1 스피커(932-1) 및 사용자의 우측 귀에 오디오 신호를 전달하기 위한 제2 스피커(932-2)를 포 함할 수 있다. 도 9를 참조하면, 증강 현실 장치는 사용자의 음성 및 주변 소리를 수신하기 위한 마이크를 포함할 수 있다. 또한, 증강 현실 장치는 적어도 하나의 카메라(예를 들어, ET 카메라, 외향 카메라, 또는 인식용 카메라(911-1, 911-2))의 정확도를 높이기 위해 적어도 하나의 발광 장치(illumination LED)(94 2)를 포함할 수 있다. 예를 들어, 발광 장치는 ET 카메라로 사용자의 동공을 촬영할 때 정확도를 높 이기 위한 보조 수단으로 사용될 수 있고, 발광 장치는 가시광 파장이 아닌 적외선 파장의 IR LED를 사용 할 수도 있다. 예를 들어, 발광 장치는 인식용 카메라(911-1, 911-2)로 사용자의 제스처 등을 촬영할 때어두운 환경으로 인해 피사체의 검출이 용이하지 않을 때 보조 수단으로 사용될 수 있다. 일 실시예에 따르면, 디스플레이 모듈은 우안에 대응하는 제1 도광판 및 좌안에 대응하는 제2 도광판 를 포함할 수 있고, 제1 도광판 및 제2 도광판를 통해 사용자에게 시각적인 정보를 제공할 수 있다. 일 실시예에 따르면, 디스플레이 모듈은 디스플레이 패널 및 렌즈(예를 들어, 유리렌즈, LC렌즈)를 포함할 수 있다. 디스플레이 패널은 유리 또는 플라스틱과 같은 투명한 재질을 포함할 수 있다. 일 실시예에 따르면, 디스플레이 모듈은 투명 소자로 구성될 수 있고, 사용자가 디스플레이 모듈을 투과하여, 디스플레이 모듈의 후면이자 사용자 전방의 실제 공간을 인지할 수 있다. 디스플레이 모듈(91 4)은 사용자에게 실제 공간의 적어도 일부에 가상 객체가 덧붙여진 것으로 보이도록 투명 소자의 적어도 일부 영역에 가상 객체를 표시할 수 있다. 일 실시예에서, 증강 현실 장치는 외향 카메라를 통해 획득한 실제 공간과 관련된 영상 정보 중 사용 자의 시야각(FoV)으로 판단되는 영역에 대응하는 적어도 일부에 포함되는 외부 객체를 확인할 수 있다. 증강 현 실 장치는 인식한 외부 객체에 기초하여 증강 현실 장치의 포즈 정보를 획득할 수 있다. 증강 현실 장치는 증강 현실 장치의 포즈 정보에 추가에 기초하여 뷰잉 윈도우가 표시될 위치 및 각도를 결정할 수 있다. 외부 객체는 실제 공간에 존재하는 사물을 포함할 수 있다. 다양한 실시예에 따르면, 증강 현실 장치 가 뷰잉 윈도우 등의 가상 객체를 표시하는 표시 영역은 디스플레이 모듈의 일부(예를 들어, 디스플 레이 패널의 적어도 일부)를 포함할 수 있다. 일 실시예에 따르면, 표시 영역은 제1 도광판 및 제2 도광판 의 적어도 일부분에 대응되는 영역일 수 있다. 일 실시예에 따르면, 증강 현실 장치는 외향 카메라를 사용하여, 증강 현실 장치의 전면 방향에 위치한 물리적 대상체와의 거리를 측정할 수도 있다. 외향 카메라는 HR(high resolution) 카메라 및 PV(photo video) 카메라와 같은 고해상도의 카메라를 포함할 수 있다. 본 개시의 일 실시예에 따른 증강 현실 장치는 전술한 구성으로 제한되는 것은 아니며, 다양한 구성요소를 다양한 위치에 다양한 개수로 포함할 수 있다. 도 10은 본 개시의 일 실시예에 따른 영상 촬영 장치의 블록도이다. 도 10을 참조하면, 영상 촬영 장치는 카메라 모듈, 센서, 프로세서, 및 메모리를 포함할 수 있다. 카메라 모듈, 센서, 프로세서, 및 메모리는 각각 전기적 및/또는 물리적으로 서로 연 결될 수 있다. 한편, 영상 촬영 장치의 구성 요소가 도 10에 도시된 바와 같이 한정되는 것은 아니다. 도 10에 도시된 구성 요소보다 많은 구성 요소들에 의해 영상 촬영 장치가 구현될 수도 있고, 도 10에 도시된 구성 요소보다 적은 구성 요소에 의해 영상 촬영 장치가 구현될 수도 있다. 본 개시의 일 실시예에서, 영상 촬영 장치는 카메라, 스마트폰, 태블릿 PC 등 영상의 촬영이 가능한 다양한 장치를 포함할 수 있다. 영상 촬영 장치는 다른 전자 장치와의 통신을 위한 통신 인터페이스나, 자체적인 출력 인터페이스를 더 포함할 수도 있다. 카메라 모듈은 현실 공간을 촬영함으로써 2차원(2D) 또는 3차원(3D) 이미지 데이터를 획득하도록 구성된다. 카메라 모듈은 RGB 카메라, 깊이 추정 기능을 포함하는 RGB-depth 카메라, 동적 비전 센서 카메라(Dynamic Vision Sensor camera), 스테레오 어안 카메라, 그레이 스케일 카메라, 또는 적외선 카메라 등 다양한 종류의 카메라로 구현될 수 있다. 일 실시예에서, 영상 촬영 장치는 복수의 카메라 모듈을 포함할 수 있다. 카메라 모듈은 렌즈 모듈, 이미지 센서, 및 이미지 프로세싱 모듈을 포함할 수 있다. 카메라 모듈은 이 미지 센서(예를 들어, CMOS 또는 CCD)에 의해 현실 장면에 관한 정지 이미지(still image) 또는 동영상(video) 을 획득할 수 있다. 동영상은 카메라 모듈을 통해 현실 영역을 촬영함으로써 실시간으로 획득되는 복수의 이미지 프레임을 포함할 수 있다. 이미지 프로세싱 모듈은 이미지 센서를 통해 획득된 단일 이미지 프레임으로 구성된 정지 이미지 또는 복수의 이미지 프레임으로 구성된 동영상 데이터를 인코딩하여 프로세서에 전달할 수 있다. 본 개시의 일 실시예에 따른 현실 장면 이미지의 촬영은, 영상 촬영 장치에 구비된 카메라 모듈(예를 들어, 이미지 센서 및 렌즈를 포함하는 카메라)를 제어하여 렌즈를 통해 형성된 광학 상(optical image)을 전기 신호로 변환하여 이미지를 획득하는 동작을 포함할 수 있다. 예를 들어, 하나 이상의 프로세서는 영상 촬영 장치에 구비된 카메라 모듈을 제어하여 영상 촬영 장치의 주변을 촬영하여 하나 이상의 프레임을 포함하는 이미지(예를 들어, 촬영 이미지)를 획득할 수 있다. 여기서, 이미지는, 라이브 뷰(live-view) 이미지를 포함할 수 있다. 센서는 현실 공간, 위치, 상황, 또는 사용자 정보를 검출하도록 구성된 센서들을 포함할 수 있다. 본 개시 의 일 실시예에서, 센서는 시선 추적 센서, IMU(Inertial Measurement Unit) 센서, GPS(Global Positioning System) 센서, BLE(Bluetooth Low Energy) 센서, UWB(Ultra Wide Broadband) 센서 또는 그 밖에 다양한 신호를 센싱할 수 있는 센서를 포함할 수 있으나, 이에 제한되는 것은 아니다. 프로세서는 메모리에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행할 수 있다. 프로 세서는 산술, 로직 및 입출력 연산과 이미지 프로세싱을 수행하는 하드웨어 구성 요소로 구성될 수 있다. 도 10에는 프로세서가 한 개인 것처럼 도시 되었으나, 이에 한정되는 것은 아니며 프로세서는 하나 이 상의 복수 개의 요소들로 구성될 수 있다. 프로세서는 CPU(Central Processing Unit), AP(Application Processor), DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공지 능 전용 프로세서일 수 있다. 메모리는 예를 들어, 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미 디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 또는 광 디스크 중 적어도 하나의 타입의 저장매체로 구성될 수 있다. 메모리에는 영상 촬영 장치가 동작하기 위한 기능 및/또는 동작과 관련된 명령어들(instructions)이 저 장될 수 있다. 일 실시예에서, 메모리에는 프로세서가 판독할 수 있는 명령어들, 알고리즘(algorithm), 데이터 구조, 프로그램 코드(program code), 및 애플리케이션 프로그램(application program) 중 적어도 하나 가 저장될 수 있다. 메모리에 저장되는 명령어들, 알고리즘, 데이터 구조, 및 프로그램 코드는 예를 들어, C, C++, 자바(Java), 어셈블러(assembler) 등과 같은 프로그래밍 또는 스크립팅 언어로 구현될 수 있다. 프로세서는 메모리에 저장된 명령어들 또는 프로그램 코드들을 실행할 수 있고, 영상 촬영 장치의 전반적인 동작을 제어할 수 있다. 프로세서는 본 개시의 일 실시예에 따른 동작들을 수행할 수 있다. 예를 들어, 프로세서는 메모리에 저장된 프로그램들을 실행함으로써, 카메라 모듈 및 센서를 전반적 으로 제어할 수 있다. 프로세서는 산술, 로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성요소로 구성될 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), 및 FPGAs(Field Programmable Gate Arrays) 중 적어도 하나로 구성될 수 있으나, 이에 제한되는 것은 아니다. 일 실시예에서, 프로세서는 메모리에 저장된 하나 이상의 명령어들을 실행함으로써, 카메라 모듈을 통해 영상 데이터를 획득하고, 센서를 통해 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 포즈 정 보를 획득하고, 영상 데이터 및 영상 촬영 장치의 포즈 정보를 증강 현실 장치에 전송할 수 있다. 본 개시는 증강 현실 장치가 가상 영상을 출력하는 방법을 제공한다. 방법은 영상 촬영 장치(증강 현실 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카메라 모듈)를 통해 촬영된 영상 데이터를 획 득하는 단계를 포함할 수 있다. 촬영된 영상은 3차원 영상을 포함할 수 있다. 방법은 영상 데이터의 촬영 시점 에서의 해당 영상 촬영 장치(카메라 모듈)의 포즈 정보를 획득하는 단계를 포함할 수 있다. 촬영 시점에서의 영 상 촬영 장치의 포즈 정보는 영상 데이터가 생성된 시점에서의 영상 촬영 장치의 촬영 위치 및 각도 정보를 포 함할 수 있다. 방법은 영상 촬영 장치의 포즈 정보에 기초하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정하는 단계를 포함할 수 있다. 방법은 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력하는 단계를 포함할 수 있다. 본 개시에 따라 뷰잉 윈도우를 통해 표시할 영 상이 실제 촬영된 시점에서의 영상 촬영 장치의 포즈 정보를 고려하여 뷰잉 윈도우의 위치 및 각도를 결정함으 로써, 영상 촬영 장치의 촬영 각도 및 높이와 동일한 각도 및 높이에서 사용자가 영상을 바라보는 것과 같은 효 과를 줄 수 있다. 일 실시예에서, 영상 촬영 장치의 포즈 정보는 영상 데이터의 촬영 시점에서 영상 촬영 장치의 높이 정보 및 영 상 데이터의 촬영 시점에서 영상 촬영 장치의 3차원 공간 각도 정보를 포함할 수 있다. 일 실시예에서, 영상 촬영 장치의 포즈 정보를 획득하는 단계는 영상 촬영 장치로부터 포즈 정보를 수신하는 단 계를 포함할 수 있다. 일 실시예에서, 영상 촬영 장치의 포즈 정보를 획득하는 단계는 영상 데이터 또는 영상 촬영 장치로부터 수신한 센서 정보 중 적어도 하나에 기초하여 포즈 정보를 획득하는 단계를 포함할 수 있다. 일 실시예에서, 영상 촬영 장치의 포즈 정보를 획득하는 단계는, 영상 데이터에 대응되는 뎁스 맵을 획득하는 단계, 뎁스 맵에 기초하여 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 지면으로부터 높이 및 3차원 공간 상의 각도를 획득하는 단계, 및 영상 촬영 장치의 지면으로부터 높이 및 3차원 공간 상의 각도에 기초하여 영상 촬영 장치의 포즈 정보를 획득하는 단계를 포함할 수 있다. 일 실시예에서, 방법은 증강 현실 장치의 포즈 정보를 획득하는 단계, 및 증강 현실 장치의 포즈 정보에 추가로 기초하여 뷰잉 윈도우의 위치 및 각도를 결정하는 단계를 더 포함할 수 있다. 일 실시예에서, 가상 영상이 표시된 뷰잉 윈도우를 출력하는 단계는, 결정된 뷰잉 윈도우의 위치 및 각도에 기 초하여 디스플레이 상 뷰잉 윈도우를 출력하는 단계, 및 출력된 뷰잉 윈도우에 영상 데이터에 대응되는 3차원 가상 영상을 렌더링하는 단계를 포함할 수 있다. 일 실시예에서, 방법은 영상 데이터에 대응되는 영상 촬영 장치의 모션 정보를 획득하는 단계, 및 영상 촬영 장 치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경하는 단계를 더 포함할 수 있다. 일 실시예에서, 영상 촬영 장치의 모션 정보는 시간에 따른 영상 촬영 장치의 포즈의 변화에 대한 정보를 포함 할 수 있다. 일 실시예에서, 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경하는 단계는, 대응 되는 가상 영상의 프레임 마다 수행되는 것일 수 있다. 본 개시는 영상 촬영 장치(증강 현실 장치에 내장된 카메라 모듈 또는 별도의 영상 촬영 장치에 포함된 외부 카 메라 모듈)를 통해 촬영된 영상을 뷰잉 윈도우를 통해 가상 영상으로 출력하는 증강 현실 장치를 제공한다. 증 강 현실 장치는 디스플레이부, 적어도 하나의 명령어를 포함하는 프로그램을 저장하는 메모리, 및 적어도 하나 의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는, 영상 촬영 장치를 통해 촬영된 영상 데이터를 획득 하고, 영상 데이터의 촬영 시점의 영상 촬영 장치의 포즈 정보를 획득하고, 영상 촬영 장치의 포즈 정보에 기초 하여 영상 데이터에 대응되는 가상 영상이 표시되는 뷰잉 윈도우의 위치 및 각도를 결정하고, 결정된 위치 및 각도에 기초하여 가상 영상이 표시된 뷰잉 윈도우를 출력할 수 있다. 일 실시예에서, 영상 촬영 장치의 포즈 정보는 영상 데이터의 촬영 시점에서 영상 촬영 장치의 높이 정보 및 영 상 데이터의 촬영 시점에서 영상 촬영 장치의 3차원 공간 각도 정보를 포함할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 영상 촬영 장치로부터 포즈 정보를 수신할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 영상 데이터 또는 영상 촬영 장치로부터 수신한 센서 정보 중 적어 도 하나에 기초하여 포즈 정보를 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 영상 데이터에 대응되는 뎁스 맵을 획득하고, 뎁스 맵에 기초하여 영상 데이터의 촬영 시점에서의 영상 촬영 장치의 지면으로부터 높이 및 3차원 공간 상의 각도를 획득하고, 영 상 촬영 장치의 지면으로부터 높이 및 3차원 공간 상의 각도에 기초하여 영상 촬영 장치의 포즈 정보를 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 증강 현실 장치의 포즈 정보를 획득하고, 증강 현실 장치의 포즈 정 보에 추가로 기초하여 뷰잉 윈도우의 위치 및 각도를 결정할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 결정된 뷰잉 윈도우의 위치 및 각도에 기초하여 디스플레이 상 뷰잉 윈도우를 출력하고, 출력된 뷰잉 윈도우에 영상 데이터에 대응되는 3차원 가상 영상을 렌더링할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는, 영상 데이터에 대응되는 영상 촬영 장치의 모션 정보를 획득하고, 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경할 수 있다. 일 실시예에서, 영상 촬영 장치의 모션 정보는 시간에 따른 카메라의 포즈의 변화에 대한 정보를 포함할 수 있 다. 적어도 하나의 프로세서는 가상 영상의 프레임 마다 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의위치 및 각도를 변경할 수 있다. 본 개시는 컴퓨터로 읽을 수 있는 저장 매체를 포함하는 컴퓨터 프로그램 제품(computer program product)을 제 공한다. 저장 매체는 개시된 방법의 실시예들 중에서 적어도 하나를 증강 현실 장치에서 실행시키기 위한, 증강 현실 장치에 의해 판독 가능한 명령어들이 저장된 것일 수 있다. 이와 같이, 본 개시의 일 실시예에 따르면, 영상의 촬영 당시의 영상 촬영 장치의 포즈를 기반으로 대응되는 가 상 영상이 표시되는 뷰잉 윈도우의 위치를 조정함으로써, 영상 시청 시 사용자의 현장감을 극대화할 수 있고, 컨텐츠와 실세계 사이의 이질감을 경감하여 가상 영상을 시청하는 사용자의 어지러움을 경감할 수 있다. 본 개시의 다양한 실시예들은 하나 이상의 컴퓨터 프로그램들에 의해 구현 또는 지원될 수 있고, 컴퓨터 프로그 램들은 컴퓨터 판독 가능한 프로그램 코드(code)로부터 형성되고, 컴퓨터로 판독 가능한 매체에 수록될 수 있다. 본 개시에서, “애플리케이션(application)” 및 “프로그램(program)”은 컴퓨터 판독 가능한 프로그램 코드에서의 구현에 적합한 하나 이상의 컴퓨터 프로그램, 소프트웨어 컴포넌트, 명령어 세트, 프로시저 (procedure), 함수, 개체(object), 클래스, 인스턴스, 관련 데이터, 또는 그것의 일부를 나타낼 수 있다. “컴 퓨터 판독 가능한 프로그램 코드”는, 소스 코드, 목적 코드, 및 실행 가능한 코드를 포함하는 다양한 유형의 컴퓨터 코드를 포함할 수 있다. “컴퓨터 판독 가능한 매체”는, ROM(read only memory), RAM(random access memory), 하드 디스크 드라이브(HDD), CD(compact disc), DVD(digital video disc), 또는 다양한 유형의 메모 리와 같이, 컴퓨터에 의해 액세스될 수 있는 다양한 유형의 매체를 포함할 수 있다. 또한, 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장 매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장 매체’는 실재(tangible)하는 장치이고, 일시적인 전기적 또는 다른 신호들을 전송하 는 유선, 무선, 광학적, 또는 다른 통신 링크들을 배제할 수 있다. 한편, 이 ‘비일시적 저장 매체’는 데이터 가 저장 매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하지 않는다. 예를 들어, ‘비 일시적 저장 매체’는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 컴퓨터 판독 가능한 매체는 컴퓨 터에 의해 액세스될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체 를 모두 포함할 수 있다. 컴퓨터 판독 가능한 매체는, 데이터가 영구적으로 저장될 수 있는 매체와 데이터가 저 장되고 나중에 덮어쓰기 될 수 있는 매체, 이를테면 재기입 가능한 광 디스크 또는 소거 가능한 메모리 디바이 스를 포함한다. 개시된 실시예들은 컴퓨터로 읽을 수 있는 저장 매체(computer-readable storage media)에 저장된 명령어를 포 함하는 S/W 프로그램으로 구현될 수 있다. 컴퓨터는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령 어에 따라 개시된 실시예에 따른 동작이 가능한 장치로서, 개시된 실시예들에 따른 전자 장치를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 S/W 프로그램, S/W 프로그램이 저장된 컴퓨터로 읽을 수 있는 저장 매체를 포함할 수 있다. 예를 들어, 컴퓨터 프로그램 제품은 디바이스의 제조사 또는 전자 마켓을 통해 전자적으로 배포되는 S/W 프로그램 형태의 상품(예를 들어, 다운로더블 앱)을 포함할 수 있다. 전 자적 배포를 위하여, S/W 프로그램의 적어도 일부는 저장 매체에 저장되거나, 임시적으로 생성될 수 있다. 이 경우, 저장 매체는 제조사의 서버, 전자 마켓의 서버, 또는 SW 프로그램을 임시적으로 저장하는 중계 서버의 저 장 매체가 될 수 있다. 컴퓨터 프로그램 제품은, 서버 및 디바이스로 구성되는 시스템에서, 서버의 저장매체 또는 디바이스의 저장매체 를 포함할 수 있다. 또는, 서버 또는 디바이스와 통신 연결되는 제 3 장치(예를 들어, 스마트폰)가 존재하는 경 우, 컴퓨터 프로그램 제품은 제 3 장치의 저장매체를 포함할 수 있다. 또는, 컴퓨터 프로그램 제품은 서버로부 터 디바이스 또는 제 3 장치로 전송되거나, 제 3 장치로부터 디바이스로 전송되는 S/W 프로그램 자체를 포함할 수 있다. 이 경우, 서버, 디바이스 및 제 3 장치 중 하나가 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방 법을 수행할 수 있다. 또는, 서버, 디바이스 및 제 3 장치 중 둘 이상이 컴퓨터 프로그램 제품을 실행하여 개시 된 실시예들에 따른 방법을 분산하여 실시할 수 있다. 예를 들면, 서버(예로, 클라우드 서버 또는 인공 지능 서버 등)가 서버에 저장된 컴퓨터 프로그램 제품을 실행 하여, 서버와 통신 연결된 디바이스가 개시된 실시예들에 따른 방법을 수행하도록 제어할 수 있다. 또 다른 예로, 제 3 장치가 컴퓨터 프로그램 제품을 실행하여, 제 3 장치와 통신 연결된 디바이스가 개시된 실 시예에 따른 방법을 수행하도록 제어할 수 있다. 제 3 장치가 컴퓨터 프로그램 제품을 실행하는 경우, 제 3 장 치는 서버로부터 컴퓨터 프로그램 제품을 다운로드하고, 다운로드 된 컴퓨터 프로그램 제품을 실행할 수 있다. 또는, 제 3 장치는 프리로드 된 상태로 제공된 컴퓨터 프로그램 제품을 실행하여 개시된 실시예들에 따른 방법 을 수행할 수도 있다."}
{"patent_id": "10-2024-0099710", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다."}
{"patent_id": "10-2024-0099710", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 증강 현실 장치가 영상 데이터에 대응되는 가상 영상을 표시하기 위한 뷰잉 윈도우의 위치 및 각도를 결정하고 그에 기초하여 가상 영상을 출력하는 방법을 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시예에 따른 증강 현실 장치가 동작하는 방법의 흐름도이다. 도 3은 본 개시의 일 실시예에 따른 영상 촬영 장치를 통해 영상 데이터를 촬영하는 동작을 설명하기 위한 도면 이다. 도 4는 본 개시의 일 실시예에 따른 증강 현실 장치를 착용한 사용자가 시선을 정면으로 고정하였을 때 뷰잉 윈 도우를 통해 가상 영상을 출력하는 동작을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른 증강 현실 장치를 착용한 사용자가 시선을 움직일 때 뷰잉 윈도우를 통해 가상 영상을 출력하는 동작을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시예에 따른 증강 현실 장치가 뷰잉 윈도우의 위치 및 각도를 결정하는 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른 증강 현실 장치가 영상 촬영 장치의 모션 정보에 기초하여 뷰잉 윈도우의 위치 및 각도를 변경하는 단계를 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른 증강 현실 장치의 블록도이다. 도 9는 본 개시의 일 실시예에 따른 글래스 타입의 증강 현실 장치를 도시한 도면이다. 도 10은 본 개시의 일 실시예에 따른 영상 촬영 장치의 블록도이다."}
