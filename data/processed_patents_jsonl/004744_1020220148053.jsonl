{"patent_id": "10-2022-0148053", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0068017", "출원번호": "10-2022-0148053", "발명의 명칭": "턴프리 대화 방법 및 장치", "출원인": "한국전자기술연구원", "발명자": "정민영"}}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "하나 또는 복수의 프로세서;상기 프로세서와 통신가능하게 연결되고, 상기 프로세서에서 실행되는 프로그램 코드를 저장하는 메모리를 포함하고, 상기 프로그램 코드는 상기 프로세서에 의해 실행되고, 사용자의 발화의 일부가 입력될 때마다 중간응답을 제공할 시점인지 판단하고,사용자의 발화에 대응하는 중간응답을 출력하는 턴프리 대화모델을 포함하는, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "청구항 1에 있어서, 상기 중간응답은 단어 또는 문장으로 표현되는 언어적 응답, 소리로 표현되는 청각적 응답, 이모티콘, 표정 또는 제스처로 표현되는 시각적 응답을 포함하며, 사용자의 발화를 인식하고 있음을 나타내는 것인, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "청구항 1에 있어서, 상기 턴프리 대화모델은 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 생성하고, 상기 조각에 타임스탬프를 부여하고,상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하며, 상기 조각그룹이 학습데이터이고 상기 타임스탬프마다 생성된 조각그룹에 대한 제2 화자의 발화가 라벨데이터인 학습데이터세트를 학습하여 생성되고, 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타임스탬프가 부여되며 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹이 생성되어 상기 턴프리 대화모델에 입력되면, 상기 턴프리 대화모델은 중간응답을 제공할 시점인지 판단하고 중간응답을 출력하는, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "청구항 3에 있어서, 상기 학습데이터는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화를 제1 화자의 발화로 지정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정하는, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "청구항 1에 있어서, 상기 프로그램 코드는 상기 프로세서에 의해 실행되고, 상기 사용자의 발화의 전부가 입력되면 상기 발화의 내용을 분석하여 실질적인응답을 제공하는 턴기반 대화모델을 더 포함하는, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2024-0068017-3-청구항 5에 있어서, 상기 프로세서는 상기 턴프리 대화모델과 상기 턴기반 대화모델은 서로 독립적으로 실행하고, 상기 턴프리 대화모델은 사용자의발화 중간에 중간응답을 제공하고, 상기 턴기반 대화모델은 상기 사용자의 발화가 완료되면 실질적인 응답을 제공하는, 턴프리 대화 장치."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "사용자의 발화를 입력받는 단계; 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타임스탬프가 부여되며 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하는 단계; 상기 타임스탬프마다 생성된 조각그룹을 턴프리 대화모델에 입력하고, 중간응답을 획득하는 단계; 상기 중간응답을 사용자에게 제공하는 단계를 포함하는, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "청구항 7에 있어서, 상기 중간응답은 단어 또는 문장으로 표현되는 언어적 응답, 소리로 표현되는 청각적 응답, 이모티콘, 표정 또는 제스처로 표현되는 시각적 응답을 포함하며, 사용자의 발화를 인식하고 있음을 나타내는 것인, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "청구항 7에 있어서, 상기 턴프리 대화모델을 생성하는 단계를 더 포함하며, 상기 턴프리 대화모델을 생성하는 단계는 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 생성하고, 상기 조각에 타임스탬프를 부여하고,상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하며, 상기 조각그룹이 학습데이터이고 상기 타임스탬프마다 생성된 조각그룹에 대한 제2 화자의 발화가 라벨데이터인 학습데이터세트를 생성하는 단계; 및 상기 학습데이터세트를 이용하여, 타임스탬프마다 생성된 조각그룹이 입력되면, 중간응답을 제공할 시점인지 판단하고 중간응답을 출력하도록 턴프리 대화모델을 학습시키는 단계를 포함하는, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "청구항 9에 있어서, 상기 학습데이터세트를 생성하는 단계는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화를 제1 화자의 발화로 지정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정하여 학습데이터와 라벨데이터를 구분하는 과정을 더 수행하고, 상기 제1 화자의 발화를 조각그룹으로 생성하여입력데이터로 정하고 상기 제2 화자의 발화를 라벨데이터로 정하는, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "청구항 7에 있어서, 상기 사용자의 발화의 전부가 입력되면 상기 발화의 내용을 분석하여 실질적인 응답을 제공하는 턴기반 대화모델을 이용하여, 상기 사용자의 발화가 완료되면 상기 발화를 상기 턴기반 대화모델에 입력하여 실질적 응답을획득하는 단계; 및 공개특허 10-2024-0068017-4-상기 실질적 응답을 사용자에게 제공하는 단계를 더 포함하는, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "청구항 11에 있어서, 상기 중간응답을 획득하는 단계와 상기 실질적 응답을 획득하는 단계는 서로 독립적으로 실행되어, 사용자의 발화 중간에는 상기 중간응답을 제공하는 단계가 수행되고, 사용자의 발화가 완료되면 실질적 응답을 제공하는 단계가 수행되는, 턴프리 대화 방법."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시에 따르면, 하나 또는 복수의 프로세서, 프로세서와 통신가능하게 연결되고, 프로세서에서 실행되는 프로 그램 코드를 저장하는 메모리를 포함하는 턴프리 대화 장치를 제공하되, 프로그램 코드는 프로세서에 의해 실행 되고, 사용자의 발화의 일부가 입력될 때마다 중간응답을 제공할 시점인지 판단하고, 사용자의 발화에 대응하는 중간응답을 출력하도록 작성되어, 사용자가 문장을 말하는 중간에 언어적, 청각적, 시각적 반응을 사용자에게 제 공하므로, 사용자가 턴프리 대화 장치와 대화를 지속하고 싶은 마음을 갖게 할 수 있고, 사용자가 사람과 대화하 는 느낌을 줄 수 있으며, 사용자의 거부감을 감소시킬 수 있다."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 턴프리 대화 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능을 이용하여 사람과 대화를 제공하는 대화 서비스가 제공되고 있다. 인공지능 장치는 사람과의 대 화에 기초하여 정해진 기능을 실행하거나, 사람이 질문한 내용에 대한 답변을 제공할 수 있다. 현재 서비스되고 있는 인공지능을 이용한 대화서비스는 사람이 문장을 말하는 턴이 종료되면 인공지능은 사람이 말한 문장을 인 식하고 답변을 제공하는 턴을 수행하는 방식이다. 이러한 턴 방식은 사람과 사람 사이의 실제의 대화와는 거리 가 있다. 선행기술문헌 특허문헌 (특허문헌 0001) KR 10-2020-0000604 A"}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 사람이 문장을 말하는 중간에 언어적, 청각적, 시각적 반응을 사용자에게 제공할 수 있는 턴프리 대 화 방법 및 장치를 제공하기 위한 것이다. 본 성과물은 1. 한국전자기술연구원의 기본연구사업의 미래전략기술개발사업 중, '멀티모달 상호작용 및 지식기 반 토론이 가능한 인공지능 복합대화 시스템 기술 연구 과제(과제번호: 401C2906, 기여율: 1/2)', 및 2. 과학기 술정보통신부의 사람중심인공지능핵심원천기술개발사업 중, '(1세부)인간과 교감하는 멀티모달 인터랙션 인공지 능 기술 과제(과제고유번호: 1711160496, 기여율: 1/2)' 의 지원을 받아 수행된 결과이다."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시에 따른 턴프리 대화 장치는, 하나 또는 복수의 프로세서, 상기 프로세서와 통신가능하게 연결되고, 상 기 프로세서에서 실행되는 프로그램 코드를 저장하는 메모리를 포함하고, 상기 프로그램 코드는 상기 프로세서 에 의해 실행되고, 사용자의 발화의 일부가 입력될 때마다 중간응답을 제공할 시점인지 판단하고, 사용자의 발 화에 대응하는 중간응답을 출력하는 턴프리 대화모델을 포함할 수 있다. 일 구현예(one embodiment)에 따르면 상기 중간응답은 단어 또는 문장으로 표현되는 언어적 응답, 소리로 표현 되는 청각적 응답, 이모티콘, 표정 또는 제스처로 표현되는 시각적 응답을 포함하며, 사용자의 발화를 인식하고 있음을 나타내는 것일 수 있다. 일 구현예에 따르면, 상기 턴프리 대화모델은 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 생 성하고, 상기 조각에 타임스탬프를 부여하고, 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하 여 조각그룹을 생성하며, 상기 조각그룹이 학습데이터이고 상기 타임스탬프마다 생성된 조각그룹에 대한 제2 화 자의 발화가 라벨데이터인 학습데이터세트를 학습하여 생성되고, 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타임스탬프가 부여되며 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결 하여 조각그룹이 생성되어 상기 턴프리 대화모델에 입력되면, 상기 턴프리 대화모델은 중간응답을 제공할 시점 인지 판단하고 중간응답을 출력할 수 있다. 일 구현예에 따르면, 상기 학습데이터는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화를 제1 화자의 발화로 지정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정할 수 있다. 일 구현예에 따르면, 상기 프로그램 코드는 상기 프로세서에 의해 실행되고, 상기 사용자의 발화의 전부가 입력 되면 상기 발화의 내용을 분석하여 실질적인 응답을 제공하는 턴기반 대화모델을 더 포함할 수 있다. 일 구현예에 따르면, 상기 프로세서는 상기 턴프리 대화모델과 상기 턴기반 대화모델은 서로 독립적으로 실행하 고, 상기 턴프리 대화모델은 사용자의 발화 중간에 중간응답을 제공하고, 상기 턴기반 대화모델은 상기 사용자 의 발화가 완료되면 실질적인 응답을 제공할 수 있다. 본 개시에 따른 턴프리 대화 방법은, 사용자의 발화를 입력받는 단계, 사용자의 발화가 입력되는 순서대로 정해 진 크기의 조각으로 생성되고 타임스탬프가 부여되며 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하는 단계, 상기 타임스탬프마다 생성된 조각그룹을 턴프리 대화모델에 입력하고, 중 간응답을 획득하는 단계, 상기 중간응답을 사용자에게 제공하는 단계를 포함할 수 있다. 일 구현예에 따른 턴프리 대화 방법은, 상기 턴프리 대화모델을 생성하는 단계를 더 포함하며, 상기 턴프리 대 화모델을 생성하는 단계는 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 생성하고, 상기 조각 에 타임스탬프를 부여하고, 상기 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성 하며, 상기 조각그룹이 학습데이터이고 상기 타임스탬프마다 생성된 조각그룹에 대한 제2 화자의 발화가 라벨데 이터인 학습데이터세트를 생성하는 단계, 및 상기 학습데이터세트를 이용하여, 타임스탬프마다 생성된 조각그룹 이 입력되면, 중간응답을 제공할 시점인지 판단하고 중간응답을 출력하도록 턴프리 대화모델을 학습시키는 단계 를 포함할 수 있다. 일 구현예에 따르면, 상기 학습데이터세트를 생성하는 단계는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화를 제1 화자의 발화로 지 정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정하여 학습데이터와 라벨데이터를 구분하는 과정을 더 수행하고, 상기 제1 화자의 발화를 조각그룹으로 생성하여 입력데이터로 정하고 상기 제2 화자의 발화를 라 벨데이터로 정할 수 있다. 본 개시에 따른 턴프리 대화 방법은, 상기 사용자의 발화의 전부가 입력되면 상기 발화의 내용을 분석하여 실질 적인 응답을 제공하는 턴기반 대화모델을 이용하여, 상기 사용자의 발화가 완료되면 상기 발화를 상기 턴기반 대화모델에 입력하여 실질적 응답을 획득하는 단계, 및 상기 실질적 응답을 사용자에게 제공하는 단계를 더 포 함할 수 있다. 일 구현예에 따르면, 상기 중간응답을 획득하는 단계와 상기 실질적 응답을 획득하는 단계는 서로 독립적으로 실행되어, 사용자의 발화 중간에는 상기 중간응답을 제공하는 단계가 수행되고, 사용자의 발화가 완료되면 실질 적 응답을 제공하는 단계가 수행될 수 있다. 본 개시의 특징 및 이점들은 첨부도면에 의거한 다음의 상세한 설명으로 더욱 명백해질 것이다. 이에 앞서 본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이고 사전적인 의미로 해석되어서는 아니 되며, 발명자가 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다 는 원칙에 입각하여 본 발명의 기술적 사상에 부합되는 의미와 개념으로 해석되어야만 한다."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일 구현예에 따르면, 턴프리 대화 장치는 사용자가 문장을 말하는 중간에 언어적, 청각적, 시각적 반 응을 사용자에게 제공하므로, 사용자가 턴프리 대화 장치와 대화를 지속하고 싶은 마음을 갖게 할 수 있다. 본 개시의 일 구현예에 따르면, 턴프리 대화 장치는 언어적, 청각적, 시각적 반응을 사용자에게 제공함으로써 사용자가 사람과 대화하는 느낌을 줄 수 있고 사용자의 거부감을 감소시킬 수 있다."}
{"patent_id": "10-2022-0148053", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 목적, 장점, 및 특징들은 첨부된 도면들과 연관되는 이하의 상세한 설명과 바람직한 구현예들로부터 더욱 명백해질 것이나, 본 개시가 반드시 이에 한정되는 것은 아니다. 또한, 본 개시를 설명함에 있어서, 관련 된 공지 기술에 대한 구체적인 설명이 본 개시의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우 그 상세한 설명은 생략한다. 도면의 구성요소들에 참조부호를 부여함에 있어서, 동일한 구성 요소들은 비록 다른 도면상에 표시되더라도 가 능한 한 동일한 참조부호가 부여되고, 유사한 구성요소에 대해서는 유사한 참조부호가 부여됨에 유의하여야 한 다. 본 개시의 일 구현예를 설명하기 위해 사용한 용어는 본 개시를 한정하려는 의도가 아니다. 단수의 표현은 문맥 상 달리 명시하지 않는 한 복수의 표현을 포함한다는 것을 알아야 한다. 본 문서에서, &quot;가진다,&quot; &quot;가질 수 있다,&quot; &quot;포함한다,&quot; 또는 &quot;포함할 수 있다&quot; 등의 표현은 해당 특징(예: 수치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가 적인 특징의 존재를 배제하지 않는다. &quot;일(one)&quot;, &quot;다른(other)&quot;, &quot;또다른(another)&quot;, &quot;제1(first)&quot;, &quot;제2(second)&quot; 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위해 사용되는 것으로, 구성요소가 상기 용어들에 의해 제한되는 것은 아니다. 본 문서에 기재된 구현예 및 첨부된 도면은 본 개시를 특정한 실시 형태에 대해 한정하려는 것이 아니다. 본 개 시는 구현예의 다양한 변경(modifications), 균등물(equivalents), 및/또는 대체물(alternatives)을 포함하는 것으로 이해되어야 한다. 도 1은 일 구현예에 따른 턴프리 대화 장치와 사용자의 대화를 나타내는 도면이다. 일 구현예에 따른 턴프리 대화 장치는 사용자의 발화에 응답을 제공할 수 있다. 사용자가 턴프리 대화 장치에 발화를 입력하면 턴프리 대화 장치는 중간응답 또는 실질적 응답을 제공할 수 있다. 중간 응답은 사용자가 말하는 중간에 턴프리 대화 장치가 응답하는 것을 말한다. 중간응답은 언어적, 청각적, 시각적 방법으로 표현될 수 있고, 비언어적 방법으로도 표현될 수 있다. 실질적 응답은 사용자의 발화가 완료되면 턴프리 대화 장치가 사용자의 발화에 대응하는 내용을 포함하는 응답을 하는 것이다. 턴프리 대화 장치는 네트워크를 통해 원격단말과 연결될 수 있다. 사용자는 원격단말을 통해 턴프리 대화 장치와 대화할 수 있다. 원격단말은 스마트폰, 인공지능 비서 단말, 태블릿 PC, 노트북 PC, 공기청정기, 스마트 TV, 자동차 등에 포함될 수 있다. 원격단말은 마이크를 통해 사용자의 발화를 수신하여 턴프리 대화 장치로 제공하고, 턴프리 대화 장치로부터 사용자에게 제공할 응답을 수신하 여 스피커로 출력할 수 있다. 중간응답은 단어 또는 문장으로 표현되는 언어적 응답, 소리로 표현되는 청각적 응답, 이모티콘, 표정 또는 제 스처로 표현되는 시각적 응답을 포함할 수 있다. 중간응답은 사용자의 발화를 인식하고 있음을 나타내는 것 이다. 중간응답은 사용자가 발화를 완성하기 전에 제공되는 것으로서, 턴프리 대화 장치가 사용자의 발화 를 인식하고 있다는 점을 사용자에게 나타내기 위해 제공될 수 있다. 중간응답이 제공됨으로써, 사용자(1 0)에게 사람과 대화하는 느낌을 줄 수 있고 사용자의 거부감을 감소시킬 수 있으며 대화를 지속하고 싶은 마음을 갖게 할 수 있다. 도 2는 일 구현예에 따른 턴프리 대화 장치를 나타내는 도면이다. 일 구현예에 따른 턴프리 대화 장치는 정보처리기능을 수행하는 컴퓨터 장치일 수 있다. 턴프리 대화 장치 는 PC, 서버컴퓨터, 태블릿PC 등을 포함할 수 있다. 턴프리 대화 장치는, 하나 또는 복수의 프로세서 , 프로세서와 통신가능하게 연결되고, 프로세서에서 실행되는 프로그램 코드를 저장하는 메모 리를 포함하고, 프로그램 코드는 프로세서에 의해 실행되고, 사용자의 발화의 일부가 입력될 때마다 중간응답을 제공할 시점인지 판단하고, 사용자의 발화에 대응하는 중간응답을 출력하는 턴프리 대화 모델을 포함할 수 있다. 턴프리 대화 장치는 네트워크와 연결되어 데이터를 송수신하는 통신부, 또는 사용자의 발화를 수신하고 사용자에게 응답을 제공하는 입출력부를 더 포함할 수 있다. 프로 세서, 메모리, 통신부, 입출력부는 서로 통신가능하게 연결될 수 있다. 프로세서는 턴프리 대화 방법을 수행하도록 작성된 프로그램 코드를 실행할 수 있다. 프로세서는 CPU, GPU, 그 밖의 정보처리소자를 포함할 수 있다. 메모리는 프로세서에서 실행되는 프로그램 코드, 그 밖에 턴프리 대화 방법을 수행하기 위하여 필 요한 데이터를 저장할 수 있다. 메모리는 하드디스크, 메모리칩, 데이터베이스, 클라우드 저장소 등을 포함할 수 있다. 통신부는 Wi-fi, bluetooth, zigbee 등의 근거리 무선통신, LTE, 5G, 6G 등의 이동통신, ethernet, LAN, IP4, IP6 등의 통신방식 등, 알려진 통신방식을 이용할 수 있다. 통신부는 원격단말과 유선 또는 무선 네트워크를 통해 연결될 수 있다. 입출력부는 마이크, 스피커, 디스플레이를 포함할 수 있다. 입출력부는 마이크를 통해 사용자의 발 화를 입력받을 수 있다. 입출력부는 스피커를 통해 응답을 청각적으로 출력할 수 있다. 입출력부는 디 스플레이를 통해 응답을 시각적으로 출력할 수 있다. 프로세서는 메모리에 저장되어 있는 프로그램 코드를 읽어들여 실행하여 턴프리 대화 방법을 수행 할 수 있다. 본 문서에서 '턴프리(Turn-free)'는 사용자의 발화가 끝나기 전에도 응답을 제공하는 방식을 말한 다. '턴기반(Turn-based)'은 사용자의 발화가 끝나면 응답을 제공하는 방식을 말한다. 프로그램 코드는 턴프리 대화모델을 포함할 수 있다. 턴프리 대화모델은 학습데이터세트를 학습 하여 생성된 인공지능 모델이다. 턴프리 대화모델은 사용자의 발화의 일부를 순서대로 입력받는 즉시 중간응답을 제공할 시점인지 판단하고 중간응답을 출력할 수 있다. 프로세서는 턴프리 대화모델이 출 력하는 중간응답을 입출력부 또는 원격단말을 통해 사용자에게 출력할 수 있다. 턴프리 대화모델은 RNN, LSTM, Seq2Seq, Transformer 기반 사전학습모델 BERT, GPT, T5 등의 모델로 구현 될 수 있다. 턴프리 대화모델은 본 문서에서 기재한 모델 이외의 다른 구조의 모델로도 구현될 수 있다. 턴프리 대화모델은 이후에 설명하는 학습데이터세트를 통해 학습함으로써, 중간응답을 제공할 시점인지 여 부와, 적절한 중간응답이 무엇인지를 학습할 수 있다. 프로그램 코드는 프로세서에 의해 실행되고, 사용자의 발화의 전부가 입력되면 발화의 내용을 분 석하여 실질적인 응답을 제공하는 턴기반 대화모델을 더 포함할 수 있다. 턴기반 대화모델은 알려진 방법을 이용하여 생성된 인공지능 모델이다. 턴기반 대화모델은 사용자의 발화를 분석하고 사용자(1 0)의 발화에 대응하는 내용의 실질적 응답을 출력할 수 있다. 프로세서는 턴기반 대화모델이 출력하는 실질적 응답을 입출력부 또는 원격단말을 통해 사용자에게 출력할 수 있다. 프로세서는 턴프리 대화모델과 상기 턴기반 대화모델은 서로 독립적으로 실행하고, 턴프리 대화 모델은 사용자의 발화 중간에 중간응답을 제공하고, 턴기반 대화모델은 사용자의 발화가 완 료되면 실질적인 응답을 제공할 수 있다. 턴프리 대화 장치는 사용자가 발화를 하는 도중에 중간응답을 제공할 수 있고, 사용자가 발화를 완료하면 실질적 응답을 제공할 수 있다. 따라서 사용자는 턴프리 대 화 장치로부터 사람과 대화를 하는 것과 같은 자연스러움을 얻으면서 턴프리 대화 장치로부터 정보나 기능을 제공받을 수 있다. 도 3은 일 구현예에 따른 턴프리 대화모델의 학습데이터세트를 설명하는 도면이다. 턴프리 대화모델은 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 생성하고, 조각에 타임스탬프를 부여하고, 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하며, 조 각그룹이 학습데이터이고 타임스탬프마다 생성된 조각그룹에 대한 제2 화자의 발화가 라벨데이터인 학습데 이터세트를 학습하여 생성될 수 있다. 턴프리 대화모델은 인공지능 모델에 학습데이터세트를 학습시켜 생성될 수 있다. 학습데이터세트는 학습데 이터와, 학습데이터에 대한 라벨데이터를 포함할 수 있다. 학습데이터세트는 사람과 사람 사이의 대화 데이터 를 통해 생성할 수 있다. 사람과 사람 사이의 대화 데이터는 오픈소스에 공개된 데이터나, 드라마, 영화, TV쇼와 같은 방송프로그램 등으로부터 획득할 수 있다. 대화 데이터는 소리 형태(Sound Form, SF)로 존재할 수 있다. 소리 형태의 대화 데이터 중에서 발화 를 추출하여 학습데이터세트를 생성할 수 있다. 소리 형태의 대화 데이터는 STT(Speech To Text) 엔진을 이용하여 텍스트 형태(Text Form, TF)로 변환할 수 있다. 소리 형태의 대화 데이터로부터 텍스트 형태의 대화 데이터를 생성하는 과정에서 화자를 분리하고 문 장을 분리할 수 있다. 화자의 분리는 Speaker Seperation 기술을 이용하여 수행될 수 있다. 혼합된 상태의 복수 의 화자의 발화는 Speaker Seperation 기술을 이용하여 각각의 화자의 발화로 분리되고, 하나의 화자가 발화하 는 여러개의 문장은 음성 활동 감지(Voice Activity Detection, VAD)를 이용하여 하나의 문장씩 분리될 수 있다. 예를 들어, 2명의 화자가 대화를 하는 경우, Speaker Seperation 기술을 이용하여 제1 화자의 발화와 제2 화자의 발화를 구별하고, 음성 활동 감지를 이용하여 제1 화자의 여러 문장을 분리하고, 제2 화자 의 여러 문장을 분리할 수 있다. 텍스트 형태의 대화 데이터를 생성함에 있어서, 정해진 크기의 조각으로 발화를 분리할 수 있다. 조각은 사용자의 발화의 일부이다. 조각의 크기는 시간을 기준으로 결정될 수 있다. 예를 들어, 조각은 1초로 결정 될 수 있고, 총 발화의 길이가 3초인 경우, 3개의 조각으로 분리될 수 있다. 또는, 조각의 크기는 음절 또는 어 절을 기준으로 결정될 수 있다. 사용자의 발화에서 어절의 개수를 기준으로 조각이 분리될 수 있다. 사용자의 발화를 조각으로 분리함에 있어서, 각 조각이 발화된 타임스탬프(time stamp)를 부여할 수 있다. 타임스탬프는 각 조각의 순서를 나타낼 수 있다. 타임스탬프는 조각 사이의 간격을 나타낼 수도 있다. 타임스탬 프는 발화에서 첫 조각을 0초로 하고, 이후의 조각까지의 실제 시간을 나타낼 수도 있다. 학습데이터세트는 조각으로 분리된 사용자의 발화와 타임스탬프에 기초하여 생성될 수 있다. 학습데이터세 트는 학습데이터와 라벨데이터를 포함하며, 타임스탬프마다 하나씩 생성될 수 있다. 학습데이터는 타임스탬프에 해당하는 조각과 이전 타임스탬프의 조각을 누적하여 포함할 수 있다. 예를 들어, 제1 화자의 발화는 3개 의 조각으로 분리되고, 제1 타임스탬프, 제2 타임스탬프, 제3 타임스탬프가 부여될 수 있다. 제1 타임스탬프 (t1)에서 학습데이터가 되는 조각은 '오늘' 이고, 라벨데이터는 제2 화자의 발화인 '응' 이 될 수 있다. 제2 타임스탬프에서 학습데이터가 되는 조각그룹은 제1 타임스탬프의 '오늘'과 제2 타임스탬프의 '날씨'를 포함 하고, 라벨데이터는 제2 화자의 발화가 없으므로 '빈공간' 이 될 수 있다. 제3 타임스탬프에서 학습데이터 가 되는 조각그룹은 제1 타임스탬프의 '오늘', 제2 타임스탬프의 '날씨', 제3 타임스탬프의 '어때?'를 포함하고, 라벨데이터는 제2 화자의 발화가 없으므로 '빈공간' 이 될 수 있다. 라벨데이터가 빈공간이라는 것은 라벨데이터가 존재하지 않는다는 의미이다. '빈공간'은 도면에서 점선 네모로 표시하였다. '빈공간'은 룩 업테이블에 대응하는 코드인 'C3빈공간'으로 지정되어 저장될 수 있다. 프로세서는 턴프리 대화모델이 'C3빈공간'을 출력하면 중간응답을 제공하지 않는 것으로 판단할 수 있다. 라벨데이터인 제2 화자의 발화는 중간응답에 해당한다. 제2 화자의 발화는 단어 또는 문장으로 표현 되는 언어적 응답을 포함할 수 있다. 예를 들어, 제2 화자의 발화는 '응', '아니', '그래서', '그리고', '계속해' 등과 같은 언어적 응답을 포함할 수 있다. 그리고, 제2 화자의 발화는 소리로 표현되는 청각적 응답일 수 있다. 예를 들어, 제2 화자의 발화는 한숨, 놀라는 음성, '음...', '어...', 등과 같은 비언어 적이고 청각적인 표현을 포함할 수 있다. 제2 화자의 발화를 대신하여 이모티콘, 표정 또는 제스처와 같은 시각적 응답이 부가될 수 있다. 예를 들 어, 제2 화자의 발화는 놀라는 표정, 행복한 표정, 슬픈 표정, 화난 표정 등을 포함할 수 있다. 시각적 응 답은 입출력부의 디스플레이를 통해 사용자에게 제공될 수 있다. 학습데이터세트를 생성함에 있어서, 언어적 응답과 청각적 응답은 대화 데이터에서 음성 활동 감지 방법으 로 획득할 수 있고, 시각적 응답은 별도의 데이터를 이용하여 부여할 수 있다. 라벨데이터에서 언어적 응답은 텍스트 형태로 저장될 수 있다. 청각적 응답과 시각적 응답은 룩업테이블 방식으로 저장될 수 있다. 룩업테이블 은 메모리에 저장될 수 있다. '아!'이라는 청각적 응답과 이에 대응하는 'C1놀람'이 룩업테이블에 저장되고, '음...'이라는 청각적 응답과 이에 대응하는 'C1음'이 룩업테이블에 저장되고, '놀라는 표정'이라는시각적 응답과 이에 대응하는 'C2놀람'이 룩업테이블에 저장되고, '화나는 표정'이라는 시각적 응답과 이에 대 응하는 'C2화남'이 룩업테이블에 저장될 수 있다. 라벨데이터에는 'C1한숨', 'C1음', 'C2놀람', 'C2화남' 등의 코드가 이용되며, 학습된 턴프리 대화모델은 코드를 출력하도록 학습될 수 있다. 프로세서는 턴프리 대화모델이 텍스트를 출력하면 언어적 응답으로서 소리 또는 텍스트로 응답을 제공하고, 턴프리 대화모델 이 코드를 출력하면, 룩업테이블을 참조하여 대응하는 청각적 응답 또는 시각적 응답을 제공할 수 있다. 이러한 과정을 통해 생성된 학습데이터세트를 학습하면, 턴프리 대화모델은 사용자의 발화 도중에 중 간응답을 언제 제공하여야 하는지 판단할 수 있고, 어떤 중간응답을 제공하여야 하는지 학습할 수 있다. 화자 분리를 수행함에 있어서, 어느 화자의 발화를 학습데이터에 배치하고, 어느 화자의 발화를 라벨데이터에 배치하여야 하는지 구분할 필요가 있다. 일 구현예에서, 학습데이터는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화를 제1 화 자의 발화로 지정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정할 수 있다. 발화가 겹치 는 것은 어느 화자가 발화하는 동안에 다른 화자가 발화하여 동일한 시간에 2개 이상의 문장이 존재하는 상태이 다. 달리 설명하면, 발화가 겹치는 것은 하나의 발화에 다른 발화가 오버랩되는 상태이다. 도 3을 참조하면, 제1 타임스탬프에서 서로 다른 사람의 발화('오늘' 과 '응')가 겹친다. 이러한 경우, 문장의 길이가 긴 화자를 제1 화자로 결정하고 긴 문장을 학습데이터에 배치할 수 있다. 본 개시에서 턴프리 대화 모델이 출력하는 중간응답은 사용자의 발화 도중에 적절한 반응을 제공하기 위한 것이므로, 발화가 겹치는 경우 길이가 짧은 발화를 제2 화자의 발화로 결정하고 라벨데이터로 배치하는 것이다. 이러한 과정을 통해 생성된 학습데이터세트로 학습된 턴프리 대화모델은, 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타임스탬프가 부여되며 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹이 생성되어 턴프리 대화모델에 입력되면, 턴프리 대화모델은 중간응답을 제공할 시점인지 판단하고 중간응답을 출력할 수 있다. 프로세서는 턴프리 대화모델을 이용하기 위하여, 사용자의 발화를 조각으로 분리하고 타임스탬프 에 따라 조각그룹을 생성하여 조각그룹을 턴프리 대화모델에 입력할 수 있다. 사용자의 발화는 소리 형태로 입력되거나, 텍스트 형태로 입력될 수 있다. 사용자는 턴프리 대화 장치 또는 원격단말에 음성으로 발화를 입력할 수 있다. 또는, 사용자는 턴프리 대화 장치 또는 원격단말에 키보드를 이 용하여 텍스트를 입력하는 방식으로 발화를 입력할 수 있다. 프로세서는 사용자의 발화가 입력되는 순 서대로 정해진 크기의 조각으로 분리하고 타임스탬프를 부여할 수 있다. 프로세서가 제1 타임스탬프에 해당 하는 조각을 턴프리 대화모델에 입력하면, 턴프리 대화모델은 학습된 중간응답을 출력하거나, 학습된 '빈공간'을 출력할 수 있다. 프로세서가 제2 타임스탬프에 해당하는 조각그룹(제1 타임스탬프의 조각 및 제 2 타임스탬프의 조각)을 턴프리 대화모델에 입력하면, 턴프리 대화모델은 학습된 중간응답을 출력하 거나, 학습된 '빈공간'을 출력할 수 있다. 프로세서는 턴프리 대화모델에서 출력된 중간응답을 입출력 부 또는 원격단말을 통해 사용자에게 제공할 수 있다. 도 4는 일 구현예에 따른 턴프리 대화 방법을 나타내는 도면이다. 도 5는 일 구현예에 따른 사용자의 발화와 응 답을 설명하는 도면이다. 도 4 및 도 5를 함께 참조한다. 턴프리 대화 방법은 사용자의 발화를 입력받는 단계(S10), 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타임스탬프가 부여되며 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하는 단계(S20), 타임스탬프마다 생성된 조각그룹을 턴프리 대화모델에 입력하고, 중간응 답을 획득하는 단계(S30), 중간응답을 사용자에게 제공하는 단계(S40)를 포함할 수 있다. 사용자의 발화를 입력받는 단계(S10)는 턴프리 대화 장치가 사용자의 발화를 입력받는 과정이다. 턴프리 대화 장치의 입출력부는 마이크 또는 키보드를 이용하여 사용자의 발화를 소리 또는 텍스트 로 입력받을 수 있다. 사용자의 발화를 입력받는 단계(S10)는 발화를 실시간으로 순서대로 입력받을 수 있 다. 조각그룹을 생성하는 단계(S20)는 사용자의 발화가 입력되는 순서대로 정해진 크기의 조각으로 생성되고 타 임스탬프가 부여되며 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성할 수 있다. 프로세서는 입출력부를 통해 입력받은 사용자의 발화를 입력되는 순서대로 정해진 크기의 조각으로 분리하고 타임스탬프를 부여할 수 있다. 프로세서는 타임스탬프에 대응하는 조각과, 이전의 타임스탬프의 조각들을 누적하여 포함하는 조각그룹을 생성할 수 있다. 프로세서는 사용자의 발화가 입력되면 순서대로 복수의 조각그룹을 생성할 수 있다. 중간응답을 획득하는 단계(S30)는 프로세서가 조각그룹을 턴프리 대화모델에 입력하고 턴프리 대화모 델이 중간응답 또는 '빈공간'을 출력하는 과정이다. 턴프리 대화모델은 입력받은 조각그룹에 대응하 는 중간응답 또는 '빈공간'을 출력할 수 있다. 중간응답을 사용자에게 제공하는 단계(S40)에서 프로세서는 턴프리 대화모델에서 출력된 중간응답 을 입출력부 또는 원격단말을 통해 사용자에게 제공할 수 있다. 턴프리 대화모델에서 '빈공간'을 출력하는 경우 해당 타임스탬프에서는 중간응답을 제공하지 않는 것으로 판단한 것이므로, 프로세서 는 사용자에게 아무런 응답을 제공하지 않을 수 있다. 일 구현예에 따른 턴프리 대화 방법은 상기 턴프리 대화모델을 생성하는 단계(S50)를 더 포함할 수 있다. 턴프리 대화모델을 생성하는 단계(S50)는 제1 화자의 발화를 입력되는 순서대로 정해진 크기의 조각 으로 생성하고, 조각에 타임스탬프를 부여하고, 타임스탬프마다 이전 타임스탬프의 조각을 누적으로 연결하여 조각그룹을 생성하며, 조각그룹이 학습데이터이고 타임스탬프마다 생성된 조각그룹에 대한 제2 화자의 발 화가 라벨데이터인 학습데이터세트를 생성하는 단계(S51), 및 학습데이터세트를 이용하여, 타임스탬프마다 생성 된 조각그룹이 입력되면, 중간응답을 제공할 시점인지 판단하고 중간응답을 출력하도록 턴프리 대화모델을 학습시키는 단계(S52)를 포함할 수 있다. 학습데이터세트를 생성하는 단계(S51)는 수집된 대화 데이터에서 두 사람의 발화가 겹치는 경우, 겹치는 발화를 포함하는 문장의 길이를 비교하고 문장의 길이가 긴 화자의 발화 를 제1 화자의 발화로 지정하고, 길이가 짧은 화자의 발화를 제2 화자의 발화로 지정하여 학습데이터 와 라벨데이터를 구분하는 과정을 더 수행하고, 상기 제1 화자의 발화를 조각그룹으로 생성하여 입력데이 터로 정하고 상기 제2 화자의 발화를 라벨데이터로 정할 수 있다. 학습데이터세트를 생성하는 단계(S51)는 도 3을 참조하여 설명하였으므로 자세한 설명은 생략한다. 턴프리 대화 모델을 학습시키는 단계(S52)는 학습데이터세트를 이용하여 인공지능 모델을 학습시키는 과정이다. 인공지 능 모델은 다층 신경망 구조를 포함할 수 있다. 턴프리 대화모델을 학습시키는 단계(S52)는 역전파 방식을 이용하여 라벨데이터의 추정 정확도가 향상되도록 다층 신경망의 가중치를 조정하는 과정을 통해 수행될 수 있 다. 일 구현예에 따르면 턴프리 대화 방법은, 사용자의 발화의 전부가 입력되면 발화의 내용을 분석하여 실질 적인 응답을 제공하는 턴기반 대화모델을 이용하여, 사용자의 발화가 완료되면 발화를 상기 턴기반 대 화모델에 입력하여 실질적 응답을 획득하는 단계(S60), 및 실질적 응답을 사용자에게 제공하는 단계를 더 포함할 수 있다. 그리고, 중간응답을 획득하는 단계(S30)와 실질적 응답을 획득하는 단계(S60)는 서로 독립 적으로 실행되어, 사용자의 발화 중간에는 상기 중간응답을 제공하는 단계가 수행될 수 있고, 사용자의 발화가 완료되면 실질적 응답을 제공하는 단계(S70)가 수행될 수 있다. 사용자가 발화를 시작하면 프로세서는 사용자의 발화를 입력받는 단계(S10)를 수행한다. 다음으로, 프로세서는 사용자의 발화가 완료되지 않은 상태이므로 조각그룹을 생성하는 단계(S20)를 수행한다. 프 로세서는 사용자의 발화의 일부를 조각으로 분리한다. 제1 타임스탬프에서, 조각 1은 그대로 조각그룹 1이 될 수 있다. 다음으로, 프로세서는 중간응답을 획득하는 단계(S30)를 수행한다. 프로세서는 조각그 룹 1을 턴프리 대화모델에 입력하고 중간응답 또는 '빈공간'을 획득할 수 있다. 다음으로, 프로세서는 중간응답을 사용자에게 제공하는 단계(S40)를 수행한다. 그리고, 제2 타임스탬프에서, 프로세서는 조각그룹을 생성하는 단계(S20)를 수행하며, 제1 타임스탬프의 조 각 1과 제2 타임스탬프의 조각 2는 조각그룹 2가 될 수 있다. 다음으로, 프로세서는 중간응답을 획득하는 단계(S30)를 수행한다. 프로세서는 조각그룹 2를 턴프리 대화모델에 입력하고 중간응답 또는 '빈공 간'을 획득할 수 있다. 다음으로, 프로세서는 중간응답을 사용자에게 제공하는 단계(S40)를 수행한다. 그리고, 제3 타임스탬프에서, 프로세서는 조각 1, 조각 2, 조각 3을 포함하는 조각그룹 3을 턴프리 대화모 델에 입력하고 중간응답 또는 '빈공간'을 획득하고 중간응답을 사용자에게 제공할 수 있다. 유사하게, 제4 타임스탬프에서, 프로세서는 조각 1, 조각 2, 조각 3, 조각 4를 포함하는 조각그룹 4을 턴프리 대화모 델에 입력하고 중간응답 또는 '빈공간'을 획득하고 중간응답을 사용자에게 제공할 수 있다. 사용자의 발화 입력이 더이상 들어오지 않는 경우, 프로세서는 사용자의 발화가 완료된 것으로 판 단할 수 있다. 프로세서는 완료된 사용자의 발화를 턴기반 대화모델에 입력하고 실질적 응답을 획 득할 수 있다. 프로세서는 실질적 응답을 원격단말 또는 입출력부를 통해 사용자에게 제공할수 있다. 설명한 내용을 종합하여, 일 구현예에 따른 턴프리 대화모델을 위한 학습데이터세트를 하나 더 설명한다. 제1 화자가 '아까 그 누구지 진영이를 우연히 봤어' 라는 문장을 발화한 경우, 제2 화자가 중간응답 으로서 언어적 응답, 시각적 응답, 청각적 응답을 제공하는 학습데이터세트(표 1)를 생성할 수 있다. 표 1 학습데이터세트 타임스탬프 학습데이터 라벨데이터 t1 조각그룹아까 네 t2 조각그룹아까 그 C3빈공간 t3 조각그룹아까 그 누구지 C2궁금 t4 조각그룹아까 그 누구지 진영 C3빈공간 t5 조각그룹아까 그 누구지 진영이를 C3빈공간 t6 조각그룹아까 그 누구지 진영이를 우연히 C1놀람 t7 조각그룹아까 그 누구지 진영이를 우연히 봤어 C3빈공간 표 1의 학습데이터세트는 화자가 '아까' 라는 단어를 발화하는 것을 입력받으면, 언어적 응답으로서 '네'를 제 공하도록 턴프리 대화모델을 학습시킬 수 있다. 그리고, 표 1의 학습데이터세트는 화자가 '아까 그' 까지 발화하는 것을 입력받으면 코드 'C3빈공간'을 출력하도록 턴프리 대화모델을 학습시킬 수 있다. 계속해서, 표 1의 학습데이터세트는 화자가 '아까 그 누구지' 까지 발화하는 것을 입력받으면 코드'C2궁금'을 출력하도록 턴프리 대화모델을 학습시킬 수 있다. 계속해서, 표 1의 학습데이터세트는 화자가 '아까 그 누구지 진영이 를 우연히' 까지 발화하는 것을 입력받으면 코드'C1놀람'을 출력하도록 턴프리 대화모델을 학습시킬 수 있 다. 프로세서는 표 1의 학습데이터세트를 이용하여 인공지능모델을 학습시켜 턴프리 대화모델을 생성할 수 있다. 표 1의 학습데이터세트를 학습한 턴프리 대화모델은 사용자가 '아까' 라는 단어를 발화하는 것을 입력 받으면, 텍스트인 '네'를 출력하고, 프로세서는 언어적 응답으로서 '네'를 소리 또는 텍스트로 제공할 수 있다. 그리고, 턴프리 대화모델은 사용자가 '아까 그' 까지 발화하는 것을 입력받으면 코드'C3빈공간'을 출 력하므로 프로세서는 중간응답을 제공하지 않는다. 계속해서, 턴프리 대화모델은 사용자가 '아까 그 누구지' 까지 발화하는 것을 입력받으면 코드'C2궁금'을 출력하므로 프로세서는 메모리에 저장된 룩 업테이블을 참조하여 '궁금해하는 표정'이라는 시각적 중간응답을 제공할 수 있다. 턴프리 대화모델은 사 용자가 '아까 그 누구지 진영이를 우연히' 까지 발화하는 것을 입력받으면 코드'C1놀람'을 출력하므로 프로 세서는 메모리에 저장된 룩업테이블을 참조하여 '아!'이라는 청각적 중간응답을 제공할 수 있다. 사용자의 발화가 종료되면, 프로세서는 사용자가 발화한 문장을 턴기반 대화모델에 제공하고, 턴 기반 대화모델이 출력하는 응답인 '어디서 보셨나요?'라는 응답을 사용자에게 제공할 수 있다. 설명한 바와 같이, 턴프리 대화 방법은 사용자가 발화를 수행하는 중간에는 턴프리 대화모델을 이용하 여 중간응답을 제공하고, 사용자가 발화를 완료하면 턴기반 대화모델을 이용하여 실질적 응답을 제공 할 수 있다. 사용자는 중간응답을 제공받음으로써 실제로 사람과 대화하는 것과 같은 느낌을 가질 수 있다. 이상 본 개시를 구체적인 구현예를 통하여 상세히 설명하였다. 구현예는 본 개시를 구체적으로 설명하기 위한 것으로, 본 개시는 이에 한정되지 않는다. 본 개시의 기술적 사상 내에서 당해 분야의 통상의 지식을 가진 자에 의해 그 변형이나 개량이 가능함은 명백하다고 할 것이다. 본 개시의 단순한 변형 내지 변경은 모두 본 개시의 영역에 속하는 것으로 본 개시의 구체적인 보호 범위는 첨 부된 특허청구범위에 의하여 명확해질 것이다."}
{"patent_id": "10-2022-0148053", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 구현예에 따른 턴프리 대화 장치와 사용자의 대화를 나타내는 도면이다. 도 2는 일 구현예에 따른 턴프리 대화 장치를 나타내는 도면이다. 도 3은 일 구현예에 따른 턴프리 대화모델의 학습데이터세트를 설명하는 도면이다. 도 4는 일 구현예에 따른 턴프리 대화 방법을 나타내는 도면이다. 도 5는 일 구현예에 따른 사용자의 발화와 응답을 설명하는 도면이다."}
