{"patent_id": "10-2023-0065863", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0168170", "출원번호": "10-2023-0065863", "발명의 명칭": "센서 데이터에 대한 융합 장치 및 방법", "출원인": "건국대학교 산학협력단", "발명자": "조기춘"}}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "센서 데이터에 대한 융합 방법에 있어서,(a) 제1센서를 이용하여 복수의 제1센서 데이터를 획득하고, 상기 제1센서 대비 긴 센싱 주기로 동작하는 제2센서를 이용하여 상기 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득하는 단계;(b) 상기 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생성하는 단계;(c) 상기 제2센서 데이터로부터 제2특징 정보를 추출하는 단계; 및(d) 상기 제1특징 정보 및 상기 제2특징 정보를 융합하는 단계,를 포함하는, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 (b) 단계는,(b1) 합성곱 신경망(Convolutional Neural Network) 기반의 제1특징 추출 모델을 이용하여 상기 복수의 제1센서 데이터 각각으로부터 상기 복수의 특징 정보를 추출하는 단계; 및(b2) 순환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모델을 이용하여 상기 복수의 특징 정보를융합하여 제1특징 정보를 생성하는 단계,를 포함하는 것인, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 (c) 단계는,합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하여 상기 제2센서 데이터로부터 제2특징 정보를 추출하는 것인, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1센서는 카메라 센서이고, 상기 제2센서는 라이다 센서인 것을 특징으로 하는, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 복수의 제1센서 데이터는 상기 카메라 센서에 의해 시계열적으로 촬영된 복수의 이미지 데이터를포함하고,상기 제2센서 데이터는 상기 복수의 이미지 데이터의 촬영 시점에 대응하여 상기 라이다 센서에 의해 계측된 포인트 클라우드 데이터를 포함하는 것인, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,공개특허 10-2024-0168170-3-상기 (d) 단계는,상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서의 형태를 일치시키는 변환을수행하는 단계; 및상기 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응하도록 연결(Concatenation)하는 단계,를 포함하는 것인, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 (d) 단계는,상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성하는 단계; 및상기 어텐션 마스크를 이용하여 상기 제1텐서 및 상기 제2텐서에 대한 가중합(weighted sum) 연산을 수행하는단계,를 포함하는 것인, 융합 방법."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "센서 데이터에 대한 융합 장치에 있어서,제1센서를 이용하여 복수의 제1센서 데이터를 획득하고, 상기 제1센서 대비 긴 센싱 주기로 동작하는 제2센서를이용하여 상기 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득하는 수집부;상기 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생성하고, 상기제2센서 데이터로부터 제2특징 정보를 추출하는 특징 분석부; 및상기 제1특징 정보 및 상기 제2특징 정보를 융합하는 특징 융합부,를 포함하는, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 특징 분석부는,합성곱 신경망(Convolutional Neural Network) 기반의 제1특징 추출 모델을 이용하여 상기 복수의 제1센서 데이터 각각으로부터 상기 복수의 특징 정보를 추출하고,순환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모델을 이용하여 상기 복수의 특징 정보를 융합하여 제1특징 정보를 생성하는 것인, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 특징 분석부는,합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하여 상기 제2센서 데이터로부터 제2특징 정보를 추출하는 것인, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서,상기 제1센서는 카메라 센서이고, 상기 제2센서는 라이다 센서인 것을 특징으로 하는, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2024-0168170-4-제11항에 있어서,상기 복수의 제1센서 데이터는 상기 카메라 센서에 의해 시계열적으로 촬영된 복수의 이미지 데이터를포함하고,상기 제2센서 데이터는 상기 복수의 이미지 데이터의 촬영 시점에 대응하여 상기 라이다 센서에 의해 계측된 포인트 클라우드 데이터를 포함하는 것인, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 특징 융합부는,상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서의 형태를 일치시키는 변환을수행하고, 상기 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응하도록 연결(Concatenation)하는것인, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 특징 융합부는,상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성하고, 상기 어텐션 마스크를 이용하여 상기 제1텐서 및 상기 제2텐서에 대한 가중합(weighted sum) 연산을 수행하는 것인, 융합 장치."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "센서 데이터에 대한 융합 장치 및 방법이 개시되며, 본원의 일 실시예에 따른 센서 데이터에 대한 융합 방법은, (a) 제1센서를 이용하여 복수의 제1센서 데이터를 획득하고, 상기 제1센서 대비 긴 센싱 주기로 동작하는 제2센 서를 이용하여 상기 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득하는 단계, (b) 상기 복수의 제1센 서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생성하는 단계, (c) 상기 제2센서 데 이터로부터 제2특징 정보를 추출하는 단계 및 (d) 상기 제1특징 정보 및 상기 제2특징 정보를 융합하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본원은 센서 데이터에 대한 융합 장치 및 방법에 관한 것이다. 예를 들면, 본원은 센서 데이터의 손실이 없는 카메라 데이터 및 라이다 데이터의 융합 기법에 관한 것이다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자율주행 인지 분야에서는 서로 다른 센서의 데이터를 융합하는 센서 퓨전을 통해 보다 높은 성능의 인지 성능 을 달성할 수 있다. 이와 관련하여 어느 한 시점의 데이터만을 사용하여 차량 주변의 객체를 인식하는 것이 아 니라, 연속된 데이터를 이용하여 인지를 수행하는 알고리즘이 개발되고 있으며, 이러한 인지 알고리즘들은 보다 연속적이고 안정적인 인지를 위해 연속 데이터를 사용하여 인지 성능을 개선하였다. 특히, 이러한 센서 퓨전 인지에 있어 데이터를 융합하기 위해서는 동일 시간에 취득한 데이터가 필요하며, 이를 위해 높은 주기의 센서를 낮은 주기의 센서와 동기화하는 시간 동기화 작업이 필수적으로 요구되는데, 종래의 센서 융합 인지 알고리즘은 카메라와 같은 높은 주기의 센서를 상대적으로 낮은 주기를 가지는 라이다 센서와 동기화하는 경우, 동일한 시점에 취득된 데이터만을 사용하여 연산을 수행하여 높은 주기의 센서 데이터가 손실 되는 문제가 있었다. 따라서, 주기가 서로 다른 센서에 의해 독립적으로 계측된 센터 데이터를 융합할 때, 높은 주기의 센서 데이터 를 손실없이 융합하여 높은 성능을 유지할 수 있는 기법에 대한 개발이 요구되는 실정이다. 본원의 배경이 되는 기술은 한국등록특허공보 제10-1848312호에 개시되어 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본원은 전술한 종래 기술의 문제점을 해결하기 위한 것으로서, 상대적으로 높은 주기의 센서 데이터를 손실하지 않고, 계측 주기(데이터 획득 주기)가 서로 다른 센서의 데이터를 융합할 수 있는 센서 데이터에 대한 융합 장 치 및 방법을 제공하려는 것을 목적으로 한다.다만, 본원의 실시예가 이루고자 하는 기술적 과제는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 또 다른 기술적 과제들이 존재할 수 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기한 기술적 과제를 달성하기 위한 기술적 수단으로서, 본원의 일 실시예에 따른 센서 데이터에 대한 융합 방 법은, (a) 제1센서를 이용하여 복수의 제1센서 데이터를 획득하고, 상기 제1센서 대비 긴 센싱 주기로 동작하는 제2센서를 이용하여 상기 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득하는 단계, (b) 상기 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생성하는 단계, (c) 상기 제2 센서 데이터로부터 제2특징 정보를 추출하는 단계 및 (d) 상기 제1특징 정보 및 상기 제2특징 정보를 융합하는 단계를 포함할 수 있다. 또한, 상기 (b) 단계는, (b1) 합성곱 신경망(Convolutional Neural Network) 기반의 제1특징 추출 모델을 이용 하여 상기 복수의 제1센서 데이터 각각으로부터 상기 복수의 특징 정보를 추출하는 단계 및 (b2) 순환 신경망 (Recurrent Neural Network) 기반의 제2특징 추출 모델을 이용하여 상기 복수의 특징 정보를 융합하여 제1특징 정보를 생성하는 단계를 포함할 수 있다. 또한, 상기 (c) 단계는, 합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하여 상기 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 또한, 상기 제1센서는 카메라 센서이고, 상기 제2센서는 라이다 센서일 수 있다. 또한, 상기 복수의 제1센서 데이터는 상기 카메라 센서에 의해 시계열적으로 촬영된 복수의 이미지 데이터를 포 함할 수 있다. 또한, 상기 제2센서 데이터는 상기 복수의 이미지 데이터의 촬영 시점에 대응하여 상기 라이다 센서에 의해 계 측된 포인트 클라우드 데이터를 포함할 수 있다. 또한, 상기 (d) 단계는, 상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서의 형 태를 일치시키는 변환을 수행하는 단계 및 상기 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응 하도록 연결(Concatenation)하는 단계를 포함할 수 있다. 또한, 상기 (d) 단계는, 상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서를 이 용하여 어텐션 마스크(Attention Mask)를 생성하는 단계 및 상기 어텐션 마스크를 이용하여 상기 제1텐서 및 상 기 제2텐서에 대한 가중합(weighted sum) 연산을 수행하는 단계를 포함할 수 있다. 한편, 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치는, 제1센서를 이용하여 복수의 제1센서 데이터를 획득하고, 상기 제1센서 대비 긴 센싱 주기로 동작하는 제2센서를 이용하여 상기 복수의 제1센서 데이터에 대응 하는 제2센서 데이터를 획득하는 수집부, 상기 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생성하고, 상기 제2센서 데이터로부터 제2특징 정보를 추출하는 특징 분석부 및 상기 제 1특징 정보 및 상기 제2특징 정보를 융합하는 특징 융합부를 포함할 수 있다. 또한, 상기 특징 분석부는, 합성곱 신경망(Convolutional Neural Network) 기반의 제1특징 추출 모델을 이용하 여 상기 복수의 제1센서 데이터 각각으로부터 상기 복수의 특징 정보를 추출할 수 있다. 또한, 상기 특징 분석부는, 순환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모델을 이용하여 상기 복수의 특징 정보를 융합하여 제1특징 정보를 생성할 수 있다. 또한, 상기 특징 분석부는, 합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하 여 상기 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 또한, 상기 특징 융합부는, 상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서의 형태를 일치시키는 변환을 수행하고, 상기 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응하도록 연결(Concatenation)할 수 있다. 또한, 상기 특징 융합부는, 상기 제1특징 정보에 대응하는 제1텐서 및 상기 제2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성하고, 상기 어텐션 마스크를 이용하여 상기 제1텐서 및 상기 제 2텐서에 대한 가중합(weighted sum) 연산을 수행할 수 있다. 상술한 과제 해결 수단은 단지 예시적인 것으로서, 본원을 제한하려는 의도로 해석되지 않아야 한다. 상술한 예 시적인 실시예 외에도, 도면 및 발명의 상세한 설명에 추가적인 실시예가 존재할 수 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "전술한 본원의 과제 해결 수단에 의하면, 상대적으로 높은 주기의 센서 데이터를 손실하지 않고, 계측 주기(데 이터 획득 주기)가 서로 다른 센서의 데이터를 융합할 수 있는 센서 데이터에 대한 융합 장치 및 방법을 제공할 수 있다. 전술한 본원의 과제 해결 수단에 의하면, 손실 없는 풍부한 카메라 정보를 사용함으로써, 객체 탐지 (Detection), 분할(Segmentation) 등의 태스크의 성능을 향상시킬 수 있다. 다만, 본원에서 얻을 수 있는 효과는 상기된 바와 같은 효과들로 한정되지 않으며, 또 다른 효과들이 존재할 수 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본원이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본원의 실시예를 상세히 설명한다. 그러나 본원은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본원을 명확하게 설명하기 위해서 설명과 관계없는 부분 은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본원 명세서 전체에서, 어떤 부분이 다른 부분과 \"연결\"되어 있다고 할 때, 이는 \"직접적으로 연결\"되어 있는 경우뿐 아니라, 그 중간에 다른 소자를 사이에 두고 \"전기적으로 연결\" 또는 \"간접적으로 연결\"되어 있는 경우 도 포함한다. 본원 명세서 전체에서, 어떤 부재가 다른 부재 \"상에\", \"상부에\", \"상단에\", \"하에\", \"하부에\", \"하단에\" 위치 하고 있다고 할 때, 이는 어떤 부재가 다른 부재에 접해 있는 경우뿐 아니라 두 부재 사이에 또 다른 부재가 존 재하는 경우도 포함한다. 본원 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 본원은 센서 데이터에 대한 융합 장치 및 방법에 관한 것이다. 예를 들면, 본원은 센서 데이터의 손실이 없는 카메라 데이터 및 라이다 데이터의 융합 기법에 관한 것이다. 도 1은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치를 포함하는 센서 퓨전 기반 인지 시스템의 개략 적인 구성도이다.도 1을 참조하면, 본원의 일 실시예에 따른 센서 퓨전 기반 인지 시스템은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치(이하, '센서 데이터 융합 장치'라 한다.), 센서 디바이스(201, 202), 사용 자 단말 및 데이터베이스를 포함할 수 있다. 센서 데이터 융합 장치, 센서 디바이스(201, 202), 사용자 단말 및 데이터베이스 상호간은 네트 워크를 통해 통신할 수 있다. 네트워크는 단말들 및 서버들과 같은 각각의 노드 상호간에 정보 교환이 가능한 연결 구조를 의미하는 것으로, 이러한 네트워크의 일 예에는, 3GPP(3rd Generation Partnership Project) 네트워크, LTE(Long Term Evolution) 네트워크, 5G 네트워크, WIMAX(World Interoperability for Microwave Access) 네트워크, 인터넷(Internet), LAN(Local Area Network), Wireless LAN(Wireless Local Area Network), WAN(Wide Area Network), PAN(Personal Area Network), wifi 네트워크, 블루투스(Bluetooth) 네트워크, 위성 방송 네트워크, 아날로그 방송 네트워크, DMB(Digital Multimedia Broadcasting) 네트워크 등 이 포함되나 이에 한정되지는 않는다. 사용자 단말은 예를 들면, 스마트폰(Smartphone), 스마트패드(SmartPad), 태블릿 PC등과 PCS(Personal Communication System), GSM(Global System for Mobile communication), PDC(Personal Digital Cellular), PHS(Personal Handyphone System), PDA(Personal Digital Assistant), IMT(International Mobile Telecommunication)-2000, CDMA(Code Division Multiple Access)-2000, W-CDMA(W-Code Division Multiple Access), Wibro(Wireless Broadband Internet) 단말기 같은 모든 종류의 무선 통신 장치일 수 있다. 이하에서는 도 2 내지 도 9를 참조하여 센서 데이터 융합 장치의 구체적인 기능 및 동작에 대하여 설명하 도록 한다. 도 2는 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치의 동작 프로세스를 나타낸 개념도이다. 도 2를 참조하면, 종래의 센서 퓨전 인지 알고리즘들이 상대적으로 짧은(높은) 주기로 센서 데이터를 획득하는 제1센서를 상대적으로 긴(낮은) 주기로 센서 데이터를 획득하는 제2센서와 동기화 하여 동일한 시점 에 획득된 센서 데이터(달리 말해, 특정 시점에 제1센서 및 제2센서 각각에 의해 계측된 제1센서 데 이터 및 제2센서 데이터)만을 이용하여 센서 융합을 위한 연산을 수행하는 것과 달리, 본원에서 개시하는 센서 데이터 융합 장치는 높은(짧은) 주기의 센서 데이터를 낮은(긴) 주기의 센서 데이터와 동기화 하지 않고, 높은(짧은) 주기의 센서 데이터(제1센서 데이터)에 반영된 시계열 특징을 이용하여 특징을 추출한 후, 낮은(긴) 주기의 센서 데이터(제2센서 데이터)와 융합할 수 있다. 구체적으로 도 2를 참조하면, 센서 데이터 융합 장치는 제1센서를 이용하여 복수의 제1센서 데이터를 획득할 수 있다. 또한, 센서 데이터 융합 장치는 제1센서 대비 긴 센싱 주기로 동작하는 제2센서 를 이용하여 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득할 수 있다. 이와 관련하여 도 2를 참조하면, 제1센서는 카메라 센서일 수 있으며, 이에 따라 복수의 제1센서 데 이터는 카메라 센서에 의해 시계열적으로 촬영된 복수의 이미지 데이터(도 2의 시계열 이미지 참조)를 포함할 수 있다. 또한, 본원의 일 실시예에 따르면, 제2센서는 라이다 센서일 수 있으며, 이에 따라 제2센서 데이터는 복수 의 이미지 데이터의 촬영 시점에 대응하여 라이다 센서에 의해 계측된 포인트 클라우드 데이터(도 2의 포 인트 클라우드 참조)를 포함할 수 있다. 다만, 제1센서 및 제2센서은 본원의 구현예에 따라서 전술한 카메라 센서, 라이다 센서 유형 외에도, 차량의 주행 시 차량 주변 영역에 존재하는 다양한 객체를 인지(탐지)하기 위하여 구비되고, 서로 센싱 주기가 상이하게 동작하는 다양한 유형의 센서를 폭넓게 포함할 수 있음은 물론이다. 또한, 도 2를 참조하면, 센서 데이터 융합 장치는 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특 징 정보를 기초로 제1특징 정보를 생성할 수 있다. 예를 들어, 센서 데이터 융합 장치는 시계열 이미지 로부터 인공지능 기반의 특징 추출 모델을 통해 카메라 특징 정보를 제1특징 정보로써 도출할 수 있다. 구체적으로 도 2를 참조하면, 센서 데이터 융합 장치는 합성곱 신경망(Convolutional Neural Network) 기 반의 제1특징 추출 모델(M1)을 이용하여 복수의 제1센서 데이터 각각으로부터 복수의 특징 정보를 추출하고, 순 환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모델(M2)을 이용하여 복수의 특징 정보를 융합하여 제1특징 정보를 생성할 수 있다.또한, 도 2를 참조하면, 센서 데이터 융합 장치는 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 예를 들어, 센서 데이터 융합 장치는 포인트 클라우드로부터 인공지능 기반의 특징 추출 모델을 통해 라이다 특징 정보를 제2특징 정보로써 도출할 수 있다. 구체적으로 도 2를 참조하면, 센서 데이터 융합 장치는 합성곱 신경망(Convolutional Neural Network) 기 반의 제3특징 추출 모델(M3)을 이용하여 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 또한, 도 2를 참조하면, 센서 데이터 융합 장치는 제1특징 정보 및 제2특징 정보를 융합할 수 있다. 예를 들어, 센서 데이터 융합 장치는 제1특징 정보로써 도출된 카메라 특징 정보와 제2특징 정보로써 도출된 라 이다 특징 정보를 융합하여 융합된 카메라-라이다 특징 정보를 도출할 수 있다. 이와 관련하여 도 3은 본원의 제1실시예에 따라 복수의 제1센서 데이터 및 제2센서 데이터를 이용한 융합 특징 정보를 생성하는 프로세스를 나타낸 개념도이다. 구체적으로 도 3은 10Hz의 주파수에 대응하는 계측 주기를 가지는 라이다 센서와 40Hz의 주파수에 대응하 는 계측 주기를 가지는 카메라 센서에 의해 획득되는 시계열 이미지와 포인트 클라우드를 센서 데 이터 융합 장치가 융합하는 프로세스를 나타낸 도면이다. 도 3을 참조하면, 제1센서와 제2센서의 계측 주기(주파수)가 4배에 해당하므로, 센서 데이터 융합 장 치는 4개의 이미지 데이터와 하나의 센서 데이터를 융합하도록 동작할 수 있다. 도 3을 참조하면, 센서 데이터 융합 장치는 합성곱 신경망 기반의 인코더(Encoder)에 해당하는 제1특징 추 출 모델(M1)을 이용하여 Frame t-3 내지 Frame t를 포함하는 4개의 프레임의 시계열 이미지로부터 피처 수 준(feature level)의 특징을 생성한 후, 순환 신경망 기반의 특징 추출기에 해당하는 제2특징 추출 모델(M2)을 이용하여 생성된 특징 정보를 강화하여 제1특징 정보(카메라 특징 정보)를 도출할 수 있다. 또한, 센서 데이터 융합 장치는 합성곱 신경망 기반의 인코더(Encoder)에 해당하는 제3특징 추출 모델(M 3)을 이용하여 Frame t의 포인트 클라우드로부터 제2특징 정보(라이다 특징 정보)를 도출한 후, 제1특징 정 보와 제2특징 정보를 융합하여 융합된 피처(Feature)를 생성할 수 있다. 한편, 도 4는 본원의 제2실시예에 따라 복수의 제1센서 데이터 및 제2센서 데이터를 이용한 융합 특징 정보를 생성하는 프로세스를 나타낸 개념도이다. 도 4를 참조하면, 본원의 제2실시예에 따르면, 센서 데이터 융합 장치는 시계열 이미지에 반영된 연속 된 시계열 특성을 고려하기 위해 순환 신경망 구조의 특징 추출 모델을 먼저 적용하여 시계열 이미지로부터 특징을 추출한 후, 합성곱 기반의 인코더(Encoder)를 센서 데이터 각각에 대하여 적용한 후 융합을 수행할 수 있다. 달리 말해, 도 3을 통해 전술한 본원의 제1실시예에 따른 센서 데이터 융합 방식의 경우, 시계열 이미지에 대하여 합성곱 기반의 인코더(Encoder)에 해당하는 제1특징 추출 모델(M1) 및 순환 신경망 기반의 특징 추출기 에 해당하는 제2특징 추출 모델(M-2)을 순차적으로 적용하여 제1특징 정보를 도출하는 반면, 도 4를 통해 설명 한 본원의 제2실시예에 따른 센서 데이터 융합 방식의 경우, 순환 신경망 기반의 특징 추출기에 먼저 시계열 이 미지를 입력하여 연속된 시계열 특징을 우선 추출한 후, 합성곱 기반의 인코더(Encoder)에 통과시켜 제1특 징 정보를 도출하도록 동작할 수 있다. 이와 관련하여, 센서 데이터 융합 장치의 리소스, 성능, 제1센서의 계측 주기(주파수), 제1센서(20 1)와 제2센서의 계측(센싱) 주기 차이 등에 따라서 복수의 이미지 데이터(프레임)를 포함하는 시계열 이미 지로부터 순환 신경망 기반의 특징 추출기를 이용하여 이미지 특징을 직접적으로 추출하는 것이 어려운 경 우, 도 3을 통해 설명한 본원의 제1실시예에 따른 센서 데이터 융합 방식이 센서 데이터의 손실이 상대적으로 덜 발생할 수 있다는 점에서 유리할 수 있다. 보다 구체적으로 본원의 일 실시예에 따르면, 센서 데이터 융합 장치는 제1센서와 제2센서의 계 측(센싱) 주기 차이에 따라 제1특징 정보를 도출하기 위하여 융합되어야 하는 시계열 이미지에 포함되는 프 레임 수가 미리 설정된 임계 프레임 수 이상이면, 제1특징 정보 도출시, 합성곱 기반의 인코더(Encoder)에 해당 하는 제1특징 추출 모델(M1) 및 순환 신경망 기반의 특징 추출기에 해당하는 제2특징 추출 모델(M-2)을 순차적으로 적용하는 제1방식을 우선적으로 적용하도록 동작할 수 있으나, 이에만 한정되는 것은 아니다. 이하에서는 도 5 내지 도 8을 참조하여 특징 추출 모델의 네트워크 구조에 대하여 상세히 설명하도록 한다. 도 5 및 도 6은 합성곱 신경망(Convolutional Neural Network) 기반의 특징 추출 모델을 설명하기 위한 도면이 다. 도 5 및 도 6을 참조하면, 제1센서에 의해 계측된 이미지 데이터를 입력으로 하여 특징 정보를 출력하는 합성곱 신경망(Convolutional Neural Network) 기반의 특징 추출 모델인 제1특징 추출 모델(M1)의 경우, 입력 데이터는 가로 x 세로 x C(예를 들면, RGB 3채널) 형태의 텐서(Tensor)일 수 있고, 네트워크 구조는 2차원 컨볼 루션 레이어, ReLU 레이어, 배치 정규화(Batch Normalization) 레이어 등을 포함하는 구조일 수 있고, 출력 데 이터는 1 x 1 x C 크기를 가질 수 있다. 달리 말해, 제1특징 추출 모델(M1)의 경우, 하드웨어(H/W) 리소스를 최 소화하고, 깊은 채널(C)을 가질 수 있는 네트워크 구조를 가질 수 있다. 또한, 도 5 및 도 6을 참조하면, 제2센서에 의해 계측된 포인트 클라우드 데이터를 입력으로 하여 특징 정 보를 출력하는 합성곱 신경망(Convolutional Neural Network) 기반의 특징 추출 모델인 제3특징 추출 모델(M3) 의 경우, 입력 데이터는 N x 3(x, y, z) 형태의 텐서(Tensor)일 수 있고, 네트워크 구조는 다층 퍼셉트론 (Multi-layer perceptron, MLP), 합성공 신경망(Convolutional Neural Network, CNN) 등의 하위 네트워크를 포 함할 수 있다. 예를 들어, 제3특징 추출 모델(M3)은 입력된 포인트를 Shared-MLP 구조로 처리하여 1024 채널의 전역 특징(Global feature)을 생성하도록 동작하는 PointNet 일 수 있으나, 이에만 한정되는 것은 아니다. 또한, 제3특징 추출 모델(M3)은 깊은 채널(C)을 통해 전역적인 특징을 가지도록 1 x C 크기의 출력 데이터를 출 력할 수 있다. 도 7 및 도 8은 순환 신경망(Recurrent Neural Network) 기반의 특징 추출 모델을 설명하기 위한 도면이다. 도 7 및 도 8을 참조하면, 시계열 데이터를 처리하기 위하여 주로 사용되는 순환 신경망(Recurrent Neural Network, RNN) 기반의 제2특징 추출 모델(M2)은 복수의 이미지 피처 맵을 입력하기 위해 도 7의 Many-to-one 구 조를 채택할 수 있으며, 도 8에 도시된 바와 같이, 제1특징 모델(M1)로부터 도출된 이미지 특징 맵을 각각의 입 력(예를 들어, 도 3 및 도 4에서 상술한 바와 같이 4개의 프레임을 포함하는 시계열 이미지로부터 제1특징 정보를 추출하는 경우, x1 내지 x4의 4개의 입력)할 수 있다. 또한, 본원의 구현예에 따라서 제2특징 모델(M2)로 인가되는 입력 데이터와 제2특징 모델(M2)에 의해 도출되는 출력 데이터는 다양하게 설정되되, 제3특징 모델(M3)과 연계된 포인트 클라우드 피처맵과 동일하도록 출력 데이 터의 크기 등이 설정될 수 있다. 예를 들어, 제2특징 모델(M2)은 N(입력 이미지 프레임 수) x C의 입력 데이터 로부터 1 x C 크기의 출력 데이터를 도출하도록 동작할 수 있다. 도 9는 제1특징 정보 및 제2특징 정보를 융합하는 프로세스를 설명하기 위한 개념도이다. 도 9의 (a)를 참조하면, 본원의 제1실시예에 따르면, 센서 데이터 융합 장치는 제1특징 정보에 대응하는 제1텐서 및 제2특징 정보에 대응하는 제2텐서의 형태를 일치시키는 변환을 수행하고, 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응하도록 연결(Concatenation)할 수 있다. 달리 말해, 본원의 제1실시예에 따르면, 센서 데이터 융합 장치는 서로 다른 센서에 의해 도출된 특징 텐 서의 형태(Shape)를 동일하게 변형시켜 일치시키고, 두 텐서를 특정 차원에 대하여 Concatenation 결합함으로써 데이터 융합을 수행할 수 있다. 또한, 도 9의 (b)를 참조하면, 본원의 제2실시예에 따르면, 센서 데이터 융합 장치는 제1특징 정보에 대응 하는 제1텐서 및 제2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성하고, 생 성된 어텐션 마스크를 이용하여 제1텐서 및 제2텐서에 대한 가중합(weighted sum) 연산을 수행할 수 있다. 달리 말해, 본원의 제2실시예에 따르면, 센서 데이터 융합 장치는 두 텐서를 합성곱 신경망(CNN)으로 처리 하여 가중합 연산을 위한 어텐션 마스크(Attention Mask)를 생성한 후, 각 텐서에 생성된 어텐션 마스크를 곱하 고, 곱연산된 결과를 합하여 가중합 연산을 수행하는 방식으로 데이터 융합을 수행할 수 있다. 도 10은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치의 개략적인 구성도이다. 도 10을 참조하면, 센서 데이터 융합 장치는 수집부, 특징 분석부 및 특징 융합부를 포함 할 수 있다. 수집부는 제1센서를 이용하여 복수의 제1센서 데이터를 획득할 수 있다. 또한, 수집부는 제1센 서 대비 긴 센싱 주기로 동작하는 제2센서를 이용하여 복수의 제1센서 데이터에 대응하는 제2센서 데 이터를 획득할 수 있다. 특징 분석부는 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보를 기초로 제1특징 정보를 생 성할 수 있다. 구체적으로 특징 분석부는 합성곱 신경망(Convolutional Neural Network) 기반의 제1특징 추출 모델을 이 용하여 복수의 제1센서 데이터 각각으로부터 복수의 특징 정보를 추출하고, 순환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모델을 이용하여 복수의 특징 정보를 융합하여 제1특징 정보를 생성할 수 있다. 또한, 특징 분석부는 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 구체적으로 특징 분석부(12 0)는 합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하여 제2센서 데이터로부 터 제2특징 정보를 추출할 수 있다. 특징 융합부는 제1특징 정보 및 제2특징 정보를 융합할 수 있다. 구체적으로 본원의 일 실시예에 따르면, 특징 융합부는 제1특징 정보에 대응하는 제1텐서 및 제2특징 정보 에 대응하는 제2텐서의 형태를 일치시키는 변환을 수행할 수 있다. 또한, 본원의 일 실시예에 따르면, 특징 융합부는 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원 에 대응하도록 연결(Concatenation)할 수 있다. 또한, 본원의 다른 실시예에 따르면, 특징 융합부는 제1특징 정보에 대응하는 제1텐서 및 제2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성할 수 있다. 또한, 본원의 다른 실시예에 따르면, 특징 융합부는 생성된 어텐션 마스크를 이용하여 제1텐서 및 제2텐서 에 대한 가중합(weighted sum) 연산을 수행할 수 있다. 이하에서는 상기에 자세히 설명된 내용을 기반으로, 본원의 동작 흐름을 간단히 살펴보기로 한다. 도 11은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 방법에 대한 동작 흐름도이다. 도 11에 도시된 센서 데이터에 대한 융합 방법은 앞서 설명된 센서 데이터 융합 장치에 의하여 수행될 수 있다. 따라서, 이하 생략된 내용이라고 하더라도 센서 데이터 융합 장치에 대하여 설명된 내용은 센서 데 이터에 대한 융합 방법에 대한 설명에도 동일하게 적용될 수 있다. 도 11을 참조하면, 단계 S11에서 수집부는 (a) 제1센서를 이용하여 복수의 제1센서 데이터를 획득할 수 있다. 또한, 단계 S11에서 수집부는 제1센서 대비 긴 센싱 주기로 동작하는 제2센서를 이용 하여 복수의 제1센서 데이터에 대응하는 제2센서 데이터를 획득할 수 있다. 다음으로, 단계 S12에서 특징 분석부는 (b) 복수의 제1센서 데이터를 이용하여 도출되는 복수의 특징 정보 를 기초로 제1특징 정보를 생성할 수 있다. 구체적으로 단계 S12에서 특징 분석부는 (b1) 합성곱 신경망(Convolutional Neural Network) 기반의 제1 특징 추출 모델을 이용하여 복수의 제1센서 데이터 각각으로부터 복수의 특징 정보를 추출할 수 있다. 또한, 단계 S12에서 특징 분석부는 (b2) 순환 신경망(Recurrent Neural Network) 기반의 제2특징 추출 모 델을 이용하여 복수의 특징 정보를 융합하여 제1특징 정보를 생성할 수 있다. 다음으로, 단계 S13에서 특징 분석부는 (c) 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 구체적으로 단계 S13에서 특징 분석부는 합성곱 신경망(Convolutional Neural Network) 기반의 제3특징 추출 모델을 이용하여 제2센서 데이터로부터 제2특징 정보를 추출할 수 있다. 다음으로, 단계 S14에서 특징 융합부는 (d) 제1특징 정보 및 제2특징 정보를 융합할 수 있다. 구체적으로 본원의 일 실시예에 따르면, 단계 S14에서 특징 융합부는 제1특징 정보에 대응하는 제1텐서 및 제2특징 정보에 대응하는 제2텐서의 형태를 일치시키는 변환을 수행할 수 있다.또한, 본원의 일 실시예에 따르면, 단계 S14에서 특징 융합부는 변환이 수행된 제1텐서 및 제2텐서를 미리 설정된 차원에 대응하도록 연결(Concatenation)할 수 있다. 또한, 본원의 다른 실시예에 따르면, 단계 S14에서 특징 융합부는 제1특징 정보에 대응하는 제1텐서 및 제 2특징 정보에 대응하는 제2텐서를 이용하여 어텐션 마스크(Attention Mask)를 생성할 수 있다. 또한, 본원의 다른 실시예에 따르면, 단계 S14에서 특징 융합부는 생성된 어텐션 마스크를 이용하여 제1텐 서 및 제2텐서에 대한 가중합(weighted sum) 연산을 수행할 수 있다. 상술한 설명에서, 단계 S11 내지 S14는 본원의 구현예에 따라서, 추가적인 단계들로 더 분할되거나, 더 적은 단 계들로 조합될 수 있다. 또한, 일부 단계는 필요에 따라 생략될 수도 있고, 단계 간의 순서가 변경될 수도 있다. 본원의 일 실시예에 따른 센서 데이터에 대한 융합 방법은 다양한 컴퓨터 수단을 통하여 수행될 수 있는 프로그 램 명령 형태로 구현되어 컴퓨터 판독 가능 매체에 기록될 수 있다. 상기 컴퓨터 판독 가능 매체는 프로그램 명 령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함할 수 있다. 상기 매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 당업자에게 공지되어 사용 가 능한 것일 수도 있다. 컴퓨터 판독 가능 기록 매체의 예에는 하드 디스크, 플로피 디스크 및 자기 테이프와 같 은 자기 매체(magnetic media), CD-ROM, DVD와 같은 광기록 매체(optical media), 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치가 포함된다. 프로그램 명령의 예에는 컴파일러에 의 해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함한다. 상기된 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어 모듈로서 작동하도록 구성될 수 있으며, 그 역도 마찬가지이다. 또한, 전술한 센서 데이터에 대한 융합 방법은 기록 매체에 저장되는 컴퓨터에 의해 실행되는 컴퓨터 프로그램 또는 애플리케이션의 형태로도 구현될 수 있다."}
{"patent_id": "10-2023-0065863", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 본원의 설명은 예시를 위한 것이며, 본원이 속하는 기술분야의 통상의 지식을 가진 자는 본원의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본원의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본원의 범위에 포함되는 것으로 해 석되어야 한다."}
{"patent_id": "10-2023-0065863", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치를 포함하는 센서 퓨전 기반 인지 시스템의 개략 적인 구성도이다. 도 2는 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치의 동작 프로세스를 나타낸 개념도이다. 도 3은 본원의 제1실시예에 따라 복수의 제1센서 데이터 및 제2센서 데이터를 이용한 융합 특징 정보를 생성하 는 프로세스를 나타낸 개념도이다. 도 4는 본원의 제2실시예에 따라 복수의 제1센서 데이터 및 제2센서 데이터를 이용한 융합 특징 정보를 생성하 는 프로세스를 나타낸 개념도이다. 도 5 및 도 6은 합성곱 신경망(Convolutional Neural Network) 기반의 특징 추출 모델을 설명하기 위한 도면이 다. 도 7 및 도 8은 순환 신경망(Recurrent Neural Network) 기반의 특징 추출 모델을 설명하기 위한 도면이다. 도 9는 제1특징 정보 및 제2특징 정보를 융합하는 프로세스를 설명하기 위한 개념도이다. 도 10은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 장치의 개략적인 구성도이다. 도 11은 본원의 일 실시예에 따른 센서 데이터에 대한 융합 방법에 대한 동작 흐름도이다."}
