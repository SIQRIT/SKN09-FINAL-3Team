{"patent_id": "10-2021-0158836", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0072270", "출원번호": "10-2021-0158836", "발명의 명칭": "경쟁 환경에서 모델 기반 강화 학습을 이용한 대중 인식 예측을 위한 컴퓨터 장치 및 그의 방", "출원인": "한국과학기술원", "발명자": "이상완"}}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "컴퓨터 장치에 있어서, 메모리; 및상기 메모리와 연결되고, 상기 메모리에 저장된 적어도 하나의 명령을 실행하도록 구성된 프로세서를 포함하고, 상기 프로세서는,복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning)을 이용하여, 상기 참가자들의 각각의 행동을 예측하도록 구성되는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 참가자들은 일 참가자로서의 자신 및 다른 참가자들을 포함하고, 상기 프로세서는,상기 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로 상기 다른 참가자들의 각각의 행동을 예측하도록 구성되는 추론 모듈을 포함하는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서, 상기 프로세서는,상기 다른 참가자들의 각각의 행동에 기반하여, 상기 자신의 행동을 결정하도록 구성되는 정책 모듈을 더 포함하는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서, 상기 모델 기반 강화 학습은 메타 강화 학습(meta reinforcement learning) 알고리즘을 사용하여, 구현되는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2023-0072270-3-제 3 항에 있어서, 상기 추론 모듈은,상기 경쟁 환경의 이전 상태, 및 상기 자신의 이전 행동에 기반하여, 상기 다른 참가자들의 이전 행동들을 기반으로 상기 다른 참가자들의 각각의 현재 인지 상태를 추적하도록 구성되는 장단기메모리(long short termmemory; LSTM); 및상기 다른 참가자들의 각각의 상기 현재 인지 상태로부터 상기 다른 참가자들의 다음 행동들을 각각 예측하도록구성되는 제 1 순방향신경망을 포함하는, 컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 5 항에 있어서, 상기 정책 모듈은,상기 경쟁 환경의 이전 상태, 상기 자신의 이전 행동, 및 상기 다른 참가자들의 다음 행동들에 기반하여, 상기자신의 다음 행동을 결정하도록 구성되는 제 2 순방향신경망을 포함하는, 컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서,상기 제 2 순방향 신경망은,상기 경쟁 환경의 이전 보상 기록에 더 기반하여, 상기 자신의 다음 행동을 결정하도록 구성되는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 경쟁 환경은,복수의 라운드들에 걸쳐 진행되면서 상기 참가자들 중 일부가 제거되는 적자생존 환경, 또는 상기 참가자들이 복수의 그룹들로 구분되어 상기 그룹들의 각각의 참가자들이 협력하는 협력 환경중 적어도 하나를 포함하는,컴퓨터 장치."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "컴퓨터 장치의 방법에 있어서, 복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning)을 이용하여, 상기 참가자들의 각각의 행동을 예측하는 단계공개특허 10-2023-0072270-4-를 포함하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 참가자들은 일 참가자로서의 자신 및 다른 참가자들을 포함하고, 상기 참가자들의 각각의 행동을 예측하는 단계는,추론 모듈에 의해, 상기 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로 상기 다른 참가자들의 각각의 행동을 예측하는 단계를 포함하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제 10 항에 있어서, 상기 참가자들의 각각의 행동을 예측하는 단계는,정책 모듈에 의해, 상기 다른 참가자들의 각각의 행동에 기반하여, 상기 자신의 행동을 결정하는 단계를 더 포함하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 9 항에 있어서, 상기 모델 기반 강화 학습은 메타 강화 학습(meta reinforcement learning) 알고리즘을 사용하여, 구현되는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 11 항에 있어서,상기 추론 모듈은,장단기메모리 및 제 1 순방향신경망을 포함하고,상기 다른 참가자들의 각각의 행동을 예측하는 단계는,상기 장단기메모리에 의해, 상기 경쟁 환경의 이전 상태, 및 상기 자신의 이전 행동에 기반하여, 상기 다른 참가자들의 이전 행동들을 기반으로 상기 다른 참가자들의 각각의 현재 인지 상태를 추적하는 단계; 및상기 제 1 순방향신경망에 의해, 상기 다른 참가자들의 각각의 상기 현재 인지 상태로부터 상기 다른 참가자들의 다음 행동들을 각각 예측하는 단계를 포함하는,컴퓨터 장치의 방법. 공개특허 10-2023-0072270-5-청구항 14 제 13 항에 있어서, 상기 정책 모듈은, 제 2 순방향신경망을 포함하고, 상기 자신의 행동을 결정하는 단계는,상기 제 2 순방향신경망에 의해, 상기 경쟁 환경의 이전 상태, 상기 자신의 이전 행동, 및 상기 다른 참가자들의 다음 행동들에 기반하여, 상기 자신의 다음 행동을 결정하는 단계를 포함하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 14 항에 있어서, 상기 자신의 다음 행동을 결정하는 단계는,상기 제 2 순방향신경망에 의해, 상기 경쟁 환경의 이전 보상 기록에 더 기반하여, 상기 자신의 다음 행동을 결정하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 9 항에 있어서, 상기 경쟁 환경은,복수의 라운드들에 걸쳐 진행되면서 상기 참가자들 중 일부가 제거되는 적자생존 환경, 또는 상기 참가자들이 복수의 그룹들로 구분되어 상기 그룹들의 각각의 참가자들이 협력하는 협력 환경중 적어도 하나를 포함하는,컴퓨터 장치의 방법."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "복수의 참가자들로 구성되는 경쟁 환경에 대한 예측 방법을 컴퓨터 장치에 실행시키기 위한 하나 이상의 프로그램들이 기록되어 있는 비-일시적인 컴퓨터 판독 가능한 기록 매체에 있어서,상기 방법은,복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning)을 이용하여, 상기 참가자들의 각각의 행동을 예측하는 단계를 포함하는,비-일시적인 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17 항에 있어서, 공개특허 10-2023-0072270-6-상기 참가자들은 일 참가자로서의 자신 및 다른 참가자들을 포함하고, 상기 참가자들의 각각의 행동을 예측하는 단계는,추론 모듈에 의해, 상기 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로 상기 다른 참가자들의 각각의 행동을 예측하는 단계를 포함하는,비-일시적인 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18 항에 있어서, 상기 참가자들의 각각의 행동을 예측하는 단계는,정책 모듈에 의해, 상기 다른 참가자들의 각각의 행동에 기반하여, 상기 자신의 행동을 결정하는 단계를 더 포함하는,비-일시적인 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2021-0158836", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 17 항에 있어서,상기 모델 기반 강화 학습은 메타 강화 학습(meta reinforcement learning) 알고리즘을 사용하여, 구현되는,비-일시적인 컴퓨터 판독 가능한 기록 매체."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "다양한 실시예들은 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning)을 이용한 대중 인 식 예측을 위한 컴퓨터 장치 및 그의 방법을 제공한다. 다양한 실시예들은, 추론 모듈이 다른 참가자들의 각각의 다음 행동을 예측하고, 정책 모듈이 이를 활용하여 자신의 다음 행동을 결정하는 기술로서, 고차원 추론이 가능 한 기술이다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "다양한 실시예들은 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning; MB RL)을 이용한 대중 인식 예측을 위한 컴퓨터 장치 및 그의 방법에 관한 것이다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "다수가 경쟁하는 실제 환경에서는 다른 참가자들의 의도나 인식을 예측하는 것이 중요하다. 참가자들의 의사결 정에 따라 경쟁률이 결정되는 각종 입시, 사용자들의 예약 경쟁이 치열한 전국적 숙박/교통/의료 통합 예약시스 템, 참가자들의 시장 인식에 따라 주가가 결정되는 주식 시장 등이 전형적인 예이다. 이러한 상황은 다른 참가 자들의 의도를 예측하여 의사결정을 내리는 추론 문제로 볼 수 있다. 이 문제에 예측이나 추론이 가능한 기존의 인공지능 기술을 단순 적용하는 것은 가능하지만, 기존의 기술들이 고려하지 않고 있는 이 문제의 핵심은 다른 참가자들이 모두 동일하지 않다는 것과 다른 참가자들 역시 역추론 이 가능하다는 점이다. 기존의 알고리즘은 설계의 편의성을 위해 다른 참가자들 모두를 단일 유형으로 가정하거 나, 참가자들 각각의 의도나 인식을 예측하기보다는 전체적인 행동의 추세를 학습한다. 그러나, 실제 환경에서 는 다양한 종류의 참가자들이 존재하기에 성능이 낮아진다. 또한 새로운 참가자가 등장하면(대부분의 참가자들 은 기존에 상대해 본 참가자들임에도 불구하고), 처음부터 다시 학습해야 하는 비효율이 발생한다. 따라서, 경 쟁 관계에 있는 각 참가자의 현재 인지 상태를 추적하며 그에 맞는 추론을 할 수 있는 고차원 추론 기술이 필요 하다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "다양한 실시예들은 경쟁 환경에서 모델 기반 강화 학습을 이용한 대중 인식 예측을 위한 컴퓨터 장치 및 그의 방법을 제공한다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "다양한 실시예들에 따른 컴퓨터 장치는, 메모리, 및 상기 메모리와 연결되고, 상기 메모리에 저장된 적어도 하 나의 명령을 실행하도록 구성된 프로세서를 포함하고, 상기 프로세서는, 복수의 참가자들로 구성되는 경쟁 환경 에서 모델 기반 강화 학습을 이용하여, 상기 참가자들의 각각의 행동을 예측하도록 구성될 수 있다. 다양한 실시예들에 따른 컴퓨터 장치의 방법은, 복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학 습을 이용하여, 상기 참가자들의 각각의 행동을 예측하는 단계를 포함할 수 있다. 다양한 실시예들에 따른 비-일시적인 컴퓨터 판독 가능한 기록 매체는, 복수의 참가자들로 구성되는 경쟁 환경 에 대한 예측 방법을 컴퓨터 장치에 실행시키기 위한 하나 이상의 프로그램들이 기록되어 있으며, 상기 방법은, 복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학습을 이용하여, 상기 참가자들의 각각의 행동을 예측하는 단계를 포함할 수 있다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "다양한 실시예들은, 모델 기반 강화 학습을 이용하여 경쟁 환경의 참가자들의 각각의 행동을 예측할 수 있다. 즉, 기존의 기술들은 다른 참가자들을 모두 단일 유형으로 가정하거나 참가자들의 전체적인 행동의 추세를 학습 하는 반면, 다양한 실시예들은 참가자들의 각각의 인식을 추정하고 다음 행동을 예측할 수 있다. 이 때, 다양한 실시예들은 다른 참가자들의 다음 행동들을 기반으로, 자신의 다음 행동을 보다 적절하게 결정할 수 있다. 이와 같이, 다양한 실시예들은 모델 기반 강화 학습을 이용함으로써, 고차원 추론이 가능하다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 문서의 다양한 실시예들이 첨부된 도면을 참조하여 설명된다. 도 1은 다양한 실시예들에 따른 컴퓨터 장치의 구성을 개략적으로 도시하는 도면이다. 도 2는 도 1의 프로 세서의 구성을 개략적으로 도시하는 도면이다. 도 3은 도 1의 프로세서의 구성을 예시적으로 도시하 는 도면이다. 도 1을 참조하면, 컴퓨터 장치는 입력 모듈, 출력 모듈, 메모리, 또는 프로세서 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 컴퓨터 장치의 구성 요소들 중 적어도 하나가 생략될 수 있으며, 적어도 하나의 다른 구성 요소가 추가될 수 있다. 어떤 실시예에서, 컴퓨터 장치의 구성 요소들 중 적어도 두 개가 하나의 통합된 회로로 구현될 수 있다. 입력 모듈은 컴퓨터 장치의 적어도 하나의 구성 요소에 사용될 신호를 입력할 수 있다. 입력 모듈 은, 사용자가 컴퓨터 장치에 직접적으로 신호를 입력하도록 구성되는 입력 장치, 주변의 변화를 감지 하여 신호를 발생하도록 구성되는 센서 장치, 또는 외부 기기로부터 신호를 수신하도록 구성되는 수신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 입력 장치는 마이크로폰(microphone), 마우스(mouse) 또는 키보드 (keyboard) 중 적어도 하나를 포함할 수 있다. 어떤 실시예에서, 입력 장치는 터치를 감지하도록 설정된 터치 회로(touch circuitry) 또는 터치에 의해 발생되는 힘의 세기를 측정하도록 설정된 센서 회로 중 적어도 하나를 포함할 수 있다. 이 때, 입력 모듈은 PPG 센서를 포함할 수 있다. 출력 모듈은 컴퓨터 장치의 외부로 정보를 출력할 수 있다. 출력 모듈은, 정보를 시각적으로 출 력하도록 구성되는 표시 장치, 정보를 오디오 신호로 출력할 수 있는 오디오 출력 장치, 또는 정보를 무선으로 송신할 수 있는 송신 장치 중 적어도 하나를 포함할 수 있다. 예를 들면, 표시 장치는 디스플레이, 홀로그램 장 치 또는 프로젝터 중 적어도 하나를 포함할 수 있다. 일 예로, 표시 장치는 입력 모듈의 터치 회로 또는 센서 회로 중 적어도 하나와 조립되어, 터치 스크린으로 구현될 수 있다. 예를 들면, 오디오 출력 장치는 스피 커 또는 리시버 중 적어도 하나를 포함할 수 있다. 일부 실시예들에 따르면, 수신 장치와 송신 장치는 통신 모듈로 구현될 수 있다. 통신 모듈은 컴퓨터 장치(10 0)에서 외부 기기와 통신을 수행할 수 있다. 통신 모듈은 컴퓨터 장치와 외부 기기 간 통신 채널을 수립하 고, 통신 채널을 통해, 외부 기기와 통신을 수행할 수 있다. 여기서, 외부 기기는 차량, 위성, 기지국, 서버 또 는 다른 컴퓨터 시스템 중 적어도 하나를 포함할 수 있다. 통신 모듈은 유선 통신 모듈 또는 무선 통신 모듈 중 적어도 하나를 포함할 수 있다. 유선 통신 모듈은 외부 기기와 유선으로 연결되어, 유선으로 통신할 수 있다. 무선 통신 모듈은 근거리 통신 모듈 또는 원거리 통신 모듈 중 적어도 하나를 포함할 수 있다. 근거리 통신 모 듈은 외부 기기와 근거리 통신 방식으로 통신할 수 있다. 예를 들면, 근거리 통신 방식은, 블루투스 (Bluetooth), 와이파이 다이렉트(WiFi direct), 또는 적외선 통신(IrDA; infrared data association) 중 적어 도 하나를 포함할 수 있다. 원거리 통신 모듈은 외부 기기와 원거리 통신 방식으로 통신할 수 있다. 여기서, 원 거리 통신 모듈은 네트워크를 통해 외부 기기와 통신할 수 있다. 예를 들면, 네트워크는 셀룰러 네트워크, 인터 넷, 또는 LAN(local area network)이나 WAN(wide area network)과 같은 컴퓨터 네트워크 중 적어도 하나를 포 함할 수 있다. 메모리는 컴퓨터 장치의 적어도 하나의 구성 요소에 의해 사용되는 다양한 데이터를 저장할 수 있다. 예를 들면, 메모리는 휘발성 메모리 또는 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 데이터는 적 어도 하나의 프로그램 및 이와 관련된 입력 데이터 또는 출력 데이터를 포함할 수 있다. 프로그램은 메모리 에 적어도 하나의 명령을 포함하는 소프트웨어로서 저장될 수 있으며, 운영 체제, 미들 웨어 또는 어플리 케이션 중 적어도 하나를 포함할 수 있다. 프로세서는 메모리의 프로그램을 실행하여, 컴퓨터 장치의 적어도 하나의 구성 요소를 제어할 수 있다. 이를 통해, 프로세서는 데이터 처리 또는 연산을 수행할 수 있다. 이 때, 프로세서는 메모 리에 저장된 명령을 실행할 수 있다. 다양한 실시예들에 따르면, 프로세서는 경쟁 환경에서 모델 기반 강화 학습(model-based reinforcement learning; MB RL)을 이용하여 대중 인식을 예측하도록 구성될 수 있다. 이를 위해, 프로세서는 도 2에 도 시된 바와 같이 추론 모듈 및 정책 모듈을 포함할 수 있다. 여기서, 경쟁 환경의 대중, 바꿔 말해 참 가자들은 일 참가자로서의 자신과 다른 참가자들로 구분될 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습 은 메타 강화 학습(meta reinforcement learning; Meta-RL) 알고리즘을 사용하여 구현될 수 있으나, 이에 제한 되지 않는다. 추론 모듈은 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로, 다른 참가자들의 각각의 현재의 인지 상태를 추적하고 다음 행동을 예측할 수 있다. 구체적으로, 추론 모듈은 이전 상태에 기반하여, 다른 참가자들의 각각의 현재의 인지 상태를 추적하고 다음 행동을 예측할 수 있다. 이 때, 추론 모듈은 다른 참가자들의 각각에 대해 다음 행동을 예측하기 때문에, 인공지능 알고리즘을 사용함에 있어서 흔히 겪을 수 있 는 블랙박스 문제가 방지될 수 있다. 그리고, 추론 모듈은 적어도 하나의 다른 참가자가 추가되거나 제거 되는 변화된 환경에서도 이전에 학습한 정보를 그대로 사용할 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습이 메타 강화 학습 알고리즘을 사용하여 구현되는 경우, 추론 모듈 은 도 3에 도시된 바와 같이 시계열 데이터 학습이 가능한 장단기메모리(long short term memory; LSTM) 및 제 1 순방향신경망(feedforward neural network; NN)으로 이루어질 수 있다. 도 3에서, St 는 이전 라운드(last round)의 상태(state), 즉 모든 참가자들의 이전 라운드 행동을 나타내고, at-1은 자신의 이전 행동(last action)을 나타내며, Pt는 다른 참가자들에 대해 예측되는 다음 행동들(predicted actions)을 나타낼 수 있다. 구체적으로, 장단기메모리는 이전 라운드의 상태 및 자신의 이전 행동에 기반하여, 다른 참가자들의 현재 상태를 추적하고, 제 1 순방향신경망은 다른 참가자들의 이전 행동들로부터 다른 참가자 들의 다음 행동들을 각각 예측할 수 있다. 이를 통해, 추론 모듈은 다른 참가자들의 각각에 대해 현재의 인지 상태를 추적하고 다음 행동을 예측할 수 있다. 정책 모듈은 추론 모듈을 통해 다른 참가자들에 대해 예측되는 다음 행동들에 기반하여, 자신의 다음 행동을 결정할 수 있다. 이 때, 정책 모듈은 이전 상태와 자신의 이전 행동에 더 기반하여, 자신의 다음 행동을 결정할 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습이 메타 강화 학습 알고리즘을 사용하여 구현되는 경우, 정책 모듈 은 도 3에 도시된 바와 같이 제 2 순방향신경망(NN)으로 이루어질 수 있다. 도 3에서, Rt-1은 이전 보 상 기록(last rewards)을 나타내고, at는 자신의 다음 행동을 나타낸다. 구체적으로, 제 2 순방향신경망은 이전 라운드의 상태, 자신의 이전 행동, 이전 보상 기록 및 다른 참가자들에 대해 예측되는 다음 행동들을 종합 하여, 자신의 다음 행동을 결정할 수 있다. 도 4, 도 5, 도 6, 및 도 7은 다양한 실시예들에 따른 컴퓨터 장치의 동작 성능을 설명하기 위한 도면들이 다. 다양한 실시예들에 따른 컴퓨터 장치의 동작 성능을 확인하기 위해, 케인스 미인대회(Keynesian beauty contest)를 기반으로, 다양한 시뮬레이션들을 수행하였다. 그 결과, 도 4, 도 5, 도 6, 및 도 7에 도시된 바와 같은 결과가 도출되었다. 도 4 및 도 5에서, x 축은 라운드 넘버(round number)를 나타내고, y 축은 라운드당 누적 보상(cumulative reward per round)를 나타내며, 음영 영역은 표준 편차를 나타낸다. 도 6에서, x 축은 세대 넘버(generation number)를 나타내고, y 축은 참가자들, 즉 에이전트(agent)들의 수를 나타내며, 음영 영 역은 표준 편차를 나타낸다. 도 7에서, x 축은 라운드 넘버를 나타내고, y 축은 해당 라운드의 보상과 관련된 넘버(rewarded number)를 나타내며, 음영 영역은 표준 편차를 나타낸다. 첫 번째 시뮬레이션을 위해, 10 명의 참가자들이 경쟁하는 경쟁 환경이 구성되었다. 즉, 해당 경쟁 환경에서, 일 참가자로서의 자신은 9 명의 다른 참가자들과 경쟁한다. 이러한 경우, 도 4에 도시된 바와 같이, 본 개시의 메타 강화 학습 알고리즘과 같은 모델 기반 강화 학습을 이용하는 경우(본 개시의 경우, Meta-RL로 표기됨), 다 른 강화 학습 기술들에 비해, 라운드 넘버가 커질수록 라운드당 누적 보상이 점차로 증가되었다. 이는, 컴퓨터 장치가 라운드가 진행될수록, 자신의 다음 행동을 보다 적절하게 결정하였음을 의미한다. 두 번째 시뮬레이션을 위해, 30 명의 참가자들이 경쟁하는 경쟁 환경이 구성되었다. 즉, 해당 경쟁 환경에서, 일 참가자로서의 자신은 29 명의 다른 참가자들과 경쟁한다. 이러한 경우, 도 5에 도시된 바와 같이, 본 개시의 메타 강화 학습 알고리즘과 같은 모델 기반 강화 학습을 이용하는 경우(본 개시의 경우, Meta-RL로 표기됨), 기 존의 기술들에 비해, 라운드 넘버가 커질수록 라운드당 누적 보상이 점차로 증가되었다. 이는, 컴퓨터 장치 가 라운드가 진행될수록, 자신의 다음 행동을 보다 적절하게 결정하였음을 의미한다. 세 번째 시뮬레이션을 위해, 적자생존 법칙이 적용된 경쟁 환경이 10 세대(generation)들에 걸쳐 진행되었다. 여기서, 한 세대는 10K 라운드들로 구성된다. 참가자들의 수는 무작위로 결정되었다. 이 때, 각 세대에서, 누적 보상에 기반하여, 상위 25 %의 에이전트들은 두 배로 되고, 하위 25 %의 에이전트들은 제거되었다. 이러한 경우, 도 6에 도시된 바와 같이, 본 개시의 메타 강화 학습 알고리즘과 같은 모델 기반 강화 학습을 이용하는 경우(본 개시의 경우, Meta-RL로 표기됨), 기존의 기술들에 비해, 모든 세대들에 걸쳐 에이전트들의 생존율 (survival rate)이 가장 높았다. 이는, 컴퓨터 장치가 참가자들의 수에 관계없이, 많은 수의 라운드들이 진행되는 동안, 견고하게 동작할 수 있음을 의미한다. 네 번째 시뮬레이션을 위해, 10 명의 참가자들이 경쟁하는 경쟁 환경이 구성되었다. 이러한 경우, 도 7에 도시 된 바와 같이, 본 개시의 메타 강화 학습 알고리즘과 같은 모델 기반 강화 학습을 이용하는 경우(본 개시의 경 우, Meta-RL로 표기됨), 기존의 기술들에 비해, 더 빨리 수렴하였다. 즉, 내쉬 균형(Nash equilibrium)에 접근 하였으며, 이는 최적의 정책이 선택되었음을 의미한다. 바꿔 말해, 이는, 컴퓨터 장치가 라운드가 진행될 수록, 자신의 다음 행동을 보다 적절하게 결정하였음을 의미한다. 다섯 번째 시뮬레이션을 위해, 동일한 타입의 참가자들로 이루어지는 복수의 그룹들이 협력하는 경쟁 환경이 구 성되었다. 이러한 경우, 하기 [표 1]과 같이, 본 개시의 메타 강화 학습 알고리즘과 같은 모델 기반 강화 학습 을 이용하는 경우(본 개시의 경우, Meta-RL로 표기됨), 기존의 기술들에 비해, 각 그룹의 참가자들에 대한 총 보상 량이 가장 많았다. 이는, 컴퓨터 장치가 참가자들이 협력하는 경쟁 환경에서도, 자신의 다음 행동을 보다 적절하게 결정하였음을 의미한다. 표 1"}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "전술한 바와 같은 다양한 시뮬레이션들을 통해, 다양한 실시예들에 따른 컴퓨터 장치의 고차원 추론 능력 과 내쉬 균형으로의 수렴 특성이 확인되었다. 즉, 다양한 수의 참가자들과 다양한 타입들의 경쟁 환경들에서도, 다양한 실시예들에 따른 컴퓨터 장치가 우수한 동작 성능을 나타냄이 확인되었다. 도 8은 다양한 실시예들에 따른 컴퓨터 장치의 방법을 도시하는 도면이다. 도 8을 참조하면, 컴퓨터 장치는 810 단계에서 경쟁 환경의 이전 상태 및 자신의 이전 행동을 확인할 수 있다. 이전 상태는 이전 라운드의 상태로서, 모든 이전 행동들을 나타낼 수 있다. 이 때, 프로세서의 추론 모듈이 경쟁 환경의 이전 상태 및 자신의 이전 행동을 확인할 수 있다. 다음으로, 컴퓨터 장치는 820 단계에서 다른 참가자들의 이전 행동들을 기반으로, 다른 참가자들의 각각의 현재 인지 상태를 추적할 수 있다. 이 때, 프로세서의 추론 모듈은 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로, 다른 참가자들의 이전 행동들을 기반으로, 다른 참가자들의 각각의 현재 인지 상태를 추적할 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습이 도 3에 도시된 바와 같은 메타 강화 학습 알고리 즘을 사용하여 구현되는 경우, 장단기메모리가 이전 라운드의 상태 및 자신의 이전 행동에 기반하여, 다른 참가자들의 각각의 현재 인지 상태를 추적할 수 있다. 다음으로, 컴퓨터 장치는 830 단계에서 다른 참가자들의 다음 행동들을 각각 예측할 수 있다. 이 때, 프로 세서의 추론 모듈은 다른 참가자들의 각각에게 부여된 모델을 기반으로, 다른 참가자들의 다음 행동 들을 각각 예측할 수 있다. 즉, 추론 모듈은 다른 참가자들의 각각의 현재 인지 상태를 기반으로, 다른 참 가자들의 다음 행동들을 각각 예측할 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습이 도 3에 도시된 바와 같은 메타 강화 학습 알고리즘을 사용하여 구현되는 경우, 제 1 순방향신경망이 다른 참가자들의 이전 행 동들로부터 다른 참가자들의 다음 행동들을 각각 예측할 수 있다. 다음으로, 컴퓨터 장치는 840 단계에서 자신의 다음 행동을 결정할 수 있다. 이 때, 프로세서의 정책 모듈은 추론 모듈을 통해 다른 참가자들에 대해 예측되는 다음 행동들에 기반하여, 자신의 다음 행동 을 결정할 수 있다. 여기서, 정책 모듈은 이전 상태와 자신의 이전 행동에 더 기반하여, 자신의 다음 행동 을 결정할 수 있다. 일 실시예에 따르면, 모델 기반 강화 학습이 도 3에 도시된 바와 같은 메타 강화 학습 알고 리즘을 사용하여 구현되는 경우, 제 2 순방향신경망이 이전 라운드의 상태, 자신의 이전 행동, 이전 보상 기록, 및 다른 참가자들에 대해 예측되는 다음 행동들을 종합하여, 자신의 다음 행동을 결정할 수 있다. 전술한 바와 같이, 다양한 실시예들은, 추론 모듈이 다른 참가자들의 각각의 다음 행동을 예측하고, 정책 모듈이 이를 활용하여 자신의 다음 행동을 결정하는 기술로서, 고차원 추론이 가능한 기술이다. 요컨대, 기존의 기술들은 다른 참가자들을 모두 단일 유형으로 가정하거나, 참가자들의 각각의 의도나 인식을 예측하기 보다는 전체적인 행동의 추세를 학습하는 반면, 다양한 실시예들은 참가자들의 각각의 인식을 추정하고 다음 행 동을 예측한다. 이를 토대로, 다양한 실시예들은 원하는 추론 수준 및 참가자들에 따라서 모델을 다양하게 구성할 수 있다. 그 리고, 참가자들의 수가 많은 환경에서 비슷한 유형의 참가자들을 묶어서 하나의 모델에서 처리하는 등의 다양한 확장이 가능하다. 또한, 다양한 실시예들의 기본 원리는 특정 알고리즘에 비의존적이므로, 다양한 실시예들은 GPT 계열의 자기주의집중 방식의 신경망과 같은 최신 기술을 이용해 구현 가능하다. 이 경우 복잡도가 높은 상 황에서 운용이 가능하다. 아울러, 다양한 실시예들은 매 시점 참가자들의 각각의 인지 상태 추적이나 행동 예측을 명확하게 얻을 수 있기 때문에, 기존의 기술들 사용 시 야기되는 블랙박스 문제도 해결할 수 있다. 나아가, 일부 참가자가 제외되고 새 로운 참가자가 추가되는 변화된 환경에서도 기존 참가자들에 대한 모델을 그대로 사용할 수 있다. 다양한 실시예들은 다양한 분야들에 응용 또는 적용 가능하다. 코로나19를 겪으면서 많은 산업이 어려움에 처했 지만, 금융, 헬스케어 등의 분야는 큰 폭으로 성장했다. 국내 주식시장 참여자 수나 거래액이 크게 증가한 것으 로 볼 때 로보어드바이저 등의 AI 핀테크 분야에 고차원 추론 기술이 널리 쓰일 수 있다. 또한 인구의 고령화 등으로 인해 헬스케어 분야에 대한 투자와 수요는 지속적으로 증가할 것으로 보이는 바 의료 AI 분야에도 적용 가능한 고차원 추론 기술은 잠재력이 클 것으로 기대된다. 이에 따라, 높은 수준의 전문가나 인간의 추론이 필 요했던 일을 인공지능을 이용하여 일반화하고, 이를 대중화하고자 하는 모든 기업 및 서비스에 적용할 수 있다. 구체적으로, 다양한 실시예들은 다음과 같은 분야들에 응용 또는 적용 가능하다. 첫 번째는, AI 핀테크 분야이다. 예로부터, 주가 전망, 원자재가격 예측 등 여러 금융분야에서 기계학습이 활용 되었다. 그러나, 많은 변수와 불확실성 때문에 정확도가 높지 않고, 그나마도 규칙 기반 알고리즘에 기반하는 경우가 많다. 최근 \"인간의 간섭을 가능한 최소화한 인공지능 투자자문역\"을 표방한 로보어드바이저들이 등장하 고 있으나, 여전히 주로 이용하는 변수(예: 주가수익비율, 주가순자산비율)나 경계값은 인간에 의해 정의되거나 결정된다. 이러한 상황에서 고차원 추론 기술은 그 동안 중요하게 여겨지지 않던 변수나 상관관계를 제시해 주 고, 이를 통해 더 높은 수준의 금융 인공지능을 개발할 수 있다. 두 번째는, 입시 경향 분석 시스템이다. 고차원 추론 기술은 지원자들의 전체적 의사결정에 따라서 경쟁률이 결 정되는 각종 입시에서 그 경향성을 분석하고 결과를 예측하는 시스템 개발에 이용될 수 있다. 이를 통해, 시스 템 사용자는 보다 경쟁률이 낮은 모집단위에 지원하거나 예상 커트라인을 얻는 등의 효과를 누릴 수 있다. 세 번째는, 숙박/교통/의료 통합 예약 및 관제 시스템이다. 소비자를 위한 시스템으로는 다른 소비자들의 행동 을 예측하여 특정시간대의 혼잡도나 가능한 대체 슬롯을 제공해 주는 시스템을 개발할 수 있다. 또한, 이는 판 매자 혹은 서비스 제공자에게 가장 높은 판매수익을 기대할 수 있는 최적의 시간대나 가격을 제시해 주는 시스 템으로도 확장될 수 있다. 네 번째는, 의료 AI 분야이다. 최근 임상 진단 및 치료를 돕는 인공지능 개발이 많이 이루어지고 있다. 하지만, 여전히 특정 분야에서 특정 의료기기로 수집된 정보만을 학습, 분류할 수 있는 경우가 대부분이다. 또한, 결과 를 도출하는 과정에 대한 설명보다는 정상/비정상 식의 이분법적인 결과를 주는 경우가 많다. 고차원 추론 기술 을 활용하면, 결과에 도달하는 과정을 보다 자세히 알 수 있다. 나아가, 다양한 의료기기를 통해 수집된 정보를 조합하여 진단하는 인공지능을 개발할 수 있다. 다양한 실시예들에 따른 컴퓨터 장치는 메모리, 및 메모리와 연결되고, 메모리에 저장된 적어도 하나의 명령을 실행하도록 구성된 프로세서를 포함할 수 있다. 다양한 실시예들에 따르면, 프로세서는 복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강화 학습을 이용하여, 참가자들의 각각의 행동을 예측하도록 구성될 수 있다. 다양한 실시예들에 따르면, 참가자들은 일 참가자로서의 자신 및 다른 참가자들을 포함할 수 있다. 다양한 실시예들에 따르면, 프로세서는 다른 참가자들의 각각에게 모델을 부여하고, 이를 기반으로 다른 참가자들의 각각의 행동을 예측하도록 구성되는 추론 모듈을 포함할 수 있다. 다양한 실시예들에 따르면, 프로세서는 다른 참가자들의 각각의 행동에 기반하여, 자신의 행동을 결정하도 록 구성되는 정책 모듈을 더 포함할 수 있다. 다양한 실시예들에 따르면, 모델 기반 강화 학습은 메타 강화 학습(meta reinforcement learning) 알고리즘을 사용하여, 구현될 수 있다. 다양한 실시예들에 따르면, 추론 모듈은, 경쟁 환경의 이전 상태, 및 자신의 이전 행동에 기반하여, 다른 참가자들의 이전 행동들을 기반으로 다른 참가자들의 각각의 현재 인지 상태를 추적하도록 구성되는 장단기메모 리, 및 다른 참가자들의 각각의 현재 인지 상태로부터 다른 참가자들의 다음 행동들을 각각 예측하도록 구 성되는 제 1 순방향신경망을 포함할 수 있다. 다양한 실시예들에 따르면, 정책 모듈은, 경쟁 환경의 이전 상태, 자신의 이전 행동, 및 다른 참가자들의 다음 행동들에 기반하여, 자신의 다음 행동을 결정하도록 구성되는 제 2 순방향신경망을 포함할 수 있다. 다양한 실시예들에 따르면, 제 2 순방향 신경망은 경쟁 환경의 이전 보상 기록에 더 기반하여, 자신의 다 음 행동을 결정하도록 구성될 수 있다. 다양한 실시예들에 따르면, 경쟁 환경은 복수의 라운드들에 걸쳐 진행되면서 참가자들 중 일부가 제거되는 적자 생존 환경, 또는 참가자들이 복수의 그룹들로 구분되어 그룹들의 각각의 참가자들이 협력하는 협력 환경 중 적 어도 하나를 포함할 수 있다. 다양한 실시예들에 따른 컴퓨터 장치의 방법은, 복수의 참가자들로 구성되는 경쟁 환경에서 모델 기반 강 화 학습을 이용하여, 참가자들의 각각의 행동을 예측하는 단계를 포함할 수 있다. 다양한 실시예들에 따르면, 참가자들은 일 참가자로서의 자신 및 다른 참가자들을 포함할 수 있다. 다양한 실시예들에 따르면, 참가자들의 각각의 행동을 예측하는 단계는, 추론 모듈에 의해, 다른 참가자들 의 각각에게 모델을 부여하고, 이를 기반으로 다른 참가자들의 각각의 행동을 예측하는 단계(810 단계, 820 단 계, 및 830 단계)를 포함할 수 있다. 다양한 실시예들에 따르면, 참가자들의 각각의 행동을 예측하는 단계는, 정책 모듈에 의해, 다른 참가자들의 각 각의 행동에 기반하여, 자신의 행동을 결정하는 단계(840 단계)를 더 포함할 수 있다. 다양한 실시예들에 따르면, 모델 기반 강화 학습은 메타 강화 학습 알고리즘을 사용하여, 구현될 수 있다. 다양한 실시예들에 따르면, 추론 모듈은 장단기메모리 및 제 1 순방향신경망을 포함할 수 있다. 다양한 실시예들에 따르면, 다른 참가자들의 각각의 행동을 예측하는 단계(810 단계, 820 단계, 및 830 단계)는, 장단기메모리에 의해, 경쟁 환경의 이전 상태, 및 자신의 이전 행동에 기반하여, 다른 참가자들 의 이전 행동들을 기반으로 상기 다른 참가자들의 각각의 현재 인지 상태를 추적하는 단계(820 단계), 및 제 1 순방향신경망에 의해, 다른 참가자들의 각각의 현재 인지 상태로부터 다른 참가자들의 다음 행동들을 각각 예측하는 단계(830 단계)를 포함할 수 있다. 다양한 실시예들에 따르면, 정책 모듈은 제 2 순방향신경망을 포함할 수 있다. 다양한 실시예들에 따르면, 자신의 행동을 결정하는 단계(840 단계)는, 제 2 순방향신경망에 의해, 경쟁 환경의 이전 상태, 자신의 이전 행동, 및 다른 참가자들의 다음 행동들에 기반하여, 자신의 다음 행동을 결정하 는 단계를 포함할 수 있다. 다양한 실시예들에 따르면, 자신의 다음 행동을 결정하는 단계(840 딘계)는, 제 2 순방향신경망에 의해, 경쟁 환경의 이전 보상 기록에 더 기반하여, 자신의 다음 행동을 결정할 수 있다. 다양한 실시예들에 따르면, 경쟁 환경은 복수의 라운드들에 걸쳐 진행되면서 참가자들 중 일부가 제거되는 적자 생존 환경, 또는 참가자들이 복수의 그룹들로 구분되어 그룹들의 각각의 참가자들이 협력하는 협력 환경 중 적 어도 하나를 포함할 수 있다. 상술한 방법은 컴퓨터에서 실행하기 위해 컴퓨터 판독 가능한 기록 매체에 저장된 컴퓨터 프로그램으로 제공될 수 있다. 매체는 컴퓨터로 실행 가능 한 프로그램을 계속 저장하거나, 실행 또는 다운로드를 위해 임시 저장하 는 것일 수도 있다. 또한, 매체는 단일 또는 수개 하드웨어가 결합된 형태의 다양한 기록 수단 또는 저장수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트워크 상에 분산 존재하는 것일 수 도 있다. 매체의 예시로는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD 와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등을 포함하여 프로그램 명령어가 저장되도록 구성된 것이 있을 수 있다. 또한, 다른 매체 의 예시로, 애플리케이션을 유통하는 앱 스토어나 기타 다양한 소프트웨어를 공급 내지 유통하는 사이트, 서버 등에서 관리하는 기록매체 내지 저장매체도 들 수 있다. 본 개시의 방법, 동작 또는 기법들은 다양한 수단에 의해 구현될 수도 있다. 예를 들어, 이러한 기법들은 하드 웨어, 펌웨어, 소프트웨어, 또는 이들의 조합으로 구현될 수도 있다. 본원의 개시와 연계하여 설명된 다양한 예 시적인 논리적 블록들, 모듈들, 회로들, 및 알고리즘 단계들은 전자 하드웨어, 컴퓨터 소프트웨어, 또는 양자의 조합들로 구현될 수도 있음을 통상의 기술자들은 이해할 것이다. 하드웨어 및 소프트웨어의 이러한 상호 대체를명확하게 설명하기 위해, 다양한 예시적인 구성요소들, 블록들, 모듈들, 회로들, 및 단계들이 그들의 기능적 관 점에서 일반적으로 위에서 설명되었다. 그러한 기능이 하드웨어로서 구현되는지 또는 소프트웨어로서 구현되는 지의 여부는, 특정 애플리케이션 및 전체 시스템에 부과되는 설계 요구사항들에 따라 달라진다. 통상의 기술자 들은 각각의 특정 애플 리케이션을 위해 다양한 방식들로 설명된 기능을 구현할 수도 있으나, 그러한 구현들은 본 개시의 범위로부터 벗어나게 하는 것으로 해석되어서는 안된다. 하드웨어 구현에서, 기법들을 수행하는 데 이용되는 프로세싱 유닛들은, 하나 이상의 ASIC들, DSP들, 디지털 신 호 프로세싱 디바이스들(digital signal processing devices; DSPD들), 프로그램가능 논리 디바이스들 (programmable logic devices; PLD들), 필드 프로그램가능 게이트 어레이들(field programmable gate arrays; FPGA들), 프로세서들, 제어기들, 마이크로제어기들, 마이크로프로세서들, 전자 디바이스들, 본 개시에 설명된 기능들을 수행하도록 설계된 다른 전자 유닛들, 컴퓨터, 또는 이들의 조합 내에서 구현될 수도 있다. 따라서, 본 개시와 연계하여 설명된 다양한 예시적인 논리 블록들, 모듈들, 및 회로들은 범용 프로세서, DSP, ASIC, FPGA나 다른 프로그램 가능 논리 디바이스, 이산 게이트나 트랜지스터 로직, 이산 하드웨어 컴포넌트들, 또는 본원에 설명된 기능들을 수행하도록 설계된 것들의 임의의 조합으로 구현되거나 수행될 수도 있다. 범용 프로세서는 마이크로프로세서일 수도 있지만, 대안으로, 프로세서는 임의의 종래의 프로세서, 제어기, 마이크로 제어기, 또는 상태 머신일 수도 있다. 프로세서는 또한, 컴퓨팅 디바이스들의 조합, 예를 들면, DSP와 마이크로 프로세서, 복수의 마이크로프로세서들, DSP 코어와 연계한 하나 이상의 마이크로프로세서들, 또는 임의의 다른 구성의 조합으로서 구현될 수도 있다. 펌웨어 및/또는 소프트웨어 구현에 있어서, 기법들은 랜덤 액세스 메모리(random access memory; RAM), 판독 전 용 메모리(read-only memory; ROM), 비휘발성 RAM(non-volatile random access memory; NVRAM), PROM(programmable read-only memory), EPROM(erasable programmable read-only memory), EEPROM(electrically erasable PROM), 플래시 메모리, 컴팩트 디스크(compact disc; CD), 자기 또는 광학 데이 터 스토리지 디바이스 등과 같은 컴퓨터 판독가능 매체 상에 저장된 명령들로서 구현될 수도 있다. 명령들은 하 나 이상의 프로세서들에 의해 실행 가능할 수도 있고, 프로세서(들)로 하여금 본 개시에 설명된 기능의 특정 양 태들을 수행하게 할 수도 있다. 이상 설명된 실시예들이 하나 이상의 독립형 컴퓨터 시스템에서 현재 개시된 주제의 양태들을 활용하는 것으로 기술되었으나, 본 개시는 이에 한정되 지 않고, 네트워크나 분산 컴퓨팅 환경과 같은 임의의 컴퓨팅 환경과 연 계하여 구현될 수도 있다. 또 나아가, 본 개시에서 주제의 양상들은 복수의 프로세싱 칩들이나 장치들에서 구현 될 수도 있고, 스토리지는 복수의 장치들에 걸쳐 유사하게 영향을 받게 될 수도 있다. 이러한 장치들은 PC들, 네트워크 서버들, 및 휴대용 장치들을 포함할 수도 있다."}
{"patent_id": "10-2021-0158836", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본 개시가 일부 실시예들과 관련하여 설명되었지만, 본 개시의 발명이 속하는 기술분야의 통상의 기술자가 이해 할 수 있는 본 개시의 범위를 벗어나지 않는 범위에서 다양한 변형 및 변경이 이루어질 수 있다. 또한, 그러한 변형 및 변경은 본 명세서에 첨부된 청구범위 내에 속하는 것으로 생각되어야 한다."}
{"patent_id": "10-2021-0158836", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 다양한 실시예들에 따른 컴퓨터 장치의 구성을 도시하는 도면이다. 도 2는 도 1의 프로세서의 구성을 개략적으로 도시하는 도면이다. 도 3은 도 1의 프로세서의 구성을 예시적으로 도시하는 도면이다. 도 4, 도 5, 도 6, 및 도 7은 다양한 실시예들에 따른 컴퓨터 장치의 동작 성능을 설명하기 위한 도면들이다. 도 8은 다양한 실시예들에 따른 컴퓨터 장치의 방법을 도시하는 도면이다."}
