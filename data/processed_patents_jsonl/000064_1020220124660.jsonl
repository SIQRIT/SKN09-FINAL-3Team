{"patent_id": "10-2022-0124660", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0161317", "출원번호": "10-2022-0124660", "발명의 명칭": "AR 뷰를 생성하는 방법 및 장치", "출원인": "삼성전자주식회사", "발명자": "지서원"}}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "제1 전자장치(1000)에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법에 있어서 , 제2 전자장치(2000)에서 촬영된 공간 영상, 및 상기 공간에서 적어도 하나의 AR(augmented reality) 객체에 대한 제2 사용자의 AR 경험 정보를 획득하는 단계;상기 공간 영상, 및 상기 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경로(750, 760) 를 획득하는 단계; 상기 적어도 하나의 경로에 기초하여, AR 뷰 (1231)를 생성하는 단계; 및상기 생성된 AR 뷰를 출력하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 적어도 하나의 경로를 획득하는 단계는, 상기 공간 영상 및 상기 적어도 하나의 AR 객체에 대한 정보에 기초하여, 상기 공간을 모델링 (720)하는 단계;상기 모델링된 공간 및 상기 공간 영상에 기초하여, 공간 뷰에 대한 제1 경로 (750)를 획득하는 단계; 및상기 제1 경로 및 상기 적어도 하나의 AR 객체에 대한 정보에 기초하여, 객체 뷰에 대한 제2 경로 (760)를 획득하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항에 있어서, 상기 AR 뷰를 생성하는 단계는,상기 공간 영상 및 상기 제1 경로에 기초하여, 상기 제1 경로에 대응하는 공간 뷰를 획득 하는 단계; 상기 적어도 하나의 객체에 대한 객체 모델 및 상기 제2 경로에 기초하여, 상기 제2 경로에 대응하는 객체 뷰를획득하는 단계; 및상기 공간 뷰 및 상기 객체 뷰를 합성하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 공간 뷰를 획득하는 단계;는, 상기 공간 영상에서 추출한 적어도 하나의 프레임을 워핑(warping)하는 단계; 및상기 워핑된 적어도 하나의 프레임을 융합(fusion)하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3 항에 있어서, 상기 객체 뷰를 획득하는 단계;는, 상기 공간의 스타일 특징을 추출하는 단계; 및상기 공간의 스타일 특징에 기초하여, 상기 객체의 스타일을 변환하는 단계;를 더 포함하는, 방법.공개특허 10-2023-0161317-3-청구항 6 제1 항에 있어서, 상기 방법은, 상기 제2 사용자의 AR 경험정보 또는 상기 공간의 분석 결과 중 적어도 하나에 기초하여, 배치 가능한 AR 객체를 추천하는 단계;를 더 포함하는, 방법."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 사용자를 위한 AR 뷰를 생성하는 제1 전자장치에 있어서,디스플레이;통신부;적어도 하나의 명령어(instruction)를 포함하는 프로그램을 저장하는 저장부; 및상기 저장부에 저장된 적어도 하나의 명령어를 실행하는 적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는 상기 적어도 하나의 명령어를 실행함으로써,상기 통신부를 통해 제2 전자장치에서 촬영된 공간 영상, 및 상기 공간에서 적어도 하나의 AR(augmentedreality) 객체에 대한 제2 사용자의 AR 경험 정보를 획득하고, 상기 공간 영상, 및 상기 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경로를획득하고, 상기 적어도 하나의 경로에 기초하여, AR 뷰를 생성하고, 상기 디스플레이에 상기 생성된 AR 뷰를 출력하는, 전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 적어도 하나의 프로세서는, 상기 공간 영상 및 상기 적어도 하나의 AR 객체에 대한 정보에 기초하여, 상기 공간을 모델링 하고, 상기 모델링된 공간 및 상기 공간 영상에 기초하여, 공간 뷰에 대한 제1 경로 를 획득하고, 상기 제1 경로 및 상기 적어도 하나의 AR 객체에 대한 정보에 기초하여, 객체 뷰에 대한 제2 경로 를 획득하는,전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7 항에 있어서,상기 적어도 하나의 프로세서는, 상기 공간 영상 및 상기 제1 경로에 기초하여, 상기 제1 경로에 대응하는 공간 뷰를 획득 하고, 상기 적어도 하나의 객체에 대한 객체 모델 및 상기 제2 경로에 기초하여, 상기 제2 경로에 대응하는 객체 뷰를획득하고,상기 공간 뷰 및 상기 객체 뷰를 합성하여 AR 뷰를 생성하는, 전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 적어도 하나의 프로세서는, 상기 공간 영상에서 추출한 적어도 하나의 프레임을 워핑(warping)하고, 상기 워핑된 적어도 하나의 프레임을 융합(fusion)하여, 공개특허 10-2023-0161317-4-상기 공간 뷰를 획득하는, 전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9 항에 있어서,상기 적어도 하나의 프로세서는, 상기 공간의 스타일 특징을 추출하고, 상기 공간의 스타일 특징에 기초하여, 상기 객체의 스타일을 변환하여, 상기 객체 뷰를 획득하는, 전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제7 항에 있어서, 상기 적어도 하나의 프로세서는, 상기 제2 사용자의 AR 경험정보 또는 상기 공간의 분석 결과 중 적어도 하나에 기초하여, 배치 가능한 AR 객체를 추천하는, 전자장치."}
{"patent_id": "10-2022-0124660", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1 항 내지 제6 항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법은, 제2 전자장치에서 촬영된 공간 영상, 및 공간에서 적어도 하나의 AR(augmented reality) 객체에 대한 제2 사용자의 AR 경험 정보를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생 성하는 방법은, 공간 영상, 및 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경 로를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰 를 생성하는 방법은, 적어도 하나의 경로에 기초하여, AR 뷰를 생성하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법은, 생성된 AR 뷰를 출력하는 단계; 를 포함할 수 있다."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 증강 현실(AR, augmented reality) 경험을 위한 증강 현실 뷰를 생성하는 방법 및 그 장치에 관한 것 이다. 증강 현실 또는 AR은 현실 세계의 물리적 환경 공간 내에 가상 이미지를 함께 보여주거나 현실 객체와 가 상 객체 이미지를 함께 보여주는 것을 의미한다."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "증강 현실 또는 AR은 현실 세계의 물리적 환경 공간 내에 가상 이미지를 함께 보여주거나 현실 객체와 가상 객 체 이미지를 함께 보여주는 것을 의미한다. 이와 같은 증강 현실 기술을 이용해 사용자가 증강 현실을 경험하는 것을 AR 경험이라 한다. AR 경험은 사용자가 실제로 경험하지 못한 세계를 인지하고 감지할 수 있도록 함으로써, 현실과 가상의 경계 없이 사용자 경험이 확장될 수 있다. 또한, 실제 공간에서 직접 AR 경험을 획득 한 사용자의 경험을 다른 사용자에게 공유함으로써, 시간과 공간의 제약 없이 특정 사용자의 사용자 경험이 다 른 사용자에게 확장될 수 있다."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법은, 제2 전자장치에 서 촬영된 공간 영상, 및 공간에서 적어도 하나의 AR(augmented reality) 객체에 대한 제2 사용자의 AR 경험 정 보를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법은, 공간 영상, 및 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하 나의 경로를 획득하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위 한 AR 뷰를 생성하는 방법은, 적어도 하나의 경로에 기초하여, AR 뷰를 생성하는 단계를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치에서, 제1 사용자를 위한 AR 뷰를 생성하는 방법은, 생성된 AR 뷰를 출력하는 단계;를 포함할 수 있다. 본 개시의 일 실시예에 따른 제1 사용자를 위한 AR 뷰를 생성하는 제1 전자장치는, 디스플레이; 통신부; 적어 도 하나의 명령어(instruction)를 포함하는 프로그램을 저장하는 저장부; 및 저장부에 저장된 적어도 하나의 명 령어를 실행하는 적어도 하나의 프로세서를 포함할 수 있다. 적어도 하나의 프로세서는, 적어도 하나의 명령어 를 실행함으로써 통신부를 통해 제2 전자장치에서 촬영된 공간 영상, 및 공간에서 적어도 하나의 AR(augmented reality) 객체에 대한 제2 사용자의 AR 경험 정보를 획득하도록 구성될 수 있다. 적어도 하나의 프로세서는, 적어도 하나의 명령어를 실행함으로써 공간 영상, 및 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경로를 획득하도록 구성될 수 있다. 적어도 하나의 프로세서는, 적어도 하나의 명령어를 실행함으로써 적어도 하나의 경로에 기초하여, AR 뷰를 생성하도록 구성될 수 있다. 적어도 하나의 프로세서는, 적어도 하나의 명령어를 실행함으로써 디스플레이에 생성된 AR 뷰를 출력하도록 구성될 수 있다. 한편, 본 개시의 일 실시예에 따르면, 전술한 방법을 실행하기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체를 제공한다."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참조하여 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 개시의 실시예를 상세히 설명한다. 실시 예를 설명함에 있어서 본 개시가 속하는 기술 분야에 익 히 알려져 있고 본 개시와 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명 을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되었다. 또한, 각 구성요소의 크기는 실제 크기 를 전적으로 반영하는 것이 아니다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그 리고 도면에서 본 개시를 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하 여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시 예들은 본 개시가 완전하도록 하고, 본 개시가 속하는"}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 본 개시의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 개시 는 청구항의 범주에 의해 정의될 뿐이다. 이 때, 처리 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행 될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가 능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들 을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트 럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재 되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하 다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 이 때, 본 실시예에서 사용되는 '~부'라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구성요소를 의미하며, '~부'는 어떤 역할들을 수행한다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아 니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재 생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요 소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이 터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 또한 실시예에서 ‘~부’는 하나 이상의 프로세서를 포함할 수 있다. 본 개시에 따른 인공지능(artificial intelligence, AI)과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저 장된 기 정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 나타 낼 수 있다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서 버 및/또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습 (reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 인공지능 모델은, 복수의 신경망 레이어들을 포함할 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 가질 수 있고, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신 경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트 (cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등 이 있으나, 전술한 예에 한정되지 않는다. 본 개시에서 ‘서버(server)’는 사용자 단말(User Equipment, UE) 또는 클라이언트에게 네트워크를 통해 정보 나 서비스를 제공하는 컴퓨터 시스템으로서, 서버 프로그램(server program) 또는 장치(device)를 나타낼 수 있 다. 서버는 파일 관리 등 네트워크 전체를 감시 또는 제어하거나, 상기 네트워크를 메인 프레임이나 공중망을 통해 다른 네트워크와 연결할 수 있도록 한다. 서버는 데이터, 프로그램, 파일 등과 같은 소프트웨어 자원이나 모뎀, 팩스, 공유기 등과 같은 하드웨어 자원을 공유할 수 있도록 한다. 서버는 사용자(클라이언트)의 요청에 따라 서비스를 제공할 수 있다. 서버에서는 하나 이상의 응용 프로그램들이 상호 협력적인 환경에서 분산 처리 형태로 운용될 수 있다. 본 개시에서, 증강 현실 또는 AR(Augmented Reality)은 현실 세계의 물리적 환경 공간 내에 가상 이미지를 함 께 보여주거나 현실 객체와 가상 객체 이미지를 함께 보여주는 것을 의미한다. AR 글래스 등 AR 전용 전자장치 에서는 시스루(see-through) 디스플레이를 통과해 보여지는 현실 객체와, 장치의 디스플레이에 표시되는 가상 객체 이미지가 함께 보여진다. 모바일 장치에서는 카메라를 통해 촬영되는 현실 객체와 가상 객체가 함께 디스 플레이에 표시되어 보여진다. 이미지, 배경, 주변 환경, 사물 등이 모두 가상의 이미지인 가상 현실(VR, virtual reality)과 달리, 증강 현실에서는 배경이나 주변 환경, 사물 등은 현실 객체이며 추가되는 정보만이 가상이다. 증강 현실은 부가적인 정보를 제공하는 동시에 현실의 효과를 더욱 증가시킬 수 있으므로, 다양한 산 업 분야에서 응용될 수 있다. 이와 같은 증강 현실 기술을 이용해 사용자가 증강 현실을 경험하는 것을 AR 경험이라 한다. AR 경험은 사용자 가 실제로 경험하지 못한 세계를 인지하고 감지할 수 있도록 함으로써, 현실과 가상의 경계 없이 사용자 경험이 확장될 수 있다. AR 경험은 AR 공간에서 AR 객체와 관련된 사용자의 모든 인터랙션을 포함할 수 있다. 예를 들어, 사용자가 실 제 공간을 카메라로 촬영한 영상에서 AR 객체를 배치하는 경우, AR 경험 정보는 배치한 AR 객체의 식별 정보(예 를 들어, 종류, 모델, 색상 등), 배치한 AR 객체의 배치 정보(예를 들어, 위치, 방향 등), 배치를 고려한 AR 객 체의 식별 정보, 배치를 고려한 AR 객체의 배치 정보를 포함할 수 있다 . AR 뷰는, 카메라로 촬영된 현실 객체 이미지에 가상 객체 이미지를 오버레이한 뷰를 의미한다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은, 본 개시의 일 실시예에 따른, AR 뷰를 도시한 것이다. 도 1을 참조하면, AR뷰는 전자장치의 카메라로 촬영된 현실 객체 이미지에, 가상 객체 이미지 를 오버레이한 이미지를 의미한다. 현실 객체는 전자장치가 실제로 위치하는 공간과 같은 공간에 위치하는 객체를 의미하며, 현실 객체 이미 지는 스틸이미지이거나 동영상일 수 있다. 가상 객체는 실제 공간에 존재하지 않는 가상의 객체로 AR 객 체로 지칭될 수 있으며, 가상 객체 이미지는 외부 장치(예를 들어, 서버)로부터 획득되거나 전자장치 의 저장부에 기 저장된 이미지 또는 3D 객체 모델일 수 있다. 예를 들어, 전자장치 사용자(미도시)가 새로운 가전 제품을 구매하고자 하는 경우, 전자장치 사용자는 AR 뷰를 이용하여 실제 공간에 가상의 제품을 배치하여 공간에 잘 어울리는지 또는 원하는 공간에 배치 가능한지 여부를 판단할 수 있다. 전자 장치 사용자가 카메라를 실행하고 공간을 촬영하면서 원하는 위치에서 원하는 제품을 선택하면, 전자장치 는 해당 위치에 선택된 제품의 가상 객체를 배치한 AR 뷰를 생성하여 사용자에게 제공할 수 있다. 뿐만 아니라, 전자장치 사용자가 카메라를 실행하고 공간을 촬영하면 자동으로 공간의 크기를 측정하여, 선택된 제품을 배치할 수 있는지 여부를 알려줄 수 있다. 또는, 선택된 제품을 배치하기 적당한 위치를 선택하여 알려줄 수 있다. 실제 공간에서 직접 AR 경험을 획득한 직접 사용자의 AR 경험이 다른 사용자에게 공유하여 AR 경험을 확장할 수 있다. 직접 사용자의 AR 경험이 공간 영상과 함께 녹화된 동영상 형태로 공유되는 경우, 간접 사용자는 직접 사 용자의 AR 경험을 일반적으로 감상하는 것에 지나지 않는다. 반대로, 직접 사용자의 AR 경험 공간을 상세 모델 링하여 가상 현실화 하는 경우, 간접 사용자의 만족스러운 AR 경험을 위해서는 고가의 공간 촬영 장비가 필요하 며 복잡한 연산을 수행해야 한다. 직접 사용자의 AR 경험을 간접 사용자에게 보다 효과적으로 확장하고, 간접 사용자가 능동적으로 AR 체험을 재 현하고, 가상 객체를 변경할 수 있도록 하는 등, AR 경험을 확장하기 위해서는, 영상의 품질을 유지하면서 시점 을 이동할 수 있는 AR 뷰를 제공할 수 있는 방법이 필요하다. 도 2a는, 본 개시의 일 실시예에 따른, 제2 사용자의 AR 뷰 및 제2 전자장치의 디스플레이를 나타낸다. 도 2a를 참조하면, 제2 사용자(또는 직접 사용자라 한다)는, 제2 전자장치를 이용하여 실제 공간 을 동영상으로 촬영할 수 있다. 본 개시의 일 실시예에 따른 제2 전자장치의 디스플레이는, AR 뷰를 제공하기 위한 영역 및 AR 객체를 선택하기 위한 영역을 포함할 수 있으며, AR 뷰를 제공 하기 위한 영역에는 실제 공간의 영상에 AR 객체가 오버레이되어 표시될 수 있다. 제2 사용자는, 제2 전자장치를 이용하여 실제 공간을 동영상으로 촬영하면서, 선택된 AR 객 체를 실제 공간의 동영상의 원하는 위치에, 원하는 방향으로 배치할 수 있다. 또한, 제2 전자장치 는 공간에 배치한 AR 객체를 대체하여 배치 가능한 제품을 제공하여, 제2 사용자가 보 다 용이하게 다른 객체로 변경 가능하게 함으로써 보다 다양한 AR 경험을 획득하도록 할 수 있다. 이와 같이, AR 객체에 대응하는 실제 객체를 배치하고자 하는 공간에 위치하는 제2 사용자는 실제 공간 영상에 다양한 AR 객체를, 다양한 위치와 방향으로 배치함으로써, 현장에서 AR 경험 또는 AR 체험을 획득할 수 있다. 이러한 제2 사용자의 AR 경험과, AR 경험이 획득된 공간에 대한 정보를 다른 사용자에게 공유할 수 있다면, 시 간과 공간의 제약이 없이 다른 장소에 있는 사용자에게 또는 화상통화 등으로 실시간으로 공간 영상을 확인할 수 없는 사용자에게 AR 경험을 확장할 수 있다. 또한, 제2 사용자가 추후 추가적으로 AR 경험을 시도하는 경우, 제2 전자장치로 촬영한 공간 영상 및 제2 사용자의 AR 경험에 기초하여 시공간의 제약 없이 새로운 AR 경험을 추가할 수 있다. 도 2b는, 본 개시의 일 실시예에 따른, 제1 사용자의 AR 뷰 및 제1 전자장치의 디스플레이를 나타낸다. 도 2b를 참조하면, 제1 전자장치는, 제2 전자장치에서 촬영된 공간 영상 및 제2 사용자의 AR 경험을 획득하고, 획득된 공간 영상 및 제2 사용자의 AR 경험에 기초하여 AR 뷰를 생성하여 표시할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치의 디스플레이는, AR 뷰가 표시되는 영역, 대체 가 능한 AR 객체가 표시되는 영역, 및 추천 AR 객체가 표시되는 영역을 포함할 수 있으며, AR 뷰가 표 시되는 영역에는 제2 전자장치에서 촬영된 공간에 기초하여 생성된 공간 모델에 AR 객체가 오버레이되어 표시될 수 있다. 본 개시의 일 실시예에 따르면, 제2 전자장치에서 촬영된 공간에 기초하여 생성된 공간 모델에 오 버레이되는 AR 객체는, 제2 사용자가 배치한 AR 객체와 동일한 AR 객체일 수 있으며, 또는, 제2 사용자의 AR 경험에 기초하여 선택된 AR 객체나 공간 분석 결과에 기초하여 선택된 AR 객체를 더 포함할 수 있다. 이 때, 공간 영상의 촬영 품질은 제1 사용자(또는 간접 사용자라 한다)의 AR 경험에 영향을 미친다. 예를 들어, 공간 영상 촬영자가 영상 촬영에 익숙하지 않은 경우 공간 영상이 과도하게 흔들릴 수 있고, 제1 사용자 는 공유된 공간 영상 자체에서 만족스러운 AR 경험을 획득하기 어렵다. 또한, AR 경험을 위한 시점이 공 간 영상 촬영자의 시점으로 고정되는 경우 공간 영상 촬영자와 다른 시점에서 공간 또는 AR 객체를 관찰할 수 없으며, 공간 영상 촬영 중 AR 객체가 배치되는 공간을 통과한 경우 원하는 공간에 AR 객체를 배치할 수 없다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은, 촬영된 공간 영상, AR 객체 정보 및 제2 사용자의 AR 경 험에 기초하여 공간을 모델링하고, 모델링된 공간을 관찰 또는 감상하기 위한 제1 경로(또는, 공간 뷰 경로라 한다)에 기초하여 AR뷰를 생성함으로써 공간 영상의 촬영 품질을 개선하여 제1 사용자에게 안정적인 AR경험을 제공할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은, 모델링된 공간에서 AR 객체를 관찰 또는 감상하기 위한 제2 경 로(또는, 객체 뷰 경로라 한다)에 기초하여 AR뷰를 생성함으로써 처리할 데이터의 양을 감소시키면서 사용자가 AR 객체를 자세히 감상할 수 있는 객체 뷰를 제공할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은, 공간을 관찰 또는 감상하기 위한 제1 경로(또는 공간 뷰 경 로)에서 공간 뷰를 제공하고, AR 객체를 관찰 또는 감상하기 위한 제2 경로(또는 객체 뷰 경로)에서 객체 뷰를 제공함으로써 사용자가 공간 내의 다양한 위치 및 방향에서 AR 객체에 대한 경험을 획득할 수 있도록 한다. 도 3은, 본 개시의 일 실시예에 따른 전자장치의 블록도이다. 도 3을 참조하면, 본 개시의 일 실시예에 따른 전자장치는 입력부, 출력부, 저장부, 프로세서, 및 통신 인터페이스를 포함할 수 있다. 일 실시 예에 따르면, 프로세서는 적어도 하나의 프로세서로 지칭됨으로써 전자장치의 다른 구성 요소들에 대한 동작을 제어하는 하나의 구성으로 이해될 수도 있다. 다양한 실시 예에 따르면, 전자장치 의 구성은 도 3에 도시된 바에 한정되지 않으며, 도 3에 도시되지 않은 구성을 추가로 포함하거나, 도 3에 도시 된 구성 중 일부를 생략할 수도 있다. 예를 들면, 전자장치는 인공지능 모델, 예컨대, 공간 뷰 또는 객체 뷰를 획득하기 위한 적어도 하나의 학습 모델을 위한 별도의 프로세서, 예컨대, NPU(neural processing unit)를 더 포함할 수도 있다. 또한, 저장부에 포함되는 뷰 합성 모듈 중 적어도 일부는 저장부에 저 장된 소프트웨어 모듈이 아닌 별도의 하드웨어 모듈로 구현될 수도 있다. 입력부는, 사용자가 전자장치를 제어하기 위한 데이터를 입력하는 수단을 의미하며, 카메라, 마이크, 및 버튼부(미도시)를 포함할 수 있다. 카메라는, 이미지를 획득하기 위한 카메라 또는 카메라 모듈과 동일 또는 유사한 구성으로 이해될 수 있 다. 일 실시 예에서, 카메라 1110는 렌즈, 근접 센서 및 이미지 센서를 포함할 수 있다. 다양한 실시 예에 따르 면, 카메라 1110는 기능 또는 목적에 따라 하나 이상일 수 있다. 예를 들면, 카메라 1110는 광각 렌즈를 포함하 는 제1 카메라 센서 및 망원 렌즈를 포함하는 제2 카메라를 포함할 수 있다. 본 개시의 일 실시예에 따르면, 카메라는, 특정 공간에서 직접 사용자가 AR 경험을 위한 공간 영상을 촬 영하는 수단일 수 있으며, 촬영된 공간 영상은 간접 사용자에게 공유될 수 있다. 마이크는, 음성 또는 음향 신호를 입력 받아 전기적인 데이터로 처리한다. 예를 들어, 마이크로폰은 외부 장치 또는 화자로부터 음향 신호(예컨대, 음성 명령)를 수신할 수 있다. 마이크로폰은 외부의 음향 신호를 입력 받는 과정에서 발생 되는 잡음(noise)을 제거하기 위한 다양한 잡음 제거 알고리즘을 이용할 수 있다. 입력부에는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항 막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등), 조그 휠, 조그 스위치 등이 있을 수 있으나 이에 한정되는 것은 아니다. 본 개시의 일 실시예에 의하면, 입력부는 전원 버튼, 밝기 설정 버튼, 예약 설정 버튼, 모드 변경 버튼, 음량 조정 버튼 등을 포함하는 버튼부(미도시)를 포함할 수 있다. 출력부는, 오디오 신호 또는 비디오 신호의 출력을 위한 것이다. 출력부는 스피커 및 디스플 레이를 포함할 수 있다. 스피커는, 통신 인터페이스로부터 수신되거나 저장부에 저장된 오디오 데이터를 출력할 수 있다. 복수의 스피커는 입체 음향 신호를 출력할 수도 있다. 예를 들어, 복수의 스피커는 우측 채널 스피커, 좌 측 채널 스피커, 모노 채널 스피커 중 적어도 하나로 구현될 수 있다. 또한, 복수의 스피커는 전자장치 에서 수행되는 기능(예를 들어, 알림음, 안내 음성, 가이드 음성)과 관련된 음향 신호를 출력할 수 있다. 디스플레이는 액정 디스플레이부(liquid crystal display), 박막 트랜지스터 액정 디스플레이부(thin film transistor-liquid crystal display), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디 스플레이부(flexible display), 3차원 디스플레이부(3D display), 전기영동 디스플레이부(electrophoretic display) 중에서 적어도 하나를 포함할 수 있다. 그리고 전자장치의 구현 형태에 따라 전자장치는 디스플레이를 2개 이상 포함할 수도 있다.디스플레이와 터치패드가 레이어 구조를 이루어 터치스크린으로 구성되는 경우, 디스플레이는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 저장부는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 저장부는 플래시 메 모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 저장부에 저장된 프로그램들은 그 기능에 따라 복수 개의 모듈들로 분류할 수 있는데, 예를 들어, 저장부 는 공간 모델링 모듈, 경로 생성 모듈, 및 뷰 합성 모듈을 포함할 수 있고, AR 객체 모델을 별도의 DB로 저장할 수 있다. AR 객체 모델은 AR 객체의 대상이 되는 제품의 제조사 등으로부터 사전에 제공되어 전자장치에 저 장될 수 있으며, AR 객체의 3D 모델을 포함하고 있는 경우 AR 뷰를 생성하기 위한 별도의 3D 렌더링이 필요없다. 공간 모델링 모듈은 AR 뷰를 생성하기 위하여, AR 객체를 포함하는 공간을 모델링한다. AR 객체를 포함하 는 공간 모델은 실제 공간에는 존재하지 않는 AR 객체를 고려한 공간 모델을 의미한다. 모델링된 공간은 실제 객체, AR 객체 및 공간 평면을 포함할 수 있다. AR 객체를 포함하는 공간을 모델링하는 구체적인 방법은 도 6a 내지 도 6d에서 후술한다. 경로 생성 모듈은, 간접 사용자의 AR 경험을 위한 AR 뷰를 생성하기 위한 적어도 하나의 경로를 생성한다. 경로 생성 모듈은, 공간 모델 및 공간 영상에 기초하여 공간 뷰에 대한 제1 경로(또는 공간 뷰 경로라 한다)를 획득하고, 제1 경로 및 AR 객체에 대한 정보에 기초하여 제2 경로(또는 객체 뷰 경로라 한다)를 획득할 수 있다. 제1 경로 및 제2 경로를 획득하는 구체적인 방법은 도 6a 내지 도 6d에서 후술한다. 뷰 합성 모듈은, 공간을 관찰 또는 감상할 수 있는 제1 경로를 따라 생성된 공간 뷰 및 AR 객체를 관찰 또는 감상할 수 있는 제2 경로를 따라 생성된 객체 뷰를 획득하고, 획득된 공간 뷰 및 객체 뷰를 합성하여 간접 사용자의 AR 경험을 위한 AR 뷰를 생성한다. AR 뷰 생성을 위해 이용되는 기술은, 대표적으로 워핑(warping) 및 융합(fusion), 스타일 변환 기술 등이 있다. 공간 뷰 획득 모듈 및 객체 뷰 획득 모듈은 인공지능 모델을 포함할 수 있다. 인공지능 모델은 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지능 전용 프로세서에 의해 처리될 수 있다. 인공지능 모델은 학습 을 통해 만들어 질 수 있다. 여기서, 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의 된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이 어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 추론 예측은 정보를 판단하여 논 리적으로 추론하고 예측하는 기술로서, 지식/확률 기반 추론(Knowledge based Reasoning), 최적화 예측 (Optimization Prediction), 선호 기반 계획(Preference-based Planning), 추천(Recommendation) 등을 포함한 다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법에 있어서, 공간 뷰 획득 모듈은, 공간 영상에서 추출된 영상 프레 임이 입력되면 공간상의 소정의 위치에서 바라본 공간 뷰를 출력하도록 학습된 인공지능 모델에 기초하여 공간 뷰를 획득할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법에 있어서, 객체 뷰 획득 모듈은, 공간 영상 프레임에서 추출된 촬영 위치 및 각도가 입력되면 영상의 intensity를 출력하도록 학습된 인공지능 모델에 기초 하여 AR 뷰를 획득할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법에 있어서, 객체 뷰 획득 모듈은, 공간 영상 및 AR 객체 모델이 입력되면, 공간의 스타일 피처에 기초하여 AR 객체의 스타일을 변환하도록 학습된 인공지능 모델에 기초하여 AR 뷰를 획득할 수 있다. 공간 뷰 및 AR 뷰를 생성하는 구체적인 방법은 도 7a 내지 도 8b에서 후술한다. 인공지능 모델은 전자장치에 온 디바이스 (on-device) 방식으로 구현될 수 있으며, 또한 서버 방식으로 구현될 수도 있다. 서버 방식의 인공지능 모델은, 전자장치가 필요한 정보를 인공지능 서버로 전송하면, 정보를 획득한 인공지능 서버가 인공지능 모델을 이용한 연산을 수행하고 추론 결과를 다시 전자장 치로 전송한다. 서버 방식의 인공지능 모델을 이용하는 경우, 전자장치의 공간 뷰 획득 모듈 및 객 체 뷰 획득 모듈은 서버로부터 획득한 추론 결과에 기초하여 공간 뷰 및 객체 뷰를 획득할 수 있다. 서버 방식 의 인공지능 모델에서, 서버의 구체적인 동작 및 블록도는 도 4에서 후술한다. 프로세서는, 전자장치의 전반적인 동작을 제어한다. 예를 들어, 프로세서는, 전술한 저장부 에 저장된 프로그램들을 실행함으로써, 본 명세서에서의 AR 뷰를 생성하기 위한 전자장치의 기능을 제어할 수 있다. 프로세서는 산술, 로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성 요소들을 포함할 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 및 기타 연산 회로 중 적어도 하 나를 포함할 수 있으나, 이에 제한되는 것은 아니다. 본 개시의 일 실시예에 의하면, 프로세서는 학습 네트워크 모델을 생성하기 위한 인공지능(AI) 프로세서 를 포함할 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에 의하면, AI 프로세서는 프로세서 와 별도의 칩으로 구현될 수도 있다. 본 개시의 일 실시예에 의하면, AI 프로세서는 범용 칩일 수도 있다. 통신 인터페이스는, 전자장치와 외부의 다른 전자장치(미도시) 또는 서버 사이의 유선 또는 무선 통신 채널의 수립 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 일 실시 예에 따르면, 통신 인터페이스는 유선 통신 또는 무선 통신을 통해 외부의 다른 전자장치 또는 서버로부터 데이 터를 수신하거나 또는 외부의 다른 전자장치 또는 서버에 대해 데이터를 송신할 수 있다. 통신 인터페이스는 전자장치가 AR 뷰를 생성하기 위해 필요한 정보를 서버와 송수신할 수 있 다. 또한, 통신 인터페이스는 AR 뷰를 생성하기 위하여 다른 디바이스(미도시) 또는 다른 서버(미도시)와 통신할 수 있다. 다양한 실시 예에 따르면, 통신 인터페이스는 무선 통신 모듈(예: 셀룰러 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있고, 그 중 어느 하나의 통신 모듈을 이용하여 적어 도 하나의 네트워크, 예컨대, 근거리 통신 네트워크 (예: 블루투스, WiFi direct 또는 IrDA(infrared data association)) 또는 원거리 통신 네트워크(예: 셀룰러 네트워크, 인터넷, 또는 컴퓨터 네트워크(예: LAN 또는 WAN))를 통하여 외부 전자장치 또는 서버와 통신할 수 있다. 도 4는, 본 개시의 일 실시예에 따른 인공지능 서버의 블록도이다. 도 4를 참조하면, 서버는 통신 인터페이스, 프로세서, 및 저장부를 포함할 수 있다. 일 실시 예에 따르면, 프로세서는 적어도 하나의 프로세서로 지칭됨으로써 서버의 다른 구성 요소 들에 대한 동작을 제어하는 하나의 구성으로 이해될 수도 있다. 다양한 실시 예에 따르면, 서버의 구성은 도 4에 도시된 바에 한정되지 않으며, 도 4에 도시되지 않은 구성을 추가로 포함하거나, 도 4에 도시된 구성 중 일부를 생략할 수도 있다. 예를 들면, 서버 3000는 인공지능 모델, 예컨대, 적어도 하나의 학습 모델을 위한 별 도의 프로세서, 예컨대, NPU(neural processing unit)를 더 포함할 수도 있다. 다른 예를 들면, 저장부 에 포함되는 공간 뷰 생성 모듈 중 적어도 일부는 저장부에 저장된 소프트웨어 모듈이 아닌 별도의 하드웨어 모듈로 구현될 수도 있다. 통신 인터페이스는, 서버와 외부의 다른 서버(미도시) 또는 전자장치 사이의 유선 또는 무선 통신 채널의 수립 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 일 실시 예에 따르면, 통신 인터페 이스는 유선 통신 또는 무선 통신을 통해 외부의 다른 서버(미도시) 또는 전자장치로부터 데이터를 수신하거나 또는 외부의 다른 서버(미도시) 또는 전자장치에 대해 데이터를 송신할 수 있다. 통신 인터페이스는 서버 3000가 AR 뷰를 생성하기 위해 필요한 정보를 전자장치와 송수신할 수 있 다. 또한, 통신 인터페이스는 AR 뷰를 생성하기 위하여 다른 디바이스(미도시) 및 다른 서버(미도시)와통신할 수 있다. 다양한 실시 예에 따르면, 통신 인터페이스는 무선 통신 모듈(예: 셀룰러 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있고, 그 중 어느 하나의 통신 모듈을 이용하여 적어 도 하나의 네트워크, 예컨대, 근거리 통신 네트워크 (예: 블루투스, WiFi direct 또는 IrDA(infrared data association)) 또는 원거리 통신 네트워크(예: 셀룰러 네트워크, 인터넷, 또는 컴퓨터 네트워크(예: LAN 또는 WAN))를 통하여 외부 전자장치(미도시) 또는 서버와 통신할 수 있다. 프로세서는, 서버의 전반적인 동작을 제어한다. 예를 들어, 프로세서는, 후술할 저장부 에 저장된 프로그램들을 실행함으로써, 본 명세서에서의 아바타 서비스를 제공하기 위한 서버의 기 능을 제어할 수 있다. 프로세서는 산술, 로직 및 입출력 연산과 시그널 프로세싱을 수행하는 하드웨어 구성 요소들을 포함할 수 있다. 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 및 기타 연산 회로 중 적어도 하 나를 포함할 수 있으나, 이에 제한되는 것은 아니다. 본 개시의 일 실시예에 의하면, 프로세서는 학습 네트워크 모델을 생성하기 위한 인공지능(AI) 프로세서 를 포함할 수 있으나, 이에 한정되는 것은 아니다. 본 개시의 일 실시예에 의하면, AI 프로세서는 프로세서 와 별도의 칩으로 구현될 수도 있다. 본 개시의 일 실시예에 의하면, AI 프로세서는 범용 칩일 수도 있다. 저장부는 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수 있다. 저장부는 플래시 메 모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 저장부에 저장된 프로그램들은 그 기능에 따라 복수 개의 모듈들로 분류할 수 있는데, 예를 들어, 저장부 는 공간 뷰 생성 모듈, 및 객체 뷰 생성 모듈을 포함할 수 있고, AR 객체 모델을 별도 의 DB로 저장할 수 있다. AR 객체 모델은 AR 객체의 대상이 되는 제품의 제조사 등으로부터 사전에 제공 되어 서버에 저장될 수 있으며, AR 객체의 3D 모델을 포함하고 있는 경우 AR 뷰를 생성하기 위한 별도의 3D 렌더링이 필요없다. AR 뷰 생성을 위해 이용되는 기술은, 대표적으로 워핑(warping) 및 융합(fusion), 스타 일 변환 기술 등이 있다. 공간 뷰 생성 모듈은 공간 영상에서 추출된 영상 프레임이 입력되면 공간상의 소정의 위치에서 바라본 공 간 뷰를 출력하도록 학습된 인공지능 모델에 기초하여 공간 뷰를 획득할 수 있다. 공간 뷰 생성 모듈은 워핑(warping)부 및 융합(fusion)부를 포함할 수 있으며, 촬영된 공간 영상에서 추출된 적어도 하나의 영상 프 레임을 워핑하고, 워핑된 프레임들을 융합하여 결과 영상 프레임을 획득할 수 있다. 워핑은, 영상에서 픽셀의 위치, 즉 이미지의 형태를 변형하여 영상을 의도적으로 왜곡하거나 또는 왜곡을 제거 하는 기술로, 워핑부는 입력된 촬영된 공간 영상에서 추출된 적어도 하나의 영상 프레임 각각을 공간 뷰 경로상 의 특정 지점에서 바라본 뷰로 전환하는 과정을 수행한다. 융합부는 적어도 하나의 워핑된 이미지 프레임들을 결합함으로써, 워핑된 이미지 프레임 각각에 존재하는 미표시 영역 또는 왜곡 영역을 제거하여 완전한 프레임, 즉 뷰를 생성할 수 있다. 객체 뷰 생성 모듈은 공간 영상 프레임에서 추출된 프레임에서, 촬영 위치 (x, y, z) 및 각도(θ, ?)가 입력되면 영상의 intensity (r, g, b)를 출력하도록 학습된 인공지능 모델에 기초하여 AR 뷰를 획득할 수 있다. 이 때, 촬영된 공간 영상에서 추출된 공간 위치 (x, y, z) 및 촬영 각도 (θ, ?)에 기초하여, 영상 intensity (r, g, b)를 예측하는 인공지능 모델의 fully-connected layers 가 최적화 되면, 예측된 영상 intensity (r, g, b)와 촬영된 실제 영상의 intensity 차이(loss)를 최소화 할 수 있다. 이와 같은, fully-connected layer 는 임의의 위치와 각도에 대한 모든 뷰를 생성할 수 있지만, 각 픽셀(ray) 단위로 결과가 획득되므로, 연산량이많고 수행 속도가 느리다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 공간을 감상 또는 관찰하기 위해 필요한 시점에서의 데이터만 처리하여 연산량 및 연산 속도를 줄일 수 있다. 또한, AR 객체의 3D 모델은 메모리에 DB 형태로 저 장되어 있으므로 AR 객체에 대한 별도의 3D 모델링이 필요 없으며, 전체 영역이 아닌 실제 배경 영역과 AR 객체 의 경계 영역을 합성함으로써, AR 객체와 배경의 경계에서의 왜곡을 방지하면서 영상 합성을 수행해야 하는 영 역을 최소화 할 수 있다. 객체 뷰 생성 모듈은 공간의 스타일 피처에 기초하여 AR 객체의 스타일을 변환하도록 학습된 인공지능 모 델에 기초하여 객체 뷰를 획득할 수 있다. 본 개시의 일 실시예에 따르면, AR 객체를 배경 이미지에 합성할 때, AR 객체가 배경에 보다 자연스럽게 어울리 도록 하기 위해 배경의 환경 또는 특징에 기초하여 AR 객체를 변환할 수 있다. 본 개시의 일 실시예에 따르면, AR 객체로부터 추출된 스타일 피처(feature) 와 콘텐트 피처 중 스타일 특징을 촬영된 공간 영상으로부터 추출된 스타일 특징으로 대체함으로써, AR 객체를 실제 공간에서 촬영된 것과 유사한 스타일로 변환할 수 있다. 이 때, 스타일 변환은 스타일 변환 네트워크(style transfer network)를 이용하여 학 습된 인공지능 모델에 기초하여 수행될 수 있다. 공간 뷰 및 AR 뷰를 생성하는 구체적인 방법은 도 7a 내지 도 8b에서 후술한다. 서버 3000에 포함되는 각 모듈은 외부 서버 또는 3rd 파티의 결과가 필요하면 외부 API를 이용하여 외부 서버 또는 3rd 파티의 결과를 획득할 수 있고, 획득한 외부 서버 또는 3rd 파티의 결과를 이용하여 서버 3000의 출력 을 생성할 수 있다. 도 5는, 본 개시의 일 실시예에 따른 제1 전자장치가 AR 뷰를 제공하는 방법의 순서도이다. 도 5를 참조하면, 제2 사용자는 실제 공간에서 AR 경험을 획득하는 직접 사용자를 의미하며, 제1 사용자 는 제2 사용자의 AR 경험에 기초하여 확장된 AR 경험을 획득하는 간접 사용자를 의미한다. 또한, 제2 전 자장치는 제2 사용자가 AR 경험을 획득한 공간 영상을 촬영한 전자장치를 의미하며, 제1 전자장치 는 제2 전자장치로부터 공유된 공간 영상 및 제2 사용자의 AR 경험에 기초하여 AR 뷰를 생성하고, 제공하는 전 자장치를 의미한다. 단계 S501에서, 제1 전자장치는 제2 전자장치에서 촬영된 공간 영상, 및 공간에서 적어도 하나의 AR 객체 에 대한 제2 사용자의 AR 경험 정보를 획득할 수 있다. 제2 사용자(또는 직접 사용자라 한다)는, 제2 전자장치를 이용하여 실제 공간을 동영상으로 촬영할 수 있다. 제2 사용자는, 제2 전자장치를 이용하여 실제 공간을 동영상으로 촬영하면서, 선택 된 AR 객체를 공간 영상의 원하는 위치에, 원하는 방향으로 배치하여 AR 경험을 획득할 수 있다. 제1 전자장치는, 제1 사용자(또는 간접 사용자라 한다)의 간접적인 AR 경험을 위한 AR 뷰를 생성하 기 위하여, 제2 전자장치에서 촬영된 공간 영상 및 제2 사용자의 AR 경험을 획득할 수 있다. 단계 S502에서, 제1 전자장치는 공간 영상, 및 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경로를 획득할 수 있다. AR 뷰를 생성하기 위한 적어도 하나의 경로는 공간을 감상 또는 관찰할 수 있는 공간 뷰를 생성하기 위한 제1 경로(또는 공간 뷰 경로라 한다), 및 AR 객체를 감상 또는 관찰할 수 있는 객체 뷰를 생성하기 위한 제2 경로 (또는 객체 뷰 경로라 한다)를 포함할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 공간 영상 및 AR 객체에 대한 정보에 기초하여 공간을 모델링 하고, 모델링된 공간 및 공간 영상에 기초하여, 제1 경로를 획득하고, 제1 경로 및 AR 객체에 대한 정보에 기초 하여, 제2 경로를 획득할 수 있다. 제1 경로 및 제2 경로를 획득하는 구체적인 방법은 도 6a 내지 도 6d에서 후술한다. 단계 S503에서, 제1 전자장치는 AR 뷰를 생성하기 위한 적어도 하나의 경로에 기초하여 AR 뷰를 생성할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 픽셀(ray) 단위로 영상을 생성하는 것이 아니라, 기존의 공간 영상 프레임을 워핑 (warping) 및 융합 (fusion)함으로써 공간 뷰를 생성한다. 본 개시의 일 실시예에 따른 AR 공간 평면내 제1 경로상의 지점(view point)에서 바라본 공간 뷰(target view)는 촬영된 공간 영상에서 선택된 적어도 하나의 프레임들에 기초하여 획득될 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 공간 영상 및 제1 경로에 기초하여 제1 경로에 대응하는 공간 뷰를 생성하고, 객체 모델 및 제2 경로에 기초하여 제2 경로에 대응하는 객체 뷰를 생성하고, 공간 뷰 및 객체 뷰를 합성하여 AR 뷰를 생성할 수 있다. 공간 뷰 및 객체 뷰를 생성하는 구체적인 방법은 도 7a 내지 도 8b 에서 후술한다. 단계 S504에서, 제1 전자장치는 생성된 AR 뷰를 출력할 수 있다. 본 개시의 일 실시예예 따른 AR 뷰 생성 방법은, 제1 전자장치의 디스플레이에 생성된 AR 뷰를 표 시할 수 있다. 본 개시의 일 실시예에 따른 제1 전자장치의 디스플레이는, AR 뷰가 표시되는 영역, 대체 가 능한 AR 객체가 표시되는 영역, 및 추천 AR 객체가 표시되는 영역을 포함할 수 있으며, AR 뷰가 표 시되는 영역에는 제2 전자장치에서 촬영된 공간에 기초하여 생성된 공간 모델에 AR 객체가 오버레이되어 표시될 수 있다. 본 개시의 일 실시예에 따르면, 제2 전자장치에서 촬영된 공간에 기초하여 생성된 공간 모델에 오 버레이되는 AR 객체는, 제2 사용자가 배치한 AR 객체와 동일한 AR 객체일 수 있으며, 또는, 제2 사용자의 AR 경험에 기초하여 선택된 AR 객체나 공간 분석 결과에 기초하여 선택된 AR 객체를 더 포함할 수 있다. 도 6a 내지 도 6d는, 본 개시의 일 실시예에 따라, AR 객체를 포함하는 공간 모델에서 AR 경험을 위한 경로를 결정하는 방법을 설명하기 위한 도면이다. 공간 모델은 실제 객체(1232-1, 1232-2), AR 객체 및 AR 공간 평면를 포함할 수 있다. AR 객체를 포함하는 공간 모델은 실제 공간에는 존재하지 않는 AR 객체를 고려한 공간 모델을 의미한다. 실제 공간에 AR 객체를 배치하는 경우, AR 객체가 배치될 위치는 실제로 아무런 물체가 존재하지 않으므로 제2 사용 자는 공간을 촬영하면서 AR 객체가 배치될 공간 안으로 이동할 수 있다. 그러나, AR 객체의 배치를 고려하면 해 당 공간은 AR 객체가 배치되어야 하므로 제2 사용자는 해당 공간 안으로 들어갈 수 없다. 따라서, 이와 같은 경 우 제2 사용자의 촬영 경로는 AR 객체의 위치에 기초하여 수정되어야 한다. 도 6a를 참조하면, 본 개시의 일 실시예에 따른 제1 전자장치는 제2 사용자가 촬영한 공간 영상에 기초하 여 제2 사용자의 촬영 경로와 AR 공간 평면의 교차점(734 및 735)이 존재하는지 여부를 식별할 수 있 다. 제1 전자장치는 제2 사용자의 촬영 경로 내에서 소정의 기준점(P_base)을 결정한다. 예를 들어, 제2 사용자의 촬영 경로의 시작점과 종료점의 중간점이 기준점으로 결정될 수 있다. 도 6b를 참조하면, 본 개시의 일 실시예에 따른 제1 전자장치는, 식별된 교차점에 기초하여 수정된 촬영 경로를 획득할 수 있다. 제2 사용자의 촬영 경로와 AR 공간 평면의 교차점이 두개 존재하고, 각 각을 P_A, P_B라고 하면, 교차점 P_A와 P_B 구간에서 제2 사용자의 촬영 경로 및 AR 공간 평면 과 기준점(P_base, 732)의 거리에 따른 수정된 촬영경로상의 점들의 집합 r은 다음과 같이 결정될 수 있다. [수식 1] if P_A < p1 < P_B r = D(p1, P_base) < D(q, P_base) ? p1 : q else r = p1 이 때, q는 (기준점, p1) 위에 있는 AR 공간 plane의 점이고, p1은 제2 사용자의 촬영 경로상의 점들의 집 합이고, r은 수정된 촬영 경로상의 점들의 집합으로, 각각 다음과 같이 정의된다. [수식 2] p1 ∈ Path_사용자 = {P_시작,…, P_A,…, P_B,…, P_끝} [수식 3] r ∈ Path_수정 또한, D(x, y)는 y부터 x까지의 유클리디안(Euclidean) 거리를 나타내며, [수식 1]은, 제2 사용자의 촬영 경로 상에서 [P_A, P_B] 구간의 점 p1이 직선 [P_A, P_B]상의 점으로 1:1 매핑되는 것을 의미한다. 도 6c를 참조하면, 본 개시의 일 실시예에 따른 제1 전자장치는, 수정된 촬영 경로를 추가로 수정하 여 모델링된 공간을 감상하기 적합한, 공간 뷰 경로(750, Path_R)를 결정할 수 있다. 공간 영상 촬영자, 예를 들어 제2 전자장치의 사용자가 영상 촬영에 익숙하지 않은 경우, 촬영된 공간 영상은 공간 영상 촬 영자의 이동 경로에 따라 의도하지 않은 흔들림이나 화질 열화가 발생할 수 있다. 이 때, 스무딩 기술 등을 적 용하여 촬영 경로를 안정적으로 수정하고, 공간 영상을 수정된 경로에서 관찰하도록 하면 보다 안정적인 공간 영상을 획득할 수 있다. 본 개시의 일 실시예에 따르면, 제1 전자장치는 제2 전자장치 사용자의 촬영 경로 또는 공간 모델에 기초하여 수정된 촬영 경로를 스무딩하여 공간 뷰 경로를 획득할 수 있다. 예를 들어, Mean 필터 스 무딩, L0 필터 스무딩 등을 사용하면 빠른 속도로 필터 기반 경로 스무딩을 수행할 수 있다. 제2 사용자의 촬영 경로와 AR 공간 평면의 교차점(734 및 735)이 존재하지 않는 경우, 제1 전자장치 는 제2 전자장치 사용자의 촬영 경로에 경로 스무딩을 수행하여 공간 뷰 경로를 획득할 수 있 다. 반면, 제2 사용자의 촬영 경로와 AR 공간 평면의 교차점(734 및 735)이 존재하는 경우, 제1 전자 장치는 공간 모델에 기초하여 수정된 촬영 경로에 경로 스무딩을 수행하여 공간 뷰 경로를 획 득할 수 있다. 도 6d를 참조하면, 본 개시의 일 실시예에 따른 제1 전자장치는, AR 객체 및 공간 뷰 경로에 기초하여, AR 객체를 감상하기 적합한, 객체 뷰 경로(760, Path_AR)를 결정할 수 있다. 예를 들어, AR 객체 의 중심을 기준점(P_AR)으로 결정하고, 공간 뷰 경로상의 점들 p2 에서 AR 객체의 중심 에서 가장 가까운 점인 p_closest을 결정한다. 이를 식으로 표현하면 다음과 같다. [수식 4] p_closest = argmin(D(p2, p_AR)) 여기서, p2는 공간 뷰 경로상의 점들의 집합으로 다음과 같이 정의된다. [수식 5] p2 ∈ Path_R 이 때, 객체 뷰 경로(760, Path_AR)는 p_AR을 중심으로 p_closest를 지나는 원으로 결정될 수 있다. 도 7a 및 도 7b는, 본 개시의 일 실시예에 따라, 공간 뷰를 생성하는 방법을 나타내는 도면이다. 본 개시의 일 실시예에 따라 공간 뷰를 생성하는 방법은, 픽셀(ray) 단위로 영상을 생성하는 것이 아니라, 기존 의 공간 영상을 워핑 (warping) 및 융합 (fusion)함으로써 공간 뷰를 생성한다. 도 7a를 참조하면, 본 개시의 일 실시예에 따른 AR 공간 평면 내 공간 뷰 경로상의 지점 view point에서 바라본 공간 뷰(target view)는 촬영된 공간 영상에서 선택된 적어도 하나의 프레임들에 기초하 여 획득될 수 있다. 본 개시의 일 실시예에 따르면, 공간 뷰 경로상의 지점 view point에서 바라본 공간 뷰(target view)는 view point로부터 제2 사용자의 촬영 경로로 그은 적어도 하나의 지점(예를 들어, 736,737, 738)에서의 촬영된 공간 영상의 프레임(예를 들어, I1, I2, I3 )을 입력받아, view point에서 바라 본 공간 뷰(target view)를 출력하도록 학습된 인공지능 모델에 기초하여 획득될 수 있다. 도 7b를 참조하면, 본 개시의 일 실시예에 따른 공간 뷰를 생성하는 전자장치는, 워핑부 및 융합부 를 포함할 수 있으며, 촬영된 공간 영상에서 추출된 적어도 하나의 영상 프레임(811 내지 81n)을 워핑 하고, 워핑된 프레임(831 내지 83n)을 융합하여 결과 영상 프레임을 획득할 수 있다. 워핑은 , 영상에서 픽셀의 위치, 즉 이미지의 형태를 변형하여 영상을 의도적으로 왜곡하거나 또는 왜곡을 제거 하는 기술로, 워핑부는 입력된 촬영된 공간 영상에서 추출된 적어도 하나의 영상 프레임 각각을 공간 뷰 경로상의 지점 view point에서 바라본 뷰로 전환하는 과정을 수행한다. 예를 들어, 도 7의 경우와 같이 n=3이고, I1은 사용자 촬영 경로상의 지점 736에서 촬영된 영상 프레임, I2는 사용자 촬영 경로상의 지점 737에서 촬영된 영상 프레임이고, I3은 사용자 촬영 경로상의 지점 738에서 촬영된 영상 프레임이라 가정한다. I1, I2, 및 I3는 서로 다른 위치에서 촬영되어 그대로 융합(fusion)할 수 없으므로 I1, I2, 및 I3를 융합 가능한 상태로 변환하는 전처리 과정에 해당한다 . 본 개시의 일 실시예에 따르면, 워핑은 STN(Spatial Transformer Network)를 이용하여 학습된 AI 모델에 기초 하여 수행될 수 있다. STN는 입력 피처(feature)에 기초하여, 해석(translation), 스케일(scaling), 및 회전 (rotation)을 변환하는 행렬 M을 출력할 수 있다. STN을 이용하는 경우, end-to end 학습이 가능하므로 합성에 적합한 워핑 학습이 가능하다. 이 때, 적어도 하나의 입력 영상 프레임과 하나의 타겟 출력 영상 프레임으로 DB 를 구성하고, 손실 함수를 타겟 영상과 결과 영상 사이의 L1 norm으로 지정하여 인공지능 모델을 학습할 수 있 다. 본 개시의 일 실시예에 따르면, 워핑은 I1, I2, 및 I3 가 촬영된 시점 사이의 각 변화량(차이) 및 intrinsic 계수에 기초하여 변환 행렬 H를 획득할 수 있다. 이 때, 변환 행렬 H는 Homography matrix일 수 있 다. 워핑된 이미지 프레임 W1, W2, 및 W3는 각각 I1, I2, 및 I3가 view point에서 바라본 영상으로 변환된 결 과물이다. 그러나, 워핑된 이미지 프레임 W1, W2, 및 W3는 표시할 수 없는 영역 또는 왜곡된 영역이 존재하게 된다. 융합부는 적어도 하나의 워핑된 이미지 프레임들을 결합함으로써, 워핑된 이미지 프레임 각각에 존재하는 미표시 영역 또는 왜곡 영역을 제거하여 완전한 프레임, 즉 뷰를 생성할 수 있다. 본 개시의 일 실시예에 따르면, 융합부는 가중치가 적용된 워핑 이미지들을 연결(concatenation)하여, 인 코딩한 후, 입력된 피쳐들 중 공간 도메인에서 가중치를 높여야 하는 부분을 하이라이트처리한 후 디코딩한다. 예를 들어, 이미지 프레임 W1에서 추출된 피쳐는 결과 영상에서 필요한 영역(attention 영역)에 위치한 피쳐값 들이 하이라이트되도록 학습된다. 도 8a 및 도 8b는, 본 개시의 일 실시예에 따라, 객체 뷰를 생성하는 방법을 나타내는 도면이다. 본 개시의 일 실시예에 따른 AR 공간 평면 내 객체 뷰 경로에서 바라본 객체의 AR 뷰를 의미하며, 이 때 객체의 AR 뷰는 360?뷰일 수 있다. 도 8a를 참조하면, 본 개시의 일 실시예에 따른 객체 뷰는, 촬영된 영상의 3차원 촬영 공간 위치 (x, y, z) 및 촬영 각도 (θ, ?)를 입력으로, 영상의 (r, g, b)를 추론하는 인공지능 모델을 학습함으로써 획득될 수 있다. 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하 여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만 들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각 은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간 의 연산을 통해 신경망 연산을 수행한다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예측하는 기술로서, 지식/확률 기반 추론(Knowledge based Reasoning), 최적화 예측(Optimization Prediction), 선호 기반 계획(Preference-based Planning), 추천 (Recommendation) 등을 포함한다.이 때, 촬영된 공간 영상에서 추출된 공간 위치 (x, y, z) 및 촬영 각도 (θ, ?)에 기초하여, 영상 intensity (r, g, b)를 예측하는 인공지능 모델의 fully-connected layers 가 최적화 되면, 예측된 영상 intensity (r, g, b)와 촬영된 실제 영상의 intensity 차이(loss)를 최소화 할 수 있다. 이와 같은, fully-connected layer 는 임의의 위치와 각도에 대한 모든 뷰를 생성할 수 있지만, 각 픽셀(ray) 단위로 결과가 획득되므로, 연산량이 많고 수행 속도가 느리다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 촬영된 공간 영상으로부터 공간 뷰 경로(750, Path_R)상 시점 에서의 영상을 합성함으로써, 공간을 감상 또는 관찰하기 위해 필요한 시점에서의 데이터만 처리하여 연산량 및 연산 속도를 줄일 수 있다. 또한, AR 객체의 3D 모델은 제1 전자장치의 메모리에 미리 저장 되어 있을 수 있으며, 이러한 경우 AR 객체에 대한 별도의 3D 모델링이 필요 없다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 촬영된 영상에서 공간 위치 (x, y, z) 및 촬영 각도 (θ, ?)를 추출하고, 전체 영역이 아닌 실제 배경 영역과 AR 객체의 경계 영역의 뷰를 생성함으로써, AR 객체와 배경의 경 계에서의 왜곡을 방지하면서 영상 합성을 수행해야 하는 영역을 최소화 할 수 있다. 도 8b를 참조하면, 본 개시의 일 실시예 따른 AR뷰는, 스타일 변환에 기초하여 생성될 수 있다. 본 개시의 일 실시예에 따르면, AR 객체를 배경 이미지에 합성할 때, AR 객체가 배경에 보다 자연스럽게 어울리 도록 하기 위해 배경의 환경 또는 특징에 기초하여 AR 객체를 변환할 수 있다. 본 개시의 일 실시예에 따르면, AR 객체로부터 추출된 스타일 피처(feature) 와 콘텐트 피처 중 스타일 특징을 촬영된 공간 영상으로부터 추출된 스타일 특징으로 대체함으로써, AR 객체를 실제 공간에서 촬영된 것과 유사한 스타일로 변환할 수 있다. 이 때, 스타일 변환은 스타일 변환 네트워크(style transfer network)를 이용하여 학 습된 인공지능 모델에 기초하여 수행될 수 있다. 도 9는, 본 개시의 일 실시예에 따라, AR 뷰를 생성하는 방법을 나타내는 도면이다. AR 객체로 인해 촬영 경로 에서 공간 뷰 경로로 경로가 크게 수정되는 영역에서는 합성된 뷰의 품질 이 하락할 수 있으며, 서로 다른 두 경로가 인접하는 영역에서 합성 뷰의 차이가 발생하면 사용자가 이질감을 느껴 AR 경험 품질이 저하될 수 있다. 이와 같은 문제를 해결하기 위하여, 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 공간 뷰 경로에서 객체 뷰 경로로의 자연스러운 이동(전환)을 위하여 뷰를 통합하는 과 정을 포함할 수 있다. 도 9을 참조하면, 촬영 경로 에서 공간 뷰 경로의 접점 p_e 및 p_s에서 뷰 통합이 수행될 수 있다. 공간 뷰 경로상 {ps,…, pe} 사이에서 점들의 집합을 p3 ∈ {ps,…, pe}라 하면, p3에서의 뷰 viewp3는 다음과 같이 결정될 수 있다. [수식 6] viewp3 = Alpha_blending( ) 이 때, 은 공간 뷰 경로 Path_R 의 p3 지점에서의 뷰를 의미하며, 은 객체 뷰 경로 PathAR 의 반지름을 수정하여 PathR 과 접하는 p3 지점에서의 뷰를 의미한다. 즉, 공간 뷰 경로상의 [p_e, p_s] 구간에서 와 를 블렌딩하 여 뷰를 통합한다. 도 10은, 본 개시의 일 실시예에 따른 AR 뷰 제공 방법에서, 객체를 추천하는 방법을 나타내는 도면이다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은, 현장 사용자, 즉 제2 사용자의 AR 경험 데이터와 공간 모델링 을 진행하며 획득된 데이터에 기초하여, 간접 사용자, 즉 제1 사용자가 능동적으로 AR 경험을 진행할 수 있도록 AR 객체를 추천할 수 있다. 도 10a를 참조하면, 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은 제2 사용자가 AR 객체를 배치한 위치인 A 지점 또는 제2 사용자가 AR 객체를 배치하지는 않았지만 머무른 기록이 있는 위치인 B 지점에서 추가적인 AR 객 체 추천을 제공할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은 제2 사용자가 AR 객체를 배치한 위치인 A 지점에서 제2 사용자 가 배치한 AR 객체의 종류(예를 들어, TV)와 어울리는 종류의 제품(예를 들어, 사운드 바)을 추천하거나, 제2 사용자가 AR 객체를 배치한 위치인 A 지점에서 제2 사용자가 배치한 AR 객체와 같은 카테고리의 제품을 추천하 거나, 해당 위치에서 제2 사용자가 살펴보았지만 실제로 배치하지는 않았던 제품을 추천할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 제공 방법은 제2 사용자가 AR 객체를 배치하지는 않았지만 머무른 기록이 있 는 위치인 B 지점에서 해당 공간에 추가적으로 배치 가능한 제품을 추천하거나, 해당 공간에 있는 제품을 대체 가능한 제품을 추천하거나, 해당 위치에서 제2 사용자가 살펴보았지만 실제로 배치하지는 않았던 제품을 추천할 수 있다. 도 10b를 참조하면, 본 개시의 일 실시예에 따른 AR 뷰를 제공하는 제1 전자장치의 디스플레이 영역 에는 AR 뷰를 표시하는 영역, 대체 가능 제품을 표시하는 영역 및 추천 제품을 표시하는 영 역이 포함될 수 있다. 본 개시의 일 실시예에 따르면, 대체 가능 제품을 표시하는 영역에는 해당 위치에서 제2 사용자가 공간에 배치해보았던 제품, 제2 사용자가 배치를 고려하였지만 실제로는 배치하지 않았던 제품, 또는 제2 사용자가 배 치한 AR 객체와 같은 카테고리의 제품이 대체 가능 제품으로 표시될 수 있다. 본 개시의 일 실시예에 따르면, 추천 제품을 표시하는 영역에는 공간 분석 결과에 기초하여, AR 객체를 추가로 배치할 수 있는 공간(1235-1 내지 1235-1) 및 대응하는 제품군(예를 들어, 정수기, 조리기기, 및 식기세 척기)에 포함되는 제품들을 추천 제품으로 표시될 수 있다. 본 개시의 일 실시예에 따르면, 추천 제품을 표시하는 영역에는 제2 사용자가 배치한 AR 객체의 종류(예 를 들어, TV)와 어울리는 종류의 제품(예를 들어, 사운드 바) 또는 공간 내 특정 위치에서 제2 사용자가 살펴보 았지만 실제로 배치하지는 않았던 제품을 추천 제품으로 표시될 수 있다. 본 개시의 일 실시예에 따르면, 생성된 AR 뷰는 표준화된 코덱으로 인코딩된 동영상 형태로 제공될 수 있으며, 이러한 경우 전자장치의 OS(operation system)나 실행 환경(예를 들어, AR 뷰 생성 및/또는 제공을 위한 애플리 케이션 설치 여부 등)의 제약 없이 다른 사용자에게 공유할 수 있다. 이 때, 현장 사용자의 AR 경험의 하이라이 트가 AR 뷰 동영상에 포함될 수 있으며, 대체 가능 제품 또는 추천 제품은 동영상 내 PIP(picture-in- picture)와 같은 형태로 제공될 수 있다. 도 11은, 본 개시의 일 실시예에 따른 AR 뷰 생성 시스템에서, 인공지능 서버를 이용하여 AR 뷰를 생성하는 방 법의 동작 흐름도이다. 도 11을 참조하면, 본 개시의 일 실시예에 따른 AR 뷰 생성 시스템은, 제1 전자장치, 제2 전자장치 , 및 인공지능 서버를 포함할 수 있다. 도 11의 각 단계 및 동작에서, 도 5 내지 도 9b와 중복되는 내용은 간략히 기재하거나 설명을 생략한다. 단계 S1101에서, 제2 전자장치는, AR 경험을 위한 공간의 공간 영상을 촬영하면서, 제2 사용자의 AR 경험 을 획득할 수 있다(단계 S1101-1). 제2 사용자는 실제 공간에서 AR 경험을 획득하는 직접 사용자를 의미 하며, 제2 전자장치는 제2 사용자가 AR 경험을 획득한 공간 영상을 촬영한 전자장치를 의미한다. 단계 S1102에서, 제2 전자장치는, 촬영된 공간 영상 및 제2 사용자의 AR 경험을 제1 전자장치로 전 송할 수 있다. 단계 S1103에서, 제1 전자장치는, 획득된 공간 영상, 및 제2 사용자의 AR 경험 정보에 기초하여 AR 뷰를 생성하기 위한 적어도 하나의 경로를 획득할 수 있다. 제1 전자장치는 공간 영상에 기초하여 공간을 모델링(단계 S1103-1)하고, 모델링된 공간에서 공간을 감상 또는 관찰할 수 있는 공간 뷰를 생성하기 위한 제1 경로(또는 공간 뷰 경로라 한다)를 획득(단계 S1103-2)하고, 및 AR 객체를 감상 또는 관찰할 수 있는 객체 뷰를 생성하기 위한 제2 경로(또는 객체 뷰 경로라 한다)를 획득 (단계 S1103-3)할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 공간 영상 및 AR 객체에 대한 정보에 기초하여 공간을 모델링 하고, 모델링된 공간 및 공간 영상에 기초하여, 제1 경로를 획득하고, 제1 경로 및 AR 객체에 대한 정보에 기초 하여, 제2 경로를 획득할 수 있다. 단계 S1104에서, 제1 전자장치는, 공간영상, 제1 경로 및 제2 경로를 인공지능 서버로 전송할 수 있다. 단계 S1105에서, 인공지능 서버는, 공간영상, 제1 경로 및 제2 경로에 기초하여, 제1 경로에 기초한 공간 뷰를 생성(단계 S1105-1)하고, 제2 경로에 기초한 객체 뷰를 생성(단계 S1105-2)할 수 있다. 본 개시의 일 실시예에 따른 AR 뷰 생성 방법은, 픽셀(ray) 단위로 영상을 생성하는 것이 아니라, 기존의 공간 영상 프레임을 워핑 (warping) 및 융합 (fusion)함으로써 공간 뷰를 생성한다. 본 개시의 일 실시예에 따른 AR 공간 평면내 제1 경로상의 지점(view point)에서 바라본 공간 뷰(target view)는 촬영된 공간 영상에서 선택된 적어도 하나의 프레임들에 기초하여 획득될 수 있다. 본 개시의 일 실시예에 따른 공간 뷰 및 객체 뷰는 학습을 통해 만들어진 인공지능 모델을 이용하여 획득될 수 있다. 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하 여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만 들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각 은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간 의 연산을 통해 신경망 연산을 수행한다. 추론 예측은 정보를 판단하여 논리적으로 추론하고 예측하는 기술로서, 지식/확률 기반 추론(Knowledge based Reasoning), 최적화 예측(Optimization Prediction), 선호 기반 계획(Preference-based Planning), 추천 (Recommendation) 등을 포함한다. 단계 S1106에서, 인공지능 서버는, 생성된 공간 뷰 및 객체 뷰를 제1 전자장치로 전송할 수 있다. 단계 S1107 및 단계 S1108에서, 제1 전자장치는, 획득한 공간 뷰 및 객체 뷰를 합성하여 AR 뷰를 생성하 고, 디스플레이에 생성된 AR 뷰를 표시할 수 있다."}
{"patent_id": "10-2022-0124660", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비 일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미 할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하 지 않는다. 예로 , '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다.도면 도면1 도면2a 도면2b 도면3 도면4 도면5 도면6a 도면6b 도면6c 도면6d 도면7a 도면7b 도면8a 도면8b 도면9 도면10a 도면10b 도면11"}
{"patent_id": "10-2022-0124660", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은, 본 개시의 일 실시예에 따른, AR 뷰를 도시한 것이다. 도 2a는, 본 개시의 일 실시예에 따른, 제2 사용자의 AR 뷰 및 제2 전자장치의 디스플레이를 나타낸다. 도 2b는, 본 개시의 일 실시예에 따른, 제1 사용자의 AR 뷰 및 제1 전자장치의 디스플레이를 나타낸다. 도 3은, 본 개시의 일 실시예에 따른 전자장치의 블록도이다. 도 4는, 본 개시의 일 실시예에 따른 인공지능 서버의 블록도이다. 도 5는, 본 개시의 일 실시예에 따른 제1 전자장치가 AR 뷰를 제공하는 방법의 순서도이다. 도 6a 내지 도 6d는, 본 개시의 일 실시예에 따라, AR 객체를 포함하는 공간 모델에서 AR 경험을 위한 경로를 결정하는 방법을 설명하기 위한 도면이다. 도 7a 및 도 7b는, 본 개시의 일 실시예에 따라, 공간 뷰를 생성하는 방법을 나타내는 도면이다. 도 8a 및 도 8b는, 본 개시의 일 실시예에 따라, 객체 뷰를 생성하는 방법을 나타내는 도면이다. 도 9는, 본 개시의 일 실시예에 따라, AR 뷰를 생성하는 방법을 나타내는 도면이다. 도 10a 및 도 10b는, 본 개시의 일 실시예에 따른 AR 뷰 제공 방법에서, 객체를 추천하는 방법을 나타내는 도면 이다. 도 11은, 본 개시의 일 실시예에 따른 AR 뷰 생성 시스템에서, 인공지능 서버를 이용하여 AR 뷰를 생성하는 방 법의 동작 흐름도이다."}
