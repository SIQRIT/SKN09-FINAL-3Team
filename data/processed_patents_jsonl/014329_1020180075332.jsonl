{"patent_id": "10-2018-0075332", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0002148", "출원번호": "10-2018-0075332", "발명의 명칭": "실시간 소리 분석 방법 및 장치", "출원인": "주식회사 디플리", "발명자": "류명훈"}}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "실시간으로 발생하는 소리를 수집하는 입력부; 수집된 실시간 소리 데이터를 머신 러닝이 용이하도록 가공하는 신호처리부; 사전 수집된 소리 데이터를 머신 러닝(Machine Learning) 방식으로 학습하여, 소리 유형(Category) 정보를 구분하기 위한 제1 함수를 훈련시키는 제1 학습부; 및 상기 제1 함수에 의해 신호처리 된 소리 데이터를 소리 유형으로 분류하는 제1 분류기를 포함하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 소리 데이터에 관한 정보를 전송하고 수신하는 제1 통신부를 포함하고, 상기 제1 통신부는 신호처리 된 소리 데이터를 추가 분석 장치로 전송하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 제1 통신부는 추가 분석 장치로부터 딥 러닝에 의해 학습된 제2 함수를 통해 소리 원인을 분석한 결과를수신하는, 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 상기 제1 학습부는 상기 실시간 소리 데이터를 머신 러닝 방식으로 학습하여 상기 제1 함수를 보완하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서, 상기 제1 학습부는 사용자가 입력한 피드백을 전송받아 상기 피드백에 대응하는 실시간 소리 데이터를 머신 러닝 방식으로 학습하여 제1 함수를 보완하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2020-0002148-3-제5항에 있어서, 제1 피드백 수용부를 더 포함하고, 상기 제1 피드백 수용부는 사용자로부터 직접 피드백을 입력받거나 다른 장치 또는 모듈로부터 피드백을 전달받는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 제1 학습부는 사용자가 입력한 피드백을 전송받아 상기 피드백에 대응하는 실시간 소리 데이터를 머신 러닝 방식으로 학습하여 제1 함수를 보완하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 제1 제어부를 더 포함하고, 상기 제1 제어부는 상기 제1 분류기에 의해 분류된 소리 유형이 관심 소리에 해당하는지 판단하여 상기 분류된소리 유형이 관심 소리에 해당하는 경우, 신호처리 된 소리 데이터를 추가 분석 장치로 전송하도록 제어하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제3항에 있어서, 상기 제1 학습부는 수집된 소리 데이터에 대하여 준 지도 학습(Semi-supervised Learning)에 기반한 자동 레이블링을 수행하는, 인공지능에 기반한 실시간 소리 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 자동 레이블링은 정해진 알고리즘에 의해 수행되거나 사용자의 피드백에 의해 수행되는, 인공지능에 기반한 실시간 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항 내지 제3항에 있어서, 상기 신호처리부는 실시간 소리 데이터의 전처리, 프레임 생성 및 특징 벡터 추출을 수행하되, 전처리 전에 실시간 소리 데이터의 일부만을 핵심 벡터로 생성하는, 인공지능에 기반한 실시간 분석 장치."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "공개특허 10-2020-0002148-4-사전 수집된 소리 데이터를 머신 러닝(Machine Learning) 방식으로 학습하여, 소리 유형(Category) 정보를 구분하기 위한 제1 함수를 훈련시키는 단계(S110); 입력부를 통해 실시간으로 발생하는 소리를 수집하는 단계(S120); 수집된 실시간 소리 데이터를 학습이 용이하도록 신호처리하는 단계(S130); 신호처리 된 실시간 소리 데이터를 상기 제1 함수를 통해 소리 유형으로 분류하는 단계(S140); 상기 소리 유형으로 분류하는 단계에서 분류된 소리 유형이 관심 소리에 해당하는지 판단하는 단계(S150); 분류된 소리 유형이 관심 소리에 해당하는 경우, 신호처리 된 실시간 소리 데이터를 실시간 소리 분석 장치에서추가 분석 장치로 전송하는 단계(S160); 및상기 실시간 소리 데이터를 머신 러닝 방식으로 학습하여, 상기 제1 함수를 보완하는 단계(S190)를 포함하는, 인공지능에 기반한 실시간 소리 분석 방법."}
{"patent_id": "10-2018-0075332", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 실시간 소리 분석 장치가 상기 추가 분석 장치로부터 딥 러닝에 의해 학습된 제2 함수를 통해 소리 원인을분석한 결과를 수신하는 단계(S170)를 포함하는, 인공지능에 기반한 실시간 소리 분석 방법."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 실시간 소리 분석 장치는 실시간으로 발생하는 소리를 수집하는 입력부, 수집된 실 시간 소리 데이터를 머신 러닝이 용이하도록 가공하는 신호처리부, 사전 수집된 소리 데이터를 머신 러닝 (Machine Learning) 방식으로 학습하여, 소리 유형(Category) 정보를 구분하기 위한 제1 함수를 훈련시키는 제1 학습부, 및 상기 제1 함수에 의해 신호처리 된 소리 데이터를 소리 유형으로 분류하는 제1 분류기를 포함하는 것 을 특징으로 한다. 본 발명의 일 실시예에 따르면, 머신 러닝에 기초하여 실시간으로 수집되는 소리의 유형 및 원인을 학습할 수 있고, 실시간으로 수집되는 소리의 유형과 원인에 대하여 보다 정확한 예측이 가능하다."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 실시간 소리를 분석하는 방법 및 장치에 관한 것으로, 더 상세하게는 실시간으로 발생하는 주변 소리 를 인공지능에 기반한 기계학습 방식으로 학습하고 분석하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음향 기술의 발달로 소리를 감지하여 분류할 수 있는 기능을 가진 장치들이 다양하게 출시되고 있다. 주파수 분 석을 통해 소리를 분류하고 사용자에게 결과값을 제공해주는 기능은 대중들의 모바일 장치를 통해 널리 활용되 고 있고, 최근에는 인공지능 스피커가 출시되어 사용자의 언어적 소리에 반응하고 질문이나 명령에 대한 적절한 피드백을 제공하기도 하는 등 소리 분석을 위한 도구들은 점점 다양해지는 추세다. 대한민국 특허 제10-1092473호의 경우 주변의 다양한 소리 중 아기의 울음소리를 감지할 수 있는 진동수 및 지 속 패턴을 이용한 아기 울음소리 감지방법 및 장치를 제공한다. 이는 아기가 울고 있는지 여부를 감지하여 부모 에게 알려주거나 자동으로 엄마의 심장 박동음을 들려주는 등의 피드백 기능을 탑재하여 육아의 부담을 덜어주 는 것을 목적으로 한다. 그러나 이와 같은 기술은 아기의 울음 여부만 알려줄 뿐 아기가 우는 이유에 대한 정보 는 제공하지 않고, 아기가 우는 이유(예: 배고픔, 아픔 등)가 다양할 수 있음에도 불구하고 일관된 피드백(예: 엄마의 심장 박동음 들려주기)만을 제공하는 등 경우에 따라서는 적절치 않은 피드백을 주게 되는 문제점을 안 고 있다. 한편, 최근 출시되는 인공지능 스피커의 경우에는 언어적 음성에 한정하여 반응하므로 글로 표현될 수 없는 비 언어적 소리(예: 아기 울음소리)에 대해서는 피드백을 제공할 수 없다는 문제가 있다. 선행기술문헌특허문헌 (특허문헌 0001) 대한민국 등록특허공보 제10-1092473호 (2011년 12월 5일 등록)"}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 위와 같은 문제점을 해결하기 위하여 제안된 것으로, 머신 러닝에 의해 소리를 학습하여 실시간으로 소리를 분류해낼 뿐 아니라 소리가 발생하게 된 원인을 학습함으로써, 소리의 종류 뿐 아니라 그 원인까지 분석 할 수 있는 방법 및 장치를 제공하는 것을 목적으로 한다."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 실시간 소리 분석 장치는 실시간으로 발생하는 소리를 수집하는 입력부, 수집된 실 시간 소리 데이터를 머신 러닝이 용이하도록 가공하는 신호처리부, 사전 수집된 소리 데이터를 머신 러닝 (Machine Learning) 방식으로 학습하여, 소리 유형(Category) 정보를 구분하기 위한 제1 함수를 훈련시키는 제1 학습부, 및 상기 제1 함수에 의해 신호처리 된 소리 데이터를 소리 유형으로 분류하는 제1 분류기를 포함하는 것을 특징으로 한다. 본 발명의 일 실시예에 따른 실시간 소리 분석 장치는 소리 데이터에 관한 정보를 전송하고 수신하는 제1 통신 부를 포함하고, 상기 제1 통신부는 신호처리 된 소리 데이터를 추가 분석 장치로 전송할 수 있다. 상기 제1 통신부는 추가 분석 장치로부터 딥 러닝에 의해 학습된 제2 함수를 통해 소리 원인을 분석한 결과를 수신할 수 있다. 본 발명의 일 실시예에서, 상기 제1 학습부는 상기 실시간 소리 데이터를 머신 러닝 방식으로 학습하여 상기 제 1 함수를 보완할 수 있다. 본 발명의 일 실시예에서, 상기 제1 학습부는 사용자가 입력한 피드백을 전송받아 상기 피드백에 대응하는 실시 간 소리 데이터를 머신 러닝 방식으로 학습하여 제1 함수를 보완할 수 있다. 본 발명의 일 실시예에 따른 실시간 소리 분석 장치는 제1 피드백 수용부를 더 포함할 수 있고, 상기 제1 피드 백 수용부는 사용자로부터 직접 피드백을 입력받거나 다른 장치 또는 모듈로부터 피드백을 전달받을 수 있다. 본문에서 쓰이는 '함수'란, 머신 러닝을 위해 주어진 데이터 및 학습 알고리즘을 통해 지속적으로 보강되는 도 구를 의미한다. 구체적으로, 입력(소리)과 출력(유형 또는 원인)의 관계를 예측하는 도구를 의미한다. 따라서, 최초 학습시에는 관리자에 의해 함수가 사전결정될 수 있다. 학습하는 데이터가 많아질수록 더욱 정확해지는 상기 제1 함수는 사전에 수집한 소리 데이터를 머신 러닝 방식 으로 학습시킴으로써 주변 소리를 유형별로 분류하는 유용한 도구가 될 수 있다. 예를 들어 상기 제1 함수는 관 심 소리가 환자의 소리인 경우 사전에 수집한 환자 소리를 머신 러닝 방식으로 학습시키면 환자가 신음 소리를 내는지, 일상적 대화를 하는지, 웃음 소리를 내는지 구분할 수 있다. 이러한 머신 러닝 방식에서는 분류기 (Classifier)가 학습될 수 있고, 바람직하게는 상기 분류기는 로지스틱 회귀분석 분류기(Logistic Regression Classifier)일 수 있으나, 이에 한정되지 않는다. 즉, 분류기의 함수는 데이터에 의해 머신 러닝 방식으로 훈련 되어 성능이 개선될 수 있다. 이러한 학습 과정은 실시간 소리 데이터가 수집되면서 지속적으로 반복되어 분류 기가 더 정확한 결과를 도출할 수 있도록 한다. 실시간 소리 분석 장치와 통신하는 추가 분석 장치는, 실시간 소리 데이터를 제2 머신 러닝 방식으로 학습함으 로써 제2 함수를 보완하는 제2 학습부를 포함할 수 있다. 학습하는 데이터가 많아질수록 더욱 정확해지는 제2 함수는 사전에 수집한 소리 데이터를 머신 러닝 방식으로 학습시킴으로써 주변 소리가 발생하는 원인을 유형별 로 분류할 수 있다. 예를 들어 상기 제2 함수는 관심 소리가 환자의 소리인 경우 사전에 수집한 환자 소리를 머 신 러닝 방식으로 학습시키면 환자가 내는 소리를 원인 별로 분류하여 환자가 신경통을 호소하는지, 고열에 의 한 통증을 호소하는지, 자세의 불편감을 호소하는지 구분할 수 있다. 바람직하게는 상기 제2 머신 러닝 방식은딥 러닝 방식일 수 있다. 바람직하게는 상기 딥 러닝 방식에서는 오류 역전파(Error Backpropagation) 방식이 사용될 수 있으나, 이에 한정되지 않는다. 이러한 학습 과정은 실시간 소리 데이터가 수집되면서 지속적으로 반 복되어 분류기가 더 정확한 결과를 도출할 수 있도록 한다. 또한, 상기 추가 분석 장치는 실시간 소리 분석 장치에서 얻어진 정보를 추가 학습 데이터로 사용할 수 있다. 제1 학습부에서 소리의 로우 데이터(Raw Data)로부터 특징 벡터(Feature Vectors)를 추출하고 이를 활 용하여 머신 러닝으로 소리의 유형(Category)을 분류하였다면, 제2 학습부에서는 상기 유형까지도 특징 벡터로 고려하여 학습을 반복함으로써 소리가 발생하는 원인을 더 신속하고 정확하게 분석할 수 있다. 머신 러닝 또는 딥 러닝에서는 학습 대상의 특징 벡터가 다양하고 정확할수록 더 빠른 학습이 가능하기 때문에, 상기 방식은 분 석의 정확도를 높이는데 매우 유용하다. 본 발명의 일 실시예에서, 상기 제1 학습부는 상기 실시간 소리 데이터를 머신 러닝 방식으로 학습하여 상기 제 1 함수를 보완할 수 있다. 본 발명의 일 실시예에서, 상기 제1 학습부는 사용자가 입력한 피드백을 전송받아 상기 피드백에 대응하는 실시 간 소리 데이터를 머신 러닝 방식으로 학습하여 제1 함수를 보완할 수 있다. 본 발명의 일 실시예에서, 실시간 소리 분석 장치는 제1 피드백 수용부를 더 포함할 수 있고, 상기 제1 피드백 수용부는 사용자로부터 직접 피드백을 입력받거나 다른 장치 또는 모듈로부터 피드백을 전달받을 수 있다. 본 발명의 일 실시예에서, 실시간 소리 분석 장치는 제1 제어부를 더 포함할 수 있고, 상기 제1 제어부는 상기 제1 분류기에 의해 분류된 소리 유형이 관심 소리에 해당하는지 판단하여 상기 분류된 소리 유형이 관심 소리에 해당하는 경우, 신호처리 된 소리 데이터를 추가 분석 장치로 전송하도록 제어할 수 있다. 본 발명의 일 실시예에서, 상기 제1 학습부는 수집된 소리 데이터에 대하여 준 지도 학습(Semi-supervised Learning)에 기반한 자동 레이블링을 수행할 수 있다. 상기 자동 레이블링은 정해진 알고리즘에 의해 수행되거 나 사용자의 피드백에 의해 수행될 수 있다. 즉, 상기 자동 레이블링은 평소에 정해진 알고리즘에 의해 수행되 다가 오류에 대한 사용자의 피드백을 수신하면 피드백에 대응하는 데이터에 사용자의 피드백에 맞는 레이블링을 수행한 후 다시 머신 러닝으로 함수를 학습시킨다. 바람직하게는, 상기 신호처리부는 전처리, 프레임 생성, 특징 벡터 추출을 수행한다. 상기 전처리는 정규화(Normalization), 주파수 필터링(Frequency Filtering), 템포럴 필터링(Temporal Filtering), 윈도잉(Windowing) 중 적어도 하나를 포함할 수 있다. 상기 프레임 생성은 전처리된 소리 데이터를 시간 영역(time domain)의 복수의 프레임들로 구분하는 작업이다. 상기 특징 벡터 추출은 상기 복수의 프레임들 중 단일의 프레임 별로 수행되거나 동일한 개수의 프레임으로 구 성된 프레임 그룹 별로 수행될 수 있다. 상기 신호처리부에서 추출된 특징 벡터는 적어도 하나의 차원(Dimension)으로 구성될 수 있다. 즉, 하나의 특징 벡터가 사용될 수도 있고 복수의 특징 벡터가 사용될 수도 있다. 상기 신호처리부는 실시간 소리 데이터의 전처리, 프레임 생성 및 특징 벡터 추출을 수행하되, 전처리 전에 실 시간 소리 데이터의 일부만을 핵심 벡터로 생성할 수 있다. 실시간 소리 데이터의 양이 방대하므로 원본 데이터 를 모두 저장하지 않고 반드시 필요한 핵심 벡터만으로 가공한 후 전처리, 프레임 생성 및 특징 벡터 추출을 수 행하는 작업을 수행할 수 있다. 상기 핵심 벡터는 추가 분석 장치로 전송될 수 있다. 상기 특징 벡터의 적어도 하나의 차원은 상기 소리 유형(Category)에 관한 차원을 포함할 수 있다. 이는 소리의 발생 원인을 구분하기 위한 제2 함수를 학습시키는 추가 분석 장치의 제2 학습부에서 소리 데이터의 특징 벡터 로서 소리 유형까지 포함하는 경우 더욱 정확한 원인 예측이 가능하기 때문이다. 다만, 특징 벡터에 소리 유형 이 아닌 다른 요소들이 포함될 수 있고, 추가할 수 있는 특징 벡터의 요소가 소리 유형로 한정되는 것은 아니다. 바람직하게는, 상기 실시간 소리 분석 장치가 수행하는 제1 머신 러닝 방식은 최소 평균 제곱법(Least Mean Square; LMS)을 포함하고, 상기 최소 평균 제곱법으로 회귀분석 분류기(Logistic Regression Classifier)를 학 습할 수 있다. 바람직하게는, 상기 추가 분석 장치가 수행하는 제2 머신 러닝 방식은 딥 러닝(Deep Learning) 방식이고, 오류 역전파(Backpropagation)를 통해 상기 제2 함수를 최적화할 수 있다. 상기 신호처리부는 연속된 프레임들을 복수의 프레임 그룹들(Frame Group)로 재정의하는 프레임 그룹 형성 단계 를 더 포함할 수 있다. 상기 복수의 프레임 그룹들 중 각 프레임 그룹이 포함하는 프레임들의 집합은 상기 복수 의 프레임 그룹들 중 다른 프레임 그룹이 포함하는 프레임들의 집합과는 다르고, 각 프레임 그룹들 간의 시간 간격은 일정한 것이 바람직하다. 특징 벡터의 추출 및 소리 유형 및 원인 분류는 각 프레임 그룹을 일 단위로 하여 수행될 수 있다. 상기 제1 학습부는 사용자가 입력한 피드백을 전송받아 상기 피드백에 대응하는 실시간 소리 데이터를 머신 러 닝 방식으로 학습하여 제1 함수를 보완할 수 있다. 이를 위해 실시간 소리 분석 장치는 피드백 수용부를 포함할 수 있다. 상기 제1 피드백 수용부는 사용자로부터 직접 피드백을 입력받거나 다른 장치 또는 모듈로부터 피드백을 전달받을 수 있다. 본 발명의 일 실시예에서, 인공지능에 기반한 실시간 소리 분석 장치는 피드백 수용부를 더 포함할 수 있고, 상 기 피드백 수용부는 사용자가 입력한 피드백을 제1 학습부 및 제2 학습부 중 적어도 어느 하나로 전달하고, 상 기 피드백을 전송받은 학습부는 대응하는 함수를 보완할 수 있다. 예를 들면, 상기 제2 학습부는 상기 실시간 소리 분석 장치에서 얻어진 정보를 추가 학습 데이터로 사용할 수 있다. 상기 실시간 소리 분석 장치는 제1 표시부를 더 포함할 수 있고, 상기 추가 분석 장치는 제2 표시부를 더 포함 할 수 있고, 각 표시부는 대응하는 분석 장치에서 분류된 소리 유형 및/또는 소리 원인을 출력할 수 있다. 상기 추가 분석 장치는 서버이거나 이동통신용 단말기일 수 있다. 상기 추가 분석 장치가 서버인 경우, 제2 통 신부는 상기 소리 유형 및 소리 원인 중 적어도 하나를 이동통신용 단말기로 전송할 수 있고, 상기 이동통신용 단말기에서 입력받은 사용자의 피드백을 다시 수신할 수 있다. 상기 추가 분석 장치가 이동통신용 단말기인 경 우, 소리 원인 분석을 이동통신용 단말기가 직접 수행하고, 사용자가 이동통신용 단말기에 피드백을 입력하면 이동통신용 단말기에서 실시간 소리 분석 장치에 사용자의 피드백을 직접 전송할 수도 있다. 바람직하게는, 상기 제1 통신부가 상기 소리 유형에 관한 사용자의 피드백을 받은 경우, 상기 제1 학습부는 상 기 피드백에 대응하는 소리 데이터에 관하여 제1 머신 러닝 방식으로 학습함으로써 상기 제1 분류기를 보완할 수 있다. 이러한 학습 과정은 실시간 소리 데이터가 수집되고 피드백을 받는 과정이 지속적으로 반복되어 분류 기가 더 정확한 결과를 도출할 수 있도록 한다. 바람직하게는, 상기 제2 통신부가 상기 소리 원인에 관한 사용자의 피드백을 받은 경우, 상기 제2 학습부는 상 기 피드백에 대응하는 소리 데이터에 관하여 제2 머신 러닝 방식으로 학습함으로써 상기 제2 분류기를 보완할 수 있다. 이러한 학습 과정은 실시간 소리 데이터가 수집되고 피드백을 받는 과정이 지속적으로 반복되어분류기 가 더 정확한 결과를 도출할 수 있도록 한다. 예를 들면, 상기 소리 유형 및 소리 원인에 대한 사용자의 피드백을 수신하면 상기 피드백을 기초로 머신 러닝 및/또는 딥 러닝을 통해 상기 제1 분류기 및/또는 제2 분류기를 발전시킬 수 있다. 상기 신호처리부는 상기 실시간 소리 데이터를 가공하기 용이하도록 최적화하는 신호처리를 수행하되, 상기 실 시간 소리 데이터를 전처리한 후, 전처리된 소리 데이터를 시간 영역(time domain)의 복수의 프레임들로 구분하 고, 상기 복수의 프레임의 각각의 프레임으로부터 특징 벡터를 추출할 수 있다. 상기 전처리는 예를 들어 정규 화(Normalization), 주파수 필터링(Frequency Filtering), 템포럴 필터링(Temporal Filtering), 윈도잉 (Windowing)일 수 있다. 상기 특징 벡터의 적어도 하나의 차원은 상기 소리 유형 정보에 관한 차원일 수 있다. 바람직하게는, 상기 제2 머신 러닝 방식은 딥 러닝(Deep Learning) 방식이고, 오류 역전파(Backpropagation)를 통해 상기 제2 분류기를 발전시킬(Developing) 수 있다. 본 발명의 일 실시예에 따른 실시간 소리 분석 방법은 사전 수집된 소리 데이터를 머신 러닝(Machine Learning) 방식으로 학습하여, 소리 유형(Category) 정보를 구분하기 위한 제1 함수를 훈련시키는 단계(S110), 입력부를 통해 실시간으로 발생하는 소리를 수집하는 단계(S120), 수집된 실시간 소리 데이터를 학습이 용이하도록 신호 처리하는 단계(S130), 신호처리 된 실시간 소리 데이터를 상기 제1 함수를 통해 소리 유형으로 분류하는 단계 (S140), 상기 소리 유형으로 분류하는 단계에서 분류된 소리 유형이 관심 소리에 해당하는지 판단하는 단계 (S150), 분류된 소리 유형이 관심 소리에 해당하는 경우, 신호처리 된 실시간 소리 데이터를 실시간 소리 분석장치에서 추가 분석 장치로 전송하는 단계(S160) 및 상기 실시간 소리 데이터를 머신 러닝 방식으로 학습하여, 상기 제1 함수를 보완하는 단계(S190)를 포함하는 것을 특징으로 한다. 바람직하게는, 상기 실시간 소리 분석 장치가 상기 추가 분석 장치로부터 딥 러닝에 의해 학습된 제2 함수를 통 해 소리 원인을 분석한 결과를 수신하는 단계(S170)를 포함할 수 있다. 본 발명의 일 실시예에서, 관심 소리 여부 및/또는 관심 소리에 대한 분석 결과를 제1 표시부(D1)에 출력하는 단계(S180)를 더 포함할 수 있다."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 머신 러닝에 기초하여 실시간으로 수집되는 소리의 유형 및 원인을 학습할 수 있고, 실시간으로 수집되는 소리의 유형과 원인에 대하여 보다 정확한 예측이 가능하다."}
{"patent_id": "10-2018-0075332", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이 해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 도 1은 본 발명과 관련된 실시간 소리 분석 방법 및 장치를 설명하기 위한 개념도이다. 주변 소리가 발생하면 이는 실시간으로 마이크와 같은 입력부를 통해 감지되고 데이터로 저장된다. 주 변 소리는 소리가 가의 없는 무음일 수도 있고, 사용자가 관심을 가지지 않는 소리, 즉 잡음일 수 도 있으며, 사용자가 분류하거나 분석하고자 하는 관심 소리일 수도 있다. 상기 관심 소리는 경우에 따 라 환자의 신음일 수도 있고, 아기 울음 소리일 수도 있고, 성인의 음성일 수도 있다. 그러나 상기 관심 소리는 위 3가지 예에 한정되지 않고 교통사고 충돌 소리, 차량 작동 소리, 동물 소리 등 모든 소리가 될 수 있다. 예를 들어 관심 소리가 성인의 음성인 경우, 아기 울음 소리는 잡음으로 분류될 수 있다. 예 를 들어 관심 소리가 동물 소리인 경우, 환자의 신음, 아기 울음 소리, 성인의 음성 및 교 통사고 충돌 소리 등은 잡음으로 분류될 수 있다. 이와 같은 소리 유형의 분류는 실시간 소리 분석 장치에서 제1 분류기에 의해 수행될 수 있다. 상기 제1 분류기는 제1 학습부를 통해 머신 러닝 방식으로 기능이 강화될 수 있다. 우선, 사전 수집된 소 리 데이터(S001)의 적어도 일부에 소리 유형을 레이블링(Labeling)한다. 이후, 상기 제1 학습부는 소리 유형이 레이블된(Labeled) 사전 수집된 소리 데이터(S001)를 활용하여 머신 러닝 방식으로 제1 분류기의 제1 함수(f1)를 학습시킨다. 상기 제1 분류기는 로지스틱 회귀분석 분류기(Logistic Regression Classifier) 일 수 있다. 지도 학습(Supervised Learning)은 트레이닝 데이터를 이용해 하나의 함수를 훈련시키기 위한 머신 러닝 방법 중 하나인데, 트레이닝 데이터는 일반적으로 입력 객체에 대한 속성을 벡터 형태로 포함하고 있으며 각각의 벡 터에 대해 원하는 결과가 무엇인지 표시되어 있다. 이렇게 훈련된 함수 중 연속적인 값을 출력하는 것을 회귀분 석(Regression)이라 하고 주어진 입력 벡터가 어떤 종류의 값인지 표식하는 것을 분류(Classification)라 한다. 반면 비 지도 학습(Unsupervised Learning)은 지도 학습과는 달리 입력값에 대한 목표치가 주어지지 않는다. 바람직하게는, 본 발명의 일 실시예에서, 제1 학습부는 지도 학습과 비 지도 학습의 중간 성격을 갖는 준 지도 학습(Semi-supervised Learning) 방식을 사용할 수 있다. 상기 준 지도 학습은 목표값이 표시된 데이터와 표시되지 않은 데이터를 모두 훈련에 사용하는 것을 말한다. 대개의 경우 이러한 방법에 사용되는 트레이닝 데 이터는 목표값이 표시된 데이터가 적고 표시되지 않은 데이터를 많이 갖고 있다. 상기 준 지도 학습을 사용하면 레이블링 작업에 드는 시간과 비용을 크게 아낄 수 있다. 상기 목표값을 표시하는 작업이 레이블링(Labeling)이다. 예를 들면, 주변 소리가 발생하고, 그 소리 데이 터를 입력이라 하면, 그 소리의 유형이 무음인지, 잡음인지 또는 관심 소리인지에 대하여 표시하는 것이 레이블링 작업이다. 즉, 레이블링은 데이터에 출력의 예를 미리 표시하고 이를 머신 러닝 알고리즘에 의해 함수를 학습시키기 위한 기초 작업이다. 사람이 직접 표시하는 것이 지도 학습, 표시하지 않는 것이 비 지도 학습이고, 일부는 사람이 직접 표시하고, 나머지는 표시하지 않는 것이 준 지도 학습이다. 본 발명의 일 실시예에서, 제1 분석 장치는 준 지도 학습에 기반한 자동 레이블링(Auto-labeling) 작업을 수행할 수 있다. 레이블(Label)이란, 함수가 출력해야 할 결과값들을 의미한다. 예를 들어, 상기 레이블은 무음, 잡음, 아기 울음 소리, 울음 소리를 제외한 아기 소리 등의 결과값들이다. 상기 자동 레이블링은 아래와 같은 순서로 수행될 수 있다. 상기 자동 레이블링은 예를 들어 제1 학습부가 수행할 수 있다. 우선, 사람이 개입하여 일정한 개수(예를 들어, 100개)의 데이터에 대해 레이블을 표시한다. 이후부터 수집되는 소리 데이터에 대해서는 레이블을 표시하는 작업을 하지 않고 적절한 신호처리를 진행한 후 차원 축소 작업을 거친다. 동질성을 지닌 집단을 분류하는 클러스터링 기법을 사용하여 하나의 동질성으로 분류되는 복수의 데이 터들을 하나의 데이터 군으로 묶는다. 이 때, 상기 클러스터링 기법은 사전 결정된 하이퍼 파라미터 (Hyperparameter)를 기준으로 하여 분류를 진행하되, 상기 하이퍼 파라미터는 향후 수행되는 학습 정확도에 따 라 변경될 수 있다. 다음으로, 복수의 데이터 군이 형성되면 각 데이터 군 별로 사전 결정된 개수(예를 들어, 4개의 데이터)만큼만 무작위로 골라 어떤 특징을 가진 요소인지를 판별한다. 예를 들면, 만약, 제1 데이터 군에서 고른 4개의 데이터 중 3개 이상이 잡음에 해당하는 것으로 확인되면, 제1 데이터 군은 모두 잡음으로 간주하고 제1 데이터 군 내의 모든 데이터를 잡음으로 레이블링 한다. 만약, 제2 데이터 군에서 고른 4개의 데이터 중 2개 이하가 아기 울음 소리에 해당한다면 제2 데이터 군 내의 모든 데이터를 잡음 또는 무음으로 레이블링 한다. 다음으로, 이렇게 사전 결정된 알고리즘으로 레이블링을 수행하고, 레이블 된 데이터들은 학습 데이터로 활용한 다. 이 경우 정확도 지표가 높아지면 상기 알고리즘으로 레이블링을 지속하고, 정확도 지표가 낮아지면 차원 축 소 방식을 변경하거나 클러스터링의 파라미터를 변경하고, 앞의 과정을 다시 수행한다. 한편, 실시간 소리 분석 장치가 관심 소리를 감지하여 표시함으로써 사용자에게 편리함을 제공하기 는 하지만, 사용자는 청력을 가진 인간으로서, 현재 주변에서 환자가 신음을 내는지 안 내는지 인지할 수 있 고, 아기가 우는지 안 우는지 인지할 수 있으며, 동물이 소리를 내는지 안 내는지 인지할 수 있다. 이는 인간의 오감 중 하나인 청력이 손상되지 않았다면 구분할 수 있는 요소이다. 그러나 사용자는 환자가 신음을 낼 때 그 소리만 듣고 어느 부위가 아파서 신음을 내는지를 알기는 어렵다. 마찬가지로, 사용자는 아기가 울 때 그 소리만 듣고 아기가 무엇을 원하는지 알기는 어렵다. 관심 소리가 감지되면 실시간 소리 분석 장치는 추가 분석 장치로 신호처리된 실시간 소리 데이 터를 전송한다. 관심 소리가 발생하는 원인은 제1 원인, 제2 원인 및 제3 원인 등 여러가지가 있을 수 있고, 사용자의 수요는 관심 소리의 발생 원인에 집중된다. 예를 들어 상기 관심 소리가 아기 울음 소리인 경우, 아기는 배가 고파서 울었을 수도 있고, 변의(便 意)를 느껴 울었을 수도 있고, 기저귀에 대소변을 싼 후 불편감 때문에 울었을 수도 있고, 졸려서 울었을 수도 있다. 또는, 감정 상태에 따라 슬퍼서 울었을 수도 있고, 슬퍼하다가 기뻐하면서 울음 소리를 낼 수도 있다. 이 처럼, 아기의 울음 소리는 성인이 듣기에 유사하게 들릴지 몰라도 그 원인은 다양하다. 예를 들어 상기 관심 소리가 환자의 신음인 경우, 본 발명의 일 실시예에 따르면 환자의 목소리에서 발생하는 다양한 소리를 통해 발견하기 어려운 특정 질병을 조기에 발견하도록 하는 것이 가능하다. 또한, 환자 의 신음이 아니라 환자의 몸에서 발생하는 다양한 소리 역시 상기 관심 소리가 될 수 있다. 구체적으 로, 실시간 소리 분석 장치로 환자의 소변 소리를 관심 소리로서 감지한 후 추가 분석 장치로 환 자가 전립선 비대증을 앓고 있는지 여부를 분석할 수 있다. 예를 들어 상기 관심 소리가 베어링 마찰음인 경우, 본 발명의 일 실시예에 따르면 베어링이 회전하며 발생 하는 다양한 소리를 통해 사고 원인이 될 수 있는 결함을 조기에 발견하도록 하는 것이 가능하다. 이와 같은 소리 원인의 분류는 추가 분석 장치에서 제2 분류기에 의해 수행될 수 있다. 상기 제2 분 류기는 제2 학습부를 통해 딥 러닝 방식으로 기능이 강화될 수 있다. 우선, 사전 수집된 소리 데이터 (S001)의 적어도 일부에 소리 원인을 레이블링(Labeling)한다. 이후, 상기 제2 학습부는 소리 원인이 레이 블된(Labeled) 사전 수집된 소리 데이터(S001)를 활용하여 딥 러닝 방식으로 제2 분류기의 제2 함수(f2)를 학습시킨다. 실시간 소리 분석 장치 및 추가 분석 장치 간의 통신으로 사용자는 관심 소리의 발생 여부 및 관심 소리의 발생 원인(21, 22, 23)을 파악할 수 있다. 본 발명의 일 실시예에서, 상기 소리 원인은 소리를 발생시키는 주체의 상태(State)일 수 있다. 즉, 아기가 우 는 '원인'이 배고픔이라면, 마찬가지로 아기는 배가 고픈 '상태'에 있다고 볼 수 있다. 상기 '상태'라는 용어는 아기가 울고 있다는 1차적인 의미로 이해될 수도 있으나, 본 발명의 일 실시예의 추가 분석 장치에서 얻고 자 하는 데이터는 아기가 울고 있는 이유와 같은 2차적인 의미로 이해되는 것이 바람직하다. 본 발명의 일 실시예에서, 실시간 소리 분석 장치는 소리가 아닌 다른 정보를 감지하여 소리와 함께 분석 을 수행함으로써 분석 대상의 상태(소리 발생의 원인) 분석 정확도를 향상시킬 수 있다. 예를 들면, 아기가 뒤 척이는 진동을 감지하여 추가로 분석할 수 있다. 이에 따라, 진동을 감지하는 장치가 추가로 구성될 수 있다. 또는 진동을 감지하는 모듈이 상기 실시간 소리 분석 장치에 장착될 수 있다. 진동을 감지하는 장치는 일 예일 뿐이고, 설정된 관심 소리와 관련된 정보를 감지하는 장치라면 추가가 가능하다. 본 발명의 일 실시예에서, 실시간 소리 분석 장치는 복수의 관심 소리를 감지하여 소리와 함께 분석을 수행함으로써 분석 대상의 상태(소리 발생의 원인) 분석 정확도를 향상시킬 수 있다. 예를 들면, 누군가 넘어지는 소리 및 부딪히는 소리가 감지된 후 아기의 울음 소리가 감지되었을 경우, 장치가 아기 울음 소리만 분석하면 그 원인이 '고통스러움'으로 분석될 확률이 낮을 수 있으나(예를 들면, 60%), 넘어 지는 소리 및 부딪히는 소리가 울음 소리 직전에 발생했다는 정보를 함께 분석하여 아기 울음 소리의 원인이 ' 고통스러움'일 것이라고 더 높은 확률(예를 들면, 90%)로 분석할 수 있다. 즉, 장치의 신뢰도가 향상될 수 있다. 본 발명의 일 실시예에서, 상기 실시간 소리 분석 장치는 사용자가 소리를 감지하고자 하는 대상의 근 처에 배치되는 것이 바람직하다. 따라서 상기 실시간 소리 분석 장치는 이동성이 요구될 수 있고, 그 데이 터 저장 용량은 작을 수 있다. 즉, 이동이 필요한 장치에 포함되는 센서 등의 소형(또는 초소형) 장치의 경우 컴퓨팅 자원(메모리 사용량, CPU 사용량), 네트워크 자원, 배터리 자원이 일반 데스크탑 또는 서버 환경에 비해 매우 부족한 것이 일반적이다. 즉, 실시간 소리 분석 장치 배치 후 주변 소리가 발생하면 이에 대한 원본 데이터 중 인공지능 분석, 특히 머신 러닝 또는 딥 러닝을 위해 필요한 핵심 정보만이 저장되는 것이 바람 직하다. 예를 들면 마이크로 컨트롤러 유닛(Micro Controller Unit; MCU) 기반의 프로세서는 데스크탑 컴퓨터가 사용하 는 프로세서에 비해 대략 수십만분의 1 수준에 그친다. 특히, 소리 데이터와 같은 미디어 데이터의 경우 데이터 의 크기가 매우 크기 때문에, 데스크탑 컴퓨터처럼 원본 데이터를 메모리에 저장해두고 처리를 하는 것은 불가 능하다. 예를 들어 4분 길이의 음성 데이터(44.1KHz sampling rate)는 크기가 보통 40MB 정도인데, 고성능 MCU 의 시스템의 전체 메모리 용량은 64KB 수준에 그쳐 약 600분의 1 수준에 그친다. 따라서 본 발명의 일 실시예에 따른 실시간 소리 분석 장치는 분석할 원본 데이터를 메모리에 저장해두고 처리를 하는 기존 방식과는 달리 원본 데이터에 대한 중간 처리(예를 들면, FFT, Arithmetic computation 등)를 먼저 진행한 후 인공지능 분석 과정에 필요한 일부 정보만 핵심 벡터로 생성한다. 상기 핵심 벡터는 전처리(Preprocessing) 및 특징 벡터(Feature Vector)와는 다른 것으로서, 원본 데이터를 실 시간으로 전처리한 후 그 결과를 이용해 곧바로 특징 벡터 연산을 하는 과정을 거치지 않는다. 구체적으로, 이 후에 구해낼 특징 벡터의 연산에 필요한 전처리 중간 연산값 및 원본 데이터의 중간 연산값을 저장한다. 이는 엄격히 말하면 원본 데이터의 압축은 아니다. 따라서 전처리 및 특징 벡터 추출보다 핵심 벡터 연산이 먼저 수행되고, 실시간 소리 분석 장치는 원본 데 이터 대신 핵심 벡터를 저장함으로써 부족한 연산 능력 및 저장 공간의 한계를 극복할 수 있다. 바람직하게는, 실시간 소리 분석 장치에서 추가 분석 장치로 (또는 다른 장치로) 전송하는 데이터는 실시간 소리 데이터의 핵심 벡터 정보일 수 있다. 즉, 실시간으로 수집되는 소리를 추가 분석 장치로 (또 는 다른 장치로) 전송하는 작업 역시 실시간으로 수행되어야 하므로 실시간 소리 분석 장치의 신호처리부 에서 생성된 핵심 벡터 정보만을 추가 분석 장치로 전송하는 것이 유리하다. 이하, 음원, 실시간 소리 분석 장치, 추가 분석 장치, 이동통신용 단말기 및 사용자 간의 상호작용에 대하여 도 2 내지 5를 활용하여 상세히 설명한다. 도 2는 본 발명에 의한 실시간 소리 분석 장치의 제1 실시예를 나타내는 도면이다. 음원은 아기일 수도 있고, 동물일 수도 있고, 물건일 수도 있다. 도 2에서는 우는 아기가 도시되어 있다. 예 를 들어 아기 울음 소리가 입력부에 의해 감지되면, 실시간 소리 데이터(S002)로 저장되어 신호처리 부에 의해 머신 러닝에 맞도록 신호처리 된다. 신호처리 된 실시간 소리 데이터는 제1 함수(f1)를 포함하 는 제1 분류기에 의해 소리 유형으로 분류된다. 제1 분류기에 의해 소리 유형으로 분류된 실시간 소리 데이터는 제1 통신부와 제2 통신부의 통 신에 의해 추가 분석 장치에 전송된다. 전송된 실시간 소리 데이터 중 관심 소리에 관한 데이터는 제2 분 류기에 의해 소리 원인으로 분류된다. 제1 학습부는 머신 러닝에 의해 제1 분류기의 제1 함수(f1)를 학습시킨다. 여기서 입력은 주변 소리 고, 출력은 소리 유형이다. 상기 소리 유형은 무음, 잡음 및 관심 소리를 포함하되, 다른 유형 이 포함될 수 있다. 예를 들어 복수의 관심 소리를 두어 소리 유형에 무음, 잡음, 제1 관심 소리 및 제 2 관심 소리, 제3 관심 소리가 포함될 수 있다. 예를 들어 상기 무음과 잡음도 다른 유형으로 변경될 수 있다. 상기 제1 분류기는 사전 수집된 소리 데이터(S001)를 활용하여 학습된 제1 함수(f1)를 포함한다. 즉, 입력 인 실시간 소리 데이터가 제1 함수(f1)를 거쳐 출력인 소리 유형으로 분류될 수 있도록 사전 학습이 이루어진다. 그러나 상기 사전 학습이 이루어지더라도 제1 함수(f1)가 완벽하지 않으므로 지속적으로 보완되는 것이 바람직하다. 실시간 소리 데이터(S002)가 지속적으로 유입되고 이에 대한 결과값이 출력된 후, 사용자 가 오류가 있는 결과값들에 대해 피드백을 입력하면 제1 학습부가 이를 반영하여 제1 분류기를 다시 학습시킨다. 이러한 과정이 반복되면서 제1 함수(f1)는 점점 보완되고, 소리 유형 분류 정확도가 향상된다. 상기 제2 분류기는 사전 수집된 소리 데이터(S001)를 활용하여 학습된 제2 함수(f2)를 포함한다. 즉, 입력 인 실시간 소리 데이터가 제2 함수(f2)를 거쳐 출력인 소리 원인으로 분류될 수 있도록 사전 학습이 이루어진다. 그러나 상기 사전 학습이 이루어지더라도 제2 함수(f2)가 완벽하지 않으므로 지속적으로 보완되는 것이 바람직하다. 실시간 소리 데이터(S002)가 지속적으로 유입되고 이에 대한 결과값이 출력된 후, 사용자 가 오류가 있는 결과값들에 대해 피드백을 입력하면 제2 학습부가 이를 반영하여 제2 분류기를 다시 학습시킨다. 이러한 과정이 반복되면서 제2 함수(f2)는 점점 보완되고, 소리 원인 분류 정확도가 향상된다. 실시간 소리 분석 장치는 제1 표시부를 포함할 수 있다. 제1 표시부는 예를 들면 조명, 스피커, 텍스트 표시부 및 디스플레이 패널일 수 있다. 상기 제1 표시부는 소리 유형을 표시할 수 있고, 바람직하 게는 추가 분석 장치로부터 전송받은 소리 원인을 표시할 수도 있다. 추가 분석 장치는 제2 표시부를 포함할 수 있다. 제2 표시부는 예를 들면 조명, 스피커, 텍스트 표시부 및 디스플레이 패널일 수 있다. 상기 제2 표시부는 소리 원인을 표시할 수 있고, 바람직하게는 실시간 소리 분석 장치로부터 전송받은 소리 유형을 표시할 수도 있다. 실시간 소리 분석 장치의 구성 요소들은 제1 제어부에 의해 제어된다. 제1 제어부는 입력부 에 의해 주변 소리가 감지되면 신호처리 및 분류를 실행하도록 신호처리부 및 제1 분류기에 명령을 내릴 수 있고, 분류 결과 및 실시간 소리 데이터를 추가 분석 장치에 전송하도록 제1 통신부 에 명령을 전달할 수 있다. 또한, 실시간 소리 데이터의 유입에 따라 제1 학습부가 제1 분류기를 보 완하는 학습을 시킬 것인지 결정할 수 있다. 또한, 상기 제1 제어부는 분류 결과를 제1 표시부에 표 시하도록 제어할 수 있다. 추가 분석 장치의 구성 요소들은 제2 제어부에 의해 제어된다. 제2 제어부는 실시간 소리 분석 장치로부터 데이터를 전송받으면 분류를 실행하도록 제2 분류기에 명령을 내릴 수 있고, 분류 결과를 실시간 소리 분석 장치에 전송하도록 제2 통신부에 명령을 전달할 수 있다. 또한, 실시간 소리 데이 터의 유입에 따라 제2 학습부가 제2 분류기를 보완하는 학습을 시킬 것인지 결정할 수 있다. 또한, 상기 제2 제어부는 분류 결과를 제2 표시부에 표시하도록 제어할 수 있다. 사용자는 이동통신용 단말기에 설치된 애플리케이션을 통해 소리의 유형 및 원인에 대한 분석을 제공받 는다. 즉, 실시간 소리 분석 장치는 제1 통신부에서 신호처리된 실시간 소리 데이터 및 소리 유형 분 류 결과를 제2 통신부로 전송하고 추가 분석 장치는 전송받은 데이터를 기초로 소리 원인을 분류한다. 이후 추가 분석 장치는 실시간 소리 분석 장치 및 추가 분석 장치에서 수행된 분석 결과를 이동통신용 단말기로 전송하고, 사용자는 애플리케이션을 통해 상기 분석 결과에 접근할 수 있 다. 사용자는 분석 결과가 맞는지 틀린지에 대한 피드백을 애플리케이션을 통하여 제공할 수 있고, 상기 피드백 은 추가 분석 장치로 전송된다. 실시간 소리 분석 장치 및 추가 분석 장치는 상기 피드백을 공 유하고, 각 제어부(660, 760)에 의해 대응하는 함수(f1, f2)를 재학습시킨다. 즉, 피드백에 대응하는 실시간 소 리 데이터에 상기 피드백을 반영하여 레이블링하고, 학습부(650, 750)가 분류기(630, 730)를 학습시킴으로써 각 함수의 정확도가 향상된다. 도 2의 실시예에서 추가 분석 장치는 서버일 수 있다. 도 3은 본 발명에 의한 실시간 소리 분석 장치의 제2 실시예를 나타내는 도면이다. 도 2와 중복되는 부분에 대 한 설명은 생략한다. 사용자는 실시간 소리 분석 장치로부터 직접 소리의 유형 및 원인에 대한 분석 결과를 제공받을 수 있 다. 상기 분석 결과는 제1 표시부를 통해 제공될 수 있다. 사용자는 분석 결과가 맞는지 틀린지에 대한 피드백을 실시간 소리 분석 장치에 직접 제공할 수 있고, 상기 피드백은 추가 분석 장치로 전송된다. 실시간 소리 분석 장치 및 추가 분석 장치는 상기 피드백을 공유하고, 각 제어부(660, 760)에 의해 대응하는 함수(f1, f2)를 재학습시킨다. 즉, 피드백에 대응하는 실시간 소리 데이터에 상기 피드백을 반영하여 레이블링하고, 학습부(650, 750)가 분류기(630, 730)를 학습시킴으로써 각 함수의 정확도가 향상된다. 도 3의 실시예에서 추가 분석 장치는 서버일 수 있다. 도 4는 본 발명에 의한 실시간 소리 분석 장치의 제3 실시예를 나타내는 도면이다. 도 2와 중복되는 부분에 대 한 설명은 생략한다. 사용자는 추가 분석 장치로부터 직접 소리의 유형 및 원인에 대한 분석 결과를 제공받을 수 있다. 상기 분석 결과는 제2 표시부를 통해 제공될 수 있다. 사용자는 분석 결과가 맞는지 틀린지에 대한 피드백을 추가 분석 장치에 직접 제공할 수 있고, 상기 피드백은 실시간 소리 분석 장치로 전송된다. 실시간 소리 분석 장치 및 추가 분석 장치는 상기 피드백을 공유하고, 각 제어부(660, 760)에 의해 대응하는 함수(f1, f2)를 재학습시킨다. 즉, 피드백에 대응하는 실시간 소리 데이터에 상기 피드백을 반영하여 레이블링 하고, 학습부(650, 750)가 분류기(630, 730)를 학습시킴으로써 각 함수의 정확도가 향상된다. 도 4의 실시예에서 추가 분석 장치는 이동통신용 단말기의 일부일 수 있다. 즉, 이동통신용 단말기가 추가 분석 장치를 포함할 수 있고, 이 경우 사용자가 추가 분석 장치에 직접 피드백을 입력할 수 있다. 도 5는 본 발명에 의한 실시간 소리 분석 방법에 관한 블록도이다. 본 발명에 의한 실시간 소리 분석 방법 및 장치는 실시간 소리 분석 장치 및 추가 분석 장치의 상호 작용에 의해 작동한다. 사전 수집된 소리 데이터(S001)는 크롤링(Crawling)의 방식으로 수집될 수 있으나 이에 한정되지 않는다. 각 분류기(630, 730)가 최소한의 기능을 수행할 수 있게 하려면 실시간 소리 분석 장치 의 제1 학습부 및 추가 분석 장치의 제2 학습부 모두 적어도 일부가 레이블 된 사전 수집된 소 리 데이터(S001)가 필요하다. 사전 수집된 소리 데이터(S001)는 각 분석 장치(600, 700)로 전달된다. 이러한 사 전 수집된 소리 데이터(S001)에 의해 제1 함수(f1) 및 제2 함수(f2)를 훈련시키는 작업은 분류 작업에 선행된다. 제1 함수(f1) 및 제2 함수(f2)가 훈련에 의해 어느 정도 최적화된 후 입력부를 통해 실시간 소리 데이터 (S002)가 입력되고 나면, 전처리 및 특징 벡터 추출이 포함된 신호처리 단계(S130)가 수행된다. 이후 제1 함수 (f1)를 통해 소리 유형 별로 분류한다. 소리 유형에는 무음, 잡음이 있을 수 있고, 사용자가 관심을 갖는 관심 소리는 적어도 하나가 지정 될 수 있다. 예를 들어 관심 소리는 아기 울음 소리일 수 있고, 관심 소리가 아기 울음 소리 및 부모의 음성일 수 있다. 제1 제어부는 분류된 소리 유형이 관심 소리에 해당하는지 판단할 수 있다. 만약 분류된 소리 유형이 관심 소리에 해당하는 경우, 신호처리 된 실시간 소리 데이터를 실시간 소리 분석 장치에서 추가 분석 장치로 전송한다. 신호처리 된 실시간 소리 데이터를 전송받은 제2 통신부는 이 정보를 제2 분류기로 전달하고, 제2 분 류기는 제2 함수(f2)를 통해 소리 원인 별로 분류한다. 소리 원인에 대한 분류 결과는 외부 장치로 전송될 수 있는데, 상기 외부 장치는 실시간 소리 분석 장치일 수 있으나, 다른 장치일 수도 있다. 제2 통신부를 통해 제1 통신부로 소리 원인 분류 결과를 전송한 후 각 분석 장치(600, 700)의 표시부 는 소리 유형 및/또는 소리 원인에 대한 분석 결과를 출력할 수 있다. 일련의 과정을 거친 후 제1 학습부는 수집된 실시간 소리 데이터를 머신 러닝 방식으로 학습함으로써 제1 함수를 보완할 수 있다. 여기서 사용자의 피드백을 받은 경우에는 상기 피드백에 대응하는 실시간 소리 데이터 를 머신 러닝 방식으로 학습함으로써 제1 함수를 개선하는 것이 바람직하다. 일련의 과정을 거친 후 제2 학습부는 수집된 실시간 소리 데이터를 딥 러닝 방식으로 학습함으로써 제2 함 수를 보완할 수 있다. 여기서 사용자의 피드백을 받은 경우에는 상기 피드백에 대응하는 실시간 소리 데이터를 딥 러닝 방식으로 학습함으로써 제2 함수를 개선하는 것이 바람직하다. 실시간 소리 분석 장치는 신호처리 후 특징 벡터를 추출하여 소리 유형으로 분류한다. 추가 분석 장치 는 실시간 소리 분석 장치로부터 소리 유형이 분류된 실시간 소리 데이터를 수신하여 제2 함수를 통 해 소리 원인으로 분류한다. 각 분석 장치(600, 700)에서 분류 작업 수행이 완료되면 각 함수(f1, f2)를 보완할 수 있다. 본 발명의 일 실시예에서 관심 소리가 아기 울음 소리인 경우가 아닌 단순한 아기 소리인 경우에는 본 발명에 의한 실시간 소리 분석 방법 및 장치가 사용자에게 더욱 유용한 정보를 제공할 수 있다. 즉, 아기는 울기 전에 내는 울음 전 소리(Pre-crying Sound)를 내기도 하는데, 관심 소리가 상기 울음 전 소리이고, 사용자가 이에 대한 소리 유형 및 원인 분석을 제공받을 경우, 아기가 울고 나서 아기 울음 소리 에 대한 분석을 제공받을 경우보다 더 빠른 대응이 가능하다. 도 6은 소리 데이터의 신호처리에 관한 블록도이다. 신호처리부에서는 머신 러닝에 용이하도록 실시간 소리 데이터를 최적화하는데, 최적화는 신호처리에 의해 수행될 수 있다. 바람직하게는 상기 신호처리부는 예를 들어 정규화(Normalization), 주파수 필터링(Frequency Filtering), 템포럴 필터링(Temporal Filtering), 윈도잉(Windowing)과 같은 전처리를 거치고, 전처리된 소리 데이터를 시간 영역의 복수의 프레임들로 구분한 후, 각 프레임 또는 프레임 그룹의 특징 벡터를 추출할 수 있 다. 특징 벡터로 표현된 실시간 소리 데이터는 프레임 별로 또는 프레임 그룹 별로 하나의 단위를 구성할 수 있다. 도 7은 소리 데이터를 프레임별로 분류하여 특징 벡터를 추출하는 일 실시예를 나타내는 도면이다. 시간 영역에서 100ms 단위로 자른 각 프레임(FR1, FR2, FR3, FR4, FR5)을 정의하였고, 이로부터 단일 프레임 특 징 벡터(V1)가 추출된다. 도 7에서처럼 연속된 프레임 5개를 묶어 하나의 프레임 그룹(FG1, FG2, FG3)으로 정의 하였고, 이로부터 프레임 그룹 특징 벡터(V2)가 추출된다. 단일 프레임 별로 분석을 수행할 수도 있으나, 데이 터 처리에 따른 과부하 방지 및 정확성 향상을 위해 프레임 그룹(FG1, FG2, FG3) 별로 분석을 수행할 수도 있다."}
{"patent_id": "10-2018-0075332", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명과 관련된 실시간 소리 분석 방법 및 장치를 설명하기 위한 개념도이다. 도 2는 본 발명에 의한 실시간 소리 분석 장치의 제1 실시예를 나타내는 도면이다. 도 3은 본 발명에 의한 실시간 소리 분석 장치의 제2 실시예를 나타내는 도면이다. 도 4는 본 발명에 의한 실시간 소리 분석 장치의 제3 실시예를 나타내는 도면이다. 도 5는 본 발명에 의한 실시간 소리 분석 방법에 관한 블록도이다. 도 6은 소리 데이터의 신호처리에 관한 블록도이다. 도 7은 소리 데이터를 프레임별로 분류하여 특징 벡터를 추출하는 일 실시예를 나타내는 도면이다."}
