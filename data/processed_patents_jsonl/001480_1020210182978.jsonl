{"patent_id": "10-2021-0182978", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0093938", "출원번호": "10-2021-0182978", "발명의 명칭": "명시적 특성 상호작용을 학습하는 신경망 모델을 활용하는 전자 장치", "출원인": "강원대학교산학협력단", "발명자": "김진호"}}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 메모리; 및상기 신경망 모델에 복수의 입력 특성을 입력하여, 상기 복수의 입력 특성 간의 상호작용을 식별하는,프로세서;를 포함하고,상기 신경망 모델은,Graph Convolutional Network에 해당하는, 제1 모델; 및적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network에해당하는, 제2 모델;을 포함하는, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제2 모델을 구성하는 각 계층의 특성 행렬은 이하 수학식들에 따라 정의되고,X(init)는 상기 제1 모델 및 상기 제2 모델 각각에 입력되는 원시 특성 행렬이고, X(0)는 선형 변환된 원시 특성행렬이고, X(l)은 l번째 계층의 특성 행렬이고, 는 정규화된 인접 행렬이고, W(l)은 l번째 계층의 학습 가중치행렬인, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제2 모델의 각 계층은, 상기 복수의 입력 특성 간의 서로 다른 상호작용에 대하여 학습되는, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1 모델의 각 계층은 상기 복수의 입력 특성의 고차 특성 상호작용에 대하여 학습되는, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 제1 모델의 최종 표현 행렬(X(out)gcn)은, 이하 수학식에 따라 정의되고,Lgcn은 상기 제1 모델의 계층 수인, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,공개특허 10-2023-0093938-3-상기 제2 모델의 최종 표현 행렬(X(out)cross)은, 이하 수학식에 따라 정의되고,Lcross는 상기 제2 모델의 계층 수인, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 신경망 모델의 정답 레이블은, 이하 수학식에 따라 정의되고,W(out)은 학습 가능한 출력 행렬인, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 프로세서는,상기 신경망 모델의 출력을 기반으로 적어도 두 개의 입력 특성 간의 관계를 나타내는 그래프 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 전자 장치의 제어 방법에 있어서,상기 신경망 모델에 복수의 입력 특성을 입력하는 단계; 및상기 신경망 모델의 출력을 기초로, 상기 복수의 입력 특성 간의 상호작용을 식별하는 단계;를 포함하고,상기 신경망 모델은,Graph Convolutional Network에 해당하는, 제1 모델; 및적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network에해당하는, 제2 모델;을 포함하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 제2 모델의 각 계층은, 상기 복수의 입력 특성 간의 서로 다른 상호작용에 대하여 학습되는, 전자 장치의제어 방법."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 제1 모델의 각 계층은, 상기 복수의 입력 특성의 고차 특성 상호작용에 대하여 학습되는, 전자 장치의 제어 방법."}
{"patent_id": "10-2021-0182978", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전자 장치에 있어서,서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 메모리; 및상기 신경망 모델에 복수의 입력 특성을 입력하여, 상기 복수의 입력 특성 간의 상호작용을 식별하는,프로세서;를 포함하고,공개특허 10-2023-0093938-4-상기 신경망 모델은,적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network를포함하는, 전자 장치."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는, 서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저 장된 메모리, 신경망 모델에 복수의 입력 특성을 입력하여, 복수의 입력 특성 간의 상호작용을 식별하는, 프로세 서를 포함한다. 본 신경망 모델은, Graph Convolutional Network에 해당하는 제1 모델, 적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network에 해당하는 제2 모델을 포함한다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공지능 모델을 활용하는 전자 장치에 관한 것으로, 보다 상세하게는, 특성 간의 상호작용을 학습하 는 인공지능 모델을 활용하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "특성 상호작용이란 서로 다른 입력 특성들의 결합이 예측 값에 미치는 영향을 의미한다. 기존의 심층 학습(Deep Learning) 방법들은 고차 특성 상호작용만 학습할 수 있다. 때문에, 저차 특성 상호작용에서만 얻을 수 있는 정 보를 놓치는 경우가 있다. 이러한 문제를 해결하기 위해, 다양한 수준의 특성 상호작용을 동시에 학습하기 위한 방법들이 제안되었다. Wide & Deep Learning은 선형 회귀와 심층 학습을 결합한 방법이다. 선형 회귀는 1차 특성 상호작용을 학습하고, 심층 학습은 고차 특성 상호작용을 학습한다. 도 1은 Wide & Deep Learning의 구조를 보여준다. Wide & Deep Learning은 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "여기서, ∏는 Wide 컴포넌트, a는 Deep 컴포넌트, x는 입력 데이터, W는 학습 가능한 가중치, 그리고 b는 학습 가능한 편향이다. Cross Network은 명시적 고차 특성 상호작용을 학습할 수 있는 네트워크이다. 심층 학습 모델과 유사한 구조를 갖고 있으며, 레이어 수에 따라 다른 특성 상호작용을 학습할 수 있다. 예를 들어, 레이어가 1개인 Cross Network는 1차 특성 상호작용을 학습할 수 있으며, 레이어가 2개인 Cross Network는 2차 특성 상호작용을 학습 할 수 있다. 도 2는 Cross Network의 구조를 보여준다. Cross Network는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "여기서, x0는 입력 특징, wl은 학습 가능한 가중치, 그리고 bl은 학습 가능한 편향이다. DeepFM은 행렬 분해 모델인 Factorization Machine(FM)과 심층 학습을 결합한 것이다. FM은 2차 특성 상호작용 을 학습한다. 도 3은 DeepFM의 구조를 보여준다. DeepFM은 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "여기서, yFM은 FM 컴포넌트, 그리고 a는 Deep 컴포넌트이다. 위에서 설명한 방법들의 문제점은 그래프 데이터를 다룰 수 없다는 것이다. 그래프는 복잡한 객체 관계를 효율 적으로 표현할 수 있는 데이터로, 그래프를 사용하면, 객체 사이의 구조나 패턴을 찾아내기가 쉬워진다. 최근, 그래프 구조를 사용해 성능을 향상시키기 위한 다양한 심층 학습 방법들이 제안되었다.Graph Neural Network은 그래프 데이터를 직접 다루기 위한 최초의 심층 학습 방법이다. 현재 정점의 새로운 표 현 벡터를 생성하기 위해 연결된 이웃 정점들의 표현 벡터를 집계한다. 도 4는 Graph Neural Network의 구조를 보여준다. Graph Neural Network는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 5, "content": "여기서 xn은 정점 n의 표현 벡터, fw는 집계 함수, ln은 정점 n의 특징, lco[n]은 연결된 간선의 특징, xne[n]은 이 웃 정점들의 표현 벡터, 그리고 lne[n]은 이웃 정점들의 특징이다. on은 정점 n의 출력 값, gw는 출력 함수이다. Graph Convolutional Network는 기존의 Graph Neural Network를 개선한 방법이다. 합성곱 신경망의 개념을 적 용해, 시간이 오래 걸리던 표현 벡터 수렴 과정을 제거했다. 도 5는 Graph Convolutional Network의 구조를 보 여준다. Graph Convolutional Network는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 6, "content": "여기서 는 는 자가 연결이 추가된 그래프의 인접 행렬, 는 의 차수 행렬, W(l) 은 학습 가중치, 그리고 은 활성화 함수이다. Graph Attention Network는 Graph Convolutional Network의 발전된 형태로 이웃 정점들의 중요도를 계산할 수 있다. 중요한 정점은 다음 계층에 더 큰 영향을 미치며, 결과적으로 모델의 성능이 향상된다. 도 6은 Graph Attention Network의 구조를 보여준다. Graph Attention Network는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 7, "content": "여기서 a는 어텐션 벡터, W는 학습 가중치, hi는 정점 i의 표현 벡터, 그리고 σ는 활성화 함수이다. 위에서 설명한 그래프 방법들은 모두 암시적 고차 특성 상호작용만 학습 할 수 있다. 따라서, 저차 특성 상호작 용에서 발생하는 정보를 포착하지 못한다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 기존 방법들의 문제를 해결하기 위해 그래프에서 다양한 수준의 명시적 특성 상호작용을 학습할 수 있는 그래프 교차 네트워크(Graph Cross Network, GCross)를 제안하고, 다양한 수준의 명시적 및 암시적 특성 상호작용을 동시에 학습할 수 있도록 그래프 교차 네트워크와 기존의 Graph Convolutional Network를 결합한 GCN & GCross를 제안한다. 본 개시의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 개시의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 개시의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 개시의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 전자 장치는, 서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 메모리, 상기 신경망 모델에 복수의 입력 특성을 입력하여, 상기 복수의 입력 특성 간의 상호작용을 식 별하는, 프로세서를 포함한다. 상기 신경망 모델은, Graph Convolutional Network에 해당하는, 제1 모델, 적어 도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network에 해 당하는, 제2 모델을 포함한다. 본 개시의 일 실시 예에 따라 서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 전자 장치의 제어 방법은, 상기 신경망 모델에 복수의 입력 특성을 입력하는 단계, 상기 신경망 모델의 출력을 기초 로, 상기 복수의 입력 특성 간의 상호작용을 식별하는 단계를 포함한다. 상기 신경망 모델은, Graph Convolutional Network에 해당하는, 제1 모델, 적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으로 대체된 Graph Cross Network에 해당하는, 제2 모델을 포함한다. 본 개시의 일 실시 예에 따른 전자 장치는, 서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델이 저장된 메모리, 상기 신경망 모델에 복수의 입력 특성을 입력하여, 상기 복수의 입력 특성 간의 상호작용을 식 별하는, 프로세서를 포함한다. 상기 신경망 모델은, 적어도 하나의 Graph Convolutional Network의 활성화 함수 가 특성 교차 항으로 대체된 Graph Cross Network를 포함한다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에서 제안하는 Graph Cross Network와 GCN & GCross는 그래프 분석이 필요한 모든 분야에 적용 가능하다. 최근 그래프 데이터 및 그래프 분석에 대한 관심과 중요도가 높아지면서 다양한 연구 분야 및 산업 분야에 응용될 수 있을 것으로 기대한다. 특히, 그래프 데이터가 많이 활용되는 추천 시스템 및 계산 생물학 등 에 성공적으로 활용될 수 있을 것이다. 추천 시스템은 사회 관계망 분석 사용자-아이템 상호작용 분석 등 그래 프 데이터가 많이 활용되며, 그래프 분석이 매우 중요하다. 본 개시에서 제안하는 방법들을 사용하면, 추천 시 스템에서 정확도를 높이고, 어떤 특성들이 추천에 중요한 역할을 하는지 알아낼 수 있을 것이다. 또한, 계산 생 물학에서도 단백진 상호작용 분석, 화학 반응 분석 등 그래프 분석이 많이 필요하다. 본 개시에 따른 Graph Cross Network와 GCN & GCross를 사용하면 계산 생물학에서도 좋은 성과를 얻을 수 있을 것으로 기대한다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소를 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 먼저, 도 7은 본 개시의 일 실시 예에 따라 하나 이상의 신경망 네트워크를 포함하는 전자 장치의 구성을 설명 하기 위한 블록도이다. 도 7을 참조하면, 전자 장치는 메모리 및 프로세서를 포함한다. 전자 장치는 다양한 컴퓨 터 내지는 전자 기기에 해당할 수 있다. 구체적인 예로, 전자 장치는 서버, 스마트폰, 데스크탑 PC, 노트 북 PC 등 다양한 기기로 구현될 수 있다. 또한, 전자 장치는 서로 통신 가능한 하나 이상의 전자 기기가 결합된 시스템으로 구현될 수도 있다. 메모리는 전자 장치에 포함되는 각 구성의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 전자 장치의 구성요소와 관련된 적어도 하나의 인스트럭션 또는 데이터를 저장하기 위한 구성이다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘발성 메모 리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid state drive) 등을 포함할 수도 있다. 메모리는 서로 다른 입력 특성 간의 상호작용을 판단하기 위한 신경망 모델을 저장할 수 있다. 도 7을 참조하면, 메모리에 포함된 신경망 모델은, Graph Convolutional Network에 해당하는 제1 모델 , Graph Cross Network에 해당하는 제2 모델을 각각 포함할 수 있다. 이때, 제1 모델 및 제2 모델은 서로 결합된 형태일 수 있다. 여기서, Graph Cross Network는, 적어도 하나의 Graph Convolutional Network의 활성화 함수가 특성 교차 항으 로 대체된 것일 수 있으나, 이에 한정되지 않는다. 프로세서는 전자 장치를 전반적으로 제어하기 위한 구성이다. 구체적으로, 프로세서는 메모리 와 연결되는 한편 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써 본 개시의 다양한 실시 예들에 따른 동작을 수행할 수 있다. 프로세서는 하나 이상의 프로세서로 구성될 수 있다. 이때, 하나 이상의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로 세서 또는 NPU와 같은 인공지능 전용 프로세서 등을 포함할 수 있다. 프로세서는 메모리에 저장된 상술한 인공지능 모델들(121, 122)을 각각 구동할 수 있으며, 각 모델의 훈련도 수행할 수 있다. 구체적인 예로, 프로세서는 신경망 모델(111, 112)에 복수의 입력 특성을 입력하여, 복수의 입력 특성 간 의 상호작용을 식별할 수 있다. 이하 내용을 통해서는, 명시적 특성 상호작용을 학습하기 위한 새로운 그래프 방법인 그래프 교차 네트워크 (112. Graph Cross Network, GCross)가 보다 구체적으로 설명된다. Graph Cross Network를 소개하기에 앞서, 먼저, 기존의 Graph Convolutional Network를 특성 상호작용 관 점에서 새롭게 조명해 본다. Graph Convolutional Network는 두 개의 항으로 이루어진 인공 신경망 모델이다. 첫 번째 항 은 정점 집계 항이다. 여기서는 이웃 정점들의 정보가 집계된다. 두 번째 항 은 특성 집계 항이다. 여기서는 특성들의 정보가 집계된다. 따라서, Graph Convolutional Network를 다음과 같이 일반화할 수 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 v는 현재 정점, 그리고 N(v)는 정점 v의 이웃 정점들이다. 우리의 목표는 특성 집계 항 FeatAgg(ㆍ)을 명시적으로 만드는 것이다. 제안하는 Graph Cross Network를 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 X(init)는 원시 특성 행렬, 는 정규화된 인접 행렬, W(l)은 l번째 계층의 학습 가중치 행렬이다. 구체적 으로, Graph Cross Network는 Graph Convolutional Network에서 활성화 함수 대신 특성 교차 항이 추가된 것이다. Graph Cross Network는 레이어 수가 증가하면 더 높은 수준의 명시적 특성 상호작용을 학습할 수 있다. 특 성 교차 덕분에 Graph Cross Network는 활성화 함수 없이도 데이터의 비선형성을 포착할 수 있다. 즉, 명시적 특성 상호작용을 학습할 수 있다. 실제로 Graph Cross Network가 어떻게 명시적 특성 상호작용을 학습하는지 이하 확인될 수 있다. 수식의 간결함 을 위해 여기서 학습 가중치는 생략된다. X는 선형 변환된 원시 특성 행렬을 나타낸다. 즉, X는 1차 특성 상호작용들의 합이다. 다음과 같이 수식을 간결화 할 수 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 M은 원시 특성의 수이다. X(l)은 이웃 정점 집계와 특성 교차가 적용된 X(l-1)의 선형 변환이다. 이하와 같 이 다음 단계의 특성 상호작용을 학습하기 위해 X(l-1)에 특성 교차가 적용될 수 있다. 이하와 같이 수식이 간결 화된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이것은 이웃 정점들의 특성을 고려한 (l+1)차 특성 상호작용의 합을 나타낸다. 기존의 Cross Network와는 다르 게 Graph Cross Network는 연재 정점의 특성 뿐 아니라 이웃 정점들의 특성까지 고려할 수 있다. 도 8은 본 개 시에 따라 제안되는 Graph Cross Network의 구조를 보여준다. Graph Cross Network에서 l계층의 표현 벡터는 (l-1)차 특성 상호작용을 포함하고 있다. 정점 X1의 이웃 정점은 X2, X3, X4, X5이다. Graph Cross Network의 l 번째 계층에서 X1의 새로운 표현 벡터를 생성하기 위해서는 이웃 정점들의 정보를 집계한 후 X와 특성 교차를 한다. 최종적으로 생성된 표현 벡터의 한 셀에는 이하와 같은 특성 상호작용이 내포되어 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "다양한 수준의 명시적 특성 상호작용과 암시적 특성 상호작용을 결합하기 위해 Graph Convolutional Network와 Graph Cross Network의 결합인 GCN & GCross를 함께 제안한다(도 7의 메모리는 이렇듯 두 가지 네트워크 가 결합된 형태를 저장할 수 있다). Graph Cross Network에서 각각의 계층은 서로 다른 특성 상호작용을 학습한 다. Graph Convolutional Network에서 각각의 계층은 고차 특성 상호작용을 학습한다. 하지만, 상위 계층에서는 하위 계층 보다 더 복잡한 데이터의 패턴을 포착할 수 있다. 따라서, Graph Cross Network와 Graph Convolutional Network의 각 계층은 서로 다른 데이터의 패턴을 포착한다. 따라서, Graph Cross Network와 Graph Convolutional Network의 모든 계층을 사용해서 예측하는 것은 정확도 향상에 도움이 된다. 다음과 같이 그래프 교차 네트워크의 최종 표현 행렬이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서 는 연결연산자이고 Lcross는 Graph Cross Network의 계층 수이다. Graph Cross Network와 유사하게, Graph Convolutional Network의 최종 표현 행렬은 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기서 Lgcn은 Graph Convolutional Network의 계층 수이다. 마지막으로, 제안하는 GCN & GCross는 다음과 같이 정의될 수 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기서 W(out)은 학습 가능한 출력 행렬이다. 구체적으로 GCN & GCross는 그래프 교차 네트워크와 Graph Convolutional Network의 모든 계층의 결과를 사용해서 선형회귀를 수행하는 것이다. 알고리즘 1은 GCN & GCross의 학습 과정을 보여준다. 본 개시에서 제안하는 Graph Cross Network와 CN & GCross의 성능을 알아보기 위해 세 개의 대표적인 데이터셋 인 Cora, CiteSeer 그리고 PubMed를 사용한다. 또한, Graph Cross Network를 사용해 학습된 명시적 특성 상호 작용이 노드 임베딩에 효과적인지 알아보기 위해 두 개의 Open Graph Benchmark (OGB) 데이터셋인 ogbn- arxiv와 ogbn-mag을 사용한다. 표 1은 실험 데이터셋들의 통계 값을 보여준다. 표 1"}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "이 다섯 개의 데이터셋들은 모두 인용 네트워크 데이터셋이다. 각 데이터셋의 목표는 노드들 (즉, 논문들)이 어 떤 분야에 속하는지 클래스 레이블을 예측하는 것이다. Cora, CiteSeer 그리고 PubMed 데이터셋에 대해 GCN & GCross의 초-매개변수는 표 2와 같이 설정되었다.표 2"}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "ogbn-arxiv와 ogbn-mag 데이터셋에 대해 각 모델들의 초-매개변수는 표 3과 같이 설정되었다. 또한, 도 9는 실 험에 사용된 각 모델들의 구조를 보여준다. 도 9의 (e)는 본 발명에서 제안하는 GCN & GCross의 구조를 보여준 다. Xgcn은 Graph Convolutional Network에서 생성된 표현 행렬들이며, Xcross는 Graph Cross Network에서 생성된 표현 행렬들이다. 원시 입력 특성 X(init)은 두 모델에 입력된다. X는 X(init)의 선형 변환으로 정의된다. 그 후 각 모델에서 표현 행렬을 생성하고, 생성된 표현 행렬들이 모두 연결되고 X(out)을 정의한다. X(out)은 다시 선형 변환되어 최종 예측 값 를 생성한다. 표 3"}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "본 개시에 따른 GCN & GCross는, 세 개의 대표적인 인용 네트워크 데이터셋인 Cora, CiteSeer 그리고 PubMed를 사용해 기존의 최고 성능 방법인 GCN, FastGCN, GraphSage 그리고 ASGCN과 비교되었다. 표 4는 각 모델의 평가 정확도를 보여준다. 본 개시에서 제안하는 GCN & GCross가 모든 데이터셋에서 가장 높은 정확도를 보인다. 이는 다양한 수준의 명시적 특성 상호작용과 암시적 특성 상호작용을 결합해 학습하는 것이 노드 임베딩에 효과적임을 보여준다. 여기서, ASGCN과 달리 모델을 학습하는데 어떠한 부분 샘플링 방법도 사용 되지 않았다. 또한, Graph Cross Network를 통해 학습된 명시적 특성 상호작용이 정말 노드 임베딩에 효과적인지 알아보기 위 해 두 개의 OGB 데이터셋인 ogbn-arxiv와 ogbn-mag이 사용되었다. 표 4 표 5는 각각의 실험 모델들에 대한 평가 정확도를 보여준다. GCN과 GCross는 마지막 계층의 결과만 사용해 출력 값을 예측한다. 즉, 한 수준의 특성 상호작용만 학습할 수 있다. GCN(concat)과 GCross(concat)은 모든 계층의 결과를 사용해 출력 값을 예측한다. 그리고 GCN & GCross는 GCN(concat)과 GCross(concat)을 결합한 모델이다. 즉, 여러 수준의 명시적 특성 상호작용과 암시적 특성 상호작용을 동시에 학습할 수 있다. 표 5"}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "공정한 평가를 위해 두 개의 GCN(concat)을 결합한 GCN & GCN 그리고 두 개의 GCross를 결합한 GCross & GCross도 같이 실험이 진행되었다. GCN(concat)과 GCross(concat)이 각각 GCN과 GCross 보다 높은 정확도를 보 인다. 이는 한 수준의 특성 상호작용만 사용해 학습하는 것 보다 여러 수준의 특성 상호작용을 결합해 학습하는 것이 노드 임베딩에 효과적임을 보여준다. 또한, GCN & GCross는 GCN & GCN 그리고 GCross & GCross 보다 높은 정확도를 보인다. 이는 암시적 특성 상호작용 또는 명시적 특성 상호작용 한 가지만 학습하는 것 보다 두 가지 를 모두 같이 학습하는 것이 효과적이라는 것을 보여준다. 실험 결과를 통해 제안하는 GCN & GCross가 기존의 최고 성능 방법들 보다 더 높은 정확도를 갖고 있으며, GCross를 사용해 명시적 특성 상호작용을 동시에 학습하는 것에 효과적임을 알 수 있다. Graph Cross Network는 근본적으로 선형 모델이기 때문에, 만약 계층이 출력 계층과 연결되어 있다면, 계층에 존재하는 특성들의 중요도를 얻을 수 있다. 이것은 Graph Cross Network가 설명 가능한 인공지능임을 보여준다. 한 가지 흥미로운 점은 대부분의 설명 가능한 인공지능 모델들은 설명 가능성을 얻기 위해 성능을 희생한다, 반 면, 본 개시에서 제안하는 Graph Cross Network는 기존의 Graph Convolutional Network와 거의 같은 성능을 보 인다는 것이다. 더 나아가 Graph Cross Network를 다른 기계 학습 방법과 결합하면 더 높은 성능을 얻을 수 있다. 한편, 특성의 중요도를 얻기 위한 과정을 정의하고 각 특성이 예측 결과에 얼마나 영향을 미치는지 히트 맵을 사용해 시각화 할 수 있다. 먼저 1차 특성의 중요도는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "여기서, an,i는 노드 n의 1차 특성 i의 표현 벡터, W(out):u,c는 [0:u]으로 인덱싱된 클래스 c의 출력 벡터, u는 계층의 유닛 수, 그리고 ec,n,i는 클래스 c에 대한 노드 n의 1차 특성 i의 중요도이다. an,i는 중요도 ec,n,i 를 얻기 위해 W(out):u,c와 곱해진다. 비슷하게, 2차 특성의 중요도는 다음과 같이 정의된다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "여기서 an,i,j는 2차 특성 의 표현 벡터, 그리고 ec,n,i,j는 클래스 c에 대한 2차 특성 의 중요도이다. 따라서, 다음과 같이 수식이 일반화될 수 있다."}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "위의 수식을 통해, 임의의 수의 특성들의 결합이 출력 값에 미치는 영향이 확인될 수 있다. 도 10은 Cora 데이터셋에서 클래스 1에 대한 노드 1의 1차와 2차 특성들의 중요도를 보여준다. 노드에는 아홉 개의 특성들이 활성화되어 있다. 빨간 셀은 긍정적인 영향을 미치는 특성을 의미하며, 파란 셀은 부정적인 영향 을 미치는 특성을 의미한다. 한편, 이상에서 설명된 다양한 실시 예들은 서로 저촉되지 않는 한 복수의 실시 예가 결합되어 구현될 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 시스템 내 각 장치에서의 처리동작을 수행하기 위한 컴퓨터 명령어(computer instructions) 또는 컴퓨터 프로그램은 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어 또는 컴퓨터 프로그램은 특정 기기 내지는 시스템 내의 적어도 하나의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따라 시스템에 포함되는 각 모듈에서의 처리 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2021-0182978", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2021-0182978", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래 기술에 해당하는 Wide & Deep Learning의 구조를 설명하기 위한 도면, 도 2는 종래 기술에 해당하는 Cross Network의 구조를 설명하기 위한 도면, 도 3은 종래 기술에 해당하는 DeepFM의 구조를 설명하기 위한 도면, 도 4는 종래 기술에 해당하는 Graph Neural Network의 구조를 설명하기 위한 도면, 도 5는 종래 기술에 해당하는 Graph Convolutional Network의 구조를 설명하기 위한 도면, 도 6은 종래 기술에 해당하는 Graph Attention Network의 구조를 설명하기 위한 도면, 도 7은 본 개시의 일 실시 예에 따라 하나 이상의 신경망 네트워크를 포함하는 전자 장치의 구성을 설명하기 위 한 블록도, 도 8은 본 개시의 일 실시 예에 따른 Graph Cross Network의 구조를 설명하기 위한 도면, 도 9는 본 개시에 따른 신경망 모델의 성능을 입증하기 위한 실험에 사용된 각 모델들의 구조를 설명하기 위한 도면, 그리고 도 10은 Cora 데이터셋에서 클래스 1에 대한 노드 1의 1차와 2차 특성들의 중요도를 나타낸 도면이다."}
