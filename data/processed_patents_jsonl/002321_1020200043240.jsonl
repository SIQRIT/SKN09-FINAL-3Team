{"patent_id": "10-2020-0043240", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0125719", "출원번호": "10-2020-0043240", "발명의 명칭": "인공지능 서비스를 위한 서버 클라이언트 통신 장치 및 방법", "출원인": "한국전자통신연구원", "발명자": "김상철"}}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "서버가 클라이언트 응용의 공개 메시지 채널을 통한 서비스 등록 요청에 기반하여 상기 클라이언트 응용에 대한전용 서비스 스레드를 생성하여 상기 서비스를 등록하는 단계;상기 전용 서비스 스레드가 공유 메모리에 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및상기 클라이언트 응용의 비공개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 전용 서비스 스레드를 해제하여 상기 서비스를 등록 해제하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 서비스를 등록하는 단계는 상기 서버가 상기 클라이언트 응용이 접속할 수 있는 상기 공개 메시지 채널을 생성하는 단계;상기 클라이언트 응용이 상기 공개 메시지 채널을 통해 서비스 등록 요청을 송신하는 단계;상기 서버가 상기 서비스 등록 요청에 기반하여 상기 클라이언트 응용을 위한 상기 전용 서비스 스레드를 생성하는 단계;상기 전용 서비스 스레드가 상기 클라이언트 응용과 통신할 수 있는 상기 비공개 메시지 채널을 생성하는 단계;상기 전용 서비스 스레드가 상기 공유 메모리 내에 전용 공유 메모리를 생성하는 단계; 및상기 전용 서비스 스레드가 상기 전용 공유 메모리의 주소를 상기 비공개메시지 채널을 통해 상기 클라이언트응용에게 전달하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 상기 신경망을 생성하는 단계는상기 전용 서비스 스레드가 복수개의 NPU(Neuromorphic Processing Unit)들 중 하나의 전용 NPU와 연결되는 단계;상기 전용 NPU가 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion)방법을 이용하여 연결되는 단계; 및상기 전용 NPU가 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서, 상기 신경망을 생성하는 단계는상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서비스 스레드에게 신경망 생성 요청을송신하는 단계;상기 전용 서비스 스레드가 상기 신경망 생성 요청에 기반하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및상기 전용 서비스 스레드가 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 신경망 생성 완료 통지공개특허 10-2021-0125719-3-를 전달하는 단계를 더 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4 항에 있어서, 상기 신경망을 생성하는 단계는상기 클라이언트 응용이 상기 신경망 생성 완료 통지에 기반하여 상기 신경망의 출력 결과를 수신하는 단계; 및상기 클라이언트 응용이 상기 서비스의 종료 여부를 판단하는 단계를 더 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 상기 서비스의 종료 여부를 판단하는 단계는상기 클라이언트 응용이 신경망 생성이 더 필요한 경우, 상기 전용 서비스 스레드에게 신경망 생성 요청을 송신하는 단계; 및상기 클라이언트 응용이 신경망 생성이 더 이상 필요하지 않은 경우, 상기 전용 서비스 스레드에게 서비스 등록해제 요청을 송신하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제2 항에 있어서, 상기 서비스를 등록 해제하는 단계는상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서비스 스레드에게 서비스 등록 해제 요청을 송신하는 단계;상기 전용 서비스 스레드가 생성한 상기 신경망 및 전용 공유 메모리를 해제하는 단계; 상기 전용 서비스 스레드가 상기 비공개 메시지를 통하여 상기 클라이언트 응용에게 해제 완료를 통보하는단계; 및상기 전용 서비스 스레드가 서비스를 종료하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2 항에 있어서,상기 신경망을 생성하는 단계는상기 클라이언트 응용이 상기 전용 공유 메모리의 입력 버퍼에 입력 이미지를 입력하는 단계;상기 클라이언트 응용이 상기 비공개 메시지 채널을 통해 상기 전용 서비스 스레드에게 상기 입력 이미지에 대한 추론을 요청하는 단계;상기 전용 서비스 스레드가 상기 입력 이미지로부터 신경망을 생성하는 단계;상기 전용 서비스 스레드가 상기 전용 공유 메모리의 출력 버퍼에 상기 신경망의 추론 결과를 저장하는 단계;및상기 전용 서비스 스레드가 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 추론 완료를 통보하는단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제3 항에 있어서,공개특허 10-2021-0125719-4-상기 신경망을 생성하는 단계는로더(Loader)에 의해 사전에 입력된 커널(Kernel)을 이용하는 것이고,상기 커널은 가중치, 편차 및 배치 정규 표준화를 위한 스케일링 파라미터를 포함하는, 인공지능 서비스를 위한서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항에 있어서상기 공개 메시지 채널은상기 서버가 사전에 공개 Key를 상기 클라이언트 응용에게 공개하고,상기 클라이언트 응용이 상기 공개 Key를 통해 상기 서버에 접속하는 방식으로 생성되는 것인, 인공지능 서비스를 위한 서버 클라이언트 통신 방법."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "클라이언트 응용으로부터 서비스 등록 요청을 수신할 수 있는 공개 메시지 채널을 생성하고, 상기 클라이언트 응용의 서비스 등록 요청에 기반하여 상기 클라이언트 응용의 전용 서비스 스레드를 생성하고,상기 클라이언트 응용과 통신할 수 있는 비공개 메시지 채널을 생성하고,적어도 하나 이상의 가속기를 이용하여 신경망을 생성하고,상기 클라이언트 응용의 상기 비공개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 클라이언트응용의 전용 서비스 스레드를 해제하는 프로세서; 및 상기 전용 서비스 스레드가 생성한 신경망을 저장하는 공유 메모리를 포함하는, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 프로세서는상기 클라이언트 응용이 접속할 수 있는 상기 공개 메시지 채널을 생성하고,상기 클라이언트 응용이 상기 공개 메시지 채널을 통해 송신한 서비스 등록 요청을 수신하고,상기 등록 요청에 기반하여 상기 클라이언트 응용을 위한 상기 전용 서비스 스레드를 생성하는 것이고, 상기 전용 서비스 스레드는 상기 클라이언트 응용과 통신할 수 있는 상기 비공개 메시지 채널을 생성하고, 상기 공유 메모리 내에 전용 공유 메모리를 생성하고,상기 전용 공유 메모리의 주소를 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 전달하는 것인, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서,상기 전용 서비스 스레드가 상기 클라이언트 응용으로부터 상기 비공개 메시지 채널을 통해 신경망 생성 요청을 수신하고,복수개의 NPU(Neuromorphic Processing Unit)들 중에서 하나의 전용 NPU와 연결되고,상기 전용 NPU는 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion)방법을 이용하여 연결되고,상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 것인, 인공지능 서비스를 위한 서버.공개특허 10-2021-0125719-5-청구항 14 제13 항에 있어서,상기 전용 서비스 스레드가상기 클라이언트 응용의 상기 비공개 메시지를 통한 신경망 생성 요청을 수신하고,상기 신경망 생성 요청에 기반하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하고,상기 비공개 메시지를 통하여 상기 클라이언트 응용에게 신경망 생성 완료 통지를 전달하는 것인, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12 항에 있어서,상기 전용 서비스 스레드는 상기 클라이언트 응용의 상기 비공개 메시지를 통한 서비스 등록 해제 요청을 수신하고, 생성한 상기 신경망 및 상기 전용 공유 메모리를 해제하고,상기 비공개 메시지 채널을 통하여 상기 클라이언트 응용에게 해제 완료를 통보하고, 상기 비공개 메시지 채널을 제거하는 것인, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12 항에 있어서,상기 전용 서비스 스레드는상기 클라이언트 응용의 상기 비공개 메시지 채널을 통한 추론 요청에 기반하여, 상기 클라이언트 응용이 상기 전용 공유 메모리에 입력한 상기 입력 이미지로부터 신경망을 생성하고, 상기 전용 공유 메모리의 출력 버퍼에 상기 신경망의 추론 결과를 저장하고,상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 추론 완료를 통보하고, 상기 서비스를 종료하는 것인, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제13 항에 있어서,상기 전용 서비스 스레드는 상기 신경망을 생성하기 위하여 로더(Loader)에 의해 사전에 입력된 커널(Kernel)을 이용하는 것이고,상기 커널은 가중치, 편차 및 배치 정규 표준화를 위한 스케일링 파라미터를 포함하는, 인공지능 서비스를 위한서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11 항에 있어서,상기 공개 메시지 채널은상기 프로세서가 사전에 공개 Key를 상기 클라이언트 응용에게 공개하고,상기 클라이언트 응용이 상기 공개 Key를 통해 상기 프로세서에 접속하는 방식으로 생성되는 것인, 인공지능 서비스를 위한 서버."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "공개특허 10-2021-0125719-6-서버에게 공개 메시지 채널을 통해 서비스 등록 요청을 송신하고,상기 서버가 상기 서비스 등록 요청에 기반하여 생성한 전용 서비스 스레드에게 비공개 메시지 채널을 통해 신경망 생성 요청을 송신하고,상기 전용 서비스 스레드가 공유 메모리에 생성한 신경망의 출력 결과를 수신하는 프로세서; 및상기 신경망의 출력 결과를 저장하는 공유 메모리를 포함하는, 인공지능 서비스를 위한 클라이언트."}
{"patent_id": "10-2020-0043240", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제19 항에 있어서,상기 프로세서는 신경망 생성이 더 필요한 경우, 상기 전용 서비스 스레드에게 신경망 생성 요청을 송신하고,신경망 생성이 더 이상 필요하지 않은 경우, 상기 전용 서비스 스레드에게 서비스 등록 해제 요청을 송신하는것인, 인공지능 서비스를 위한 클라이언트."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "기재된 실시예는 본 발명은 인공지능 서비스를 수행하기 위한 인공 지능 가속칩을 운영하고 제어하는 장치 및 방 법에 관한 것으로, 서버가 클라이언트 응용의 공개 메시지 채널을 통한 서비스 등록 요청에 기반하여 상기 클라 이언트 응용에 대한 전용 서비스 스레드를 생성하여 상기 서비스를 등록하는 단계; 상기 전용 서비스 스레드가 공유 메모리에 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및 상기 클라이언트 응용의 비공 개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 전용 서비스 스레드를 해제하여 상기 서비스를 등록 해제하는 단계를 포함하는, 인공지능 서비스를 위한 서버 클라이언트 통신 방법에 관한 것이다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 서비스를 수행하기 위한 인공 지능 가속칩을 운영하고 제어하는 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 인공지능 처리에 대한 요구사항이 높아짐에 따라 이를 위한 전용 칩이 속속 출시되고 있다. 이러한 전용 칩은 그래픽 처리를 위한 전용칩 GPU(Graphic Processing Unit)와는 구별되는 것으로, 대량의 뉴럴 프로세싱 (Neural Processing, 신경망 처리)만을 목적으로 하기 때문에 NPU(Neuromorphic Processing Unit)라고 부른다. NPU는 인공지능 관련 가속기를 내장하고 있는 칩으로서 컨벌루션(Convolution), 풀링(Pooling), 배치 정규화 (Batch Normalization) 등 많은 양의 뉴런 연산을 빠르게 처리해 낼 수 있다. 상기 NPU의 능력도 NPU의 가속기 에 뉴런 집적도가 높아짐에 따라 점점 진화하고 있고, 이에 따라 인공지능 응용 프로그램이 요구하는 연산의 양 이나 개수도 점차 증가하고 있다. 따라서 NPU내의 뉴럴 프로세싱 가속기와 구동 소프트웨어는 다양한 인공지능 응용 프로그램들의 요구 사항을 만 족시켜 줄 수 있도록 설계되어야 하고, 특히 복수개의 응용 프로그램이나 복수개의 NPU를 동시에 또는 이시에 구동할 수 있도록 지원하여야 한다. 따라서 인공지능 응용의 수가 증가할 경우에는 NPU와 상기 응용을 연결하는 중개자가 반드시 필요하다. 따라서 이와 같이 중개자 역할을 하는 서버 운영체제인 서버와 인공지능 응용 프로그램들과의 통신 방법을 본 발명에서 제안하고자 한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허 제 10-2019-0110500호(2019. 9. 30.)"}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 복수개의 응용 프로그램이 복수개의 NPU와 가속기를 효율적으로 이용하여 신경망 처리를 할 수 있도록, 인공지능 서비스를 위한 서버 클라이언트 통신 장치 및 방법을 제공함에 있다. 또한, 본 발명의 목적은 복수의 인공지능 응용 프로그램이 실행될 때 서버 클라이언트 형태로 실행됨으로써, 인 공지능 서비스를 위한 절차와 수행 역할의 구분을 명확하게 하고, 복잡하고 다양한 인공지능 서비스를 운영하는 시스템을 쉽게 구성할 수 있는 방법을 제공함에 있다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법은 서버가 클라이언트 응용의 공개 메시지 채 널을 통한 서비스 등록 요청에 기반하여 상기 클라이언트 응용에 대한 전용 서비스 스레드를 생성하여 상기 서 비스를 등록하는 단계; 상기 전용 서비스 스레드가 공유 메모리에 적어도 하나 이상의 가속기를 이용하여 신경 망을 생성하는 단계; 및 상기 클라이언트 응용의 비공개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 전용 서비스 스레드를 해제하여 상기 서비스를 등록 해제하는 단계를 포함한다. 이 때, 상기 서비스를 등록하는 단계는 상기 서버가 상기 클라이언트 응용이 접속할 수 있는 상기 공개 메시지 채널을 생성하는 단계; 상기 클라이언트 응용이 상기 공개 메시지 채널을 통해 서비스 등록 요청을 송신하는 단 계; 상기 서버가 상기 서비스 등록 요청에 기반하여 상기 클라이언트 응용을 위한 상기 전용 서비스 스레드를 생성하는 단계; 상기 전용 서비스 스레드가 상기 클라이언트 응용과 통신할 수 있는 상기 비공개 메시지 채널을 생성하는 단계; 상기 전용 서비스 스레드가 상기 공유 메모리 내에 전용 공유 메모리를 생성하는 단계; 및 상기 전용 서비스 스레드가 상기 전용 공유 메모리의 주소를 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용 에게 전달하는 단계를 포함할 수 있다. 이 때, 상기 신경망을 생성하는 단계는 상기 전용 서비스 스레드가 복수개의 NPU(Neuromorphic Processing Unit)들 중 하나의 전용 NPU와 연결되는 단계; 상기 전용 NPU가 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion) 방법을 이용하여 연결되는 단계; 및 상기 전용 NPU가 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계를 포함할 수 있다. 이 때, 상기 신경망을 생성하는 단계는 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서비스 스레드에게 신경망 생성 요청을 송신하는 단계; 상기 전용 서비스 스레드가 상기 신경망 생성 요청에 기 반하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및 상기 전용 서비스 스레드가 상 기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 신경망 생성 완료 통지를 전달하는 단계를 더 포함할 수 있다. 이 때, 상기 신경망을 생성하는 단계는 상기 클라이언트 응용이 상기 신경망 생성 완료 통지에 기반하여 상기 신경망의 출력 결과를 수신하는 단계; 및 상기 클라이언트 응용이 상기 서비스의 종료 여부를 판단하는 단계를 더 포함할 수 있다. 이 때, 상기 서비스의 종료 여부를 판단하는 단계는 상기 클라이언트 응용이 신경망 생성이 더 필요한 경우, 상 기 전용 서비스 스레드에게 신경망 생성 요청을 송신하는 단계; 및 상기 클라이언트 응용이 신경망 생성이 더 이상 필요하지 않은 경우, 상기 전용 서비스 스레드에게 서비스 등록 해제 요청을 송신하는 단계를 포함할 수 있다. 이 때, 상기 서비스를 등록 해제하는 단계는 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서비스 스레드에게 서비스 등록 해제 요청을 송신하는 단계; 상기 전용 서비스 스레드가 생성한 상기 신경 망 및 전용 공유 메모리를 해제하는 단계; 상기 전용 서비스 스레드가 상기 비공개 메시지를 통하여 상기 클라 이언트 응용에게 해제 완료를 통보하는 단계; 및 상기 전용 서비스 스레드가 서비스를 종료하는 단계를 포함할 수 있다. 이 때, 상기 신경망을 생성하는 단계는 상기 클라이언트 응용이 상기 전용 공유 메모리의 입력 버퍼에 입력 이 미지를 입력하는 단계; 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통해 상기 전용 서비스 스레드에게 상기 입력 이미지에 대한 추론을 요청하는 단계; 상기 전용 서비스 스레드가 상기 입력 이미지로부터 신경망을 생성하는 단계; 상기 전용 서비스 스레드가 상기 전용 공유 메모리의 출력 버퍼에 상기 신경망의 추론 결과를 저장하는 단계; 및 상기 전용 서비스 스레드가 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 추론 완료를 통보하는 단계를 포함할 수 있다. 이 때, 상기 신경망을 생성하는 단계는 로더(Loader)에 의해 사전에 입력된 커널(Kernel)을 이용하는 것이고, 상기 커널은 가중치, 편차 및 배치 정규 표준화를 위한 스케일링 파라미터를 포함할 수 있다. 이 때, 상기 공개 메시지 채널은 상기 서버가 사전에 공개 Key를 상기 클라이언트 응용에게 공개하고, 상기 클 라이언트 응용이 상기 공개 Key를 통해 상기 서버에 접속하는 방식으로 생성되는 것일 수 있다. 또한, 실시예에 따른 인공지능 서비스를 위한 서버는 클라이언트 응용으로부터 서비스 등록 요청을 수신할 수 있는 공개 메시지 채널을 생성하고, 상기 클라이언트 응용의 서비스 등록 요청에 기반하여 상기 클라이언트 응 용의 전용 서비스 스레드를 생성하고, 상기 클라이언트 응용과 통신할 수 있는 비공개 메시지 채널을 생성하고, 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하고, 상기 클라이언트 응용의 상기 비공개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 클라이언트 응용의 전용 서비스 스레드를 해제하는 프로세서; 및 상기 전용 서비스 스레드가 생성한 신경망을 저장하는 공유 메모리를 포함한다. 이 때, 상기 프로세서는 상기 클라이언트 응용이 접속할 수 있는 상기 공개 메시지 채널을 생성하고, 상기 클라 이언트 응용이 상기 공개 메시지 채널을 통해 송신한 서비스 등록 요청을 수신하고, 상기 등록 요청에 기반하여 상기 클라이언트 응용을 위한 상기 전용 서비스 스레드를 생성하는 것이고, 상기 전용 서비스 스레드는 상기 클 라이언트 응용과 통신할 수 있는 상기 비공개 메시지 채널을 생성하고, 상기 공유 메모리 내에 전용 공유 메모 리를 생성하고, 상기 전용 공유 메모리의 주소를 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 전 달하는 것일 수 있다. 이 때, 상기 전용 서비스 스레드가 상기 클라이언트 응용으로부터 상기 비공개 메시지 채널을 통해 신경망 생성 요청을 수신하고, 복수개의 NPU(Neuromorphic Processing Unit)들 중에서 하나의 전용 NPU와 연결되고, 상기 전용 NPU는 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion) 방법을 이용하여 연결되고, 상기 적어도 하 나 이상의 가속기를 이용하여 신경망을 생성하는 것일 수 있다. 이 때, 상기 전용 서비스 스레드가 상기 클라이언트 응용의 상기 비공개 메시지를 통한 신경망 생성 요청을 수 신하고, 상기 신경망 생성 요청에 기반하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하고, 상 기 비공개 메시지를 통하여 상기 클라이언트 응용에게 신경망 생성 완료 통지 신호를 송신하는 것일 수 있다. 이 때, 상기 전용 서비스 스레드는 상기 클라이언트 응용의 상기 비공개 메시지를 통한 서비스 등록 해제 요청 을 수신하고, 생성한 상기 신경망 및 상기 전용 공유 메모리를 해제하고, 상기 비공개 메시지 채널을 통하여 상 기 클라이언트 응용에게 해제 완료를 통보하고, 상기 비공개 메시지 채널을 제거하는 것일 수 있다. 이 때, 상기 전용 서비스 스레드는 상기 클라이언트 응용의 상기 비공개 메시지 채널을 통한 추론 요청에 기반 하여, 상기 클라이언트 응용이 상기 전용 공유 메모리에 입력한 상기 입력 이미지로부터 신경망을 생성하고, 상 기 전용 공유 메모리의 출력 버퍼에 상기 신경망의 추론 결과를 저장하고, 상기 비공개 메시지 채널을 통해 상 기 클라이언트 응용에게 추론 완료를 통보하고, 상기 서비스를 종료하는 것일 수 있다. 이 때, 상기 전용 서비스 스레드는 상기 신경망을 생성하기 위하여 로더(Loader)에 의해 사전에 입력된 커널 (Kernel)을 이용하는 것이고, 상기 커널은 가중치, 편차 및 배치 정규 표준화를 위한 스케일링 파라미터를 포함 하는 것일 수 있다. 이 때, 상기 공개 메시지 채널은 상기 프로세서가 사전에 공개 Key를 상기 클라이언트 응용에게 공개하고, 상기 클라이언트 응용이 상기 공개 Key를 통해 상기 프로세서에 접속하는 방식으로 생성되는 것일 수 있다. 또한, 실시예에 따른 인공지능 서비스를 위한 클라이언트는 서버에게 공개 메시지 채널을 통해 서비스 등록 요 청을 송신하고, 상기 서버가 상기 서비스 등록 요청에 기반하여 생성한 전용 서비스 스레드에게 비공개 메시지 채널을 통해 신경망 생성 요청을 송신하고, 상기 전용 서비스 스레드가 공유 메모리에 생성한 신경망의 출력 결 과를 수신하는 프로세서; 및 상기 신경망의 출력 결과를 저장하는 공유 메모리를 포함하는 것일 수 있다. 이 때, 상기 프로세서는 신경망 생성이 더 필요한 경우, 상기 전용 서비스 스레드에게 신경망 생성 요청을 송신 하고, 신경망 생성이 더 이상 필요하지 않은 경우, 상기 전용 서비스 스레드에게 서비스 등록 해제 요청을 송신 하는 것일 수 있다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 복수개의 응용 프로그램이 복수개의 NPU와 가속기를 효율적으로 이용하여 신경망 처리를 할 수 있도 록, 인공지능 서비스를 위한 서버 클라이언트 통신 장치 및 방법을 제공할 수 있다. 또한, 본 발명은 복수의 인공지능 응용 프로그램이 실행될 때 서버 클라이언트 형태로 실행됨으로써, 인공지능 서비스를 위한 절차와 수행 역할의 구분을 명확하게 하고, 복잡하고 다양한 인공지능 서비스를 운영하는 시스템 을 쉽게 구성할 수 있는 방법을 제공할 수 있다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 것이며, 단지 본 실시예들은 본 발명의 개시가 완전하도록 하며, 본 발명이 속하는"}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 비록 \"제1\" 또는 \"제2\" 등이 다양한 구성요소를 서술하기 위해서 사용되나, 이러한 구성요소는 상기와 같은 용 어에 의해 제한되지 않는다. 상기와 같은 용어는 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사 용될 수 있다. 따라서, 이하에서 언급되는 제1 구성요소는 본 발명의 기술적 사상 내에서 제2 구성요소일 수도 있다. 본 명세서에서 사용된 용어는 실시예를 설명하기 위한 것이며 본 발명을 제한하고자 하는 것은 아니다. 본 명세 서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 명세서에서 사용되는 \"포함한다 (comprises)\" 또는 \"포함하는(comprising)\"은 언급된 구성요소 또는 단계가 하나 이상의 다른 구성요소 또는 단 계의 존재 또는 추가를 배제하지 않는다는 의미를 내포한다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어는 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 해석될 수 있다. 또한, 일반적으로 사용되는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 이하에서는, 도 1 내지 도 10을 참조하여 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법 및 장치가 상세히 설명된다. 도 1은 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 장치의 일 예를 나타낸 블록도이다. 상기 통신 장치는 인공지능 서비스를 위한 서버와 인공지능 서비스를 위한 클라이언트를 포함하는 것이다. 도 1을 참조하면, 인공지능을 위한 서버 클라이언트 통신 장치는 인공지능을서버와 복수개의 클라이언트 응용들을 포함한다. 도 1에서는 상기 클라이언트 응용들의 수를 설명의 편의를 위해 3개만 도시하였다. 인공지능 응용의 수가 증가할 경우에는 NPU와 상기 응용을 연결하는 중개자가 반드시 필요하다. 본 발명에서는 이 중개자 역할을 하는 개체가 서버(또는 서버 운영체제)이다. 상기 서버는 인공지능 가속기 디바이스인 NPU를 관리하면서 인공지능 클라이언트 응용 프로그램에게 인공지능 서비스를 제공하는 시스템이다. 그리고 상기 클라 이언트 응용은 인공지능 관련 서비스가 필요할 때 상기 서비스를 서버에 의뢰하는 응용 프로그램이다. 본 발명의 인공지능을 위한 서버 클라이언트 시스템은 클라이언트 응용과 서버가 쌍을 이루는 개념이다. 도 1을 참조하면, 서버는 CPU에서 실행 되면서 NPU를 제어할 수 있다. 그리고 상기 CPU는 상기 NPU와 네트워크로 연결되어 있고, 상기 NPU는 가속기와 연결되어 있어, 신경망 생성 과정에서 가속기를 이 용할 수 있다. 이 과정에서 복수의 클라이언트 응용들은 서버와 통신을 할 필요가 있는데 여기서 메시지 교환을 위한 통신 프로토콜이 필요하다. 본 발명에서 제안하는 바람직한 실시예에 의한 통신 프로토콜, 즉 인공지능 서 비스를 위한 서버 클라이언트 통신 방법을 이하에서 설명한다. 인공지능 서비스를 위한 서버 클라이언트 통신 방법은, 서버가 클라이언트 응용의 공개 메시지 채널 을 통한 서비스 등록 요청에 기반하여 상기 클라이언트 응용에 대한 전용 서비스 스레드를 생성하여 상기 서비 스를 등록하는 단계; 상기 전용 서비스 스레드가 공유 메모리에 적어도 하나 이상의 가속기를 이용하여 신경망 을 생성하는 단계; 및 상기 클라이언트 응용의 비공개 메시지 채널을 통한 서비스 등록 해제 요청에 기반하여 상기 전용 서비스 스레드를 해제하여 상기 서비스를 등록 해제하는 단계를 포함한다. 상기 서비스를 등록하는 단계는 서버가 클라이언트 응용이 접속할 수 있는 공개 메시지 채널을 생성 하는 것으로 시작된다. 상기 공개 메시지 채널은 상기 서버가 사전에 공개 Key를 상기 클라이언트 응용에게 공 개하고, 상기 클라이언트 응용이 상기 공개 Key를 통해 상기 서버에 접속하는 방식으로 생성되는 것이다. 그리고 상기 클라이언트 응용이 인공지능 서비스를 위하여 신경망을 생성 또는 처리할 필요가 생기면, 상 기 공개 메시지 채널을 통해 상기 서버에 서비스 등록 요청을 송신한다. 상기 서버가 상기 클라이언트 응용으로부터 상기 서비스 등록 요청을 수신하게 되면, 상기 클라이언 트 응용을 위한 전용 서비스 스레드를 생성한다. 이 때 상기 전용 서비스 스레드는 NPU 내에 생성될 수 있 다. 상기 전용 서비스 스레드는 상기 클라이언트 응용과 통신을 하기 위한 비공개 메시지 채널을 생성한다. 상기 비 공개 메시지 채널은 상기 공개 메시지 채널과는 달리 양방향으로 생성되어야 한다. 상기 비공개 메시지 채널을 통해 상기 전용 서비스 스레드와 상기 클라이언트 응용은 공유 메모리, 신경망과 같은 정보를 교환하고, 클라이 언트 응용의 요청이나 상기 요청의 결과에 대한 상태 등의 메시지의 교환이 이루어진다. 상기 전용 서비스 스레드는 상기 클라이언트 전용 공유 메모리를 생성한다. 인공지능 관련 서비스를 위해서는 상기 클라이언트 응용과 상기 전용 서비스 스레드간 대량의 AI 데이터를 공유하여야 하므로, 이 때 공유 메모리 를 사용하여 상기 AI 데이터를 공유한다. 여기서 공유 메모리란 반드시 물리적인 것을 말하는 것이 아니고, 가 상 공유 메모리일 수 있다. 일단 상기 클라이언트 전용 공유 메모리가 생성되면 상기 공유 메모리에 대한 정보 를 상기 클라이언트 응용에게 상기 비공개 메시지 채널을 통해 전달한다. 여기서 상기 공유 메모리에 대한 정보 는 상기 공유 메모리의 Key, 주소 등이 포함될 수 있다. 이상과 같이 서비스 등록 단계에서 비공개 메시지 채널과 전용 공유 메모리가 생성되면, 신경망을 생성하는 단 계로 넘어갈 수 있다. 상기 신경망을 생성하는 단계에서 전용 서비스 스레드는 상기 클라이언트 응용이 요청한 신경망을 생성하는 동 작을 수행한다. 이 때 신경망 객체를 공유 메모리 내에 생성하게 되면 상기 신경망에 대한 공유가 가능하다. 그 리고 상기 신경망이 입력 데이터와 출력 결과 데이터 버퍼에 대한 정보를 갖고 있으므로 상기 클라이언트 응용 과 서비스 스레드간 상기 신경망을 공유하는 것이 바람직하다. 그리고 상기 신경망은 이미 전용 공유 메모리의 주소를 비공개 메시지 채널을 통해서 공유하였기 때문에 전용 서비스 스레드가 상기 신경망을 생성 완료하여 상 기 클라이언트 응용에 알려주는 방식으로 공유될 수 있다. 그리고 전용 서비스 스레드가 상기 신경망을 생성할 때 필요한 커널(Kernel)은 로더(Loader)에 의해 이미 적재 되어 있을 수 있다. 여기서 커널은 가중치(Weights), 편차(Biases), 그리고 배치 정규 표준화를 위한 스케일링 파라미터(Scaling Parameter) 등을 포함한다. 상기 신경망을 생성하는 단계는 상기 전용 서비스 스레드가 복수개의 NPU들 중 하나의 전용 NPU와 연결되 는 단계; 상기 전용 NPU가 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion) 방법을 이용하여 연결되는 단계; 및 상기 전용 NPU가 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계를 포 함할 수 있다. 상기 전용 서비스 스레드는 신경망을 생성(실행)할 때 가속기를 이용한다. NPU의 가속기는 복수개가 될 수 있으 며 이는 전용 서비스 스레드를 위한 공유 자원으로 등록이 된다. 특정 서비스 스레드가 생성되고 등록될 때 서버에 의해서 서비스 스레드가 사용할 수 있는 NPU는 미리 결정되는 것이 바람직하며 가속기 자원은 실행시간 및 상황에 따라 동적으로 선택이 될 수 있다. 이때 가속기 자원은 MUTEX(Mutual Exclusion) 방법을 통해서 여러 서 비스 스레드간 상호배제가 되도록 보장해 주어야 한다. 또한, 상기 신경망을 생성하는 단계는 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서 비스 스레드에게 신경망 생성 요청을 송신하는 단계; 상기 전용 서비스 스레드가 상기 신경망 생성 요청에 기반 하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및 상기 전용 서비스 스레드가 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 신경망 생성 완료를 통지하는 단계를 더 포함할 수 있다. 즉 상기 전용 서비스 스레드는 상기 클라이언트 응용의 신경망 생성 요청에 기반하여 신경망을 생성하고, 상기 신경망 생성이 완료되면 상기 클라이언트 응용에 신경망 생성 완료 통지를 전달하는 것으로 신경망 생성 동작을 완료할 수 있다. 그리고 상기 클라이언트 응용이 상기 신경망 생성 완료 통지에 기반하여 상기 신경망의 출력 결과를 수신하게 되고, 상기 클라이언트 응용은 상기 서비스의 종료 여부를 판단하는 동작을 수행할 수 있다. 상기 서비스의 종 료 여부를 판단하는 동작은 상기 클라이언트 응용이 신경망 생성이 더 필요한 경우, 상기 전용 서비스 스레드에 게 신경망 생성 요청을 송신하고, 상기 클라이언트 응용이 신경망 생성이 더 이상 필요하지 않은 경우, 상기 전 용 서비스 스레드에게 서비스 등록 해제 요청을 송신할 수 있다. 결국 상기 전용 서비스 스레드가 서비스 등록 해제 요청을 수신하게 되면 상기 서비스를 등록 해제하는 단계를 진행하게 된다. 상기 서비스를 등록 해제하는 단계는 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서 비스 스레드에게 서비스 등록 해제 요청을 송신하는 단계; 상기 전용 서비스 스레드가 생성한 상기 신경망 및 전용 공유 메모리를 해제하는 단계; 상기 전용 서비스 스레드가 상기 비공개 메시지를 통하여 상기 클라이언트 응용에게 해제 완료를 통보하는 단계; 및 상기 전용 서비스 스레드가 서비스를 종료하는 단계를 포함할 수 있다. 즉, 상기 클라이언트 응용이 더 이상 인공지능 관련 서비스, 특히 신경망 생성 및 처리와 관련된 서비스가 필요 없다고 판단하는 경우, 상기 전용 서비스 스레드에 비공개 메시지 채널을 통하여 서비스 등록 해제 요청을 송신 한다. 상기 전용 서비스 스레드가 등록 해제 요청을 수신하게 되면 생성된 신경망, 전용 공유 메모리 자원을 해 제하고, 상기 클라이언트 응용과의 통신이 더 이상 필요하지 않으므로 비공개 메시지를 통하여 해제 완료를 통 보하고 상기 비공개 메시지 채널도 해제한다. 이와 같이 모든 자원이 해제된 후 상기 전용 서비스 스레드는 스 스로를 종료하여 서버 운영체제에 반납될 수 있다. 도 2는 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 시스템을 하드웨어와 소프트웨어로 나누어 나타 낸 도면이다. 도 2를 참조하면, 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 시스템은 다음과 같은 구성을 갖는다. 하드웨어 구성은 클라이언트 CPU, 호스트 CPU, 다중 NPU 를 포함할 수 있다. 소 프트웨어 구성은 HW위에서 실행되는 AI(Artificial Intelligence) 서버와 AI 클라이언트 응용 프로 그램을 포함할 수 있다. 또한 NPU 내부에는 1개 이상의 가속기가 존재하며, 또 특수한 경우에는 NPU 내부 에 CPU 코어인 N-CPU 가 존재할 수 있다. 우선 소프트웨어의 측면에서 살펴보면, 클라이언트 응용은 다양한 형태로 작성될 수 있다. C, Python, Java 등 여러 프로그래밍 언어를 기반으로 작성될 수 있으며 Pytorch, Caffe, Tensorflow 등 딥러닝 프레임워크 와 라이브러리를 이용하여 작성되기도 하며 SparkML, Python ML 패키지 등 머신러닝 라이브러리를 사용하기도 한다. 본 발명에서는 응용 프로그램을 뉴럴 네트워크(Neural Network, 신경망)를 실행하는 절차를 포함하는 인공지능 응용 프로그램에 국한한다. 여기서 상기 인공지능 응용 프로그램은 이미지, 영상 및 음성 처리를 위한 딥러닝 (Deep-Learning) 기술을 필요로 하는 프로그램을 말한다. 클라이언트 응용은 인공지능 관련 서비스가 필요할 때 서버 운영체제에 의뢰를 하게 된다. 서버 운영체제 는 딥러닝 추론과 같은 인공지능 서비스를 실행하고 그 결과를 클라이언트 응용 프로그램에게 반환할 수 있다. 상기 클라이언트 응용은 클라이언트 CPU에서 실행이 가능하며, 호스트 CPU와 클라이언트 CPU의 구분없이 호스트 CPU에서 클라이언트 응용이 실행될 수도 있다. 그리고 하드웨어의 측면에서 살펴보면, 클라이언트 CPU는 상기 클라이언트 응용을 실행하는 CPU이다. 상기 클라이언트 CPU는 도 2에 표현된 바와 같이 호스트 CPU와 별도로 존재할 수도 있고, 호스트 CPU 내에 존재할 수도 있다. 호스트 CPU와 클라이언트 CPU가 동일한 CPU인 경우에는 HW/SW 스레드나 프로세스 로 그 역할이 구분될 수 있다. 호스트 CPU는 서버가 실행되는 CPU이다. 서버는 호스트 CPU에 단독으로 실행될 수도 있고, 호스트 CPU와 N-CPU에 분산되어 실행될 수도 있다. 후자와 같이 분산되어 실행되는 경우에는 계층적인 구조 로서 호스트 CPU에서 실행되는 서버를 L1 운영체제, N-CPU에서 실행되는 서버를 L2 운영체제라 명명할 수 있다. 그리고 L1 운영체제는 복수개의 NPU 자원을 관리하며 의뢰 받은 신경망을 N-CPU에 존재하는 L2 운영체제에 전달 하는 역할을 하고, L2 운영체제에서 실행된 신경망 실행 결과를 다시 L1에 반환하면 L1 운영체제는 클라이언트 응용에게 상기 신경망 실행 결과를 반환하게 된다. NPU는 인공지능 응용 프로그램의 구성 요소 중 신경망들을 실행하는 주체이자, 신경망들의 실행을 책임지 는 유닛이다. 그리고 상기 NPU는 경우에 따라 복수개일 수 있고, 신경망을 처리할 때 가속기뿐만 아니라 내부 CPU인 N-CPU를 이용하여 처리할 수 있다. N-CPU는 NPU 내에 위치하는 CPU로 가속기를 운용하고 통제하는 SW를 실행한다. 신경망이 입력으로 들어오 면 가속기의 현재 상태 정보를 파악하여 입력 신경망들을 가속기에 어떻게 실행할 것인지를 결정한다. 그리고 N-CPU는 경우에 따라 NPU내에 존재하지 않을 수 있고, 이런 경우 호스트 CPU가 해당 역할을 수행한다. 가속기(Neural Processing Accelerator)는 인공지능을 위한 가속기로, 일반적으로 NXN의 시스톨릭 배열이기도 하며 GPU 아키텍처(Architecture)와 같이 대량의 하드웨어 스레드로 구성될 수도 있다 도 3은 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법에있어서, 인공지능 서비스 등록 및 신경망 생성 절차를 나타낸 도면이다. 도 3을 참조하면, 인공지능 서비스 등록 절차는 우선 서버가 공개 메시지 채널을 생성한다. 즉, 상기 서버 는 클라이언트 응용이 접속할 수 있는 공개 채널을 생성하는데, 상기 클라이언트 응용이 상기 공개 채널에 접속할 수 있도록 Key를 공개하고, 공개한 Key를 미리 상기 클라이언트 응용들에게 미리 제공할 수 있다. 그리고 상기 클라이언트 응용이 추론 등의 인공지능 서비스를 이용하기 위하여 상기 공개 메시지 채널을 통해 상기 서버에 전용 서비스 등록을 요청하게 된다. 그리고 상기 서버는 상기 서비스 등록 요청에 기반하여, 해당 클라이언트 응용을 위한 전용 서비스 스레드 를 생성한다. 이 전용 서비스 스레드는 클라이언트 응용을 위한 에이전트(비서) 역할을 하게 된다. 그리고 상기 전용 서비스 스레드는 상기 클라이언트 응용과 통신할 수 있는 비공개 메시지 채널을 생 성한다. 상기 채널은 서버가 생성한 공개 메시지 채널과 달리 반드시 양방향이어야 한다. 상기 전용 서비스 스 레드와 상기 클라이언트 응용은 상기 비공개 메시지 채널을 통해서 클라이언트 응용과 서비스 스레드 간에 공유 메모리(또는 공유 메모리의 주소), 신경망과 같은 정보를 교환하고, 클라이언트 응용의 요청이나 요 청 결과에 대한 상태 등의 메시지 교환도 이루어진다. 상기 전용 서비스 스레드는 상기 클라이언트 응용 전용의 공유 메모리를 생성한다. 인공지능 관련 서비스 를 위해서는 클라이언트 응용과 서비스 스레드간 대량의 AI 데이터를 공유하여야 하므로 공유메모리가 필요하다. 여기서 공유메모리란 가상 공유 메모리를 의미하며, 일단 공유모리가 생성이 되면 공유 메모리에 대 한 정보인 공유 메모리의 Key 와 생성 주소 등을 상기 클라이언트 응용에게 상기 비공개 메시지 채널을 통하여 전달할 수 있다. 그리고 상기 서비스 스레드는 신경망(뉴럴넷)을 생성한다. 이 때 신경망 객체를 공유 메모리 내에 생성하게 되 면 상기 신경망에 대한 공유가 가능하다. 그리고 상기 신경망이 입력 데이터와 출력 결과 데이터 버퍼에 대한 정보를 갖고 있으므로 상기 클라이언트 응용과 서비스 스레드간 상기 신경망을 공유하는 것이 바람직하다. 그리 고 상기 신경망은 이미 전용 공유 메모리의 주소를 비공개 메시지 채널을 통해서 공유하였기 때문에 전용 서비 스 스레드가 상기 신경망을 생성 완료하여 상기 클라이언트 응용에 알려주는 방식으로 공유될 수 있다. 상기 신경망을 생성하는 단계는 상기 전용 서비스 스레드가 복수개의 NPU들 중 하나의 전용 NPU와 연결되 는 단계; 상기 전용 NPU가 적어도 하나 이상의 가속기와 MUTEX(Mutual Exclusion) 방법을 이용하여 연결되는 단 계; 및 상기 전용 NPU가 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계를 포함할 수 있다. 또한, 상기 신경망을 생성하는 단계는 상기 클라이언트 응용이 상기 비공개 메시지 채널을 통하여 상기 전용 서 비스 스레드에게 신경망 생성 요청을 송신하는 단계; 상기 전용 서비스 스레드가 상기 신경망 생성 요청에 기반 하여 상기 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하는 단계; 및 상기 전용 서비스 스레드가 상기 비공개 메시지 채널을 통해 상기 클라이언트 응용에게 신경망 생성 완료 통지를 전달하는 단계를 더 포함할 수 있다. 그리고 상기 클라이언트 응용이 상기 신경망 생성 완료 통지에 기반하여 상기 신경망의 출력 결과를 수신하게 되고, 상기 클라이언트 응용은 상기 서비스의 종료 여부를 판단하는 동작을 수행할 수 있다. 상기 서비스의 종 료 여부를 판단하는 동작은 상기 클라이언트 응용이 신경망 생성이 더 필요한 경우, 상기 전용 서비스 스레드에 게 신경망 생성 요청을 송신하고, 상기 클라이언트 응용이 신경망 생성이 더 이상 필요하지 않은 경우, 상기 전 용 서비스 스레드에게 서비스 등록 해제 요청을 송신할 수 있다. 이후, 상기 전용 서비스 스레드가 서비스 등록 해제 요청을 수신하게 되면 상기 서비스를 등록 해제하는 단계를 진행하게 된다. 도 4는 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법에있어서, 신경망을 생성하는 단계에 서 AI 추론 서비스를 실행하는 예를 나타낸 도면이다. 서버가 생성한 전용 서비스 스레드가 서비스 등록 절차를 완료하면, 상기 클라이언트 응용은 상기 전 용 서비스 스레드을 통해서 신경망을 생성하는 단계를 수행할 수 있다. 이하에서 상기 신경망을 생성하는 단계를 AI 추론 서비스를 실행하는 예를 들어 설명한다. 우선 클라이언트 응 용이 입력이 되는 이미지을 상기 공유 메모리의 입력 버퍼에 복사한다. 이 때, 상기 클라 이언트 응용은 상기 공유 메모리의 입력 버퍼의 주소를 이미 비공개 메시지 채널을 통해 상기 클라이언트 응용이 전달 받아 알고 있다. 그리고 상기 클라이언트 응용은 비공개 메시지 채널을 통해 상기 전용 서비스 스레드에 추론 요청 (Predict)을 하게 되고, 상기 전용 서비스 스레드는 상기 입력 버퍼에 저장된 입력 이미지을 이용하 여 신경망을 실행 및 추론하고(Inference), 상기 추론의 결과를 상기 공유 메모리의 출력 버퍼에 저장한다. 그리고 상기 전용 서비스 스레드는 비공개 메시지 채널응 통하여 상기 클라이언트 응용에 추론 완료 를 통보(Done)한다. 도 4에서는 상기 추론 서비스 실행을 통해서 입력 이미지에서 개, 자전거, 자동차의 3개의 객체가 사각형 으로 검출된 예를 보여주고 있다. 도 5는 서버 운영체제의 호스트 CPU와 N-CPU의 역할 분산 개념을 나타낸 도면이다. 도 5를 참조하여, 호스트 CPU, N-CPU에 서버 운영체제의 역할이 분산되어 있는 경우를 설명한다. 만약 인공지능을 위한 서버 클라이언트 시스템의 서버 운영체제의 역할이 분산되어 있지 않다면, 전용 서비스 스레드는 호스트 CPU에서 생성되고 수행된다. 그러나, 도 5와 같이 호스트 CPU, N-CPU에 서버 운영체제의 역할이 분산되어 있는 경우에는 호 스트 CPU에서 실행되는 L1 운영체제는 단순히 클라이언트 응용의 응용 프로세스의 요청을 NPU에 분배하거나 NPU로부터 오는 응답을 클라이언트 응용에 전달하는 등 단순히 중계 프로세스로써 역할하게 된 다. 실제 전용 서비스 스레드를 운영하는 역할은 N-CPU에서 실행되는 L2 운영체제가 담당하여, N- CPU에서 서비스 스레드가 생성되고 실행이 된다. 그리고 일단 중계 프로세스를 통해서 전용 서비스 스레드가 생성이 되고 서비스가 등록이 되면 인공 지능 관련 서비스의 실행은 중계 프로세스 개입 없이 응용 프로세스와 전용 서비스 스레드 간에 직접통신을 통하여 이루어 질 수 있다. 도 6은 전용 서비스 스레드와 가속기의 관계를 나타낸 도면이다. 도 6을 참조하면, NPU는 전용 서비스 스레드와 가속기를 포함할 수 있다. 그리고 상기 전용 서비스 스레드 는 신경망을 생성 또는 실행할 때 가속기를 이용한다. NPU의 가속기는 복수개가 될 수 있으며 서버 운영체제에서는 서비스 스레드를 위한 공유 자원으로 등록이 될 수 있다. 그리고 특정 서비스 스레드가 N- CPU에 생성되고 등록될 때 서버 운영체제에 의해서 서비스 스레드가 사용할 수 있는 NPU는 미리 결정 되는 것이 바람직하며, 상기 가속기 자원은 상황에 따라 동적으로 선택될 수 있는데, 이때 가속기 자원은 MUTEX(Mutual Exclusion) 방법을 통해서 여러 서비스 스레드간 상호배제가 되도록 보장해 주는 것이 바람직하다. 도 7은 실시예에 따른 인공지능 서비스를 위한 클라이언트의 동작 흐름도를 나타낸 도면이다. 도 7을 참조하면, 우선 상기 클라이언트, 즉 클라이언트 응용은 공개 메시지 채널을 통해 서버 운영체제에 전용 서비스 요청, 즉 서비스 등록 요청을 송신한다(S710). 이 후 서버 운영체제는 전용 서비스 스레드를 생성 완료 하였는지를 판단한다(S720). 만약 상기 전용 서비스 스 레드가 생성 완료되었다고 판단되면 상기 전용 서비스 스레드가 생성한 공유 메모리 및 뉴럴넷(신경망) 정보를 비공개 메시지 채널을 통해 수신한다(S730). 이로써 상기 클라이언트 응용이 인공지능 서비스를 받을 준비가 완 료되는 것이다. 그리고 상기 클라이언트 응용가 AI 서비스를 요청할 경우에는 AI용 입력 데이터를 상기 공유 메모리에 복사하고 (S740), 상기 비공개 메시지 채널을 통해 AI 서비스 실행을 요청한다(S750). 상기 서비스 실행 요청을 받은 전용 서비스 스레드는 적어도 하나 이상의 가속기를 이용하여 신경망을 생성하게 되고, 상기 신경망을 생성 완료하면 상기 클라이언트 응용에 생성 완료 통지를 하게 된다. 상기 클라이언트 응용이 비공개 메시지 채널을 통해 실행 완료를 수신하게 되면, 해당 서비스를 계속 이용할 것 인지, 종료할 것인지를 판단한다(S770). 만약 해당 서비스를 계속 이용할 필요가 있다면 AI 용 입력 데이터를 공유 메모리에 복사하는 단계(S740)로 돌아가서 신경망 실행 절차를 다시 진행한다. 만약 상기 클라이언트 응용이 신경망 생성 절차가 더 이상 필요하지 않다면, 즉, 전용 서비스 스레드의 종료가 필요하다고 판단되면, 상기 전용 서비스 스레드에 서비스 등록 해제 요청을 송신한다(S780). 도 8은 실시예에 따른 인공지능 서비스를 위한 서버의 동작 흐름도를 나타낸 도면이다. 도 8을 참조하면, 서버는 클라이언트 응용이 접속할 수 있도록 공개 메시지 채널을 생성한다(S810). 그리고 클 라이언트 응용이 전용 서비스 요청을 할 때까지 대기한다(S820). 만일 상기 클라이언트 응용으로부터 서비스 등록 요청을 수신하게 되면, 상기 클라이언트 응용을 위한 전용 서 비스 스레드를 생성한다(S830). 이 때 상기 서버는 상기 전용 서비스 스레드를 호스트 CPU에서 생성할 것인지 N-CPU에서 생성할 것인지를 결정하고, 해당 전용 서비스 스레드를 결정된 CPU에 할당(생성)하게 된다. 그리고 이상과 같이 상기 전용 서비스 스레드가 생성되면, 서버는 인공지능서비스를 위한 구체적인 작업을 전부 상기 전용 서비스 스레드에 맡기고, 다른 클라이언트 응용의 요청을 기다리며 대기 모드(S820)로 들어가게 된다. 도 9는 실시예에 따른 인공지능 서비스를 위한 서버가 생성한 전용 서비스 스레드의 동작 흐름도를 나타낸 도면 이다. 도 9를 참조하면, 서버에 의해서 생성된 전용 서비스 스레드는 생성된 직후, 클라이언트 응용과 통신하기 위한 비공개 메시지 채널을 생성한다(S910). 그리고 상기 클라이언트 응용 전용 공유 메모리를 생성한다(S920). 그리 고 상기 전용 공유 메모리에 뉴럴넷(신경망)을 생성한다(S930). 이후 상기 전용 서비스 스레드는 클라이언트 응용의 AI 서비스 요청 메시지(신호)를 받기 위해서 대기한다 (S940). 만약 상기 전용 서비스 스레드가 상기 메시지를 수신하게 되면, 상기 메시지가 추론(Predict)인지 아니 면 종료(Close)인지에 따라서 분기가 달라진다. 상기 전용 서비스 스레드는 상기 메시지 내용이 추론인지 여부를 판단하고(S950), 만약 추론이라고 판단되면, 추론을 실행한다(S960). 즉, 입력 이미지에 대한 추론을 실행하고 추론 결과 데이터를 공유 메모리에 저장한다. 그리고 추론 완료 메시지를 상기 클라이언트 응용에게 전송한다(S970). 만약 상기 메시지 내용이 추론이 아닐 경우, 상기 메시지 내용이 종료인지 판단하고(S980), 만약 종료라고 판단 되면, 생성된 신경망, 공유 메모리, 비공개 메시지 채널을 모두 해제하고 스스로 종료하여 스레드 자원을 서버 운영체제에 반납하게 된다(S990). 도 10은 실시예에 따른 컴퓨터 시스템 구성을 나타낸 도면이다. 실시예에 따른 실시예에 따른 인공지능 서비스를 위한 서버 및 인공지능 서비스를 위한 클라이언트는 모두 컴퓨 터로 읽을 수 있는 기록매체와 같은 컴퓨터 시스템에서 구현될 수 있다. 컴퓨터 시스템은 버스를 통하여 서로 통신하는 하나 이상의 프로세서, 메모리, 사용자 인터페이스 입력 장치, 사용자 인터페이스 출력 장치 및 스토리지를 포함할 수 있다. 또한, 컴퓨터 시스템은 네트워크에 연결되는 네트워크 인터페이스를 더 포함할 수 있다. 프로세서 는 중앙 처리 장치 또는 메모리나 스토리지에 저장된 프로그램 또는 프로세싱 인스트럭션들 을 실행하는 반도체 장치일 수 있다. 메모리 및 스토리지는 휘발성 매체, 비휘발성 매체, 분리형 매체, 비분리형 매체, 통신 매체, 또는 정보 전달 매체 중에서 적어도 하나 이상을 포함하는 저장 매체일 수 있 다. 예를 들어, 메모리는 ROM이나 RAM을 포함할 수 있다. 이상에서 설명된 실시예에 따르면, 복수개의 응용 프로그램이 복수개의 NPU와 가속기를 효율적으로 이용하여 신 경망 처리를 할 수 있도록, 인공지능 서비스를 위한 서버 클라이언트 통신 장치 및 방법을 제공할 수 있다. 또한, 본 발명은 복수의 인공지능 응용 프로그램이 실행될 때 서버 클라이언트 형태로 실행됨으로써, 인공지능 서비스를 위한 절차와 수행 역할의 구분을 명확하게 하고, 복잡하고 다양한 인공지능 서비스를 운영하는 시스템 을 쉽게 구성할 수 있는 방법을 제공할 수 있다."}
{"patent_id": "10-2020-0043240", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이상에서 첨부된 도면을 참조하여 본 발명의 실시예들을 설명하였지만, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자는 본 발명이 그 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 실시 될 수 있다는 것을 이해할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다."}
{"patent_id": "10-2020-0043240", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 장치의 일 예를 나타낸 블록도이다. 도 2는 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 시스템을 하드웨어와 소프트웨어로 나누어 나타 낸 도면이다. 도 3은 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법에있어서, 인공지능 서비스 등록 및 신경망 생성 절차를 나타낸 도면이다. 도 4는 실시예에 따른 인공지능 서비스를 위한 서버 클라이언트 통신 방법에 있어서, 신경망을 생성하는 단계에 서 AI 추론 서비스를 실행하는 예를 나타낸 도면이다. 도 5는 서버 운영체제의 호스트 CPU와 N-CPU의 역할 분산 개념을 나타낸 도면이다. 도 6은 전용 서비스 스레드와 가속기의 관계를 나타낸 도면이다. 도 7은 실시예에 따른 인공지능 서비스를 위한 클라이언트의 동작 흐름도를 나타낸 도면이다. 도 8은 실시예에 따른 인공지능 서비스를 위한 서버의 동작 흐름도를 나타낸 도면이다. 도 9는 실시예에 따른 인공지능 서비스를 위한 서버가 생성한 전용 서비스 스레드의 동작 흐름도를 나타낸 도면 이다. 도 10은 실시예에 따른 컴퓨터 시스템 구성을 나타낸 도면이다."}
