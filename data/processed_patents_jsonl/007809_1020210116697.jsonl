{"patent_id": "10-2021-0116697", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0033820", "출원번호": "10-2021-0116697", "발명의 명칭": "이미지 프레임 내 얼굴의 위치 및 비율을 기반으로 이미지 프레임을 변환하는 전자 장치", "출원인": "신대근", "발명자": "신대근"}}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,비디오 편집을 위한 적어도 하나의 인공지능 모델이 저장된 메모리; 및상기 메모리와 연결된 프로세서;를 포함하고,상기 프로세서는,비디오를 구성하는 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에서 얼굴을 포함하는 영역을 식별하고,상기 이미지 프레임 내 상기 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 상기 복수의 이미지 프레임을변환하고,상기 변환된 복수의 이미지 프레임을 상기 적어도 하나의 인공지능 모델에 입력하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 이미지 프레임의 가로축에 해당하는 x축 및 세로축에 해당하는 y축을 기준으로, 상기 얼굴을 포함하는 영역의 2차원적 위치 정보를 식별하고,상기 이미지 프레임 내 상기 얼굴을 포함하는 영역의 비율을 기반으로 비율 정보를 식별하고,상기 인공지능 모델에 매칭되는 위치 정보에 대한 상기 식별된 위치 정보의 차이 및 상기 인공지능 모델에 매칭되는 비율 정보에 대한 상기 식별된 비율 정보의 차이를 기반으로, 변환 정보를 생성하고,상기 생성된 변환 정보를 이용하여 상기 복수의 이미지 프레임을 변환하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 인공지능 모델에 매칭되는 위치 정보는,상기 인공지능 모델의 훈련에 이용된 적어도 하나의 이미지 내 얼굴의 위치에 따라 설정된 것이고,상기 인공지능 모델에 매칭되는 비율 정보는,상기 인공지능 모델의 훈련에 이용된 적어도 하나의 이미지 내 얼굴의 비율에 따라 설정된 것인, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는,상기 복수의 이미지 프레임 각각에 포함된 얼굴의 위치 및 식별 정보 중 적어도 하나에 따라, 상기 복수의 이미지 프레임을 제1 복수의 이미지 프레임 및 제2 복수의 이미지 프레임으로 구분하고,상기 제1 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 상기 제1 복수의 이미지 프레임을 변환하고,상기 제2 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내 얼굴을 포함하는 영역의 위치 및 비율을 기공개특허 10-2023-0033820-3-반으로, 상기 제2 복수의 이미지 프레임을 변환하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 비디오 편집을 위한 적어도 하나의 인공지능 모델은,이미지 프레임 내에서 입 모양을 식별하는 제1 인공지능 모델; 및상기 제1 인공지능 모델을 통해 식별된 입 모양에 대한 정보 및 상기 비디오의 오디오 데이터를 기반으로, 상기복수의 이미지 프레임 중 상기 비디오의 등장 인물의 음성에 매칭되는 적어도 하나의 이미지 프레임을선택하는, 제2 인공지능 모델;을 포함하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 제2 인공지능 모델은,상기 식별된 입 모양에 대한 정보 및 상기 오디오 데이터의 시간 별 진폭에 따라, 상기 복수의 이미지 프레임중 상기 비디오 내 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제2 인공지능 모델은,상기 복수의 이미지 프레임 각각에 대하여, 이전 이미지 프레임과의 입 모양 차이에 따른 제1 중요도를 획득하는, 제1 모듈;상기 복수의 이미지 프레임 각각에 매칭되는 오디오 데이터의 시간 구간 별로, 진폭에 따른 제2 중요도를 획득하는, 제2 모듈; 및상기 제1 중요도 및 상기 제2 중요도에 따라, 상기 복수의 이미지 프레임 중 상기 비디오 내 음성에 매칭되는적어도 하나의 이미지 프레임을 선택하는, 제3 모듈;을 포함하는, 전자 장치."}
{"patent_id": "10-2021-0116697", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "비디오 편집을 위한 적어도 하나의 인공지능 모델을 포함하는 전자 장치의 제어 방법에 있어서,복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에서 얼굴을 포함하는 영역을 식별하는 단계;상기 이미지 프레임 내 상기 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 상기 복수의 이미지 프레임을변환하는 단계; 및상기 변환된 복수의 이미지 프레임을 상기 적어도 하나의 인공지능 모델에 입력하는 단계;를 포함하는, 전자 장치의 제어 방법."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 전자 장치는, 비디오 편집을 위한 적어도 하나의 인공지능 모델이 저장된 메모리, 메 모리와 연결된 프로세서를 포함한다. 프로세서는, 비디오를 구성하는 복수의 이미지 프레임 중 적어도 하나의 이 미지 프레임 내에서 얼굴을 포함하는 영역을 식별하고, 이미지 프레임 내 얼굴을 포함하는 영역의 위치 및 비율 을 기반으로, 복수의 이미지 프레임을 변환하고, 변환된 복수의 이미지 프레임을 적어도 하나의 인공지능 모델에 입력한다."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 이미지 프레임을 변환하는 전자 장치에 관한 것으로, 보다 상세하게는, AI(Artificial Intelligence) 기반 컷 편집 작업에 적합해지도록 이미지 프레임을 변환하는 전자 장치에 관한 것이다."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "동영상 직접 제작 내지는 개인 방송이 성행함에 따라, 개개인의 동영상 편집을 지원하는 AI(Artificial Intelligence) 기반 솔루션들이 다양하게 이용되고 있다. AI 모델의 경우 일반적으로 특정한 크기 및 해상도에 매칭되거나 또는 특정한 객체를 포함하는 이미지들에 대하 여 전문적으로 훈련됨으로써 그 정확도 및 효율성이 담보될 수 있다. 다만, 일반적으로 개개인이 촬영한 (편집 전의) 비디오(이미지 프레임들, 오디오 데이터)는 제한없이 다양한 콘 텐츠를 담을 수 있음은 물론 다양한 촬영 기기를 통해 획득될 수 있고, 촬영 파라미터 값도 폭넓게 설정될 수 있다. 즉, 개개인이 촬영한 비디오는 솔루션(ex. 편집용 애플리케이션, 프로그램 등) 내에서 편집을 지원하는 AI 모델 과 fit하게 맞지 않는 경우가 많다. 선행기술문헌 특허문헌 (특허문헌 0001) 공개특허공보 제10-20190117416호(동영상 프레임 해상도를 향상시키기 위한 방법 및 장치)"}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 편집을 위한 인공지능 모델에 최적화되도록 각 이미지 프레임을 변환하는 전자 장치의 제어 방법을 제공한다. 구체적으로, 본 개시는 비디오 내 등장 인물의 발화 장면을 추출하기 위한 하나 이상의 인공지능 모델에 대하여 최적화된 전자 장치의 제어 방법을 제공한다. 본 개시는 비디오 내 등장 인물의 음성은 물론 등장 인물의 입 모양까지 활용하여, 음성을 포함하는 비디오 내 시간 구간을 선택할 수 있는 전자 장치의 제어 방법을 제공한다. 본 개시의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 개시의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 개시의 실시 예에 의해 보다 분명하게 이해될 것이다. 또한, 본 개시의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 전자 장치는, 비디오 편집을 위한 적어도 하나의 인공지능 모델이 저장된 메모리, 상기 메모리와 연결된 프로세서를 포함한다. 상기 프로세서는, 비디오를 구성하는 복수의 이미지 프레임 중 적 어도 하나의 이미지 프레임 내에서 얼굴을 포함하는 영역을 식별하고, 상기 이미지 프레임 내 상기 얼굴을 포함 하는 영역의 위치 및 비율을 기반으로, 상기 복수의 이미지 프레임을 변환하고, 상기 변환된 복수의 이미지 프 레임을 상기 적어도 하나의 인공지능 모델에 입력한다. 상기 프로세서는, 상기 이미지 프레임의 가로축에 해당하는 x축 및 세로축에 해당하는 y축을 기준으로, 상기 얼 굴을 포함하는 영역의 2차원적 위치 정보를 식별하고, 상기 이미지 프레임 내 상기 얼굴을 포함하는 영역의 비 율을 기반으로 비율 정보를 식별하고, 상기 인공지능 모델에 매칭되는 위치 정보에 대한 상기 식별된 위치 정보 의 차이 및 상기 인공지능 모델에 매칭되는 비율 정보에 대한 상기 식별된 비율 정보의 차이를 기반으로, 변환 정보를 생성하고, 상기 생성된 변환 정보를 이용하여 상기 복수의 이미지 프레임을 변환할 수 있다. 여기서, 상기 인공지능 모델에 매칭되는 위치 정보는, 상기 인공지능 모델의 훈련에 이용된 적어도 하나의 이미 지 내 얼굴의 위치에 따라 설정된 것이고, 상기 인공지능 모델에 매칭되는 비율 정보는, 상기 인공지능 모델의 훈련에 이용된 적어도 하나의 이미지 내 얼굴의 비율에 따라 설정된 것일 수 있다. 또한, 상기 프로세서는, 상기 복수의 이미지 프레임 각각에 포함된 얼굴의 위치 및 식별 정보 중 적어도 하나에 따라, 상기 복수의 이미지 프레임을 제1 복수의 이미지 프레임 및 제2 복수의 이미지 프레임으로 구분하고, 상 기 제1 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내 얼굴을 포함하는 영역의 위치 및 비율을 기반 으로, 상기 제1 복수의 이미지 프레임을 변환하고, 상기 제2 복수의 이미지 프레임 중 적어도 하나의 이미지 프 레임 내 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 상기 제2 복수의 이미지 프레임을 변환할 수도 있다.한편, 상기 비디오 편집을 위한 적어도 하나의 인공지능 모델은, 이미지 프레임 내에서 입 모양을 식별하는 제1 인공지능 모델, 상기 제1 인공지능 모델을 통해 식별된 입 모양에 대한 정보 및 상기 비디오의 오디오 데이터를 기반으로, 상기 복수의 이미지 프레임 중 상기 비디오의 등장 인물의 음성에 매칭되는 적어도 하나의 이미지 프 레임을 선택하는, 제2 인공지능 모델을 포함할 수 있다. 이 경우, 상기 제2 인공지능 모델은, 상기 식별된 입 모양에 대한 정보 및 상기 오디오 데이터의 시간 별 진폭 에 따라, 상기 복수의 이미지 프레임 중 상기 비디오 내 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택 할 수 있다. 상기 제2 인공지능 모델은, 상기 복수의 이미지 프레임 각각에 대하여, 이전 이미지 프레임과의 입 모양 차이에 따른 제1 중요도를 획득하는, 제1 모듈, 상기 복수의 이미지 프레임 각각에 매칭되는 오디오 데이터의 시간 구 간 별로, 진폭에 따른 제2 중요도를 획득하는, 제2 모듈, 상기 제1 중요도 및 상기 제2 중요도에 따라, 상기 복 수의 이미지 프레임 중 상기 비디오 내 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택하는, 제3 모듈을 포함할 수 있다. 본 개시의 일 실시 예에 따라 비디오 편집을 위한 적어도 하나의 인공지능 모델을 포함하는 전자 장치의 제어 방법은, 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에서 얼굴을 포함하는 영역을 식별하는 단계, 상기 이미지 프레임 내 상기 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 상기 복수의 이미지 프레임을 변환하는 단계, 상기 변환된 복수의 이미지 프레임을 상기 적어도 하나의 인공지능 모델에 입력하는 단계를 포 함한다."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따른 전자 장치의 제어 방법은, 비디오의 편집에 이용되는 AI 모델에 최적화된 이미지를 제공한다는 효과가 있다. 구체적으로, 본 개시에 따른 전자 장치의 제어 방법은, 얼굴의 위치 및 비율에 따른 이미지를 전처리를 제공함 으로써, 등장 인물의 음성에 매칭되는 장면을 추출하는 AI 모델이 입 모양을 통해 최적의 이미지 프레임들을 선 택할 수 있도록 한다. 본 개시에 따른 전자 장치의 제어 방법은, 비디오 내에서 등장 인물의 발화 장면에 매칭되는 시간 구간만을 선 택적으로 추출할 수 있다. 구체적으로, 본 개시에 따른 전자 장치의 제어 방법은, 오디오 데이터 내 음성 분포 뿐 아니라 장면(이미지 프 레임들) 내 입 모양의 변화 모습까지 반영된 최적의 발화 장면을 선택할 수 있다."}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기 술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소를 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 도 1은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 1을 참조하면, 전자 장치는 메모리 및 프로세서를 포함한다. 전자 장치는 스마트폰, 태블릿 PC, 데스크탑 PC, 노트북 PC, PDA, 디지털 카메라 등 다양한 전자 기기에 해당할 수 있다. 전자 장치는 적어도 하나의 애플리케이션 또는 프로그램을 통해 비디오 내 이미지 프레임 들을 변환하고, 변환된 이미지 프레임들을 활용하여 비디오를 편집할 수 있다. 또한, 전자 장치는 서버로 구현될 수도 있다. 이 경우, 전자 장치는 다양한 단말 장치로부터 수신된 비디오 내 이미지 프레임들을 변환하고, 변환된 이미지 프레임들을 활용하여 비디오를 편집하고, 편집된 비디오 를 단말 장치로 제공할 수 있다. 구체적으로, 전자 장치는 비디오 편집 기능을 제공하는 적어도 하나의 웹 페이지 또는 애플리케이션을 통해 이미지 변환 기능 및/또는 비디오 편집 기능을 제공할 수 있다. 메모리는 전자 장치 내 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System), 적어도 하나의 인스트럭션 및 데이터를 저장하기 위한 구성이다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘발성 메모 리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid state drive) 등을 포함할 수도 있다. 메모리는 하나 이상의 비디오를 저장할 수 있다. 여기서, 비디오는, 다양한 장르의 영상물에 해당할 수 있 으며, 복수의 이미지 프레임 및 오디오 데이터를 포함할 수 있다. 비디오는, 전자 장치의 카메라 및 마이 크를 통해 입력된 이미지 및 오디오 데이터를 포함할 수도 있고, 적어도 하나의 외부 장치로부터 전자 장치 로 수신된 것일 수도 있다. 메모리는 비디오의 편집을 위한 적어도 하나의 인공지능 모델을 포함할 수 있다. 인공지능 모델은 객체 인식, 이미지 프레임 선택, 오디오 데이터의 구분 및 편집 등 다양한 편집 기능을 수행하도록 구현될 수 있다. 인공지능 모델은, 지도 학습, 비지도 학습, 준지도 학습, 강화 학습 등 다양한 방식의 머신 러닝에 기반한 모델 일 수 있다. 또한, 인공지능 모델은 배치 학습(batch learning), 온라인 학습(online learning) 등에 기반한 모델일 수 있다. 일 실시 예로, 인공지능 모델은, 신경망(Neural Network)을 기반으로 하는 네트워크 모델(신경망 모델)에 해당할 수 있다. 네트워크 모델은 가중치를 가지는 복수의 네트워크 노드들을 포함할 수 있다. 복수의 네트워크 노드들은 서로 다른 레이어의 노드 간 가중치를 기반으로 연결 관계를 형성할 수 있다. 신경망 모델은, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network), GAN(Generative Adversarial Network) 및 심층 Q-네트워크(Deep Q-Networks) 등으로 구 성될 수 있으나, 이에 한정되지 않는다. 일 실시 예로, 인공지능 모델은 비디오 내 복수의 이미지 프레임 중 등장 인물의 음성에 매칭되는 장면에 해당하는 하나 이상의 이미지 프레임을 선택할 수 있다. 등장 인물의 음성에 매칭되는 장면이란, 비디오 내의 등장 인물이 말하는 순간을 포함하는 장면(비디오 내 시간 구간)을 의미한다. 구체적으로, 인공지능 모델은, 이미지 프레임 내에서 입 모양을 추출하기 위한 인공지능 모델, 음성에 매 칭되는 장면의 이미지 프레임들을 선택하기 위한 인공지능 모델 등을 각각 포함할 수 있는 바, 도 3을 통해 보 다 상세하게 후술한다. 프로세서는 전자 장치에 포함된 각 구성을 전반적으로 제어하기 위한 구성으로, CPU(Central Processing Unit), AP(Application Processor), GPU(Graphic Processing Unit), VPU(Visual Processing Unit), NPU(Neural Processing Unit) 등 다양한 유닛으로 구성될 수 있다. 프로세서는 메모리에 저장된 인스트럭션을 실행함으로써 전자 장치를 제어할 수 있다. 도 1을 참조하면, 프로세서는 이미지 변환 엔진, 비디오 편집 엔진 등을 포함할 수 있다. 본 엔 진들은, 소프트웨어 및/또는 하드웨어를 기반으로 구현되어 프로세서를 통해 제어될 수 있는 기능적 구성 들이다. 이미지 변환 엔진은, 후술할 비디오 편집 엔진의 편집 기능에 적합하도록 비디오 내 복수의 이미지 프레임을 변환하기 위한 구성이다. 구체적으로, 이미지 변환 엔진은 비디오 편집 엔진을 통해 활용되는 상술한 인공지능 모델에 적 합하도록 각 이미지 프레임을 변환할 수 있다. 일 실시 예로, 이미지 변환 엔진은 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에 포함된 얼 굴의 위치 및/또는 비율에 따라 복수의 이미지 프레임을 변환할 수 있는 바, 도 2를 통해 보다 자세히 후술한다. 비디오 편집 엔진은 비디오를 편집하기 위한 구성이다. 일 실시 예로, 비디오 편집 엔진은 비디오를 구성하는 복수의 이미지 프레임 중 등장 인물의 음성에 매칭 되는 적어도 하나의 장면(비디오 내 시간 구간)을 선택할 수 있다. 구체적으로, 비디오 편집 엔진은 앞서 이미지 변환 엔진을 통해 변환된 복수의 이미지 프레임을 인공 지능 모델에 입력할 수 있다. 이 경우, 인공지능 모델은 복수의 이미지 프레임 중 음성에 매칭되는 이미지 프레임들을 선택하여 출력할 수 있다. 그리고, 비디오 편집 엔진은 비디오 내에서 선택된 이미지 프레임들이 포함된 타겟 시간 구간을 제외한 나 머지 시간 구간을 삭제할 수 있다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 상술한 엔진들(121, 122)을 통해 도 2의 동작들을 설명한다. 도 2를 참조하면, 이미지 변환 엔진은 비디오를 구성하는 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에서 얼굴을 포함하는 영역을 식별할 수 있다(S210). 여기서, 이미지 변환 엔진은 각 이미지 프레임 내 픽셀 별 RGB 값, 픽셀 별 RGB 값의 변화 패턴 등을 이용 하여 인간 등의 얼굴을 인식할 수 있다. 예를 들어, 일정 범위의 RGB 값(ex. 피부색에 매칭되는 컬러 값)을 가 지는 픽셀들로 구성된 영역이 일정 크기 또는 기설정된 형상에 매칭되는 경우, 해당 영역은 얼굴을 포함하는 것으로 인식될 수 있다. 또는, 이미지 변환 엔진은 이미지가 입력되면 얼굴(ex. 인간, 캐릭터 등의 얼굴)을 포함하는 영역을 식별 하도록 훈련된 적어도 하나의 CNN(Convolutional Neural Network) 기반 인공지능 모델을 이용할 수도 있다. 본 인공지능 모델은, 얼굴을 포함하는 다양한 이미지를 통해 훈련된 모델일 수 있다. 상술한 바와 같이 얼굴을 포함하는 영역이 인식되면, 이미지 변환 엔진은 이미지 프레임 내 얼굴을 포함하 는 영역의 위치 및 비율을 기반으로, 복수의 이미지 프레임을 변환할 수 있다(S220). 구체적으로, 이미지 변환 엔진은 이미지 프레임의 가로축에 해당하는 x축 및 세로축에 해당하는 y축을 기 준으로, 얼굴을 포함하는 영역의 2차원적 위치 정보를 식별할 수 있다. 또한, 이미지 변환 엔진은, 이미지 프레임 내 얼굴을 포함하는 영역의 비율을 기반으로 비율 정보를 식별 할 수 있다. 여기서, 비율 정보는, 이미지 프레임의 전체 크기에 대한 얼굴 영역의 비율에 해당할 수도 있고, 이미지 프레임의 전체 해상도에 대한 얼굴 영역의 비율에 해당할 수도 있으며, 이미지 프레임 내 적어도 하나의 다른 객체에 대한 얼굴 영역의 비율에 해당할 수도 있다. 그리고, 이미지 변환 엔진은, (편집을 위한) 인공지능 모델에 매칭되는 위치 정보에 대한 식별된 위 치 정보의 차이 및 인공지능 모델에 매칭되는 비율 정보에 대한 식별된 비율 정보의 차이를 기반으로, 변 환 정보를 생성할 수 있다. 여기서, 인공지능 모델에 매칭되는 위치 정보는, 인공지능 모델의 훈련에 이용된 적어도 하나의 이미 지 내 얼굴의 위치에 따라 설정된 것일 수 있다. 또한, 인공지능 모델에 매칭되는 비율 정보는, 인공지능 모델의 훈련에 이용된 적어도 하나의 이미지 내 얼굴의 비율에 따라 설정된 것일 수 있다. 구체적으로, 이미지 변환 엔진은 상술한 위치 정보/비율 정보의 차이가 줄어들거나 없어지도록 하는 변환 정보를 생성할 수 있다. 변환 정보는, 이미지 프레임의 크기, 가로세로 비율, 해상도, 얼굴을 포함하는 영역의 위치, 이미지 프레임 내 적어도 일부 영역의 크기 등 다양한 요소를 변경하기 위한 정보를 포함할 수 있다. 그리고, 이미지 변환 엔진은 생성된 변환 정보를 이용하여 복수의 이미지 프레임을 변환할 수 있다. 이 경 우, 얼굴을 포함하는 이미지 프레임 외에 얼굴을 포함하지 않는 이미지 프레임 역시 동일한 변환 정보에 따라 변환될 수 있다. 한편, 이미지 변환 엔진은 비디오 내 복수의 이미지 프레임을 둘 이상의 그룹으로 구분하고, 그룹 별로 독 립적인 변환을 수행할 수도 있다. 일 실시 예로, 이미지 변환 엔진은 복수의 이미지 프레임 각각에 포함된 얼굴의 위치 및 식별 정보 중 적 어도 하나에 따라, 복수의 이미지 프레임을 제1 복수의 이미지 프레임(제1 그룹) 및 제2 복수의 이미지 프레임 (제2 그룹)으로 구분할 수 있다. 이 경우, 이미지 변환 엔진은 제1 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내 얼굴을 포함 하는 영역의 위치 및 비율을 기반으로, 제1 복수의 이미지 프레임을 변환할 수 있다. 구체적으로, 이미지 변환 엔진은 이미지 프레임 내 얼굴의 위치 정보 및 비율 정보를 인공지능 모델 에 매칭되는 위치 정보 및 비율 정보와 각각 비교하여 제1 변환 정보를 생성할 수 있다. 그리고, 이미지 변환 엔진은 제1 변환 정보를 이용하여 제1 복수의 이미지 프레임을 변환할 수 있다. 또한, 이미지 변환 엔진은 제2 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내 얼굴을 포함하는 영역의 위치 및 비율을 기반으로, 제2 복수의 이미지 프레임을 변환할 수 있다. 구체적으로, 이미지 변환 엔진은 이미지 프레임 내 얼굴의 위치 정보 및 비율 정보를 인공지능 모델 에 매칭되는 위치 정보 및 비율 정보와 각각 비교하여 제2 변환 정보를 생성할 수 있다. 그리고, 이미지 변환 엔진은 제2 변환 정보를 이용하여 제2 복수의 이미지 프레임을 변환할 수 있다. 상술한 다양한 실시 예에 따라 비디오 내 복수의 이미지 프레임이 변환되면, 비디오 편집 엔진은 변환된 복수의 이미지 프레임을 인공지능 모델에 입력할 수 있다(S230). 이 경우, 비디오 편집 엔진은 변환된 복수의 이미지 프레임과 함께 비디오 내 오디오 데이터를 함께 입력 할 수 있다. 그 결과, 인공지능 모델은 음성에 매칭되는 장면을 구성하는 이미지 프레임들을 선택하여 출력할 수 있다. 그리고, 비디오 편집 엔진은 비디오 내에서 선택된 이미지 프레임들을 포함하는 타겟 시간 구간을 제외한 시간 구간(이미지 프레임들, 오디오 데이터)을 삭제할 수 있다. 도 3은 본 개시의 일 실시 예에 따른 전자 장치의 각 엔진의 구체적인 기능을 설명하기 위한 블록도이다. 도 3을 참조하면, 이미지 변환 엔진은 얼굴 인식 엔진(121-1), 변환 정보 생성 엔진(121-2) 등을 포함할 수 있다. 얼굴 인식 엔진(121-1)은, 상술한 S210 단계와 같이, 비디오를 구성하는 복수의 이미지 프레임 중 적어도 하나 에 포함된 얼굴을 식별하기 위한 구성이다. 변환 정보 생성 엔진(121-2)은, 상술한 S220 단계와 같이, 이미지 프레임 내 얼굴 영역의 위치 정보 및 비율 정 보 각각을 인공지능 모델에 매칭되는 위치 정보 및 비율 정보를 비교하여 변환 정보를 생성하기 위한 구성 이다. 구체적으로, 변환 정보 생성 엔진(121-2)은 입 모양을 식별하도록 훈련된 제1 인공지능 모델(115-1)에 매칭되는 위치 정보 및 비율 정보를 이용하여 변환 정보를 생성할 수 있다. 이미지 변환 엔진은 변환 정보 생성 엔진(121-2)을 통해 생성된 변환 정보를 이용하여 복수의 이미지 프레 임 각각을 변환할 수 있다. 도 3을 참조하면, 비디오 편집 엔진은 입 모양 추출 엔진(122-1), 프레임 선택 엔진(122-2), 컷 편집 엔진 (122-3) 등을 포함할 수 있다. 비디오 편집 엔진은 편집을 위한 인공지능 모델을 구성하는 하나 이상의 인공지능 모델(115-1, 2)을 이용할 수 있다. 입 모양 추출 엔진(122-1)은, 비디오를 구성하는 (변환된) 복수의 이미지 프레임 각각에 포함된 등장 인물의 입 모양을 식별하기 위한 구성이다. 일 예로, 입 모양 추출 엔진(122-1)은 등장 인물(ex. 사람, 캐릭터)의 입 모양을 식별하도록 훈련된 제1 인공지 능 모델(115-1)을 활용할 수 있다. 제1 인공지능 모델(115-1)은, CNN(Convolutional Neural Network) 기반 모델로, 다양한 등장 인물의 입을 포함 하는 훈련용 이미지를 통해 훈련된 것일 수 있으나, 이에 한정되지 않는다. 프레임 선택 엔진(122-2)은, 비디오 내에서 등장 인물의 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택 하기 위한 구성이다. 일 예로, 프레임 선택 엔진은, 입 모양 추출 엔진을 통해 추출된 입 모양에 대한 정보 및 (비디오의) 오디오 데이터를 기반으로 등장 인물의 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택하는 제2 인공지능 모델(115-2)을 활용할 수 있다. 컷 편집 엔진(122-3)은, 비디오 내 적어도 하나의 시간 구간을 편집 내지는 제거하기 위한 구성이다. 일 예로, 컷 편집 엔진(122-3)은 비디오 내에서 프레임 선택 엔진(122-2)을 통해 선택된 이미지 프레임들을 포 함하는 타겟 시간 구간을 식별하고, 비디오 내에서 타겟 시간 구간을 제외한 나머지 시간 구간을 모두 제거할 수 있다.구체적으로, 컷 편집 엔진(122-3)은 타겟 시간 구간을 제외한 시간 구간에 포함되는 이미지 프레임 및 오디오 데이터를 제거함으로써, 편집된 비디오를 획득할 수 있다. 한편, 도 4는 본 개시의 일 실시 예에 따라 이미지 프레임을 변환하는 전자 장치의 동작을 설명하기 위한 도면 이다. 도 4는 이미지 변환 엔진의 동작의 일 예에 해당한다. 도 4를 참조하면, 이미지 변환 엔진은 비디오를 구성하는 복수의 이미지 프레임 중 적어도 하나의 이미지 프레임 내에서 얼굴 영역을 인식할 수 있다. 그리고, 이미지 변환 엔진은 이미지 프레임 내 얼굴 영역의 위치 정보 및 비율 정보를 식별할 수 있다. 여기서, 이미지 변환 엔진은 얼굴 영역의 위치 정보 및 비율 정보가 인공지능 모델(ex. 115-1)에 매 칭되는 위치 정보 및 비율 정보가 되도록 이미지를 변환할 수 있다. 그 결과, 변경된 얼굴 영역을 포함하는 변환 이미지가 획득될 수 있고, 본 이미지는 인공지능 모델(115, 115-1, 115-2)의 입 모양 추출 내지는 프레임 선택에 적합한 상태로 이용될 수 있다. 한편, 도 5는 본 개시의 일 실시 예에 따른 전자 장치가 변환된 복수의 이미지 프레임을 이용하여 편집을 수해 하는 동작을 설명하기 위한 흐름도이다. 도 5는, 상술한 비디오 편집 엔진의 동작을 설명하기 위한 것이다. 도 5를 참조하면, 입 모양 추출 엔진(122-1)은, 제1 인공지능 모델(115-1)을 통해, 변환된 복수의 이미지 프레 임 각각에 포함된 등장 인물의 입 모양을 이미지 프레임 별로 식별할 수 있다(S510). 입 모양 추출 엔진(122-1)은 다양한 형태로 입 모양에 대한 정보를 획득할 수 있다. 구체적으로, 입 모양 추출 엔진(122-1)은 이미지 프레임 별로 입을 포함하는 영역(부분 이미지)을 추출할 수 있 다. 또한, 입 모양 추출 엔진은 인식된 입 모양의 속성 정보를 추출할 수도 있다. 예를 들어, 입 모양 추출 엔 진은 입이 벌어진 정도, 입이 기울어진 방향, 입이 튀어나온 정도 등에 대한 정보를 획득할 수 있다. 이를 위해, 다양한 모양의 입을 포함하는 훈련 이미지들을 통해 훈련된 제1 인공지능 모델(115-1)이 이용될 수 있다. 다만, 등장 인물의 입 내지는 얼굴이 포함되지 않은 이미지 프레임 내에서는 입 모양에 대한 정보가 추출되지 않을 수 있다. 이 경우, 입 모양에 대한 정보는 “없음”과 같은 형태로 정의될 수 있다. 프레임 선택 엔진(122-2)은, 입 모양 추출 엔진(122-1)을 통해 식별된 입 모양에 대한 정보 및 비디오의 오디오 데이터를 제2 인공지능 모델(115-2)에 입력하여, 비디오 내 음성과 매칭되는 적어도 하나의 이미지 프레임을 선 택할 수 있다(S520). 이 경우, 제2 인공지능 모델(115-2)은, 입 모양에 대한 정보 및 오디오 데이터의 시간 별 진폭에 따라, 비디오 내 복수의 이미지 프레임 중 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택할 수 있다. 일 예로, 제2 인공지능 모델(115-2)은 연속되는 이미지 프레임을 통해 구현되는 입 모양의 변화에 따라 각 이미 지 프레임의 중요도를 판단할 수 있다. 구체적으로, 제2 인공지능 모델(115-2)은, 비디오 내 복수의 이미지 프레임 각각에 대하여, 이전 이미지 프레임 과의 입 모양 차이에 따른 제1 중요도를 획득하는, 제1 모듈을 포함할 수 있다. 일 예로, 제1 모듈은, 이전 이 미지 프레임과의 입 모양 차이가 큰 이미지 프레임일수록 제1 중요도를 더 크게 산출할 수 있다. 또한, 제2 인공지능 모델(115-2)은 오디오 데이터의 진폭 변화에 따라 단위 시간 구간 별 오디오 데이터의 중요 도를 판단할 수도 있다. 구체적으로, 제2 인공지능 모델(115-2)은, 복수의 이미지 프레임 각각에 매칭되는 오디오 데이터의 (단위) 시간 구간 별로, 진폭에 따른 제2 중요도를 획득할 수 있다. 이미지 프레임에 매칭되는 단위 시간 구간이란, 비디오 의 재생 시 이미지 프레임의 출력과 동시에 출력되는 오디오 데이터의 시간 구간을 의미한다. 일 예로, 이전 단위 시간 구간의 진폭과 비교하여 진폭 변화가 큰 단위 시간 구간일수록 제2 중요도가 더 크게 산출될 수도 있다. 여기서, 제2 중요도는, 비록 제1 중요도와 달리 이미지 프레임에 대하여 직접적으로 산출된 것은 아니기는 하나, 각 이미지 프레임에 매칭되는 오디오 데이터의 단위 시간 구간에 대하여 산출된 것이므로, 제2 중요도 역 시 각 이미지 프레임의 중요도로 이해되어도 무방하다고 볼 수 있다. 그리고, 제2 인공지능 모델(115-2)은, 프레임별로 산출된 상술한 제1 중요도 및 제2 중요도를 활용하여 음성에 매칭되는 이미지 프레임을 하나 이상 선택할 수 있다. 구체적으로, 제2 인공지능 모델(115-2)은, 제1 중요도 및 제2 중요도를 활용하여 복수의 이미지 프레임 중 비디 오 내 음성에 매칭되는 적어도 하나의 이미지 프레임을 선택하는 제3 모듈을 포함할 수 있다. 일 실시 예로, 제3 모듈은, 연속되는 복수의 이미지 프레임 각각에 대한 제1 중요도의 제1 변화 패턴, 및 연속 되는 복수의 이미지 프레임 각각에 대한 제2 중요도의 제2 변화 패턴을 기반으로, 복수의 이미지 프레임 중 음 성에 매칭되는 적어도 하나의 이미지 프레임을 선택할 수 있다. 이를 위해, 제3 모듈은, 훈련용 비디오의 편집(ex. 전문가에 의해 수행된 편집) 전후의 제1 중요도 및 제2 중요 도 각각의 변화 패턴을 통해 훈련될 수 있으나, 이에 한정되지 않는다. 한편, 제3 모듈은, 다양한 알고리즘을 기반으로 하나 이상의 이미지 프레임을 선택하도록 설계될 수 있다. 일 예로, 제3 모듈은, 프레임별로 제1 중요도와 제2 중요도를 합산하여 중요도를 획득할 수 있다. 이 경우, 제3 모듈은, 복수의 이미지 프레임 중 중요도가 가장 높은 메인 이미지 프레임이 포함된 연속된 이미 지 프레임들을 선택할 수 있다. 여기서, 제3 모듈은, 선택된 이미지 프레임들에 대하여 중요도의 평균이 임계치 (ex. 기설정된 값) 이상이 되도록, 이미지 프레임들을 선택할 수 있다. 또한, 제3 모듈은 제1 중요도 및 제2 중요도에 따라 하나 이상의 주요 이미지 프레임을 식별할 수도 있다. 주요 이미지 프레임은, 음성에 매칭되는 이미지 프레임으로 선택되어야 하는 이미지 프레임으로 정의될 수 있다. 예를 들어, 제3 모듈은 제1 중요도 및 제2 중요도 중 적어도 하나가 임계치 이상인 이미지 프레임들을 각각 주 요 이미지 프레임으로 식별할 수 있다. 여기서, 임계치는 일정 값으로 기설정된 것일 수도 있고, 비디오 내 이 미지 프레임들의 제1 중요도 또는 제2 중요도의 평균 값에 따라 설정된 것일 수도 있다. 일 예로, 제1 중요도가 평균 값보다 30% 이상 높은 이미지 프레임이 주요 이미지 프레임으로 식별될 수 있다. 또는, 제3 모듈은 제1 중요도 및 제2 중요도가 합산된 중요도가 임계치 이상인 이미지 프레임들을 각각 주요 이 미지 프레임으로 식별할 수도 있다. 여기서, 임계치는 일정 값으로 기설정된 것일 수도 있고, 비디오 내 이미지 프레임들의 (합산된) 중요도의 평균 값에 따라 설정된 것일 수도 있다. 이렇듯 주요 이미지 프레임들이 식별되면, 제3 모듈은, 식별된 주요 이미지 프레임을 둘 이상 포함하는 연속된 이미지 프레임들을 선택할 수 있다. 이때, 선택된 (연속된) 이미지 프레임들 내에서, 주요 이미지 프레임 간의 간격은 임계 시간을 넘지 않을 수 있 다. 여기서, 선택된 이미지 프레임들 내 첫 번째 이미지 프레임 또는 마지막 이미지 프레임은, 주요 이미지 프 레임이거나 또는 적어도 하나의 주요 이미지 프레임으로부터 일정 시간 범위 내일 수도 있다. 상술한 바와 같이 제2 인공지능 모델(115-2)의 제3 모듈은, 제1 중요도 및 제2 중요도를 바탕으로 다양한 방식 에 따라 하나 이상의 이미지 프레임을 선택할 수 있는 바, 상술한 실시 예들은 서로 저촉되지 않는 한 둘 이상 이 결합되어 활용될 수도 있다. 한편, 상술한 제1 내지 제3 모듈은 각각 구분된 하나 이상의 신경망으로 구현될 수 있으며, 각각 독립적으로 훈 련되었거나 및/또는 전체가 함께 훈련되었을 수 있다. 컷 편집 엔진(122-3)은 비디오 중 프레임 선택 엔진(122-2)을 통해 선택된 이미지 프레임들이 포함된 타겟 시간 구간을 제외한 시간 구간을 삭제할 수 있다(S530). 결과적으로, 음성에 매칭되는 장면만이 남겨진 편집된 비디오(선택된 이미지 프레임들, 선택된 이미지 프레임들 과 매칭되는 오디오 데이터)가 획득될 수 있다. 편집된 비디오는, 앞서 이미지 변환 엔진을 통해 변환된 상태인 이미지 프레임들로 구성될 수도 있고, 변 환되지 않은 상태인 이미지 프레임들로 구성될 수도 있다. 도 6은 본 개시의 일 실시 예에 따른 전자 장치가 각각의 이미지 프레임 및 오디오 데이터를 이용하여 시간 구 간 별 중요도를 판단하는 동작을 설명하기 위한 도면이다.도 6을 참조하면, 프레임 선택 엔진(122-2)은 제2 인공지능 모델(115-2)을 통해 각 이미지 프레임(611, 612, 613, …)의 제1 중요도를 산출할 수 있다. 구체적으로, 프레임 선택 엔진(122-2)은 각 이미지 프레임의 이전 프 레임과의 입 모양 차이를 기반으로 제1 중요도를 산출할 수 있다. 또한, 프레임 선택 엔진(122-2)은 제2 인공지능 모델(115-2)을 통해 각 이미지 프레임(611, 612, 613, …)과 매 칭되는 각 (단위) 시간 구간(621, 622, 623)의 오디오 데이터마다 제2 중요도를 산출할 수 있다. 그리고, 프레임 선택 엔진(122-2)은 프레임별로 산출된 제1 중요도 및 제2 중요도를 활용하여, 비디오 내 복수 의 이미지 프레임 중 음성에 매칭되는 이미지 프레임들을 선택할 수 있다. 구체적으로, 제2 인공지능 모델(115-2)의 상술한 제3 모듈은 제1 중요도 및 제2 중요도를 다양한 방식으로 활용 하여 음성에 매칭되는 이미지 프레임들을 선택할 수 있다. 일 실시 예로, 제3 모듈은, 이미지 프레임들(611, 612, 613, …) 각각에 대하여 산출된 제1 중요도의 제1 변화 패턴, 및 이미지 프레임들에 매칭되는 시간 구간들(621, 622, 623, …) 각각에 대하여 산출된 제2 중요도의 제2 변화 패턴을 기반으로, 비디오 내 복수의 이미지 프레임 중 음성에 매칭되는 적어도 하나의 이미지 프레임을 선 택할 수 있다. 도 7은 본 개시의 일 실시 예에 따른 전자 장치가 타겟 시간 구간을 제외한 시간 구간을 삭제하는 동작을 설명 하기 위한 도면이다. 도 7는 도 6의 과정을 통해 비디오 내에서 음성에 매칭되는 이미지 프레임들이 선택 된 상황을 가정한다. 도 7을 참조하면, 컷 편집 엔진(122-3)은 비디오 내에서 선택된 이미지 프레임들을 포함하는 타겟 시간 구 간을 식별하고, 타겟 시간 구간을 제외한 시간 구간(이미지 프레임들, 오디오 데이터)을 삭제할 수 있다. 그 결 과, 컷 편집 엔진(122-3)은 편집된 비디오를 획득할 수 있다. 한편, 도 8은 본 개시의 다양한 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 8을 참조하면, 전자 장치는 메모리 및 프로세서 외에 통신부, 사용자 입력부, 출 력부 등을 더 포함할 수 있다. 통신부는 전자 장치가 다양한 외부 장치와 데이터를 송수신하기 위한 구성으로, 통신을 위한 적어도 하나의 회로를 포함할 수 있다. 일 예로, 전자 장치는 통신부를 통해 연결된 적어도 하나의 외부 장치로부터 비디오를 수신하여, 수 신된 비디오에 대하여 상술한 다양한 실시 예에 따른 제어 방법(이미지 변환, 컷 편집 등)을 수행할 수 있다. 그리고, 전자 장치는 편집된 비디오를 다시 외부 장치로 전송할 수 있다. 통신부는 TCP/IP(Transmission Control Protocol/Internet Protocol), UDP(User Datagram Protocol), HTTP(Hyper Text Transfer Protocol), HTTPS(Secure Hyper Text Transfer Protocol), FTP(File Transfer Protocol), SFTP(Secure File Transfer Protocol), MQTT(Message Queuing Telemetry Transport) 등의 통신 규 약(프로토콜)을 이용하여 하나 이상의 외부 전자 장치와 다양한 정보를 송수신할 수 있다. 이를 위해, 통신부는 유선 통신 및/또는 무선 통신을 통해 구현된 네트워크를 기반으로, 외부 장치와 연결 될 수 있다. 이때, 통신부는 외부 장치와 직접적으로 연결될 수도 있지만, 네트워크를 제공하는 하나 이상 의 외부 서버(ex. ISP(Internet Service Provider))를 통해서 외부 전자 장치와 연결될 수도 있다. 네트워크는 영역 또는 규모에 따라 개인 통신망(PAN; Personal Area Network), 근거리 통신망(LAN; Local Area Network), 광역 통신망(WAN; Wide Area Network) 등일 수 있으며, 네트워크의 개방성에 따라 인트라넷 (Intranet), 엑스트라넷(Extranet), 또는 인터넷(Internet) 등일 수 있다. 무선 통신은 LTE(long-term evolution), LTE-A(LTE Advance), 5G(5th Generation) 이동통신, CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), GSM(Global System for Mobile Communications), DMA(Time Division Multiple Access), WiFi(Wi-Fi), WiFi Direct, Bluetooth, NFC(near field communication), Zigbee 등의 통신 방식 중 적어도 하나를 포함할 수 있다. 유선 통신은 이더넷(Ethernet), 광 네트워크(optical network), USB(Universal Serial Bus), 선더볼트 (ThunderBolt) 등의 통신 방식 중 적어도 하나를 포함할 수 있다. 여기서, 통신부는 상술한 유무선 통신 방식에 따른 네트워크 인터페이스(Network Interface) 또는 네트워 크 칩을 포함할 수 있다. 한편, 통신 방식은 상술한 예에 한정되지 아니하고, 기술의 발전에 따라 새롭게 등장 하는 통신 방식을 포함할 수 있다. 사용자 입력부는 사용자 명령 또는 사용자 정보 등을 입력 받기 위한 구성이다. 전자 장치가 스마트폰, 노트북 PC 등 사용자 단말로 구현된 경우, 사용자 입력부는 터치 센서, 버튼, 카메라, 마이크, 키보드 등으로 구현될 수 있다. 전자 장치가 데스크탑 PC인 경우, 사용자 입력부는 키보드 또는 마우스 등 다양한 입력 인터페이스와 연결된 단자로 구현될 수 있다. 일 예로, 전자 장치는 사용자 입력부를 통해 수신된 사용자 명령에 따라 적어도 하나의 비디오에 대 하여 상술한 제어 방법(이미지 변환, 컷 편집)을 수행할 수 있다. 출력부는 다양한 정보를 출력하여 사용자에게 제공하기 위한 구성이다. 전자 장치가 스마트폰 등의 사용자 단말로 구현된 경우, 출력부는 디스플레이, 스피커, 이어폰/헤드 셋 단자 등을 포함할 수 있다. 일 실시 예로, 전자 장치는 출력부를 통해 변환 전후 및/또는 편집 전후의 비디오를 시각적/청각적으 로 제공할 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 서로 저촉되지 않는 한 복수의 실시 예가 결합되어 구현될 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 시스템 내 각 장치에서의 처리동작을 수행하기 위한 컴퓨터 명령어(computer instructions) 또는 컴퓨터 프로그램은 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어 또는 컴퓨터 프로그램은 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 전자장치에 서의 처리 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2021-0116697", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2021-0116697", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도, 도 2는 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도, 도 3은 본 개시의 일 실시 예에 따른 전자 장치의 각 엔진의 구체적인 기능을 설명하기 위한 블록도, 도 4는 본 개시의 일 실시 예에 따라 이미지 프레임을 변환하는 전자 장치의 동작을 설명하기 위한 도면, 도 5는 본 개시의 일 실시 예에 따른 전자 장치가 변환된 복수의 이미지 프레임을 이용하여 편집을 수행하는 동 작을 설명하기 위한 흐름도, 도 6은 본 개시의 일 실시 예에 따른 전자 장치가 각각의 이미지 프레임 및 오디오 데이터를 이용하여 시간 구 간 별 중요도를 판단하는 동작을 설명하기 위한 도면, 도 7은 본 개시의 일 실시 예에 따른 전자 장치가 타겟 시간 구간을 제외한 시간 구간을 삭제하는 동작을 설명 하기 위한 도면, 그리고 도 8은 본 개시의 다양한 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다."}
