{"patent_id": "10-2023-0066082", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0169164", "출원번호": "10-2023-0066082", "발명의 명칭": "인공지능 학습 방법", "출원인": "주식회사 인피닉", "발명자": "노성운"}}
{"patent_id": "10-2023-0066082", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공지능 학습 방법 공개특허10-2024-0169164 CPC특허분류 G06N 3/049 (2023.01) G06Q 10/0633 (2023.01) G06Q 20/14 (2013.01) G06Q 20/18 (2023.02) G06Q 20/20 (2023.05) G06T 7/251 (2017.01) G06V 10/82 (2022.01) G06V 40/20 (2022.01) H04N 7/18 (2023.01)명 세 서 청구범위 청구항 1 인공지능 학습 방법 발명의 설명"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "인공지능 학습 방법"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "76_발명의 설명 76_발명의 명칭 고객 행동에 기반한 결제 프로세스 제어 방법 및 이를 위한 시스템{Method for controlling payment processes based on customer behavior, and system therefor}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "76_기술분야 본 발명은 인공지능(Artificial Intelligence, AI)을 이용한 무인 점포(unmanned store)에 관한 것이다. 보다 상세하게는, 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 방법 및 이를 위한 시스템에 관한 것이다. 76_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 한편, 무인 점포(unmanned store)는 상품의 결제 및 관리를 수행하는 판매자 매장 내에 존재하지 않고 소비자가 구입을 원하는 물건을 가져와 직접 결제하는 점포를 말한다. 최근 들어, IT(Information Technology) 산업기술의 발달, 인건비 상승과 심야 시간대의 효율적인 운영 등의 이 유로 무인 점포에 대한 관심이 높아지고 있는 추세다. 이와 같은, 무인 점포는 주로 커피 전문점, 아이스크림 전문점 등과 같이 고객의 다양한 니즈를 반영하지 않아76_도 되는 상품을 취급하는 매장에서 제한적으로 적용되 고 있으나, 최근 그 적용 영역이 확대되고 있다. 이와 같은, 무인 점포 내에는 고객이 구매하고자 하는 상품을 직접 결제할 수 있는 무인 셀프 결제기(예를 들어, 키오스크 등)가 설치되어 있으며, 무인 점포 내에 비치되어 있는 각종 제품과 무인 셀프 결제기의 76_도난이나 파손 등을 감시하기 위해 폐쇄 회로 텔레비전(Closed Circuit TeleVision, CCTV)가 설치되어 있다. 76_선행기술문헌 76_특허문헌 대한민국 공개특허공보 제10-2020-0023990호, ‘무인 점포의 결제 처리 시스템, 방법 및 컴퓨터 프로그램’, (2020.03.06. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "76_발명의 내용 76_해결하고자 하는 과제 본 발명의 일 목적은 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 방법을 제공하는 것이다. 본 발명의 다른 목적은 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 시스템을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "76_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 방법을 제안한다. 상기 방법은 매장 내에 방문한 고객에 대한 영상을 촬영하는 단계; 상기 촬영된 영상을 시계열적으로 분석하여, 상기 고객의 행동 패턴을 식별하는 단계; 상기 식별된 고객의 행동 패턴을 기반으로, 상기 매장 내에 고정 설치된 무인 결제 장치가 수행해야할 결제 프로세스를 결정하는 단계; 및 상기 고객의 행 동 패턴을 기반으로 결정된 결제 프로세스와 상기 고객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여, 상기 결정된 결제 프로세스를 검증하는 단계를 포함할 수 있다. 구체적으로, 상기 고객의 행동 패턴을 식별하는 단계는 사전에 학습된 분류기(classifier)를 이용하여 사전에 정의된 행동 패턴 그룹들 중 상기 고객의 행동 패턴에 대응하는 하나의 그룹을 식별할 수 있다. 이와 같은, 상 기 고객의 행동 패턴에는 상기 고객의 상기 매장 내 상대적인 위치, 상기 고객의 상기 매장 내 이동 경로, 상기 고객의 신체 자세, 상기 고객의 시선 방향 및 상기 고객의 상품 파지 유무를 포함할 수 있다. 상기 고객의 행동 패턴을 식별하는 단계는 상기 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신체 부위 들을 식별하고, 상기 식별된 신체 부위들 각각의 상대적인 위치 및 방향성을 기초로 상기 식별된 신체 부위들을 서로 연결하여 스켈레톤(skeleton)을 생성하고, 생기 생성된 스켈레톤의 형상 및 모양을 기초로 상기 고객의 신 체 자세, 상기 고객의 시선 방향 및 상기 고객의 상품 파지 유무를 식별할 수 있다. 상기 결제 프로세스는 상품의 결제가 시작된 후 완료되기까지 상기 무인 결제 장치가 수행해야할 동작들이 개별 적으로 정의된 복수 개의 과정들을 포함하고, 상기 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정 되어 있다. 이 경우, 상기 결제 프로세스를 검증하는 단계는 상기 선행 결제 프로세스와 상기 결정된 결제 프로 세스에 각각 설정된 진행 순서가 연속적인지 여부를 판단할 수 있다. 또한, 상기 결제 프로세스를 검증하는 단계는 상기 결정된 결제 프로세스 이후에 진행 가능한 하나 이상의 후행 결제 프로세스의 과정을 식별하고, 상기 식별된 하나 이상의 후행 결제 프로세스의 과정에 대응하는 그룹에 가 중치를 부여할 수 있다. 이 경우, 상기 고객의 행동 패턴을 식별하는 단계는 상기 분류기로부터 출력된 그룹별 확률 값에 상기 가중치를 적용하여, 상기 고객의 행동 패턴에 대응하는 그룹을 식별할 수 있다. 이와 같은, 상기 결제 프로세스는 상기 무인 결제 장치 상의 일 측에 하나 이상의 상품이 적재된 결제 시작 과 정, 상기 무인 결제 장치 상에 적재된 하나 이상의 상품을 이송하는 상품 이송 과정, 상기 이송된 하나 이상의 상품을 개별적으로 식별하는 상품 식별 과정, 상기 개별적으로 식별된 상품의 가격을 합산하여 결제 금액을 산 출하는 금액 산출 과정, 고객에 대응하여 사전에 지정된 지불 수단을 이용하여 상기 산출된 결제 금액에 대한 결제를 수행하는 결제 수행 과정 및 상기 결제가 완료된 이후 하나 이상의 상품을 상기 무인 결제 장치의 타 측 으로 이송하는 상품 배출 과정을 포함할 수 있다. 한편, 상기 고객의 행동 패턴을 식별하는 단계는 상기 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전 에 정의된 부정 행위 그룹에 해당되는 경우, 고객의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력할 수 있다. 이 경우, 상기 부정 행위 그룹은 고객이 상기 무인 결제 장치 상에 복수 개의 상품을 서로 적층되게 얹어(put on) 하나 이상의 상품을 숨기는 행위에 대응하는 그룹이 될 수 있다.상기 고객의 행동 패턴을 식별하는 단계는 상기 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전에 정 의된 이용 경험 미숙 그룹에 해당되는 경우, 상기 무인 결제 장치의 사용법과 관련하여 사전에 지정된 음성 메 시지를 출력할 수 있다. 이 경우, 상기 이용 경험 미숙 그룹은 상기 무인 결제 장치 상에 하나 이상의 상품이 적재된 이후, 사전에 지정된 시간 내에 결제 프로세스가 다음 단계로 진행되지 않은 경우에 대응하는 그룹이 될 수 있다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 시스템을 제안한다. 상기 시스템은 매장 내에 고정 설치되어, 사전에 지정된 상품 투입 영역에 적재된 하 나 이상의 상품을 대상으로, 결제 프로세스에 따라 결제하는 무인 결제 장치; 상기 매장 내에 고정 설치되어, 상기 매장 내에 방문한 고객에 대한 영상을 촬영하는 원거리 비전 센서; 및 상기 원거리 비전 센서를 통해 촬영 된 영상을 시계열적으로 분석하여 고객의 행동 패턴을 식별하고, 상기 식별된 고객의 행동 패턴을 기반으로 상 기 무인 결제 장치가 수행해야 할 상기 결제 프로세스를 결정하고, 상기 고객의 행동 패턴을 기반으로 결정된 결제 프로세스와 상기 고객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여 상기 결정된 결제 프로세스를 검증하는 결제 제어 서버를 포함하여 구성될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 76_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "76_발명의 효과 본 발명의 실시 예들에 따르면, 매장 내에 결제를 수행하기 위한 인력이 존재하지 않아76_도, 매장에 방문한 고 객의 행동에 기반하여 무인 결제를 수행할 수 있게 된다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "76_도면의 간단한 설명 76_도 1은 본 발명의 일 실시예에 따른 무인 결제 시스템의 구성76_도이다. 76_도 2는 본 발명의 다른 실시예에 따른 무인 결제 시스템의 구성76_도이다. 76_도 3은 본 발명의 일 실시예에 따른 결제 제어 서버의 논리적 구성76_도이다. 76_도 4는 본 발명의 일 실시예에 따른 결제 제어 서버의 하드웨어 구성76_도이다. 76_도 5는 본 발명의 일 실시예에 따라 식별되는 고객의 행동을 설명하기 위한 예시76_도이다. 76_도 6은 본 발명의 일 실시예에 따른 구성요소가 배치된 매장을 설명하기 위한 예시76_도이다. 76_도 7은 본 발명의 일 실시예에 따른 운반 기구를 설명하기 위한 예시76_도이다. 76_도 8은 본 발명의 일 실시예에 따른 무인 결제 장치를 설명하기 위한 예시76_도이다. 76_도 9는 본 발명의 일 실시예에 따른 결제 프로세스의 과정들을 설명하기 위한 예시76_도이다. 76_도 10은 본 발명의 일 실시예에 따른 무인 결제 방법을 설명하기 위한 순서76_도이다. 76_도 11은 본 발명의 일 실시예에 따라 고객 및 상품을 관리하는 단계를 설명하기 위한 순서76_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "76_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의76_도가 아님을 유의해야 한다. 또한, 본 명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과76_도하게 포괄적인 의미로 해석되거나, 과76_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못 된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석 되어야 하며, 과76_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수76_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한 다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소76_도 제1 구성 요소로 명명될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수76_도 있지만, 중간에 다른 구성 요소가 존재할 수76_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 76_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, 76_도면 부호에 관계없 이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 76_도면은 본 발명의 사상을 쉽게 이해할 수 있76_도 록 하기 위한 것일 뿐, 첨부된 76_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아니 됨을 유의 해야 한다. 본 발명의 사상은 첨부된 76_도면 외에 모든 변경, 균등물 내지 대체물에 까지76_도 확장되는 것으 로 해석되어야 한다. 상술한 바와 같이, 무인 점포는 상품의 결제 및 관리를 수행하는 판매자 매장 내에 존재하지 않고 소비자가 구 입을 원하는 물건을 가져와 직접 결제하는 점포를 말한다. 이와 같은, 무인 점포를 운영하기 위해서는 고객, 상 품을 어떻게 관리하고, 결제를 어떻게 진행할지 등에 관한 다양한 수단들이 요구된다. 이러한 요구에 부합하고자, 본 발명은 무인 점포를 운영하기 위한 다양한 수단들을 제안하고자 한다. 76_도 1은 본 발명의 일 실시예에 따른 무인 결제 시스템의 구성76_도이다. 그리고, 76_도 2는 본 발명의 다른 실시예에 따른 무인 결제 시스템의 구성76_도이다. 76_도 1에 76_도시된 바와 같이, 본 발명의 일 실시예에 따른 무인 결제 시스템은 결제 제어 서버, 원거리 비전 센서, 운반 기구, 무인 결제 장치 및 안내 출력 장치를 포함하여 구성될 수 있다. 또한, 76_도 2에 76_도시된 바와 같이, 본 발명의 다른 실시예에 따른 무인 결제 시스템은 결제 제어 서버(10 0)와 무인 결제 장치가 서로 통합되어 하나의 장치로 구현될 수 있다. 이하, 본 발명의 다양한 실시예들을 설명함에 있어, 결제 제어 서버가 무인 결제 장치와 독립된 장치 인 것을 전제로 기술할 것이나, 결제 제어 서버가 무인 결제 장치의 일 구성요소 또는 무인 결제 장"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "치가 결제 제어 서버의 일 구성요소로 구현될 수 있음은 본 발명이 속한 기술분야에서 통상의 지식을 가진 자에게 자명할 것이다. 결제 제어 서버 및 무인 결제 장치 이외에76_도, 무인 결제 시스템의 구성 요소들은 기능적으로 구분 되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되 거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 결제 제어 서버는 고객 및 상품을 관리하고, 고객이 구매하고자 하 는 상품의 결제 프로세스를 제어할 수 있다. 구체적으로, 결제 제어 서버는 결제 제어 서버는 원거리 비전 센서 등에 의해 촬영된 영상을 시 계열적으로 분석하여, 매장 내에 방문한 고객을 다른 고객과 구별하기 위한 트래킹 아이디(tracking identifier)를 부여함으로써 고객을 추적할 수 있다. 결제 제어 서버는 원거리 비전 센서 등에 의해 촬영된 영상을 시계열적으로 분석하여 고객의 행동 패턴을 식별하고, 식별된 고객의 행동 패턴에 따라 고객이 구매할 것으로 예상되는 상품을 식별할 수 있다. 결제 제어 서버는 식별된 고객의 행동 패턴 및 고객의 트래킹 아이디를 기반으로, 무인 결제 장치가 수행해야 할 결제 프로세스를 결정할 수 있다. 그리고, 결제 제어 서버는 결정된 프로세스에 따라 무인 결제 장치의 상품 결제 과정을 제어할 수 있다. 결제 제어 서버는 원거리 비전 센서, 운반 기구, 무인 결제 장치 및 안내 출력 장치 와 데이터를 송수신할 수 있고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라76_도 허용될 수 있다. 예를 들어, 결제 제어 서버는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 그러나, 이에 한정되지 아니하 고, 결제 제어 서버는 스마트폰(smart phone), 랩탑(laptop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이동식 컴퓨팅 장치 중 어느 하나가 될 수76_도 있다. 이와 같은, 결제 제어 서버의 구체적인 구성 및 동작에 대해서는 76_도 3, 76_도 4, 76_도 9 및 76_도 10 을 참조하여 보다 구체적으로 후술하기로 한다. 다음 구성으로, 원거리 비전 센서는 매장 내부의 일 영역을 촬영할 수 있76_도록 매장 내에 고정 설치된 하나 이상의 비전 센서(vision sensor)이다. 이와 같은, 원거리 비전 센서는 단수 또는 복수 개로 구성될 수 있으며, 폐쇄 회로 텔레비전(Closed Circuit TeleVision, CCTV), 카메라(camera), 레이더(radar) 또는 라이다(lidar) 중 어느 하나가 될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 원거리 비전 센서는 결제 제어 서버에 의해 설정된 촬영 주기마다 매장의 내부 공간을 촬영할 수 있다. 만약, 매장 내에 고객이 방문한 경우, 원거리 비전 센서는 매장 내에 방문한 고객에 대한 영상을 촬영할 수 있다. 그리고, 원거리 비전 센서는 촬영된 하나 이상의 영상을 결제 제어 서버에 송신할 수 있다. 이와 같은, 원거리 비전 센서가 복수 개의 비전 센서로 구성된 경우, 원거리 비전 센서는 동일한 고 객을 서로 다른 방향에서 복수 개의 영상을 제각각 촬영할 수 있다. 또한, 원거리 비전 센서는 고객 또는 상품의 추적에 필요한 경우, 결제 제어 서버의 제어에 따라 틸 트(tilt), 스윙(swing), 피봇(pivot) 등을 수행하여 영상의 촬영 방향을 변경할 수76_도 있다. 다음 구성으로, 운반 기구는 일 측이 개방된 바구니(basket) 또는 카트(cart) 형태를 가지며, 고객이 구매 하고자 하는 상품을 임시적으로 적재하여 운반하는데 사용될 수 있는 기구이다. 구체적으로, 운반 기구는 상품이 적재될 수 있는 내부 공간을 촬영할 수 있는 근접 비전 센서를 포함하여 구성될 수 있다. 일 실시예에 따르면, 운반 기구는 결제 제어 서버의 제어에 대응하여, 근접 비전 센서를 통해 운반 기구의 내부 공간을 촬영할 수 있다. 그리고, 운반 기구는 근접 비전 센서에 의해 촬영된 영상을 결 제 제어 서버에 송신할 수 있다. 다른 실시예에 따르면, 운반 기구는 내부에 근접 센서(proximity sensor)를 더 구비할 수 있다. 이 경우, 운반 기구의 근접 비전 센서는 근접 센서를 통해 물체의 접근이 감지된 경우에만, 운반 기구의 내부 공간을 촬영하76_도록 제어될 수 있다. 그리고, 운반 기구는 근접 비전 센서에 의해 촬영된 영상을 결제 제어 서버에 송신할 수 있다. 다른 실시 예에 따르면, 운반 기구는 내부에 3축 또는 6축 가속76_도 센서(accelerometer)를 더 구비할 수 있다. 이 경우, 운반 기구의 근접 비전 센서는 가속76_도 센서를 통해 진동이 감지된 경우에만, 운반 기구 의 내부 공간을 촬영하76_도록 제어될 수 있다. 그리고, 운반 기구는 근접 비전 센서에 의해 촬영된 영상을 결제 제어 서버에 송신할 수 있다. 한편, 운반 기구는 내부에 적재될 수 복수 개의 상품들이 서로 이격된 상태로 투입되76_도록, 운반 기구 의 내부 공간을 복수 개의 세부 영역으로 구획할 수 있는 하나 이상의 파티션(partition)을 포함하여 구성 될 수 있다. 이와 같은, 파티션은 고객이 운반 기구의 내부에 적재하고자 하는 상품의 부피에 따라 자유롭 게 그 위치가 변동될 수 있다.이 경우, 운반 기구의 근접 비전 센서는 운반 기구를 구성하고 있는 하나 이상의 파티션의 위치가 외 력에 의해 변동되는 경우에만, 운반 기구의 내부 공간을 촬영하76_도록 제어될 수 있다. 그리고, 운반 기 구는 근접 비전 센서에 의해 촬영된 영상을 결제 제어 서버에 송신할 수 있다. 다음 구성으로, 무인 결제 장치는 매장 내의 일 영역에 설치되어, 사전에 지정된 상품 투입 영역에 적재된 하나 이상의 상품을 대상으로, 결제 제어 서버가 결정한 결제 프로세스에 따라 결제를 수행할 수 있다. 이와 같은, 무인 결제 장치의 상면은 고객이 구매하고자 하는 하나 이상의 상품을 적재할 수 있는 상품 투 입 영역, 하나 이상의 상품이 개별적으로 식별될 수 있는 상품 식별 영역, 및 고객이 결제가 완료된 하나 이상 의 상품을 픽업할 수 있는 상품 배출 영역으로 구분될 수 있다. 구체적으로, 무인 결제 장치는 결제 제어 서버가 자체적으로 결정한 결제 프로세스에 따른 명령을 수 신할 수 있다. 그리고, 무인 결제 장치는 결제 제어 서버로부터 수신된 명령에 따라 동작을 수행할 수 있다. 이 경우, 결제 프로세스는 상품의 결제가 시작된 후 완료되기 까지의 무인 결제 장치가 수행해야 할 동작 들이 개별적으로 정의된 복수 개의 과정들을 포함한다. 이와 같은, 결제 프로세스는 결제 시작 과정, 상품 이송 과정, 상품 식별 과정, 금액 산출 과정, 결제 수행 과정 및 상품 배출 과정을 포함하여 구성될 수 있다. 그리고, 결제 프로세스에 포함된 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정되어 있다. 보다 상세하게, 결제 시작 과정은 무인 결제 장치 상의 일 측(즉, 상품 투입 영역)에 하나 이상의 상품이 적재된 상태에서 무인 결제 장치가 수행해야할 하나 이상의 동작이 정의된 과정이다. 상품 이송 과정은 무인 결제 장치 상에 적재된 하나 이상의 상품을 무인 결제 장치상의 중앙(즉, 상 품 식별 영역)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 상품 식별 과정은 이송된 하나 이상의 상품을 깊이 카메라(depth camera) 등을 이용하여 개별적으로 식별하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 금액 산출 과정은 개별적으로 식별된 상품의 가격을 합산하여 결제 금액을 산출하기 위하여, 무인 결제 장치 가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 결제 수행 과정은 고객에 대응하여 사전에 지정된 지불 수단을 이용하여, 산출된 결제 금액에 대한 결제를 수행 하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 그리고, 상품 배출 과정은 결제가 완료된 이후 하나 이상의 상품을 무인 결제 장치의 타 측(즉, 상품 배출 영역)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 한편, 무인 결제 장치는 인접한 사람(즉, 고객) 또는 물체(즉, 상품 등)을 촬영할 수 있는 근거리 비전 센 서를 하나 이상 포함하여 구성될 수 있다. 이와 같은, 근거리 비전 센서는 카메라, 레이더 또는 라이다 중 어느 하나가 될 수 있으며, 이에 한정되는 것은 아니다. 무인 결제 장치의 근거리 비전 센서는 사람 또는 물체의 접근이 감지되거나 또는 결제 제어 서버의 제어에 대응하여, 영상의 촬영을 수행할 수 있다. 그리고, 무인 결제 장치는 촬영된 영상을 결제 제어 서 버에 전송할 수 있다. 다음 구성으로, 안내 출력 장치는 매장 내의 일 영역에 설치되어, 상품의 결제 과정에서 고객에게 전달하 고자 하는 안내를 출력할 수 있다. 이와 같은, 안내 출력 장치는 안내를 출력하기 위한 디스플레이 및 스 피커를 포함하여 구성될 수 있다. 구체적으로, 안내 출력 장치는 결제 제어 서버로부터 영상 또는 음성을 수신할 수 있다. 그리고, 안 내 출력 장치는 수신된 영상 또는 음성을 출력할 수 있다. 예를 들어, 안내 출력 장치는 무인 결제 장치에 의해 하나 이상의 상품이 식별되지 않은 경우 결제 제어 서버의 제어에 따라, 인식되지 않은 전부 또는 일부의 상품에 관한 영상을 출력할 수 있다. 또한, 안 내 출력 장치는 결제 제어 서버의 제어에 따라, 식별되지 않은 상품의 리스트를 출력할 수 있다. 이와 다르게, 안내 출력 장치는 결제 제어 서버의 제어에 따라, 고객의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력하거나, 또는 무인 결제 장치의 사용법과 관련하여 사전에 지정된 음성 메시지를 출 력할 수76_도 있다.지금까지 상술한 바와 같은, 무인 결제 시스템의 결제 제어 서버, 원거리 비전 센서, 운반 기구 , 무인 결제 장치 및 안내 출력 장치는 장치들 사이를 직접 연결하는 보안 회선, 공용 유선 통 신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것76_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와 이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱 텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은 특징을 가지는 결제 제어 서버의 구성에 대하여 보다 구체적으로 설명하기로 한다. 76_도 3은 본 발명의 일 실시예에 따른 결제 제어 서버의 논리적 구성76_도이다. 76_도 3에 76_도시된 바와 같이, 결제 제어 서버는 통신부, 입출력부, 저장부, 고객 관리 부, 상품 관리부 및 프로세스 제어부를 포함하여 구성될 수 있다. 이와 같은, 결제 제어 서버의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 원거리 비전 센서, 운반 기구, 무인 결제 장치 및 안내 출력 장치 중 하나 이상과 데이터를 송수신할 수 있다. 구체적으로, 통신부는 원거리 비전 센서로부터 매장 내부 공간이 촬영된 하나 이상의 영상을 수신할 수 있다. 통신부는 운반 기구로부터 근접 비전 센서에 의해 운반 기구의 내부 공간이 촬영된 하 나 이상의 영상을 수신할 수 있다. 통신부는 결제 프로세스를 수행하기 위한 하나 이상의 명령을 무인 결제 장치에 전송할 수 있다. 그 리고, 통신부는 안내 출력 장치가 출력해야 할 영상 또는 음성을 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해, 사용자로부터 신호를 입력 받 거나 또는 연산 결과를 출력할 수 있다. 구체적으로, 입출력부는 분류기(classifier)를 학습시키기 위한 데이터를 입력 받을 수 있다. 입출력부 는 고객의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력할 수 있다. 입출력부는 매장에 방문한 고객의 리스트 및 행동 분석 결과, 판매된 상품의 리스트, 결제된 금액 등과 관 련된 정보를 출력할 수 있다. 다음 구성으로, 저장부는 결제 제어 서버의 동작에 필요한 정보를 저장할 수 있다. 구체적으로, 저장부는 매장에 구비된 상품 재고의 현황을 저장할 수 있다. 저장부는 상품 결제를 위 해 고객에 대응하여 사전에 지정된 지불 수단에 관한 정보를 저장할 수 있다. 저장부는 분류기를 학습시키기 위한 데이터를 저장할 수 있다. 그리고, 저장부는 매장에 방문한 고객 의 리스트 및 행동 분석 결과, 판매된 상품의 리스트, 결제된 금액과 관련된 정보를 저장할 수 있다. 다음 구성으로, 고객 관리부는 매장 내 방문한 고객을 추적 및 관리할 수 있다. 구체적으로, 고객 관리부는 통신부를 통해 매장 내에 방문한 고객에 대한 영상을 수신할 수 있다. 고 객 관리부는 수신된 영상을 시계열적으로 분석하여, 매장 내에 방문한 고객이 새롭게 등장한 고객에 해당 되는지 판단할 수 있다. 만약, 매장 내에 방문한 고객이 새롭게 등장한 고객에 해당되는 경우, 고객 관리부는 매장 내에 방문한 고 객에 대한 트래킹 아이디(tracking identifier)를 할당할 수 있다. 여기서, 트래킹 아이디는 매장에 막 방문한 고객을 매장 내에 기 방문한 다른 고객과 구별하기 위한 식별자이다.보다 상세하게, 고객 관리부는 고객에 대한 영상으로부터 고객의 안면 영역을 식별할 수 있다. 고객 관리 부는 식별된 안면 영역에 포함된 복수 개의 랜드마크(landmark)를 인식할 수 있다. 여기서, 랜드마크는 제 1 사람의 안면을 제2 사람의 안면과 구별할 수 있는 기준이 되는 신체 부위를 의미한다. 예를 들어, 랜드마크에 는 눈, 코 및 입이 포함될 수 있으나, 이에 한정되는 것은 아니다. 그리고, 고객 관리부는 인식된 복수 개의 랜드마크들의 배치 및 형성, 복수 개의 랜드마크들 사이의 거리 를 기초로, 해당 고객에게 고유한 트래킹 아이디를 할당할 수 있다. 고객 관리부는 고객에게 할당된 트래킹 아이디를, 해당 고객에게 매칭된 속성에 추가할 수 있다. 여기서, 고객의 속성은 고객의 고유한 성질 또는 특징을 관리하기 위한 데이터 구조이다. 예를 들어, 고객의 속성에는 고객의 방문 일시, 행동 패턴 및 추적 허용 범위가 포함될 수 있으나, 이에 한정되지 않고, 고객이 직접 제공한 고객의 성명, 나이, 성별, 주소 및 지불 수단 등에 관한 정보가 더 포함될 수76_도 있다. 한편, 통신부를 통해 매장에 방문한 동일한 고객을 서로 다른 방향에서 촬영된 복수 개의 영상이 수신된 경우, 고객 관리부는 고객의 매장 내 이동 경로를 예측하고, 예측된 이동 경로를 기반으로 복수 개의 영상 내에 제각각 포함된 고객에게 동일한 트래킹 아이디를 할당할 수 있다. 이를 위한, 고객의 매장 내 이동 경로는 고객에 대한 영상으로부터 식별된 고객의 스켈레톤(skeleton) 구조를 기초로 예측될 수 있다. 예를 들어, 고객 관리부는 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신 체 부위들을 식별하고, 식별된 신체 부위들 각각의 상대적인 위치 및 방향성을 기초로 신체 부위들을 서로 연결 하여 스켈레톤을 생성할 수 있다. 고객 관리부는 생성된 스켈레톤의 형상 및 모양을 기초로, 고객의 신체 자세, 고객의 시선 방향, 고객의 상품 파지 유무를 식별할 수 있다. 고객 관리부는 식별된 고객의 신체 자 세, 고객의 시선 방향 및 고객의 상품 파지 유무를 기초로, 고객의 매장 내 이동 경로를 예측할 수 있다. 또한, 고객 관리부는 고객에게 트래킹 아이디를 추가함에 있어, 고객을 연속적으로 추적하기 위한 추적 허 용 범위를 설정할 수 있다. 이를 위하여, 고객 관리부는 고객에 대한 영상으로부터 사전에 지정된 복수 개 의 신체 부위들을 식별하고, 식별된 신체 부위들에 대한 평균 RGB(Red, Green, Blue) 값들을 산출할 수 있다. 고객 관리부는 산출된 평균 RGB 값들을 포함하는 추적 허용 범위를 설정하고, 설정된 추적 허용 범위를 해 당 고객에게 매칭된 속성에 추가할 수 있다. 여기서, 추적 허용 범위는 고객에 대한 영상 속에 포함된 고객이 다른 객체(object)에 의해 가려진 경우에76_도 해당 고객을 시계열적으로 연속하여 추적할 수 있76_도록 지정된 범위이다. 한편, 고객 관리부는 매장 내에 위치하는 고객에 대한 영상을 시계열적으로 분석하여 고객의 행동 패턴을 식별할 수 있다. 보다 상세하게, 고객 관리부는 사전에 기계 학습된 분류기(classifier)를 이용하여, 사전에 정의된 행동 패턴 그룹들 중에서 고객의 행동 패턴에 대응하는 하나의 그룹을 식별할 수 있다. 여기서, 고객의 행동 패턴에 는 고객의 매장 내 상대적인 위치, 고객의 매장 내 이동 경로, 고객의 신체 자세, 고객의 시선 방향 및 고객의 상품 파지 유무를 포함할 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같이, 고객 관리부는 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신체 부위들을 식별 하고, 식별된 신체 부위들 각각의 상대적인 위치 및 방향성을 기초로 신체 부위들을 서로 연결하여 스켈레톤을 생성할 수 있다. 고객 관리부는 생성된 스켈레톤의 형상 및 모양을 기초로, 고객의 신체 자세, 고객의 시 선 방향, 고객의 상품 파지 유무를 식별할 수 있다. 이 경우, 고객 관리부는 프로세스 제어부에 의해 가중치가 부여된 그룹을 추가적으로 고려할 수 있다. 예를 들어, 고객 관리부는 고객의 행동 패턴을 식별함에 있어, 분류기로부터 출력된 그룹별 확률 값 에 프로세스 제어부에 의해 부여된 가중치를 적용하여, 고객의 행동 패턴에 대응하는 그룹을 식별할 수 있 다. 만약, 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전에 정의된 부정 행위 그룹에 해당되는 경우, 고 객 관리부는 안내 출력 장치를 통해 고객의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력 할 수 있다. 이 경우, 부정 행위 그룹은 고객이 상기 무인 결제 장치 상에 복수 개의 상품을 서로 적층되게 얹 어(put on) 하나 이상의 상품을 숨기는 행위에 대응하거나, 또는 고객이 의류 또는 소지품을 이용하여 하나 이 상의 상품을 숨기는 행위에 대응하는 그룹이 될 수 있다. 만약, 상품 관리부를 통해 고객이 상품을 파지한 것으로 판단된 이후, 해당 상품이 원거리 비전 센서 또는 무인 결제 장치의 근거리 비전 센서에 의해 후속으로 촬영된 영상을 기초로, 해당 고객이 파지한 것으로 기 판단된 상품이 더 이상 파지되지 않은 것으로 판단되는 경우, 고객 관리부는 해당 고객에 대하여 기 식별된 그룹을 부정 행위 그룹으로 변경할 수76_도 있다. 만약, 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전에 정의된 이용 경험 미숙 그룹에 해당되는 경우, 고객 관리부는 안내 출력 장치를 통해 무인 결제 장치의 사용법과 관련하여 사전에 지정 된 음성 메시지를 출력할 수 있다. 이 경우, 이용 경험 미숙 그룹은 무인 결제 장치 상에 하나 이상의 상 품이 적재된 이후, 사전에 지정된 시간 내에 결제 프로세스가 다음 단계로 진행되지 않은 경우에 대응하는 그룹 이다. 다음 구성으로, 상품 관리부는 매장 내에 존재하는 모든 상품들을 관리하고, 고객이 구매할 것으로 예상되 는 상품들을 추적할 수 있다. 기본적으로, 상품 관리부는 매장 내에 존재하는 모든 상품들에 대한 속성을 관리할 수 있다. 여기서, 상품 의 속성은 각각의 상품을 관리하기 위한 데이터 구조이다. 예를 들어, 상품의 속성에는 상품의 입고 날짜, 재고 현황, 판매 금액, 상품의 부피, 상품의 무게, 유통 기한, 할인 적용 가부 등의 정보가 포함될 수 있으나, 이에 한정되지 않는다. 그리고, 상품 관리부는 매장 내에 존재하는 모든 상품들에 대한 속성을 저장부를 통해 저장 및 수정할 수 있다. 또한, 상품 관리부는 고객이 구매할 것으로 예상되는 상품들을 추적할 수 있다. 본 발명의 일 실시예에 따르면, 상품 관리부는 운반 기구를 이용하여, 고객이 구매할 것으로 예상되 는 상품들을 추적할 수 있다. 이와 같은 실시예는 고객이 대량의 상품을 구매하기 위하여 바구니 또는 카트 등 과 같은 운반 기구를 이용할 때 적합할 수 있다. 구체적으로, 상품 관리부는 통신부를 통해 운반 기구로부터 근접 비전 센서에 의해 운반 기구의 내부 공간이 촬영된 영상을 수신할 수 있다. 이 경우, 상품 관리부는 매장 내에 고정 설치되어 있는 원거리 비전 센서에 의해 촬영된 영상을 기초 로, 매장 내에 방문한 고객이 운반 기구를 파지한 것으로 판단되지 않는 경우, 해당 운반 기구로부터 수신된 영상을 무시할 수 있다. 상품 관리부는 운반 기구로부터 수신된 영상과, 해당 운반 기구로부터 기존에 수신된 이전 영상 을 서로 대비하여, 운반 기구의 내부에 새롭게 투입된 하나 이상의 상품을 식별할 수 있다. 그리고, 상품 관리부는 식별된 상품에 매칭된 속성에, 운반 기구를 이용하고 있는 고객에게 할당된 트래킹 아이디를 추가할 수 있다. 이와 같이, 운반 기구로부터 영상을 수신하는 단계 및 새롭게 투입된 상품을 식별하는 단계가 일 회 이상 수행된 이후, 상품 관리부는 운반 기구에 투입된 것으로 식별된 모든 상품의 가격을 합산하여 결제 금액을 산출할 수 있다. 만약, 매장 내에 고정 설치된 무인 결제 장치 상에 운반 기구가 통째로 적재된 경우(즉, 고객이 운반 기구로부터 상품을 꺼내어 무인 결제 장치 상에 적재하지 않고, 상품이 적재된 상태의 운반 기구 를 그대로 적재한 경우), 상품 관리부는 운반 기구의 무게를 측정하76_도록, 무인 결제 장치 를 제어할 수 있다. 이와 동시에, 상품 관리부는 운반 기구에 투입된 것으로 식별된 모든 상품 의 무게를 합산한 결과를 기초로, 운반 기구의 무게를 추정할 수 있다. 상품 관리부는 모든 상품의 무게를 기초로 추정된 무게와, 무인 결제 장치에 의해 측정된 무게를 서 로 대비할 수 있다. 상품 관리부는 무게를 서로 대비한 결과를 이용하여, 운반 기구의 근접 비전 센 서에 의해 촬영된 영상으로부터 식별되지 않은 상품이 존재하는지 검증할 수 있다. 근접 비전 센서에 의해 촬영된 영상으로부터 식별되지 않은 상품이 존재하는 경우, 상품 관리부는 모든 상 품의 무게를 기초로 추정된 무게와 무인 결제 장치에 의해 측정된 무게 사이의 차이 값과, 운반 기구(30 0)를 파지한 고객을 대상으로 예측된 이동 경로를 기반으로, 근접 비전 센서에 의해 촬영된 영상으로부터 식별 되지 않은 상품을 추정할 수 있다. 그리고, 상품 관리부는 운반 기구에 투입된 것으로 식별된 모든 상품의 리스트와, 근접 비전 센서에 의해 촬영된 영상으로부터 식별되지 않은 상품의 리스트를 안내 출력 장치를 통해 출력할 수 있다.본 발명의 다른 실시예에 따르면, 상품 관리부는 원거리 비전 센서를 이용하여, 고객이 구매할 것으 로 예상되는 상품들을 추적할 수 있다. 이와 같은 실시예는 고객이 소량의 상품을 구매하기 위하여 운반 기구 를 이용하여 않고, 손으로 직접 상품을 운반할 때 적합할 수 있다. 구체적으로, 상품 관리부는 고객 관리부에 의해 식별된 고객의 행동 패턴을 기반으로, 해당 고객이 상품을 파지하는 것으로 판단되는 경우, 해당 고객의 행동 패턴을 기반으로 해당 고객이 파지한 상품을 식별할 수 있다. 예를 들어, 상품 관리부는 고객의 매장 내 상대적인 위치, 고객의 신체 자세 및 고객의 시선 방 향을 기초로, 해당 고객이 파지한 상품을 식별할 수 있다. 그리고, 상품 관리부는 고객이 파지한 것으로 식별된 상품에 매칭된 속성에, 해당 상품을 파지한 고객에게 할당된 트래킹 아이디를 추가할 수 있다. 이와 같이, 고객이 파지한 상품을 식별하는 단계를 일 회 이상 수행된 이후, 상품 관리부는 고객이 파지한 모든 상품의 가격을 합산하여 결제 금액을 산출할 수 있다. 상품 관리부는 상품의 속성에 동일한 트래킹 아이디가 추가된 하나 이상의 상품의 가격을 합산하여, 고객 에 대한 결제 예상 금액을 추정할 수 있다. 그리고, 상품 관리부는 트래킹 아이디를 기초로 추정된 결제 예상 금액과 고객이 파지한 모든 상품의 가격을 합산하여 산출된 결제 금액을 서로 대비할 수 있다. 상품 관리 부는 결제 예상 금액과 산출된 결제 금액을 서로 대비한 결과를 기초로, 고객의 부정 행위 여부를 검증할 수 있다. 또한, 상품 관리부는 고객 관리부에 의해 식별된 고객의 행동 패턴을 기반으로, 해당 고객이 기 파지 한 상품을 내려놓는 것으로 판단되는 경우, 고객이 내려놓은 상품을 대상으로 사전에 지정된 원위치와 고객의 매장 내 상대적인 위치가 서로 매칭되지 않은 경우, 고객이 내려놓은 상품을 추가 관리 대상으로 지정할 수76_ 도 있다. 다음 구성으로, 프로세스 제어부는 고객이 구매하고자 하는 상품을 결제하기 위한 결제 프로세스를 결정하 고, 결정된 결제 프로세스를 기반으로 무인 결제 장치의 동작을 제어할 수 있다. 구체적으로, 프로세스 제어부는 고객 관리부를 통해 할당 및 식별된 고객의 트래킹 아이디 및 고객의 행동 패턴, 상품 관리부에 의해 산출된 결제 금액을 기반으로, 해당 고객에 대하여 매장 내에 고정 설치된 무인 결제 장치가 수행해야할 결제 프로세스를 결정할 수 있다. 이 경우, 결제 프로세스는 상품의 결제가 시작된 후 완료되기 까지의 무인 결제 장치가 수행해야 할 동작 들이 개별적으로 정의된 복수 개의 과정들을 포함한다. 이와 같은, 결제 프로세스는 결제 시작 과정, 상품 이송 과정, 상품 식별 과정, 금액 산출 과정, 결제 수행 과정 및 상품 배출 과정을 포함하여 구성될 수 있다. 그리고, 결제 프로세스에 포함된 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정되어 있다. 보다 상세하게, 결제 시작 과정은 무인 결제 장치 상의 일 측(즉, 상품 투입 영역)에 하나 이상의 상품이 적재된 상태에서 무인 결제 장치가 수행해야할 하나 이상의 동작이 정의된 과정이다. 상품 이송 과정은 무인 결제 장치 상에 적재된 하나 이상의 상품을 무인 결제 장치상의 중앙(즉, 상 품 식별 영역)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 상품 식별 과정은 이송된 하나 이상의 상품을 깊이 카메라(depth camera) 등을 이용하여 개별적으로 식별하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 금액 산출 과정은 개별적으로 식별된 상품의 가격을 합산하여 결제 금액을 산출하기 위하여, 무인 결제 장치 가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 결제 수행 과정은 고객에 대응하여 사전에 지정된 지불 수단을 이용하여, 산출된 결제 금액에 대한 결제를 수행 하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 그리고, 상품 배출 과정은 결제가 완료된 이후 하나 이상의 상품을 무인 결제 장치의 타 측(즉, 상품 배출 영역)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 한편, 프로세스 제어부는 고객의 행동 패턴 등을 기반으로 결정된 결제 프로세스와, 해당 고객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여, 고객의 행동 패턴 등을 기반으로 결정된 결제 프로세스가 올바르게 결정되었는지 검증할 수 있다.예를 들어, 프로세스 제어부는 선행 결제 프로세스의 과정과 고객의 행동 패턴에 기반하여 결정된 결제 프 로세스의 과정에 각각 설정된 진행 순서가 연속적인지 여부를 판단하여, 고객의 행동 패턴을 기반으로 결정된 결제 프로세스가 올바르게 결정되었는지 검증할 수 있다. 또한, 프로세스 제어부는 고객의 행동 패턴을 기반으로 결정된 결제 프로세스의 과정 이후에 진행 가능한 하나 이상의 후행 결제 프로세스의 과정을 식별할 수 있다. 그리고, 프로세스 제어부는 식별된 하나 이상 의 후행 결제 프로세스의 과정에 대응하는 그룹에 가중치를 부여할 수 있다. 이와 같은, 가중치는 고객 관리부 가 고객의 행동 패턴에 기반하여 후속으로 식별할 그룹이 후행 결제 프로세스에 부합되76_도록 유76_도하 는 역할을 수행할 수 있다. 그리고, 프로세스 제어부는 결정된 결제 프로세스에 대응하여 사전에 설정된 명령을 무인 결제 장치 및 안내 출력 장치에 전송할 수 있다. 예를 들어, 결제 시작 과정에 있어서, 프로세스 제어부는 원거리 비전 센서를 통해 촬영된 영상을 기 반으로 할당된 고객의 트래킹 아이디와, 무인 결제 장치의 근거리 비전 센서를 통해 촬영된 영상으로부터 할당될 수 있는 트래킹 아이디가 서로 일치하는지 검증할 수 있다. 결제 시작 과정에 있어서, 프로세스 제어부는 고객 관리부에 의해 식별된 고객의 행동 패턴을 기반으 로 해당 고객이 매장으로부터 이탈할 것으로 판단되는 경우, 고객에 대응하여 사전에 지정된 지불 수단을 이용 하여, 상품 관리부에 의해 산출된 결제 금액에 대한 결제를 무인 결제 장치를 통해 수행할 수 있다. 이와 같은, 결제를 시76_도하였으나 실패한 경우, 프로세스 제어부는 결제 실패와 관련하여 사전에 지정된 음성 메시지를 안내 출력 장치를 통해 출력할 수 있다. 상품 식별 과정에 있어서, 프로세스 제어부는 무인 결제 장치 상에 적재된 하나 이상의 상품 중 전부 또는 일부 상품이 식별되지 않은 경우, 식별되지 않은 전부 또는 일부 상품에 관한 영상을 안내 출력 장치(50 0)를 통해 출력할 수 있다. 이하, 상술한 바와 같은 결제 제어 서버의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구체 적으로 설명한다. 76_도 4는 본 발명의 일 실시예에 따른 결제 제어 서버의 하드웨어 구성76_도이다. 76_도 4에 76_도시된 바와 같이, 결제 제어 서버는 프로세서(Processor, 150), 메모리(Memory, 155), 송 수신기(Transceiver, 160), 입출력장치(Input/output device, 165), 데이터 버스(Bus, 170) 및 스토리지 (Storage, 175)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(180a)에 따른 명 령어를 기초로, 결제 제어 서버의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(180a)가 상주(loading)될 수 있다. 송수신기는 원거리 비전 센서, 운 반 기구, 무인 결제 장치 및 안내 출력 장치와 데이터를 송수신할 수 있다. 입출력장치는 결제 제어 서버의 동작에 필요한 데이터를 입력 받고, 연산 결과를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이 션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스 (resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어 (180b)를 저장할 수 있다. 또한, 스토리지는 본 발명의 실시예들에 따른 방법의 수행에 필요한 정보들을 저장된 데이터베이스를 더 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 결제 프로세스 제어를 위한 소프트웨어(180a, 180b)는 프로세서가 원거리 비전 센서에 의해 촬영된 영상을 시계열적으로 분 석하여 고객의 행동 패턴을 식별하는 단계, 및 프로세서가 식별된 고객의 행동 패턴을 기반으로 매장 내에 고정 설치된 무인 결제 장치가 수행해야할 결제 프로세스를 결정하는 단계를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다.본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 고객 추적을 위한 소 프트웨어(180a, 180b)는 프로세서가 원거리 비전 센서에 의해 촬영된 영상을 시계열적으로 분석하여 고객을 상기 매장 내에 기 방문한 다른 고객과 구별하기 위한 트래킹 아이디를 할당한 후, 고객에 매칭된 속성 에 할당된 트래킹 아이디를 추가하는 단계, 및 프로세서가 할당된 트래킹 아이디를 기반으로 고객에 대한 결제 프로세스를 결정하는 단계를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 무인 결제를 위한 소 프트웨어(180a, 180b)는 프로세서가 운반 기구를 구성하고 있는 근접 비전 센서에 의해 운반 기구 의 내부 공간이 촬영된 영상을 수신하는 단계, 상기 프로세서가 수신된 영상과 운반 기구로부터 기존에 수신된 이전 영상을 서로 대비하여 운반 기구의 내부에 새롭게 투입된 하나 이상의 상품을 식별하 는 단계, 및 영상을 수신하는 단계 및 상기 새롭게 투입된 상품을 식별하는 단계가 일 회 이상 수행된 이후, 프 로세서가 운반 기구의 내부에 투입된 것으로 식별된 모든 상품의 가격을 합산하여 결제 금액을 산출 하는 단계를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 자동 결제를 위한 소프트웨어(180a, 180b)는 프로세서가 원거리 비전 센서에 의해 촬영된 영상을 시계열적으로 분석하 여 고객의 행동 패턴을 식별하는 단계, 및 식별된 고객의 행동 패턴을 기반으로 고객이 상품을 파지하는 것으로 판단되는 경우 고객이 파지한 상품을 식별하는 단계, 및 고객이 파지한 모든 상품의 가격을 합산하여 결제 금액 을 산출하는 단계를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. 76_도 4에 76_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그 것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이 상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수76_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하76_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드 를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작 동하76_도록 구성될 수 있으며, 그 역76_도 마찬가지이다.이하, 상술한 바와 같은 본 발명의 다양한 실시예에 따른 무인 결제 시스템의 특징에 대하여, 76_도면을 참조하 여 구체적으로 설명하기로 한다. 76_도 5는 본 발명의 일 실시예에 따라 식별되는 고객의 행동을 설명하기 위한 예시76_도이다. 76_도 5에 76_도시된 바와 같이, 본 발명의 일 실시예에 따른 무인 결제 시스템의 결제 제어 서버는 원거 리 비전 센서 등에 의해 촬영된 고객에 대한 영상을 시계열적으로 분석하여, 고객의 행동 패턴을 식별할 수 있다. 구체적으로, 결제 제어 서버는 원거리 비전 센서 등으로부터 하나 이상의 고객에 대한 영상을 수 신할 수 있다. 결제 제어 서버는 사전에 기계 학습된 분류기(classifier)를 이용하여, 사전에 정의된 행동 패턴 그룹들 중에서 고객의 행동 패턴에 대응하는 하나의 그룹을 식별할 수 있다. 여기서, 고객의 행동 패턴에는 고객의 매장 내 상대적인 위치, 고객의 매장 내 이동 경로, 고객의 신체 자세, 고객의 시선 방향 및 고객의 상품 파지 유무를 포함할 수 있으나, 이에 한정되는 것은 아니 다. 이를 위하여, 결제 제어 서버는 원거리 비전 센서 등으로부터 수신된 영상으로부터 고객의 매장 내 상대적인 위치를 식별할 수 있다. 다음으로, 결제 제어 서버는 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신체 부위들을 식별 하고, 식별된 신체 부위들 각각의 상대적인 위치 및 방향성을 기초로 신체 부위들을 서로 연결하여 스켈레톤 을 생성할 수 있다. 결제 제어 서버는 생성된 스켈레톤의 형상 및 모양을 기초로, 고객의 신체 자세, 고객의 시선 방향, 고객의 상품 파지 유무를 식별할 수 있다. 그리고, 결제 제어 서버는 식별된, 고객의 매장 내 상대적인 위치, 고객의 매장 내 이동 경로, 고 객의 신체 자세, 고객의 시선 방향 및 고객의 상품 파지 유무를 분류기에 입력하고, 분류기로부터 출력된 확률 값에 기반하여 고객의 행동 패턴에 대응하는 그룹을 식별할 수 있다. 이 경우, 결제 제어 서버는 분류기로부터 출력된 그룹별 확률 값에 후행 결제 프로세스의 과정에 대응하여 사전에 부여된 가중치를 적용하여, 고객의 행동 패턴에 대응하는 그룹을 식별할 수76_도 있다. 76_도 6은 본 발명의 일 실시예에 따른 구성요소가 배치된 매장을 설명하기 위한 예시76_도이다. 76_도 6에 76_도시된 바와 같이, 본 발명의 일 실시예에 따른 무인 결제 시스템은 결제 제어 서버 외에76_ 도, 매장 내에 고정 설치된 하나 이상의 원거리 비전 센서, 무인 결제 장치 및 안내 출력 장치 를 필수적 구성으로 포함하고 있다. 보다 구체적으로, 원거리 비전 센서는 매장 내부의 일 영역을 촬영할 수 있76_도록 매장 내에 고정 설치된 하나 이상의 비전 센서이다. 이와 같은, 원거리 비전 센서는 단수 또는 복수 개로 구성될 수 있으며, 폐쇄 회로 텔레비전(CCTV), 카메 라, 레이더 또는 라이다 중 어느 하나가 될 수 있으며, 이에 한정되는 것은 아니다. 원거리 비전 센서는 결제 제어 서버에 의해 설정된 촬영 주기마다 매장의 내부 공간을 촬영할 수 있 다. 만약, 매장 내에 고객이 방문한 경우, 원거리 비전 센서는 매장 내에 방문한 고객에 대한 영상을 촬영 할 수 있다. 그리고, 원거리 비전 센서는 촬영된 하나 이상의 영상을 결제 제어 서버에 송신할 수 있 다. 이와 같은, 원거리 비전 센서가 복수 개의 비전 센서로 구성된 경우, 원거리 비전 센서는 동일한 고 객을 서로 다른 방향에서 복수 개의 영상을 제각각 촬영할 수76_도 있다. 또한, 원거리 비전 센서는 고객 또는 상품의 추적에 필요한 경우, 결제 제어 서버의 제어에 따라 틸 트, 스윙, 피봇 등을 수행하여 영상의 촬영 방향을 변경할 수76_도 있다. 무인 결제 장치는 매장 내의 일 영역에 설치되어, 사전에 지정된 상품 투입 영역에 적재된 하나 이상의 상 품을 대상으로, 결제 제어 서버가 결정한 결제 프로세스에 따라 결제를 수행할 수 있다. 이와 같은, 결제 프로세스는 결제 시작 과정, 상품 이송 과정, 상품 식별 과정, 금액 산출 과정, 결제 수행 과 정 및 상품 배출 과정을 포함하여 구성될 수 있다. 그리고, 안내 출력 장치는 매장 내의 일 영역에 설치되어, 상품의 결제 과정에서 고객에게 전달하고자 하 는 안내를 출력할 수 있다. 이와 같은, 안내 출력 장치는 안내를 출력하기 위한 디스플레이 및 스피커를 포함하여 구성될 수 있다. 이를 위하여, 안내 출력 장치는 결제 제어 서버로부터 영상 또는 음성을 수신할 수 있다. 그리고, 안 내 출력 장치는 수신된 영상 또는 음성을 출력할 수 있다. 예를 들어, 안내 출력 장치는 무인 결제 장치에 의해 하나 이상의 상품이 식별되지 않은 경우 결제 제어 서버의 제어에 따라, 인식되지 않은 전부 또는 일부의 상품에 관한 영상을 출력할 수 있다. 또한, 안 내 출력 장치는 결제 제어 서버의 제어에 따라, 식별되지 않은 상품의 리스트를 출력할 수 있다. 이와 다르게, 안내 출력 장치는 결제 제어 서버의 제어에 따라, 고객의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력하거나, 또는 무인 결제 장치의 사용법과 관련하여 사전에 지정된 음성 메시지를 출 력할 수76_도 있다. 76_도 7은 본 발명의 일 실시예에 따른 운반 기구를 설명하기 위한 예시76_도이다. 76_도 7에 76_도시된 바와 같이, 본 발명의 일 실시예에 따른 무인 결제 시스템은 운반 기구를 더 포함하 여 구성될 수 있다. 구체적으로, 운반 기구는 일 측이 개방된 바구니(basket) 또는 카트(cart) 형태를 가지며, 고객이 구매하 고자 하는 상품을 임시적으로 적재하여 운반하는데 사용될 수 있는 기구이다. 그리고, 운반 기구는 상품이 적재될 수 있는 내부 공간을 촬영할 수 있는 근접 비전 센서를 포함하여 구성될 수 있다. 일 실시예에 따르면, 운반 기구는 결제 제어 서버의 제어에 대응하여, 근접 비전 센서를 통해 운반 기구의 내부 공간을 촬영할 수 있다. 다른 실시예에 따르면, 운반 기구는 내부에 근접 센서(미76_도시)를 더 구비할 수 있다. 이 경우, 운반 기 구의 근접 비전 센서는 근접 센서를 통해 물체의 접근이 감지된 경우에만, 운반 기구의 내부 공 간을 촬영하76_도록 제어될 수 있다. 또 다른 실시 예에 따르면, 운반 기구는 내부에 3축 또는 6축 가속76_도 센서(미76_도시)를 더 구비할 수 있다. 이 경우, 운반 기구의 근접 비전 센서는 가속76_도 센서를 통해 진동이 감지된 경우에만, 운반 기구의 내부 공간을 촬영하76_도록 제어될 수 있다. 그리고, 운반 기구는 근접 비전 센서에 의해 촬영된 영상을 결제 제어 서버에 송신할 수 있다. 한편, 운반 기구는 내부에 적재될 수 복수 개의 상품들이 서로 이격된 상태로 투입되76_도록, 운반 기구 의 내부 공간을 복수 개의 세부 영역으로 구획할 수 있는 하나 이상의 파티션을 포함하여 구성될 수 있다. 이와 같은, 파티션은 고객이 운반 기구의 내부에 적재하고자 하는 상품의 부피에 따라 자유롭 게 그 위치가 변동될 수 있다. 이 경우, 운반 기구의 근접 비전 센서는 운반 기구를 구성하고 있는 하나 이상의 파티션의 위치가 외력에 의해 변동되는 경우에만, 운반 기구의 내부 공간을 촬영하76_도록 제어될 수 있다. 그리고, 운반 기구는 근접 비전 센서에 의해 촬영된 영상을 결제 제어 서버에 송신할 수 있다. 76_도 8은 본 발명의 일 실시예에 따른 무인 결제 장치를 설명하기 위한 예시76_도이다. 그리고, 76_도 9는 본 발명의 일 실시예에 따른 결제 프로세스의 과정들을 설명하기 위한 예시76_도이다. 우선, 76_도 8에 76_도시된 바와 같이, 본 발명의 일 실시예에 따른 무인 결제 시스템의 무인 결제 장치의 상면은 고객이 구매하고자 하는 하나 이상의 상품을 적재할 수 있는 상품 투입 영역(area 1), 하나 이상의 상품 이 개별적으로 식별될 수 있는 상품 식별 영역(area 2), 및 고객이 결제가 완료된 하나 이상의 상품을 픽업할수 있는 상품 배출 영역(area 3)으로 구분될 수 있다. 구체적으로, 무인 결제 장치는 결제 제어 서버가 자체적으로 결정한 결제 프로세스에 따른 명령을 수 신할 수 있다. 그리고, 무인 결제 장치는 결제 제어 서버로부터 수신된 명령에 따라 동작을 수행할 수 있다. 76_도 9에 76_도시된 바와 같이, 결제 프로세스는 상품의 결제가 시작된 후 완료되기 까지의 무인 결제 장치 가 수행해야 할 동작들이 개별적으로 정의된 복수 개의 과정들을 포함한다. 이와 같은, 결제 프로세스는 결제 시작 과정, 상품 이송 과정, 상품 식별 과정, 금액 산출 과정, 결제 수행 과정 및 상품 배출 과정을 포함 하여 구성될 수 있다. 그리고, 결제 프로세스에 포함된 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정되어 있다. 보다 상세하게, 결제 시작 과정(S10)은 무인 결제 장치 상의 상품 투입 영역(area 1)에 하나 이상의 상품 이 적재된 상태에서 무인 결제 장치가 수행해야할 하나 이상의 동작이 정의된 과정이다. 상품 이송 과정(S20)은 무인 결제 장치 상에 적재된 하나 이상의 상품을 무인 결제 장치상의 상품 식 별 영역(area 2)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이 다. 상품 식별 과정(S30)은 이송된 하나 이상의 상품을 깊이 카메라 등을 이용하여 개별적으로 식별하기 위하여, 무 인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 금액 산출 과정(S40)은 개별적으로 식별된 상품의 가격을 합산하여 결제 금액을 산출하기 위하여, 무인 결제 장 치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 결제 수행 과정(S50)은 고객에 대응하여 사전에 지정된 지불 수단을 이용하여, 산출된 결제 금액에 대한 결제를 수행하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 그리고, 상품 배출 과정(S60)은 결제가 완료된 이후 하나 이상의 상품을 무인 결제 장치의 상품 배출 영역 (area 3)으로 이송하기 위하여, 무인 결제 장치가 수행해야 할 하나 이상의 동작이 정의된 과정이다. 이와 같이, 무인 결제 장치의 동작이 개별적으로 정의된 결제 프로세스는 결제 제어 서버가 고객의 행동 패턴에 기반하여 결정할 수 있다. 그리고, 결제 제어 서버가 고객의 행동 패턴에 기반하여 결정한 결 제 프로세스는 해당 고객에 대응하여 기 결정되었던 선행 결제 프로세스와 대비되어 올바르게 결정되었는지 검 증될 수 있다. 예를 들어, 고객의 행동 패턴에 기반하여 결정된 결제 프로세스가 상품 배출 과정(S60)이나 해당 고객에 대응하 여 기 결정되었던 선행 결제 프로세스가 금액 산출 과정(S40)인 경우, 상품 배출 과정(S60)이 금액 산출 과정 (S40)의 연속적인 과정에 해당하지 아니므로, 결제 제어 서버는 고객의 행동 패턴에 기반하여 결정된 상품 배출 과정(S60)의 결정에 오류가 존재한 것으로 판단할 수 있다. 이러한 오류를 최소화하기 위하여, 결제 제어 서버는 고객의 행동 패턴을 기반으로 기 결정된 결제 프로세 스 이후에 진행 가능한 하나 이상의 후행 결제 프로세스의 과정을 식별하고, 식별된 하나 이상의 후행 결제 프 로세스의 과정에 대응하는 그룹에 가중치를 부여할 수 있다. 그리고, 결제 제어 서버는 고객의 후속 행동 패턴을 식별함에 있어, 분류기로부터 분류된 그룹별 확률 값에 기 부여된 가중치를 적용하여, 고객의 행동 패턴 에 대응하는 그룹을 식별할 수 있다. 한편, 무인 결제 장치는 인접한 사람(즉, 고객) 또는 물체(즉, 상품 등)을 촬영할 수 있는 근거리 비전 센 서를 하나 이상 포함하여 구성될 수 있다. 이와 같은, 근거리 비전 센서는 카메라, 레이더 또는 라이 다 중 어느 하나가 될 수 있으며, 이에 한정되는 것은 아니다. 무인 결제 장치의 근거리 비전 센서는 사람 또는 물체의 접근이 감지되거나 또는 결제 제어 서버 의 제어에 대응하여, 영상의 촬영을 수행할 수 있다. 그리고, 무인 결제 장치는 촬영된 영상을 결제 제어 서버에 전송할 수 있다. 이하, 상술한 바와 같은 본 발명의 다양한 실시예에 따른 무인 결제 시스템의 동작에 대하여, 76_도면을 참조하 여 구체적으로 설명하기로 한다.76_도 10은 본 발명의 일 실시예에 따른 무인 결제 방법을 설명하기 위한 순서76_도이다. 76_도 10을 참조하면, 본 발명의 일 실시예에 따른 무인 결제 시스템의 원거리 비전 센서, 근거리 비전 센 서 및 근접 비전 센서 중 하나 이상은 매장 내에 방문한 고객에 대한 영상을 촬영할 수 있다 (S100). 결제 제어 서버는 원거리 비전 센서, 근거리 비전 센서 및 근접 비전 센서 중 하나 이상이 촬영한 영상을 기초로, 고객을 관리하기 위한 트래킹 아이디 할당 및 고객의 행동 패턴을 식별하고, 상 품을 관리하여 결제 금액을 산출할 수 있다(S200). 이와 같은, 고객 관리 및 상품 관리를 위한 단계는 추후 76_ 도 10을 참조하여 보다 구체적으로 설명하기로 한다. 결제 제어 서버는 식별된 고객의 행동 패턴을 기반으로, 상품 결제가 시작되는지 판단할 수 있다(S300). 판단 결과, 상품 결제가 아직 시작되지 않은 경우, 결제 제어 서버는 고객 관리 및 상품 관리를 위한 단계 를 반복적으로 수행할 수 있다. 판단 결과, 상품 결제가 시작되는 경우, 결제 제어 서버는 고객에게 할당된 트래킹 아이디, 고객의 행 동 패턴, 산출된 결제 금액을 기반으로, 해당 고객에 대하여 매장 내에 고정 설치된 무인 결제 장치가 수행해야할 결제 프로세스를 결정할 수 있다(S400). 이 경우, 결제 프로세스는 상품의 결제가 시작된 후 완료되기 까지의 무인 결제 장치가 수행해야 할 동작 들이 개별적으로 정의된 복수 개의 과정들을 포함한다. 이와 같은, 결제 프로세스는 결제 시작 과정, 상품 이송 과정, 상품 식별 과정, 금액 산출 과정, 결제 수행 과정 및 상품 배출 과정을 포함하여 구성될 수 있다. 그리고, 결제 프로세스에 포함된 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정되어 있다. 이러한 결제 프로세스의 과정들의 진행 순서를 기초로, 결제 제어 서버는 고객의 행동 패턴 등을 기반으로 결정된 결제 프로세스와, 해당 고객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여, 고객의 행 동 패턴 등을 기반으로 결정된 결제 프로세스가 올바르게 결정되었는지 검증할 수76_도 있다. 그리고, 무인 결제 장치는 결제 프로세스에 따른 결제 제어 서버의 명령에 대응하여, 상품의 이송, 식별, 결제 및 배출 과정의 동작을 수행할 수 있다(S500) 76_도 11은 본 발명의 일 실시예에 따라 고객 및 상품을 관리하는 단계를 설명하기 위한 순서76_도이다. 76_도 11을 참조하면, 본 발명의 일 실시예에 따른 결제 제어 서버는 매장 내에 방문한 고객에 대 한 영상을 수신할 수 있다(S210). 결제 제어 서버는 수신된 영상을 시계열적으로 분석하여, 매장 내에 방문한 고객이 새롭게 등장한 고객에 해당되는지 판단할 수 있다(S220). 판단 결과, 영상 속의 고객이 매장 내에 기 방문한 고객에 해당되는 경우, 결제 제어 서버는 트래킹 아이디를 할당하기 위한 단계(S230)를 수행하지 않는다. 판단 결과, 영상 속의 고객이 새롭게 등장한 고객에 해당되는 경우, 결제 제어 서버는 매장 내에 방문한 고객에 대한 트래킹 아이디를 할당할 수 있다(S230). 여기서, 트래킹 아이디는 매장에 막 방문 한 고객을 매장 내에 기 방문한 다른 고객과 구별하기 위한 식별자이다. 보다 상세하게, 결제 제어 서버는 고객에 대한 영상으로부터 고객의 안면 영역을 식별할 수 있다. 결 제 제어 서버는 식별된 안면 영역에 포함된 복수 개의 랜드마크를 인식할 수 있다. 여기서, 랜드마크는 제 1 사람의 안면을 제2 사람의 안면과 구별할 수 있는 기준이 되는 신체 부위를 의미한다. 예를 들어, 랜드마크에 는 눈, 코 및 입이 포함될 수 있으나, 이에 한정되는 것은 아니다. 그리고, 결제 제어 서버는 인식된 복수 개의 랜드마크들의 배치 및 형성, 복수 개의 랜드마크들 사이의 거리를 기초로, 해당 고객에게 고유한 트래 킹 아이디를 할당할 수 있다. 결제 제어 서버는 영상을 시계열적으로 분석하여, 고객의 행동 패턴을 식별할 수 있다(S240). 보다 상세하게, 결제 제어 서버는 사전에 기계 학습된 분류기를 이용하여, 사전에 정의된 행동 패턴 그룹 들 중에서 고객의 행동 패턴에 대응하는 하나의 그룹을 식별할 수 있다. 여기서, 고객의 행동 패턴에는 고 객의 매장 내 상대적인 위치, 고객의 매장 내 이동 경로, 고객의 신체 자세, 고객의 시선 방향 및 고객의 상품 파지 유무를 포함할 수 있으나, 이에 한정되는 것은 아니다.이를 위하여, 결제 제어 서버는 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신체 부위들을 식 별하고, 식별된 신체 부위들 각각의 상대적인 위치 및 방향성을 기초로 신체 부위들을 서로 연결하여 스켈레톤 을 생성할 수 있다. 결제 제어 서버는 생성된 스켈레톤의 형상 및 모양을 기초로, 고객의 신체 자세, 고객의 시선 방향, 고객의 상품 파지 유무를 식별할 수 있다. 결제 제어 서버는 고객의 행동 패턴을 기반으로, 해당 고객이 상품을 파지하였는지 판단할 수 있 다(S250). 판단 결과, 고객이 상품을 파지하지 않은 것으로 판단되는 경우, 결제 제어 서버는 상품을 식별하고 결제 금액을 산출하는 단계(S260)를 수행하지 않는다. 판단 결과, 고객이 상품을 파지한 것으로 판단되는 경우, 결제 제어 서버는 고객의 행동 패턴을 기반으로 해당 고객이 파지한 상품을 식별할 수 있다. 예를 들어, 결제 제어 서버는 고객의 매장 내 상대적인 위치, 고객의 신체 자세 및 고객의 시선 방향을 기초로, 해당 고객이 파지한 상품을 식별할 수 있다. 그리고, 결제 제어 서버는 고객이 파지한 모든 상품의 가격을 합산하여 결제 금액을 산출할 수 있다(S260). 이상과 같이, 본 명세서와 76_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실시 예 외에76_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하는 기 술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 76_도면에서 특정 용어들이 사용되 었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한적으 로 해석되어서는 아니 되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석 에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. 76_부호의 설명 고객: 10 매장: 30 결제 제어 서버: 100 원거리 비전 센서: 200 운반 기구: 300 무인 결제 장치: 400 안내 출력 장치: 500 통신부: 105 입출력부: 110 저장부: 115 고객 관리부: 120 상품 관리부: 125 프로세스 제어부: 130 76_청구범위 76_청구항 1 매장 내에 방문한 고객에 대한 영상을 촬영하는 단계; 상기 촬영된 영상을 시계열적으로 분석하여, 상기 고객의 행동 패턴을 식별하는 단계; 상기 식별된 고객의 행동 패턴을 기반으로, 상기 매장 내에 고정 설치된 무인 결제 장치가 수행해야할 결제 프 로세스를 결정하는 단계; 및 상기 고객의 행동 패턴을 기반으로 결정된 결제 프로세스와 상기 고객에 대응하여 기 결정되었던 선행 결제 프 로세스를 서로 대비하여, 상기 결정된 결제 프로세스를 검증하는 단계를 포함하는, 결제 프로세스 제어 방법. 76_청구항 2 제1 항에 있어서, 상기 고객의 행동 패턴을 식별하는 단계는 사전에 학습된 분류기(classifier)를 이용하여 사전에 정의된 행동 패턴 그룹들 중 상기 고객의 행동 패턴에 대 응하는 하나의 그룹을 식별하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 3 제2 항에 있어서, 상기 고객의 행동 패턴에는 상기 고객의 상기 매장 내 상대적인 위치, 상기 고객의 상기 매장 내 이동 경로, 상기 고객의 신체 자세, 상기 고객의 시선 방향 및 상기 고객의 상품 파지 유무를 포함하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 4 제3 항에 있어서, 상기 고객의 행동 패턴을 식별하는 단계는 상기 고객에 대한 영상으로부터 사전에 지정된 복수 개의 신체 부위들을 식별하고, 상기 식별된 신체 부위들 각 각의 상대적인 위치 및 방향성을 기초로 상기 식별된 신체 부위들을 서로 연결하여 스켈레톤(skeleton)을 생성 하고, 생기 생성된 스켈레톤의 형상 및 모양을 기초로 상기 고객의 신체 자세, 상기 고객의 시선 방향 및 상기 고객의 상품 파지 유무를 식별하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 5 제4 항에 있어서, 상기 결제 프로세스는 상품의 결제가 시작된 후 완료되기까지 상기 무인 결제 장치가 수행해야할 동작들이 개별적으로 정의된 복수 개 의 과정들을 포함하고, 상기 복수 개의 과정들은 시계열적으로 진행 순서가 사전에 설정되어 있으며, 상기 결제 프로세스를 검증하는 단계는 상기 선행 결제 프로세스와 상기 결정된 결제 프로세스에 각각 설정된 진행 순서가 연속적인지 여부를 판단하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 6 제5 항에 있어서, 상기 결제 프로세스를 검증하는 단계는 상기 결정된 결제 프로세스 이후에 진행 가능한 하나 이상의 후행 결제 프로세스의 과정을 식별하고, 상기 식별 된 하나 이상의 후행 결제 프로세스의 과정에 대응하는 그룹에 가중치를 부여하고, 상기 고객의 행동 패턴을 식별하는 단계는 상기 분류기로부터 출력된 그룹별 확률 값에 상기 가중치를 적용하여, 상기 고객의 행동 패턴에 대응하는 그룹 을 식별하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 7 제6 항에 있어서, 상기 결제 프로세스는 상기 무인 결제 장치 상의 일 측에 하나 이상의 상품이 적재된 결제 시작 과정, 상기 무인 결제 장치 상에 적재 된 하나 이상의 상품을 이송하는 상품 이송 과정, 상기 이송된 하나 이상의 상품을 개별적으로 식별하는 상품 식별 과정, 상기 개별적으로 식별된 상품의 가격을 합산하여 결제 금액을 산출하는 금액 산출 과정, 고객에 대 응하여 사전에 지정된 지불 수단을 이용하여 상기 산출된 결제 금액에 대한 결제를 수행하는 결제 수행 과정 및 상기 결제가 완료된 이후 하나 이상의 상품을 상기 무인 결제 장치의 타 측으로 이송하는 상품 배출 과정을 포 함하는 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 8 제2 항에 있어서, 상기 고객의 행동 패턴을 식별하는 단계는 상기 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전에 정의된 부정 행위 그룹에 해당되는 경우, 고객 의 부정 행위와 관련하여 사전에 지정된 음성 메시지를 출력하되, 상기 부정 행위 그룹은 고객이 상기 무인 결제 장치 상에 복수 개의 상품을 서로 적층되게 얹어(put on) 하나 이상의 상품을 숨기는 행 위에 대응하는 그룹인 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 9 제2 항에 있어서, 상기 고객의 행동 패턴을 식별하는 단계는 상기 고객의 행동 패턴에 대응하여 식별된 하나의 그룹이 사전에 정의된 이용 경험 미숙 그룹에 해당되는 경우, 상기 무인 결제 장치의 사용법과 관련하여 사전에 지정된 음성 메시지를 출력하되, 상기 이용 경험 미숙 그룹은 상기 무인 결제 장치 상에 하나 이상의 상품이 적재된 이후, 사전에 지정된 시간 내에 결제 프로세스가 다음 단 계로 진행되지 않은 경우에 대응하는 그룹인 것을 특징으로 하는, 결제 프로세스 제어 방법. 76_청구항 10 매장 내에 고정 설치되어, 사전에 지정된 상품 투입 영역에 적재된 하나 이상의 상품을 대상으로, 결제 프로세 스에 따라 결제하는 무인 결제 장치; 상기 매장 내에 고정 설치되어, 상기 매장 내에 방문한 고객에 대한 영상을 촬영하는 원거리 비전 센서; 및 상기 원거리 비전 센서를 통해 촬영된 영상을 시계열적으로 분석하여 고객의 행동 패턴을 식별하고, 상기 식별 된 고객의 행동 패턴을 기반으로 상기 무인 결제 장치가 수행해야 할 상기 결제 프로세스를 결정하고, 상기 고 객의 행동 패턴을 기반으로 결정된 결제 프로세스와 상기 고객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여 상기 결정된 결제 프로세스를 검증하는 결제 제어 서버를 포함하는, 결제 프로세스 제어 시스템."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "76_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "76_요약 본 발명은 고객 행동에 기반하여 결제 프로세스를 제어할 수 있는 방법을 제안한다. 상기 방법은 매장 내에 방 문한 고객에 대한 영상을 촬영하는 단계; 상기 촬영된 영상을 시계열적으로 분석하여, 상기 고객의 행동 패턴을 식별하는 단계; 상기 식별된 고객의 행동 패턴을 기반으로, 상기 매장 내에 고정 설치된 무인 결제 장치가 수행 해야할 결제 프로세스를 결정하는 단계; 및 상기 고객의 행동 패턴을 기반으로 결정된 결제 프로세스와 상기 고 객에 대응하여 기 결정되었던 선행 결제 프로세스를 서로 대비하여, 상기 결정된 결제 프로세스를 검증하는 단 계를 포함할 수 있다. 이와 같은, 본 발명에 따르면, 매장 내에 결제를 수행하기 위한 인력이 존재하지 않아76_ 도, 매장에 방문한 고객의 행동에 기반하여 무인 결제를 수행할 수 있게 된다. 76_대표76_도 76_도 9 76_도면 76_도 1 76_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "76_도 3 76_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "76_도 5 76_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "76_도 7 76_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "76_도 9 76_도 10 76_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "A_01_발명의 설명 A_01_발명의 명칭 수어 텍스트 번역 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for translate sign language text, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 18, "content": "A_01_기술분야 본 발명은 언어 번역(language translation)에 관한 것이다. 보다 상세하게는, 높은 정확A_01_도로 자연어 (natural language)를 수어(sign language) 테스트(text)로 번역하기 위한, 수어 텍스트 번역 방법 및 이를 실 행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. A_01_발명의 배경이 되는 기술 수어(수화, sign language)는 소리로 하는 언어가 아닌 손짓을 이용하여 뜻을 전달할 수 있는 언어의 일종이다. 음성언어가 청각으로 이해되고 음성으로 표현되는 청각-음성 체계임에 반하여, 수어는 시각으로 이해되고 손운 동으로 표현되는 시각-운동 체계이다. 수어는 대부분 청각 장애인의 의사소통을 위해 사용된다. 이러한, 수어는 수지신호와 비수지신호로 구성되어 있다. 수지신호는 수위(손의 위치), 수형(손의 모양), 수동 (손의 움직임) 등이 있다. 비수지신호는 얼굴의 표정과 머리와 몸의 움직임 등이 있으며, 놀람, 공포, 기쁨, 증 오, 행복, 슬픔, 혐오, 비웃음 등의 감정을 나타낼 수 있다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_01_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다. 그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_01_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. A_01_선행기술문헌 A_01_특허문헌 대한민국 등록특허공보 제10-1915088호, ‘수화번역장치’, (2018.10.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 19, "content": "A_01_발명의 내용 A_01_해결하고자 하는 과제 본 발명의 일 목적은 높은 정확A_01_도로 자연어(natural language)를 수어(sign language) 텍스트(text)로 번 역하기 위한, 수어 텍스트 번역 방법을 제공하는 것이다. 본 발명의 다른 목적은 높은 정확A_01_도로 자연어를 수어 텍스트로 번역하기 위한, 수어 텍스트 번역 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 20, "content": "A_01_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 높은 정확A_01_도로 자연어를 수어 텍스트로 번역 하기 위한, 수어 텍스트 번역 방법을 제안한다. 상기 방법은 번역서버가, 자연어 텍스트(text)를 입력 받는 단 계, 상기 번역서버가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터 (vector)를 생성하는 단계 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 벡터를 디코 딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계를 포함한다. 구체적으로, 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성하는 단 계 및 자연어 문장으로 사전 기계 학습된 인공지능을 통해 상기 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터를 생성하는 단계를 포함하는 것을 특징으로 한다. 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어 및 상기 각 단어의 언어 자질을 토큰화 한 제2 토 큰을 생성하는 단계 및 상기 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성하는 단계를 포함하는 것을 특징 으로 한다. 상기 벡터를 생성하는 단계는 상기 제1 컨텍스트 벡터 및 상기 제2 컨텍스트 벡터를 합성한 혼합 특징 벡터 (mixed feature vector)를 생성하고, 상기 생성된 혼합 특징 벡터를 상기 수어 텍스트를 생성하기 위한 인공지 능에 입력하는 것을 특징으로 한다. 상기 제1 토큰을 생성하는 단계 및 상기 제2 토큰을 생성하는 단계 이전에 상기 자연어 텍스트 중 적어A_01_도 둘 이상의 의미를 갖는 단어를 검출하고, 상기 검출된 단어를 의미 단위로 띄어쓰기 처리하는 것을 특징으로 한 다. 상기 제2 토큰을 생성하는 단계는 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 상기 자연어 텍스트를 임베딩(embedding)하여 상기 제2 토큰을 생성하는 것을 특 징으로 한다. 상기 수어 텍스트를 생성하는 단계는 상기 번역서버가, 상기 자연어 텍스트와 대응하는 혼합 특징 벡터를 입력 받는 단계, 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습된 인공 지능을 통해, 상기 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하는 단계 및 상기 추출된 수어 토큰을 기초로 수어 텍스트를 생성하는 단계를 포함하는 것을 특징으로 한다.상기 수어 토큰을 추출하는 단계는 상기 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 상기 혼합 특징 벡터에 포함된 토큰 중 하나로 대체하 는 것을 특징으로 한다. 상기 수어 토큰을 추출하는 단계는 상기 혼합 특징 벡터를 기초로 상기 자연어 텍스트의 문장 유형을 추정하고, 상기 추정된 문장 유형에 따른 비수지기호를 추출하고, 상기 추출된 비수지기호를 상기 수어 토큰에 임베딩하는 것을 특징으로 한다. 상기 수어 토큰을 추출하는 단계는 상기 추정된 문장 유형에 따라 상기 수어 텍스트를 수어로 동작하는데 따른 속A_01_도 지수를 A_01_도출하고, 상기 A_01_도출된 속A_01_도 지수를 상기 수어 토큰에 임베딩하고, 상기 수어 텍스트를 생성하는 단계는 상기 속A_01_도 지수를 나타내는 문자를 상기 수어 텍스트에 포함시키는 것을 특징으 로 한다. 상기 수어 토큰을 추출하는 단계는 상기 혼합 특징 벡터에 포함된 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 상기 문장 유형을 식별하고, 상기 식별된 문장 유형을 기초로 상기 속A_01_도 지수를 결 정하는 것을 특징으로 한다. 상기 문장 유형은 평소문, 의무문, 명령문, 청유문 및 감탄문 중 적어A_01_도 하나를 포함하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 수어 텍스트 번역 방법을 실행하기 위하여 기록매 체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있 다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가 자연어 텍스트(text)를 입력 받는 단계, 상기 프로세서가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성 하는 단계 및 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_01_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 21, "content": "A_01_발명의 효과 본 발명의 실시 예들에 따르면, 자연어 및 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 높은 정확A_01_도로 자연어를 수어 텍스트로 변 환할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 22, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 23, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 24, "content": "A_01_도면의 간단한 설명 A_01_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_01_도이다. A_01_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_01_도이다. A_01_도 3은 본 발명의 일 실시예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_01_도이다. A_01_도 4 및 A_01_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_01_도이다. A_01_도 6은 본 발명의 일 실시예에 따른 수어텍스트생성부의 기능을 설명하기 위한 예시A_01_도이다. A_01_도 7은 본 발명의 일 실시예에 따른 제2 인공지능을 설명하기 위한 예시A_01_도이다. A_01_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_01_도이다. A_01_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_01_도이다. A_01_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_01_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 25, "content": "A_01_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_01_도이다. A_01_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_01_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_01_도하게 포괄적인 의미로 해석되거나, 과A_01_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_01_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, '구성된다' 또는 '가지다' 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_01_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_01_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 '연결되어' 있다거나 '접속되어' 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_01_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_01_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"'직접 연결되어' 있다거나 '직접 접속되어' 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_01_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_01_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_01_도면은 본 발명의 사상을 쉽게 이해할 수 있A_01_도록 하기 위한 것일 뿐, 첨부된 A_01_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_01_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_01_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여 를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_01_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다. 그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_01_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. 이러한 한계를 극복하고자, 본 발명은 높은 정확A_01_도로 자연어(natural language)를 수어(sign language)로 번역할 수 있는 다양한 수단들을 제안하고자 한다. A_01_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_01_도이다. A_01_도 1을 참조하면, 본 발명의 일 실시예에 따른 수어변역시스템은 적어A_01_도 하나의 단말기(terminal, 100a, 100b, 100c, …, 100n; 100) 및 번역서버를 포함하여 구성될 수 있다.이와 같은, 본 발명의 일 실시예에 따른 수어번역시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나 타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 단말기는 사용자로부터 자연어(natural language) 텍스트를 입력 받 거나, 번역서버에 의해 번역된 수어(sign language) 텍스트(text) 또는 수어 영상(video)을 출력하여 사용 자에게 제공할 수 있는 장치이다. 여기서, 자연어는 인간이 일상생활에서 의사 소통을 위해 사용하는 언어가 될 수 있다. 특히, 자연어는 한국어, 영어, 독일어, 스페인어, 프랑스어, 이탈리아어 등 다양한 국가의 언어가 해당될 수 있다. 구체적으로, 자연어 는 각 국가의 언어 중에서A_01_도 구어체(colloquial style), 문어체(literary style) 등이 해당될 수 있다. 이러한, 단말기는 사용자로부터 자연어를 입력 받기 위한 입력 장치(input device) 및 번역 서버에 의해 번역된 수어 텍스트 또는 수어 영상을 출력하기 위한 출력 장치(output device)를 포함하여 구성될 수 있 다. 또한, 단말기는 번역서버를 포함한 다른 장치들과 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_01_도 허용될 수 있다. 예를 들어, 단말기는 3GPP(3rd Generation Partnership Project)에서 규정하고 있는 사용자 장치(User Equipment, UE) 및 IEEE(Institute of Electrical and Electronics Engineers)에서 규정하고 있는 모바일 스테이션(Mobile Station, MS) 중 어느 하나에 해당될 수 있다. 그러나 이에 한정되지 아니하고, 단말기는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버 (server)와 같은 고정식 컴퓨팅 장치, 또는 랩탑(laptop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디 어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이동식 컴퓨팅 장치 중 어느 하나가 될 수A_01_도 있다. 다음 구성으로, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 수 어 텍스트 및 수어 영상 중 적어A_01_도 하나로 번역하여, 번역된 수어 텍스트 및 수어 영상 중 적어A_01_도 하 나를 단말기에 제공할 수 있는 장치가 될 수 있다. 이러한, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 인코딩 (incoding)하여, 자연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력된 자연어 텍스트를 토큰화(tokenization)하고, 토큰화 작 업 전후에 자연어 텍스트를 용A_01_도에 맞게 정제(cleaning) 및 정규화(normalization)하여 전처리하고, 전처 리 된 토큰들을 압축해서 하나의 벡터로 만들 수 있다. 또한, 번역 서버는 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습 (machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성하여 단말기에 제공할 수 있다. 그리고, 번역 서버는 번역된 수어 텍스트를 수어 영상으로 생성하여, 단말기에 제공할 수 있다. 이와 같은, 번역서버는 단말기와 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_01_도 허용될 수 있다. 예를 들어, 번역서버는 데스크탑, 워크스 테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 이러한, 특징을 가지는 번역서버의 구체적인 구성 및 동작에 대해서는 A_01_도 2 내지 A_01_도 7을 참조하 여 후술하기로 한다. 지금까지 상술한 바와 같은, 수어번역시스템을 구성하는 단말기 및 번역서버는 장치들 사이를 직 접 연결하는 보안 회선, 공용 유선 통신망 또는 이동통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터 를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC) 및 광가입자망(Fiber To The Home, FTTH) 중 하나 이상이 포함될 수 있으 나, 이에 한정되는 것은 아니다. 또한, 이동통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다 중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE) 및 5세대 이동통신(5th generation mobile telecommunication) 중 하나 이상이 포함될 수 있 으나, 이에 한정되는 것A_01_도 아니다. 이하, 상술한 바와 같은 특징을 가지는, 번역서버의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_01_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_01_도이고, A_01_도 3은 본 발명의 일 실시 예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_01_도이고, A_01_도 4 및 A_01_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_01_도이고, A_01_도 6은 본 발명의 일 실시예에 따른 수어 텍스트생성부의 기능을 설명하기 위한 예시A_01_도이고, A_01_도 7은 본 발명의 일 실시예에 따른 제2 인공지능 을 설명하기 위한 예시A_01_도이다. 우선적으로, A_01_도 2를 참조하면, 본 발명의 일 실시예에 따른 번역 서버는 통신부, 입출력부 , 저장부, 데이터전처리부, 수어텍스트생성부 및 수어영상생성부를 포함하여 구성될 수 있다. 이와 같은, 본 발명의 일 실시예에 따른 번역서버의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각 구성 요소에 대하여 설명하면, 통신부는 단말기와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 단말기로부터 자연어 텍스트를 입력 받을 수 있고, 입력 받은 자연어 텍스트를 번역한 수어 텍스트 및 수어 영상 중 적어A_01_도 하나를 단말기로 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해, 관리자로부터 명령을 입력 받거나 또는 연산 결과를 출력할 수 있다. 이 경우, 관리자는 번역 서비스를 제공하는 서비스 제공자로 지칭될 수 있으며, 이에 한정되지 않는다. 구체적으로, 입출력부는 관리자로부터 인공지능을 학습하기 위한 데이터 셋을 입력 받을 수 있다. 예를 들 어, 입출력부는 제1 인공지능의 학습을 위하여, 다양한 형태의 자연어 문장에 관한 데이터 셋을 입력 받을 수 있다. 또한, 입출력부는 제2 인공지능의 학습을 위하여, 자연어 및 자연어와 매칭되는 수어 데이터 셋 을 입력 받을 수 있다. 또한, 입출력부는 데이터전처리부로부터 생성된 결과 값, 수어텍스트생성부로부터 생성된 수어 텍스트 및 수어영상생성부로부터 생성된 수어 영상 중 적어A_01_도 하나를 출력할 수 있다. 다음 구성으로, 저장부는 번역서버의 동작에 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 데이터전처리부, 수어텍스트생성부 및 수어영상생성부에 의해 주기 적으로 갱신되는 데이터베이스를 저장할 수 있다. 또한, 저장부는 인공지능(AI) 학습을 위한 데이터 셋을 저장할 수 있다. 그리고, 저장부는 데이터전처리부, 수어텍스트생성부 및 수어영상생성부 에서 사용되는 인공지능 모델을 저장할 수 있다. 다음 구성으로, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자 연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 즉, 데이터전처리부는 수어텍스트생성부가 수어 텍스트를 생성하기 위하여, 입력 받은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 구체적으로, 데이터전처리부는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 예를 들 어, A_01_도 3에 A_01_도시된 바와 같이, 데이터전처리부는 \"죽음기와 발명품이 잔뜩 우리 아이들과의 과 학 놀이터\"라는 문장을 입력 받은 경우, 문장에 포함된 각 단어를 토큰화(tokenization)하여 \"a1, a2, 쪋, a7\" 과 같은 제1 토큰을 생성할 수 있다. 이때, 데이터전처리부는 인공지능 성능을 향상시키기 위하여, 제1 토 큰 생성 이전에, 자연어 텍스트 중 적어A_01_도 둘 이상의 의미를 갖는 단어를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다. 또한, 데이터전처리부는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩 (contextual embedding)을 수행할 수 있다. 즉, 데이터전처리부는 자연어 문장으로 사전 기계 학습된 인공 지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 예를 들어, 데이터전처리부는 자연어 텍스트를 토큰화하여 생성된 \"a1, a2, 쪋, a7\"과 같은 제1 토큰을 제 1 인공지능에 입력하여 \"h1, h2, 쪋, h7\"를 포함하는 제1 컨텍스트 벡터를 생성할 수 있다. 이때, 제1 컨텍스트 벡터는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 예를 들어, 데이터전처리부는 BERT(Bidirectional Encoder Representations from Transformers) 모델에 기반한 인공지능(AI)을 이용하여, 제1 컨텍스트 벡터를 생성할 수 있다. 보다 상세하게 A_01_도 4 및 A_01_도 5를 참조하면, BERT 모델은 트랜스포머(transformer)를 기반으로, 인코더 (encoder)만을 사용하는 모델에 해당된다. BERT 모델은 일반적인 트랜스포머와 다르게, 토큰 임베딩(token embeddings), 토큰의 포지션 임베딩(position embeddings) 및 세그먼트 임베딩(segment embedding)으로 이루어 진 입력 값을 가진다. 이러한, BERT 모델은 복수 개의 인코딩 블록으로 구성될 수 있다. 기본 BERT 모델은 12개의 인코딩 블록으로 구 성되고, 대형 BERT 모델은 24개의 인코딩 블록으로 구성될 수 있으나, 이에 한정되는 것은 아니다. 각각의 인코 더 블록은 이전의 출력 값을 현재의 입력 값으로 가지며, BERT 모델은 인코더 블록의 개수만큼 재귀적으로 반복 처리되는 형태로 복수 개의 인코더들이 구성될 수 있다. 그리고, 각각의 인코더 블록의 출력 값은 매번 잔차 연 결(residual connections)되게 처리될 수 있다. 각 인코더 블록을 구성하는 멀티 헤드 어텐션(multi-head attention)은 다음의 수식 1과 같이, 서로 다른 가중 치 행렬(weight matrix)를 이용하여 어텐션(attention)을 h번 계산한 다음 이를 서로 연결(concatenates)한 결 과를 출력할 수 있다. [수식 1] MultiHead(Q, K, V) = [head1; …; headh]wO 여기서, headi는 Attention(QWiQ, KWiK, VWiV)4이다. Q는 디코더의 히든 스테이지(hidden stage), K는 인코더의 히든 스테이지, V는 K에 어텐션을 부여받은 정규화된 가중치(normalized weight)이며, Q, K, V에 대한 스케일드 닷-프로덕트 어텐션(scaled dot-product attention)은 다음의 수식 2를 통해 산출될 수 있다. [수식 2] Attention(Q, K, V) = softmax(QKT/root(dk))V 그리고, 어텐션 결과를 받은 피드-포워드 네트워크(Feed Forward Network, FFN)는 두 개의 리니어 트랜스포메이 션(linear transformation)으로 구성되어, GELU(Gaussian Error Linear Units)가 적용된 다음의 수식 3을 기반 으로 구현될 수 있다. [수식 3] FFN(x) = max(0, xW1 + b1)W2 + b2 또한, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식 (NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다. 예를 들어, 데이터전처리부는 점별 예측(pointwise prediction) 모델, 확률 기반의 모델(probabilistic model), 신경망 기반의 모델(neural network based model)을 기반으로, 자연어 텍스트를 형태소 단위로 나눈 뒤, 각 형태소에 해당 품사를 태깅(tagging)할 수 있다. 또한, 데이터전처리부는 자연어 텍스트의 개체명(named entity)을 인식하고, 인식된 개체명의 종류를 분류 할 수 있다. 즉 데이터전처리부는 자연어 텍스트에 포함된 각 단어가 어떤 유형에 속하는지 인식할 수 있 다. 데이터전처리부는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 데이터전처리부 는 제2 토큰을 고정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다.이후, 데이터전처리부는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합 (concat)한 혼합 특징 벡터(mixed feature vector)를 생성하고, 생성된 혼합 특징 벡터를 수어텍스트생성부 에 전달할 수 있다. 예를 들어, 데이터전처리부는 \"h1, h2, 쪋, h7\"을 포함하는 제1 컨텍스트 벡터와, \"z1, z2, 쪋, z8\"를 포 함하는 제2 컨텍스트 벡터를 혼합하여, \"x1, x2, 쪋, x7\"을 포함하는 혼합 특징 벡터를 생성할 수 있다. 여기서, 데이터전처리부는 생성된 혼합 특징 벡터를 수어텍스트생성부로 전달하여, 제2 인공지능의 입력으로 사용하A_01_도록 함과 동시에, 제2 인공지능에 의한 결과 값 중 일부를 대체하는 데 사용하A_01_도록 할 수 있다. 다음 구성으로, 수어텍스트생성부는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 데이터전처리부로부터 전달받은 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성할 수 있다. 구체적으로 A_01_도 7에 A_01_도시된 바와 같이, 수어텍스트생성부는 데이터전처리부부터 전달받은 혼합 특징 벡터를 디코딩하기 위한, 트랜스포머(transformer) 모델의 디코더(decoder)에 해당될 수 있다. 이러 한, 트랜스포머 모델은 복수 개의 디코딩 블록으로 구성될 수 있다. 각 디코더 블록을 구성하는 첫번째 서브층인 마스크드 멀티 헤드 셀프 어텐션(masked multi-head self- attention)은 전술한 인코더의 서브층인 멀티 헤드 어텐션과 동일한 연산을 수행하되, 어텐션 스코어 행렬에서 마스킹을 적용하는 점에서 일부 상이하다. 즉, 서브층인 마스크드 멀티 헤드 셀프 어텐션은 현재 처리중인 단어 보다 앞쪽에 해당하는 단어에 대해서만 어텐션 점수를 참고할 수 있A_01_도록 하기 위하여 마스킹을 적용할 수 있다. 그리고, 디코더는 두번째 서브층인 멀티 헤드 어텐션(multi-head attention)을 통해 엔코더의 출력 값인 혼합 특징 벡터를 입력 받고, 입력 받은 혼합 특징 벡터를 멀티 헤드 어텐션(multi-head attention) 및 세번째 서브 층인 피드-포워드 네트워크(Feed Forward Network, FFN)를 통과시키고, 리니어 레이어(Linear Layer) 및 소프 트맥스 레이어(softmax layer)를 거쳐 학습된 수어 단어 데이터베이스 중 가장 관계가 높은 수어 토큰을 출력할 수 있다. 이때, 리니어 레이어는 완전 접속망(fully-connected network)으로 디코더가 마지막으로 출력한 벡터를 그보다 훨씬 더 큰 사이즈의 벡터인 로짓(logits) 벡터로 투영시킬 수 있다. 여기서, 로짓 벡터의 각 셀은 각 단어에 대한 점수가 될 수 있다. 그리고, 소프트맥스 레이어는 이 점수들을 확률로 변환해주며, 가장 높은 확률 값을 가지는 셀에 해당하는 단어 를 최종 수어 텍스트로서 출력할 수 있다. 이때, 수어텍스트생성부는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 혼합 특징 벡터에 포함된 토큰 중 하나로 대체할 수 있다. 이 때, 수어텍스트생성부는 혼합 특징 벡터에 포함된 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰에 적합한 확률을 산출하고, 확률이 사전 설정된 값 이상인 토큰으로 대체할 수 있다. 즉, 수어텍스트생성부는 수어 텍스트를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of-vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 데이터전처리부의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 수어텍스트생성부는 디코더에 카피 어텐션(copy attention)을 별A_01_도로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사 전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력 할 확률A_01_도 함께 계산할 수 있다. 또한, 수어텍스트생성부는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출할 수 있다. 그리고, 수어텍스트생성부는 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의무문, 명령문, 청유문 및 감탄문 중 적어A_01_도 하나를 포 함할 수 있다. 이때, 수어텍스트생성부는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 하지만, 이에 한정된 것은 아니고, 수어텍스트생성부는 데 이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유형을 식별할 수A_01_도 있다. 즉, 수어텍스트생성부는 추정된 문자 유형에 따라 수어 텍스트를 수어로 동작하는데 따른 속A_01_도 지수 를 A_01_도출하고, A_01_도출된 속A_01_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 수어텍스트생성부(22 5)는 속A_01_도 지수를 나타내는 문자를 수어 텍스트에 포함시킬 수 있다. 예를 들어, 수어텍스트생성부는 생성된 수어 텍스트의 각 단어 사이에 속A_01_도를 의미하는 속A_01_도 지수를 삽입하여 출력할 수 있다. 여기 서, 속A_01_도 지수는 특정 속A_01_도 범위를 나타내는 문자가 될 수 있다. 예를 들어, 빠른 속A_01_도를 나타 내는 속A_01_도 지수는 'a', 보통 속A_01_도를 나타내는 속A_01_도 지수는 'b', 느린 속A_01_도를 나타내는 속 A_01_도 지수는 'c'가 될 수 있다. 즉, 수어텍스트생성부는 \"(a)죽임기 또 발명 물건 크다 많다 아이 과학 놀다 곳\"과 같이, 출력된 수어 텍스트의 전단에 수어 동작을 수행하는데 따른 속A_01_도 지수를 표시하거나, \" 죽임기(a)또(a)발명(b)발명(c)물건(a)크다(a)많다(b)아이(a) 과학(a)놀다(b)곳(c)과 같이, 각 단어 사이에 속 A_01_도 지수를 나타내는 문자를 표시하여, 수어 텍스트를 수어 동작으로 표현하는 것을 지원할 수 있다. 다음 구성으로, 수어영상생성부는 변환된 수어 텍스트와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로, 수어영상생성부는 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 이후, 수어영상생성부는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키 포인트(keypoint)를 추출할 수 있다. 즉, 수어영상생성부는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 예를 들어, 수어영상생성부는 오픈포즈(openpose) 모델을 통해 2D 키포인트를 추출할 수 있다. 여기서, 오 픈포즈 모델은 단일 이미지에서 실시간으로 몸체, 손, 얼굴 그리고 발들의 키포인트들을 최대 130개까지 인식할 수 있으며, 입력된 이미지 또는 비디오로부터 2D 키포인트를 추출하여, 배경이미지와 키포인트가 합쳐진 이미지 또는 키포인트만 가진 이미지를 JSON, XML, 영상 데이터 등으로 저장할 수 있다. 또한, 수어영상생성부는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 수어영상생성부 는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인 트를 기초로 손실(loss)이 최소화되A_01_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환 할 수 있다. 여기서, 수어영상생성부는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하 는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 수어영상 생성부는 손의 조인트 21개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 수어영상생성부는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 수어영 상생성부는 중수지관절을 대상으로 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_도와의 상관 관계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_도를 추정할 수 있다. 이때, 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_도를 추정하기 위한 인공지능은 손목의 회전 각A_01_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 이후, 수어영상생성부는 생성된 3D 조인트 및 동작 정보를 기초로 수어 텍스트의 각 단어별 영상을 생성할 수 있다. 즉, 수어영상생성부는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 수어영상생성부는 가상 인간이 수화 를 수행하는 영상을 생성할 수 있다. 그리고, 수어영상생성부는 각 단어별 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 수어영상 생성부는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_01_도 하나의 이미지를 생 성할 수 있다. 여기서, 수어영상생성부는 연속되는 각 단어별 영상 사이에 적어A_01_도 하나의 이미지를 생성하되, 선행 되는 제1 단어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이 미지를 삽입할 수 있다. 그리고, 수어영상생성부는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상의 최초 프레임 사이에 적어A_01_도 하나의 이미지를 생성할 수 있다. 즉, 수어영상생 성부는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식 별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 수어영상생성부는 자연어 텍스트 의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영상의 재생 속A_01_도를 결정할 수 있다. 여기서, 수어영상생성부는 데이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유 형을 식별하거나, 수어텍스트생성부에 의해 분석된 문장 유형 결과를 가져올 수 있다. 예를 들어, 수어영상생성부는 문장 유형이 청유문으로 식별되는 경우, 기본 자세의 유지 시간을 길게 하거 나, 수어 영상의 재생 속A_01_도를 느리게 하여, 정중한 표현이 될 수 있A_01_도록 할 수 있다. 이와 같이, 수 어영상생성부는 단순히 동작 영상을 출력할 뿐만 아니라, 비수지신호를 고려하여 동작 영상을 생성할 수 있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수 어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_01_도 중 적어A_01_도 하나를 결정할 수 있다. 이하, 상술한 바와 같은 번역서버의 논리적 구성 요소를 구현하기 위한 하드웨어에 대하여 보다 구체적으 로 설명한다. A_01_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_01_도이다. A_01_도 8에 A_01_도시된 바와 같이, 본 발명의 일 실시예에 따른 번역서버는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기, 입출력장치(Input/output device, 165), 데이터 버스(Bus, 270) 및 스토 리지(Storage, 275)를 포함하여 구성될 수 있다. 구체적으로, 프로세서는 메모리에 상주된 수어 텍스트 또는 수어 영상 번역 방법이 구현된 소프트웨 어(280a)에 따른 명령어를 기초로, 번역서버의 동작 및 기능을 구현할 수 있다. 메모리에는 스토리지에 저장된 번역 방법이 구현된 소프트웨어(280b)가 상주(loading)될 수 있다. 송수신기는 복수 개의 단말기와 데이터를 송수신할 수 있다. 입출력장치는 프로세서의 명령에 따라, 번역서버의 동작에 필요한 신호를 입력 받거나 연산 결 과를 외부로 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 각각 연 결되어, 각각의 구성 요소 사이에서 신호를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구 현된 소프트웨어(280b)를 저장할 수 있다. 그리고, 스토리지는 인공지능 및 인공지능을 학습하기 위한 데 이터 셋을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 텍스트 번역 방법 을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단계, 프로세서 가, 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 그리고, 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 영 상 번역 방법을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단 계, 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해 자연어 텍스트를 수어 텍스트로 변환하는 단계, 프로세서가, 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 상세하게, 프로세서는 중앙 처리 장치(Central Processing Unit, CPU), ASIC(Application-Specific Integrated Circuit), 칩셋(chipset), 논리 회로 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않 는다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리(flash memory), 메모리 카 드(memory card) 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 입출력장치는 버튼(button), 스위치(switch), 키보드(keyboard), 마우스(mouse), 조이스틱(joystick) 및 터치스크린(touch screen) 등과 같은 입력 장치와, LCD(Liquid Crystal Display), LED(Light Emitting Diode), 유기 발광 다이오드(Organic LED, OLED), 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED), 프린터 (printer), 플로터(plotter) 등과 같은 출력 장치 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 제각각 수행하는 모듈 (과정, 기능 등)들로 구현될 수 있다. 각각의 모듈은 메모리에 상주되고 프로세서에 의해 실행될 수 있다. 메모리는 프로세서의 내부 또는 외부에 존재할 수 있고, 널리 알려진 다양한 수단으로 프로세 서와 연결될 수 있다. A_01_도 8에 A_01_도시된 각 구성 요소는 다양한 수단(예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등)에 의해 구현될 수 있다. 하드웨어에 의해 구현될 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의해 구현될 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 업계의 통상의 지식을 가진 자에게 공지되어 사용 가능한 것일 수A_01_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_01_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용 해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_01_도록 구성될 수 있으며, 그 역A_01_도 마찬가지 이다. 이하, 상술한 바와 같은 번역서버의 동작에 대하여 보다 구체적으로 설명한다. A_01_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_01_도이고, A_01_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_01_도이다. A_01_도 9를 참조하면, S100 단계에서 번역서버는 단말기로부터 자연어 텍스트를 입력 받을 수 있다. 다음으로, S200 단계에서 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 전처리할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응 하는 벡터(vector)를 생성할 수 있다. 즉, 번역서버는 S300 단계에서 수어 텍스트를 생성하기 위하여, 입력 받 은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 즉, 번역서버는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 이때, 번역서버는 인공지능 성능을 향상시키기 위하여, 제1 토큰 생성 이전에, 자연어 텍스트 중 적어A_01_도 둘 이상의 의미를 갖는 단어 를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다.또한, 번역서버는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩(contextual embedding)을 수행할 수 있다. 즉, 번역 서버는 자연어 문장으로 사전 기계 학습된 인공지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 이때, 제1 컨텍스트 벡터 는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 또한, 번역서버는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다. 또한, 번역서버는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 번역서버는 제2 토큰을 고 정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다. 이후, 번역 서버는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합(concat)한 혼합 특징 벡터(mixed feature vector)를 생성할 수 있다. 다음으로, S300 단계에서 번역서버는 S200 단계에서 생성된 혼합 특징 벡터를 입력 받아 수어 텍스트를 생성할 수 있다. 이때, 번역서버는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성할 수 있다. 이때, 번역서버는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 혼합 특징 벡터에 포함된 토큰 중 하나로 대체할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰에 적합한 확률을 산출하 고, 확률이 사전 설정된 값 이상인 토큰으로 대체할 수 있다. 즉, 번역서버는 수어 텍스트를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of- vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 S200 단계의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 번역서버는 디코더에 카피 어텐션(copy attention)을 별A_01_도 로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력할 확률A_01_도 함께 계산할 수 있다. 또한, 번역서버는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출하고, 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의 무문, 명령문, 청유문 및 감탄문 중 적어A_01_도 하나를 포함할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 즉, 번역서버는 추정된 문자 유형에 따라 수어 텍스트를 수어로 동작하는데 따른 속A_01_도 지수를 A_01_도출하 고, A_01_도출된 속A_01_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 번역서버는 속A_01_도 지수를 나타내 는 문자를 수어 텍스트에 포함시킬 수 있다. 그리고, S400 단계에서 번역서버는 변환된 수어 텍스트와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로 A_01_도 10에 A_01_도시된 바와 같이, S410 단계에서 번역 서버는 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 다음으로, S420 단계에서 번역서버는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키포인트(keypoint) 를 추출할 수 있다. 즉, 번역서버는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인 공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 여기서, 2D 키포인트를 추출하기 위한 인공 지능은 결과 값인 추출된 2D 키포인트 및 후술할 변환된 3D 조인트를 2D 이미지에 프로젝션(projection) 시킨 이미지를 포함하는 데이터 셋을 통해 학습될 수 있다. 다음으로, S430 단계에서 번역서버는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 번역서 버는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인트를 기초로 손실(loss)이 최소화되A_01_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환할 수 있다. 여기서, 번역서버는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 번역서버는 손의 조인트 21개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 다음으로, S440 단계에서 번역서버는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 번역서버는 중수지관절을 대상으로 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_도와의 상관 관 계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_ 도를 추정할 수 있다. 이때, 손목의 회전 각A_01_도 및 팔꿈치의 회전 각A_01_도를 추정하기 위한 인공지능은 손목의 회전 각A_01_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 다음으로, S450 단계에서 번역서버는 생성된 3D 조인트 및 동작 정보를 기초로 수어 텍스트의 각 단어별 수어 영상을 생성할 수 있다. 즉, 번역서버는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 번역서버는 가상 인간이 수화를 수행 하는 영상을 생성할 수 있다. 그리고, S450 단계에서 번역서버는 각 단어별 수어 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인 터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_01_도 하나의 이미지를 생성 할 수 있다. 여기서, 번역서버는 연속되는 각 단어별 영상 사이에 적어A_01_도 하나의 이미지를 생성하되, 선행되는 제1 단 어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이미지를 삽입 할 수 있다. 그리고, 번역서버는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상 의 최초 프레임 사이에 적어A_01_도 하나의 이미지를 생성할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영상의 재생 속A_01_도를 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_01_도 중 적어A_01_도 하나를 결정할 수 있다. A_01_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_01_도이다. A_01_도 11을 참조하면, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방 지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_01_도 하나의 이미지를 생성할 수 있다. 이때, A_01_도 11에 A_01_도시된 바와 같이, 선행되는 제1 단어(기술) 영상의 최후 프레임이 'a'이고, 후행되는 제2 단어(전통) 영상의 최초 프레임이 'b'라고 가정하면, 번역서버는 선행되는 제1 단어 영상의 최후 프레임' a'과, 후행되는 제2 단어 영상의 최초 프레임'b'사이에 사전 저장된 예비 동작 이미지'c'를 삽입할 수 있다. 그리고, 번역서버는 예비 동작 이미지'c'를 기준으로, 제1 단어 영상의 최후 프레임'a' 및 제2 단어 영상의 최 초 프레임'b' 사이에, 모션 인터폴레이션을 통해 적어A_01_도 하나의 이미지를 생성할 수 있다. 이를 통해, 번역서버는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. 이상과 같이, 본 명세서와 A_01_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_01_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_01_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_01_부호의 설명 100 : 단말기 200 : 번역서버 205 : 통신부 210 : 입출력부 215 : 저장부 220 : 데이터전처리부 225 : 수어텍스트생성부 230 : 수어영상생성부 A_01_청구범위 A_01_청구항 1 번역서버가, 자연어 텍스트(text)를 입력 받는 단계; 상기 번역서버가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계; 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계; 를 포함하는, 수어 변역 방법. A_01_청구항 2 제1 항에 있어서, 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성하는 단계; 및 자연어 문장으로 사전 기계 학습된 인공지능을 통해 상기 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스 트 벡터를 생성하는 단계; 를 포함하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 3 제2 항에 있어서, 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어 및 상기 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성하는 단계; 및 상기 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성하는 단계; 를 포함하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 4 제3 항에 있어서, 상기 벡터를 생성하는 단계는 상기 제1 컨텍스트 벡터 및 상기 제2 컨텍스트 벡터를 합성한 혼합 특징 벡터(mixed feature vector)를 생성하 고, 상기 생성된 혼합 특징 벡터를 상기 수어 텍스트를 생성하기 위한 인공지능에 입력하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 5 제3 항에 있어서, 상기 제1 토큰을 생성하는 단계 및 상기 제2 토큰을 생성하는 단계 이전에 상기 자연어 텍스트 중 적어A_01_도 둘 이상의 의미를 갖는 단어를 검출하고, 상기 검출된 단어를 의미 단위로 띄어쓰기 처리하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 6 제4 항에 있어서, 상기 수어 텍스트를 생성하는 단계는 상기 번역서버가, 상기 자연어 텍스트와 대응하는 혼합 특징 벡터를 입력 받는 단계; 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습된 인공지능을 통해, 상기 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하는 단계; 및 상기 추출된 수어 토큰을 기초로 수어 텍스트를 생성하는 단계; 를 포함하는 것을 특징으로 하는, 수어 번역 방 법. A_01_청구항 7 제6 항에 있어서, 상기 수어 토큰을 추출하는 단계는 상기 혼합 특징 벡터를 기초로 상기 자연어 텍스트의 문장 유형을 추정하고, 상기 추정된 문장 유형에 따른 비 수지기호를 추출하고, 상기 추출된 비수지기호를 상기 수어 토큰에 임베딩하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 8 제7 항에 있어서, 상기 수어 토큰을 추출하는 단계는 상기 추정된 문장 유형에 따라 상기 수어 텍스트를 수어로 동작하는데 따른 속A_01_도 지수를 A_01_도출하고, 상기 A_01_도출된 속A_01_도 지수를 상기 수어 토큰에 임베딩하는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 9 제8 항에 있어서, 상기 수어 텍스트를 생성하는 단계는 상기 속A_01_도 지수를 나타내는 문자를 상기 수어 텍스트에 포함시키는 것을 특징으로 하는, 수어 번역 방법. A_01_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 자연어 텍스트(text)를 입력 받는 단계; 상기 프로세서가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계; 및 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 26, "content": "A_01_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 27, "content": "A_01_요약 본 발명은 높은 정확A_01_도로 자연어를 수어 텍스트로 번역하기 위한, 수어 텍스트 번역 방법을 제안한다. 상 기 방법은 번역서버가, 자연어 텍스트(text)를 입력 받는 단계, 상기 번역서버가, 상기 자연어 텍스트를 인코딩 (encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능 (Artificial Intelligence, AI)을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수 어 텍스트를 생성하는 단계를 포함할 수 있다. A_01_대표A_01_도 A_01_도 3 A_01_도면 A_01_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 28, "content": "A_01_도 2 A_01_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 29, "content": "A_01_도 4 A_01_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 30, "content": "A_01_도 6 A_01_도 7 A_01_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 31, "content": "A_01_도 9 A_01_도 10 A_01_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 32, "content": "A_02_발명의 설명 A_02_발명의 명칭 수어 영상 번역 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for translate sign language video, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 33, "content": "A_02_기술분야 본 발명은 언어 번역(language translation)에 관한 것이다. 보다 상세하게는, 높은 정확A_02_도로 자연어 (natural language)를 수어(sign language) 영상(video)으로 번역하기 위한, 수어 영상 번역 방법 및 이를 실 행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. A_02_발명의 배경이 되는 기술 수어(수화, sign language)는 소리로 하는 언어가 아닌 손짓을 이용하여 뜻을 전달할 수 있는 언어의 일종이다. 음성언어가 청각으로 이해되고 음성으로 표현되는 청각-음성 체계임에 반하여, 수어는 시각으로 이해되고 손운 동으로 표현되는 시각-운동 체계이다. 수어는 대부분 청각 장애인의 의사소통을 위해 사용된다. 이러한, 수어는 수지신호와 비수지신호로 구성되어 있다. 수지신호는 수위(손의 위치), 수형(손의 모양), 수동 (손의 움직임) 등이 있다. 비수지신호는 얼굴의 표정과 머리와 몸의 움직임 등이 있으며, 놀람, 공포, 기쁨, 증 오, 행복, 슬픔, 혐오, 비웃음 등의 감정을 나타낼 수 있다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여 를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_02_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다.그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_02_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. A_02_선행기술문헌 A_02_특허문헌 대한민국 등록특허공보 제10-1915088호, ‘수화번역장치’, (2018.10.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 34, "content": "A_02_발명의 내용 A_02_해결하고자 하는 과제 본 발명의 일 목적은 높은 정확A_02_도로 자연어(natural language)를 수어(sign language) 영상(video)으로 번역하기 위한, 수어 영상 번역 방법을 제공하는 것이다. 본 발명의 다른 목적은 높은 정확A_02_도로 자연어를 수어 영상으로 번역하기 위한, 수어 영상 번역 방법을 실 행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 35, "content": "A_02_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 높은 정확A_02_도로 자연어를 수어 영상으로 번역 하기 위한, 수어 영상 번역 방법을 제안한다. 상기 방법은 번역서버가, 자연어 텍스트(text)를 입력 받는 단계, 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 자연어 텍스트를 수어 텍스트로 변환하는 단계, 상기 번역서버가, 상기 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계를 포함한다. 구체적으로, 상기 수어 영상을 생성하는 단계는 상기 번역서버가, 상기 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 단어 수화 영상을 추출하는 단계, 상기 번역서버가, 상기 추출된 단어 수화 영상에 포함된 프레임 각 각에서 2D 키포인트(keypoint)를 추출하는 단계, 상기 번역서버가, 상기 추출된 2D 키포인트를 3D 조인트 (joint)로 변환하는 단계, 상기 번역서버가, 상기 변환된 3D 조인트를 기초로 상기 3D 조인트에 따른 동작 정보 를 생성하는 단계, 상기 3D 조인트 및 상기 동작 정보를 기초로 상기 수어 텍스트의 각 단어별 수어 영상을 생 성하는 단계 및 상기 각 단어별 영상을 조합하여 문장 수어 영상을 생성하는 단계를 포함하는 것을 특징으로 한 다. 상기 2D 키포인트를 추출하는 단계는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인 공지능을 통해, 상기 단어 수화 영상에서 상기 2D 키포인트를 추출하는 것을 특징으로 한다. 상기 3D 조인트로 변환하는 단계는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 상기 인 공지능을 통해 추출된 2D 키포인트를 기초로 손실(loss)이 최소화되A_02_도록 학습된 인공지능을 통해, 상기 추 출된 2D 키포인트를 3D 조인트로 변환하는 것을 특징으로 한다. 상기 3D 조인트로 변환하는 단계는 상기 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하는 2D 키포인트를 추출하고, 상기 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환하는 것을 특징으로 한다. 상기 동작 정보를 생성하는 단계는 상기 중수지관절을 대상으로 손목의 회전 각A_02_도 및 팔꿈치의 회전 각 A_02_도와의 상관 관계를 기초로 사전 학습된 인공지능을 통해, 상기 3D 조인트에 따른 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도를 추정하는 것을 특징으로 한다. 상기 수어 영상을 생성하는 단계는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방 지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 상기 연속되는 각 단어별 영상 사이에 적어 A_02_도 하나의 이미지를 생성하는 것을 특징으로 한다. 상기 수어 영상을 생성하는 단계는 상기 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성하 되, 선행되는 제1 단어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이미지를 삽입하는 것을 특징으로 한다.상기 수어 영상을 생성하는 단계는 상기 예비 동작 이미지를 기준으로, 상기 제1 단어 영상의 최후 프레임 및 상기 제2 단어 영상의 최초 프레임 사이에 적어A_02_도 하나의 이미지를 생성하는 것을 특징으로 한다. 상기 수어 영상을 생성하는 단계는 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형 을 식별하고, 상기 식별된 문장 유형에 따라 상기 예비 동작의 유지 시간을 결정하는 것을 특징으로 한다. 상기 수어 영상을 생성하는 단계는 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형 을 식별하고, 상기 식별된 문장 유형에 따라 상기 생성된 수어 영상의 재생 속A_02_도를 결정하는 것을 특징으 로 한다. 상기 각 단어별 영상을 생성하는 단계는 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트를 작성 한 화자와, 상기 생성된 수어 영상을 청취하는 청자 각각의 지휘를 식별하고, 상기 식별된 지휘 기초로 예비 동 작 유지 시간 및 수어 영상 재생 속A_02_도 중 적어A_02_도 하나를 결정하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 수어 영상 번역 방법을 실행하기 위하여 기록매체 에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상 기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 자연어 텍스트(text)를 입력 받는 단계, 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공 지능(Artificial Intelligence, AI)을 통해 상기 자연어 텍스트를 수어 텍스트로 변환하는 단계 및 상기 프로세 서가, 상기 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계를 실행시키기 위하여, 기록매체에 기록 된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_02_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 36, "content": "A_02_발명의 효과 본 발명의 실시 예들에 따르면, 자연어 및 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 높은 정확A_02_도로 자연어를 수어 텍스트로 변 환하고, 변환된 수어 텍스트를 수어 영상으로 제공할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 37, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 38, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 39, "content": "A_02_도면의 간단한 설명 A_02_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_02_도이다. A_02_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_02_도이다. A_02_도 3은 본 발명의 일 실시예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_02_도이다. A_02_도 4 및 A_02_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_02_도이다. A_02_도 6은 본 발명의 일 실시예에 따른 수어텍스트생성부의 기능을 설명하기 위한 예시A_02_도이다. A_02_도 7은 본 발명의 일 실시예에 따른 제2 인공지능을 설명하기 위한 예시A_02_도이다. A_02_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_02_도이다. A_02_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_02_도이다. A_02_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_02_도이다. A_02_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_02_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 40, "content": "A_02_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_02_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_02_도하게 포괄적인 의미로 해석되거나, 과A_02_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_02_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, '구성된다' 또는 '가지다' 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_02_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_02_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 '연결되어' 있다거나 '접속되어' 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_02_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_02_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"'직접 연결되어' 있다거나 '직접 접속되어' 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_02_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_02_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_02_도면은 본 발명의 사상을 쉽게 이해할 수 있A_02_도록 하기 위한 것일 뿐, 첨부된 A_02_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_02_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_02_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여 를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_02_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다. 그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_02_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. 이러한 한계를 극복하고자, 본 발명은 높은 정확A_02_도로 자연어(natural language)를 수어(sign language)로 번역할 수 있는 다양한 수단들을 제안하고자 한다. A_02_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_02_도이다. A_02_도 1을 참조하면, 본 발명의 일 실시예에 따른 수어변역시스템은 적어A_02_도 하나의 단말기(terminal, 100a, 100b, 100c, …, 100n; 100) 및 번역서버를 포함하여 구성될 수 있다. 이와 같은, 본 발명의 일 실시예에 따른 수어번역시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나 타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 단말기는 사용자로부터 자연어(natural language) 텍스트를 입력 받 거나, 번역서버에 의해 번역된 수어(sign language) 텍스트(text) 또는 수어 영상(video)을 출력하여 사용 자에게 제공할 수 있는 장치이다.여기서, 자연어는 인간이 일상생활에서 의사 소통을 위해 사용하는 언어가 될 수 있다. 특히, 자연어는 한국어, 영어, 독일어, 스페인어, 프랑스어, 이탈리아어 등 다양한 국가의 언어가 해당될 수 있다. 구체적으로, 자연어 는 각 국가의 언어 중에서A_02_도 구어체(colloquial style), 문어체(literary style) 등이 해당될 수 있다. 이러한, 단말기는 사용자로부터 자연어를 입력 받기 위한 입력 장치(input device) 및 번역 서버에 의해 번역된 수어 텍스트 또는 수어 영상을 출력하기 위한 출력 장치(output device)를 포함하여 구성될 수 있 다. 또한, 단말기는 번역서버를 포함한 다른 장치들과 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_02_도 허용될 수 있다. 예를 들어, 단말기는 3GPP(3rd Generation Partnership Project)에서 규정하고 있는 사용자 장치(User Equipment, UE) 및 IEEE(Institute of Electrical and Electronics Engineers)에서 규정하고 있는 모바일 스테이션(Mobile Station, MS) 중 어느 하나에 해당될 수 있다. 그러나 이에 한정되지 아니하고, 단말기는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버 (server)와 같은 고정식 컴퓨팅 장치, 또는 랩탑(laptop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디 어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이동식 컴퓨팅 장치 중 어느 하나가 될 수A_02_도 있다. 다음 구성으로, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 수 어 텍스트 및 수어 영상 중 적어A_02_도 하나로 번역하여, 번역된 수어 텍스트 및 수어 영상 중 적어A_02_도 하 나를 단말기에 제공할 수 있는 장치가 될 수 있다. 이러한, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 인코딩 (incoding)하여, 자연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력된 자연어 텍스트를 토큰화(tokenization)하고, 토큰화 작 업 전후에 자연어 텍스트를 용A_02_도에 맞게 정제(cleaning) 및 정규화(normalization)하여 전처리하고, 전처 리 된 토큰들을 압축해서 하나의 벡터로 만들 수 있다. 또한, 번역 서버는 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습 (machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성하여 단말기에 제공할 수 있다. 그리고, 번역 서버는 번역된 수어 텍스트를 수어 영상으로 생성하여, 단말기에 제공할 수 있다. 이와 같은, 번역서버는 단말기와 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_02_도 허용될 수 있다. 예를 들어, 번역서버는 데스크탑, 워크스 테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 이러한, 특징을 가지는 번역서버의 구체적인 구성 및 동작에 대해서는 A_02_도 2 내지 A_02_도 7을 참조하 여 후술하기로 한다. 지금까지 상술한 바와 같은, 수어번역시스템을 구성하는 단말기 및 번역서버는 장치들 사이를 직 접 연결하는 보안 회선, 공용 유선 통신망 또는 이동통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터 를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC) 및 광가입자망(Fiber To The Home, FTTH) 중 하나 이상이 포함될 수 있으 나, 이에 한정되는 것은 아니다. 또한, 이동통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다 중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE) 및 5세대 이동통신(5th generation mobile telecommunication) 중 하나 이상이 포함될 수 있 으나, 이에 한정되는 것A_02_도 아니다. 이하, 상술한 바와 같은 특징을 가지는, 번역서버의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_02_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_02_도이고, A_02_도 3은 본 발명의 일 실시 예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_02_도이고, A_02_도 4 및 A_02_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_02_도이고, A_02_도 6은 본 발명의 일 실시예에 따른 수어 텍스트생성부의 기능을 설명하기 위한 예시A_02_도이고, A_02_도 7은 본 발명의 일 실시예에 따른 제2 인공지능 을 설명하기 위한 예시A_02_도이다. 우선적으로, A_02_도 2를 참조하면, 본 발명의 일 실시예에 따른 번역 서버는 통신부, 입출력부 , 저장부, 데이터전처리부, 수어텍스트생성부 및 수어영상생성부를 포함하여 구성될 수 있다. 이와 같은, 본 발명의 일 실시예에 따른 번역서버의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각 구성 요소에 대하여 설명하면, 통신부는 단말기와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 단말기로부터 자연어 텍스트를 입력 받을 수 있고, 입력 받은 자연어 텍스트를 번역한 수어 텍스트 및 수어 영상 중 적어A_02_도 하나를 단말기로 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해, 관리자로부터 명령을 입력 받거나 또는 연산 결과를 출력할 수 있다. 이 경우, 관리자는 번역 서비스를 제공하는 서비스 제공자로 지칭될 수 있으며, 이에 한정되지 않는다. 구체적으로, 입출력부는 관리자로부터 인공지능을 학습하기 위한 데이터 셋을 입력 받을 수 있다. 예를 들 어, 입출력부는 제1 인공지능의 학습을 위하여, 다양한 형태의 자연어 문장에 관한 데이터 셋을 입력 받을 수 있다. 또한, 입출력부는 제2 인공지능의 학습을 위하여, 자연어 및 자연어와 매칭되는 수어 데이터 셋 을 입력 받을 수 있다. 또한, 입출력부는 데이터전처리부로부터 생성된 결과 값, 수어텍스트생성부로부터 생성된 수어 텍스트 및 수어영상생성부로부터 생성된 수어 영상 중 적어A_02_도 하나를 출력할 수 있다. 다음 구성으로, 저장부는 번역서버의 동작에 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 데이터전처리부, 수어텍스트생성부 및 수어영상생성부에 의해 주기 적으로 갱신되는 데이터베이스를 저장할 수 있다. 또한, 저장부는 인공지능(AI) 학습을 위한 데이터 셋을 저장할 수 있다. 그리고, 저장부는 데이터전처리부, 수어텍스트생성부 및 수어영상생성부 에서 사용되는 인공지능 모델을 저장할 수 있다. 다음 구성으로, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자 연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 즉, 데이터전처리부는 수어텍스트생성부가 수어 텍스트를 생성하기 위하여, 입력 받은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 구체적으로, 데이터전처리부는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 예를 들 어, A_02_도 3에 A_02_도시된 바와 같이, 데이터전처리부는 \"죽음기와 발명품이 잔뜩 우리 아이들과의 과 학 놀이터\"라는 문장을 입력 받은 경우, 문장에 포함된 각 단어를 토큰화(tokenization)하여 \"a1, a2, 쪋, a7\" 과 같은 제1 토큰을 생성할 수 있다. 이때, 데이터전처리부는 인공지능 성능을 향상시키기 위하여, 제1 토 큰 생성 이전에, 자연어 텍스트 중 적어A_02_도 둘 이상의 의미를 갖는 단어를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다. 또한, 데이터전처리부는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩 (contextual embedding)을 수행할 수 있다. 즉, 데이터전처리부는 자연어 문장으로 사전 기계 학습된 인공 지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 예를 들어, 데이터전처리부는 자연어 텍스트를 토큰화하여 생성된 \"a1, a2, 쪋, a7\"과 같은 제1 토큰을 제 1 인공지능에 입력하여 \"h1, h2, 쪋, h7\"를 포함하는 제1 컨텍스트 벡터를 생성할 수 있다. 이때, 제1 컨텍스트 벡터는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 예를 들어, 데이터전처리부는 BERT(Bidirectional Encoder Representations from Transformers) 모델에 기반한 인공지능(AI)을 이용하여, 제1 컨텍스트 벡터를 생성할 수 있다.보다 상세하게 A_02_도 4 및 A_02_도 5를 참조하면, BERT 모델은 트랜스포머(transformer)를 기반으로, 인코더 (encoder)만을 사용하는 모델에 해당된다. BERT 모델은 일반적인 트랜스포머와 다르게, 토큰 임베딩(token embeddings), 토큰의 포지션 임베딩(position embeddings) 및 세그먼트 임베딩(segment embedding)으로 이루어 진 입력 값을 가진다. 이러한, BERT 모델은 복수 개의 인코딩 블록으로 구성될 수 있다. 기본 BERT 모델은 12개의 인코딩 블록으로 구 성되고, 대형 BERT 모델은 24개의 인코딩 블록으로 구성될 수 있으나, 이에 한정되는 것은 아니다. 각각의 인코 더 블록은 이전의 출력 값을 현재의 입력 값으로 가지며, BERT 모델은 인코더 블록의 개수만큼 재귀적으로 반복 처리되는 형태로 복수 개의 인코더들이 구성될 수 있다. 그리고, 각각의 인코더 블록의 출력 값은 매번 잔차 연 결(residual connections)되게 처리될 수 있다. 각 인코더 블록을 구성하는 멀티 헤드 어텐션(multi-head attention)은 다음의 수식 1과 같이, 서로 다른 가중 치 행렬(weight matrix)를 이용하여 어텐션(attention)을 h번 계산한 다음 이를 서로 연결(concatenates)한 결 과를 출력할 수 있다. [수식 1] MultiHead(Q, K, V) = [head1; …; headh]wO 여기서, headi는 Attention(QWiQ, KWiK, VWiV)4이다. Q는 디코더의 히든 스테이지(hidden stage), K는 인코더의 히든 스테이지, V는 K에 어텐션을 부여받은 정규화된 가중치(normalized weight)이며, Q, K, V에 대한 스케일드 닷-프로덕트 어텐션(scaled dot-product attention)은 다음의 수식 2를 통해 산출될 수 있다. [수식 2] Attention(Q, K, V) = softmax(QKT/root(dk))V 그리고, 어텐션 결과를 받은 피드-포워드 네트워크(Feed Forward Network, FFN)는 두 개의 리니어 트랜스포메이 션(linear transformation)으로 구성되어, GELU(Gaussian Error Linear Units)가 적용된 다음의 수식 3을 기반 으로 구현될 수 있다. [수식 3] FFN(x) = max(0, xW1 + b1)W2 + b2 또한, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식 (NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다. 예를 들어, 데이터전처리부는 점별 예측(pointwise prediction) 모델, 확률 기반의 모델(probabilistic model), 신경망 기반의 모델(neural network based model)을 기반으로, 자연어 텍스트를 형태소 단위로 나눈 뒤, 각 형태소에 해당 품사를 태깅(tagging)할 수 있다. 또한, 데이터전처리부는 자연어 텍스트의 개체명(named entity)을 인식하고, 인식된 개체명의 종류를 분류 할 수 있다. 즉 데이터전처리부는 자연어 텍스트에 포함된 각 단어가 어떤 유형에 속하는지 인식할 수 있 다. 데이터전처리부는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 데이터전처리부 는 제2 토큰을 고정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다. 이후, 데이터전처리부는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합 (concat)한 혼합 특징 벡터(mixed feature vector)를 생성하고, 생성된 혼합 특징 벡터를 수어텍스트생성부 에 전달할 수 있다. 예를 들어, 데이터전처리부는 \"h1, h2, 쪋, h7\"을 포함하는 제1 컨텍스트 벡터와, \"z1, z2, 쪋, z8\"를 포 함하는 제2 컨텍스트 벡터를 혼합하여, \"x1, x2, 쪋, x7\"을 포함하는 혼합 특징 벡터를 생성할 수 있다. 여기서, 데이터전처리부는 생성된 혼합 특징 벡터를 수어텍스트생성부로 전달하여, 제2 인공지능의 입력으로 사용하A_02_도록 함과 동시에, 제2 인공지능에 의한 결과 값 중 일부를 대체하는 데 사용하A_02_도록 할 수 있다. 다음 구성으로, 수어텍스트생성부는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 데이터전처리부로부터 전달받은 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성할 수 있다. 구체적으로 A_02_도 7에 A_02_도시된 바와 같이, 수어텍스트생성부는 데이터전처리부부터 전달받은 혼합 특징 벡터를 디코딩하기 위한, 트랜스포머(transformer) 모델의 디코더(decoder)에 해당될 수 있다. 이러 한, 트랜스포머 모델은 복수 개의 디코딩 블록으로 구성될 수 있다. 각 디코더 블록을 구성하는 첫번째 서브층인 마스크드 멀티 헤드 셀프 어텐션(masked multi-head self- attention)은 전술한 인코더의 서브층인 멀티 헤드 어텐션과 동일한 연산을 수행하되, 어텐션 스코어 행렬에서 마스킹을 적용하는 점에서 일부 상이하다. 즉, 서브층인 마스크드 멀티 헤드 셀프 어텐션은 현재 처리중인 단어 보다 앞쪽에 해당하는 단어에 대해서만 어텐션 점수를 참고할 수 있A_02_도록 하기 위하여 마스킹을 적용할 수 있다. 그리고, 디코더는 두번째 서브층인 멀티 헤드 어텐션(multi-head attention)을 통해 엔코더의 출력 값인 혼합 특징 벡터를 입력 받고, 입력 받은 혼합 특징 벡터를 멀티 헤드 어텐션(multi-head attention) 및 세번째 서브 층인 피드-포워드 네트워크(Feed Forward Network, FFN)를 통과시키고, 리니어 레이어(Linear Layer) 및 소프 트맥스 레이어(softmax layer)를 거쳐 학습된 수어 단어 데이터베이스 중 가장 관계가 높은 수어 토큰을 출력할 수 있다. 이때, 리니어 레이어는 완전 접속망(fully-connected network)으로 디코더가 마지막으로 출력한 벡터를 그보다 훨씬 더 큰 사이즈의 벡터인 로짓(logits) 벡터로 투영시킬 수 있다. 여기서, 로짓 벡터의 각 셀은 각 단어에 대한 점수가 될 수 있다. 그리고, 소프트맥스 레이어는 이 점수들을 확률로 변환해주며, 가장 높은 확률 값을 가지는 셀에 해당하는 단어 를 최종 수어 텍스트로서 출력할 수 있다. 이때, 수어텍스트생성부는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 혼합 특징 벡터에 포함된 토큰 중 하나로 대체할 수 있다. 이 때, 수어텍스트생성부는 혼합 특징 벡터에 포함된 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰에 적합한 확률을 산출하고, 확률이 사전 설정된 값 이상인 토큰으로 대체할 수 있다. 즉, 수어텍스트생성부는 수어 텍스트를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of-vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 데이터전처리부의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 수어텍스트생성부는 디코더에 카피 어텐션(copy attention)을 별A_02_도로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사 전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력 할 확률A_02_도 함께 계산할 수 있다. 또한, 수어텍스트생성부는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출할 수 있다. 그리고, 수어텍스트생성부는 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의무문, 명령문, 청유문 및 감탄문 중 적어A_02_도 하나를 포 함할 수 있다. 이때, 수어텍스트생성부는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 하지만, 이에 한정된 것은 아니고, 수어텍스트생성부는 데 이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유형을 식별할 수A_02_도 있다. 즉, 수어텍스트생성부는 추정된 문자 유형에 따라 수어 텍스트를 수어로 동작하는데 따른 속A_02_도 지수 를 A_02_도출하고, A_02_도출된 속A_02_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 수어텍스트생성부(22 5)는 속A_02_도 지수를 나타내는 문자를 수어 텍스트에 포함시킬 수 있다. 예를 들어, 수어텍스트생성부는 생성된 수어 텍스트의 각 단어 사이에 속A_02_도를 의미하는 속A_02_도 지수를 삽입하여 출력할 수 있다. 여기 서, 속A_02_도 지수는 특정 속A_02_도 범위를 나타내는 문자가 될 수 있다. 예를 들어, 빠른 속A_02_도를 나타 내는 속A_02_도 지수는 'a', 보통 속A_02_도를 나타내는 속A_02_도 지수는 'b', 느린 속A_02_도를 나타내는 속 A_02_도 지수는 'c'가 될 수 있다. 즉, 수어텍스트생성부는 \"(a)죽임기 또 발명 물건 크다 많다 아이 과학 놀다 곳\"과 같이, 출력된 수어 텍스트의 전단에 수어 동작을 수행하는데 따른 속A_02_도 지수를 표시하거나, \"죽임기(a)또(a)발명(b)발명(c)물건(a)크다(a)많다(b)아이(a) 과학(a)놀다(b)곳(c)과 같이, 각 단어 사이에 속 A_02_도 지수를 나타내는 문자를 표시하여, 수어 텍스트를 수어 동작으로 표현하는 것을 지원할 수 있다. 다음 구성으로, 수어영상생성부는 변환된 수어 텍스트와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로, 수어영상생성부는 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 이후, 수어영상생성부는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키 포인트(keypoint)를 추출할 수 있다. 즉, 수어영상생성부는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 예를 들어, 수어영상생성부는 오픈포즈(openpose) 모델을 통해 2D 키포인트를 추출할 수 있다. 여기서, 오 픈포즈 모델은 단일 이미지에서 실시간으로 몸체, 손, 얼굴 그리고 발들의 키포인트들을 최대 130개까지 인식할 수 있으며, 입력된 이미지 또는 비디오로부터 2D 키포인트를 추출하여, 배경이미지와 키포인트가 합쳐진 이미지 또는 키포인트만 가진 이미지를 JSON, XML, 영상 데이터 등으로 저장할 수 있다. 또한, 수어영상생성부는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 수어영상생성부 는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인 트를 기초로 손실(loss)이 최소화되A_02_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환 할 수 있다. 여기서, 수어영상생성부는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하 는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 수어영상 생성부는 손의 조인트 21개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 수어영상생성부는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 수어영 상생성부는 중수지관절을 대상으로 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도와의 상관 관계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도를 추정할 수 있다. 이때, 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도를 추정하기 위한 인공지능은 손목의 회전 각A_02_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 이후, 수어영상생성부는 생성된 3D 조인트 및 동작 정보를 기초로 수어 텍스트의 각 단어별 영상을 생성할 수 있다. 즉, 수어영상생성부는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 수어영상생성부는 가상 인간이 수화 를 수행하는 영상을 생성할 수 있다. 그리고, 수어영상생성부는 각 단어별 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 수어영상 생성부는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생 성할 수 있다. 여기서, 수어영상생성부는 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성하되, 선행 되는 제1 단어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이 미지를 삽입할 수 있다. 그리고, 수어영상생성부는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상의 최초 프레임 사이에 적어A_02_도 하나의 이미지를 생성할 수 있다. 즉, 수어영상생 성부는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식 별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 수어영상생성부는 자연어 텍스트 의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영 상의 재생 속A_02_도를 결정할 수 있다. 여기서, 수어영상생성부는 데이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유 형을 식별하거나, 수어텍스트생성부에 의해 분석된 문장 유형 결과를 가져올 수 있다. 예를 들어, 수어영상생성부는 문장 유형이 청유문으로 식별되는 경우, 기본 자세의 유지 시간을 길게 하거 나, 수어 영상의 재생 속A_02_도를 느리게 하여, 정중한 표현이 될 수 있A_02_도록 할 수 있다. 이와 같이, 수 어영상생성부는 단순히 동작 영상을 출력할 뿐만 아니라, 비수지신호를 고려하여 동작 영상을 생성할 수있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수 어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_02_도 중 적어A_02_도 하나를 결정할 수 있다. 이하, 상술한 바와 같은 번역서버의 논리적 구성 요소를 구현하기 위한 하드웨어에 대하여 보다 구체적으 로 설명한다. A_02_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_02_도이다. A_02_도 8에 A_02_도시된 바와 같이, 본 발명의 일 실시예에 따른 번역서버는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기, 입출력장치(Input/output device, 165), 데이터 버스(Bus, 270) 및 스토 리지(Storage, 275)를 포함하여 구성될 수 있다. 구체적으로, 프로세서는 메모리에 상주된 수어 텍스트 또는 수어 영상 번역 방법이 구현된 소프트웨 어(280a)에 따른 명령어를 기초로, 번역서버의 동작 및 기능을 구현할 수 있다. 메모리에는 스토리지에 저장된 번역 방법이 구현된 소프트웨어(280b)가 상주(loading)될 수 있다. 송수신기는 복수 개의 단말기와 데이터를 송수신할 수 있다. 입출력장치는 프로세서의 명령에 따라, 번역서버의 동작에 필요한 신호를 입력 받거나 연산 결 과를 외부로 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 각각 연 결되어, 각각의 구성 요소 사이에서 신호를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구 현된 소프트웨어(280b)를 저장할 수 있다. 그리고, 스토리지는 인공지능 및 인공지능을 학습하기 위한 데 이터 셋을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 텍스트 번역 방법 을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단계, 프로세서 가, 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 그리고, 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 영 상 번역 방법을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단 계, 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해 자연어 텍스트를 수어 텍스트로 변환하는 단계, 프로세서가, 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 상세하게, 프로세서는 중앙 처리 장치(Central Processing Unit, CPU), ASIC(Application-Specific Integrated Circuit), 칩셋(chipset), 논리 회로 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않 는다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리(flash memory), 메모리 카 드(memory card) 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 입출력장치는 버튼(button), 스위치(switch), 키보드(keyboard), 마우스(mouse), 조이스틱(joystick) 및 터치스크린(touch screen) 등과 같은 입력 장치와, LCD(Liquid Crystal Display), LED(Light Emitting Diode),유기 발광 다이오드(Organic LED, OLED), 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED), 프린터 (printer), 플로터(plotter) 등과 같은 출력 장치 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 제각각 수행하는 모듈 (과정, 기능 등)들로 구현될 수 있다. 각각의 모듈은 메모리에 상주되고 프로세서에 의해 실행될 수 있다. 메모리는 프로세서의 내부 또는 외부에 존재할 수 있고, 널리 알려진 다양한 수단으로 프로세 서와 연결될 수 있다. A_02_도 8에 A_02_도시된 각 구성 요소는 다양한 수단(예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등)에 의해 구현될 수 있다. 하드웨어에 의해 구현될 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의해 구현될 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 업계의 통상의 지식을 가진 자에게 공지되어 사용 가능한 것일 수A_02_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_02_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용 해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_02_도록 구성될 수 있으며, 그 역A_02_도 마찬가지 이다. 이하, 상술한 바와 같은 번역서버의 동작에 대하여 보다 구체적으로 설명한다. A_02_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_02_도이고, A_02_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_02_도이다. A_02_도 9를 참조하면, S100 단계에서 번역서버는 단말기로부터 자연어 텍스트를 입력 받을 수 있다. 다음으로, S200 단계에서 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 전처리할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응 하는 벡터(vector)를 생성할 수 있다. 즉, 번역서버는 S300 단계에서 수어 텍스트를 생성하기 위하여, 입력 받 은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 즉, 번역서버는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 이때, 번역서버는 인공지능 성능을 향상시키기 위하여, 제1 토큰 생성 이전에, 자연어 텍스트 중 적어A_02_도 둘 이상의 의미를 갖는 단어 를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다. 또한, 번역서버는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩(contextual embedding)을 수행할 수 있다. 즉, 번역 서버는 자연어 문장으로 사전 기계 학습된 인공지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 이때, 제1 컨텍스트 벡터 는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 또한, 번역서버는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다.또한, 번역서버는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 번역서버는 제2 토큰을 고 정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다. 이후, 번역 서버는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합(concat)한 혼합 특징 벡터(mixed feature vector)를 생성할 수 있다. 다음으로, S300 단계에서 번역서버는 S200 단계에서 생성된 혼합 특징 벡터를 입력 받아 수어 텍스트를 생성할 수 있다. 이때, 번역서버는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 텍스트를 생성할 수 있다. 이때, 번역서버는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 혼합 특징 벡터에 포함된 토큰 중 하나로 대체할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰에 적합한 확률을 산출하 고, 확률이 사전 설정된 값 이상인 토큰으로 대체할 수 있다. 즉, 번역서버는 수어 텍스트를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of- vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 S200 단계의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 번역서버는 디코더에 카피 어텐션(copy attention)을 별A_02_도 로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력할 확률A_02_도 함께 계산할 수 있다. 또한, 번역서버는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출하고, 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의 무문, 명령문, 청유문 및 감탄문 중 적어A_02_도 하나를 포함할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 즉, 번역서버는 추정된 문자 유형에 따라 수어 텍스트를 수어로 동작하는데 따른 속A_02_도 지수를 A_02_도출하 고, A_02_도출된 속A_02_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 번역서버는 속A_02_도 지수를 나타내 는 문자를 수어 텍스트에 포함시킬 수 있다. 그리고, S400 단계에서 번역서버는 변환된 수어 텍스트와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로 A_02_도 10에 A_02_도시된 바와 같이, S410 단계에서 번역 서버는 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 다음으로, S420 단계에서 번역서버는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키포인트(keypoint) 를 추출할 수 있다. 즉, 번역서버는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인 공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 여기서, 2D 키포인트를 추출하기 위한 인공 지능은 결과 값인 추출된 2D 키포인트 및 후술할 변환된 3D 조인트를 2D 이미지에 프로젝션(projection) 시킨 이미지를 포함하는 데이터 셋을 통해 학습될 수 있다. 다음으로, S430 단계에서 번역서버는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 번역서 버는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인트를 기초로 손실(loss)이 최소화되A_02_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환할 수 있다. 여기서, 번역서버는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 번역서버는 손의 조인트 21 개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 다음으로, S440 단계에서 번역서버는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 번역서버는 중수지관절을 대상으로 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도와의 상관 관 계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_ 도를 추정할 수 있다. 이때, 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도를 추정하기 위한 인공지능은 손목의 회전 각A_02_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 다음으로, S450 단계에서 번역서버는 생성된 3D 조인트 및 동작 정보를 기초로 수어 텍스트의 각 단어별 수어 영상을 생성할 수 있다. 즉, 번역서버는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 번역서버는 가상 인간이 수화를 수행 하는 영상을 생성할 수 있다. 그리고, S450 단계에서 번역서버는 각 단어별 수어 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인 터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성 할 수 있다. 여기서, 번역서버는 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성하되, 선행되는 제1 단 어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이미지를 삽입 할 수 있다. 그리고, 번역서버는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상 의 최초 프레임 사이에 적어A_02_도 하나의 이미지를 생성할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영상의 재생 속A_02_도를 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_02_도 중 적어A_02_도 하나를 결정할 수 있다. A_02_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_02_도이다. A_02_도 11을 참조하면, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방 지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성할 수 있다. 이때, A_02_도 11에 A_02_도시된 바와 같이, 선행되는 제1 단어(기술) 영상의 최후 프레임이 'a'이고, 후행되는 제2 단어(전통) 영상의 최초 프레임이 'b'라고 가정하면, 번역서버는 선행되는 제1 단어 영상의 최후 프레임' a'과, 후행되는 제2 단어 영상의 최초 프레임'b'사이에 사전 저장된 예비 동작 이미지'c'를 삽입할 수 있다. 그리고, 번역서버는 예비 동작 이미지'c'를 기준으로, 제1 단어 영상의 최후 프레임'a' 및 제2 단어 영상의 최 초 프레임'b' 사이에, 모션 인터폴레이션을 통해 적어A_02_도 하나의 이미지를 생성할 수 있다. 이를 통해, 번역서버는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. 이상과 같이, 본 명세서와 A_02_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_02_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_02_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_02_부호의 설명 100 : 단말기 200 : 번역서버 205 : 통신부 210 : 입출력부 215 : 저장부 220 : 데이터전처리부 225 : 수어텍스트생성부 230 : 수어영상생성부 A_02_청구범위 A_02_청구항 1 번역서버가, 자연어 텍스트(text)를 입력 받는 단계; 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 자연어 텍스트를 수어 텍스트로 변환하는 단계; 상기 번역서버가, 상기 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계; 를 포함하는, 수어 영상 생 성 방법. A_02_청구항 2 제1 항에 있어서, 상기 수어 영상을 생성하는 단계는 상기 번역서버가, 상기 변환된 수어 텍스트에 포함된 각 단어와 매칭되는 단어 수화 영상을 추출하는 단계; 상기 번역서버가, 상기 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키포인트(keypoint)를 추출하는 단 계; 상기 번역서버가, 상기 추출된 2D 키포인트를 3D 조인트(joint)로 변환하는 단계; 상기 번역서버가, 상기 변환된 3D 조인트를 기초로 상기 3D 조인트에 따른 동작 정보를 생성하는 단계; 상기 3D 조인트 및 상기 동작 정보를 기초로 상기 수어 텍스트의 각 단어별 수어 영상을 생성하는 단계; 및 상기 각 단어별 영상을 조합하여 문장 수어 영상을 생성하는 단계; 를 포함하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 3 제2 항에 있어서, 상기 3D 조인트로 변환하는 단계는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 상기 인공지능을 통해 추출된 2D 키포인트를 기초로 손실(loss)이 최소화되A_02_도록 학습된 인공지능을 통해, 상기 추출된 2D 키포인트를 3D 조인트로 변환 하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 4 제3 항에 있어서, 상기 3D 조인트로 변환하는 단계는 상기 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하는 2D 키포인트를 추출하고, 상기 중수지 관절에 해당하는 2D 키포인트를 3D 조인트로 변환하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 5 제4 항에 있어서, 상기 동작 정보를 생성하는 단계는 상기 중수지관절을 대상으로 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도와의 상관 관계를 기초로 사전 학습된 인공지능을 통해, 상기 3D 조인트에 따른 손목의 회전 각A_02_도 및 팔꿈치의 회전 각A_02_도를 추정하 는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 6 제5 항에 있어서, 상기 문장 수어 영상을 생성하는 단계는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인터폴레이션 (motion interpolation)을 통해 상기 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 7 제6 항에 있어서, 상기 문장 수어 영상을 생성하는 단계는 상기 연속되는 각 단어별 영상 사이에 적어A_02_도 하나의 이미지를 생성하되, 선행되는 제1 단어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이미지를 삽입하는 것을 특징 으로 하는, 수어 영상 생성 방법. A_02_청구항 8 제7 항에 있어서, 상기 문장 수어 영상을 생성하는 단계는 상기 예비 동작 이미지를 기준으로, 상기 제1 단어 영상의 최후 프레임 및 상기 제2 단어 영상의 최초 프레임 사이에 적어A_02_도 하나의 이미지를 생성하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 9 지8 항에 있어서, 상기 각 단어별 수어 영상을 생성하는 단계는 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트를 작성한 화자와, 상기 생성된 수어 영상을 청 취하는 청자 각각의 지휘를 식별하고, 상기 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속 A_02_도 중 적어A_02_도 하나를 결정하는 것을 특징으로 하는, 수어 영상 생성 방법. A_02_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 자연어 텍스트(text)를 입력 받는 단계; 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해 상기 자연어 텍스트를 수어 텍스트로 변환하는 단 계; 및 상기 프로세서가, 상기 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계; 를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 41, "content": "A_02_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 42, "content": "A_02_요약 본 발명은 높은 정확A_02_도로 자연어를 수어 영상으로 번역하기 위한, 수어 영상 번역 방법을 제안한다. 상기 방법은 번역서버가, 자연어 텍스트(text)를 입력 받는 단계, 상기 번역서버가, 자연어 및 상기 자연어와 매칭되 는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 상기 자연어 텍스트를 수어 텍스트로 변환하는 단계, 상기 번역서버가, 상기 변환된 수어 텍스트와 매칭되는 수어 영상을 생성하는 단계를 포함한다. A_02_대표A_02_도 A_02_도 11 A_02_도면 A_02_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 43, "content": "A_02_도 2 A_02_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 44, "content": "A_02_도 4 A_02_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 45, "content": "A_02_도 6 A_02_도 7 A_02_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 46, "content": "A_02_도 9 A_02_도 10 A_02_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 47, "content": "A_03_발명의 설명 A_03_발명의 명칭 트랜스포머를 이용한 수어 글로스 번역 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램 {Method for translate sign language gloss using transformer, and computer program recorded on record- medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 48, "content": "A_03_기술분야 본 발명은 언어 번역(language translation)에 관한 것이다. 보다 상세하게는, 높은 정확A_03_도로 자연어 (natural language)를 수어(sign language) 글로스(gloss)로 번역하기 위한, 트랜스포머를 이용한 수어 글로스 번역 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. A_03_발명의 배경이 되는 기술 수어(수화, sign language)는 소리로 하는 언어가 아닌 손짓을 이용하여 뜻을 전달할 수 있는 언어의 일종이다. 음성언어가 청각으로 이해되고 음성으로 표현되는 청각-음성 체계임에 반하여, 수어는 시각으로 이해되고 손운 동으로 표현되는 시각-운동 체계이다. 수어는 대부분 청각 장애인의 의사소통을 위해 사용된다. 이러한, 수어는 수지신호와 비수지신호로 구성되어 있다. 수지신호는 수위(손의 위치), 수형(손의 모양), 수동 (손의 움직임) 등이 있다. 비수지신호는 얼굴의 표정과 머리와 몸의 움직임 등이 있으며, 놀람, 공포, 기쁨, 증 오, 행복, 슬픔, 혐오, 비웃음 등의 감정을 나타낼 수 있다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여 를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_03_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다.그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_03_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. A_03_선행기술문헌 A_03_특허문헌 대한민국 등록특허공보 제10-1915088호, ‘수화번역장치’, (2018.10.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 49, "content": "A_03_발명의 내용 A_03_해결하고자 하는 과제 본 발명의 일 목적은 높은 정확A_03_도로 자연어(natural language)를 수어(sign language) 글로스(gloss)로 번역하기 위한, 트랜스포머를 이용한 수어 글로스 번역 방법을 제공하는 것이다. 본 발명의 다른 목적은 높은 정확A_03_도로 자연어를 수어 글로스로 번역하기 위한, 트랜스포머를 이용한 수어 글로스 번역 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 50, "content": "A_03_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 높은 정확A_03_도로 자연어를 수어 글로스로 번역 하기 위한, 트랜스포머를 이용한 수어 글로스 번역 방법을 제안한다. 상기 방법은 번역서버가, 자연어 텍스트 (text)를 입력 받는 단계, 상기 번역서버가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 트랜스포머(transformer) 모델을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 글로스를 생성하는 단계를 포함할 수 있다. 구체적으로, 상기 수어 글로스를 생성하는 단계는 상기 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적 으로 사용하여 상기 수어 글로스를 생성하는 것을 특징으로 한다. 또한, 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성하고, 상기 자 연어 텍스트의 각 단어 및 상기 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성하는 단계 및 자연어 문장으 로 사전 기계 학습된 인공지능을 통해 상기 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터를 생성 하고, 상기 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성하는 단계를 포함하는 것을 특징으로 한다. 상기 벡터를 생성하는 단계는 상기 제1 컨텍스트 벡터 및 상기 제2 컨텍스트 벡터를 합성한 혼합 특징 벡터 (mixed feature vector)를 생성하고, 상기 생성된 혼합 특징 벡터를 상기 수어 글로스를 생성하기 위한 인공지 능에 입력하는 것을 특징으로 한다. 상기 제1 토큰을 생성하는 단계 및 상기 제2 토큰을 생성하는 단계 이전에 상기 자연어 텍스트 중 적어A_03_도 둘 이상의 의미를 갖는 단어를 검출하고, 상기 검출된 단어를 의미 단위로 띄어쓰기 처리하는 것을 특징으로 한 다. 상기 제2 토큰을 생성하는 단계는 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 상기 자연어 텍스트를 임베딩(embedding)하여 상기 제2 토큰을 생성하는 것을 특 징으로 한다. 상기 수어 글로스를 생성하는 단계는 상기 번역서버가, 상기 자연어 텍스트와 대응하는 혼합 특징 벡터를 입력 받는 단계 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습된 인 공지능을 통해, 상기 혼합 특징 벡터와 매칭되는 수어에 관한 출력 토큰을 추출하는 단계; 를 포함하는 것을 특 징으로 한다. 상기 출력 토큰을 추출하는 단계는 하기의 수학식을 통해 상기 인코딩 과정에서 생성된 입력 토큰을 사용할 스 코어를 산출하는 것을 특징으로 한다.[수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 51, "content": "(여기서, 는 디코딩 과정의 t시점에 인코딩 과정의 j시점의 상기 입력 토큰을 카피하는 것에 대한 스코어를 의미하고, X는 입력 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터(state vector)를 의미하고, ht는 인코더에서 나온 t시점의 결과 벡터를 의미한다.) 상기 출력 토큰을 추출하는 단계는 하기의 수학식을 통해 상기 수어 토큰의 스코어를 산출하는 것을 특징으로 한다. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 52, "content": "(여기서, 는 디코더에서 t 시점에 수어 데이터 셋의 i번째 토큰을 출력하는 것에 대한 스코어를 의미하고, V는 수어 데이터 셋에 포함된 수어 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터 (state vector)를 의미한다.) 이때, t시점의 생성 토큰을 Gt라고 했을 때, 하기의 수학식에 기재된 네가지 경우로 나뉘어지게 된다. 상기 출력 토큰을 추출하는 단계는 하기의 수학식을 이용하여 최종 토큰 출력 확률을 산출하며, 가장 확률이 높 은 토큰을 출력하는 것을 특징으로 한다. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 53, "content": "[수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 54, "content": "[수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 55, "content": "[수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 56, "content": "상기 출력 토큰을 추출하는 단계는 산출된 스코어를 기초로 상기 수어 글로스를 생성하기 위한 출력 토큰의 확 률을 산출하되, 하기의 수학식을 기초로 카피와 관련된 정보를 다음 출력 토큰을 추측할 때 제공해 주기 위한 selective read 값을 산출하는 것을 특징으로 한다. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 57, "content": "(여기서, 는 하기의 수학식을 통해 산출된다.) [수학식] 인 경우,"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 58, "content": "그러지 않은 경우, = 0 (여기서 K는 하기의 수학식을 통해 산출된다.) [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 59, "content": "상기 수어 토큰을 추출하는 단계는 상기 혼합 특징 벡터를 기초로 상기 자연어 텍스트의 문장 유형을 추정하고, 상기 추정된 문장 유형에 따른 비수지기호를 추출하고, 상기 추출된 비수지기호를 상기 수어 토큰에 임베딩하는 것을 특징으로 한다. 상기 출력 토큰을 추출하는 단계는 상기 추정된 문장 유형에 따라 상기 수어 글로스를 수어로 동작하는데 따른 속A_03_도 지수를 A_03_도출하고, 상기 A_03_도출된 속A_03_도 지수를 상기 수어 토큰에 임베딩하고, 상기 속 A_03_도 지수를 나타내는 문자를 상기 수어 글로스에 포함시키는 것을 특징으로 한다. 상기 출력 토큰을 추출하는 단계는 상기 혼합 특징 벡터에 포함된 상기 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 상기 문장 유형을 식별하고, 상기 식별된 문장 유형을 기초로 상기 속A_03_도 지수를 결 정하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 수어 글로스 번역 방법을 실행하기 위하여 기록매 체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있 다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 자연어 텍스트(text)를 입력 받는 단계, 상기 프로세서 가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습 (machine learning)된 트랜스포머(transformer) 모델을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 글로스를 생성하는 단계를 포함하고, 상기 수어 글로스를 생성하는 단계는 상기 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적으로 사용하여 상기 수어 글로스를 생성하는 것을 특징으로 하는, 수어 번역 방법을 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다.기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_03_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 60, "content": "A_03_발명의 효과 본 발명의 실시 예들에 따르면, 자연어 및 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 높은 정확A_03_도로 자연어를 수어 글로스로 변 환할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 61, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 62, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 63, "content": "A_03_도면의 간단한 설명 A_03_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_03_도이다. A_03_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_03_도이다. A_03_도 3은 본 발명의 일 실시예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_03_도이다. A_03_도 4 및 A_03_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_03_도이다. A_03_도 6은 본 발명의 일 실시예에 따른 수어글로스생성부의 기능을 설명하기 위한 예시A_03_도이다. A_03_도 7은 본 발명의 일 실시예에 따른 제2 인공지능을 설명하기 위한 예시A_03_도이다. A_03_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_03_도이다. A_03_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_03_도이다. A_03_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_03_도이다. A_03_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_03_도이다. A_03_도 12는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 예시A_03_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 64, "content": "A_03_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_03_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_03_도하게 포괄적인 의미로 해석되거나, 과A_03_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_03_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, '구성된다' 또는 '가지다' 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_03_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_03_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 '연결되어' 있다거나 '접속되어' 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_03_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_03_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"'직접 연결되어' 있다거나 '직접 접속되어' 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다.이하, 첨부된 A_03_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_03_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_03_도면은 본 발명의 사상을 쉽게 이해할 수 있A_03_도록 하기 위한 것일 뿐, 첨부된 A_03_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_03_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_03_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 정보통신 수단에 의한 사회복지 향상에 대해 많은 사람들이 관심을 보이고 있다. 구체적으로, 일상 생활 및 사회 참여에 곤란을 겪고 있는 사람들의 특수한 요구에 부응하여, 그들의 일상 생활 및 사회 참여 를 지원하는 다양한 시스템 개발 및 구축이 중요한 문제로 대두되고 있다. 특히, 청각 장애인들이 자신들의 주된 의사소통 수단인 수어를 이용하여 정보통신 서비스를 받을 수 있A_03_도 록, 자연어를 수어로 자동 번역할 수 있는 시스템에 대한 다양한 연구가 진행되고 있다. 그러나, 수어는 자연어와 사용하는 문법, 단어, 어순, 표현 방법 등에 차이가 있다. 이에 따라, 수어의 문법, 단어, 어순, 표현 방법 등을 고려하여, 높은 정확A_03_도로 자연어를 수어로 변환할 수 있는 시스템의 개발이 요구되고 있다. 이러한 한계를 극복하고자, 본 발명은 높은 정확A_03_도로 자연어(natural language)를 수어(sign language)로 번역할 수 있는 다양한 수단들을 제안하고자 한다. A_03_도 1은 본 발명의 일 실시예에 따른 수어번역시스템의 구성A_03_도이다. A_03_도 1을 참조하면, 본 발명의 일 실시예에 따른 수어변역시스템은 적어A_03_도 하나의 단말기(terminal, 100a, 100b, 100c, …, 100n; 100) 및 번역서버를 포함하여 구성될 수 있다. 이와 같은, 본 발명의 일 실시예에 따른 수어번역시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나 타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 단말기는 사용자로부터 자연어(natural language) 텍스트를 입력 받 거나, 번역서버에 의해 번역된 수어(sign language) 텍스트(text) 또는 수어 영상(video)을 출력하여 사용 자에게 제공할 수 있는 장치이다. 여기서, 자연어는 인간이 일상생활에서 의사 소통을 위해 사용하는 언어가 될 수 있다. 특히, 자연어는 한국어, 영어, 독일어, 스페인어, 프랑스어, 이탈리아어 등 다양한 국가의 언어가 해당될 수 있다. 구체적으로, 자연어 는 각 국가의 언어 중에서A_03_도 구어체(colloquial style), 문어체(literary style) 등이 해당될 수 있다. 이러한, 단말기는 사용자로부터 자연어를 입력 받기 위한 입력 장치(input device) 및 번역 서버에 의해 번역된 수어 글로스 또는 수어 영상을 출력하기 위한 출력 장치(output device)를 포함하여 구성될 수 있 다. 또한, 단말기는 번역서버를 포함한 다른 장치들과 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_03_도 허용될 수 있다. 예를 들어, 단말기는 3GPP(3rd Generation Partnership Project)에서 규정하고 있는 사용자 장치(User Equipment, UE) 및 IEEE(Institute of Electrical and Electronics Engineers)에서 규정하고 있는 모바일 스테이션(Mobile Station, MS) 중 어느 하나에 해당될 수 있다. 그러나 이에 한정되지 아니하고, 단말기는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버 (server)와 같은 고정식 컴퓨팅 장치, 또는 랩탑(laptop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디 어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이동식 컴퓨팅 장치 중 어느 하나가 될 수A_03_도 있다. 다음 구성으로, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 수 어 글로스 및 수어 영상 중 적어A_03_도 하나로 번역하여, 번역된 수어 글로스 및 수어 영상 중 적어A_03_도 하 나를 단말기에 제공할 수 있는 장치가 될 수 있다. 이러한, 번역서버는 단말기로부터 자연어 텍스트를 입력 받고, 입력 받은 자연어 텍스트를 인코딩 (incoding)하여, 자연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력된 자연어 텍스트를 토큰화(tokenization)하고, 토큰화 작 업 전후에 자연어 텍스트를 용A_03_도에 맞게 정제(cleaning) 및 정규화(normalization)하여 전처리하고, 전처 리 된 토큰들을 압축해서 하나의 벡터로 만들 수 있다. 또한, 번역 서버는 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습 (machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 글로스를 생성하여 단말기에 제공할 수 있다. 그리고, 번역 서버는 번역된 수어 글로스를 수어 영상으로 생성하여, 단말기에 제공할 수 있다. 이와 같은, 번역서버는 단말기와 데이터를 송수신할 수 있으며, 송수신된 데이터를 기반으로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_03_도 허용될 수 있다. 예를 들어, 번역서버는 데스크탑, 워크스 테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 이러한, 특징을 가지는 번역서버의 구체적인 구성 및 동작에 대해서는 A_03_도 2 내지 A_03_도 7을 참조하 여 후술하기로 한다. 지금까지 상술한 바와 같은, 수어번역시스템을 구성하는 단말기 및 번역서버는 장치들 사이를 직 접 연결하는 보안 회선, 공용 유선 통신망 또는 이동통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터 를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC) 및 광가입자망(Fiber To The Home, FTTH) 중 하나 이상이 포함될 수 있으 나, 이에 한정되는 것은 아니다. 또한, 이동통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다 중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE) 및 5세대 이동통신(5th generation mobile telecommunication) 중 하나 이상이 포함될 수 있 으나, 이에 한정되는 것A_03_도 아니다. 이하, 상술한 바와 같은 특징을 가지는, 번역서버의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_03_도 2는 본 발명의 일 실시예에 따른 번역서버의 논리적 구성A_03_도이고, A_03_도 3은 본 발명의 일 실시 예에 따른 데이터전처리부의 기능을 설명하기 위한 예시A_03_도이고, A_03_도 4 및 A_03_도 5는 본 발명의 일 실시예에 따른 제1 인공지능을 설명하기 위한 예시A_03_도이고, A_03_도 6은 본 발명의 일 실시예에 따른 수어 글로스생성부의 기능을 설명하기 위한 예시A_03_도이고, A_03_도 7은 본 발명의 일 실시예에 따른 제2 인공지능 을 설명하기 위한 예시A_03_도이다. 우선적으로, A_03_도 2를 참조하면, 본 발명의 일 실시예에 따른 번역 서버는 통신부, 입출력부 , 저장부, 데이터전처리부, 수어글로스생성부 및 수어영상생성부를 포함하여 구성될 수 있다. 이와 같은, 본 발명의 일 실시예에 따른 번역서버의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각 구성 요소에 대하여 설명하면, 통신부는 단말기와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 단말기로부터 자연어 텍스트를 입력 받을 수 있고, 입력 받은 자연어 텍스트를 번역한 수어 글로스 및 수어 영상 중 적어A_03_도 하나를 단말기로 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해, 관리자로부터 명령을 입력 받거나 또는 연산 결과를 출력할 수 있다. 이 경우, 관리자는 번역 서비스를 제공하는 서비스 제공자로 지칭될 수 있으며, 이에한정되지 않는다. 구체적으로, 입출력부는 관리자로부터 인공지능을 학습하기 위한 데이터 셋을 입력 받을 수 있다. 예를 들 어, 입출력부는 제1 인공지능의 학습을 위하여, 다양한 형태의 자연어 문장에 관한 데이터 셋을 입력 받을 수 있다. 또한, 입출력부는 제2 인공지능의 학습을 위하여, 자연어 및 자연어와 매칭되는 수어 데이터 셋 을 입력 받을 수 있다. 또한, 입출력부는 데이터전처리부로부터 생성된 결과 값, 수어글로스생성부로부터 생성된 수어 글로스 및 수어영상생성부로부터 생성된 수어 영상 중 적어A_03_도 하나를 출력할 수 있다. 다음 구성으로, 저장부는 번역서버의 동작에 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 데이터전처리부, 수어글로스생성부 및 수어영상생성부에 의해 주기 적으로 갱신되는 데이터베이스를 저장할 수 있다. 또한, 저장부는 인공지능(AI) 학습을 위한 데이터 셋을 저장할 수 있다. 그리고, 저장부는 데이터전처리부, 수어글로스생성부 및 수어영상생성부 에서 사용되는 인공지능 모델을 저장할 수 있다. 다음 구성으로, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자 연어 텍스트와 대응하는 벡터(vector)를 생성할 수 있다. 즉, 데이터전처리부는 수어글로스생성부가 수어 글로스를 생성하기 위하여, 입력 받은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 구체적으로, 데이터전처리부는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 예를 들 어, A_03_도 3에 A_03_도시된 바와 같이, 데이터전처리부는 \"죽음기와 발명품이 잔뜩 우리 아이들과의 과 학 놀이터\"라는 문장을 입력 받은 경우, 문장에 포함된 각 단어를 토큰화(tokenization)하여 \"a1, a2, 쪋, a7\" 과 같은 제1 토큰을 생성할 수 있다. 이때, 데이터전처리부는 인공지능 성능을 향상시키기 위하여, 제1 토 큰 생성 이전에, 자연어 텍스트 중 적어A_03_도 둘 이상의 의미를 갖는 단어를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다. 또한, 데이터전처리부는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩 (contextual embedding)을 수행할 수 있다. 즉, 데이터전처리부는 자연어 문장으로 사전 기계 학습된 인공 지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 예를 들어, 데이터전처리부는 자연어 텍스트를 토큰화하여 생성된 \"a1, a2, 쪋, a7\"과 같은 제1 토큰을 제 1 인공지능에 입력하여 \"h1, h2, 쪋, h7\"를 포함하는 제1 컨텍스트 벡터를 생성할 수 있다. 이때, 제1 컨텍스트 벡터는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 예를 들어, 데이터전처리부는 BERT(Bidirectional Encoder Representations from Transformers) 모델에 기반한 인공지능(AI)을 이용하여, 제1 컨텍스트 벡터를 생성할 수 있다. 보다 상세하게 A_03_도 4 및 A_03_도 5를 참조하면, BERT 모델은 트랜스포머(transformer)를 기반으로, 인코더 (encoder)만을 사용하는 모델에 해당된다. BERT 모델은 일반적인 트랜스포머와 다르게, 토큰 임베딩(token embeddings), 토큰의 포지션 임베딩(position embeddings) 및 세그먼트 임베딩(segment embedding)으로 이루어 진 입력 값을 가진다. 이러한, BERT 모델은 복수 개의 인코딩 블록으로 구성될 수 있다. 기본 BERT 모델은 12개의 인코딩 블록으로 구 성되고, 대형 BERT 모델은 24개의 인코딩 블록으로 구성될 수 있으나, 이에 한정되는 것은 아니다. 각각의 인코 더 블록은 이전의 출력 값을 현재의 입력 값으로 가지며, BERT 모델은 인코더 블록의 개수만큼 재귀적으로 반복 처리되는 형태로 복수 개의 인코더들이 구성될 수 있다. 그리고, 각각의 인코더 블록의 출력 값은 매번 잔차 연 결(residual connections)되게 처리될 수 있다. 각 인코더 블록을 구성하는 멀티 헤드 어텐션(multi-head attention)은 다음의 수식 1과 같이, 서로 다른 가중 치 행렬(weight matrix)를 이용하여 어텐션(attention)을 h번 계산한 다음 이를 서로 연결(concatenates)한 결 과를 출력할 수 있다. [수학식 1] MultiHead(Q, K, V) = [head1; …; headh]wO 여기서, headi는 Attention(QWiQ, KWiK, VWiV)4이다. Q는 디코더의 히든 스테이지(hidden stage), K는 인코더의 히든 스테이지, V는 K에 어텐션을 부여받은 정규화된 가중치(normalized weight)이며, Q, K, V에 대한 스케일드 닷-프로덕트 어텐션(scaled dot-product attention)은 다음의 수식 2를 통해 산출될 수 있다. [수학식 2] Attention(Q, K, V) = softmax(QKT/root(dk))V 그리고, 어텐션 결과를 받은 피드-포워드 네트워크(Feed Forward Network, FFN)는 두 개의 리니어 트랜스포메이 션(linear transformation)으로 구성되어, GELU(Gaussian Error Linear Units)가 적용된 다음의 수식 3을 기반 으로 구현될 수 있다. [수학식 3] FFN(x) = max(0, xW1 + b1)W2 + b2 또한, 데이터전처리부는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식 (NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다. 예를 들어, 데이터전처리부는 점별 예측(pointwise prediction) 모델, 확률 기반의 모델(probabilistic model), 신경망 기반의 모델(neural network based model)을 기반으로, 자연어 텍스트를 형태소 단위로 나눈 뒤, 각 형태소에 해당 품사를 태깅(tagging)할 수 있다. 또한, 데이터전처리부는 자연어 텍스트의 개체명(named entity)을 인식하고, 인식된 개체명의 종류를 분류 할 수 있다. 즉 데이터전처리부는 자연어 텍스트에 포함된 각 단어가 어떤 유형에 속하는지 인식할 수 있 다. 데이터전처리부는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 데이터전처리부 는 제2 토큰을 고정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다. 이후, 데이터전처리부는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합 (concat)한 혼합 특징 벡터(mixed feature vector)를 생성하고, 생성된 혼합 특징 벡터를 수어글로스생성부 에 전달할 수 있다. 예를 들어, 데이터전처리부는 \"h1, h2, 쪋, h7\"을 포함하는 제1 컨텍스트 벡터와, \"z1, z2, 쪋, z8\"를 포 함하는 제2 컨텍스트 벡터를 혼합하여, \"x1, x2, 쪋, x7\"을 포함하는 혼합 특징 벡터를 생성할 수 있다. 여기서, 데이터전처리부는 생성된 혼합 특징 벡터를 수어글로스생성부로 전달하여, 제2 인공지능의 입력으로 사용하A_03_도록 함과 동시에, 제2 인공지능에 의한 결과 값 중 일부를 대체하는 데 사용하A_03_도록 할 수 있다. 다음 구성으로, 수어글로스생성부는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 데이터전처리부로부터 전달받은 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 글로스를 생성할 수 있다. 구체적으로 A_03_도 7에 A_03_도시된 바와 같이, 수어글로스생성부는 데이터전처리부부터 전달받은 혼합 특징 벡터를 디코딩하기 위한, 트랜스포머(transformer) 모델의 디코더(decoder)에 해당될 수 있다. 이러 한, 트랜스포머 모델은 복수 개의 디코딩 블록으로 구성될 수 있다. 각 디코더 블록을 구성하는 첫번째 서브층인 마스크드 멀티 헤드 셀프 어텐션(masked multi-head self- attention)은 전술한 인코더의 서브층인 멀티 헤드 어텐션과 동일한 연산을 수행하되, 어텐션 스코어 행렬에서 마스킹을 적용하는 점에서 일부 상이하다. 즉, 서브층인 마스크드 멀티 헤드 셀프 어텐션은 현재 처리중인 단어 보다 앞쪽에 해당하는 단어에 대해서만 어텐션 점수를 참고할 수 있A_03_도록 하기 위하여 마스킹을 적용할 수 있다. 그리고, 디코더는 두번째 서브층인 멀티 헤드 어텐션(multi-head attention)을 통해 엔코더의 출력 값인 혼합 특징 벡터를 입력 받고, 입력 받은 혼합 특징 벡터를 멀티 헤드 어텐션(multi-head attention) 및 세번째 서브 층인 피드-포워드 네트워크(Feed Forward Network, FFN)를 통과시키고, 리니어 레이어(Linear Layer) 및 소프 트맥스 레이어(softmax layer)를 거쳐 학습된 수어 단어 데이터베이스 중 가장 관계가 높은 수어 토큰을 출력할수 있다. 이때, 리니어 레이어는 완전 접속망(fully-connected network)으로 디코더가 마지막으로 출력한 벡터를 그보다 훨씬 더 큰 사이즈의 벡터인 로짓(logits) 벡터로 투영시킬 수 있다. 여기서, 로짓 벡터의 각 셀은 각 단어에 대한 점수가 될 수 있다. 그리고, 소프트맥스 레이어는 이 점수들을 확률로 변환해주며, 가장 높은 확률 값을 가지는 셀에 해당하는 단어 를 최종 수어 글로스로서 출력할 수 있다. 이때, 수어글로스생성부는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 수어 토큰을 혼합 특징 벡터 에 포함된 토큰 중 하나로 대체할 수 있다. 즉, 수어글로스생성부는 수어 글로스를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of-vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 데이터전처리부의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 수어글로스생성부는 디코더에 카피 어텐션(copy attention)을 별A_03_도로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사 전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력 할 확률A_03_도 함께 계산할 수 있다. 한편, 일반적인 번역 모델의 경우, 입력으로 주어진 텍스트 시퀀스와 출력으로 주어지는 텍스트 시퀀스가 다른 언어이다. 반면에, 수어 번역은 입력과 출력이 모두 동일한 한국어 기반 데이터 셋이라는 점에서 차이가 있다. 이에 따라, 입력에 이용된 고유 명사가 출력에A_03_도 동일하게 나타나며, 고유명사가 아닌 경우에A_03_도 입력 에 나타난 토큰이 출력에A_03_도 동일하게 나타나는 경우가 다수 존재한다. 따라서, 수어글로스생성부는 위와 같은 특성을 고려하여, 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적으로 사용하여 수어 글로스를 생성할 수 있다. 이를 위해, 번역 서버는 하기의 수학식 4를 통해 상기 인코딩 과정에서 생성된 입력 토큰을 사용할 스코어를 산 출할 수 있다. [수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 65, "content": "(여기서, 는 디코딩 과정의 t시점에 인코딩 과정의 j시점의 상기 입력 토큰을 카피하는 것에 대한 스코어를 의미하고, X는 입력 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터(state vector)를 의미하고, ht는 인코더에서 나온 t시점의 결과 벡터를 의미한다.) 즉, 입력 토큰을 사용할 스코어를 산출하기 위하여, 인코더의 j시점의 입력 토큰의 결과 벡터(은닉 벡터)를 Wc와 비 선형 함수인 σ를 통해 임베딩하고, 이를 st와 내적함으로써 스코어를 산출한다. 또한, 수어글로스생성부는 하기의 수학식 5를 통해 수어 토큰의 스코어를 산출할 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 66, "content": "(여기서, 는 디코더에서 t 시점에 수어 데이터 셋의 i번째 토큰을 출력하는 것에 대한 스코어를 의미하고, V는 수어 데이터 셋에 포함된 수어 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터 (state vector)를 의미한다.)수어글로스생성부는 수학식 4 및 수학식 5를 통해 산출된 스코어를 기초로 수어 글로스를 생성하기 위한 출력 토큰의 확률을 산출할 수 있다. 이때, t시점의 생성 토큰을 Gt라고 했을 때, 하기의 수학식 6 내지 9에 기재된 네가지 경우로 나뉘어지게 된다. 수어글로스생성부는 하기의 수학식을 이용하여 최종 토큰 출력 확률을 산출하며, 가장 확률이 높은 토큰을 출력할 수 있다. [수학식 6]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 67, "content": "[수학식 7]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 68, "content": "[수학식 8]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 69, "content": "[수학식 9]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 70, "content": "또한, 수어글로스생성부는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출할 수 있다. 그리고, 수어글로스생성부는 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의무문, 명령문, 청유문 및 감탄문 중 적어A_03_도 하나를 포 함할 수 있다. 이때, 수어글로스생성부는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 하지만, 이에 한정된 것은 아니고, 수어글로스생성부는 데 이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유형을 식별할 수A_03_도 있다. 즉, 수어글로스생성부는 추정된 문자 유형에 따라 수어 글로스를 수어로 동작하는데 따른 속A_03_도 지수 를 A_03_도출하고, A_03_도출된 속A_03_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 수어글로스생성부(22 5)는 속A_03_도 지수를 나타내는 문자를 수어 글로스에 포함시킬 수 있다. 예를 들어, 수어글로스생성부는 생성된 수어 글로스의 각 단어 사이에 속A_03_도를 의미하는 속A_03_도 지수를 삽입하여 출력할 수 있다. 여기 서, 속A_03_도 지수는 특정 속A_03_도 범위를 나타내는 문자가 될 수 있다. 예를 들어, 빠른 속A_03_도를 나타 내는 속A_03_도 지수는 'a', 보통 속A_03_도를 나타내는 속A_03_도 지수는 'b', 느린 속A_03_도를 나타내는 속 A_03_도 지수는 'c'가 될 수 있다. 즉, 수어글로스생성부는 \"(a)죽임기 또 발명 물건 크다 많다 아이 과학 놀다 곳\"과 같이, 출력된 수어 글로스의 전단에 수어 동작을 수행하는데 따른 속A_03_도 지수를 표시하거나, \"죽임기(a)또(a)발명(b)발명(c)물건(a)크다(a)많다(b)아이(a) 과학(a)놀다(b)곳(c)과 같이, 각 단어 사이에 속 A_03_도 지수를 나타내는 문자를 표시하여, 수어 글로스를 수어 동작으로 표현하는 것을 지원할 수 있다. 다음 구성으로, 수어영상생성부는 변환된 수어 글로스와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로, 수어영상생성부는 변환된 수어 글로스에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 이후, 수어영상생성부는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키 포인트(keypoint)를 추출할 수 있다. 즉, 수어영상생성부는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 예를 들어, 수어영상생성부는 오픈포즈(openpose) 모델을 통해 2D 키포인트를 추출할 수 있다. 여기서, 오 픈포즈 모델은 단일 이미지에서 실시간으로 몸체, 손, 얼굴 그리고 발들의 키포인트들을 최대 130개까지 인식할 수 있으며, 입력된 이미지 또는 비디오로부터 2D 키포인트를 추출하여, 배경이미지와 키포인트가 합쳐진 이미지 또는 키포인트만 가진 이미지를 JSON, XML, 영상 데이터 등으로 저장할 수 있다. 또한, 수어영상생성부는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 수어영상생성부 는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인 트를 기초로 손실(loss)이 최소화되A_03_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환 할 수 있다. 여기서, 수어영상생성부는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하 는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 수어영상 생성부는 손의 조인트 21개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 수어영상생성부는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 수어영 상생성부는 중수지관절을 대상으로 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_도와의 상관 관계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_도를 추정할 수 있다. 이때, 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_도를 추정하기 위한 인공지능은 손목의 회전 각A_03_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 이후, 수어영상생성부는 생성된 3D 조인트 및 동작 정보를 기초로 수어 글로스의 각 단어별 영상을 생성할 수 있다. 즉, 수어영상생성부는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 수어영상생성부는 가상 인간이 수화 를 수행하는 영상을 생성할 수 있다. 그리고, 수어영상생성부는 각 단어별 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 수어영상 생성부는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_03_도 하나의 이미지를 생 성할 수 있다. 여기서, 수어영상생성부는 연속되는 각 단어별 영상 사이에 적어A_03_도 하나의 이미지를 생성하되, 선행 되는 제1 단어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이 미지를 삽입할 수 있다. 그리고, 수어영상생성부는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상의 최초 프레임 사이에 적어A_03_도 하나의 이미지를 생성할 수 있다. 즉, 수어영상생 성부는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식 별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 수어영상생성부는 자연어 텍스트 의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영 상의 재생 속A_03_도를 결정할 수 있다. 여기서, 수어영상생성부는 데이터전처리부에 의한 품사 분석 및 개체명 인식 결과를 가져와 문장 유 형을 식별하거나, 수어글로스생성부에 의해 분석된 문장 유형 결과를 가져올 수 있다. 예를 들어, 수어영상생성부는 문장 유형이 청유문으로 식별되는 경우, 기본 자세의 유지 시간을 길게 하거 나, 수어 영상의 재생 속A_03_도를 느리게 하여, 정중한 표현이 될 수 있A_03_도록 할 수 있다. 이와 같이, 수 어영상생성부는 단순히 동작 영상을 출력할 뿐만 아니라, 비수지신호를 고려하여 동작 영상을 생성할 수있다. 또한, 수어영상생성부는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수 어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_03_도 중 적어A_03_도 하나를 결정할 수 있다. 이하, 상술한 바와 같은 번역서버의 논리적 구성 요소를 구현하기 위한 하드웨어에 대하여 보다 구체적으 로 설명한다. A_03_도 8은 본 발명의 일 실시예에 따른 번역서버의 하드웨어 구성A_03_도이다. A_03_도 8에 A_03_도시된 바와 같이, 본 발명의 일 실시예에 따른 번역서버는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기, 입출력장치(Input/output device, 165), 데이터 버스(Bus, 270) 및 스토 리지(Storage, 275)를 포함하여 구성될 수 있다. 구체적으로, 프로세서는 메모리에 상주된 수어 글로스 또는 수어 영상 번역 방법이 구현된 소프트웨 어(280a)에 따른 명령어를 기초로, 번역서버의 동작 및 기능을 구현할 수 있다. 메모리에는 스토리지에 저장된 번역 방법이 구현된 소프트웨어(280b)가 상주(loading)될 수 있다. 송수신기는 복수 개의 단말기와 데이터를 송수신할 수 있다. 입출력장치는 프로세서의 명령에 따라, 번역서버의 동작에 필요한 신호를 입력 받거나 연산 결 과를 외부로 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 각각 연 결되어, 각각의 구성 요소 사이에서 신호를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 다양한 실시예에 따른 번역 방법이 구 현된 소프트웨어(280b)를 저장할 수 있다. 그리고, 스토리지는 인공지능 및 인공지능을 학습하기 위한 데 이터 셋을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 글로스 번역 방법 을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단계, 프로세서 가, 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 글로스를 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 그리고, 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 수어 영 상 번역 방법을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 자연어 텍스트(text)를 입력 받는 단 계, 프로세서가, 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해 자연어 텍스트를 수어 글로스로 변환하는 단계, 프로세서가, 변환된 수어 글로스와 매칭되는 수어 영상을 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 상세하게, 프로세서는 중앙 처리 장치(Central Processing Unit, CPU), ASIC(Application-Specific Integrated Circuit), 칩셋(chipset), 논리 회로 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않 는다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리(flash memory), 메모리 카 드(memory card) 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 입출력장치는 버튼(button), 스위치(switch), 키보드(keyboard), 마우스(mouse), 조이스틱(joystick) 및 터치스크린(touch screen) 등과 같은 입력 장치와, LCD(Liquid Crystal Display), LED(Light Emitting Diode),유기 발광 다이오드(Organic LED, OLED), 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED), 프린터 (printer), 플로터(plotter) 등과 같은 출력 장치 중 하나 이상을 포함하여 구성될 수 있으며, 이에 한정되지 않는다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 제각각 수행하는 모듈 (과정, 기능 등)들로 구현될 수 있다. 각각의 모듈은 메모리에 상주되고 프로세서에 의해 실행될 수 있다. 메모리는 프로세서의 내부 또는 외부에 존재할 수 있고, 널리 알려진 다양한 수단으로 프로세 서와 연결될 수 있다. A_03_도 8에 A_03_도시된 각 구성 요소는 다양한 수단(예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등)에 의해 구현될 수 있다. 하드웨어에 의해 구현될 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의해 구현될 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨터 소프트웨어 업계의 통상의 지식을 가진 자에게 공지되어 사용 가능한 것일 수A_03_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_03_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용 해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_03_도록 구성될 수 있으며, 그 역A_03_도 마찬가지 이다. 이하, 상술한 바와 같은 번역서버의 동작에 대하여 보다 구체적으로 설명한다. A_03_도 9는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 순서A_03_도이고, A_03_도 10은 본 발명의 일 실시예에 따른 수어 영상 생성 단계를 설명하기 위한 순서A_03_도이다. A_03_도 9를 참조하면, S100 단계에서 번역서버는 단말기로부터 자연어 텍스트를 입력 받을 수 있다. 다음으로, S200 단계에서 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 전처리할 수 있다. 구체적으로, 번역서버는 단말기로부터 입력 받은 자연어 텍스트를 인코딩(encoding)하여 자연어 텍스트와 대응 하는 벡터(vector)를 생성할 수 있다. 즉, 번역서버는 S300 단계에서 수어 글로스를 생성하기 위하여, 입력 받 은 자연어 텍스트를 전처리하는 역할을 수행할 수 있다. 즉, 번역서버는 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성할 수 있다. 이때, 번역서버는 인공지능 성능을 향상시키기 위하여, 제1 토큰 생성 이전에, 자연어 텍스트 중 적어A_03_도 둘 이상의 의미를 갖는 단어 를 검출하고, 검출된 단어를 의미 단위로 띄어쓰기 처리할 수 있다. 또한, 번역서버는 생성된 제1 토큰을 제1 인공지능에 입력하여, 문맥 정보를 반영하는 임베딩(contextual embedding)을 수행할 수 있다. 즉, 번역 서버는 자연어 문장으로 사전 기계 학습된 인공지능을 통해 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스트 벡터(context vector)를 생성할 수 있다. 이때, 제1 컨텍스트 벡터 는 제1 인공지능으로부터 연산된 마지막 히든 레이어(hidden layer)가 될 수 있다. 또한, 번역서버는 단말기로부터 입력 받은 자연어 텍스트의 각 단어 및 각 단어의 언어 자질을 토큰화 한 제2 토큰을 생성할 수 있다. 여기서, 제2 토큰은 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 자연어 텍스트를 임베딩(embedding)하여 생성될 수 있다.또한, 번역서버는 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성할 수 있다. 즉, 번역서버는 제2 토큰을 고 정된 차원의 실수 벡터로 변환하여 제2 컨텍스트 벡터를 생성할 수 있다. 이후, 번역 서버는 상술한 바와 같이 생성된 제1 컨텍스트 벡터 및 제2 컨텍스트 벡터를 혼합(concat)한 혼합 특징 벡터(mixed feature vector)를 생성할 수 있다. 다음으로, S300 단계에서 번역서버는 S200 단계에서 생성된 혼합 특징 벡터를 입력 받아 수어 글로스를 생성할 수 있다. 이때, 번역서버는 자연어 및 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 인공지능(Artificial Intelligence, AI)을 통해, 혼합 특징 벡터를 디코딩(decoding)하여 자연어 텍스트와 매칭되는 수어 글로스를 생성할 수 있다. 이때, 번역서버는 혼합 특징 벡터와 매칭되는 수어 토큰을 추출하되, 추출된 수어 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰을 혼합 특징 벡터에 포함된 토큰 중 하나로 대체할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 토큰 중 매칭 확률 값이 사전 설정된 값보다 낮은 수어 토큰에 적합한 확률을 산출하 고, 확률이 사전 설정된 값 이상인 토큰으로 대체할 수 있다. 즉, 번역서버는 수어 글로스를 생성할 때, 필요한 어휘가 출력 사전(output vocabulary)에 없는 문제(out-of- vocabulary)와 고유명사들의 출력 확률이 작아지는 문제를 해결하기 위하여, 출력에 필요한 어휘를 S200 단계의 출력에서 찾아 복사(copy)할 수 있다. 여기서, 번역서버는 디코더에 카피 어텐션(copy attention)을 별A_03_도 로 구비하여, 디코딩 과정에서 각 시간별 출력 어휘를 예측할 때, 출력 사전에 있는 어휘들의 확률과 함께 혼합 특징 벡터 열 중에서 카피 어텐션 점수가 가장 높은 어휘를 그대로 출력할 확률A_03_도 함께 계산할 수 있다. 또한, 번역서버는 혼합 특징 벡터를 기초로 자연어 텍스트의 문장 유형을 추정하고, 추정된 문장 유형에 따른 비수지기호를 추출하고, 추출된 비수지기호를 수어 토큰에 임베딩할 수 있다. 여기서, 문장 유형은 평소문, 의 무문, 명령문, 청유문 및 감탄문 중 적어A_03_도 하나를 포함할 수 있다. 이때, 번역서버는 혼합 특징 벡터에 포함된 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별할 수 있다. 즉, 번역서버는 추정된 문자 유형에 따라 수어 글로스를 수어로 동작하는데 따른 속A_03_도 지수를 A_03_도출하 고, A_03_도출된 속A_03_도 지수를 수어 토큰에 임베딩할 수 있다. 이후, 번역서버는 속A_03_도 지수를 나타내 는 문자를 수어 글로스에 포함시킬 수 있다. 그리고, S400 단계에서 번역서버는 변환된 수어 글로스와 매칭되는 수어 영상을 생성할 수 있다. 구체적으로 A_03_도 10에 A_03_도시된 바와 같이, S410 단계에서 번역 서버는 변환된 수어 글로스에 포함된 각 단어와 매칭되는 사전 저장된 단어 수화 영상을 추출할 수 있다. 다음으로, S420 단계에서 번역서버는 추출된 단어 수화 영상에 포함된 프레임 각각에서 2D 키포인트(keypoint) 를 추출할 수 있다. 즉, 번역서버는 2D 키포인트가 포함된 수화 영상 데이터 셋을 기초로 사전 기계 학습된 인 공지능을 통해, 단어 수화 영상에서 2D 키포인트를 추출할 수 있다. 여기서, 2D 키포인트를 추출하기 위한 인공 지능은 결과 값인 추출된 2D 키포인트 및 후술할 변환된 3D 조인트를 2D 이미지에 프로젝션(projection) 시킨 이미지를 포함하는 데이터 셋을 통해 학습될 수 있다. 다음으로, S430 단계에서 번역서버는 추출된 2D 키포인트를 3D 조인트(joint)로 변환할 수 있다. 이때, 번역서 버는 3D 조인트를 2D 이미지 위에 프로젝션(projection) 시킨 이미지 및 인공지능을 통해 추출된 2D 키포인트를 기초로 손실(loss)이 최소화되A_03_도록 학습된 인공지능을 통해, 추출된 2D 키포인트를 3D 조인트로 변환할 수 있다. 여기서, 번역서버는 2D 키포인트 중 중수지관절(metacarpophalangeal joint)에 해당하는 2D 키포인트를 추출하고, 중수지관절에 해당하는 2D 키포인트를 3D 조인트로 변환할 수 있다. 즉, 번역서버는 손의 조인트 21 개 전부를 사용하는 것이 아닌 중수지관절을 사용할 수 있다. 다음으로, S440 단계에서 번역서버는 변환된 3D 조인트를 기초로 3D 조인트에 따른 동작 정보를 생성할 수 있다. 즉, 번역서버는 중수지관절을 대상으로 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_도와의 상관 관 계를 기초로 사전 학습된 인공지능을 통해, 3D 조인트에 따른 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_ 도를 추정할 수 있다. 이때, 손목의 회전 각A_03_도 및 팔꿈치의 회전 각A_03_도를 추정하기 위한 인공지능은 손목의 회전 각A_03_도 특징을 팔꿈치를 포함하는 몸의 특징과 관계성을 형성하여 학습될 수 있다. 다음으로, S450 단계에서 번역서버는 생성된 3D 조인트 및 동작 정보를 기초로 수어 글로스의 각 단어별 수어 영상을 생성할 수 있다. 즉, 번역서버는 3D 조인트 및 동작 정보를 기초로 3D 매쉬(mesh)를 생성하고, 생성된 3D 매쉬를 2D 이미지에 투영시켜 영상으로 변환시킬 수 있다. 예를 들어, 번역서버는 가상 인간이 수화를 수행 하는 영상을 생성할 수 있다. 그리고, S450 단계에서 번역서버는 각 단어별 수어 영상을 조합하여 문장 수어 영상을 생성할 수 있다. 이때, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방지하기 위하여, 모션 인 터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_03_도 하나의 이미지를 생성 할 수 있다. 여기서, 번역서버는 연속되는 각 단어별 영상 사이에 적어A_03_도 하나의 이미지를 생성하되, 선행되는 제1 단 어 영상의 최후 프레임과, 후행되는 제2 단어 영상의 최초 프레임 사이에 사전 저장된 예비 동작 이미지를 삽입 할 수 있다. 그리고, 번역서버는 예비 동작 이미지를 기준으로, 제1 단어 영상의 최후 프레임 및 제2 단어 영상 의 최초 프레임 사이에 적어A_03_도 하나의 이미지를 생성할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 기본 자세의 유지 시간을 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 상기 자연어 텍스트의 문장 유형을 식별하고, 식별된 문장 유형에 따라 생성된 수어 영상의 재생 속A_03_도를 결정할 수 있다. 또한, 번역서버는 자연어 텍스트의 언어 자질을 기초로, 자연어 텍스트를 작성한 화자와, 생성된 수어 영상을 청취하는 청자 각각의 지휘를 식별하고, 식별된 지휘 기초로 예비 동작 유지 시간 및 수어 영상 재생 속A_03_도 중 적어A_03_도 하나를 결정할 수 있다. A_03_도 11은 본 발명의 일 실시예에 따른 수어 영상 생성 방법을 설명하기 위한 예시A_03_도이다. A_03_도 11을 참조하면, 번역서버는 연속되는 각 단어별 수어 영상 사이의 모션 저더(motion judder) 현상을 방 지하기 위하여, 모션 인터폴레이션(motion interpolation)을 통해 연속되는 각 단어별 영상 사이에 적어A_03_도 하나의 이미지를 생성할 수 있다. 이때, A_03_도 11에 A_03_도시된 바와 같이, 선행되는 제1 단어(기술) 영상의 최후 프레임이 'a'이고, 후행되는 제2 단어(전통) 영상의 최초 프레임이 'b'라고 가정하면, 번역서버는 선행되는 제1 단어 영상의 최후 프레임' a'과, 후행되는 제2 단어 영상의 최초 프레임'b'사이에 사전 저장된 예비 동작 이미지'c'를 삽입할 수 있다. 그리고, 번역서버는 예비 동작 이미지'c'를 기준으로, 제1 단어 영상의 최후 프레임'a' 및 제2 단어 영상의 최 초 프레임'b' 사이에, 모션 인터폴레이션을 통해 적어A_03_도 하나의 이미지를 생성할 수 있다. 이를 통해, 번역서버는 단순히 각 단어별 영상 사이의 연관성을 통해 예측되는 이미지를 삽입하는 것이 아니고, 예비 동작 이미지를 각 단어별 영상 사이에 삽입한 후에, 각 단어별 영상과 예비 동작과의 모션 인터폴레이션을 통해 보다 자연스러운 수어 영상을 생성할 수 있다. A_03_도 12는 본 발명의 일 실시예에 따른 번역 방법을 설명하기 위한 예시A_03_도이다. 한편, A_03_도 12를 참조하면, 일반적인 번역 모델의 경우, 입력으로 주어진 텍스트 시퀀스와 출력으로 주어지 는 텍스트 시퀀스가 다른 언어이다. 반면에, 본 발명의 일 실시예에 따른 번역 방법은 입력과 출력이 모두 동일한 한국어 기반 데이터 셋이라는 점 에서 차이가 있다. 이에 따라, 입력에 이용된 고유 명사가 출력에A_03_도 동일하게 나타나며, 고유명사가 아닌 경우에A_03_도 입력 에 나타난 토큰이 출력에A_03_도 동일하게 나타나는 경우가 다수 존재한다. 따라서, 본 발명의 일 실시예에 따른 번역 서버는 위와 같은 특성을 고려하여, 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적으로 사용하여 수어 글로스를 생성할 수 있다. 이를 위해, 번역 서버는 하기의 수학식 4를 통해 상기 인코딩 과정에서 생성된 입력 토큰을 사용할 스코어를 산 출할 수 있다.[수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 71, "content": "(여기서, 는 디코딩 과정의 t시점에 인코딩 과정의 j시점의 상기 입력 토큰을 카피하는 것에 대한 스코어를 의미하고, X는 입력 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터(state vector)를 의미하고, ht는 인코더에서 나온 t시점의 결과 벡터를 의미한다.) 즉, 입력 토큰을 사용할 스코어를 산출하기 위하여, 인코더의 j시점의 입력 토큰의 결과 벡터(은닉 벡터)를 Wc와 비 선형 함수인 σ를 통해 임베딩하고, 이를 st와 내적함으로써 스코어를 산출한다. 또한, 번역 서버는 하기의 수학식 5를 통해 수어 토큰의 스코어를 산출할 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 72, "content": "(여기서, 는 디코더에서 t 시점에 수어 데이터 셋의 i번째 토큰을 출력하는 것에 대한 스코어를 의미하고, V는 수어 데이터 셋에 포함된 수어 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터 (state vector)를 의미한다.) 번역 서버는 수학식 4 및 수학식 5를 통해 산출된 스코어를 기초로 수어 글로스를 생성하기 위한 출력 토큰의 확률을 산출할 수 있다. 이때, t시점의 생성 토큰을 Gt라고 했을 때, 하기의 수학식 6 내지 9에 기재된 네가지 경우로 나뉘어지게 된다. [수학식 6]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 73, "content": "[수학식 7]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 74, "content": "[수학식 8]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 75, "content": "[수학식 9]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 76, "content": "그리고, 번역 서버는 수학식 4 및 수학식 5를 통해 산출된 스코어를 기초로 수어 글로스를 생성하기 위한 출력 토큰의 확률을 산출하되, 하기의 수학식 10을 기초로 카피와 관련된 정보를 다음 출력 토큰을 추측할 때 제공해 주기 위한 selective read 값을 산출할 수 있다. 여기서, t-1 시점의 selective read 값은 로 표현될 수 있다. [수학식 10]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 77, "content": "(여기서, 는 하기의 수학식 11과 같이 주어진다.) [수학식 11] 인 경우,"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 78, "content": "그러지 않은 경우, = 0 (여기서 K는 하기의 수학식 12과 같이 주어진다.) [수학식 12]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 79, "content": "즉, K는 결과 토큰인 yt-1과 같은 토큰의 모든 카피 확률을 더한 것을 의미한다. 이는 카피할 토큰이 입력에 하 나가 있을 수A_03_도 있지만, 여러 개가 있을 수A_03_도 있기 때문이다. 수학식 10을 해석해보면, 디코더의 출력 토큰이 인코더의 입력 토큰과 동일한 경우, 해당 토큰의 인코더의 출력 벡터(은닉 벡터)를 가중치를 곱해 더한 값으로, 어떤 토큰을 카피하였는지 알려주는 정보를 가진 벡터이다. 다시 정리하면, 번역 서버는 디코더에서 나온 결과 벡터를 카피 어텐션을 이용하여 카피 스코어를 계산한다. 이때, A_03_도 12에서 기본적인 수어 데이터 셋의 크기는 15개이며, 입력 토큰 중 기본적인 수어 데이터 셋에 속하지 않는 토큰이 3개가 있어 입력을 고려한 추가적인 토큰이 3개 추가되어 총 18개의 스코어가 계산된다. 한편, 디코더의 기본적인 결과로 기본 수어 데이터 셋의 크기인 15개에 대한 스코어가 계산된다. 다음으로, 두 스코어를 연결하고, 확률로 변환하기 위해 소프트맥스(softmax) 함수를 통과시킨 뒤 각 토큰에 대 한 확률을 합하여 최종확률을 A_03_도출한다. 이때, 디코더의 결과는 추가적인 토큰 3개에 대한 스코어가 없으므로, 카피 스코어 부분에서 구해진 스코어를 그대로 사용한다. 결과적으로, 수어 데이터 셋에서 6번째 토큰이 0.2로 추정확률이 가장 높아 다음 토큰으로 예측된다. 예를 들어, A_03_도 12에 A_03_도시된 바와 같이 첫번째 결과로 생성된 토큰이 수어 데이터 셋의 6번째 단어이 며, 동일하게 그 단어는 입력의 4번째로 들어온 단어이기A_03_도 하다. 이때, selective read는 해당 결과 토큰이 4번째 들어온 토큰이기 때문에, 4번째 토큰의 은닉 벡터인 h4가 된다. 다음으로, 다음 디코더의 입력으로 들어가는 벡터는 6번째 단어, 동일하게 a4를 임베딩 한 것에 h4를 연결한 값 이 입력으로 들어가게 된다. 이상과 같이, 본 명세서와 A_03_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_03_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_03_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_03_부호의 설명 100 : 단말기 200 : 번역서버 205 : 통신부 210 : 입출력부 215 : 저장부 220 : 데이터전처리부 225 : 수어글로스생성부 230 : 수어영상생성부 A_03_청구범위 A_03_청구항 1 번역서버가, 자연어 텍스트(text)를 입력 받는 단계; 상기 번역서버가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계; 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 트랜스포머(transformer) 모델을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 글로스를 생성하는 단계; 를 포함하고, 상기 수어 글로스를 생성하는 단계는 상기 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적으로 사용하여 상기 수어 글로스를 생성하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법. A_03_청구항 2 제1 항에 있어서, 상기 벡터를 생성하는 단계는 상기 자연어 텍스트의 각 단어를 토큰화 한 제1 토큰을 생성하고, 상기 자연어 텍스트의 각 단어 및 상기 각 단 어의 언어 자질을 토큰화 한 제2 토큰을 생성하는 단계; 및 자연어 문장으로 사전 기계 학습된 인공지능을 통해 상기 제1 토큰과 대응하는 문맥 정보가 포함된 제1 컨텍스 트 벡터를 생성하고, 상기 제2 토큰을 임베딩하여 제2 컨텍스트 벡터를 생성하는 단계; 를 포함하는 것을 특징 으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법. A_03_청구항 3 제2 항에 있어서, 상기 벡터를 생성하는 단계는 상기 제1 컨텍스트 벡터 및 상기 제2 컨텍스트 벡터를 합성한 혼합 특징 벡터(mixed feature vector)를 생성하 고, 상기 생성된 혼합 특징 벡터를 상기 수어 글로스를 생성하기 위한 인공지능에 입력하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법.A_03_청구항 4 제3 항에 있어서, 상기 제1 토큰을 생성하는 단계 및 상기 제2 토큰을 생성하는 단계 이전에 상기 자연어 텍스트 중 적어A_03_도 둘 이상의 의미를 갖는 단어를 검출하고, 상기 검출된 단어를 의미 단위로 띄어쓰기 처리하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법. A_03_청구항 5 제4 항에 있어서, 상기 제2 토큰을 생성하는 단계는 품사(POS, Part Of Speech) 분석 및 개체명 인식(NER, Named Entity Recognition) 결과를 기초로, 상기 자연어 텍스트를 임베딩(embedding)하여 상기 제2 토큰을 생성하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글 로스 번역 방법. A_03_청구항 6 제5 항에 있어서, 상기 수어 글로스를 생성하는 단계는 상기 번역서버가, 상기 자연어 텍스트와 대응하는 혼합 특징 벡터를 입력 받는 단계; 및 상기 번역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋에 의해 사전 기계 학습된 인공지능을 통해, 상기 혼합 특징 벡터와 매칭되는 수어에 관한 출력 토큰을 추출하는 단계; 를 포함하는 것을 특징으로 하 는, 트랜스포머를 이용한 수어 글로스 번역 방법. A_03_청구항 7 제6 항에 있어서, 상기 출력 토큰을 추출하는 단계는 하기의 수학식을 통해 상기 인코딩 과정에서 생성된 입력 토큰을 사용할 스코어를 산출하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 80, "content": "(여기서, 는 디코딩 과정의 t시점에 인코딩 과정의 j시점의 상기 입력 토큰을 카피하는 것에 대한 스코어를 의미하고, X는 입력 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터(state vector)를 의미하고, ht는 인코더에서 나온 t시점의 결과 벡터를 의미한다.) A_03_청구항 8 제7 항에 있어서, 상기 출력 토큰을 추출하는 단계는 하기의 수학식을 통해 상기 수어 토큰의 스코어를 산출하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글 로스 번역 방법. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 81, "content": "(여기서, 는 디코더에서 t 시점에 수어 데이터 셋의 i번째 토큰을 출력하는 것에 대한 스코어를 의미하고, V는 수어 데이터 셋에 포함된 수어 토큰을 의미하고, St는 디코더 셀에서 나온 t시점의 상태 벡터 (state vector)를 의미한다.) A_03_청구항 9 제8 항에 있어서, 상기 출력 토큰을 추출하는 단계는 상기 산출된 스코어를 기초로 상기 수어 글로스를 생성하기 위한 출력 토큰의 확률을 산출하되, 하기의 수학식 을 기초로 카피와 관련된 정보를 다음 출력 토큰을 추측할 때 제공해 주기 위한 selective read 값을 산출하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법. [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 82, "content": "(여기서, 는 하기의 수학식과 같이 주어진다.) [수학식] 인 경우,"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 83, "content": "그러지 않은 경우, = 0 (여기서 K는 하기의 수학식과 같이 주어진다.) [수학식]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 84, "content": "A_03_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 자연어 텍스트(text)를 입력 받는 단계; 상기 프로세서가, 상기 자연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계; 및 상기 프로세서가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 트랜스포머(transformer) 모델을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와 매칭되는 수어 글로스를 생성하는 단계; 를 포함하고, 상기 수어 글로스를 생성하는 단계는 상기 인코딩 과정에서 생성된 토큰을 카피(copy)하여 선택적으로 사용하여 상기 수어 글로스를 생성하는 것을 특징으로 하는, 트랜스포머를 이용한 수어 글로스 번역 방법을 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 85, "content": "A_03_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 86, "content": "A_03_요약 본 발명은 높은 정확A_03_도로 자연어를 수어 글로스로 번역하기 위한, 트랜스포머를 이용한 수어 글로스 번역 방법을 제안한다. 상기 방법은 번역서버가, 자연어 텍스트(text)를 입력 받는 단계, 상기 번역서버가, 상기 자 연어 텍스트를 인코딩(encoding)하여 상기 자연어 텍스트와 대응하는 벡터(vector)를 생성하는 단계 및 상기 번 역서버가, 자연어 및 상기 자연어와 매칭되는 수어 데이터 셋(data set)에 의해 사전 기계 학습(machine learning)된 트랜스포머(transformer) 모델을 통해, 상기 벡터를 디코딩(decoding)하여 상기 자연어 텍스트와매칭되는 수어 글로스를 생성하는 단계를 포함할 수 있다. A_03_대표A_03_도 A_03_도 12 A_03_도면 A_03_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 87, "content": "A_03_도 2 A_03_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 88, "content": "A_03_도 4 A_03_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 89, "content": "A_03_도 6 A_03_도 7 A_03_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 90, "content": "A_03_도 9 A_03_도 10 A_03_도 11 A_03_도 12"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 91, "content": "A_04_발명의 설명 A_04_발명의 명칭 MIM 기반의 퓨샷 객체 검출 모델 학습 방법{Method for few shot object detection model based learning masked image modeling}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 92, "content": "A_04_기술분야 본 발명은 객체 검출 모델(object detection model)에 관한 것이다. 보다 상세하게는, 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 MIM 기반의 퓨샷 객체 검출 모델 학습 방법에 관한 것이다. A_04_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_04_도 학습(supervised learning), 비지A_04_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 지A_04_도 학습은 사람이 직접 각각의 입력에 대하여, 입력에 대응하는 레이블(label)을 부여한 데이터를 직접 컴퓨터에 제공하여 학습하는 방법을 의미한다. 이러한, 지A_04_도 학습은 사람이 직접 개입하므로 정확A_04_도 가 높은 데이터를 사용할 수 있는 장점이 있다. 비지A_04_도 학습은 사람 없이 컴퓨터가 레이블이 부여되지 않은 데이터를 통해 학습하는 방법을 의미한다. 이 러한, 비지A_04_도 학습은 정답이 없는 문제를 컴퓨터가 해결하는 것이므로 지A_04_도 학습에 비해 다소 정확 A_04_도가 떨어지지만 향후 기계학습이 나아갈 방향으로 설정되어 있다. 강화 학습은 현재의 상태(State)에서 어떤 행동(Action)을 취하는 것이 최적인지를 학습하는 방법을 의미한다. 구체적으로, 강화 학습은 행동을 취할 때마다 외부 환경에서 보상(Reward)이 주어지는데, 이러한 보상을 최대화 하는 방향으로 학습이 진행된다. 일반적으로 방대한 양의 학습 데이터를 요구하는 비전 태스크(vision task)는 데이터의 수집, 가공 및 학습에 많은 시간과 자원을 필요로 한다. 데이터의 양이 적은 경우 과적합이나 정확A_04_도에서 문제가 발생할 수 있지만, 최근에는 적은 양의 데이터로 A_04_도 높은 정확A_04_도의 검출이 가능한 퓨샷 객체 검출(few shot object detection)에 대한 연구가 활발히 진행되고 있다. 한편, 메타 학습(meta learning)은 적은 양의 데이터와 주어진 환경만으로A_04_도 스스로 학습하고, 학습한 정 보와 알고리즘을 새로운 문제에 적용하여 해결하는 학습 방법을 의미한다. 최근에는 메타 학습을 퓨샷 객체 검출에 적용하는 연구가 진행되고 있으나, 정확A_04_도가 보장된 만큼 복잡한 에피소드 셋팅으로 인한 연산이 비효율 적이라는 한계점이 존재했다. A_04_선행기술문헌 A_04_특허문헌 대한민국 등록특허공보 제10-2348593호, ‘기계 학습 기반의 객체 검출 방법 및 그 장치’, (2022.01.04. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 93, "content": "A_04_발명의 내용 A_04_해결하고자 하는 과제 따라서, 본 발명의 목적은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 MIM 기반 의 퓨샷 객체 검출 모델 학습 방법을 제공하는 것이다. 본 발명의 다른 목적은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 MIM 기반의 퓨샷 객체 검출 모델 학습 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 94, "content": "A_04_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검 출 성능을 발현할 수 있는 MIM 기반의 퓨샷 객체 검출 모델 학습 방법을 제안한다. 상기 방법은 학습 장치가, 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습하는 단계, 상기 학습 장치가, 상기 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하는 단계 및 상기 학습 장치가, 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 상기 제3 학습 모델을 학습하는 단계를 포함하는 것을 특징으로 한다. 구체적으로, 상기 제1 데이터 셋 및 상기 제2 데이터 셋은 적어A_04_도 하나의 객체, 상기 적어A_04_도 하나의 객체의 클래스(class) 정보 및 상기 적어A_04_도 하나의 객체에 어노테이션(annotation) 수행된 바운딩 박스 (bounding box) 정보를 포함하는 것을 특징으로 한다. 상기 제2 데이터 셋은 클래스 별 객체 수가 사전 설정된 값보다 많은 베이스 클래스(base class) 이미지 그룹 및 클래스 별 객체 수가 사전 설정된 값보다 적은 노벨 클래스(novel class) 이미지 그룹을 포함하고, 상기 노 벨 클래스 이미지 그룹 및 상기 베이스 클래스 이미지 그룹은 서로 중복되지 않는 것을 특징으로 한다. 상기 사전 학습하는 단계는 균일한 제약 조건으로 사전에 설정된 비율에 따라 상기 제1 데이터 셋에 포함된 각 이미지를 샘플링 하는 단계, 상기 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 하는 단계및 상기 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습하는 단계를 포함하는 것을 특징으로 한다. 상기 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나인 것을 특징으로 한다. 상기 마스킹 하는 단계는 상기 비전 트랜스포머가 PVT 인 경우, 상기 샘플링 된 이미지의 가시적 패치 중 무작 위로 마스킹을 수행하되, 공유 마스크 토큰을 사용하여 상기 마스킹 된 패치를 대체하는 것을 특징으로 한다. 상기 공유 마스크 토큰은 상기 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector) 인 것을 특징 으로 한다. 상기 제3 학습 모델을 생성하는 단계에서 상기 제2 학습 모델은 Faster R-CNN 인 것을 특징으로 한다. 상기 제3 학습 모델을 생성하는 단계는 상기 제1 학습 모델에 포함된 인코더(encoder)를 상기 제2 학습 모델의 백본(backbone) 층으로 삽입하는 것을 특징으로 한다. 상기 제3 학습 모델을 학습시키는 단계는 상기 베이스 클래스 이미지 그룹을 기초로 상기 제3 학습 모델을 학습 하여 제4 학습 모델을 생성하는 단계 및 상기 제2 데이터 셋을 기초로 상기 제4 학습 모델을 미세 조정(fine tuning)하는 단계를 포함하는 것을 특징으로 한다. 상기 미세 조정하는 단계는 상기 제4 학습 모델의 출력 층의 가중치를 초기화하여 미세 조정하되, 상기 출력 층 을 제외한 나머지 구성요소의 가중치는 고정하는 것을 특징으로 한다. 상기 미세 조정하는 단계는 상기 제4 학습 모델의 출력 층을 초기화 하되, 상기 출력 층을 코사인 유사A_04_도 (cosine similarity) 층으로 대체하고, 상기 코사인 유사A_04_도 층의 학습율을 사전 설정된 값으로 미세 조정 하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 학습 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리 에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함 하는 제1 데이터 셋을 기초로 제1 학습 모델을 사전 학습하는 단계, 상기 프로세서가, 상기 사전 학습된 제1 학 습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하는 단계 및 상기 프로세서가, 데 이터 수가 사전 설정된 값보다 작은 제2 데이터 셋을 기초로 상기 제3 학습 모델을 학습하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_04_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 95, "content": "A_04_발명의 효과 본 발명의 실시 예들에 따르면, 랜덤 한 이미지 데이터 셋을 통해 학습 모델을 사전 학습하여 학습 모델의 일반 화 성능을 증가시킬 수 있으며, 트랜스포머 기반의 학습 모델을 조합하여 계산 복잡A_04_도를 낮출 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 96, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 97, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 98, "content": "A_04_도면의 간단한 설명 A_04_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_04_도이다. A_04_도 3은 본 발명의 일 실시예에 따른 학습 장치의 논리적 구성A_04_도이다. A_04_도 4는 본 발명의 일 실시예에 따른 학습 장치의 하드웨어 구성A_04_도이다. A_04_도 5는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 순서A_04_도이다. A_04_도 6은 본 발명의 일 실시예에 따른 S110 단계를 구체적으로 나타낸 순서A_04_도이다. A_04_도 7은 본 발명의 일 실시예에 따른 S130 단계를 구체적으로 나타낸 순서A_04_도이다. A_04_도 8은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 순서A_04_도이다. A_04_도 9는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 예시A_04_도이다. A_04_도 10은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 예시A_04_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 99, "content": "A_04_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_04_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_04_도하게 포괄적인 의미로 해석되거나, 과A_04_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_04_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_04_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_04_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_04_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_04_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_04_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_04_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_04_도면은 본 발명의 사상을 쉽게 이해할 수 있A_04_도록 하기 위한 것일 뿐, 첨부된 A_04_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_04_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_04_ 도 확장되는 것으로 해석되어야 한다. 한편, 일반적으로 방대한 양의 학습 데이터를 요구하는 비전 태스크(vision task)는 데이터의 수집, 가공 및 학 습에 많은 시간과 자원을 필요로 한다. 데이터의 양이 적은 경우 과적합이나 정확A_04_도에서 문제가 발생할 수 있지만, 최근에는 적은 양의 데이터로 A_04_도 높은 정확A_04_도의 검출이 가능한 퓨샷 객체 검출(few shot object detection)에 대한 연구가 활발히 진행되고 있다. 한편, 메타 학습(meta learning)은 적은 양의 데이터와 주어진 환경만으로A_04_도 스스로 학습하고, 학습한 정 보와 알고리즘을 새로운 문제에 적용하여 해결하는 학습 방법을 의미한다. 최근에는 메타 학습을 퓨샷 객체 검출에 적용하는 연구가 진행되고 있으나, 정확A_04_도가 보장된 만큼 복잡한 에피소드 셋팅으로 인한 연산이 비효율 적이라는 한계점이 존재했다. 이러한 한계를 극복하고자, 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있 는 다양한 수단들을 제안하고자 한다.A_04_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_04_도이다. A_04_도 1에 A_04_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 생성 장 치, 하나 이상의 어노테이션 장치(200-1, 200-2, …, 200-n; 200), 학습 데이터 검증 장치 및 학습 장치를 포함하여 구성될 수 있다. 또한, A_04_도 2에 A_04_도시된 바와 같이, 본 발명의 다른 실시예에 따른 인공지능 학습 시스템은 하나 이상의 어노테이션 장치(200-a, 200-b, …, 200-m; 200)와 학습 데이터 검증 장치(300-a, 300-b, …, 300-m; 300)가 하나로 이루어진 복수 개의 그룹(Group-a, Group-b …, Group-m), 학습 데이터 생성 장치 및 학습 장치 를 포함하여 구성될 수 있다. 이와 같은, 다양한 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지 능(AI)을 기계 학습시키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 이와 같은, 학습 데이터 생성 장치는 기본적으로 학습 데이터 검증 장치와 구분되는 장치이나, 실제 물리적 환경에서 학습 데이터 생성 장치와 학습 데이터 검증 장치가 하나의 장치로 통합되어 구현될 수A_04_도 있다. 구체적으로, 학습 데이터 설계 장치는 학습 장치로부터 인공지능(AI) 학습과 관련된 프로젝트의 속성 을 수신할 수 있다. 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지 능(AI) 학습을 위한 데이터 구조의 설계, 수집된 데이터의 정제, 데이터의 가공, 데이터의 확장 및 데이터의 검 증을 수행할 수 있다. 우선적으로, 학습 데이터 설계 장치는 인공지능(AI) 학습을 위한 데이터 구조를 설계할 수 있다. 예를 들 어, 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지능(AI) 학습을 위한 온톨로지(ontology), 인공지능(AI) 학습을 위한 데이터의 분류 체계를 정의할 수 있다. 학습 데이터 설계 장치는 설계된 데이터 구조를 기초로, 인공지능(AI) 학습을 위한 데이터를 수집할 수 있 다. 이를 위하여, 학습 데이터 설계 장치는 외부로부터 3D 점군 데이터 및 2D 이미지들을 입력 받거나, 웹 크롤링(web crawling)을 수행하여 3D 점군 데이터 및 2D 이미지들을 수집하거나, 또는 외부 기관의 장치로부터 3D 점군 데이터 및 2D 이미지들을 다운로드 할 수 있다. 여기서, 3D 점군 데이터는 차량에 고정 설치된 라이다(lidar)에 의해 획득된 데이터이다. 차량에 고정 설치된 라이다는 레이저 펄스를 발사하고, 차량 주위에 위치하는 객체들에 의해 반사되어 돌아온 빛을 감지하여, 차량 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 즉, 3D 점군 데이터를 구성하는 점군은 라이다에 의해 3차원 공간으로 발사된 레이저 펄스를 반사시킨 점(point)들의 집합을 의미한다. 그리고, 2D 이미지는 차량에 고정 설치된 복수 개의 카메라에 의해 촬영된 이미지이다. 자율주행을 위하여 하나 의 차량에는 다수 개의 카메라가 고정 설치되어, 차량 주위에 대한 2차원 이미지를 각각 촬영할 수 있다. 예를 들어, 하나의 차량에 6개의 카메라가 설치될 수 있으나, 이에 한정되지 않는다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들 중에서 중복되거나 또는 극히 유사한 데 이터를 제거할 수 있다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들에 포함된 개인정 보를 비식별화(de-identification)할 수 있다. 학습 데이터 생성 장치는 수집 및 정제된 3D 점군 데이터 및 2D 이미지들을 복수 개의 어노테이션 장치 에 분배하여 전송할 수 있다. 이 경우, 학습 데이터 생성 장치는 어노테이션 장치의 작업자(즉, 라벨러)에 대하여 사전에 할당된 양에 따라 3D 점군 데이터 및 2D 이미지들을 분배할 수 있다. 학습 데이터 생성 장치는 어노테이션 장치로부터 직접 어노테이션 작업 결과물을 수신하거나, 또는 학습 데이터 검증 장치로부터 어노테이션 작업 결과물 및 검수 결과를 수신할 수 있다. 학습 데이터 생성 장치는 수신된 어노테이션 작업 결과물을 패키징(packaging)하여 인공지능(AI) 학습용 데이터를 생성할 수 있다. 그리고, 학습 데이터 생성 장치는 생성된 인공지능(AI) 학습용 데이터를 학습 장치에 전송할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_04_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 작업을 수행하 는데 사용될 수 있는 장치이다. 여기서, 어노테이션 작업은 바운딩 박스(bounding box)를 설정하고, 객체의 속성 정보를 포함하는 클래스 (class) 정보를 입력하는 과정을 포함할 수 있다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치 또는 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_04_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수A_04_도 있다. 다음 구성으로, 학습 데이터 검증 장치는 인공지능(AI) 학습용 데이터를 검증하는데 사용될 수 있는 장치 이다. 즉, 학습 데이터 검증 장치는 어노테이션 장치에 의해 생성된 어노테이션 작업 결과물이 사전 에 설정된 목표 품질에 부합하는지 여부, 또는 어노테이션 작업 결과물이 인공지능(AI) 학습에 유효한지 여부를 검증할 수 있는 장치이다. 구체적으로, 학습 데이터 검증 장치는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 여기서, 어노테이션 작업 결과물은 3D 점군 데이터 및 2D 이미지들로부터 특정된 객체의 좌표와, 이미지 또는 객체에 대한 메타데이터가 포함될 수 있다. 어노테이션 작업 결과물의 메타데이터에는 특정된 객체의 카테 고리(category), 객체가 2D 이미지의 화각에 의해 잘려진 비율(truncation), 객체가 다른 객체 또는 물체에 의 해 가려진 비율(occlusion), 객체의 트래킹 아이디(tracking identifier), 이미지가 촬영된 시각, 이미지가 촬 영된 날의 기상 조건 등이 포함될 수 있으며, 이에 한정되는 것은 아니다. 이와 같은, 어노테이션 작업 결과물 은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것A_04_도 아니다. 학습 데이터 검증 장치는 수신된 어노테이션 작업 결과물을 검수할 수 있다. 이를 위하여, 학습 데이터 검 증 장치는 어노테이션 작업 결과물을 대상으로 스크립트(script)를 이용하여 검수를 수행할 수 있다. 여기 서, 스크립트는 어노테이션 작업 결과물을 대상으로 사전에 설정된 목표 품질의 부합 여부 또는 데이터 유효성 여부를 검증하기 위한 코드이다. 그리고, 학습 데이터 검증 장치는 어노테이션 장치들로부터 수신된 어노테이션 작업 결과물 및 검수 결과를 학습 데이터 생성 장치에 전송할 수 있다. 상술한 바와 같은 특징을 가지는, 학습 데이터 검증 장치는 어노테이션 장치 및 학습 데이터 생성 장 치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라 A_04_도 허용될 수 있다. 예를 들어, 학습 데이터 검증 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 학습 장치는 학습 데이터 생성 장치로부터 제공된 이미지를 통해 인공지능(AI)을 기 계 학습하는데 사용될 수 있는 장치이다. 특히, 본 발명의 일 실시예에 따른 학습 장치는 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이 미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습하고, 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하고, 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다.또한, 본 발명의 다른 실시예에 따른 학습 장치는 제1 객체 검출 모델에 적어A_04_도 하나의 이미지를 입 력 받고, 제1 객체 검출 모델을 통해 적어A_04_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클 래스 간의 유사A_04_도에 기반한 제1 클래스 스코어를 산출하고, 검출된 객체를 제2 객체 검출 모델에 입력하고, 제2 객체 검출 모델을 통해 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제2 클래스 스코어를 산출하고, 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있다. 한편, 본 발명의 일 실시예에 따른 학습 장치의 구체적인 구성에 대해서는 이하, A_04_도 3 및 A_04_도 4 를 참조하여 상세히 설명하A_04_도록 한다. 이와 같은, 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_04_도 허용될 수 있다. 예를 들어, 학습 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것 은 아니다. A_04_도 3은 본 발명의 일 실시예에 따른 학습 장치의 논리적 구성A_04_도이다. A_04_도 3에 A_04_도시된 바와 같이, 본 발명의 일 실시예에 따른 학습 장치는 통신부, 입출력부 , 학습 모델 생성부, 스코어 재조정부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 데이터 셋을 입력 받을 수 있다. 학습 데이터 생성 장치로부터 수신한 데이터 셋은 적어A_04_도 하나의 객체, 적어A_04_도 하나의 객체의 클래스(class) 정보 및 적어A_04_도 하나의 객체에 어노테이션(annotation) 수행된 바운딩 박스(bounding box) 정보를 포함할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해 사용자로부터 신호를 입력 받거나, 연산 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 학습 모델을 학습하기 위한 설정 신호, 객체 검출 모델의 클래스 스코어를 재조정 하기 위한 설정 신호 등을 입력 받을 수 있다. 다음 구성으로, 학습 모델 생성부는 높은 정확A_04_도를 가지는 학습 모델을 생성할 수 있다. 이를 위해, 학습 모델 생성부는 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습할 수 있다. 예를 들어, 랜덤 한 이미지를 포함하는 제1 데이터 셋은 공개된 공공의 데이터 셋인 \"Imagenet1k\" 데이터 셋을 사용할 수 있다. 구체적으로, 학습 데이터 생성부는 균일한 제약 조건으로 사전에 설정된 비율에 따라 제1 데이터 셋에 포 함된 각 이미지를 샘플링 할 수 있다. 예를 들어, 학습 모델 생성부는 제1 데이터 셋에 포함된 각 이미지의 모든 2x2 그리드에서 패치(patch)를 샘플링 할 수 있다. 이에 따라, 샘플링 된 제1 데이터 셋은 모든 로컬 윈A_04_도우(local window)에서 균일한 양의 패치가 샘플링되기 때문에 피라미드 기반의 비전 트랜스포머(vision transformer)에 적용될 수 있다. 또한, 샘플링 된 이미지는 크기가 1/4로 줄어 연산량을 줄일 수 있다. 여기서, 피라미드 기반의 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나가 될 수 있다. 학습 모델 생성부는 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 할 수 있다. 이때, 학습 모델 생성부는 피라미드 기반의 비전 트랜스포머가 PVT 인 경우, 샘플링 된 이미지의 가시적 패치 중에서 무작위로 마스킹을 수행하되, PVT 와의 호환성을 위하여 공유 마스크 토큰을 사용하여 마스킹 된 패치를 대체할 수 있다. 여기서, 공유 마스크 토큰은 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector)가 될 수 있다. 학습 모델 생성부는 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습할 수 있다. 즉, 학습 모델 생성부는 샘플링 및 마스킹을 수행한 제1 데이터 셋을 피 라미드 기반의 비전 트랜스포머의 인코더(encoder)에 입력할 수 있다. 다음으로, 학습 모델 생성부는 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성할 수 있다. 구체적으로, 학습 모델 생성부는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 다음으로, 학습 모델 생성부는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모 델을 학습할 수 있다. 여기서, 제2 데이터 셋은 클래스 별 객체 수가 사전 설정된 값보다 많은 베이스 클래스(base class) 이미지 그 룹 및 클래스 별 객체 수가 사전 설정된 값보다 적은 노벨 클래스(novel class) 이미지 그룹을 포함할 수 있다. 이때, 노벨 클래스 이미지 그룹 및 베이스 클래스 이미지 그룹은 서로 중복되지 않을 수 있다. 구체적으로, 학습 모델 생성부는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습할 수 있다. 즉, 학습 모델 생성부는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_04_도록 할 수 있다. 또한, 학습 모델 생성부는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 모델 생성부는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵 커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 모델 생성부는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박 스를 생성할 수 있다. 이때, 학습 모델 생성부는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으 로 앵커 박스를 생성하는 기준점인 앵커를 고정할 수 있다. 또한, 학습 모델 생성부는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클 래스(class)에 대한 클래스 스코어를 산출할 수 있다. 학습 모델 생성부는 산출된 클래스 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 모델 생성부는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐 (region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 모델 생성부는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달 된 리젠 프로포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 모델 생성부는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 모델 생성부는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설 정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 모델 생성부는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결 과를 이용하여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 다음으로, 학습 모델 생성부는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있 다. 구체적으로, 학습 모델 생성부는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_04_도 (cosine similarity) 층으로 대체하고, 코사인 유사A_04_도 층의 학습율을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_04_도 층은 내적 공간의 두 벡터간 각A_04_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_04_도를 측정하는 층이다. 다음 구성으로, 스코어 재조정부는 서로 다른 학습 모델로부터 산출된 클래스 스코어를 기초로 클래스 스 코어를 재조정함으로써, 높은 정확A_04_도로 검출된 객체의 클래스를 추정할 수 있다. 이를 위해, 스코어 재조정부는 제1 객체 검출 모델에 적어A_04_도 하나의 이미지를 입력 받을 수 있다. 여 기서, 제1 객체 검출 모델은 상술한 학습 모델 생성부에 의해 생성된 제4 학습 모델이 될 수 있다. 다음으로, 스코어 재조정부는 제1 객체 검출 모델을 통해 적어A_04_도 하나의 이미지에 포함된 객체를 검 출하고, 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제1 클래스 스코어를 산출할 수 있다. 다음으로, 스코어 재조정부는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델에 입력할 수 있다. 한편, 스코어 재조정부는 객체를 제2 객체 검출 모델에 입력하기 이전에, 제2 객체 검출 모델을 사전 학습 할 수 있다. 구체적으로, 스코어 재조정부는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지 와 연관된 자연어 텍스트가 될 수 있다. 스코어 재조정부는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스트 인코더(text encoder)를 통해 이미지 임베딩 및 상기 텍스트 임베딩을 추출할 수 있다. 이후, 스코어 재조정부는 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍에서 긍정적 쌍(positive pair)의 코사인 유사A_04_도(cosine similarity)는 최대화하고, 부정적 쌍(negative pair)의 코사인 유사A_04_도는 최 소화하A_04_도록 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 스코어 재조정부는 CE 손실 함수 (cross entropy loss function)를 통해 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 다음으로, 스코어 재조정부는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래 스 간의 유사A_04_도에 기반한 제2 클래스 스코어를 산출할 수 있다. 구체적으로, 스코어 재조정부는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋 에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 스코어 재조정부는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍스트 임베딩을 추출할 수 있다. 스코어 재조정부는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_04_도에 기반한 제2 클래스 스 코어를 산출할 수 있다. 구체적으로, 스코어 재조정부는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 100, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_04_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.) 다음으로, 스코어 재조정부는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있다. 구체적으로, 스코어 재조정부는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 클래스 스코어를 재조 정할 수 있다. 즉, 스코어 재조정부는 하기의 수학식 2를 통해 클래스 스코어를 재조정할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 101, "content": "(여기서, α는 상기 제2 객체 검출 모델에 의해 산출된 유사A_04_도의 영향력을 조절하는 하이퍼 파라미터이고, Sik는 제2 클래스 스코어, sik는 제1 클래스 스코어를 의미한다.) 그리고, 스코어 재조정부는 재조정된 클래스 스코어를 기초로 클래스를 추정할 수 있다. 즉, 스코어 재조 정부는 재조정된 클래스 스코어를 기초로 클래스 스코어가 가장 높은 클래스를 검출된 객체의 클래스로 추 정할 수 있다. 이하, 상술한 바와 같은 학습 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구체적으 로 설명한다. A_04_도 4는 본 발명의 일 실시예에 따른 학습 장치의 하드웨어 구성A_04_도이다. A_04_도 4에 A_04_도시된 바와 같이, 학습 장치는 프로세서(Processor, 450), 메모리(Memory, 455), 송수 신기(Transceiver, 460), 입출력장치(Input/output device, 465), 데이터 버스(Bus, 470) 및 스토리지 (Storage, 475)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)에 따른 명 령어를 기초로, 학습 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치, 어 노테이션 장치 및 학습 데이터 검증 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 장 치의 동작에 필요한 데이터를 입력 받거나, 학습 결과물을 출력할 수 있다. 데이터 버스는 프로세서 , 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)의 실행을 위해 필요한 애플리케이 션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스 (resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어 (480b)를 저장할 수 있다. 또한, 스토리지는 본 발명의 실시예들에 따른 방법의 수행에 필요한 정보들을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 학습 방법을 구현하기 위한 소프트웨어(480a, 480b)는 프로세서가 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지 를 포함하는 제1 데이터 셋을 기초로 제1 학습 모델을 사전 학습하는 단계, 프로세서가, 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하는 단계 및 프로세서가, 데이터 수가 사전 설정된 값보다 작은 제2 데이터 셋을 기초로 제3 학습 모델을 학습하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 객체 검출 방법을 구 현하기 위한 소프트웨어(480a, 480b)는 프로세서가 제1 객체 검출 모델에 적어A_04_도 하나의 이미지를 입 력 받는 단계, 프로세서가, 제1 객체 검출 모델을 통해 상기 적어A_04_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제1 클래스 스코어를 산출하는 단계, 프로세서 가, 검출된 객체를 제2 객체 검출 모델에 입력하는 단계, 프로세서가, 제2 객체 검출 모델을 통해 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제2 클래스 스코어를 산출하는 단계, 프로세서가, 제1 클 래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정하는 단계를 실행시키기 위하여, 기록 매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_04_도 4에 A_04_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_04_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_04_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_04_도록 구성될 수 있으며, 그 역A_04_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 학습 방법에 대하여 상세히 설명하A_04_도록 한다. A_04_도 5는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 순서A_04_도이고, A_04_도 6은 본 발명의 일 실시예에 따른 S110 단계를 구체적으로 나타낸 순서A_04_도이고, A_04_도 7은 본 발명의 일 실시예에 따른 S130 단계를 구체적으로 나타낸 순서A_04_도이다. A_04_도 5 내지 A_04_도 7을 참조하면, 먼저 S110 단계에서 학습 장치는 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습할 수 있다. 구체적으로, S111 단계에서 학습 장치는 균일한 제약 조건으로 사전에 설정된 비율에 따라 제1 데이터 셋에 포 함된 각 이미지를 샘플링 할 수 있다. 예를 들어, 학습 장치는 제1 데이터 셋에 포함된 각 이미지의 모든 2x2 그리드에서 패치(patch)를 샘플링 할 수 있다. 이에 따라, 샘플링 된 제1 데이터 셋은 모든 로컬 윈A_04_도우(local window)에서 균일한 양의 패치가 샘 플링되기 때문에 피라미드 기반의 비전 트랜스포머(vision transformer)에 적용될 수 있다. 또한, 샘플링 된 이 미지는 크기가 1/4로 줄어 연산량을 줄일 수 있다.여기서, 피라미드 기반의 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나가 될 수 있다. S112 단계에서 학습 장치는 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 할 수 있다. 이때, 학습 장치는 피라미드 기반의 비전 트랜스포머가 PVT 인 경우, 샘플링 된 이미지의 가시적 패치 중에서 무작위 로 마스킹을 수행하되, PVT 와의 호환성을 위하여 공유 마스크 토큰을 사용하여 마스킹 된 패치를 대체할 수 있 다. 여기서, 공유 마스크 토큰은 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector)가 될 수 있다. S113 단계에서 학습 장치는 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습할 수 있다. 즉, 학습 장치는 샘플링 및 마스킹을 수행한 제1 데이터 셋을 피라미드 기반의 비전 트랜스포머의 인코더 (encoder)에 입력할 수 있다. 다음으로, S120 단계에서 학습 장치는 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하 여 제3 학습 모델을 생성할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 구체적으로, 학습 장치는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입 할 수 있다. 다음으로, S130 단계에서 학습 장치는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다. 구체적으로, S131 단계에서 학습 장치는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습하여 제4 학습 모델을 생성할 수 있다. 즉, 학습 장치는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_04_도록 할 수 있다. 또한, 학습 장치는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 장치는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가 지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 장치는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박스를 생성할 수 있다. 이때, 학습 장치는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으로 앵커 박스를 생성하는 기준점인 앵커를 고정할 수 있다. 또한, 학습 장치는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클래스(class)에 대한 스코어를 산출할 수 있다. 학습 장치는 산출된 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 장치는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐(region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 장치는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달된 리젠 프로 포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 장치는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 장치는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 장치는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결과를 이용하 여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 다음으로, 학습 장치는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있다. 이때, 학 습 장치는 제4 학습 모델의 출력 층의 가중치를 초기하여 미세 조정하되, 출력 층을 제외한 나머지 구성요소의 가중치는 고정시킬 수 있다. 구체적으로, 학습 장치는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_04_도(cosine similarity) 층으로 대체하고, 코사인 유사A_04_도 층의 학습률을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_04_도 층은 내적 공간의 두 벡터간 각A_04_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_04_도를 측정하는 층이다. A_04_도 8은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 순서A_04_도이다. A_04_도 8을 참조하면, 먼저 S210 단계에서 학습 장치는 제1 객체 검출 모델에 적어A_04_도 하나의 이미지를 입 력 받을 수 있다. 다음으로, S220 단계에서 학습 장치는 제1 객체 검출 모델을 통해 적어A_04_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제1 클래스 스코어를 산출할 수 있다. 다음으로, S230 단계에서 학습 장치는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델에 입력할 수 있다. 한편, 학습 장치는 객체를 제2 객체 검출 모델에 입력하기 이전에, 제2 객체 검출 모델을 사전 학습할 수 있다. 구체적으로, 학습 장치는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍 스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지 와 연관된 자연어 텍스트가 될 수 있다. 학습 장치는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스트 인코더(text encoder)를 통해 이미지 임베딩 및 상기 텍스트 임베딩을 추출할 수 있다. 이후, 학습 장치는 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍에서 긍정적 쌍(positive pair)의 코사인 유 사A_04_도(cosine similarity)는 최대화하고, 부정적 쌍(negative pair)의 코사인 유사A_04_도는 최소화하 A_04_도록 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 학습 장치는 CE 손실 함수(cross entropy loss function)를 통해 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 다음으로, S240 단계에서 학습 장치는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제2 클래스 스코어를 산출할 수 있다. 구체적으로, 학습 장치는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 학습 장치는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍 스트 임베딩을 추출할 수 있다. 학습 장치는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_04_도에 기반한 제2 클래스 스코어를 산출 할 수 있다. 구체적으로, 학습 장치는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 102, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_04_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.)다음으로, S250 단계에서 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스 를 추정할 수 있다. 구체적으로, 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 클래스 스코어를 재조정할 수 있다. 즉, 학습 장치는 하기의 수학식 2를 통해 클래스 스코어를 재조정할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 103, "content": "(여기서, α는 상기 제2 객체 검출 모델에 의해 산출된 유사A_04_도의 영향력을 조절하는 하이퍼 파라미터이고, Sik는 제2 클래스 스코어, sik는 제1 클래스 스코어를 의미한다.) 그리고, 학습 장치는 재조정된 클래스 스코어를 기초로 클래스를 추정할 수 있다. 즉, 학습 장치는 재조정된 클 래스 스코어를 기초로 클래스 스코어가 가장 높은 클래스를 검출된 객체의 클래스로 추정할 수 있다. A_04_도 9는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 예시A_04_도이다. A_04_도 9에 A_04_도시된 바와 같이, 학습 장치는 사전 학습된 제1 학습 모델(1st learning model)을 객체 검출 을 위한 제2 학습 모델(2nd learning model)과 결합하여 제3 학습 모델을 생성할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 한편, 일반적인 \"Faster R-CNN\"의 백본 층은 FPN(Feature Pyramid Network)로 구성될 수 있다. 구체적으로, 학습 장치는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입 할 수 있다. 다음으로, 학습 장치는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다. 구체적으로, 학습 장치는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습하 여 제4 학습 모델을 생성할 수 있다. 즉, 학습 장치는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_04_도록 할 수 있다. 또한, 학습 장치는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 장치는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가 지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 장치는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박스를 생성할 수 있다. 이때, 학습 장치는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으로 앵커 박스를 생성하는 기준점인 앵커를 고정할 수 있다. 또한, 학습 장치는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클래스(class)에 대한 스코어를 산출할 수 있다. 학습 장치는 산출된 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 장치는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐(region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 장치는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달된 리젠 프로 포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 장치는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 장치는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 장치는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결과를 이용하 여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 그리고, 학습 장치는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있다. 이때, 학습 장치는 제4 학습 모델의 출력 층의 가중치를 초기하여 미세 조정하되, 출력 층을 제외한 나머지 구성요소의 가 중치는 고정시킬 수 있다. 구체적으로, 학습 장치는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_04_도(cosine similarity) 층으로 대체하고, 코사인 유사A_04_도 층의 학습률을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_04_도 층은 내적 공간의 두 벡터간 각A_04_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_04_도를 측정하는 층이다. A_04_도 10은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 예시A_04_도이다. A_04_도 10에 A_04_도시된 바와 같이, 학습 장치는 객체 검출을 위한 제1 객체 검출 모델(1st detection mode l)에 적어A_04_도 하나의 이미지(image)를 입력하여 이미지로부터 식별된 객체(object 1, object 2, object 3)를 검출할 수 있다. 다음으로, S230 단계에서 학습 장치는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델(2nd detection model)에 입력할 수 있다. 학습 장치는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래스 간의 유사A_04_도에 기반한 제2 클래스 스코어를 산출할 수 있다. 구체적으로, 학습 장치는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍(I,T)을 구성할 수 있다. 여기서, 학습 장치는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍 스트 임베딩을 추출할 수 있다. 학습 장치는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_04_도에 기반한 제2 클래스 스코어를 산출 할 수 있다. 그리고, 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있 다. 이하, 본 발명의 일 실시예에 따른 학습 장치의 객체 검출 성능에 대하여 설명하A_04_도록 한다. 한편, 후술할 성능 실험을 위한 데이터 셋은 퓨샷 객체 검출 분야에서 표준적으로 쓰이는 \"PASCAL VOC 퓨샷 데 이터 셋\"이다. \"PASCAL VOC 퓨샷 데이터 셋\"은 20개의 클래스 중에서 15개의 클래스는 베이스 클래스 그룹이며, 나머지 5개의 클래스는 노벨 클래스 그룹으로 이루어져 있다. 실시예 실시예는 \"Imagenet1k 데이터셋\"을 상술한 방법으로 샘플링하고, 마스킹 한 후 SWIN 트랜스포머를 800 epoch 사 전 학습하였다. 그리고, 사전 학습된 SWIN 트랜스포머는 FPN을 대체하여 \"Faster R-CNN\"의 백본으로 삽입한 후 베이스 클래스 그룹을 통해 학습을 진행하였다. 이때, 학습 조건은 16 배치 크기, AdamW(1e-4 기본학습률, 0.05 weight decay)와 WarmupMultiStepLR(2000 iteration Warm up, 3×스케줄)을 사용하여 75 epoch 학습을 진행하였다. 그리고, 학습된 학습 모델의 마지막 출력층을 코사인 유사A_04_도 레이어로 바꾸어 해당 레이어만 1e-5의 학습 율로 미세 조정하였다. 실험 [표 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 104, "content": "한편 표 1은 \"PASCAL VOC 2007+2012 K-shot 객체 검출 학습 데이터셋\"을 학습하여 \"PASCAL VOC 2007 테스트 셋\"의 검출 성능을 여러 모델과 비교한 표이다. 표 1을 참조하면, 스플릿(split)은 특정 클래스에 대해서만 편향되는 것을 방지하기 위하여 다양한 노벨 클래스 그룹(Novel Set 1, Novel Set 2, Novel Set 3)을 적용시켰다. 실험 결과, 실시예(Ours)는 \"Faster R-CNN\"이나, \"MetaYOLO\"보다 평균적으로 높은 성능을 기록하였으며, \"TF A\"와 비슷한 성능을 보이는 것을 확인할 수 있었다. [표 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 105, "content": "한편, 표 2는 표 1의 노벨 클래스 그룹 1(Novel Set 1)의 검출 성능을 상세히 나타낸 표이다. 표 2를 참조하면, 단일 시드에 대한 성능은 (A)를 통해 확인할 수 있다. 즉, 작은 파라미터를 갖는 \"Swin-tiny\"버전은 비슷한 크기인 \"ResNet50을 사용한 \"TFA\"보다 모든 AP(average precision)에서 압A_04_도적인 성능을 보였고, \"ResNet101\"을 사용한 \"TFA\"와 비슷한 AP를 갖는다. 또한, \"Swin-tiny\"버전 대비 상대적으로 큰 파라미터를 갖는 \"Swin large\" 버전은 \"ResNet101\"을 사용한 \"TF A\"보다 AP가 3.4%, AP50이 1.2%, AP75가 4.0%로 모든 면에서 성능이 증가하였다. 또한, (B)에 표시된 여러 시드에 대한 평균은 작은 파라미터를 갖는 \"Swin-tiny\"버전이 \"TFA\"보다 성능이 우수 한 것을 확인할 수 있었다. 정리하면, 종래의 객체 검출 모델은 \"Imagenet1k 데이터셋\"을 지A_04_도 학습으로 사전 학습하지만, 실시예는 라벨이 없는 데이터셋으로 백본 층에 대한 사전 학습을 진행한다. [표 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 106, "content": "한편, 표 3은 실시예에 따라 사전 학습을 진행한 학습 모델과, 사전 학습을 진행하지 않은 학습 모델의 퓨샷 검 출 성능을 비교한 표이다. 표 3을 참조하면, 실시예는 사전 학습을 진행하지 않은 학습 모델보다 성능이 월등히 높은 것을 확인할 수 있었 고, 이에 따라 사전 학습이 학습 모델의 일반화 성능에 크게 기여함을 알 수 있었다. 또한, 실시예는 컴퓨터 비전 분야에서 표준적으로 쓰이는 FPN을 사용할 경우 AP가 더 낮아지는 결과(-11.0%)를 보였다. 이와 같이, 퓨샷 객체 검출에서는 백본망의 사전 학습 유무가 중요하므로 사전학습 되지 않은 FPN 층이 오히려 성능 저하를 일으킨다고 볼 수 있다. 결론 본 발명의 일 실시예에 따른 학습 방법은 낮은 연산양으로 높은 객체 검출 성능을 내는 MIM 기반의 사전 학습 트랜스포머를 이용한 퓨샷 객체 검출 방법을 제안한다. 앞서 실험한 바와 같이, 본 발명의 일 실시예에 따른 학습 방법은 기존의 \"CNN\"기반 모델보다 현저히 높은 성능 을 보인다는 것과, MIM을 통한 사전 학습이 일반화 성능에 크게 기여한다는 것을 확인할 수 있었다. 이상과 같이, 본 명세서와 A_04_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_04_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_04_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_04_부호의 설명 100 : 학습 데이터 생성 장치 200 : 어노테이션 장치 300 : 학습 데이터 검증 장치 400 : 학습 장치 405 : 통신부 410 : 입출력부 415 : 학습 모델 생성부 420 : 스코어 재조정부 425 : 저장부 A_04_청구범위 A_04_청구항 1 학습 장치가, 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습하는 단계; 상기 학습 장치가, 상기 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모 델을 생성하는 단계; 및상기 학습 장치가, 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 상기 제3 학습 모델을 학습하 는 단계; 를 포함하는 것을 특징으로 하는, 학습 방법. A_04_청구항 2 제1 항에 있어서, 상기 제1 데이터 셋 및 상기 제2 데이터 셋은 적어A_04_도 하나의 객체, 상기 적어A_04_도 하나의 객체의 클래스(class) 정보 및 상기 적어A_04_도 하나의 객 체에 어노테이션(annotation) 수행된 바운딩 박스(bounding box) 정보를 포함하는 것을 특징으로 하는, 학습 방 법. A_04_청구항 3 제2 항에 있어서, 상기 제2 데이터 셋은 클래스 별 객체 수가 사전 설정된 값보다 많은 베이스 클래스(base class) 이미지 그룹 및 클래스 별 객체 수가 사전 설정된 값보다 적은 노벨 클래스(novel class) 이미지 그룹을 포함하고, 상기 노벨 클래스 이미지 그룹 및 상기 베이스 클래스 이미지 그룹은 서로 중복되지 않는 것을 특징으로 하는, 학습 방법. A_04_청구항 4 제3 항에 있어서, 상기 사전 학습하는 단계는 균일한 제약 조건으로 사전에 설정된 비율에 따라 상기 제1 데이터 셋에 포함된 각 이미지를 샘플링 하는 단계; 상기 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 하는 단계; 및 상기 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습하는 단 계; 를 포함하는 것을 특징으로 하는, 학습 방법. A_04_청구항 5 제4 항에 있어서, 상기 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나인 것을 특징으로 하는, 학습 방법. A_04_청구항 6 제5 항에 있어서, 상기 마스킹 하는 단계는 상기 비전 트랜스포머가 PVT 인 경우, 상기 샘플링 된 이미지의 가시적 패치 중 무작위로 마스킹을 수행하되, 공유 마스크 토큰을 사용하여 상기 마스킹 된 패치를 대체하는 것을 특징으로 하는, 학습 방법. A_04_청구항 7 제6 항에 있어서, 상기 공유 마스크 토큰은 상기 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector) 인 것을 특징으로 하는, 학습 방법. A_04_청구항 8 제7 항에 있어서, 상기 제3 학습 모델을 생성하는 단계에서 상기 제2 학습 모델은 Faster R-CNN 인 것을 특징으로 하는, 학습 방법. A_04_청구항 9 제8 항에 있어서, 상기 제3 학습 모델을 학습시키는 단계는 상기 베이스 클래스 이미지 그룹을 기초로 상기 제3 학습 모델을 학습하여 제4 학습 모델을 생성하는 단계; 및 상기 제2 데이터 셋을 기초로 상기 제4 학습 모델을 미세 조정(fine tuning)하는 단계; 를 포함하는 것을 특징 으로 하는, 학습 방법.A_04_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기 초로 제1 학습 모델을 사전 학습하는 단계; 상기 프로세서가, 상기 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모 델을 생성하는 단계; 및 상기 프로세서가, 데이터 수가 사전 설정된 값보다 작은 제2 데이터 셋을 기초로 상기 제3 학습 모델을 학습하 는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 107, "content": "A_04_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 108, "content": "A_04_요약 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 MIM 기반의 퓨샷 객체 검 출 모델 학습 방법을 제안한다. 상기 방법은 학습 장치가, 적어A_04_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습하는 단계, 상기 학 습 장치가, 상기 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생 성하는 단계 및 상기 학습 장치가, 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 상기 제3 학 습 모델을 학습하는 단계를 포함할 수 있다. A_04_대표A_04_도 A_04_도 9 A_04_도면 A_04_도 1 A_04_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 109, "content": "A_04_도 3 A_04_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 110, "content": "A_04_도 5 A_04_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 111, "content": "A_04_도 7 A_04_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 112, "content": "A_04_도 9 A_04_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 113, "content": "A_05_발명의 설명 A_05_발명의 명칭 스코어 재조정을 통한 객체 검출 방법{Method for object detection through readjustment of score}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 114, "content": "A_05_기술분야 본 발명은 객체 검출 모델(object detection model)에 관한 것이다. 보다 상세하게는, 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 스코어 재조정을 통한 객체 검출 방법에 관한 것이다. A_05_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_05_도 학습(supervised learning), 비지A_05_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 지A_05_도 학습은 사람이 직접 각각의 입력에 대하여, 입력에 대응하는 레이블(label)을 부여한 데이터를 직접 컴퓨터에 제공하여 학습하는 방법을 의미한다. 이러한, 지A_05_도 학습은 사람이 직접 개입하므로 정확A_05_도 가 높은 데이터를 사용할 수 있는 장점이 있다. 비지A_05_도 학습은 사람 없이 컴퓨터가 레이블이 부여되지 않은 데이터를 통해 학습하는 방법을 의미한다. 이 러한, 비지A_05_도 학습은 정답이 없는 문제를 컴퓨터가 해결하는 것이므로 지A_05_도 학습에 비해 다소 정확 A_05_도가 떨어지지만 향후 기계학습이 나아갈 방향으로 설정되어 있다. 강화 학습은 현재의 상태(State)에서 어떤 행동(Action)을 취하는 것이 최적인지를 학습하는 방법을 의미한다. 구체적으로, 강화 학습은 행동을 취할 때마다 외부 환경에서 보상(Reward)이 주어지는데, 이러한 보상을 최대화 하는 방향으로 학습이 진행된다. 일반적으로 방대한 양의 학습 데이터를 요구하는 비전 태스크(vision task)는 데이터의 수집, 가공 및 학습에 많은 시간과 자원을 필요로 한다. 데이터의 양이 적은 경우 과적합이나 정확A_05_도에서 문제가 발생할 수 있지만, 최근에는 적은 양의 데이터로 A_05_도 높은 정확A_05_도의 검출이 가능한 퓨샷 객체 검출(few shot object detection)에 대한 연구가 활발히 진행되고 있다. 한편, 메타 학습(meta learning)은 적은 양의 데이터와 주어진 환경만으로A_05_도 스스로 학습하고, 학습한 정 보와 알고리즘을 새로운 문제에 적용하여 해결하는 학습 방법을 의미한다. 최근에는 메타 학습을 퓨샷 객체 검출에 적용하는 연구가 진행되고 있으나, 정확A_05_도가 보장된 만큼 복잡한 에피소드 셋팅으로 인한 연산이 비효율 적이라는 한계점이 존재했다. A_05_선행기술문헌 A_05_특허문헌 대한민국 등록특허공보 제10-2348593호, ‘기계 학습 기반의 객체 검출 방법 및 그 장치’, (2022.01.04. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 115, "content": "A_05_발명의 내용 A_05_해결하고자 하는 과제 따라서, 본 발명의 목적은 객체 검출 모델(object detection model)에 관한 것이다. 보다 상세하게는, 적은 양 의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 스코어 재조정을 통한 객체 검출 방법을 제 공하는 것이다. 본 발명의 다른 목적은 객체 검출 모델(object detection model)에 관한 것이다. 보다 상세하게는, 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 스코어 재조정을 통한 객체 검출 방법을 실행하 기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 116, "content": "A_05_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검 출 성능을 발현할 수 있는 스코어 재조정을 통한 객체 검출 방법을 제안한다. 상기 방법은 학습 장치가, 제1 객 체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받는 단계, 상기 학습 장치가, 상기 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 상기 검출된 객체 및 클래스 간의 유사A_05_ 도에 기반한 제1 클래스 스코어를 산출하는 단계, 상기 학습 장치가, 상기 검출된 객체를 제2 객체 검출 모델에 입력하는 단계, 상기 학습 장치가, 상기 제2 객체 검출 모델을 통해 상기 검출된 객체 및 클래스 간의 유사 A_05_도에 기반한 제2 클래스 스코어를 산출하는 단계, 상기 학습 장치가, 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 상기 검출된 객체의 클래스를 추정하는 단계를 포함하는 것을 특징으로 한다. 구체적으로, 상기 제2 객체 검출 모델에 입력하는 단계 이전에 상기 제2 객체 검출 모델을 사전 학습하는 단계 를 더 포함하는 것을 특징으로 한다. 상기 사전 학습하는 단계는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성하는 것을 특징으로 한다. 상기 사전 학습하는 단계는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스 트 인코더(text encoder)를 통해 상기 이미지 임베딩 및 상기 텍스트 임베딩을 추출하는 것을 특징으로 한다.상기 사전 학습하는 단계는 상기 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍에서 긍정적 쌍(positive pai r)의 코사인 유사A_05_도(cosine similarity)는 최대화하고, 부정적 쌍(negative pair)의 코사인 유사A_05_도 는 최소화하A_05_도록 상기 비전 인코더 및 상기 텍스트 인코더를 사전 학습하는 것을 특징으로 한다. 상기 사전 학습하는 단계는 CE 손실 함수(cross entropy loss function)를 통해 상기 비전 인코더 및 상기 텍스 트 인코더를 사전 학습하는 것을 특징으로 한다. 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지와 연관된 자연어 텍스트 인 것을 특징으로 한다. 상기 제2 클래스 스코어를 산출하는 단계는 상기 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임 베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성하는 단계 및 상기 제2 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단 계를 포함하는 것을 특징으로 한다. 상기 임베딩 쌍을 구성하는 단계는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 상기 이미지 임베딩 및 상기 텍스트 임베딩을 추출하는 것을 특징으로 한다. 상기 제2 클래스 스코어를 산출하는 단계는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출하는 것을 특징으로 한다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 117, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_05_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.) 상기 클래스를 추정하는 단계는 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 클래스 스코어를 재조정하는 단계 및 상기 재조정된 클래스 스코어를 기초로 상기 클래스를 추정하는 단계를 포함하는 것을 특징 으로 한다. 상기 재조정하는 단계는 하기의 수학식 2를 통해 클래스 스코어를 재조정하는 것을 특징으로 하는, 객체 검출 방법. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 118, "content": "(여기서, α는 상기 제2 객체 검출 모델에 의해 산출된 유사A_05_도의 영향력을 조절하는 하이퍼 파라미터이고, Sik는 제2 클래스 스코어, sik는 제1 클래스 스코어를 의미한다.) 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 객체 검출 방법을 실행하기 위하여 기록매체에 기 록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메 모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리 고, 상기 컴퓨터 프로그램은 상기 프로세서가, 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받는 단계, 상기 프로세서가, 상기 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체를 검 출하고, 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하는 단계, 상기 프 로세서가, 상기 검출된 객체를 제2 객체 검출 모델에 입력하는 단계, 상기 프로세서가, 상기 제2 객체 검출 모 델을 통해 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단계, 상기 프로세서가, 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 상기 검출된 객체의 클래스를 추정하 는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_05_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 119, "content": "A_05_발명의 효과 본 발명의 실시 예들에 따르면, 서로 다른 학습 모델로부터 산출된 클래스 스코어를 기초로 객체의 클래스를 추 정함으로써, 추가 학습을 수행하지 않고A_05_도 객체 검출 정확A_05_도를 증가시킬 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 120, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 121, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 122, "content": "A_05_도면의 간단한 설명 A_05_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_05_도이다. A_05_도 3은 본 발명의 일 실시예에 따른 학습 장치의 논리적 구성A_05_도이다. A_05_도 4는 본 발명의 일 실시예에 따른 학습 장치의 하드웨어 구성A_05_도이다. A_05_도 5는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 순서A_05_도이다. A_05_도 6은 본 발명의 일 실시예에 따른 S110 단계를 구체적으로 나타낸 순서A_05_도이다. A_05_도 7은 본 발명의 일 실시예에 따른 S130 단계를 구체적으로 나타낸 순서A_05_도이다. A_05_도 8은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 순서A_05_도이다. A_05_도 9는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 예시A_05_도이다. A_05_도 10은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 예시A_05_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 123, "content": "A_05_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_05_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_05_도하게 포괄적인 의미로 해석되거나, 과A_05_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_05_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_05_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_05_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_05_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_05_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_05_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_05_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_05_도면은 본 발명의 사상을 쉽게 이해할 수 있A_05_도록 하기 위한 것일 뿐, 첨부된 A_05_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_05_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_05_도 확장되는 것으로 해석되어야 한다. 한편, 일반적으로 방대한 양의 학습 데이터를 요구하는 비전 태스크(vision task)는 데이터의 수집, 가공 및 학 습에 많은 시간과 자원을 필요로 한다. 데이터의 양이 적은 경우 과적합이나 정확A_05_도에서 문제가 발생할 수 있지만, 최근에는 적은 양의 데이터로 A_05_도 높은 정확A_05_도의 검출이 가능한 퓨샷 객체 검출(few shot object detection)에 대한 연구가 활발히 진행되고 있다. 한편, 메타 학습(meta learning)은 적은 양의 데이터와 주어진 환경만으로A_05_도 스스로 학습하고, 학습한 정 보와 알고리즘을 새로운 문제에 적용하여 해결하는 학습 방법을 의미한다. 최근에는 메타 학습을 퓨샷 객체 검출에 적용하는 연구가 진행되고 있으나, 정확A_05_도가 보장된 만큼 복잡한 에피소드 셋팅으로 인한 연산이 비효율 적이라는 한계점이 존재했다. 이러한 한계를 극복하고자, 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있 는 다양한 수단들을 제안하고자 한다. A_05_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_05_도이다. A_05_도 1에 A_05_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 생성 장 치, 하나 이상의 어노테이션 장치(200-1, 200-2, …, 200-n; 200), 학습 데이터 검증 장치 및 학습 장치를 포함하여 구성될 수 있다. 또한, A_05_도 2에 A_05_도시된 바와 같이, 본 발명의 다른 실시예에 따른 인공지능 학습 시스템은 하나 이상의 어노테이션 장치(200-a, 200-b, …, 200-m; 200)와 학습 데이터 검증 장치(300-a, 300-b, …, 300-m; 300)가 하나로 이루어진 복수 개의 그룹(Group-a, Group-b …, Group-m), 학습 데이터 생성 장치 및 학습 장치 를 포함하여 구성될 수 있다. 이와 같은, 다양한 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지 능(AI)을 기계 학습시키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 이와 같은, 학습 데이터 생성 장치는 기본적으로 학습 데이터 검증 장치와 구분되는 장치이나, 실제 물리적 환경에서 학습 데이터 생성 장치와 학습 데이터 검증 장치가 하나의 장치로 통합되어 구현될 수A_05_도 있다. 구체적으로, 학습 데이터 설계 장치는 학습 장치로부터 인공지능(AI) 학습과 관련된 프로젝트의 속성 을 수신할 수 있다. 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지 능(AI) 학습을 위한 데이터 구조의 설계, 수집된 데이터의 정제, 데이터의 가공, 데이터의 확장 및 데이터의 검 증을 수행할 수 있다. 우선적으로, 학습 데이터 설계 장치는 인공지능(AI) 학습을 위한 데이터 구조를 설계할 수 있다. 예를 들 어, 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지능(AI) 학습을 위한 온톨로지(ontology), 인공지능(AI) 학습을 위한 데이터의 분류 체계를 정의할 수 있다. 학습 데이터 설계 장치는 설계된 데이터 구조를 기초로, 인공지능(AI) 학습을 위한 데이터를 수집할 수 있 다. 이를 위하여, 학습 데이터 설계 장치는 외부로부터 3D 점군 데이터 및 2D 이미지들을 입력 받거나, 웹 크롤링(web crawling)을 수행하여 3D 점군 데이터 및 2D 이미지들을 수집하거나, 또는 외부 기관의 장치로부터 3D 점군 데이터 및 2D 이미지들을 다운로드 할 수 있다. 여기서, 3D 점군 데이터는 차량에 고정 설치된 라이다(lidar)에 의해 획득된 데이터이다. 차량에 고정 설치된 라이다는 레이저 펄스를 발사하고, 차량 주위에 위치하는 객체들에 의해 반사되어 돌아온 빛을 감지하여, 차량 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 즉, 3D 점군 데이터를 구성하는 점군은라이다에 의해 3차원 공간으로 발사된 레이저 펄스를 반사시킨 점(point)들의 집합을 의미한다. 그리고, 2D 이미지는 차량에 고정 설치된 복수 개의 카메라에 의해 촬영된 이미지이다. 자율주행을 위하여 하나 의 차량에는 다수 개의 카메라가 고정 설치되어, 차량 주위에 대한 2차원 이미지를 각각 촬영할 수 있다. 예를 들어, 하나의 차량에 6개의 카메라가 설치될 수 있으나, 이에 한정되지 않는다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들 중에서 중복되거나 또는 극히 유사한 데 이터를 제거할 수 있다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들에 포함된 개인정 보를 비식별화(de-identification)할 수 있다. 학습 데이터 생성 장치는 수집 및 정제된 3D 점군 데이터 및 2D 이미지들을 복수 개의 어노테이션 장치 에 분배하여 전송할 수 있다. 이 경우, 학습 데이터 생성 장치는 어노테이션 장치의 작업자(즉, 라벨러)에 대하여 사전에 할당된 양에 따라 3D 점군 데이터 및 2D 이미지들을 분배할 수 있다. 학습 데이터 생성 장치는 어노테이션 장치로부터 직접 어노테이션 작업 결과물을 수신하거나, 또는 학습 데이터 검증 장치로부터 어노테이션 작업 결과물 및 검수 결과를 수신할 수 있다. 학습 데이터 생성 장치는 수신된 어노테이션 작업 결과물을 패키징(packaging)하여 인공지능(AI) 학습용 데이터를 생성할 수 있다. 그리고, 학습 데이터 생성 장치는 생성된 인공지능(AI) 학습용 데이터를 학습 장치에 전송할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_05_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 작업을 수행하 는데 사용될 수 있는 장치이다. 여기서, 어노테이션 작업은 바운딩 박스(bounding box)를 설정하고, 객체의 속성 정보를 포함하는 클래스 (class) 정보를 입력하는 과정을 포함할 수 있다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치 또는 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_05_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수A_05_도 있다. 다음 구성으로, 학습 데이터 검증 장치는 인공지능(AI) 학습용 데이터를 검증하는데 사용될 수 있는 장치 이다. 즉, 학습 데이터 검증 장치는 어노테이션 장치에 의해 생성된 어노테이션 작업 결과물이 사전 에 설정된 목표 품질에 부합하는지 여부, 또는 어노테이션 작업 결과물이 인공지능(AI) 학습에 유효한지 여부를 검증할 수 있는 장치이다. 구체적으로, 학습 데이터 검증 장치는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 여기서, 어노테이션 작업 결과물은 3D 점군 데이터 및 2D 이미지들로부터 특정된 객체의 좌표와, 이미지 또는 객체에 대한 메타데이터가 포함될 수 있다. 어노테이션 작업 결과물의 메타데이터에는 특정된 객체의 카테 고리(category), 객체가 2D 이미지의 화각에 의해 잘려진 비율(truncation), 객체가 다른 객체 또는 물체에 의 해 가려진 비율(occlusion), 객체의 트래킹 아이디(tracking identifier), 이미지가 촬영된 시각, 이미지가 촬 영된 날의 기상 조건 등이 포함될 수 있으며, 이에 한정되는 것은 아니다. 이와 같은, 어노테이션 작업 결과물 은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것A_05_도 아니다. 학습 데이터 검증 장치는 수신된 어노테이션 작업 결과물을 검수할 수 있다. 이를 위하여, 학습 데이터 검 증 장치는 어노테이션 작업 결과물을 대상으로 스크립트(script)를 이용하여 검수를 수행할 수 있다. 여기 서, 스크립트는 어노테이션 작업 결과물을 대상으로 사전에 설정된 목표 품질의 부합 여부 또는 데이터 유효성여부를 검증하기 위한 코드이다. 그리고, 학습 데이터 검증 장치는 어노테이션 장치들로부터 수신된 어노테이션 작업 결과물 및 검수 결과를 학습 데이터 생성 장치에 전송할 수 있다. 상술한 바와 같은 특징을 가지는, 학습 데이터 검증 장치는 어노테이션 장치 및 학습 데이터 생성 장 치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라 A_05_도 허용될 수 있다. 예를 들어, 학습 데이터 검증 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 학습 장치는 학습 데이터 생성 장치로부터 제공된 이미지를 통해 인공지능(AI)을 기 계 학습하는데 사용될 수 있는 장치이다. 특히, 본 발명의 일 실시예에 따른 학습 장치는 적어A_05_도 하나의 객체를 포함하는 랜덤(random) 한 이 미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습하고, 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하고, 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 장치는 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입 력 받고, 제1 객체 검출 모델을 통해 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클 래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하고, 검출된 객체를 제2 객체 검출 모델에 입력하고, 제2 객체 검출 모델을 통해 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하고, 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있다. 한편, 본 발명의 일 실시예에 따른 학습 장치의 구체적인 구성에 대해서는 이하, A_05_도 3 및 A_05_도 4 를 참조하여 상세히 설명하A_05_도록 한다. 이와 같은, 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_05_도 허용될 수 있다. 예를 들어, 학습 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것 은 아니다. A_05_도 3은 본 발명의 일 실시예에 따른 학습 장치의 논리적 구성A_05_도이다. A_05_도 3에 A_05_도시된 바와 같이, 본 발명의 일 실시예에 따른 학습 장치는 통신부, 입출력부 , 학습 모델 생성부, 스코어 재조정부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 데이터 셋을 입력 받을 수 있다. 학습 데이터 생성 장치로부터 수신한 데이터 셋은 적어A_05_도 하나의 객체, 적어A_05_도 하나의 객체의 클래스(class) 정보 및 적어A_05_도 하나의 객체에 어노테이션(annotation) 수행된 바운딩 박스(bounding box) 정보를 포함할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해 사용자로부터 신호를 입력 받거나, 연산 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 학습 모델을 학습하기 위한 설정 신호, 객체 검출 모델의 클래스 스코어를 재조정 하기 위한 설정 신호 등을 입력 받을 수 있다. 다음 구성으로, 학습 모델 생성부는 높은 정확A_05_도를 가지는 학습 모델을 생성할 수 있다. 이를 위해, 학습 모델 생성부는 적어A_05_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습할 수 있다. 예를 들어, 랜덤 한 이미지를 포함하는 제1 데이터 셋은 공개된 공공의 데이터 셋인 \"Imagenet1k\" 데이터 셋을 사용할 수 있다. 구체적으로, 학습 데이터 생성부는 균일한 제약 조건으로 사전에 설정된 비율에 따라 제1 데이터 셋에 포 함된 각 이미지를 샘플링 할 수 있다. 예를 들어, 학습 모델 생성부는 제1 데이터 셋에 포함된 각 이미지의 모든 2x2 그리드에서 패치(patch)를 샘플링 할 수 있다. 이에 따라, 샘플링 된 제1 데이터 셋은 모든 로컬 윈A_05_도우(local window)에서 균일한 양의 패치가 샘플링되기 때문에 피라미드 기반의 비전 트랜스포머(vision transformer)에 적용될 수 있다. 또한, 샘플링 된 이미지는 크기가 1/4로 줄어 연산량을 줄일 수 있다. 여기서, 피라미드 기반의 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나가 될 수 있다. 학습 모델 생성부는 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 할 수 있다. 이때, 학습 모델 생성부는 피라미드 기반의 비전 트랜스포머가 PVT 인 경우, 샘플링 된 이미지의 가시적 패치 중 에서 무작위로 마스킹을 수행하되, PVT 와의 호환성을 위하여 공유 마스크 토큰을 사용하여 마스킹 된 패치를 대체할 수 있다. 여기서, 공유 마스크 토큰은 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector)가 될 수 있다. 학습 모델 생성부는 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습할 수 있다. 즉, 학습 모델 생성부는 샘플링 및 마스킹을 수행한 제1 데이터 셋을 피 라미드 기반의 비전 트랜스포머의 인코더(encoder)에 입력할 수 있다. 다음으로, 학습 모델 생성부는 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성할 수 있다. 구체적으로, 학습 모델 생성부는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 다음으로, 학습 모델 생성부는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모 델을 학습할 수 있다. 여기서, 제2 데이터 셋은 클래스 별 객체 수가 사전 설정된 값보다 많은 베이스 클래스(base class) 이미지 그 룹 및 클래스 별 객체 수가 사전 설정된 값보다 적은 노벨 클래스(novel class) 이미지 그룹을 포함할 수 있다. 이때, 노벨 클래스 이미지 그룹 및 베이스 클래스 이미지 그룹은 서로 중복되지 않을 수 있다. 구체적으로, 학습 모델 생성부는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습할 수 있다. 즉, 학습 모델 생성부는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_05_도록 할 수 있다. 또한, 학습 모델 생성부는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 모델 생성부는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵 커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 모델 생성부는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박 스를 생성할 수 있다. 이때, 학습 모델 생성부는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으 로 앵커 박스를 생성하는 기준점인 앵커를 고정할 수 있다. 또한, 학습 모델 생성부는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클 래스(class)에 대한 클래스 스코어를 산출할 수 있다. 학습 모델 생성부는 산출된 클래스 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 모델 생성부는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐 (region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 모델 생성부는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달 된 리젠 프로포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 모델 생성부는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 모델 생성부는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설 정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 모델 생성부는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결 과를 이용하여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 다음으로, 학습 모델 생성부는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있 다. 구체적으로, 학습 모델 생성부는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_05_도 (cosine similarity) 층으로 대체하고, 코사인 유사A_05_도 층의 학습율을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_05_도 층은 내적 공간의 두 벡터간 각A_05_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_05_도를 측정하는 층이다. 다음 구성으로, 스코어 재조정부는 서로 다른 학습 모델로부터 산출된 클래스 스코어를 기초로 클래스 스 코어를 재조정함으로써, 높은 정확A_05_도로 검출된 객체의 클래스를 추정할 수 있다. 이를 위해, 스코어 재조정부는 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받을 수 있다. 여 기서, 제1 객체 검출 모델은 상술한 학습 모델 생성부에 의해 생성된 제4 학습 모델이 될 수 있다. 다음으로, 스코어 재조정부는 제1 객체 검출 모델을 통해 적어A_05_도 하나의 이미지에 포함된 객체를 검 출하고, 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출할 수 있다. 다음으로, 스코어 재조정부는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델에 입력할 수 있다. 한편, 스코어 재조정부는 객체를 제2 객체 검출 모델에 입력하기 이전에, 제2 객체 검출 모델을 사전 학습 할 수 있다. 구체적으로, 스코어 재조정부는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지 와 연관된 자연어 텍스트가 될 수 있다. 스코어 재조정부는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스트 인코더(text encoder)를 통해 이미지 임베딩 및 상기 텍스트 임베딩을 추출할 수 있다. 이후, 스코어 재조정부는 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍에서 긍정적 쌍(positive pair)의 코사인 유사A_05_도(cosine similarity)는 최대화하고, 부정적 쌍(negative pair)의 코사인 유사A_05_도는 최 소화하A_05_도록 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 스코어 재조정부는 CE 손실 함수 (cross entropy loss function)를 통해 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 다음으로, 스코어 재조정부는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래 스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출할 수 있다.구체적으로, 스코어 재조정부는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋 에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 스코어 재조정부는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍스트 임베딩을 추출할 수 있다. 스코어 재조정부는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_05_도에 기반한 제2 클래스 스 코어를 산출할 수 있다. 구체적으로, 스코어 재조정부는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 124, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_05_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.) 다음으로, 스코어 재조정부는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있다. 구체적으로, 스코어 재조정부는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 클래스 스코어를 재조 정할 수 있다. 즉, 스코어 재조정부는 하기의 수학식 2를 통해 클래스 스코어를 재조정할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 125, "content": "(여기서, α는 상기 제2 객체 검출 모델에 의해 산출된 유사A_05_도의 영향력을 조절하는 하이퍼 파라미터이고, Sik는 제2 클래스 스코어, sik는 제1 클래스 스코어를 의미한다.) 그리고, 스코어 재조정부는 재조정된 클래스 스코어를 기초로 클래스를 추정할 수 있다. 즉, 스코어 재조 정부는 재조정된 클래스 스코어를 기초로 클래스 스코어가 가장 높은 클래스를 검출된 객체의 클래스로 추 정할 수 있다. 이하, 상술한 바와 같은 학습 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구체적으 로 설명한다. A_05_도 4는 본 발명의 일 실시예에 따른 학습 장치의 하드웨어 구성A_05_도이다. A_05_도 4에 A_05_도시된 바와 같이, 학습 장치는 프로세서(Processor, 450), 메모리(Memory, 455), 송수 신기(Transceiver, 460), 입출력장치(Input/output device, 465), 데이터 버스(Bus, 470) 및 스토리지 (Storage, 475)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)에 따른 명 령어를 기초로, 학습 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치, 어 노테이션 장치 및 학습 데이터 검증 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 장 치의 동작에 필요한 데이터를 입력 받거나, 학습 결과물을 출력할 수 있다. 데이터 버스는 프로세서 , 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(480a)의 실행을 위해 필요한 애플리케이 션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스 (resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어 (480b)를 저장할 수 있다. 또한, 스토리지는 본 발명의 실시예들에 따른 방법의 수행에 필요한 정보들을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 학습 방법을 구현하기 위한 소프트웨어(480a, 480b)는 프로세서가 적어A_05_도 하나의 객체를 포함하는 랜덤(random) 한 이미지 를 포함하는 제1 데이터 셋을 기초로 제1 학습 모델을 사전 학습하는 단계, 프로세서가, 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하여 제3 학습 모델을 생성하는 단계 및 프로세서가, 데이터 수가 사전 설정된 값보다 작은 제2 데이터 셋을 기초로 제3 학습 모델을 학습하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 객체 검출 방법을 구 현하기 위한 소프트웨어(480a, 480b)는 프로세서가 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입 력 받는 단계, 프로세서가, 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하는 단계, 프로세서 가, 검출된 객체를 제2 객체 검출 모델에 입력하는 단계, 프로세서가, 제2 객체 검출 모델을 통해 검 출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단계, 프로세서가, 제1 클 래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정하는 단계를 실행시키기 위하여, 기록 매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_05_도 4에 A_05_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_05_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_05_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_05_도록 구성될 수 있으며, 그 역A_05_도 마찬가지이다.이하, 본 발명의 일 실시예에 따른 학습 방법에 대하여 상세히 설명하A_05_도록 한다. A_05_도 5는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 순서A_05_도이고, A_05_도 6은 본 발명의 일 실시예에 따른 S110 단계를 구체적으로 나타낸 순서A_05_도이고, A_05_도 7은 본 발명의 일 실시예에 따른 S130 단계를 구체적으로 나타낸 순서A_05_도이다. A_05_도 5 내지 A_05_도 7을 참조하면, 먼저 S110 단계에서 학습 장치는 적어A_05_도 하나의 객체를 포함하는 랜덤(random) 한 이미지를 포함하는 제1 데이터 셋을 기초로 객체 검출을 위한 제1 학습 모델을 사전 학습할 수 있다. 구체적으로, S111 단계에서 학습 장치는 균일한 제약 조건으로 사전에 설정된 비율에 따라 제1 데이터 셋에 포 함된 각 이미지를 샘플링 할 수 있다. 예를 들어, 학습 장치는 제1 데이터 셋에 포함된 각 이미지의 모든 2x2 그리드에서 패치(patch)를 샘플링 할 수 있다. 이에 따라, 샘플링 된 제1 데이터 셋은 모든 로컬 윈A_05_도우(local window)에서 균일한 양의 패치가 샘 플링되기 때문에 피라미드 기반의 비전 트랜스포머(vision transformer)에 적용될 수 있다. 또한, 샘플링 된 이 미지는 크기가 1/4로 줄어 연산량을 줄일 수 있다. 여기서, 피라미드 기반의 비전 트랜스포머는 PVT(Pyramid Vision Transformer) 및 스윈 트랜스포머(Swin Transformer) 중 하나가 될 수 있다. S112 단계에서 학습 장치는 샘플링 된 제1 데이터 셋에 포함된 각 이미지를 마스킹(masking) 할 수 있다. 이때, 학습 장치는 피라미드 기반의 비전 트랜스포머가 PVT 인 경우, 샘플링 된 이미지의 가시적 패치 중에서 무작위 로 마스킹을 수행하되, PVT 와의 호환성을 위하여 공유 마스크 토큰을 사용하여 마스킹 된 패치를 대체할 수 있 다. 여기서, 공유 마스크 토큰은 수행된 마스킹의 존재를 나타내는 임베딩 벡터(embedding vector)가 될 수 있다. S113 단계에서 학습 장치는 마스킹 된 제1 데이터 셋을 기초로 피라미드 기반의 비전 트랜스포머(vision transformer)를 학습할 수 있다. 즉, 학습 장치는 샘플링 및 마스킹을 수행한 제1 데이터 셋을 피라미드 기반의 비전 트랜스포머의 인코더 (encoder)에 입력할 수 있다. 다음으로, S120 단계에서 학습 장치는 사전 학습된 제1 학습 모델을 객체 검출을 위한 제2 학습 모델과 결합하 여 제3 학습 모델을 생성할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 구체적으로, 학습 장치는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입 할 수 있다. 다음으로, S130 단계에서 학습 장치는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다. 구체적으로, S131 단계에서 학습 장치는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습하여 제4 학습 모델을 생성할 수 있다. 즉, 학습 장치는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_05_도록 할 수 있다. 또한, 학습 장치는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 장치는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가 지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 장치는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박스를 생성할 수 있다. 이때, 학습 장치는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으로 앵커 박스를 생성하는기준점인 앵커를 고정할 수 있다. 또한, 학습 장치는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클래스(class)에 대한 스코어를 산출할 수 있다. 학습 장치는 산출된 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 장치는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐(region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 장치는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달된 리젠 프로 포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 장치는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 장치는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 장치는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결과를 이용하 여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 다음으로, 학습 장치는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있다. 이때, 학 습 장치는 제4 학습 모델의 출력 층의 가중치를 초기하여 미세 조정하되, 출력 층을 제외한 나머지 구성요소의 가중치는 고정시킬 수 있다. 구체적으로, 학습 장치는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_05_도(cosine similarity) 층으로 대체하고, 코사인 유사A_05_도 층의 학습률을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_05_도 층은 내적 공간의 두 벡터간 각A_05_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_05_도를 측정하는 층이다. A_05_도 8은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 순서A_05_도이다. A_05_도 8을 참조하면, 먼저 S210 단계에서 학습 장치는 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입 력 받을 수 있다. 다음으로, S220 단계에서 학습 장치는 제1 객체 검출 모델을 통해 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출할 수 있다. 다음으로, S230 단계에서 학습 장치는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델에 입력할 수 있다. 한편, 학습 장치는 객체를 제2 객체 검출 모델에 입력하기 이전에, 제2 객체 검출 모델을 사전 학습할 수 있다. 구체적으로, 학습 장치는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍 스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지 와 연관된 자연어 텍스트가 될 수 있다. 학습 장치는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스트 인코더(text encoder)를 통해 이미지 임베딩 및 상기 텍스트 임베딩을 추출할 수 있다. 이후, 학습 장치는 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍에서 긍정적 쌍(positive pair)의 코사인 유 사A_05_도(cosine similarity)는 최대화하고, 부정적 쌍(negative pair)의 코사인 유사A_05_도는 최소화하 A_05_도록 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 학습 장치는 CE 손실 함수(cross entropy loss function)를 통해 비전 인코더 및 텍스트 인코더를 사전 학습할 수 있다. 다음으로, S240 단계에서 학습 장치는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출할 수 있다. 구체적으로, 학습 장치는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성할 수 있다. 여기서, 학습 장치는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍 스트 임베딩을 추출할 수 있다. 학습 장치는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_05_도에 기반한 제2 클래스 스코어를 산출 할 수 있다. 구체적으로, 학습 장치는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출할 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 126, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_05_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.) 다음으로, S250 단계에서 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스 를 추정할 수 있다. 구체적으로, 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 클래스 스코어를 재조정할 수 있다. 즉, 학습 장치는 하기의 수학식 2를 통해 클래스 스코어를 재조정할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 127, "content": "(여기서, α는 상기 제2 객체 검출 모델에 의해 산출된 유사A_05_도의 영향력을 조절하는 하이퍼 파라미터이고, Sik는 제2 클래스 스코어, sik는 제1 클래스 스코어를 의미한다.) 그리고, 학습 장치는 재조정된 클래스 스코어를 기초로 클래스를 추정할 수 있다. 즉, 학습 장치는 재조정된 클 래스 스코어를 기초로 클래스 스코어가 가장 높은 클래스를 검출된 객체의 클래스로 추정할 수 있다. A_05_도 9는 본 발명의 일 실시예에 따른 학습 방법을 설명하기 위한 예시A_05_도이다. A_05_도 9에 A_05_도시된 바와 같이, 학습 장치는 사전 학습된 제1 학습 모델(1st learning model)을 객체 검출 을 위한 제2 학습 모델(2nd learning model)과 결합하여 제3 학습 모델을 생성할 수 있다. 여기서, 제2 학습 모델은 \"Faster R-CNN\"이 될 수 있다. \"Faster R-CNN\"은 백본(backbone) 층, RPN(Region Proposal Network) 층, 관심영역 풀링(ROI Pooling) 층, 관심영역 추출(ROI Feat.Extractor) 층 및 출력층으로 구성될 수 있다. 한편, 일반적인 \"Faster R-CNN\"의 백본 층은 FPN(Feature Pyramid Network)로 구성될 수 있다. 구체적으로, 학습 장치는 제1 학습 모델에 포함된 인코더(encoder)를 제2 학습 모델의 백본 층을 대체하여 삽입 할 수 있다. 다음으로, 학습 장치는 데이터 수가 사전 설정된 값보다 적은 제2 데이터 셋을 기초로 제3 학습 모델을 학습할 수 있다. 구체적으로, 학습 장치는 제2 데이터 셋에 포함된 베이스 클래스 이미지 그룹을 기초로 제3 학습 모델을 학습하 여 제4 학습 모델을 생성할 수 있다. 즉, 학습 장치는 베이스 클래스 이미지 그룹을 제3 학습 모델에 입력하여, 제3 학습 모델의 백본 층에 구비된 사전 학습된 제1 학습 모델의 인코더가 특징맵(feature map)을 생성하A_05_도록 할 수 있다.또한, 학습 장치는 생성된 특징맵을 RPN 층에 입력할 수 있다. 학습 장치는 RPN 층에서 지정된 위치에 사전에 정의한 서로 다른 크기(scale)와 가로세로비(aspect ratio)를 가 지는 바운딩 박스(bounding box)인 앵커 박스(Anchor box)를 생성할 수 있다. 여기서, 스케일은 앵커 박스의 폭 및 높이의 길이를 의미할 수 있다. 가로세로비는 폭 및 높이의 길이의 비율을 의미할 수 있다. 학습 장치는 앵커 박스를 생성하되, 이미지의 각 그리드 셀(grid cell)의 중심을 기준으로 앵커 박스를 생성할 수 있다. 이때, 학습 장치는 이미지의 서브 샘플링 비(sub sampling ratio)를 기준으로 앵커 박스를 생성하는 기준점인 앵커를 고정할 수 있다. 또한, 학습 장치는 앵커 박스에 대응한 리젠 프로포즐(region proposal)에 대하여 사전 저장된 클래스(class)에 대한 스코어를 산출할 수 있다. 학습 장치는 산출된 스코어를 통해 이미지 내에 객체가 포함되어 있는지 여부를 분류할 수 있다. 이때, 학습 장치는 클래스 스코어에 따라 사전 설정된 값보다 높은 스코어를 갖는 리젠 프로포즐(region proposal)만을 추출하여 관심영역 풀링 층으로 전달할 수 있다. 또한, 학습 장치는 사전 학습된 제1 학습 모델의 인코더에서 출력된 특징맵과 RPN 층으로부터 전달된 리젠 프로 포즐(region proposal)을 통해 관심영역 풀링을 수행하여 고정된 크기의 특징맵을 얻을 수 있다. 구체적으로, 학습 장치는 특징맵에서 리젠 프로포즐(region proposal)에 해당하는 관심영역(Region of interest)을 지정한 크기의 그리드(grid)로 나눈 후 맥스 풀링(max pooling)을 수행할 수 있다. 이후, 학습 장치는 고정된 크기의 특징맵을 FC 레이어(fully connected layer)에 입력하여 사전 설정된 크기의 특징 벡터(feature vector)를 생성할 수 있다. 학습 장치는 생성된 특징 벡터를 분류기(classifier) 및 회기자(regressor)에 입력하고, 출력된 결과를 이용하 여 멀티 태스크 로스(multi task loss) 함수를 통해 제3 학습 모델을 학습할 수 있다. 그리고, 학습 장치는 제2 데이터 셋을 기초로 제4 학습 모델을 미세 조정(fine tuning)할 수 있다. 이때, 학습 장치는 제4 학습 모델의 출력 층의 가중치를 초기하여 미세 조정하되, 출력 층을 제외한 나머지 구성요소의 가 중치는 고정시킬 수 있다. 구체적으로, 학습 장치는 제4 학습 모델의 출력 층을 초기화하여, 출력 층을 코사인 유사A_05_도(cosine similarity) 층으로 대체하고, 코사인 유사A_05_도 층의 학습률을 사전 설정된 값으로 미세 조정할 수 있다. 여기서, 코사인 유사A_05_도 층은 내적 공간의 두 벡터간 각A_05_도의 코사인 값을 이용하여 측정된 벡터 간의 유사A_05_도를 측정하는 층이다. A_05_도 10은 본 발명의 일 실시예에 따른 객체 검출 방법을 설명하기 위한 예시A_05_도이다. A_05_도 10에 A_05_도시된 바와 같이, 학습 장치는 객체 검출을 위한 제1 객체 검출 모델(1st detection mode l)에 적어A_05_도 하나의 이미지(image)를 입력하여 이미지로부터 식별된 객체(object 1, object 2, object 3)를 검출할 수 있다. 다음으로, S230 단계에서 학습 장치는 검출된 객체를 객체 검출을 위한 제2 객체 검출 모델(2nd detection model)에 입력할 수 있다. 학습 장치는 제2 객체 검출 모델을 통해 제1 객체 검출 모델에 의해 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출할 수 있다. 구체적으로, 학습 장치는 제1 객체 검출 모델에 의해 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍(I,T)을 구성할 수 있다. 여기서, 학습 장치는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 이미지 임베딩 및 텍 스트 임베딩을 추출할 수 있다. 학습 장치는 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_05_도에 기반한 제2 클래스 스코어를 산출 할 수 있다.그리고, 학습 장치는 제1 클래스 스코어 및 제2 클래스 스코어를 기초로 검출된 객체의 클래스를 추정할 수 있 다. 이하, 본 발명의 일 실시예에 따른 학습 장치의 객체 검출 성능에 대하여 설명하A_05_도록 한다. 한편, 후술할 성능 실험을 위한 데이터 셋은 퓨샷 객체 검출 분야에서 표준적으로 쓰이는 \"PASCAL VOC 퓨샷 데 이터 셋\"이다. \"PASCAL VOC 퓨샷 데이터 셋\"은 20개의 클래스 중에서 15개의 클래스는 베이스 클래스 그룹이며, 나머지 5개의 클래스는 노벨 클래스 그룹으로 이루어져 있다. 실시예 실시예는 \"Imagenet1k 데이터셋\"을 상술한 방법으로 샘플링하고, 마스킹 한 후 SWIN 트랜스포머를 800 epoch 사 전 학습하였다. 그리고, 사전 학습된 SWIN 트랜스포머는 FPN을 대체하여 \"Faster R-CNN\"의 백본으로 삽입한 후 베이스 클래스 그룹을 통해 학습을 진행하였다. 이때, 학습 조건은 16 배치 크기, AdamW(1e-4 기본학습률, 0.05 weight decay)와 WarmupMultiStepLR(2000 iteration Warm up, 3×스케줄)을 사용하여 75 epoch 학습을 진행하였다. 그리고, 학습된 학습 모델의 마지막 출력층을 코사인 유사A_05_도 레이어로 바꾸어 해당 레이어만 1e-5의 학습 율로 미세 조정하였다. 실험 [표 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 128, "content": "한편 표 1은 \"PASCAL VOC 2007+2012 K-shot 객체 검출 학습 데이터셋\"을 학습하여 \"PASCAL VOC 2007 테스트 셋\"의 검출 성능을 여러 모델과 비교한 표이다. 표 1을 참조하면, 스플릿(split)은 특정 클래스에 대해서만 편향되는 것을 방지하기 위하여 다양한 노벨 클래스 그룹(Novel Set 1, Novel Set 2, Novel Set 3)을 적용시켰다. 실험 결과, 실시예(Ours)는 \"Faster R-CNN\"이나, \"MetaYOLO\"보다 평균적으로 높은 성능을 기록하였으며, \"TF A\"와 비슷한 성능을 보이는 것을 확인할 수 있었다.[표 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 129, "content": "한편, 표 2는 표 1의 노벨 클래스 그룹 1(Novel Set 1)의 검출 성능을 상세히 나타낸 표이다. 표 2를 참조하면, 단일 시드에 대한 성능은 (A)를 통해 확인할 수 있다. 즉, 작은 파라미터를 갖는 \"Swin-tiny\"버전은 비슷한 크기인 \"ResNet50을 사용한 \"TFA\"보다 모든 AP(average precision)에서 압A_05_도적인 성능을 보였고, \"ResNet101\"을 사용한 \"TFA\"와 비슷한 AP를 갖는다. 또한, \"Swin-tiny\"버전 대비 상대적으로 큰 파라미터를 갖는 \"Swin large\" 버전은 \"ResNet101\"을 사용한 \"TF A\"보다 AP가 3.4%, AP50이 1.2%, AP75가 4.0%로 모든 면에서 성능이 증가하였다. 또한, (B)에 표시된 여러 시드에 대한 평균은 작은 파라미터를 갖는 \"Swin-tiny\"버전이 \"TFA\"보다 성능이 우수 한 것을 확인할 수 있었다. 정리하면, 종래의 객체 검출 모델은 \"Imagenet1k 데이터셋\"을 지A_05_도 학습으로 사전 학습하지만, 실시예는 라벨이 없는 데이터셋으로 백본 층에 대한 사전 학습을 진행한다. [표 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 130, "content": "한편, 표 3은 실시예에 따라 사전 학습을 진행한 학습 모델과, 사전 학습을 진행하지 않은 학습 모델의 퓨샷 검 출 성능을 비교한 표이다. 표 3을 참조하면, 실시예는 사전 학습을 진행하지 않은 학습 모델보다 성능이 월등히 높은 것을 확인할 수 있었 고, 이에 따라 사전 학습이 학습 모델의 일반화 성능에 크게 기여함을 알 수 있었다. 또한, 실시예는 컴퓨터 비전 분야에서 표준적으로 쓰이는 FPN을 사용할 경우 AP가 더 낮아지는 결과(-11.0%)를 보였다. 이와 같이, 퓨샷 객체 검출에서는 백본망의 사전 학습 유무가 중요하므로 사전학습 되지 않은 FPN 층이 오히려 성능 저하를 일으킨다고 볼 수 있다. 결론 본 발명의 일 실시예에 따른 학습 방법은 낮은 연산양으로 높은 객체 검출 성능을 내는 MIM 기반의 사전 학습 트랜스포머를 이용한 퓨샷 객체 검출 방법을 제안한다. 앞서 실험한 바와 같이, 본 발명의 일 실시예에 따른 학습 방법은 기존의 \"CNN\"기반 모델보다 현저히 높은 성능 을 보인다는 것과, MIM을 통한 사전 학습이 일반화 성능에 크게 기여한다는 것을 확인할 수 있었다. 이상과 같이, 본 명세서와 A_05_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_05_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_05_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_05_부호의 설명 100 : 학습 데이터 생성 장치 200 : 어노테이션 장치 300 : 학습 데이터 검증 장치 400 : 학습 장치 405 : 통신부 410 : 입출력부 415 : 학습 모델 생성부 420 : 스코어 재조정부 425 : 저장부 A_05_청구범위 A_05_청구항 1 학습 장치가, 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받는 단계; 상기 학습 장치가, 상기 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하는 단계; 상기 학습 장치가, 상기 검출된 객체를 제2 객체 검출 모델에 입력하는 단계; 상기 학습 장치가, 상기 제2 객체 검출 모델을 통해 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제 2 클래스 스코어를 산출하는 단계; 및 상기 학습 장치가, 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 상기 검출된 객체의 클래스를 추정하는 단계; 를 포함하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 2 제1 항에 있어서, 상기 제2 객체 검출 모델에 입력하는 단계 이전에 상기 제2 객체 검출 모델을 사전 학습하는 단계; 를 더 포함하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 3 제2 항에 있어서, 상기 사전 학습하는 단계는 사전 저장된 이미지 및 텍스트 데이터 쌍에서 이미지 임베딩(image embedding) 및 텍스트 임베딩(text embedding)을 추출하여, 복수의 제1 이미지 임베딩 및 텍스트 임베딩 쌍을 구성하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 4 제3 항에 있어서, 상기 사전 학습하는 단계는 트랜스포머(transformer)의 인코더로 구성된 비전 인코더(vision encoder) 및 텍스트 인코더(text encoder)를 통해 상기 이미지 임베딩 및 상기 텍스트 임베딩을 추출하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 5 제4 항에 있어서, 상기 사전 학습하는 단계는 CE 손실 함수(cross entropy loss function)를 통해 상기 비전 인코더 및 상기 텍스트 인코더를 사전 학습하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 6 제3 항에 있어서, 상기 사전 저장된 이미지 및 텍스트 데이터 쌍은 웹 크롤링(web crawling)을 통해 추출된 이미지 및 이미지와 연관된 자연어 텍스트 인 것을 특징으로 하는, 객 체 검출 방법. A_05_청구항 7 제1 항에 있어서, 상기 제2 클래스 스코어를 산출하는 단계는 상기 검출된 객체 및 사전 저장된 텍스트 데이터 셋에서 이미지 임베딩 및 텍스트 임베딩을 추출하여, 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍을 구성하는 단계; 및 상기 복수의 제2 이미지 임베딩 및 텍스트 임베딩 쌍의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단 계; 를 포함하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 8 제7 항에 있어서, 상기 임베딩 쌍을 구성하는 단계는 트랜스포머의 인코더로 구성된 비전 인코더 및 텍스트 인코더를 통해 상기 이미지 임베딩 및 상기 텍스트 임베 딩을 추출하는 것을 특징으로 하는, 객체 검출 방법. A_05_청구항 9 제8 항에 있어서, 상기 제2 클래스 스코어를 산출하는 단계는 하기의 수학식 1을 통해 상기 제2 클래스 스코어를 산출하는 것을 특징으로 하는, 객체 검출 방법. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 131, "content": "(여기서, I 및 T는 각각 이미지 임베딩 및 텍스트 임베딩이고, i 및 k는 각각 이미지 임베딩 및 텍스트 임베딩 의 인덱스(index)이고, 는 온A_05_도의 역수를 의미하는 하이퍼 파라미터를 의미한다.) A_05_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받는 단계; 상기 프로세서가, 상기 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체를 검출하고, 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하는 단계; 상기 프로세서가, 상기 검출된 객체를 제2 객체 검출 모델에 입력하는 단계; 상기 프로세서가, 상기 제2 객체 검출 모델을 통해 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단계; 및 상기 프로세서가, 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 상기 검출된 객체의 클래스를 추정하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 132, "content": "A_05_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 133, "content": "A_05_요약 본 발명은 적은 양의 학습 데이터를 이용하여 높은 객체 검출 성능을 발현할 수 있는 스코어 재조정을 통한 객 체 검출 방법을 제안한다. 상기 방법은 학습 장치가, 제1 객체 검출 모델에 적어A_05_도 하나의 이미지를 입력 받는 단계, 상기 학습 장치가, 상기 제1 객체 검출 모델을 통해 상기 적어A_05_도 하나의 이미지에 포함된 객체 를 검출하고, 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제1 클래스 스코어를 산출하는 단계, 상 기 학습 장치가, 상기 검출된 객체를 제2 객체 검출 모델에 입력하는 단계, 상기 학습 장치가, 상기 제2 객체 검출 모델을 통해 상기 검출된 객체 및 클래스 간의 유사A_05_도에 기반한 제2 클래스 스코어를 산출하는 단계, 상기 학습 장치가, 상기 제1 클래스 스코어 및 상기 제2 클래스 스코어를 기초로 상기 검출된 객체의 클래스를 추정하는 단계를 포함할 수 있다. A_05_대표A_05_도 A_05_도 10 A_05_도면 A_05_도 1 A_05_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 134, "content": "A_05_도 3 A_05_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 135, "content": "A_05_도 5 A_05_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 136, "content": "A_05_도 7 A_05_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 137, "content": "A_05_도 9 A_05_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 138, "content": "A_06_발명의 설명 A_06_발명의 명칭 센서 퓨전 기반의 시맨틱 세그멘테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램 {Method for semantic segmentation based on sensor fusion, and computer program recorded on record- medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 139, "content": "A_06_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 카 메라 및 라이다의 센서 퓨전을 통해 얻어낸 융합 데이터를 바탕으로 2차원의 시맨틱 세그멘테이션을 추론하기 위한, 센서 퓨전 기반의 시맨틱 세그멘테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그 램에 관한 것이다. A_06_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_06_도 학습(supervised learning), 비지A_06_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 어노테이션(annotation)을 수행하고, 메타데이터(metadat a)를 입력하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 A_06_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정A_06_도와 운전차가 차량을 제어하는 정A_06_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회가 분류한 6단계에 따르면, 레벨 0단계는 비자동화, 레벨 1단계는 운전자 보조, 레벨 2단계는 부분 자동 화, 레벨 3단계는 조건부 자동화, 레벨 4단계는 고A_06_도 자동화, 그리고 레벨 5단계는 완전 자동화 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그러나, 최근에는 자율주행 차량의 충돌 사고가 빈번히 발생함에 따라, 자율 주행의 안전성 개선에 대한 요구가 늘어나고 있다. 특히, RGB 카메라 센서와 2차원 객체 인식(object detection) 기술을 중심으로 하는 최근의 자율 주행용 첨단 운전자 보조 시스템(Advanced Driver Assistance System, ADAS)으로는 주변 객체들의 3차원적 상대 거리나 구 조 등을 알기 어려운 문제점이 있었다. 이러한 문제점을 해결하기 위하여, 최근에는 라이다(lidar)와 같은 3차원 센서 및 카메라와 같은 2차원 센서로 부터 취득한 멀티 모달(multi modal) 데이터를 융합하는 센서 퓨전(sensor fusion) 기법과, 객체 인식의 단위를 데이터의 구성 단위까지 확장하는 시맨틱 세그멘테이션(semantic segmentation) 기법에 대한 연구가 활발이 진 행되고 있다. 그러나, 카메라와 라이다의 센서 퓨전을 활용하는 기존의 시맨틱 세그멘테이션 연구들은 대부분 3차원의 라이다 점군 데이터와 2차원의 카메라 영상 데이터를 혼합하기 위해 점군 데이터를 2차원의 평면 데이터로 변환한다. 이를 이용해 만들어진 평면 형태의 추론 결과는 라이다의 장점인 3차원의 거리 정보가 손실된 상태이므로 주행 환경에 대한 3차원적 정보들을 제공하기 어려운 문제점이 있었다. A_06_선행기술문헌 A_06_특허문헌 대한민국 등록특허공보 제10-2073873호, ‘시맨틱 세그멘테이션 방법 및 그 장치’, (2020.01.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 140, "content": "A_06_발명의 내용 A_06_해결하고자 하는 과제 본 발명의 일 목적은 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 융합 데이터를 바탕으로 2차원의 시맨틱 세 그멘테이션을 추론하기 위한, 센서 퓨전 기반의 시맨틱 세그멘테이션 방법을 제공하는 것이다. 본 발명의 다른 목적은 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 융합 데이터를 바탕으로 2차원의 시맨틱 세그멘테이션을 추론하기 위한, 센서 퓨전 기반의 시맨틱 세그멘테이션 방법을 실행하기 위하여 기록매체에 기 록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 141, "content": "A_06_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 시맨틱 세그멘테이션 방법을 제안한다. 상기 방법 은 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camer a)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수신한 점군 데이터 및 상기 이미지를 전처리하는 단계 및 상기 학습 데이터 생성 장치가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상기 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립 적으로 추출하고, 상기 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성하는 단계를 포함할 수 있다. 구체적으로, 상기 수신하는 단계는 상기 점군 데이터 및 상기 이미지를 동기화 하기 위한 하기의 수학식 3으로 표현되는 캘리브레이션 행렬을 함께 취득하는 것을 특징으로 한다. [수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 142, "content": "(여기서, u 및 v는 이미지 내 픽셀들의 2차원 좌표, x, y 및 z는 점군 데이터의 3차원 좌표, fu 및 fv는 픽셀 단위의 초점 거리, u0 및 v0은 이미지 평면에서 점군 데이터가 위치하는 x 및 y 좌표를 의미한다.) 상기 전처리하는 단계는 상기 캘리브레이션 행렬을 기초로 상기 점군 데이터를 상기 이미지와 동일한 크기를 갖 는 2차원 평면 상의 좌표에 사영하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추출하는 인 코더 및 상기 심층 특징을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션을 추 론하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 인코더를 통해 심층 특징을 생성하되, 상기 사영된 점군 데이터 및 상기 이미지를 멀티 모달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 상기 복수의 컨벌루션 블록들을 이용하여 상기 사영된 점군 데이터 및 상기 이미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출하는 것을 특징 으로 한다. 상기 추론하는 단계는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상기 이미 지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출하는 것을 특징으로 한다. 상기 추론하는 단계는 연결 계층(concatenate)를 통해 상기 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루션 블록을 통해 연결된 특징 맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부 터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가 산하여 각 패치를 대표하는 임베딩 벡터를 생성하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터 (fusion based deep feature)로 변환하는 것을 특징으로 한다. 상기 추론하는 단계는 재배열(reshape) 계층을 통해 상기 변환된 심층 특징 벡터를 입력 크기 대비 1/16 크기의 특징맵으로 변환하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 변환된 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배수로 확장하되, 접합 계층을 통해 상기 복수의 업샘플링 계층 각각에서의 특징맵을 상기 인코더의 각 단계에 서 대응하는 특징맵과 접합하는 것을 특징으로 한다. 상기 추론하는 단계는 상기 복수의 업 샘플링 계층 각각에서 업샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루 션 계층에 통과시키는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 시맨틱 세그멘테이션 방법을 실행하기 위하여 기 록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 프로세서가, 상기 수신 한 점군 데이터 및 상기 이미지를 전처리하는 단계 및 상기 프로세서가, 상기 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 상기 추출된 특징을 융합하여 시맨틱 세그멘테이션을 추론하는 단계 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_06_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 143, "content": "A_06_발명의 효과 본 발명의 실시 예들에 따르면, 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 3차원 정보를 포함하는 융합 데이 터를 바탕으로 2차원의 시맨틱 세그멘테이션을 추론하고, 추론 결과를 3차원으로 재구성할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 144, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 145, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 146, "content": "A_06_도면의 간단한 설명 A_06_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_06_도이다. A_06_도 2는 본 발명의 일 실시예에 따른 학습 데이터 수집 장치의 구성을 설명하기 위한 예시A_06_도이다. A_06_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_06_도이다. A_06_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_06_도이다. A_06_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 순서A_06_ 도이다. A_06_도 6은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 예시A_06_ 도이다. A_06_도 7은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 구체적으로 설명하기 위한 예시A_06_도이 다. A_06_도 8은 본 발명의 일 실시예에 따른 3차원 해석 방법을 설명하기 위한 예시A_06_도이다. A_06_도 9는 본 발명의 일 실시예에 따른 3차원 해석 방법의 성능을 설명하기 위한 예시A_06_도이다. A_06_도 10은 본 발명의 일 실시예에 따른 2차원 시맨틱 세그맨테이션의 3차원 해석 결과를 나타낸 A_06_도면이 다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 147, "content": "A_06_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_06_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_06_도하게 포괄적인 의미로 해석되거나, 과A_06_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_06_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_06_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_06_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_06_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_06_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_06_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_06_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_06_도면은 본 발명의 사상을 쉽게 이해할 수 있A_06_도록 하기 위한 것일 뿐, 첨부된 A_06_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_06_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_06_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 라이다(lidar)와 같은 3차원 센서 및 카메라와 같은 2차원 센서로부터 취득한 멀티 모달(multi modal) 데이터를 융합하는 센서 퓨전(sensor fusion)과, 객체 인식의 단위를 데이터의 구성 단위까지 확장하는 시맨틱 세그멘테이션(semantic segmentation)이 활발히 연구되고 있다. 센서 퓨전은 이종의 센서로부터 취득한 멀티 모달 데이터를 상호 융합하는 기술이다. 센서 퓨전은 딥러닝 분야 에서 다수의 원천(source)으로부터 형성된 데이터 또는 정보들이 융합되는 시점에 따라 세 가지 종류로 구분한 다. 데이터 단계 퓨전(data level or early fusion)은 원천으로부터 취득된 데이터 자체를 융합하여 새로운 표현형 을 갖는 융합 데이터를 만든다. 이 융합데이터는 신경망에 전달되어 특징맵(feature map)을 추출, 결과 추론 맵 (prediction map)을 만드는데 활용된다. 이 방법은 신경망에 입력으로 수용되는 데이터의 종류가 적어 신경망의 전체적인 구조가 비교적 단순하게 구성된다는 장점이 있다. 이와 달리 특징 단계 퓨전(deep feature level or mid-level fusion)은 원천으로부터 취득한 멀티 모달 데이터 를 신경망에 입력으로 각각 전달한다. 병렬 구조의 신경망을 통해 각각의 입력으로부터 추출되는 멀티 모달 특 징들은 신경망의 내부에서 하나로 융합되어 심층 융합 특징을 구성하며, 이는 결과 추론 맵을 만드는데 활용된 다. 이 방법은 신경망이 결과를 추론함에 있어 멀티 모달 데이터로부터 추출한 각각의 특징부터 그들의 융합 특 징까지 다양하게 활용 가능한 장점이 있다. 추론 단계 퓨전(score level or late fusion) 또한 멀티 모달 데이터를 신경망의 입력으로 각각 전달하나, 특징 단계 퓨전과 달리 각각의 원천마다 독립된 신경망을 사용한다. 각 신경망이 독립적으로 만들어낸 결과 추론 맵 들은 합산, 평균 등의 단순 연산이나 별A_06_도로 학습된 분류기를 활용함으로써 최종 추론 맵을 A_06_도출하는 데 활용된다. 이 방법은 추론 결과가 융합에 사용된 센서들 사이의 특성 차나 상호 간섭에 의한 영향에 강인하 다는 장점이 있다. 시맨틱 세그멘테이션은 자율 주행 차량이 주행 환경을 인지하는데 필요한 핵심적인 요소 기술 중의 하나로서 카 메라로부터 취득한 2차원 RGB 영상의 객체들을 픽셀 단위로 분류(dense prediction)할 수 있다. 특히, 최근에는 기존의 컨볼루션 계층을 중심으로 구성된 심층 신경망을 사용하는 방식에서 더 나아가 셀프 어 텐션을 기반으로 특징들 사이의 상대적 중요A_06_도를 알아내 보다 표현력이 높은 심층 특징들을 추출해낼 수 있는 트랜스포머 모듈을 추가하여 2차원 시맨틱 세그멘테이션의 성능을 증진하는 연구들이 늘고 있다. 트랜스포머 모듈을 활용하면 신경망에 컨볼루션 계층을 비교적 적게 사용하더라A_06_도 표현력이 좋은 심층 특 징들을 추출할 수 있으며, 이 심층 특징들을 바탕으로 기존의 방법들보다 상대적으로 좋은 성능을 보였다. 그러나 앞서 언급한 바와 같이 이들 2차원 시맨틱 세그멘테이션 방법이 만들어내는 평면 형태의 추론 결과만으 로는 차량의 실제 주행 환경인 3차원에서의 객체의 구조나 상대적 거리 등을 파악하기 어려우며, 3차원 정보를 추론하기 위해 여러 대의 카메라를 사용할 경우에는 계산량이 과A_06_도하게 늘어난다는 문제가 있다. 또한, 카메라와 라이다의 센서 퓨전을 활용하는 기존의 시맨틱 세그멘테이션 연구들은 대부분 3차원의 라이다 점군 데이터와 2차원의 카메라 영상 데이터를 혼합하기 위해 점군 데이터를 2차원의 평면 데이터로 변환한다. 이를 이용해 만들어진 평면 형태의 추론 결과는 라이다의 장점인 3차원의 거리 정보가 손실된 상태이므로 주행 환경에 대한 3차원적 정보들을 제공하기 어렵다. 이러한 한계를 극복하고자, 본 발명은 시맨틱 세그멘테이션 기법과, 카메라 및 라이다의 센서 퓨전을 통해 얻어 낸 3차원 정보를 포함하는 융합 데이터를 기초로 한 2차원 시맨틱 세그멘테이션 결과를 3차원으로 재구성할 수 있는 다양한 수단들을 제안하고자 한다. A_06_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_06_도이다. A_06_도 1에 A_06_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 수집 장 치, 학습 데이터 생성 장치 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar) 및 카메라(camera)로부터 실 시간으로 데이터를 수집하는 장치이다. 하지만, 이에 한정된 것은 아니고, 학습 데이터 수집 장치는 레이 더(radar) 및 초음파 센서(ultrasonic sensor)를 포함할 수A_06_도 있다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비 되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라A_06_도 복수 개로 구비될 수 있다. 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하 는 센서들의 종류에 대해서는 추후 A_06_도 2를 참조하여 보다 구체적으로 설명하기로 한다. 다음 구성으로, 학습 데이터 생성 장치는 복수의 학습 데이터 수집 장치 각각으로부터 이동통신 (mobile communication)을 이용하여 각각의 학습 데이터 수집 장치에 의해 실시간으로 수집된 데이터를 수 신하고, 수신된 데이터에 대하여 어노테이션을 수행할 수 있다. 이러한, 학습 데이터 생성 장치는 학습 데이터 생성 장치는 인공지능 학습 장치로부터 인공지능 (AI) 학습용 데이터의 요청이 수신되기 이전에, 선제적으로 인공지능(AI) 학습용 데이터를 생성할 수 있는 빅데 이터(big data)를 구축해 놓을 수 있다. 특징적으로, 학습 데이터 생성 장치는 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 수신한 점군 데이터 및 이미지를 전처리하고, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성 할 수 있다. 또한, 학습 데이터 생성 장치는 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라 (camera)를 통해 촬영된 이미지(image)를 수신하고, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 점군 데이터 및 이미지로부터 2차원 시맨틱 세그멘테이션 특징맵을 생성하 고, 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치 및 인공지능 학습 장치 와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_06_ 도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 한편, 학습 데이터 생성 장치에 관한 구체적인 설명은 이하, A_06_도 3 및 A_06_도 4를 참조하여 후술하 A_06_도록 한다. 다음 구성으로, 인공지능 학습 장치는 인공지능(AI)을 개발하는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 인공지능(AI)이 개발 목적을 달성하기 위하여 인공지능(AI) 학습용 데 이터가 만족해야 하는 요구 사항을 포함하는 요구 값을 학습 데이터 생성 장치에 전송할 수 있다. 인공지 능 학습 장치는 학습 데이터 생성 장치로부터 인공지능(AI) 학습용 데이터를 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 인공지능(AI) 학습용 데이터를 이용하여, 개발하고자 하는 인공지능 (AI)을 기계 학습할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_06_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치 및 인공지능 학습 장 치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네 트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것A_06_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. A_06_도 2는 본 발명의 일 실시예에 따른 센서들을 설명하기 위한 예시A_06_도이다. A_06_도 2에 A_06_도시된 바와 같이, 본 발명의 일 실시예에 따른 학습 데이터 수집 장치는 차량에 고 정 설치된 레이더, 라이다, 카메라 및 초음파 센서 중 하나 이상을 제어하여, 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집할 수 있다. 여기서, 차량은 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집하기 위한 레이더, 라이다 , 카메라 및 초음파 센서가 설치된 차량으로, 인공지능(AI)에 의해 자율주행을 수행하는 차량과는 서로 구별될 수 있다. 레이더는 차량에 고정 설치되어 차량의 주행 방향을 향하여 전자기파(electromagnetic wave)를 발 사하고, 차량의 전방에 위치하는 객체(object)에 의해 반사되어 돌아온 전자기파를 감지하여, 차량이 전방에 대한 영상에 해당하는 감지 데이터를 생성할 수 있다. 다르게 말하면, 감지 데이터는 차량에 고정 설치된 레이더에 의해 차량의 주행 방향을 향하여 발사된 전자기파를 반사시킨 점들(points)에 대한 정보이다. 따라서, 감지 데이터에 포함된 점들의 좌표들은 차량 의 전방에 위치하는 객체의 위치 및 형상에 대응하는 값을 가질 수 있다. 이러한, 감지 데이터는 2차원 정보가 될 수 있으나, 이에 한정되지 않고 3차원 정보가 될 수A_06_도 있다. 라이다는 차량에 고정 설치되어 차량의 주위로 레이저 펄스(laser pulse)를 방사하고, 차량의 주위에 위치하는 객체에 의해 반사되어 돌아온 빛을 감지하여, 차량의 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 다르게 말하면, 3D 점군 데이터는 차량에 고정 설치된 라이다에 의해 차량의 주위로 방사된 레이저 펄 스를 반사시킨 점들에 대한 3차원 정보이다. 따라서, 3D 점군 데이터에 포함된 점들의 좌표들은 차량의 주 위에 위치하는 객체의 위치 및 형성에 대응하는 값을 가질 수 있다. 카메라는 차량에 고정 설치되어 차량의 주위에 대한 2차원 이미지를 촬영할 수 있다. 이와 같은, 카메라는 서로 다른 방향을 촬영할 수 있A_06_도록 복수 개가 지표면과 수평 또는 수평 방향으로 이격되게 설치될 수 있다. 예를 들어, A_06_도 2는 서로 다른 6개의 방향을 촬영할 수 있는 6개의 카메라가 고정 설 치된 차량의 예시를 A_06_도시하고 있으나, 차량에 설치될 수 있는 카메라가 다양한 개수로 구성될"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 148, "content": "수 있음은 본 발명이 속한 기술분야의 통상의 지식을 가진 자에게 자명할 것이다. 다르게 말하면, 2D 이미지는 차량에 고정 설치된 카메라에 의해 촬영된 이미지이다. 따라서, 2D 이미지 에는 카메라가 향하는 방향에 위치하는 객체의 색상 정보가 포함될 수 있다. 초음파 센서는 차량에 고정 설치되어 차량의 주위로 초음파(ultrasonic)를 발사하고, 차량과 인접하게 위치하는 객체에 의해 반사되어 돌아온 음파를 감지하여, 차량에 설치된 초음파 센서와 객체 사이의 거리에 해당하는 거리 정보를 생성할 수 있다. 일반적으로, 초음파 센서는 복수 개로 구성되어, 객 체와 접촉하기 쉬운 차량의 전방, 후방, 전측방 및 후측방에 고정 설치될 수 있다. 다르게 말하면, 거리 정보는 차량에 고정 설치된 초음파 센서에 의해 감지된 객체로부터의 거리에 관한 정보이다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_06_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_06_도이다. A_06_도 3을 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 사전학습부, 데이터전 처리부, 추론부, 3차원해석부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지 및 점군 데이터를 학습 데이터 수집 장치 로부터 수신할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 동기화 하기 위한 캘리브레이 션 행렬을 함께 수신할 수 있다. 또한, 통신부는 시맨틱 세그멘테이션의 3차원 해석 결과를 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 받 거나, 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 시맨틱 세그멘테이션 특징맵을 생성하거나, 생성된 시맨틱 세그멘테 이션 특징맵을 3차원 해석하기 위한 다양한 설정 값들을 입력받고, 생성된 결과 값들을 출력할 수 있다. 다음 구성으로, 사전 학습부는 2차원 시맨틱 세그멘테이션을 추론하기 위한 인공 지능을 사전 학습시킬 수 있다. 구체적으로, 사전 학습부는 사전 저장된 데이터 셋에 포함된 라이다로부터 획득된 점군 데이터, 점군 데이 터와 동시에 카메라를 통해 촬영된 이미지, 라이다 및 카메라 사이의 캘리브레이션 정보, 점군 데이터의 3차원 포인트 단위로 클래스 라벨이 명시되어 있는 정답 데이터를 기초로 인공 지능을 사전 학습시킬 수 있다. 이때, 사전 학습부는 점군 데이터 및 정답 데이터를 이미지의 2차원 평면상 좌표에 사영시키되, 이미지의 사영될 픽셀과 맨해튼 거리(Manhattan distance)가 사전 설정된 값 이내인 픽셀들을 동일한 클래스를 갖A_06_도 록 할 수 있다. 또한, 사전 학습부는 복수의 손실함수 중 적어A_06_도 하나를 통해 인공 지능을 사전 학습시킬 수 있다. 이때, 사전 학습부는 하기의 수학식 1으로 표현되는 손실함수를 통해 인공 지능을 사전 학습시킬 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 149, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 또한, 사전 학습부는 하기의 수학식 2로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시킬 수 있 다.[수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 150, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 이때, 사전 학습부는 배경 클래스에 해당하는 픽셀을 손실함수의 종류와 관계없이 손실 값의 계산에서 제 외시킬 수 있다. 다음 구성으로, 데이터전처리부는 학습 데이터 수집 장치로부터 수신한 점군 데이터 및 이미지를 전 처리할 수 있다. 구체적으로, 데이터전처리부는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데 이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 여기서, 캘리브레이션 행렬은 하기의 수학식 3으로 표현될 수 있다. [수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 151, "content": "(여기서, u 및 v는 이미지 내 픽셀들의 2차원 좌표, x, y 및 z는 점군 데이터의 3차원 좌표, fu 및 fv는 픽셀 단위의 초점 거리, u0 및 v0은 이미지 평면에서 점군 데이터가 위치하는 x 및 y 좌표를 의미한다.) 다음 구성으로, 추론부는 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence) 을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하 여 시맨틱 세그멘테이션 특징맵을 생성할 수 있다. 구체적으로, 추론부는 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추출하는 인 코더 및 심층 특징을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션을 추론할 수 있다. 이때, 추론부는 인코더를 통해 심층 특징을 생성하되, 사영된 점군 데이터 및 이미지를 멀티 모달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 복수의 컨벌루션 블록들을 이용하여 사영된 점군 데이터 및 이 미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출할 수 있다. 즉, 추론부는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상기 이미지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출할 수 있다. 다음으로, 추론부는 연결 계층(concatenate)를 통해 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루션 블 록을 통해 연결된 특징맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성할 수 있다. 다음으로, 추론부는 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가산 하여 각 패치를 대표하는 임베딩 벡터를 생성할 수 있다. 다음으로, 추론부는 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터(fusion based deep feature)로 변환할 수 있다. 다음으로, 추론부는 재배열(reshape) 계층을 통해 변환된 심층 특징 벡터를 입력 크기 대비 1/16 크기의 특징맵으로 변환할 수 있다. 다음으로, 추론부는 변환된 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배 수로 확장하되, 접합 계층을 통해 복수의 업샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하는특징맵과 접합할 수 있다. 여기서, 추론부는 복수의 업 샘플링 계층 각각에서 업샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루션 계층에 통과시킬 수 있다. 다음 구성으로, 3차원해석부는 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 구체적으로, 3차원해석부는 점군 데이터의 3차원 좌표 및 점군 데이터를 사영한 이미지의 2차원 평면상 좌 표 쌍을 준비할 수 있다. 또한, 3차원해석부는 생성된 특징맵에 대하여 각각의 2차원 평면상 좌표마다 해당 좌표를 중심으로 3*3 크 기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용할 수 있다. 또한, 3차원해석부는 최대 풀링 연산을 통해 각 좌표에 대한 세그먼트를 추정한 벡터를 획득할 수 있다. 그리고, 3차원해석부는 획득된 벡터 중 가장 큰 확률을 갖는 클래스를 점군 데이터의 3차원 좌표에 대해 추론된 클래스로 예측할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터를 설계하는데 필요한 데이터를 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. A_06_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_06_도이다. A_06_도 4를 참조하면, 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수 신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지 (Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치 및 인공지능 학습 장치와 데이터를 송 수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이터를 입력 받고, 생성된 결과 값을 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지 와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 시맨틱 세그멘테이션 방법 및 3차원 해석 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 시맨틱 세그멘테이션 방법 및 3차원 해석 방법을 수행하기 위한 프로그램을 저장하는 데 이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 수신한 점군 데이터 및 이미지를 전처리하는 단계 및 프로세서가, 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하여 시맨틱 세그멘테이션을 추론하는 단계를 실행하기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 또한, 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어 (280a, 280b)는 프로세서가 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라 (camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 프로세서, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 점군 데이터 및 이미지로부터 2차원 시맨틱 세그멘테이션 특 징맵을 생성하는 단계 및 프로세서, 생성된 특징맵으로부터 3차원 정보를 해석하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다.보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_06_도 4에 A_06_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_06_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_06_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_06_도록 구성될 수 있으며, 그 역A_06_도 마찬가지이다. A_06_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 순서A_06_ 도이다. A_06_도 5를 참조하면, 먼저 S100 단계에서 학습 데이터 생성 장치는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 수신할 수 있다. 이때, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 캘리브레이션 형렬 을 추가로 수신할 수 있다. 다음으로, S200 단계에서 학습 데이터 생성 장치는 학습 데이터 수집 장치로부터 수신한 점군 데이터 및 이미지 를 전처리할 수 있다. 구체적으로, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 다음으로, S300 단계에서 학습 데이터 생성 장치는 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하 고, 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성할 수 있다. 구체적으로, 학습 데이터 생성 장치는 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추 출하는 인코더 및 심층 특징을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션 을 추론할 수 있다. 이때, 학습 데이터 생성 장치는 인코더를 통해 심층 특징을 생성하되, 사영된 점군 데이터 및 이미지를 멀티 모 달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 복수의 컨벌루션 블록들을 이용하여 사영된 점군 데이 터 및 이미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출할 수 있다. 즉, 학습 데이터 생성 장치는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상 기 이미지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출할 수 있다. 또한, 학습 데이터 생성 장치는 연결 계층(concatenate)를 통해 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루 션 블록을 통해 연결된 특징맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성할 수 있다. 학습 데이터 생성 장치는 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가산 하여 각 패치를 대표하는 임베딩 벡터를 생성할 수 있다. 학습 데이터 생성 장치는 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터(fusion based deep feature)로 변환할 수 있다. 학습 데이터 생성 장치는 재배열(reshape) 계층을 통해 변환된 심층 특징 벡터를 입력 크기 대비 1/16 크기의 특징맵으로 변환할 수 있다. 학습 데이터 생성 장치는 변환된 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배 수로 확장하되, 접합 계층을 통해 복수의 업샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하는 특징맵과 접합할 수 있다. 여기서, 학습 데이터 생성 장치는 복수의 업 샘플링 계층 각각에서 업샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루션 계층에 통과시킬 수 있다. 그리고, S400 단계에서 학습 데이터 생성 장치는 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 구체적으로, 학습 데이터 생성 장치는 점군 데이터의 3차원 좌표 및 점군 데이터를 사영한 이미지의 2차원 평면 상 좌표 쌍을 준비할 수 있다. 또한, 학습 데이터 생성 장치는 생성된 특징맵에 대하여 각각의 2차원 평면상 좌표마다 해당 좌표를 중심으로 3*3 크기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용할 수 있다. 또한, 학습 데이터 생성 장치는 최대 풀링 연산을 통해 각 좌표에 대한 세그먼트를 추정한 벡터를 획득할 수 있 다. 그리고, 학습 데이터 생성 장치는 획득된 벡터 중 가장 큰 확률을 갖는 클래스를 점군 데이터의 3차원 좌표에 대해 추론된 클래스로 예측할 수 있다. 이하 A_06_도면을 참조하여, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법에 대하여 더 욱 상세히 설명하A_06_도록 한다. A_06_도 6은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 예시A_06_ 도이고, A_06_도 7은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 구체적으로 설명하기 위한 예시 A_06_도이고, A_06_도 8은 본 발명의 일 실시예에 따른 3차원 해석 방법을 설명하기 위한 예시A_06_도이다. A_06_도 6 내지 A_06_도 8을 참조하면, 데이터 원천(Data Source) 단계에서는 카메라와 라이다 센서로부터 각각 차량 전방의 2차원 RGB 영상과 차량 둘레 전 영역의 3차원 점군 데이터를 취득한다. 또한 두 센서의 데이터 취 득 영역이 서로 상이하므로 이를 동기화 하기 위해 사용되는 캘리브레이션 행렬을 함께 취득한다. 이어서, 데이터 전처리(data pre-process) 단계에서는 라이다로부터 취득한 데이터 중 차량의 전방 영역, 즉 카 메라가 영상을 취득하는 영역과 동일한 영역의 데이터를 선별하는 과정을 수행한다. 이를 통해 구분된 N개의 전방 영역 점군 좌표(x,y,z) 데이터(N, 3)는 앞서 구한 캘리브레이션 행렬을 바탕으로 RGB 영상과 동일한 크기(H, W)를 갖는 2차원 평면상의 사영 좌표 (u,v)로 변환된다. 이로써 전방 영역의 점군 데이터인 N개의 3차원 좌표 (x,y,z)들은 (H, W, 1) 크기의 사영 영상으로 변환되며 이 사영 영상과 (H, W, 3) 크기의 RGB 영상을 다음 단계인 추론 단계로 전달한다. 추론(prediction) 단계에서는 앞서 전달받은 두 영상을 바탕으로 심층 신경망을 이용한 2차원 시맨틱 세그멘테 이션을 수행한다. 이때 사용되는 심층 신경망은 입력으로부터 심층 특징(deep feature)을 추출하는 인코더 (encoder)와 심층 특징을 해석해 세그멘테이션 추론 맵을 만들어내는 디코더(decoder)를 포함하는 인코더-디코 더 구조로 이루어져 있다. 여기서, 인코더는 \"ResNet50\"의 컨볼루션 블록들을 바탕으로 입력 데이터의 지역적, 구조적 특징들을 추출하고 트랜스포머 모듈을 사용하여 추출된 특징들을 보다 표현력이 좋은 심층 특징들로 변환한다. 대신 신경망이 사영 영상을 위한 입력(remission input)과 RGB 영상을 위한 입력(RGB input)을 동시에 수용할 수 있A_06_도록 구성하고 신경망 내 모듈들을 센서 퓨전 방법에 따라 병렬 구조로 배치하여 기존 방법과 달리 멀티 모달 데이터의 처리를 가능하게 한다. 또한, 디코더는 인코더의 각 단계에서 추출되는 특징맵들이 심층 특징과 연결되어 입력과 동일한 (H, W, C) 크 기의 결과 추론 맵을 구성할 수 있A_06_도록 컨볼루션 계층을 비롯한 여러 신경망 계층들로 구성한다. 이 때 C 는 픽셀 단위로 객체를 분류할 수 있는 클래스의 총 개수이다. 이에 따라, A_06_도 7에 A_06_도시된 바와 같이, 신경망은 멀티 모달 입력을 수용하고 병렬 구조로 배치된 컨볼 루션 블록들을 이용하여 각 입력으로부터 각 입력의 구조적, 지역적 특징들을 상호 독립적으로 추출한다. 이에 따라, 추출된 입력 대비 1/2, 1/4 면적의 특징맵들이 추출되며 그 중 1/4 면적의 특징맵들을 특징 단계 퓨 전에 활용하기 위해 연결 계층(concatenate)을 두어 서로 이어 붙인다. 이렇게 만들어진 연결 특징맵은 세 번째 컨볼루션 블록을 통해 입력 대비 1/8 면적을 가진 융합 특징맵으로 만들어진다. 이어서, 트랜스포머 모듈에 앞서 만든 융합 특징맵을 전달하기 위해 융합 특징맵을 (2, 2) 크기의 패치로 분할 한 뒤 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고 위치 임베딩 (position embedding)을 가산하여 각 패치를 대표하는 임베딩 벡터들을 만든다. 이어서 이 임베딩 벡터들은 9개 의 연속된 트랜스포머 모듈로 구성된 트랜스포머 블록을 거치면서 융합 특징맵에 포함된 중요 특징들을 중심으 로 구성된 심층 특징 벡터(fusion based deep feature)로 변환된다. 다음으로, 디코더에서는 먼저 인코더로부터 추출한 심층 특징 벡터들을 재배열(reshape) 계층을 통해 입력 대비 1/16 크기의 특징맵으로 변환하고 3*3 커널을 사용하는 컨볼루션 계층을 통과하A_06_도록 한 후, 업 샘플링(up sampling) 계층을 통해 그 면적을 두 배로 확장한다. 이 확장된 특징맵이 앞서 심층 특징 벡터에 담겨 있던 주요 특징들을 담고 있다 하더라A_06_도 벡터화로 인해 손실된 지역적, 구조적 특성들은 컨볼루션과 업샘플링 만으로 쉽게 회복되지 않는다. 따라서, 접합 계층을 이용하여 해당 특징맵과 앞서 인코더에서 추출된 동일 크기의 특징맵을 이어 붙임으로써 특징맵이 심층 특징 벡터의 주요 특징들과 인코딩 당시 추출된 지역적, 구조적 특성들을 동시에 내포할 수 있 A_06_도록 한다. 이어서, 접합 특징맵을 3*3 커널을 사용하는 컨볼루션 계층을 이용해 서로 혼합하고, 다시 업샘플링 계층을 통 해 확장, 그리고 인코더 내 동일 크기의 특징맵을 접합하는 과정을 2회 더 반복함으로써 특징맵의 크기를 입력 대비 1/2까지 복원해 낸다. 마지막으로, 그 특징맵을 다시 한번 업샘플링하고 3*3 커널을 사용하는 컨볼루션 계층을 적용하여 입력과 동일 한 크기의 특징맵을 만들어 낸 뒤 이를 소프트맥스 활성화 함수와 1*1 커널을 사용하는 컨볼루션 계층인 세그멘 테이션 헤드에 전달하여 세그멘테이션 추론맵을 만들어 낸다. 프로세스의 마지막 단계인 데이터 후처리(data post-processing) 단계에서는 A_06_도 8에 A_06_도시된 바와 같 이 점군 데이터를 활용하여 신경망이 만들어내는 2차원의 세그멘테이션 추론 맵을 3차원 정보로 재해석하는 과 정(reconstruction)을 수행한다. 먼저, 데이터 전처리 단계에서 사영 영상 생성에 사용했던 N개의 3차원 좌표 (x,y,z)와 이를 사영한 N개의 2차 원 평면상 좌표 (u,v) 쌍을 준비한다. 이어서, 신경망에 의해 추론된 (H, W, C) 크기의 세그멘테이션 추론 맵에 대하여 각각의 2차원 좌표 (u,v)마다 해당 좌표를 중심으로 3*3 크기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용한다. 이를 통해, 본 발명은 N 개의 좌표 각각에 대해 세그먼트를 추정한 벡터를 얻어낼 수 있으며 그 중 가장 큰 확 률을 갖는 클래스를 3차원 좌표(x,y,z)에 대해 추론된 클래스로 본다.따라서, 데이터 전처리 단계에서 분리한 전방 영역의 점군 데이터 각각에 대한 시맨틱 세그멘테이션 추론 결과 를 얻을 수 있으며, 그 결과 각 좌표의 세그먼트 값을 담은 (N, C) 크기의 출력 행렬을 얻을 수 있다. 이하, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법 및 3차원 해석 방법의 성능에 대하여 설명하A_06_ 도록 한다. A_06_도 9는 본 발명의 일 실시예에 따른 3차원 해석 방법의 성능을 설명하기 위한 예시A_06_도이다. 실험 실험은 \"AMD EPYC 7742 64-Core Processor\" 급의 CPU(Central Processing Unit), \"A100 80GB\" 급의 GPU(Graphics Processing Unit) 및 2TB의 메모리가 탑재된 하드웨어 환경에서 진행하였다. 신경망 구현을 위한 소프트웨어 환경은 \"ubuntu 20.04\" 운영체제에 설치된 \"Python 3.9\"에 \"Tensorflow 2.80\" 과 \"Keras 2.80\"을 설정해 활용하였다. 인코더에 사용된 트랜스포머 모듈은 GeLU(Gauusian Error Linear Unit) 활성화 함수를 사용하A_06_도록 하였으 며, 디코더를 구성하는 모든 컨볼루션 계층은 세그멘테이션 헤드를 제외하고 모두 ReLU 활성화 함수를 사용하 A_06_도록 하였고 각각의 컨볼루션 계층 이후에는 배치 정규화 계층과 드롭 아웃 계층을 두어 신경망의 과적합 을 최소화할 수 있A_06_도록 하였다. 신경망의 학습에는 Adam(Adaptive Moment Estimation) 최적화기를 사용하였으며 학습률은 0.001로 시작해 학습 이 2에폭(Epoch)이상 진행되지 않을 때마다 0.75배씩 줄어들A_06_도록 하였다. 학습은 최대 500에폭 동안 진행하A_06_도록 설정하였으며 이때 손실 값이 연속적으로 10에폭 이상 낮아지지 않 으면 학습을 조기 종료하A_06_도록 설정하였다. 본 발명의 일 실시예에 시맨틱 세그멘테이션 방법의 학습과 평가에 사용된 데이터 셋은 \"semantic KITTI\"로 자 율 주행 분야의 3차원 객체 인식이나 시맨틱 세그멘테이션 등의 연구에 널리 활용된다. 사용된 데이터 셋은 총 21개 시퀀스로 구성되어 있으며 각 시퀀스에는 2차원 RGB 프레임 이미지들과 3차원 점군 그리고 각 데이터를 취득한 카메라와 라이다 센서 사이의 캘리브레이션 정보 그리고 각각의 3차원 포인트 단위 로 클래스 라벨이 명시되어 있는 정답(Ground Truth: GT) 데이터를 포함하고 있다. 본 실험에서는 그 중 8번 시퀀스를 제외한 0번부터 10번까지의 총 10개 시퀀스, 총 19,130건의 프레임을 학습 데이터로 사용하고 4,071건의 프레임으로 구성된 8번 시퀀스를 평가 데이터로 활용한다. 다만, 사용된 데이터 셋에는 2차원 시맨틱 세그멘테이션을 위한 정답 데이터가 존재하지 않는다. 따라서, 점군 데이터에 대해 매겨진 정답 데이터를 점군 데이터를 사영하는 방법과 동일한 방법으로 사영하여 2차원 시맨틱 세그멘테이션을 위한 정답 데이터를 만들 경우, A_06_도 9의 (a)와 같이 대부분의 픽셀의 값이 0을 갖는 희소 데이터가 만들어진다. 이는, 신경망의 학습을 극A_06_도로 저해하는 요소로 작용하므로, 이를 보완하기 위해 정답 데이터의 사영 될 픽셀과 맨해튼 거리(Manhattan distance)가 2 이내인 픽셀들을 동일한 클래스를 갖A_06_도록 하여 A_06_도 9의 (b)와 같이 정답 데이터의 희소성을 줄여 활용하였다. 실험 과정에서 신경망의 학습에 사용되는 손실함수는 하기의 수학식 1 및 2로 각각 표현되는 \"Focal loss\"함수 및 \"Dice loss\"함수를 사용하였다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 152, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 153, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) Focal loss 함수는 수학식 1과 같이 크로스 엔트로피를 기반으로 하는 추론 확률(ppred)과 정답(ptrue) 사이의 가 중치 합을 구하여 손실 값을 계산하는 방법으로 각각의 픽셀 단위로 추론 정확성(accuracy)을 증진하는 것을 목 적으로 한다. 한편, Dice loss 함수는 수학식 2와 같이 Dice 계수를 기반으로 하는 추론 확률(ppred)과 정답(ptrue) 사이의 유사 A_06_도를 구하여 손실 값을 계산하는 방법으로 전체적인 추론의 유사A_06_도(similarity)를 증진하는 것을 목 적으로 한다. 이때, 배경 클래스에 해당하는 픽셀의 경우에는 손실함수의 종류와 관계없이 손실 값의 계산에서 제외함으로써 신경망이 배경 지역을 정확히 추론하는데 편향되어 학습되는 문제를 방지하A_06_도록 한다. 본 발명의 실시예에 따른 시맨틱 세그멘테이션 성능 평가를 위해서는 시맨틱 세그멘테이션 분야에서 널리 사용 되는 하기의 수학식 4에 따른 MIoU (Mean Intersection over Union)을 사용하였다. [수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 154, "content": "MIoU는 수학식 4에서 보이는 바와 같이, 전체 N장으로 이루어진 영상 데이터 셋에서 각 영상마다 구한 IoU 값의 평균을 나타내며, IoU는 영상의 픽셀마다 추론된 클래스들과 해당 픽셀의 정답을 비교하여 그 둘이 서로 동일한 데이터의 수(TP)를 전체 추론 데이터의 수로 나누어 준 값(TP+FN+FP)을 의미한다. 이는 추론치가 클래스 라벨이 명시되어 있는 정답(GT)과 일치하는 비율을 의미하는 것으로써 픽셀 단위의 객체 추론에 대한 정확A_06_도를 나타낼 수 있다. 이때에A_06_도 손실함수의 경우와 마찬가지로 배경 픽셀에 해당하는 클래스는 평가 값의 계산에서 제외함으로써 신경망이 전경 지역을 정확히 추론하지 못함에A_06_도 성능이 높게 나오는 편향 문제를 방지하A_06_도록 한다. 절제 연구 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법에서 사용할 신경망의 구조와 손실함수의 구성 등에 따른 성능 평가를 토대로 상세 사항들을 결정하기 위해 수행한 절제 실험 결과들에 관해 서술한다. 먼저, 첫 번째 실험에서는 제안 방법에 대한 최적의 센서 퓨전을 결정하기 위해 신경망에 적용되는 퓨전 방법을 데이터 단계 퓨전, 특징 단계 퓨전 그리고 추론 단계 퓨전으로 나누어 각각 적용함으로써 센서 퓨전을 적용한 경우와 그렇지 않은 경우의 성능을 비교하였다. 이때, 데이터 단계 퓨전을 적용한 신경망은 하기의 수학식 5와 같이 연결 계층을 통해 점군의 사영 영상(XPCD)과 RGB 영상(XRGB)을 이어 붙인 (H, W, 4) 크기의 하나의 융합 데이터를 인코더의 입력으로 전달하고, 그 출력을 다 시 디코더의 입력으로 전달하여 하나의 2차원 세그멘테이션 추론맵(Yseg)을 만들어 낸다. [수학식 5]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 155, "content": "한편, 추론 단계 퓨전을 적용한 신경망은 하기의 수학식 6에 표시된 바와 같이 두 입력을 위한 서로 다른 가중 치((θen1, θde1), (θen2, θde2))를 갖는 독립적인 신경망들을 각각 적용하여 두 개의 2차원 세그멘테이션 추론 맵을 만들어내며 이들을 평균(average)하는 방법으로 하나의 2차원 세그멘테이션 추론맵을 만들어 낸다.[수학식 6]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 156, "content": "또한, 특징 단계 퓨전의 경우에는 신경망을 구성하는 초입의 블록으로부터 시작하여 한 블록씩 더 깊은 신경망 블록에서 퓨전 하A_06_도록 하여 다양한 경우의 수를 비교할 수 있A_06_도록 하였으며, 학습의 진행은 \"Focal loss\"함수를 바탕으로 수행하였다. [표 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 157, "content": "한편, 표 1은 본 발명의 실시예에 따른 시맨틱 세그멘테이션 방법에 대한 센서 융합 응용 실험 결과를 나타낸 표이다. 표 1에 기재된 바와 같이, 센서 퓨전을 사용하지 않고 입력으로 RGB 영상만을 사용한 단일 모달(single modal) 의 방법이 센서 퓨전을 사용한 그 어떤 경우보다 낮은 0.1012의 MIoU를 기록하였다. 센서 퓨전을 적용한 경우들끼리 비교할 경우에는 추론 단계 퓨전을 적용한 경우가 0.1844로 가장 낮은 MIoU를 기록하였으며 데이터 단계 퓨전이 0.2045 그리고 특징 단계 퓨전이 평균 0.2095로 가장 높은 수치를 기록하였다. 특징 단계 퓨전 중에는 인코더를 구성하는 두 번째 컨볼루션 블록의 출력을 서로 융합하여 사용할 때가 0.2152 의 MIoU로 가장 높은 수치를 기록하였으며, 그 이후에 융합하면 점차 성능이 하락하는 모습을 보였다. 이에 따라, 센서 퓨전을 사용하는 것이 그렇지 않은 것보다 더 나은 결과를 만들어 낼 수 있고, 동시에 특징 단 계 퓨전을 사용할 때 최적의 성능을 발휘할 수 있음을 알 수 있다. 두 번째 실험으로는 인코더에서 심층 특징 벡터를 구성하는 데 사용되는 트랜스포머 모듈의 수에 따른 성능 변 화를 알아보고자 모듈의 수를 6개에서부터 9, 12, 18, 24로 차례로 늘려가며 총 5개 경우에 대해 학습하고 일련 의 성능 변화를 관찰하A_06_도록 하였다. 이때, 인코더의 나머지 구성에는 앞서 첫 번째 실험에서 가장 좋은 성능을 보였던 특징 단계 퓨전을 적용한 구 성을 사용하였으며, 학습을 위한 손실함수는 이전과 동일하게 \"Focal loss\"함수를 사용하였다. 한편, 표 2는 심층 특징 벡터 형성을 위한 인코더 세부 구성 실험 결과를 나타낸 표이다. 표 2에 기재된 바와 같이, 모듈의 수가 늘어날수록 점차 성능이 상승하여 9개의 모듈을 사용할 때 MIoU를 기준 으로 가장 높은 0.2240의 성능을 보였으며, 그보다 모듈의 수가 더 늘어나면 점차 그 성능이 하락하는 모습을 보였다. 특히, 9개의 모듈을 사용한 인코더가 12개의 모듈을 사용하는 기존 방법의 구성보다 신경망이 얕음에A_06_도 불 구하고 약 1%가량 더 나은 성능을 보인 것은 센서 퓨전을 사용하면 표현력 높은 심층 특징을 비교적 빨리 만들 어 낼 수 있다는 것을 의미한다고 볼 수 있다. 세 번째 실험으로는 시맨틱 세그멘테이션 분야에서 널리 사용되는 두 손실함수인 수학식 1의 \"Focal loss\"함수 와 수학식 2의 \"Dice loss\"함수를 각각 적용하여 신경망을 학습하고 그 결과를 평가함으로써 손실함수의 변화가 성능에 미치는 영향을 관찰하고자 하였다. 또한, 하기의 수학식 7과 같이, 두 손실함수 중 하나를 주(Main) 손실함수로 함과 동시에 나머지 하나를 규제항 (regularizer)으로 추가하여 두 손실함수가 가진 각자의 장점인 픽셀 단위의 정확A_06_도 증진과 전체적인 구조 적 유사A_06_도를 동시에 증진하고자 하는 실험A_06_도 함께 수행하였다. [수학식 7]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 158, "content": "한편, 표 3은 학습에 사용된 손실함수의 구성에 따른 실험 결과를 나타낸 표이다. [표 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 159, "content": "이 때, 신경망은 앞서 두 번째 실험에서 가장 우수한 성능을 보인 본 발명의 일 실시예에 따른 시맨틱 세그멘테 이션 방법으로 만들었으며, 실험 결과 하기의 표 3에서 기재된 바와 같이 \"Dice loss\"함수를 규제항으로 하고 \"Focal loss\"함수를 사용할 때 가장 좋은 결과를 보였다. 이는 \"Focal loss\"함수를 중심으로 학습하여 픽셀 별 정확A_06_도 증진에 학습의 초점을 맞추되, \"Dice loss\"함 수를 규제항으로 함께 사용해 전체적인 유사A_06_도의 증진에 방해가 되는 부분에는 강한 규제 값을, 반대로 전 체적 유사A_06_도 증진에 A_06_도움이 되는 부분에는 약한 규제 값을 부여함으로써 모델이 보다 고르게 학습될 수 있A_06_도록 한 결과로 보인다. A_06_도 10은 본 발명의 일 실시예에 따른 2차원 시맨틱 세그멘테이션의 3차원 해석 결과를 나타낸 A_06_도면이 다. A_06_도 10을 참조하면, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 통해 2차원 세그멘테이션 MIoU 기준 31.94%의 성능을 거둘 수 있었으며, 이를 토대로 A_06_도 10에 A_06_도시된 바와 같이, 3차원 시맨틱 세그멘테이션 정보를 재해석해 낼 수 있다. [표 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 160, "content": "한편, 표 4는 본 발명의 실시예 및 비교예에 따른 시맨틱 세그멘테이션 결과를 나타낸 표이다. 이에 따라, 기존 방법으로는 구해내는 것이 불가능했던 3차원 좌표를 기준으로 하는 세그멘테이션 MIoU를 표 4 와 같이 구해낼 수 있으며, 그 수치는 17.12%를 기록하였다. 이상과 같이, 본 명세서와 A_06_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_06_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_06_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_06_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 사전 학습부 220 : 데이터 전처리부 225 : 추론부 230 : 3차원 해석부 235 : 저장부 A_06_청구범위 A_06_청구항 1 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계; 상기 학습 데이터 생성 장치가, 상기 수신한 점군 데이터 및 상기 이미지를 전처리하는 단계; 및 상기 학습 데이터 생성 장치가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligenc e)을 기초로 상기 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 상기 추출된 특 징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성하는 단계; 를 포함하는 것을 특징으로 하는, 시맨틱 세그멘 테이션 방법. A_06_청구항 2 제1 항에 있어서, 상기 수신하는 단계는 상기 점군 데이터 및 상기 이미지를 동기화 하기 위한 하기의 수학식 3으로 표현되는 캘리브레이션 행렬을 함께 취득하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. [수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 161, "content": "(여기서, u 및 v는 이미지 내 픽셀들의 2차원 좌표, x, y 및 z는 점군 데이터의 3차원 좌표, fu 및 fv는 픽셀 단위의 초점 거리, u0 및 v0은 이미지 평면에서 점군 데이터가 위치하는 x 및 y 좌표를 의미한다.) A_06_청구항 3 제2 항에 있어서, 상기 전처리하는 단계는 상기 캘리브레이션 행렬을 기초로 상기 점군 데이터를 상기 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표 에 사영하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법.A_06_청구항 4 제3 항에 있어서, 상기 추론하는 단계는 상기 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추출하는 인코더 및 상기 심층 특징 을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션을 추론하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_06_청구항 5 제4 항에 있어서, 상기 추론하는 단계는 상기 인코더를 통해 심층 특징을 생성하되, 상기 사영된 점군 데이터 및 상기 이미지를 멀티 모달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 상기 복수의 컨벌루션 블록들을 이용하여 상기 사영된 점군 데이터 및 상기 이미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출하는 것을 특징으로 하는, 시맨틱 세그 멘테이션 방법. A_06_청구항 6 제5 항에 있어서, 상기 추론하는 단계는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상기 이미지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_06_청구항 7 제6 항에 있어서, 상기 추론하는 단계는 연결 계층(concatenate)를 통해 상기 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루션 블록을 통해 연결된 특 징 맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방 법. A_06_청구항 8 제7 항에 있어서, 상기 추론하는 단계는 상기 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부터 해당 지역들을 대표 하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가산하여 각 패치를 대표 하는 임베딩 벡터를 생성하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_06_청구항 9 제8 항에 있어서, 상기 추론하는 단계는 상기 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터(fusion based deep featur e)로 변환하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_06_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬 영된 이미지(image)를 수신하는 단계; 상기 프로세서가, 상기 수신한 점군 데이터 및 상기 이미지를 전처리하는 단계; 및 상기 프로세서가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상 기 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 상기 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 162, "content": "A_06_요약서 A_06_요약 본 발명은 시맨틱 세그멘테이션 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상 기 학습 데이터 생성 장치가, 상기 수신한 점군 데이터 및 상기 이미지를 전처리하는 단계 및 상기 학습 데이터 생성 장치가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상기 전 처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 상기 추출된 특징을 융합하여 시맨 틱 세그멘테이션 특징맵을 생성하는 단계를 포함할 수 있다. A_06_대표A_06_도 A_06_도 7 A_06_도면 A_06_도 1 A_06_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 163, "content": "A_06_도 3 A_06_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 164, "content": "A_06_도 5 A_06_도 6 A_06_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 165, "content": "A_06_도 8 A_06_도 9"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 166, "content": "A_06_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 167, "content": "A_07_발명의 설명 A_07_발명의 명칭 시맨틱 세그멘테이션의 3차원 해석 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for 3D analysis of semantic segmentation, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 168, "content": "A_07_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 카 메라 및 라이다의 센서 퓨전을 통해 얻어낸 융합 데이터를 바탕으로 2차원의 시맨틱 세그멘테이션을 추론하고, 추론 결과를 3차원으로 재구성하기 위한, 시맨틱 세그멘테이션의 3차원 해석 방법 및 이를 실행하기 위하여 기 록매체에 기록된 컴퓨터 프로그램에 관한 것이다. A_07_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_07_도 학습(supervised learning), 비지A_07_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 어노테이션(annotation)을 수행하고, 메타데이터(metadat a)를 입력하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 A_07_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정A_07_도와 운전차가 차량을 제어하는 정A_07_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회가 분류한 6단계에 따르면, 레벨 0단계는 비자동화, 레벨 1단계는 운전자 보조, 레벨 2단계는 부분 자동 화, 레벨 3단계는 조건부 자동화, 레벨 4단계는 고A_07_도 자동화, 그리고 레벨 5단계는 완전 자동화 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그러나, 최근에는 자율주행 차량의 충돌 사고가 빈번히 발생함에 따라, 자율 주행의 안전성 개선에 대한 요구가 늘어나고 있다. 특히, RGB 카메라 센서와 2차원 객체 인식(object detection) 기술을 중심으로 하는 최근의 자율 주행용 첨단 운전자 보조 시스템(Advanced Driver Assistance System, ADAS)으로는 주변 객체들의 3차원적 상대 거리나 구 조 등을 알기 어려운 문제점이 있었다. 이러한 문제점을 해결하기 위하여, 최근에는 라이다(lidar)와 같은 3차원 센서 및 카메라와 같은 2차원 센서로 부터 취득한 멀티 모달(multi modal) 데이터를 융합하는 센서 퓨전(sensor fusion) 기법과, 객체 인식의 단위를 데이터의 구성 단위까지 확장하는 시맨틱 세그멘테이션(semantic segmentation) 기법에 대한 연구가 활발이 진 행되고 있다. 그러나, 카메라와 라이다의 센서 퓨전을 활용하는 기존의 시맨틱 세그멘테이션 연구들은 대부분 3차원의 라이다 점군 데이터와 2차원의 카메라 영상 데이터를 혼합하기 위해 점군 데이터를 2차원의 평면 데이터로 변환한다. 이를 이용해 만들어진 평면 형태의 추론 결과는 라이다의 장점인 3차원의 거리 정보가 손실된 상태이므로 주행 환경에 대한 3차원적 정보들을 제공하기 어려운 문제점이 있었다. A_07_선행기술문헌 A_07_특허문헌 대한민국 등록특허공보 제10-2073873호, ‘시맨틱 세그멘테이션 방법 및 그 장치’, (2020.01.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 169, "content": "A_07_발명의 내용 A_07_해결하고자 하는 과제 본 발명의 일 목적은 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 3차원 정보를 포함하는 융합 데이터를 바탕 으로 2차원의 시맨틱 세그멘테이션을 추론하고, 추론 결과를 3차원으로 재구성하기 위한, 시맨틱 세그멘테이션 의 3차원 해석 방법을 제공하는 것이다. 본 발명의 다른 목적은 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 3차원 정보를 포함하는 융합 데이터를 바 탕으로 2차원의 시맨틱 세그멘테이션을 추론하고, 추론 결과를 3차원으로 재구성하기 위한, 시맨틱 세그멘테이 션의 3차원 해석 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 170, "content": "A_07_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 2차원 시맨틱 세그멘테이션의 3차원 해석 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동 시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 학습 데이터 생성 장치가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상기 점군 데이터 및 상기 이미 지로부터 2차원 시맨틱 세그멘테이션 특징맵을 생성하는 단계 및 상기 학습 데이터 생성 장치가, 상기 생성된 특징맵으로부터 3차원 정보를 해석하는 단계를 포함할 수 있다. 구체적으로, 상기 생성하는 단계 이전에 상기 인공 지능을 사전 학습시키는 단계를 더 포함하는 것을 특징으로 한다. 상기 사전 학습시키는 단계는 사전 저장된 데이터 셋에 포함된 라이다로부터 획득된 점군 데이터, 상기 점군 데 이터와 동시에 카메라를 통해 촬영된 이미지, 상기 라이다 및 상기 카메라 사이의 캘리브레이션 정보 및 상기 점군 데이터의 3차원 포인트 단위로 클래스 라벨이 명시되어 있는 정답 데이터를 기초로 상기 인공 지능을 사전 학습시키는 것을 특징으로 한다. 상기 사전 학습시키는 단계는 상기 점군 데이터 및 상기 정답 데이터를 상기 이미지의 2차원 평면상 좌표에 사 영시키되, 상기 이미지의 사영될 픽셀과 맨해튼 거리(Manhattan distance)가 사전 설정된 값 이내인 픽셀들을 동일한 클래스를 갖A_07_도록 하는 것을 특징으로 한다. 상기 사전 학습시키는 단계는 복수의 손실함수 중 적어A_07_도 하나를 통해 상기 인공 지능을 사전 학습시키는 것을 특징으로 한다. 상기 사전 학습시키는 단계는 하기의 수학식 1로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시키는 것을 특징으로 한다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 171, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 상기 사전 학습시키는 단계는 하기의 수학식 2로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시키는 것을 특징으로 한다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 172, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 상기 사전 학습시키는 단계는 배경 클래스에 해당하는 픽셀을 손실함수의 종류와 관계없이 손실 값의 계산에서 제외시키는 것을 특징으로 한다. 상기 해석하는 단계는 상기 점군 데이터의 3차원 좌표 및 상기 점군 데이터를 사영한 이미지의 2차원 평면상 좌 표 쌍을 준비하는 것을 특징으로 한다. 상기 해석하는 단계는 상기 생성된 특징맵에 대하여 각각의 2차원 평면상 좌표마다 해당 좌표를 중심으로 3*3 크기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용하는 것을 특징으로 한다. 상기 해석하는 단계는 상기 최대 풀링 연산을 통해 각 좌표에 대한 세그먼트를 추정한 벡터를 획득하는 것을 특 징으로 한다. 상기 해석하는 단계는 상기 획득된 벡터 중 가장 큰 확률을 갖는 클래스를 상기 점군 데이터의 3차원 좌표에 대 해 추론된 클래스로 예측하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 2차원 시맨틱 세그멘테이션의 3차원 해석 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송 수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨 팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 라이다(lidar)로부터 획득된 점 군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 프로세 서, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상기 점군 데이터 및 상기 이미지로부터 2차원 시맨틱 세그멘테이션 특징맵을 생성하는 단계 및 상기 프로세서, 상기 생성된 특징 맵으로부터 3차원 정보를 해석하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있 다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_07_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 173, "content": "A_07_발명의 효과 본 발명의 실시 예들에 따르면, 카메라 및 라이다의 센서 퓨전을 통해 얻어낸 3차원 정보를 포함하는 융합 데이 터를 바탕으로 2차원의 시맨틱 세그멘테이션을 추론하고, 추론 결과를 3차원으로 재구성할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 174, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 175, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 176, "content": "A_07_도면의 간단한 설명 A_07_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_07_도이다. A_07_도 2는 본 발명의 일 실시예에 따른 학습 데이터 수집 장치의 구성을 설명하기 위한 예시A_07_도이다. A_07_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_07_도이다. A_07_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_07_도이다. A_07_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 순서A_07_ 도이다. A_07_도 6은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 예시A_07_ 도이다. A_07_도 7은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 구체적으로 설명하기 위한 예시A_07_도이 다. A_07_도 8은 본 발명의 일 실시예에 따른 3차원 해석 방법을 설명하기 위한 예시A_07_도이다. A_07_도 9는 본 발명의 일 실시예에 따른 3차원 해석 방법의 성능을 설명하기 위한 예시A_07_도이다. A_07_도 10은 본 발명의 일 실시예에 따른 2차원 시맨틱 세그맨테이션의 3차원 해석 결과를 나타낸 A_07_도면이 다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 177, "content": "A_07_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_07_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_07_도하게 포괄적인 의미로 해석되거나, 과A_07_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_07_도하게 축소된 의미로 해석되지 않아야 한다.또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_07_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_07_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_07_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_07_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_07_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_07_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_07_도면은 본 발명의 사상을 쉽게 이해할 수 있A_07_도록 하기 위한 것일 뿐, 첨부된 A_07_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_07_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_07_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 라이다(lidar)와 같은 3차원 센서 및 카메라와 같은 2차원 센서로부터 취득한 멀티 모달(multi modal) 데이터를 융합하는 센서 퓨전(sensor fusion)과, 객체 인식의 단위를 데이터의 구성 단위까지 확장하는 시맨틱 세그멘테이션(semantic segmentation)이 활발히 연구되고 있다. 센서 퓨전은 이종의 센서로부터 취득한 멀티 모달 데이터를 상호 융합하는 기술이다. 센서 퓨전은 딥러닝 분야 에서 다수의 원천(source)으로부터 형성된 데이터 또는 정보들이 융합되는 시점에 따라 세 가지 종류로 구분한 다. 데이터 단계 퓨전(data level or early fusion)은 원천으로부터 취득된 데이터 자체를 융합하여 새로운 표현형 을 갖는 융합 데이터를 만든다. 이 융합데이터는 신경망에 전달되어 특징맵(feature map)을 추출, 결과 추론 맵 (prediction map)을 만드는데 활용된다. 이 방법은 신경망에 입력으로 수용되는 데이터의 종류가 적어 신경망의 전체적인 구조가 비교적 단순하게 구성된다는 장점이 있다. 이와 달리 특징 단계 퓨전(deep feature level or mid-level fusion)은 원천으로부터 취득한 멀티 모달 데이터 를 신경망에 입력으로 각각 전달한다. 병렬 구조의 신경망을 통해 각각의 입력으로부터 추출되는 멀티 모달 특 징들은 신경망의 내부에서 하나로 융합되어 심층 융합 특징을 구성하며, 이는 결과 추론 맵을 만드는데 활용된 다. 이 방법은 신경망이 결과를 추론함에 있어 멀티 모달 데이터로부터 추출한 각각의 특징부터 그들의 융합 특 징까지 다양하게 활용 가능한 장점이 있다. 추론 단계 퓨전(score level or late fusion) 또한 멀티 모달 데이터를 신경망의 입력으로 각각 전달하나, 특징 단계 퓨전과 달리 각각의 원천마다 독립된 신경망을 사용한다. 각 신경망이 독립적으로 만들어낸 결과 추론 맵 들은 합산, 평균 등의 단순 연산이나 별A_07_도로 학습된 분류기를 활용함으로써 최종 추론 맵을 A_07_도출하는 데 활용된다. 이 방법은 추론 결과가 융합에 사용된 센서들 사이의 특성 차나 상호 간섭에 의한 영향에 강인하 다는 장점이 있다. 시맨틱 세그멘테이션은 자율 주행 차량이 주행 환경을 인지하는데 필요한 핵심적인 요소 기술 중의 하나로서 카 메라로부터 취득한 2차원 RGB 영상의 객체들을 픽셀 단위로 분류(dense prediction)할 수 있다. 특히, 최근에는 기존의 컨볼루션 계층을 중심으로 구성된 심층 신경망을 사용하는 방식에서 더 나아가 셀프 어 텐션을 기반으로 특징들 사이의 상대적 중요A_07_도를 알아내 보다 표현력이 높은 심층 특징들을 추출해낼 수있는 트랜스포머 모듈을 추가하여 2차원 시맨틱 세그멘테이션의 성능을 증진하는 연구들이 늘고 있다. 트랜스포머 모듈을 활용하면 신경망에 컨볼루션 계층을 비교적 적게 사용하더라A_07_도 표현력이 좋은 심층 특 징들을 추출할 수 있으며, 이 심층 특징들을 바탕으로 기존의 방법들보다 상대적으로 좋은 성능을 보였다. 그러나 앞서 언급한 바와 같이 이들 2차원 시맨틱 세그멘테이션 방법이 만들어내는 평면 형태의 추론 결과만으 로는 차량의 실제 주행 환경인 3차원에서의 객체의 구조나 상대적 거리 등을 파악하기 어려우며, 3차원 정보를 추론하기 위해 여러 대의 카메라를 사용할 경우에는 계산량이 과A_07_도하게 늘어난다는 문제가 있다. 또한, 카메라와 라이다의 센서 퓨전을 활용하는 기존의 시맨틱 세그멘테이션 연구들은 대부분 3차원의 라이다 점군 데이터와 2차원의 카메라 영상 데이터를 혼합하기 위해 점군 데이터를 2차원의 평면 데이터로 변환한다. 이를 이용해 만들어진 평면 형태의 추론 결과는 라이다의 장점인 3차원의 거리 정보가 손실된 상태이므로 주행 환경에 대한 3차원적 정보들을 제공하기 어렵다. 이러한 한계를 극복하고자, 본 발명은 시맨틱 세그멘테이션 기법과, 카메라 및 라이다의 센서 퓨전을 통해 얻어 낸 3차원 정보를 포함하는 융합 데이터를 기초로 한 2차원 시맨틱 세그멘테이션 결과를 3차원으로 재구성할 수 있는 다양한 수단들을 제안하고자 한다. A_07_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_07_도이다. A_07_도 1에 A_07_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 수집 장 치, 학습 데이터 생성 장치 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar) 및 카메라(camera)로부터 실 시간으로 데이터를 수집하는 장치이다. 하지만, 이에 한정된 것은 아니고, 학습 데이터 수집 장치는 레이 더(radar) 및 초음파 센서(ultrasonic sensor)를 포함할 수A_07_도 있다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비 되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라A_07_도 복수 개로 구비될 수 있다. 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하 는 센서들의 종류에 대해서는 추후 A_07_도 2를 참조하여 보다 구체적으로 설명하기로 한다. 다음 구성으로, 학습 데이터 생성 장치는 복수의 학습 데이터 수집 장치 각각으로부터 이동통신 (mobile communication)을 이용하여 각각의 학습 데이터 수집 장치에 의해 실시간으로 수집된 데이터를 수 신하고, 수신된 데이터에 대하여 어노테이션을 수행할 수 있다. 이러한, 학습 데이터 생성 장치는 학습 데이터 생성 장치는 인공지능 학습 장치로부터 인공지능 (AI) 학습용 데이터의 요청이 수신되기 이전에, 선제적으로 인공지능(AI) 학습용 데이터를 생성할 수 있는 빅데 이터(big data)를 구축해 놓을 수 있다. 특징적으로, 학습 데이터 생성 장치는 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 수신한 점군 데이터 및 이미지를 전처리하고, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성 할 수 있다. 또한, 학습 데이터 생성 장치는 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라 (camera)를 통해 촬영된 이미지(image)를 수신하고, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 점군 데이터 및 이미지로부터 2차원 시맨틱 세그멘테이션 특징맵을 생성하 고, 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치 및 인공지능 학습 장치 와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_07_ 도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 한편, 학습 데이터 생성 장치에 관한 구체적인 설명은 이하, A_07_도 3 및 A_07_도 4를 참조하여 후술하 A_07_도록 한다. 다음 구성으로, 인공지능 학습 장치는 인공지능(AI)을 개발하는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 인공지능(AI)이 개발 목적을 달성하기 위하여 인공지능(AI) 학습용 데 이터가 만족해야 하는 요구 사항을 포함하는 요구 값을 학습 데이터 생성 장치에 전송할 수 있다. 인공지 능 학습 장치는 학습 데이터 생성 장치로부터 인공지능(AI) 학습용 데이터를 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 인공지능(AI) 학습용 데이터를 이용하여, 개발하고자 하는 인공지능 (AI)을 기계 학습할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_07_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치 및 인공지능 학습 장 치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네 트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것A_07_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. A_07_도 2는 본 발명의 일 실시예에 따른 센서들을 설명하기 위한 예시A_07_도이다. A_07_도 2에 A_07_도시된 바와 같이, 본 발명의 일 실시예에 따른 학습 데이터 수집 장치는 차량에 고 정 설치된 레이더, 라이다, 카메라 및 초음파 센서 중 하나 이상을 제어하여, 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집할 수 있다. 여기서, 차량은 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집하기 위한 레이더, 라이다 , 카메라 및 초음파 센서가 설치된 차량으로, 인공지능(AI)에 의해 자율주행을 수행하는 차량과는 서로 구별될 수 있다. 레이더는 차량에 고정 설치되어 차량의 주행 방향을 향하여 전자기파(electromagnetic wave)를 발 사하고, 차량의 전방에 위치하는 객체(object)에 의해 반사되어 돌아온 전자기파를 감지하여, 차량이 전방에 대한 영상에 해당하는 감지 데이터를 생성할 수 있다. 다르게 말하면, 감지 데이터는 차량에 고정 설치된 레이더에 의해 차량의 주행 방향을 향하여 발사된 전자기파를 반사시킨 점들(points)에 대한 정보이다. 따라서, 감지 데이터에 포함된 점들의 좌표들은 차량 의 전방에 위치하는 객체의 위치 및 형상에 대응하는 값을 가질 수 있다. 이러한, 감지 데이터는 2차원 정보가 될 수 있으나, 이에 한정되지 않고 3차원 정보가 될 수A_07_도 있다. 라이다는 차량에 고정 설치되어 차량의 주위로 레이저 펄스(laser pulse)를 방사하고, 차량의 주위에 위치하는 객체에 의해 반사되어 돌아온 빛을 감지하여, 차량의 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 다르게 말하면, 3D 점군 데이터는 차량에 고정 설치된 라이다에 의해 차량의 주위로 방사된 레이저 펄 스를 반사시킨 점들에 대한 3차원 정보이다. 따라서, 3D 점군 데이터에 포함된 점들의 좌표들은 차량의 주 위에 위치하는 객체의 위치 및 형성에 대응하는 값을 가질 수 있다. 카메라는 차량에 고정 설치되어 차량의 주위에 대한 2차원 이미지를 촬영할 수 있다. 이와 같은, 카메라는 서로 다른 방향을 촬영할 수 있A_07_도록 복수 개가 지표면과 수평 또는 수평 방향으로 이격되게 설치될 수 있다. 예를 들어, A_07_도 2는 서로 다른 6개의 방향을 촬영할 수 있는 6개의 카메라가 고정 설 치된 차량의 예시를 A_07_도시하고 있으나, 차량에 설치될 수 있는 카메라가 다양한 개수로 구성될"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 178, "content": "수 있음은 본 발명이 속한 기술분야의 통상의 지식을 가진 자에게 자명할 것이다. 다르게 말하면, 2D 이미지는 차량에 고정 설치된 카메라에 의해 촬영된 이미지이다. 따라서, 2D 이미지 에는 카메라가 향하는 방향에 위치하는 객체의 색상 정보가 포함될 수 있다. 초음파 센서는 차량에 고정 설치되어 차량의 주위로 초음파(ultrasonic)를 발사하고, 차량과 인접하게 위치하는 객체에 의해 반사되어 돌아온 음파를 감지하여, 차량에 설치된 초음파 센서와 객체 사이의 거리에 해당하는 거리 정보를 생성할 수 있다. 일반적으로, 초음파 센서는 복수 개로 구성되어, 객 체와 접촉하기 쉬운 차량의 전방, 후방, 전측방 및 후측방에 고정 설치될 수 있다. 다르게 말하면, 거리 정보는 차량에 고정 설치된 초음파 센서에 의해 감지된 객체로부터의 거리에 관한 정보이다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_07_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_07_도이다. A_07_도 3을 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 사전학습부, 데이터전 처리부, 추론부, 3차원해석부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지 및 점군 데이터를 학습 데이터 수집 장치 로부터 수신할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 동기화 하기 위한 캘리브레이 션 행렬을 함께 수신할 수 있다. 또한, 통신부는 시맨틱 세그멘테이션의 3차원 해석 결과를 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 받 거나, 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 시맨틱 세그멘테이션 특징맵을 생성하거나, 생성된 시맨틱 세그멘테 이션 특징맵을 3차원 해석하기 위한 다양한 설정 값들을 입력받고, 생성된 결과 값들을 출력할 수 있다. 다음 구성으로, 사전 학습부는 2차원 시맨틱 세그멘테이션을 추론하기 위한 인공 지능을 사전 학습시킬 수 있다. 구체적으로, 사전 학습부는 사전 저장된 데이터 셋에 포함된 라이다로부터 획득된 점군 데이터, 점군 데이 터와 동시에 카메라를 통해 촬영된 이미지, 라이다 및 카메라 사이의 캘리브레이션 정보, 점군 데이터의 3차원 포인트 단위로 클래스 라벨이 명시되어 있는 정답 데이터를 기초로 인공 지능을 사전 학습시킬 수 있다. 이때, 사전 학습부는 점군 데이터 및 정답 데이터를 이미지의 2차원 평면상 좌표에 사영시키되, 이미지의 사영될 픽셀과 맨해튼 거리(Manhattan distance)가 사전 설정된 값 이내인 픽셀들을 동일한 클래스를 갖A_07_도 록 할 수 있다. 또한, 사전 학습부는 복수의 손실함수 중 적어A_07_도 하나를 통해 인공 지능을 사전 학습시킬 수 있다. 이때, 사전 학습부는 하기의 수학식 1으로 표현되는 손실함수를 통해 인공 지능을 사전 학습시킬 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 179, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 또한, 사전 학습부는 하기의 수학식 2로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시킬 수 있 다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 180, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) 이때, 사전 학습부는 배경 클래스에 해당하는 픽셀을 손실함수의 종류와 관계없이 손실 값의 계산에서 제 외시킬 수 있다. 다음 구성으로, 데이터전처리부는 학습 데이터 수집 장치로부터 수신한 점군 데이터 및 이미지를 전 처리할 수 있다. 구체적으로, 데이터전처리부는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데 이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 여기서, 캘리브레이션 행렬은 하기의 수학식 3으로 표현될 수 있다. [수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 181, "content": "(여기서, u 및 v는 이미지 내 픽셀들의 2차원 좌표, x, y 및 z는 점군 데이터의 3차원 좌표, fu 및 fv는 픽셀 단위의 초점 거리, u0 및 v0은 이미지 평면에서 점군 데이터가 위치하는 x 및 y 좌표를 의미한다.) 다음 구성으로, 추론부는 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence) 을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하 여 시맨틱 세그멘테이션 특징맵을 생성할 수 있다. 구체적으로, 추론부는 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추출하는 인 코더 및 심층 특징을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션을 추론할 수 있다. 이때, 추론부는 인코더를 통해 심층 특징을 생성하되, 사영된 점군 데이터 및 이미지를 멀티 모달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 복수의 컨벌루션 블록들을 이용하여 사영된 점군 데이터 및 이 미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출할 수 있다. 즉, 추론부는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상기 이미지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출할 수 있다. 다음으로, 추론부는 연결 계층(concatenate)를 통해 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루션 블 록을 통해 연결된 특징맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성할 수 있다. 다음으로, 추론부는 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가산하여 각 패치를 대표하는 임베딩 벡터를 생성할 수 있다. 다음으로, 추론부는 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터(fusion based deep feature)로 변환할 수 있다. 다음으로, 추론부는 재배열(reshape) 계층을 통해 변환된 심층 특징 벡터를 입력 크기 대비 1/16 크기의 특징맵으로 변환할 수 있다. 다음으로, 추론부는 변환된 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배 수로 확장하되, 접합 계층을 통해 복수의 업샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하는 특징맵과 접합할 수 있다. 여기서, 추론부는 복수의 업 샘플링 계층 각각에서 업샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루션 계층에 통과시킬 수 있다. 다음 구성으로, 3차원해석부는 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 구체적으로, 3차원해석부는 점군 데이터의 3차원 좌표 및 점군 데이터를 사영한 이미지의 2차원 평면상 좌 표 쌍을 준비할 수 있다. 또한, 3차원해석부는 생성된 특징맵에 대하여 각각의 2차원 평면상 좌표마다 해당 좌표를 중심으로 3*3 크 기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용할 수 있다. 또한, 3차원해석부는 최대 풀링 연산을 통해 각 좌표에 대한 세그먼트를 추정한 벡터를 획득할 수 있다. 그리고, 3차원해석부는 획득된 벡터 중 가장 큰 확률을 갖는 클래스를 점군 데이터의 3차원 좌표에 대해 추론된 클래스로 예측할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터를 설계하는데 필요한 데이터를 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. A_07_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_07_도이다. A_07_도 4를 참조하면, 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수 신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지 (Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치 및 인공지능 학습 장치와 데이터를 송 수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이터를 입력 받고, 생성된 결과 값을 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지 와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 시맨틱 세그멘테이션 방법 및 3차원 해석 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 시맨틱 세그멘테이션 방법 및 3차원 해석 방법을 수행하기 위한 프로그램을 저장하는 데 이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 수신한 점군 데이터 및 이미지를 전처리하는 단계 및 프로세서가,전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 추출된 특징을 융합하여 시맨틱 세그멘테이션을 추론하는 단계를 실행하기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 또한, 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어 (280a, 280b)는 프로세서가 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라 (camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 프로세서, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 점군 데이터 및 이미지로부터 2차원 시맨틱 세그멘테이션 특 징맵을 생성하는 단계 및 프로세서, 생성된 특징맵으로부터 3차원 정보를 해석하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_07_도 4에 A_07_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_07_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_07_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_07_도록 구성될 수 있으며, 그 역A_07_도 마찬가지이다. A_07_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 순서A_07_ 도이다. A_07_도 5를 참조하면, 먼저 S100 단계에서 학습 데이터 생성 장치는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 수신할 수 있다. 이때, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 캘리브레이션 형렬 을 추가로 수신할 수 있다. 다음으로, S200 단계에서 학습 데이터 생성 장치는 학습 데이터 수집 장치로부터 수신한 점군 데이터 및 이미지 를 전처리할 수 있다. 구체적으로, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 다음으로, S300 단계에서 학습 데이터 생성 장치는 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하 고, 추출된 특징을 융합하여 시맨틱 세그멘테이션 특징맵을 생성할 수 있다. 구체적으로, 학습 데이터 생성 장치는 사영된 점군 데이터 및 상기 이미지로부터 심층 특징(deep feature)을 추 출하는 인코더 및 심층 특징을 해석하여 세그멘테이션 추론 맵을 생성하는 디코더를 통해 시맨틱 세그멘테이션 을 추론할 수 있다. 이때, 학습 데이터 생성 장치는 인코더를 통해 심층 특징을 생성하되, 사영된 점군 데이터 및 이미지를 멀티 모 달(multi modal) 방식으로 수용하고, 병렬 구조로 배치된 복수의 컨벌루션 블록들을 이용하여 사영된 점군 데이 터 및 이미지로부터 지역적 특징 및 구조적 특징을 상호 독립적으로 추출할 수 있다. 즉, 학습 데이터 생성 장치는 제1 컨벌루션 블록 및 제2 컨벌루션 블록을 통해 상기 사영된 점군 데이터 및 상 기 이미지 각각을 입력 크기의 1/2 및 1/4로 단계적으로 다운 샘플링하여 특징맵을 추출할 수 있다. 또한, 학습 데이터 생성 장치는 연결 계층(concatenate)를 통해 다운 샘플링 된 특징맵을 연결하고, 제3 컨벌루 션 블록을 통해 연결된 특징맵을 입력 크기의 1/8 면적을 가진 융합 특징맵으로 생성할 수 있다. 학습 데이터 생성 장치는 생성된 융합 특징맵을 사전 설정된 크기의 패치(patch)로 분할하고, 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고, 위치 임베딩(position embedding)을 가산 하여 각 패치를 대표하는 임베딩 벡터를 생성할 수 있다. 학습 데이터 생성 장치는 생성된 임베딩 벡터를 연속된 복수의 트랜스포머 모듈을 통해 심층 특징 벡터(fusion based deep feature)로 변환할 수 있다. 학습 데이터 생성 장치는 재배열(reshape) 계층을 통해 변환된 심층 특징 벡터를 입력 크기 대비 1/16 크기의 특징맵으로 변환할 수 있다. 학습 데이터 생성 장치는 변환된 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배 수로 확장하되, 접합 계층을 통해 복수의 업샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하는 특징맵과 접합할 수 있다. 여기서, 학습 데이터 생성 장치는 복수의 업 샘플링 계층 각각에서 업샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루션 계층에 통과시킬 수 있다. 그리고, S400 단계에서 학습 데이터 생성 장치는 생성된 특징맵으로부터 3차원 정보를 해석할 수 있다. 구체적으로, 학습 데이터 생성 장치는 점군 데이터의 3차원 좌표 및 점군 데이터를 사영한 이미지의 2차원 평면 상 좌표 쌍을 준비할 수 있다. 또한, 학습 데이터 생성 장치는 생성된 특징맵에 대하여 각각의 2차원 평면상 좌표마다 해당 좌표를 중심으로 3*3 크기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용할 수 있다. 또한, 학습 데이터 생성 장치는 최대 풀링 연산을 통해 각 좌표에 대한 세그먼트를 추정한 벡터를 획득할 수 있 다. 그리고, 학습 데이터 생성 장치는 획득된 벡터 중 가장 큰 확률을 갖는 클래스를 점군 데이터의 3차원 좌표에 대해 추론된 클래스로 예측할 수 있다. 이하 A_07_도면을 참조하여, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법에 대하여 더 욱 상세히 설명하A_07_도록 한다. A_07_도 6은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 및 3차원 해석 방법을 설명하기 위한 예시A_07_ 도이고, A_07_도 7은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 구체적으로 설명하기 위한 예시 A_07_도이고, A_07_도 8은 본 발명의 일 실시예에 따른 3차원 해석 방법을 설명하기 위한 예시A_07_도이다. A_07_도 6 내지 A_07_도 8을 참조하면, 데이터 원천(Data Source) 단계에서는 카메라와 라이다 센서로부터 각각 차량 전방의 2차원 RGB 영상과 차량 둘레 전 영역의 3차원 점군 데이터를 취득한다. 또한 두 센서의 데이터 취득 영역이 서로 상이하므로 이를 동기화 하기 위해 사용되는 캘리브레이션 행렬을 함께 취득한다. 이어서, 데이터 전처리(data pre-process) 단계에서는 라이다로부터 취득한 데이터 중 차량의 전방 영역, 즉 카 메라가 영상을 취득하는 영역과 동일한 영역의 데이터를 선별하는 과정을 수행한다. 이를 통해 구분된 N개의 전방 영역 점군 좌표(x,y,z) 데이터(N, 3)는 앞서 구한 캘리브레이션 행렬을 바탕으로 RGB 영상과 동일한 크기(H, W)를 갖는 2차원 평면상의 사영 좌표 (u,v)로 변환된다. 이로써 전방 영역의 점군 데이터인 N개의 3차원 좌표 (x,y,z)들은 (H, W, 1) 크기의 사영 영상으로 변환되며 이 사영 영상과 (H, W, 3) 크기의 RGB 영상을 다음 단계인 추론 단계로 전달한다. 추론(prediction) 단계에서는 앞서 전달받은 두 영상을 바탕으로 심층 신경망을 이용한 2차원 시맨틱 세그멘테 이션을 수행한다. 이때 사용되는 심층 신경망은 입력으로부터 심층 특징(deep feature)을 추출하는 인코더 (encoder)와 심층 특징을 해석해 세그멘테이션 추론 맵을 만들어내는 디코더(decoder)를 포함하는 인코더-디코 더 구조로 이루어져 있다. 여기서, 인코더는 \"ResNet50\"의 컨볼루션 블록들을 바탕으로 입력 데이터의 지역적, 구조적 특징들을 추출하고 트랜스포머 모듈을 사용하여 추출된 특징들을 보다 표현력이 좋은 심층 특징들로 변환한다. 대신 신경망이 사영 영상을 위한 입력(remission input)과 RGB 영상을 위한 입력(RGB input)을 동시에 수용할 수 있A_07_도록 구성하고 신경망 내 모듈들을 센서 퓨전 방법에 따라 병렬 구조로 배치하여 기존 방법과 달리 멀티 모달 데이터의 처리를 가능하게 한다. 또한, 디코더는 인코더의 각 단계에서 추출되는 특징맵들이 심층 특징과 연결되어 입력과 동일한 (H, W, C) 크 기의 결과 추론 맵을 구성할 수 있A_07_도록 컨볼루션 계층을 비롯한 여러 신경망 계층들로 구성한다. 이 때 C 는 픽셀 단위로 객체를 분류할 수 있는 클래스의 총 개수이다. 이에 따라, A_07_도 7에 A_07_도시된 바와 같이, 신경망은 멀티 모달 입력을 수용하고 병렬 구조로 배치된 컨볼 루션 블록들을 이용하여 각 입력으로부터 각 입력의 구조적, 지역적 특징들을 상호 독립적으로 추출한다. 이에 따라, 추출된 입력 대비 1/2, 1/4 면적의 특징맵들이 추출되며 그 중 1/4 면적의 특징맵들을 특징 단계 퓨 전에 활용하기 위해 연결 계층(concatenate)을 두어 서로 이어 붙인다. 이렇게 만들어진 연결 특징맵은 세 번째 컨볼루션 블록을 통해 입력 대비 1/8 면적을 가진 융합 특징맵으로 만들어진다. 이어서, 트랜스포머 모듈에 앞서 만든 융합 특징맵을 전달하기 위해 융합 특징맵을 (2, 2) 크기의 패치로 분할 한 뒤 각 지역으로부터 해당 지역들을 대표하는 패치 임베딩(patch embedding)을 구성하고 위치 임베딩 (position embedding)을 가산하여 각 패치를 대표하는 임베딩 벡터들을 만든다. 이어서 이 임베딩 벡터들은 9개 의 연속된 트랜스포머 모듈로 구성된 트랜스포머 블록을 거치면서 융합 특징맵에 포함된 중요 특징들을 중심으 로 구성된 심층 특징 벡터(fusion based deep feature)로 변환된다. 다음으로, 디코더에서는 먼저 인코더로부터 추출한 심층 특징 벡터들을 재배열(reshape) 계층을 통해 입력 대비 1/16 크기의 특징맵으로 변환하고 3*3 커널을 사용하는 컨볼루션 계층을 통과하A_07_도록 한 후, 업 샘플링(up sampling) 계층을 통해 그 면적을 두 배로 확장한다. 이 확장된 특징맵이 앞서 심층 특징 벡터에 담겨 있던 주요 특징들을 담고 있다 하더라A_07_도 벡터화로 인해 손실된 지역적, 구조적 특성들은 컨볼루션과 업샘플링 만으로 쉽게 회복되지 않는다. 따라서, 접합 계층을 이용하여 해당 특징맵과 앞서 인코더에서 추출된 동일 크기의 특징맵을 이어 붙임으로써 특징맵이 심층 특징 벡터의 주요 특징들과 인코딩 당시 추출된 지역적, 구조적 특성들을 동시에 내포할 수 있 A_07_도록 한다. 이어서, 접합 특징맵을 3*3 커널을 사용하는 컨볼루션 계층을 이용해 서로 혼합하고, 다시 업샘플링 계층을 통 해 확장, 그리고 인코더 내 동일 크기의 특징맵을 접합하는 과정을 2회 더 반복함으로써 특징맵의 크기를 입력 대비 1/2까지 복원해 낸다. 마지막으로, 그 특징맵을 다시 한번 업샘플링하고 3*3 커널을 사용하는 컨볼루션 계층을 적용하여 입력과 동일 한 크기의 특징맵을 만들어 낸 뒤 이를 소프트맥스 활성화 함수와 1*1 커널을 사용하는 컨볼루션 계층인 세그멘 테이션 헤드에 전달하여 세그멘테이션 추론맵을 만들어 낸다. 프로세스의 마지막 단계인 데이터 후처리(data post-processing) 단계에서는 A_07_도 8에 A_07_도시된 바와 같 이 점군 데이터를 활용하여 신경망이 만들어내는 2차원의 세그멘테이션 추론 맵을 3차원 정보로 재해석하는 과 정(reconstruction)을 수행한다. 먼저, 데이터 전처리 단계에서 사영 영상 생성에 사용했던 N개의 3차원 좌표 (x,y,z)와 이를 사영한 N개의 2차 원 평면상 좌표 (u,v) 쌍을 준비한다. 이어서, 신경망에 의해 추론된 (H, W, C) 크기의 세그멘테이션 추론 맵에 대하여 각각의 2차원 좌표 (u,v)마다 해당 좌표를 중심으로 3*3 크기의 커널을 사용하는 최대 풀링(max pooling) 연산을 적용한다. 이를 통해, 본 발명은 N 개의 좌표 각각에 대해 세그먼트를 추정한 벡터를 얻어낼 수 있으며 그 중 가장 큰 확 률을 갖는 클래스를 3차원 좌표(x,y,z)에 대해 추론된 클래스로 본다. 따라서, 데이터 전처리 단계에서 분리한 전방 영역의 점군 데이터 각각에 대한 시맨틱 세그멘테이션 추론 결과 를 얻을 수 있으며, 그 결과 각 좌표의 세그먼트 값을 담은 (N, C) 크기의 출력 행렬을 얻을 수 있다. 이하, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법 및 3차원 해석 방법의 성능에 대하여 설명하A_07_ 도록 한다. A_07_도 9는 본 발명의 일 실시예에 따른 3차원 해석 방법의 성능을 설명하기 위한 예시A_07_도이다. 실험 실험은 \"AMD EPYC 7742 64-Core Processor\" 급의 CPU(Central Processing Unit), \"A100 80GB\" 급의 GPU(Graphics Processing Unit) 및 2TB의 메모리가 탑재된 하드웨어 환경에서 진행하였다. 신경망 구현을 위한 소프트웨어 환경은 \"ubuntu 20.04\" 운영체제에 설치된 \"Python 3.9\"에 \"Tensorflow 2.80\" 과 \"Keras 2.80\"을 설정해 활용하였다. 인코더에 사용된 트랜스포머 모듈은 GeLU(Gauusian Error Linear Unit) 활성화 함수를 사용하A_07_도록 하였으 며, 디코더를 구성하는 모든 컨볼루션 계층은 세그멘테이션 헤드를 제외하고 모두 ReLU 활성화 함수를 사용하 A_07_도록 하였고 각각의 컨볼루션 계층 이후에는 배치 정규화 계층과 드롭 아웃 계층을 두어 신경망의 과적합 을 최소화할 수 있A_07_도록 하였다. 신경망의 학습에는 Adam(Adaptive Moment Estimation) 최적화기를 사용하였으며 학습률은 0.001로 시작해 학습 이 2에폭(Epoch)이상 진행되지 않을 때마다 0.75배씩 줄어들A_07_도록 하였다. 학습은 최대 500에폭 동안 진행하A_07_도록 설정하였으며 이때 손실 값이 연속적으로 10에폭 이상 낮아지지 않 으면 학습을 조기 종료하A_07_도록 설정하였다. 본 발명의 일 실시예에 시맨틱 세그멘테이션 방법의 학습과 평가에 사용된 데이터 셋은 \"semantic KITTI\"로 자 율 주행 분야의 3차원 객체 인식이나 시맨틱 세그멘테이션 등의 연구에 널리 활용된다. 사용된 데이터 셋은 총 21개 시퀀스로 구성되어 있으며 각 시퀀스에는 2차원 RGB 프레임 이미지들과 3차원 점군 그리고 각 데이터를 취득한 카메라와 라이다 센서 사이의 캘리브레이션 정보 그리고 각각의 3차원 포인트 단위 로 클래스 라벨이 명시되어 있는 정답(Ground Truth: GT) 데이터를 포함하고 있다. 본 실험에서는 그 중 8번 시퀀스를 제외한 0번부터 10번까지의 총 10개 시퀀스, 총 19,130건의 프레임을 학습 데이터로 사용하고 4,071건의 프레임으로 구성된 8번 시퀀스를 평가 데이터로 활용한다. 다만, 사용된 데이터 셋에는 2차원 시맨틱 세그멘테이션을 위한 정답 데이터가 존재하지 않는다. 따라서, 점군 데이터에 대해 매겨진 정답 데이터를 점군 데이터를 사영하는 방법과 동일한 방법으로 사영하여 2차원 시맨틱 세그멘테이션을 위한 정답 데이터를 만들 경우, A_07_도 9의 (a)와 같이 대부분의 픽셀의 값이 0을 갖는 희소 데이터가 만들어진다. 이는, 신경망의 학습을 극A_07_도로 저해하는 요소로 작용하므로, 이를 보완하기 위해 정답 데이터의 사영 될 픽셀과 맨해튼 거리(Manhattan distance)가 2 이내인 픽셀들을 동일한 클래스를 갖A_07_도록 하여 A_07_도 9의 (b)와 같이 정답 데이터의 희소성을 줄여 활용하였다. 실험 과정에서 신경망의 학습에 사용되는 손실함수는 하기의 수학식 1 및 2로 각각 표현되는 \"Focal loss\"함수 및 \"Dice loss\"함수를 사용하였다.[수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 182, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 183, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) Focal loss 함수는 수학식 1과 같이 크로스 엔트로피를 기반으로 하는 추론 확률(ppred)과 정답(ptrue) 사이의 가 중치 합을 구하여 손실 값을 계산하는 방법으로 각각의 픽셀 단위로 추론 정확성(accuracy)을 증진하는 것을 목 적으로 한다. 한편, Dice loss 함수는 수학식 2와 같이 Dice 계수를 기반으로 하는 추론 확률(ppred)과 정답(ptrue) 사이의 유사 A_07_도를 구하여 손실 값을 계산하는 방법으로 전체적인 추론의 유사A_07_도(similarity)를 증진하는 것을 목 적으로 한다. 이때, 배경 클래스에 해당하는 픽셀의 경우에는 손실함수의 종류와 관계없이 손실 값의 계산에서 제외함으로써 신경망이 배경 지역을 정확히 추론하는데 편향되어 학습되는 문제를 방지하A_07_도록 한다. 본 발명의 실시예에 따른 시맨틱 세그멘테이션 성능 평가를 위해서는 시맨틱 세그멘테이션 분야에서 널리 사용 되는 하기의 수학식 4에 따른 MIoU (Mean Intersection over Union)을 사용하였다. [수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 184, "content": "MIoU는 수학식 4에서 보이는 바와 같이, 전체 N장으로 이루어진 영상 데이터 셋에서 각 영상마다 구한 IoU 값의 평균을 나타내며, IoU는 영상의 픽셀마다 추론된 클래스들과 해당 픽셀의 정답을 비교하여 그 둘이 서로 동일한 데이터의 수(TP)를 전체 추론 데이터의 수로 나누어 준 값(TP+FN+FP)을 의미한다. 이는 추론치가 클래스 라벨이 명시되어 있는 정답(GT)과 일치하는 비율을 의미하는 것으로써 픽셀 단위의 객체 추론에 대한 정확A_07_도를 나타낼 수 있다. 이때에A_07_도 손실함수의 경우와 마찬가지로 배경 픽셀에 해당하는 클래스는 평가 값의 계산에서 제외함으로써 신경망이 전경 지역을 정확히 추론하지 못함에A_07_도 성능이 높게 나오는 편향 문제를 방지하A_07_도록 한다. 절제 연구 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법에서 사용할 신경망의 구조와 손실함수의 구성 등에 따른 성능 평가를 토대로 상세 사항들을 결정하기 위해 수행한 절제 실험 결과들에 관해 서술한다. 먼저, 첫 번째 실험에서는 제안 방법에 대한 최적의 센서 퓨전을 결정하기 위해 신경망에 적용되는 퓨전 방법을 데이터 단계 퓨전, 특징 단계 퓨전 그리고 추론 단계 퓨전으로 나누어 각각 적용함으로써 센서 퓨전을 적용한 경우와 그렇지 않은 경우의 성능을 비교하였다. 이때, 데이터 단계 퓨전을 적용한 신경망은 하기의 수학식 5와 같이 연결 계층을 통해 점군의 사영 영상(XPCD)과 RGB 영상(XRGB)을 이어 붙인 (H, W, 4) 크기의 하나의 융합 데이터를 인코더의 입력으로 전달하고, 그 출력을 다 시 디코더의 입력으로 전달하여 하나의 2차원 세그멘테이션 추론맵(Yseg)을 만들어 낸다.[수학식 5]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 185, "content": "한편, 추론 단계 퓨전을 적용한 신경망은 하기의 수학식 6에 표시된 바와 같이 두 입력을 위한 서로 다른 가중 치((θen1, θde1), (θen2, θde2))를 갖는 독립적인 신경망들을 각각 적용하여 두 개의 2차원 세그멘테이션 추론 맵을 만들어내며 이들을 평균(average)하는 방법으로 하나의 2차원 세그멘테이션 추론맵을 만들어 낸다. [수학식 6]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 186, "content": "또한, 특징 단계 퓨전의 경우에는 신경망을 구성하는 초입의 블록으로부터 시작하여 한 블록씩 더 깊은 신경망 블록에서 퓨전 하A_07_도록 하여 다양한 경우의 수를 비교할 수 있A_07_도록 하였으며, 학습의 진행은 \"Focal loss\"함수를 바탕으로 수행하였다. [표 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 187, "content": "한편, 표 1은 본 발명의 실시예에 따른 시맨틱 세그멘테이션 방법에 대한 센서 융합 응용 실험 결과를 나타낸 표이다. 표 1에 기재된 바와 같이, 센서 퓨전을 사용하지 않고 입력으로 RGB 영상만을 사용한 단일 모달(single modal) 의 방법이 센서 퓨전을 사용한 그 어떤 경우보다 낮은 0.1012의 MIoU를 기록하였다. 센서 퓨전을 적용한 경우들끼리 비교할 경우에는 추론 단계 퓨전을 적용한 경우가 0.1844로 가장 낮은 MIoU를 기록하였으며 데이터 단계 퓨전이 0.2045 그리고 특징 단계 퓨전이 평균 0.2095로 가장 높은 수치를 기록하였다. 특징 단계 퓨전 중에는 인코더를 구성하는 두 번째 컨볼루션 블록의 출력을 서로 융합하여 사용할 때가 0.2152 의 MIoU로 가장 높은 수치를 기록하였으며, 그 이후에 융합하면 점차 성능이 하락하는 모습을 보였다. 이에 따라, 센서 퓨전을 사용하는 것이 그렇지 않은 것보다 더 나은 결과를 만들어 낼 수 있고, 동시에 특징 단 계 퓨전을 사용할 때 최적의 성능을 발휘할 수 있음을 알 수 있다. 두 번째 실험으로는 인코더에서 심층 특징 벡터를 구성하는 데 사용되는 트랜스포머 모듈의 수에 따른 성능 변 화를 알아보고자 모듈의 수를 6개에서부터 9, 12, 18, 24로 차례로 늘려가며 총 5개 경우에 대해 학습하고 일련 의 성능 변화를 관찰하A_07_도록 하였다. 이때, 인코더의 나머지 구성에는 앞서 첫 번째 실험에서 가장 좋은 성능을 보였던 특징 단계 퓨전을 적용한 구 성을 사용하였으며, 학습을 위한 손실함수는 이전과 동일하게 \"Focal loss\"함수를 사용하였다. 한편, 표 2는 심층 특징 벡터 형성을 위한 인코더 세부 구성 실험 결과를 나타낸 표이다. 표 2에 기재된 바와 같이, 모듈의 수가 늘어날수록 점차 성능이 상승하여 9개의 모듈을 사용할 때 MIoU를 기준 으로 가장 높은 0.2240의 성능을 보였으며, 그보다 모듈의 수가 더 늘어나면 점차 그 성능이 하락하는 모습을 보였다. 특히, 9개의 모듈을 사용한 인코더가 12개의 모듈을 사용하는 기존 방법의 구성보다 신경망이 얕음에A_07_도 불 구하고 약 1%가량 더 나은 성능을 보인 것은 센서 퓨전을 사용하면 표현력 높은 심층 특징을 비교적 빨리 만들 어 낼 수 있다는 것을 의미한다고 볼 수 있다. 세 번째 실험으로는 시맨틱 세그멘테이션 분야에서 널리 사용되는 두 손실함수인 수학식 1의 \"Focal loss\"함수 와 수학식 2의 \"Dice loss\"함수를 각각 적용하여 신경망을 학습하고 그 결과를 평가함으로써 손실함수의 변화가 성능에 미치는 영향을 관찰하고자 하였다. 또한, 하기의 수학식 7과 같이, 두 손실함수 중 하나를 주(Main) 손실함수로 함과 동시에 나머지 하나를 규제항 (regularizer)으로 추가하여 두 손실함수가 가진 각자의 장점인 픽셀 단위의 정확A_07_도 증진과 전체적인 구조 적 유사A_07_도를 동시에 증진하고자 하는 실험A_07_도 함께 수행하였다. [수학식 7]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 188, "content": "한편, 표 3은 학습에 사용된 손실함수의 구성에 따른 실험 결과를 나타낸 표이다. [표 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 189, "content": "이 때, 신경망은 앞서 두 번째 실험에서 가장 우수한 성능을 보인 본 발명의 일 실시예에 따른 시맨틱 세그멘테 이션 방법으로 만들었으며, 실험 결과 하기의 표 3에서 기재된 바와 같이 \"Dice loss\"함수를 규제항으로 하고 \"Focal loss\"함수를 사용할 때 가장 좋은 결과를 보였다. 이는 \"Focal loss\"함수를 중심으로 학습하여 픽셀 별 정확A_07_도 증진에 학습의 초점을 맞추되, \"Dice loss\"함 수를 규제항으로 함께 사용해 전체적인 유사A_07_도의 증진에 방해가 되는 부분에는 강한 규제 값을, 반대로 전 체적 유사A_07_도 증진에 A_07_도움이 되는 부분에는 약한 규제 값을 부여함으로써 모델이 보다 고르게 학습될 수 있A_07_도록 한 결과로 보인다. A_07_도 10은 본 발명의 일 실시예에 따른 2차원 시맨틱 세그멘테이션의 3차원 해석 결과를 나타낸 A_07_도면이 다. A_07_도 10을 참조하면, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 통해 2차원 세그멘테이션 MIoU 기준 31.94%의 성능을 거둘 수 있었으며, 이를 토대로 A_07_도 10에 A_07_도시된 바와 같이, 3차원 시맨틱 세그멘테이션 정보를 재해석해 낼 수 있다.[표 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 190, "content": "한편, 표 4는 본 발명의 실시예 및 비교예에 따른 시맨틱 세그멘테이션 결과를 나타낸 표이다. 이에 따라, 기존 방법으로는 구해내는 것이 불가능했던 3차원 좌표를 기준으로 하는 세그멘테이션 MIoU를 표 4 와 같이 구해낼 수 있으며, 그 수치는 17.12%를 기록하였다. 이상과 같이, 본 명세서와 A_07_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_07_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_07_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_07_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 사전 학습부 220 : 데이터 전처리부 225 : 추론부 230 : 3차원 해석부 235 : 저장부 A_07_청구범위 A_07_청구항 1 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계; 상기 학습 데이터 생성 장치가, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligenc e)을 기초로 상기 점군 데이터 및 상기 이미지로부터 2차원 시맨틱 세그먼테이션 특징맵을 생성하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 생성된 특징맵으로부터 3차원 정보를 해석하는 단계; 를 포함하는 것을 특 징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 2 제1 항에 있어서, 상기 생성하는 단계 이전에 상기 인공 지능을 사전 학습시키는 단계; 를 더 포함하는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해 석 방법. A_07_청구항 3 제2 항에 있어서, 상기 사전 학습시키는 단계는 사전 저장된 데이터 셋에 포함된 라이다로부터 획득된 점군 데이터, 상기 점군 데이터와 동시에 카메라를 통해 촬영된 이미지, 상기 라이다 및 상기 카메라 사이의 캘리브레이션 정보 및 상기 점군 데이터의 3차원 포인트 단 위로 클래스 라벨이 명시되어 있는 정답 데이터를 기초로 상기 인공 지능을 사전 학습시키는 것을 특징으로 하 는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 4 제3 항에 있어서, 상기 사전 학습시키는 단계는 상기 점군 데이터 및 상기 정답 데이터를 상기 이미지의 2차원 평면상 좌표에 사영시키되, 상기 이미지의 사영 될 픽셀과 맨해튼 거리(Manhattan distance)가 사전 설정된 값 이내인 픽셀들을 동일한 클래스를 갖A_07_도록 하는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 5 제4 항에 있어서, 상기 사전 학습시키는 단계는 복수의 손실함수 중 적어A_07_도 하나를 통해 상기 인공 지능을 사전 학습시키되, 상기 복수의 손실함수 중 하 나를 구제항(regularizer)로 추가하는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 6 제5 항에 있어서, 상기 사전 학습시키는 단계는 하기의 수학식 1로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시키는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 191, "content": "(여기서, ppred는 크로스 엔트로피를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) A_07_청구항 7 제5 항에 있어서, 상기 사전 학습시키는 단계는 하기의 수학식 2로 표현되는 손실함수를 통해 상기 인공 지능을 사전 학습시키는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 192, "content": "(여기서, ppred는 Dice 계수를 기반으로 하는 추론 확률, ptrue는 정답을 의미한다.) A_07_청구항 8 제5 항에 있어서, 상기 사전 학습시키는 단계는 배경 클래스에 해당하는 픽셀을 손실함수의 종류와 관계없이 손실 값의 계산에서 제외시키는 것을 특징으로 하 는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 9 제1 항에 있어서, 상기 해석하는 단계는 상기 점군 데이터의 3차원 좌표 및 상기 점군 데이터를 사영한 이미지의 2차원 평면상 좌표 쌍을 준비하는 것을 특징으로 하는, 시맨틱 세그멘테이션의 3차원 해석 방법. A_07_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬 영된 이미지(image)를 수신하는 단계; 상기 프로세서, 사전 기계 학습(machine learning)된 인공 지능(AI, Artificial Intelligence)을 기초로 상기 점군 데이터 및 상기 이미지로부터 2차원 시맨틱 세그먼테이션 특징맵을 생성하는 단계; 및 상기 프로세서, 상기 생성된 특징맵으로부터 3차원 정보를 해석하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 193, "content": "A_07_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 194, "content": "A_07_요약 본 발명은 시맨틱 세그멘테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세 서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세 서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지 (image)를 수신하는 단계, 상기 프로세서가, 상기 수신한 점군 데이터 및 상기 이미지를 전처리하는 단계 및 상 기 프로세서가, 상기 전처리 된 점군 데이터 및 이미지 각각으로부터 특징을 독립적으로 추출하고, 상기 추출된 특징을 융합하여 시맨틱 세그멘테이션을 추론하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그 램이 될 수 있다. A_07_대표A_07_도 A_07_도 8 A_07_도면 A_07_도 1 A_07_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 195, "content": "A_07_도 3 A_07_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 196, "content": "A_07_도 5 A_07_도 6 A_07_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 197, "content": "A_07_도 8 A_07_도 9"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 198, "content": "A_07_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 199, "content": "A_08_발명의 설명 A_08_발명의 명칭 멀티 스케일 특징들의 상관관계 및 지역적 연관성을 활용한 시맨틱 세그멘테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for semantic segmentation using correlations and regional associations of multi-scale features, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 200, "content": "A_08_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 자 율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행하기 위한, 멀티 스케일 특징들의 상관관계 및 지역적 연관성을 활용한 시맨 틱 세그멘테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. A_08_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_08_도 학습(supervised learning), 비지A_08_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 어노테이션(annotation)을 수행하고, 메타데이터(metadat a)를 입력하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 A_08_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정A_08_도와 운전차가 차량을 제어하는 정A_08_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회가 분류한 6단계에 따르면, 레벨 0단계는 비자동화, 레벨 1단계는 운전자 보조, 레벨 2단계는 부분 자동 화, 레벨 3단계는 조건부 자동화, 레벨 4단계는 고A_08_도 자동화, 그리고 레벨 5단계는 완전 자동화 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 최근에는 자율 주행 기능의 오인식으로 인한 차량의 사고들로 자율 주행의 안정성 문제가 대두됨에 따라, 자율 주행을 위한 인지 기술에 라이다(lidar) 같은 3차원적 특성을 가진 센서들을 접목하는 센서 퓨전(sensor fusion) 기술과 주행환경을 3차원으로 인식할 수 있는 3차원 시맨틱 세그멘테이션(semantic segmentation) 기술 등이 포함되어야 한다는 필요성이 제기되고 있다. A_08_선행기술문헌 A_08_특허문헌 대한민국 등록특허공보 제10-2073873호, ‘시맨틱 세그멘테이션 방법 및 그 장치’, (2020.01.30. 등록)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 201, "content": "A_08_발명의 내용 A_08_해결하고자 하는 과제 본 발명의 일 목적은 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행하기 위한, 멀티 스케일 특징들의 상관관계 및 지역적 연관성을 활용한 시맨틱 세그멘테이션 방법을 제공하는 것이다. 본 발명의 또 다른 목적은 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행하기 위한, 멀티 스케일 특징들의 상관관계 및 지역적 연관성을 활용한 시맨틱 세그멘테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제 공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 202, "content": "A_08_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행하기 위한, 시맨틱 세그멘테이션 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 점군 데이터 및 상기 이미지로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성하는 단계를 포함할 수 있다. 구체적으로, 상기 심층 특징을 추출하는 단계는 복수의 컨벌루션 블록들을 통해 상기 점군 데이터 및 상기 이미 지 각각으로부터 입력 대비 사전 설정된 크기로 축소된 특징맵을 추출하는 것을 특징으로 한다. 상기 심층 특징을 추출하는 단계는 점군 데이터 및 이미지 각각에 대해 1/2, 1/4의 크기로 단계적 축소된 특징 맵을 추출하고, 이를 센서 퓨전으로 융합한 심층 특징맵을 1/8, 1/16 크기까지 재차 단계적으로 축소한 특징맵 을 추출하는 것을 특징으로 한다. 상기 심층 특징을 추출하는 단계는 상기 점군 데이터 및 상기 이미지 각각으로부터 추출된 특징맵을 정합 (concatenate)하여 센서 퓨전을 수행하는 것을 특징으로 한다. 상기 복수의 컨벌루션 블록들은 상기 점군 데이터 및 상기 이미지에 포함된 특징 간 상관관계를 학습하는 분할- 어텐션 모듈 및 다중 크기 특징을 추출하는 다중 크기(multi scale) 어텐션 모듈로 구성되어, 상기 점군 데이터 및 상기 이미지에 포함된 요소들을 특징으로 추출함과 동시에, 추출된 특징 사이의 상관관계를 학습하는 것을 특징으로 한다. 상기 트랜스포머 블록은 상기 센서 퓨전 수행된 융합 특징에 컨벌루션 기반 중첩 패치 추출 방법과 자가-어텐션 방법을 적용해 패치간 지역적 연관성을 학습하는 것을 특징으로 한다. 상기 벡터를 생성하는 단계는 상기 심층 특징맵을 컨벌루션, 확장(upsampling) 및 접합 연산의 반복을 통해 (H, W, C)로 표현되는 추론맵으로 복원하는 것을 특징으로 한다. 상기 벡터를 생성하는 단계는 상기 (H, W, C)로 표현되는 추론맵을 (H*W, C)로 재구성하여 함께 출력하는 것을 특징으로 한다. 상기 벡터를 생성하는 단계는 상기 재구성된 추론맵을 손실함수 및 평가함수에 적용하는 것을 특징으로 한다. 상기 손실함수는 하기의 수학식 1과 같과 같이, 정답값 및 추론값 사이의 복수의 손실값을 합산하여 계산하는 것을 특징으로 한다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 203, "content": "상기 손실함수는 하기의 수학식 2, 수학식 3 및 수학식 4를 통해 계산된 손실값을 합산하여 계산하는 것을 특징 으로 한다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 204, "content": "[수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 205, "content": "[수학식 4] (여기서, Ypred는 세그멘테이션 추론값, Ytrue는 정답값, ε는 smooth(1e-6)을 의미한다.) 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 시맨틱 세그멘테이션 방법을 실행하기 위하여 기 록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory), 송수신기(transceiver) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 프로세서가, 상기 점군 데이터 및 상기 이미지로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하는 단계 및 상기 프로세서가, 상기 추출한 심층 특징으로부터 세그멘테 이션 추론 벡터를 생성하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_08_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 206, "content": "A_08_발명의 효과 본 발명의 실시 예들에 따르면, 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 207, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 208, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 209, "content": "A_08_도면의 간단한 설명 A_08_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_08_도이다. A_08_도 2는 본 발명의 일 실시예에 따른 학습 데이터 수집 장치의 구성을 설명하기 위한 예시A_08_도이다. A_08_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_08_도이다. A_08_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_08_도이다. A_08_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 나타낸 순서A_08_도이다 A_08_도 6 내지 A_08_도 8은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 설명하기 위한 예시A_08_ 도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 210, "content": "A_08_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_08_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_08_도하게 포괄적인 의미로 해석되거나, 과A_08_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_08_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수A_08_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_08_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_08_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_08_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_08_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_08_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_08_도면은 본 발명의 사상을 쉽게 이해할 수 있A_08_도록 하기 위한 것일 뿐, 첨부된 A_08_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_08_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_08_ 도 확장되는 것으로 해석되어야 한다. 한편, 최근에는 라이다(lidar)와 같은 3차원 센서 및 카메라와 같은 2차원 센서로부터 취득한 멀티 모달(multi modal) 데이터를 융합하는 센서 퓨전(sensor fusion)과, 객체 인식의 단위를 데이터의 구성 단위까지 확장하는 시맨틱 세그멘테이션(semantic segmentation)이 활발히 연구되고 있다. 센서 퓨전은 이종의 센서로부터 취득한 멀티 모달 데이터를 상호 융합하는 기술이다. 센서 퓨전은 딥러닝 분야 에서 다수의 원천(source)으로부터 형성된 데이터 또는 정보들이 융합되는 시점에 따라 세 가지 종류로 구분한 다. 데이터 단계 퓨전(data level or early fusion)은 원천으로부터 취득된 데이터 자체를 융합하여 새로운 표현형 을 갖는 융합 데이터를 만든다. 이 융합데이터는 신경망에 전달되어 특징맵(feature map)을 추출, 결과 추론 맵 (prediction map)을 만드는데 활용된다. 이 방법은 신경망에 입력으로 수용되는 데이터의 종류가 적어 신경망의 전체적인 구조가 비교적 단순하게 구성된다는 장점이 있다. 이와 달리 특징 단계 퓨전(deep feature level or mid-level fusion)은 원천으로부터 취득한 멀티 모달 데이터 를 신경망에 입력으로 각각 전달한다. 병렬 구조의 신경망을 통해 각각의 입력으로부터 추출되는 멀티 모달 특 징들은 신경망의 내부에서 하나로 융합되어 심층 융합 특징을 구성하며, 이는 결과 추론 맵을 만드는데 활용된 다. 이 방법은 신경망이 결과를 추론함에 있어 멀티 모달 데이터로부터 추출한 각각의 특징부터 그들의 융합 특 징까지 다양하게 활용 가능한 장점이 있다. 추론 단계 퓨전(score level or late fusion) 또한 멀티 모달 데이터를 신경망의 입력으로 각각 전달하나, 특징 단계 퓨전과 달리 각각의 원천마다 독립된 신경망을 사용한다. 각 신경망이 독립적으로 만들어낸 결과 추론 맵 들은 합산, 평균 등의 단순 연산이나 별A_08_도로 학습된 분류기를 활용함으로써 최종 추론 맵을 A_08_도출하는 데 활용된다. 이 방법은 추론 결과가 융합에 사용된 센서들 사이의 특성 차나 상호 간섭에 의한 영향에 강인하 다는 장점이 있다. 시맨틱 세그멘테이션은 자율 주행 차량이 주행 환경을 인지하는데 필요한 핵심적인 요소 기술 중의 하나로서 카 메라로부터 취득한 2차원 RGB 영상의 객체들을 픽셀 단위로 분류(dense prediction)할 수 있다. 특히, 최근에는 기존의 컨볼루션 계층을 중심으로 구성된 심층 신경망을 사용하는 방식에서 더 나아가 셀프 어 텐션을 기반으로 특징들 사이의 상대적 중요A_08_도를 알아내 보다 표현력이 높은 심층 특징들을 추출해낼 수 있는 트랜스포머 모듈을 추가하여 2차원 시맨틱 세그멘테이션의 성능을 증진하는 연구들이 늘고 있다. 트랜스포머 모듈을 활용하면 신경망에 컨볼루션 계층을 비교적 적게 사용하더라A_08_도 표현력이 좋은 심층 특 징들을 추출할 수 있으며, 이 심층 특징들을 바탕으로 기존의 방법들보다 상대적으로 좋은 성능을 보였다. 그러나 앞서 언급한 바와 같이 이들 2차원 시맨틱 세그멘테이션 방법이 만들어내는 평면 형태의 추론 결과만으 로는 차량의 실제 주행 환경인 3차원에서의 객체의 구조나 상대적 거리 등을 파악하기 어려우며, 3차원 정보를 추론하기 위해 여러 대의 카메라를 사용할 경우에는 계산량이 과A_08_도하게 늘어난다는 문제가 있다. 또한, 카메라와 라이다의 센서 퓨전을 활용하는 기존의 시맨틱 세그멘테이션 연구들은 대부분 3차원의 라이다 점군 데이터와 2차원의 카메라 영상 데이터를 혼합하기 위해 점군 데이터를 2차원의 평면 데이터로 변환한다. 이를 이용해 만들어진 평면 형태의 추론 결과는 라이다의 장점인 3차원의 거리 정보가 손실된 상태이므로 주행 환경에 대한 3차원적 정보들을 제공하기 어렵다.이러한 한계를 극복하고자, 본 발명은 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이 다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행할 수 있는 다양한 수단들을 제안하 고자 한다. A_08_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_08_도이다. A_08_도 1에 A_08_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 수집 장 치, 학습 데이터 생성 장치 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar) 및 카메라(camera)로부터 실 시간으로 데이터를 수집하는 장치이다. 하지만, 이에 한정된 것은 아니고, 학습 데이터 수집 장치는 레이 더(radar) 및 초음파 센서(ultrasonic sensor)를 포함할 수A_08_도 있다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비 되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라A_08_도 복수 개로 구비될 수 있다. 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하 는 센서들의 종류에 대해서는 추후 A_08_도 2를 참조하여 보다 구체적으로 설명하기로 한다. 다음 구성으로, 학습 데이터 생성 장치는 복수의 학습 데이터 수집 장치 각각으로부터 이동통신 (mobile communication)을 이용하여 각각의 학습 데이터 수집 장치에 의해 실시간으로 수집된 데이터를 수 신하고, 수신된 데이터에 대하여 어노테이션을 수행할 수 있다. 이러한, 학습 데이터 생성 장치는 학습 데이터 생성 장치는 인공지능 학습 장치로부터 인공지능 (AI) 학습용 데이터의 요청이 수신되기 이전에, 선제적으로 인공지능(AI) 학습용 데이터를 생성할 수 있는 빅데 이터(big data)를 구축해 놓을 수 있다. 특징적으로, 학습 데이터 생성 장치는 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 점군 데이터 및 상기 이미지로부터 다중 크기의 특징 들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하고, 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치 및 인공지능 학습 장치 와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_08_ 도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 한편, 학습 데이터 생성 장치에 관한 구체적인 설명은 이하, A_08_도 3 및 A_08_도 4를 참조하여 후술하 A_08_도록 한다. 다음 구성으로, 인공지능 학습 장치는 인공지능(AI)을 개발하는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 인공지능(AI)이 개발 목적을 달성하기 위하여 인공지능(AI) 학습용 데 이터가 만족해야 하는 요구 사항을 포함하는 요구 값을 학습 데이터 생성 장치에 전송할 수 있다. 인공지 능 학습 장치는 학습 데이터 생성 장치로부터 인공지능(AI) 학습용 데이터를 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 인공지능(AI) 학습용 데이터를 이용하여, 개발하고자 하는 인공지능 (AI)을 기계 학습할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_08_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치 및 인공지능 학습 장 치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네 트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것A_08_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. A_08_도 2는 본 발명의 일 실시예에 따른 센서들을 설명하기 위한 예시A_08_도이다. A_08_도 2에 A_08_도시된 바와 같이, 본 발명의 일 실시예에 따른 학습 데이터 수집 장치는 차량에 고 정 설치된 레이더, 라이다, 카메라 및 초음파 센서 중 하나 이상을 제어하여, 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집할 수 있다. 여기서, 차량은 인공지능(AI)을 기계 학습시키기 위한 기초 데이터를 수집하기 위한 레이더, 라이다 , 카메라 및 초음파 센서가 설치된 차량으로, 인공지능(AI)에 의해 자율주행을 수행하는 차량과는 서로 구별될 수 있다. 레이더는 차량에 고정 설치되어 차량의 주행 방향을 향하여 전자기파(electromagnetic wave)를 발 사하고, 차량의 전방에 위치하는 객체(object)에 의해 반사되어 돌아온 전자기파를 감지하여, 차량이 전방에 대한 영상에 해당하는 감지 데이터를 생성할 수 있다. 다르게 말하면, 감지 데이터는 차량에 고정 설치된 레이더에 의해 차량의 주행 방향을 향하여 발사된 전자기파를 반사시킨 점들(points)에 대한 정보이다. 따라서, 감지 데이터에 포함된 점들의 좌표들은 차량 의 전방에 위치하는 객체의 위치 및 형상에 대응하는 값을 가질 수 있다. 이러한, 감지 데이터는 2차원 정보가 될 수 있으나, 이에 한정되지 않고 3차원 정보가 될 수A_08_도 있다. 라이다는 차량에 고정 설치되어 차량의 주위로 레이저 펄스(laser pulse)를 방사하고, 차량의 주위에 위치하는 객체에 의해 반사되어 돌아온 빛을 감지하여, 차량의 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 다르게 말하면, 3D 점군 데이터는 차량에 고정 설치된 라이다에 의해 차량의 주위로 방사된 레이저 펄 스를 반사시킨 점들에 대한 3차원 정보이다. 따라서, 3D 점군 데이터에 포함된 점들의 좌표들은 차량의 주 위에 위치하는 객체의 위치 및 형성에 대응하는 값을 가질 수 있다. 카메라는 차량에 고정 설치되어 차량의 주위에 대한 2차원 이미지를 촬영할 수 있다. 이와 같은, 카메라는 서로 다른 방향을 촬영할 수 있A_08_도록 복수 개가 지표면과 수평 또는 수평 방향으로 이격되게 설치될 수 있다. 예를 들어, A_08_도 2는 서로 다른 6개의 방향을 촬영할 수 있는 6개의 카메라가 고정 설 치된 차량의 예시를 A_08_도시하고 있으나, 차량에 설치될 수 있는 카메라가 다양한 개수로 구성될"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 211, "content": "수 있음은 본 발명이 속한 기술분야의 통상의 지식을 가진 자에게 자명할 것이다. 다르게 말하면, 2D 이미지는 차량에 고정 설치된 카메라에 의해 촬영된 이미지이다. 따라서, 2D 이미지 에는 카메라가 향하는 방향에 위치하는 객체의 색상 정보가 포함될 수 있다. 초음파 센서는 차량에 고정 설치되어 차량의 주위로 초음파(ultrasonic)를 발사하고, 차량과 인접하게 위치하는 객체에 의해 반사되어 돌아온 음파를 감지하여, 차량에 설치된 초음파 센서와 객체 사이의 거리에 해당하는 거리 정보를 생성할 수 있다. 일반적으로, 초음파 센서는 복수 개로 구성되어, 객 체와 접촉하기 쉬운 차량의 전방, 후방, 전측방 및 후측방에 고정 설치될 수 있다. 다르게 말하면, 거리 정보는 차량에 고정 설치된 초음파 센서에 의해 감지된 객체로부터의 거리에 관한 정보이다.이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. A_08_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성A_08_도이다. A_08_도 3을 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 인코딩부, 디코딩부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지 및 점군 데이터를 학습 데이터 수집 장치 로부터 수신할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 동기화 하기 위한 캘리브레이 션 행렬을 함께 수신할 수 있다. 또한, 통신부는 시맨틱 세그멘테이션의 3차원 해석 결과를 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 받 거나, 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 시맨틱 세그멘테이션 특징맵을 생성하거나, 생성된 시맨틱 세그멘테 이션 특징맵을 3차원 해석하기 위한 다양한 설정 값들을 입력받고, 생성된 결과 값들을 출력할 수 있다. 다음 구성으로, 인코딩부는 점군 데이터 및 이미지로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출할 수 있다. 구체적으로, 인코딩부는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 여기서, 캘리브레이션 행렬은 하기의 수학식 5으로 표현될 수 있다. [수학식 5]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 212, "content": "(여기서, u 및 v는 이미지 내 픽셀들의 2차원 좌표, x, y 및 z는 점군 데이터의 3차원 좌표, fu 및 fv는 픽셀 단위의 초점 거리, u0 및 v0은 이미지 평면에서 점군 데이터가 위치하는 x 및 y 좌표를 의미한다.) 인코딩부는 복수의 컨벌루션 블록들을 통해 사영된 점군 데이터 및 이미지 각각으로부터 입력 대비 사전 설정된 크기로 축소된 특징맵을 추출할 수 있다. 즉, 인코딩부는 사영된 점군 데이터 및 이미지 각각을 1/2, 1/4 및 1/8의 크기로 단계적으로 축소할 수 있 다. 여기서, 복수의 컨벌루션 블록들은 사영된 점군 데이터 및 이미지에 포함된 특징 간 상관관계를 학습하는 분할- 어텐션 모듈 및 다중 크기 특징을 추출하는 다중 크기(multi-scale) 어텐션 모듈로 구성될 수 있다. 인코딩부는 분할-어텐션 모듈 및 다중 크기 어텐션 모듈을 통해 점군 데이터 및 이미지에 포함된 요소들을 특징으로 추출함과 동시에, 추출된 특징 사이의 상관관계를 학습할 수 있다. 다음으로, 인코딩부는 사영된 점군 데이터 및 이미지 각각으로부터 추출된 특징맵을 접합(concatenate)하 여 센서 퓨전을 수행할 수 있다. 인코딩부는 센서 퓨전 수행된 데이터를 사전 기계 학습된 인공 지능을 통해 지역적 연관 정보를 포함하는 1/16 크기의 심층 특징맵을 추출할 수 있다.여기서, 인공 지능은 컨벌루션 기반 중첩 패치 추출 방법과 패치간 지역적 연관성을 학습할 수 있다. 다음 구성으로, 디코딩부는 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성할 수 있다. 구체적으로, 디코딩부는 심층 특징맵을 컨벌루션, 확장(upsampling) 및 접합 연산의 반복을 통해 (H, W, C)로 표현되는 추론맵으로 복원할 수 있다. 즉, 디코딩부는 심층 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배수로 확 장하되, 접합 계층을 통해 복수의 업 샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하는 특징맵 과 접합할 수 있다. 여기서, 디코딩부는 복수의 업 샘플링 계층 각각에서 업 샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루 션 계층에 통과시킬 수 있다. 그리고, 디코딩부는 (H, W, C)로 표현되는 추론맵을 (H*W, C)로 재구성하여 함께 출력할 수 있다. 또한, 디코딩부는 재구성된 추론맵을 손실함수 및 평가함수에 적용할 수 있다. 여기서, 손실함수는 하기의 수학식 1과 같이, 정답값 및 추론값 사이의 복수의 손실값을 합산하여 계산할 수 있 다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 213, "content": "즉, 손실함수는 하기의 수학식 2, 수학식 3 및 수학식 4를 통해 계산된 손실값을 합산하여 계산할 수 있다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 214, "content": "[수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 215, "content": "[수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 216, "content": "(여기서, Ypred는 세그멘테이션 추론값, Ytrue는 정답값, ε는 smooth(1e-6)을 의미한다.) 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터를 설계하는데 필요한 데이터를 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. A_08_도 4는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성A_08_도이다. A_08_도 4를 참조하면, 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수 신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지 (Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 수집 장치 및 인공지능 학습 장치와 데이터를 송 수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이터를 입력 받고, 생성된 결과 값을 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지 와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 시맨틱 세그멘테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지 는 시맨틱 세그멘테이션 방법 및 3차원 해석 방법을 수행하기 위한 프로그램을 저장하는 데이터베이스 를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하고, 프로세서가 점군 데이터 및 이미지로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하고, 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성할 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_08_도 4에 A_08_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_08_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_08_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_08_도록 구성될 수 있으며, 그 역A_08_도 마찬가지이다. A_08_도 5는 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 설명하기 위한 순서A_08_도이다. A_08_도 5를 참조하면, 먼저 S100 단계에서 학습 데이터 생성 장치는 학습 데이터 수집 장치로부터 점군 데이터 및 이미지를 수신할 수 있다. 이때, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 캘리브레이션 형렬 을 추가로 수신할 수 있다. 다음으로, S200 단계에서 학습 데이터 생성 장치는 점군 데이터 및 이미지로부터 다중 크기의 특징들을 추출하 고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출할 수 있다. 구체적으로, 학습 데이터 생성 장치는 점군 데이터 및 이미지와 함께 수신한 캘리브레이션 행렬을 기초로 점군 데이터를 이미지와 동일한 크기를 갖는 2차원 평면 상의 좌표에 사영할 수 있다. 학습 데이터 생성 장치는 복수의 컨벌루션 블록들을 통해 사영된 점군 데이터 및 이미지 각각으로부터 입력 대 비 사전 설정된 크기로 축소된 특징맵을 추출할 수 있다. 즉, 학습 데이터 생성 장치는 사영된 점군 데이터 및 이미지 각각을 1/2, 1/4 및 1/8의 크기로 단계적으로 축소 할 수 있다. 여기서, 복수의 컨벌루션 블록들은 사영된 점군 데이터 및 이미지에 포함된 특징 간 상관관계를 학습하는 분할- 어텐션 모듈 및 다중 크기 특징을 추출하는 다중 크기(multi-scale) 어텐션 모듈로 구성될 수 있다. 학습 데이터 생성 장치는 분할-어텐션 모듈 및 다중 크기 어텐션 모듈을 통해 점군 데이터 및 이미지에 포함된 요소들을 특징으로 추출함과 동시에, 추출된 특징 사이의 상관관계를 학습할 수 있다. 다음으로, 학습 데이터 생성 장치는 사영된 점군 데이터 및 이미지 각각으로부터 추출된 특징맵을 접합 (concatenate)하여 센서 퓨전을 수행할 수 있다. 다음으로, 학습 데이터 생성 장치는 센서 퓨전 수행된 데이터를 사전 기계 학습된 인공 지능을 통해 지역적 연 관 정보를 포함하는 1/16 크기의 심층 특징맵을 추출할 수 있다. 여기서, 인공 지능은 컨벌루션 기반 중첩 패치 추출 방법과 패치간 지역적 연관성을 학습할 수 있다. 그리고, S300 단계에서 학습 데이터 생성 장치는 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성할 수 있다. 구체적으로, 학습 데이터 생성 장치는 심층 특징맵을 컨벌루션, 확장(upsampling) 및 접합 연산의 반복을 통해 (H, W, C)로 표현되는 추론맵으로 복원할 수 있다. 즉, 학습 데이터 생성 장치는 심층 특징맵을 복수의 업 샘플링(up sampling) 계층을 통해 면적을 사전 설정된 배수로 확장하되, 접합 계층을 통해 복수의 업 샘플링 계층 각각에서의 특징맵을 인코더의 각 단계에서 대응하 는 특징맵과 접합할 수 있다. 여기서, 학습 데이터 생성 장치는 복수의 업 샘플링 계층 각각에서 업 샘플링 된 특징맵을 3*3 커널을 사용하는 컨벌루션 계층에 통과시킬 수 있다. 그리고, 학습 데이터 생성 장치는 (H, W, C)로 표현되는 추론맵을 (H*W, C)로 재구성하여 함께 출력할 수 있다. 또한, 학습 데이터 생성 장치는 재구성된 추론맵을 손실함수 및 평가함수에 적용할 수 있다. A_08_도 6 내지 A_08_도 8은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법을 설명하기 위한 예시A_08_ 도이다. A_08_도 6 내지 A_08_도 8을 참조하면, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인코더와 디코더 로 구성되어 있다. 먼저, 인코더는 카메라로부터 취득한 2차원 RGB 영상과 라이다로부터 취득한 동일 영역의 3차원 포인트 클라우 드를 2차원으로 사영한 2차원 사영 영상을 입력 데이터로 수용하는 데서 시작된다. 인코더는 그를 구성하는 컨 볼루션 블록들을 이용하여 수용된 데이터로부터 입력 대비 1/2 및 1/4의 크기로 축소된 두 특징 맵 들을 추출하 며, 특징 단계의 센서 퓨전을 위해 두 번째 특징 맵을 접합(Concatenate)하여 세번째 컨볼루션 블록으로 전달한 다. 각각의 컨볼루션 블록은 A_08_도 7에 A_08_도시된 상세 구조와 같이 특징 간 상관관계를 학습하는 \"ResNeSt\"의 분할-어텐션 모듈과 다중-크기 특징을 추출하는 \"SegNeXt\"의 다중 크기(Multi-scale) 어텐션 모듈로 이루어져 입력에 존재하는 다양한 모양의 요소들을 특징으로 추출함과 동시에 그들 사이의 복잡한 상관관계를 신경망이 학습할 수 있A_08_도록 한다. 일련의 과정을 거쳐 추출되는 1/8 크기의 특징 맵은 인코더의 마지막 블록인 A_08_도 8의 \"SegFormer\"의 자가 (Self) 어텐션 모듈에 전달된다. 이 모듈은 컨볼루션 기반 중첩 패치 추출 방법과 패치 간 지역적 연관성을 학습하는 Mix-FFN을 사용하므로 신경 망은 기존의 트랜스포머와 같이 인위적인 위치 임베딩을 사용하지 않아A_08_도 특징들 사이의 지역적 연관 정보 를 포함하는 1/16 크기의 심층 특징 맵을 추출할 수 있다. 디코더는 인코더에서 추출된 네 가지 크기의 특징 맵들을 토대로 컨볼루션, 확장(upsampling), 접합 연산의 반 복을 통해 2차원의 시맨틱 세그멘테이션 추론 맵을 복원해 낸다. 또한 3차원 공간에 대한 추론결과를 신경망으로부터 직접 만들기 위해 (H, W, C) 모양의 추론 맵을 (H*W, C)모 양으로 재구성하여 함께 출력하여 손실함수와 평가함수에서 학습과 평가에 활용하A_08_도록 한다. 신경망의 학습을 위한 손실함수는 하기의 수학식 1과 같이 정답값과 추론 값 사이의 3가지 손실 값을 구해 합산 하는 방식으로 계산한다. [수학식 1]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 217, "content": "먼저, 첫 번째 손실 값은 2차원 시맨틱 세그멘테이션에 대해 픽셀 단위의 정확한 분류에 초점을 두고 가중치 기 반의 크로스 엔트로피를 사용하는 하기의 수학시 2의 \"Focal-loss\"로 계산한다. [수학식 2]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 218, "content": "두 번째 손실 값은 2차원 시맨틱 세그멘테이션의 정답과 추론 맵 사이의 유사A_08_도를 사용하는 하기의 수학식 3의 \"Dice-loss\"를 사용하되 추론 값에서 정답이 아닌 채널의 값을 0으로 만들어 사용함으로써 신경망이 정답을 맞히는데 더초점을 둘 수 있A_08_도록 하는 규제 항의 역할로 사용한다. [수학식 3]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 219, "content": "이들과 달리 세 번째 손실 값은 하기의 수학식 4에서 보이는 바와 같이 크로스 엔트로피를 사용하되 H*W개의 추 론 픽셀 중 정답으로 주어진 K개의 3차원 좌표의 사영 지점 (u, v)에 해당하는 픽셀만을 대상으로 함으로써 신 경망이 3차원 좌표의 정답을 학습에 직접 반영할 수 있다. [수학식 4]"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 220, "content": "(여기서, Ypred는 세그멘테이션 추론값, Ytrue는 정답값, ε는 smooth(1e-6)을 의미한다.) 이하, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법에 대한 성능에 대하여 설명하A_08_도록 한다. 실시예 본 발명의 일 실시예에 시맨틱 세그멘테이션 방법의 학습과 평가에 사용된 데이터 셋은 \"semantic KITTI\"로 자 율 주행 분야의 3차원 객체 인식이나 시맨틱 세그멘테이션 등의 연구에 널리 활용된다. 데이터셋은 총 21개 시퀀스로 구성되어 있으며 RGB 프레임 이미지, 포인트 클라우드, 카메라와 라이다 사이의 캘리브레이션 정보가 포함되어 있다. 그러나, 3차원 포인트 단위의 클래스 라벨은 학습용 시퀀스인 0~10번 시퀀스에만 포함되어 있으므로 본 실험에 서는 검증용 시퀀스인 8번 시퀀스(4,071건)를 평가 데이터로 사용하고 나머지 시퀀스(19,130건)를 학습 데이터 로 활용한다. 또한, RGB 이미지와 포인트 클라우드의 사영 이미지는 96x320의 크기로 조정하여 사용하며 데이터셋이 2차원 시 맨틱 세그멘테이션을 위한 라벨 데이터를 포함하지 않는 바, 이 역시A_08_도 3차원 포인트 단위의 라벨을 2차원 에 사영시켜 만든 라벨 이미지를 만들어 사용한다. 다만 이를 픽셀 단위로 사영할 경우 대부분의 영역이 0의 값을 갖는 희소 데이터가 되어 학습에 큰 악영향을 미 치므로 사영 되는 픽셀 주변의 가로 2, 세로 2의 크기로 동일한 라벨을 채우A_08_도록 하여 라벨 이미지의 희소 성을 최대한 보완한다. 또한, 3차원 추론 결과에 대한 라벨은 포인트 클라우드의 사영 좌표와 그에 따른 라벨값을 포함한다. 평가 방법으로는 시맨틱 세그멘테이션 분야에서 널리 쓰이는 mIoU(mean Intersection over Union)을 사용하였으 며 3차원 결과를 평가할 때 정답이 매겨져 있지 않은 점들은 대상에서 제외하고 평가하였다. 비교예 1 인코더에 사용된 트랜스포머 모듈은 GeLU(Gauusian Error Linear Unit) 활성화 함수를 사용하A_08_도록 하였으 며, 디코더를 구성하는 모든 컨볼루션 계층은 세그멘테이션 헤드를 제외하고 모두 ReLU 활성화 함수를 사용하 A_08_도록 하였고 각각의 컨볼루션 계층 이후에는 배치 정규화 계층과 드롭 아웃 계층을 두어 신경망의 과적합 을 최소화할 수 있A_08_도록 하였다. 신경망의 학습에는 Adam(Adaptive Moment Estimation) 최적화기를 사용하였으며 학습률은 0.001로 시작해 학습 이 2에폭(Epoch)이상 진행되지 않을 때마다 0.75배씩 줄어들A_08_도록 하였다. 학습은 최대 500에폭 동안 진행하A_08_도록 설정하였으며 이때 손실 값이 연속적으로 10에폭 이상 낮아지지 않 으면 학습을 조기 종료하A_08_도록 설정하였다. 비교예 2 비교예 2는 \"Milioto, A., Vizzo, I., Behley, J., & Stachniss, C. “Rangenet++: Fast and accurate lidar semantic segmentation.” In 2019 IEEE/RSJ international conference onintelligent robots and systems (IROS), pp. 4213-4220. IEEE. Nov. 2019\"에 개시된 시맨틱 세그멘테이션 방법을 사용하였다. 즉, 비교예 2는 다운 샘플링(down sampling)된 포인트 클라우드를 업 샘플링(up sampling) 하는 과정에서, 부족 한 클래스 데이터를 보완하기 위하여 업 샘플링 된 출력에 기존 입력 데이터를 중첩하여 KNN 알고리즘을 적용하 였다. 실험 한편, 표 1은 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법의 성능 평가 및 비교예와의 비교 결과를 나타낸 표이다. [표 1] 표 1에 나타난 바와 같이, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법은 2차원 시맨틱 세그멘테이션 의 mIoU을 기준으로 36.86%, 3차원 시맨틱 세그멘테이션을 기준으로 24.82%를 기록하였다. 이는 비교예 1[1]의 결과보다 2차원 기준 약 5%, 3차원 기준 약 7.7% 향상된 결과이며, 동시에 3차원 시맨틱 세 그멘테이션 결과를 얻기 위해 어떠한 별A_08_도의 후처리 과정A_08_도 사용하지 않은 결과이다. 비교예 1의 신경망에 사용된 ResNet의 컨볼루션 블록과 트랜스포머 블록들이 애초에 이미지 분류를 위해 설계된 바, 컨볼루션 블록은 잔차 블록이 포함된 직렬 구조로 각각의 입력마다 독립적으로 학습되는 특징들의 채널 간 정보를 반영하지 못하고, 트랜스포머는 특징 맵의 패치 분할 시 중첩이 없어 세그먼트 이미지 복원에 필요한 패 치 간의 연관 정보가 손실되어 성능이 저하된다는 문제가 있는 것을 확인할 수 있다. 이에 따라, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법이 학습하는 특징들 사이의 연관 관계 및 다 중 크기 특징 등이 시맨틱 세그멘테이션에 효과적인 요소로 작용한 것으로 보인다. 성능이 동일한 방법론을 토대로 3차원 시맨틱 세그멘테이션을 수행하는 SOTA(State-of-the-Art) 연구인 비교예 2에 비해 저조한 수준이나, 비교예 2[7]의 경우에는 3차원 시맨틱 세그멘테이션의 결과를 얻기 위해 K-NN을 사 용하는 별A_08_도의 후처리 과정이 필요하다. 반면에, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법은 학습하는 과정에서 3차원 데이터를 직접적으 로 함께 사용해 프로세스가 간단하다는 장점이 있다. 정리하면, 본 발명의 일 실시예에 따른 시맨틱 세그멘테이션 방법은 특징 단계의 센서 퓨전과 다중 크기 특징을 이용해 멀티 모달 데이터의 다양한 요소들을 담은 융합 특징들을 추출함과 동시에 그들의 복잡한 상관관계와 지 역적 연관성까지 학습하A_08_도록 하는 인코더-디코더 구조의 3차원 시맨틱 세그멘테이션 방법이다. 또한, 3차원 공간에 대한 추론 결과를 신경망으로부터 직접 만들A_08_도록 하여 손실함수와 평가함수에서 학습 과 평가에 활용할 수 있A_08_도록 함으로써, 비교예 1 및 2가 요구하던 추가적인 후처리 과정을 제거할 수 있 A_08_도록 하였다. 이를 통해 비교예 1보다 2차원 mIoU는 약 5%, 3차원 mIoU는 약 8% 향상된 결과를 보였다. 이상과 같이, 본 명세서와 A_08_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_08_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_08_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. A_08_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 인코딩부 220 : 디코딩부 225 : 저장부 A_08_청구범위 A_08_청구항 1 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계; 상기 학습 데이터 생성 장치가, 상기 점군 데이터 및 상기 이미지로부터 다중 크기의 특징들을 추출하고, 추출 한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성하는 단계; 를 포함하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 2 제1 항에 있어서, 상기 심층 특징을 추출하는 단계는 복수의 컨벌루션 블록들을 통해 상기 점군 데이터 및 상기 이미지 각각으로부터 입력 대비 사전 설정된 크기로 축소된 특징맵을 추출하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 3 제2 항에 있어서, 상기 심층 특징을 추출하는 단계는 상기 점군 데이터 및 상기 이미지 각각을 1/2, 1/4 및 1/8의 크기로 단계적으로 축소하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 4 제3 항에 있어서, 상기 복수의 컨벌루션 블록들은 상기 점군 데이터 및 상기 이미지에 포함된 특징 간 상관관계를 학습하는 분할-어텐션 모듈 및 다중 크기 특징 을 추출하는 다중 크기(multi-scale) 어텐션 모듈로 구성되어, 상기 점군 데이터 및 상기 이미지에 포함된 요소 들을 특징으로 추출함과 동시에, 추출된 특징 사이의 상관관계를 학습하는 것을 특징으로 하는, 시맨틱 세그멘 테이션 방법. A_08_청구항 5 제4 항에 있어서, 상기 심층 특징을 추출하는 단계는 상기 점군 데이터 및 상기 이미지 각각으로부터 추출된 특징맵을 접합(concatenate)하여 센서 퓨전을 수행하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 6 제5 항에 있어서, 상기 심층 특징을 추출하는 단계는 상기 센서 퓨전 수행된 데이터를 사전 기계 학습된 인공 지능을 통해 지역적 연관 정보를 포함하는 1/16 크기의 심층 특징맵을 추출하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 7 제6 항에 있어서, 상기 인공 지능은 컨벌루션 기반 중첩 패치 추출 방법과 패치간 지역적 연관성을 학습하는 것을 특징으로 하는, 시맨틱 세그멘테 이션 방법. A_08_청구항 8 제7 항에 있어서, 상기 벡터를 생성하는 단계는 상기 심층 특징맵을 컨벌루션, 확장(upsampling) 및 접합 연산의 반복을 통해 (H, W, C)로 표현되는 추론맵으로 복원하는 것을 특징으로 하는, 시맨틱 세그멘테이션 방법. A_08_청구항 9 제8 항에 있어서, 상기 벡터를 생성하는 단계는 상기 (H, W, C)로 표현되는 추론맵을 (H*W, C)로 재구성하여 함께 출력하는 것을 특징으로 하는, 시맨틱 세그멘 테이션 방법. A_08_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬 영된 이미지(image)를 수신하는 단계; 상기 프로세서가, 상기 점군 데이터 및 상기 이미지로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이 의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하는 단계; 및 상기 프로세서가, 상기 추출한 심층 특징으로부터 세그멘테이션 추론 벡터를 생성하는 단계; 를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 221, "content": "A_08_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 222, "content": "A_08_요약 본 발명은 자율 주행 차량이 주행환경을 3차원으로 인지하기 위하여 카메라 및 라이다의 센서 퓨전을 통해 멀티 모달 기반의 3차원 시맨틱 세그멘테이션을 수행하기 위한, 시맨틱 세그멘테이션 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 라이다(lidar)로부터 획득된 점군 데이터(point cloud) 및 동시에 카메라(camera)를 통해 촬영된 이미지(image)를 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 점군 데이터 및 상기 이미지 로부터 다중 크기의 특징들을 추출하고, 추출한 특징들 사이의 지역적 연관성 및 채널 간 상관관계를 활용하여 심층 특징을 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 추출한 심층 특징으로부터 세그멘테이션 추 론 벡터를 생성하는 단계를 포함할 수 있다. A_08_대표A_08_도 A_08_도 6 A_08_도면 A_08_도 1 A_08_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 223, "content": "A_08_도 3 A_08_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 224, "content": "A_08_도 5 A_08_도 6 A_08_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 225, "content": "A_08_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 226, "content": "A_09_발명의 설명 A_09_발명의 명칭 3D 점군 데이터의 어노테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for annotation for 3D point cloud data, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 227, "content": "A_09_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 인 공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션(annotation) 작업과 관련하여, 라이다(lidar)에 의해 획 득된 3D 점군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법 및 이를 실행하기 위하여 기록매체에 기 록된 컴퓨터 프로그램에 관한 것이다. A_09_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지A_09_도 학습(supervised learning), 비지A_09_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 어노테이션(annotation)을 수행하고, 메타데이터(metadat a)를 입력하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 A_09_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정A_09_도와 운전자가 차량을 제어하는 정A_09_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회가 분류한 6단계에 따르면, 레벨 0단계는 비자동화, 레벨 1단계는 운전자 보조, 레벨 2단계는 부분 자동 화, 레벨 3단계는 조건부 자동화, 레벨 4단계는 고A_09_도 자동화, 그리고 레벨 5단계는 완전 자동화 단계이다. 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메커니즘을 통해 수행된다. 그리고, 다양한 기업체들은 자율주행 메커니즘에서 인지 및 경로 계획을 인공지능(AI)을 이용하 여 구현하기 위해 개발 중에 있다. 최근에는 상술한 바와 같은, 자율주행의 인공지능(AI) 기계 학습(machine learning)에 사용되는 데이터뿐만 아 니라, 다양한 분야에서 라이다(lidar)로부터 획득된 3D 점군 데이터를 인공지능 기계 학습에 사용하기 위한 요 구가 증가하고 있다. 라이다로부터 획득된 3D 점군 데이터를 가공하기 위해서는 3D 공간에서 각 객체에 해당되는 점들을 효과적으로 그룹(group)화 하여 어노테이션 작업을 수행해야 한다. 종래의 3D 점군 데이터에 대한 어노테이션 작업은 작업자가 특정 뷰 포인트에서 객체에 해당하는 2D 바운딩 박 스(bounding box)를 지정하여 어노테이션 작업을 수행하였다. 그러나, 3D 공간의 특정 시점에서 2D 바운딩 박스를 통해 어노테이션 작업을 수행함에 따라, 특정하기 위한 객 체에 해당되지 않는 노이즈 점들까지 선택되는 문제점이 있었다. A_09_선행기술문헌 A_09_특허문헌 대한민국 등록특허공보 제10-2230144호, ‘인공 지능 심층 학습 타겟 탐지 및 속A_09_도 퍼텐셜 필드 알고리즘 기반 장애물 회피 및 자율 주행 방법 및 장치’, (2021.03.15. 공고)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 228, "content": "A_09_발명의 내용 A_09_해결하고자 하는 과제 본 발명의 일 목적은 인공지능을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획 득된 3D 점군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법을 제공하는 것이다. 본 발명의 다른 목적은 인공지능을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 229, "content": "과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다.A_09_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능을 기계 학습하기 위한 데이터의 어노테 이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법 을 제안한다. 상기 방법은 어노테이션 장치가, 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하는 단계, 상기 어노테이션 장치가, 상기 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 상기 설정된 적어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이 드 포인트(guide point)들을 생성하는 단계 및 상기 어노테이션 장치가, 상기 생성한 복수 개의 가이드 포인트 들을 기초로 객체(object)를 특정하기 위한 어노테이션(annotation)을 수행하는 단계를 포함할 수 있다. 구체적으로, 상기 가이드 포인트들을 생성하는 단계는 상기 3D 점군 데이터에서 상기 객체를 횡단하는 평면인 적어A_09_도 하나의 포인트 보드(point board)를 생성하는 단계 및 상기 적어A_09_도 하나의 포인트 보드 내에 서 상기 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받는 단계를 포함하는 것을 특징으로 한다. 또한, 상기 포인트 보드는 상기 3D 점군 데이터에 사전 설정된 3D 직교좌표계(orthogonal coordinate)에서의 좌 표 평면인 것을 특징으로 한다. 또한, 상기 포인트 보드를 생성하는 단계는 어노테이션을 수행하는 작업자의 제어에 따라 상기 객체의 특정 지 점을 선택받으면, 상기 선택받은 지점을 원점으로 하는 상기 3D 직교좌표계를 생성하고, 상기 생성된 3D 직교좌 표계에서의 각 좌표축 쌍을 기초로 생성된 좌표 평면을 포인트 보드로 생성하는 것을 특징으로 한다. 상기 포인트 보드를 생성하는 단계는 상기 적어A_09_도 하나의 포인트 보드를 생성하되, 각 포인트 보드와 평행 하며 상기 객체를 횡단하는 일정 간격 이격된 복수 개의 추가 포인트 보드들을 생성하는 것을 특징으로 한다. 상기 포인트 보드를 생성하는 단계는 상기 작업자의 제어에 따라 상기 적어A_09_도 하나의 포인트 보드를 평행 이동시켜 복수 개의 추가 포인트 보드들을 생성하는 것을 특징으로 한다. 상기 포인트 보드를 생성하는 단계는 상기 생성된 적어A_09_도 하나의 포인트 보드 각각에서의 상기 객체의 외 곽선을 식별하고, 상기 객체를 횡단하는 가상선을 생성하고, 상기 생성된 가상선 중 가장 큰 폭을 갖는 가상선 을 기준으로 상기 생성된 적어A_09_도 하나의 포인트 보드 각각의 크기를 설정하는 것을 특징으로 한다. 상기 가이드 포인트들을 선택받는 단계는 상기 적어A_09_도 하나의 포인트 보드들 각각에서의 상기 객체를 특정 하기 위한 복수 개의 가이드 포인트들을 선택받되, 상기 적어A_09_도 하나의 포인트 보드 각각에서의 상기 객체 의 외곽선을 식별하고, 상기 식별된 외곽선으로부터 상기 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시키는 것을 특징으로 한다. 상기 가이드 포인트들을 선택받는 단계는 상기 적어A_09_도 하나의 포인트 보드 각각에서의 상기 객체의 외곽선 을 식별하고, 상기 식별된 객체의 외곽선을 따라 일정 간격 이격된 복수 개의 가이드 포인트들을 자동 생성하고, 상기 식별된 객체의 외곽선으로부터 상기 생성된 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시키는 것을 특징으로 한다. 상기 어노테이션을 수행하는 단계는 상기 가이드 포인트들을 삼각형으로 연결하여 공간을 분할하되, 삼각형들의 내각의 최소값이 최대가 되A_09_도록 분할하여 생성된 삼각형을 기초로 3D 모델링을 수행하는 것을 특징으로 한 다. 상기 어노테이션을 수행하는 단계는 상기 3D 점군 데이터 내의 점군의 밀A_09_도를 기초로 상기 객체에 해당하 는 점군을 식별하고, 상기 식별된 점군 중 상기 3D 모델링된 영역의 외곽에 위치하는 점을 추출하고, 상기 추출 된 점을 기준으로 3D 모델링을 재수행하는 것을 특징으로 한다. 상기 어노테이션을 수행하는 단계는 상기 3D 모델링된 영역의 외곽에 위치하는 점에 가이드 포인트를 생성하고, 상기 생성된 가이드 포인트를 기초로 상기 3D 모델링을 재수행하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능을 기계 학습하기 위한 데이터의 어노테 이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법 을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서가, 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하는 단계, 상기 프로세서가, 상기 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 상기 적어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포 인트(guide point)들을 생성하는 단계 및 상기 프로세서가, 상기 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노테이션(annotation)을 수행하는 단계를 실행시키기 위하여, 기록매체에 기록 된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 A_09_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 230, "content": "A_09_발명의 효과 본 발명의 실시 예들에 따르면, 3D 점군 데이터의 3D 공간 상에서 객체 특정을 위한 가이드 포인트(guide point)를 생성하여 어노테이션을 수행함으로써, 정교하게 객체를 특정할 수 있으며, 객체에 해당되는 점들을 정 확하게 특정할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 231, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 232, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 233, "content": "A_09_도면의 간단한 설명 A_09_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_09_도이다. A_09_도 3은 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성A_09_도이다. A_09_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성A_09_도이다. A_09_도 5는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서A_09_도이다. A_09_도 6 내지 A_09_도 10은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시A_09_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 234, "content": "A_09_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의A_09_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과A_09_도하게 포괄적인 의미로 해석되거나, 과A_09_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과A_09_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수 개의 표현을 포함 한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포 함되지 않을 수A_09_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한 다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소A_09_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수A_09_도 있지만, 중간에 다른 구성 요소가 존재할 수 A_09_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 A_09_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, A_09_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 A_09_도면은 본 발명의 사상을 쉽게 이해할 수있A_09_도록 하기 위한 것일 뿐, 첨부된 A_09_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 A_09_도면 외에 모든 변경, 균등물 내지 대체물에 까지A_09_ 도 확장되는 것으로 해석되어야 한다. 한편, 라이다로부터 획득된 3D 점군 데이터를 가공하기 위해서는 3D 공간에서 각 객체에 해당되는 점들을 효과 적으로 그룹(group)화 하여 어노테이션 작업을 수행해야 한다. 종래의 3D 점군 데이터에 대한 어노테이션 작업은 작업자가 특정 뷰 포인트에서 객체에 해당하는 2D 바운딩 박 스(bounding box)를 지정하여 어노테이션 작업을 수행하였다. 그러나, 3D 공간의 특정 시점에서 2D 바운딩 박스를 통해 어노테이션 작업을 수행함에 따라, 특정하기 위한 객 체에 해당되지 않는 점들까지 선택되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 라이다에 의해 획득된 3D 점군 데이터에 포함된 객체를 특정할 수 있는 다양한 수단들을 제안하고자 한다. A_09_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성A_09_도이다. A_09_도 1에 A_09_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 생성 장 치, 하나 이상의 어노테이션 장치(200-1, 200-2, …, 200-n; 200), 학습 데이터 검증 장치 및 학습 장치를 포함하여 구성될 수 있다. 또한, A_09_도 2에 A_09_도시된 바와 같이, 본 발명의 다른 실시예에 따른 인공지능 학습 시스템은 하나 이상의 어노테이션 장치(200-a, 200-b, …, 200-m; 200)와 학습 데이터 검증 장치(300-a, 300-b, …, 300-m; 300)가 하나로 이루어진 복수 개의 그룹(Group-a, Group-b …, Group-m), 학습 데이터 생성 장치 및 학습 장치 를 포함하여 구성될 수 있다. 이와 같은, 다양한 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지 능(AI)을 기계 학습시키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 이와 같은, 학습 데이터 생성 장치는 기본적으로 학습 데이터 검증 장치와 구분되는 장치이나, 실제 물리적 환경에서 학습 데이터 생성 장치와 학습 데이터 검증 장치가 하나의 장치로 통합되어 구현될 수A_09_도 있다. 구체적으로, 학습 데이터 설계 장치는 학습 장치로부터 인공지능(AI) 학습과 관련된 프로젝트의 속성 을 수신할 수 있다. 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지 능(AI) 학습을 위한 데이터 구조의 설계, 수집된 데이터의 정제, 데이터의 가공, 데이터의 확장 및 데이터의 검 증을 수행할 수 있다. 우선적으로, 학습 데이터 설계 장치는 인공지능(AI) 학습을 위한 데이터 구조를 설계할 수 있다. 예를 들 어, 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지능(AI) 학습을 위한 온톨로지(ontology), 인공지능(AI) 학습을 위한 데이터의 분류 체계를 정의할 수 있다. 학습 데이터 설계 장치는 설계된 데이터 구조를 기초로, 인공지능(AI) 학습을 위한 데이터를 수집할 수 있 다. 이를 위하여, 학습 데이터 설계 장치는 외부로부터 3D 점군 데이터를 입력 받거나, 웹 크롤링(web crawling)을 수행하여 3D 점군 데이터를 수집하거나, 또는 외부 기관의 장치로부터 3D 점군 데이터를 다운로드 할 수 있다. 여기서, 3D 점군 데이터는 차량에 고정 설치된 라이다(lidar)에 의해 획득된 데이터이다. 차량에 고정 설치된 라이다는 레이저 펄스를 발사하고, 차량 주위에 위치하는 객체들에 의해 반사되어 돌아온 빛을 감지하여, 차량 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 즉, 3D 점군 데이터를 구성하는 점군은 라이다에 의해 3차원 공간으로 발사된 레이저 펄스를 반사시킨 점(point)들의 집합을 의미한다.학습 데이터 생성 장치는 수집된 3D 점군 데이터 중에서 중복되거나 또는 극히 유사한 데이터를 제거할 수 있다. 학습 데이터 생성 장치는 수집 및 정제된 3D 점군 데이터를 복수 개의 어노테이션 장치에 분배하여 전송할 수 있다. 이 경우, 학습 데이터 생성 장치는 어노테이션 장치의 작업자(즉, 라벨러)에 대하여 사전에 할당된 양에 따라 3D 점군 데이터를 분배할 수 있다. 학습 데이터 생성 장치는 어노테이션 장치로부터 직접 어노테이션 작업 결과물을 수신하거나, 또는 학습 데이터 검증 장치로부터 어노테이션 작업 결과물 및 검수 결과를 수신할 수 있다. 학습 데이터 생성 장치는 수신된 어노테이션 작업 결과물을 패키징(packaging)하여 인공지능(AI) 학습용 데이터를 생성할 수 있다. 그리고, 학습 데이터 생성 장치는 생성된 인공지능(AI) 학습용 데이터를 학습 장치에 전송할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라A_09_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 작업을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 어노테이션 작업은 3D 점군 데이터 내에서 객체를 특정하고, 특정된 객체의 속성 정보를 포함하는 클래스(class) 정보를 입력하는 과정을 포함할 수 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하고, 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 설정된 적 어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포인트(guide point)들 을 생성할 수 있다. 그리고, 어노테이션 장치는 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노테이션(annotation)을 수행할 수 있다. 한편, 어노테이션 장치에 관한 구체적인 내용은 이하 A_09_도 3 및 A_09_도 4를 참조하여 후술하A_09_도록 한다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치 또는 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_09_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수A_09_도 있다. 다음 구성으로, 학습 데이터 검증 장치는 인공지능(AI) 학습용 데이터를 검증하는데 사용될 수 있는 장치 이다. 즉, 학습 데이터 검증 장치는 어노테이션 장치에 의해 생성된 어노테이션 작업 결과물이 사전 에 설정된 목표 품질에 부합하는지 여부, 또는 어노테이션 작업 결과물이 인공지능(AI) 학습에 유효한지 여부를 검증할 수 있는 장치이다. 구체적으로, 학습 데이터 검증 장치는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 여기서, 어노테이션 작업 결과물은 3D 점군 데이터로부터 특정된 객체의 좌표와, 이미지 또는 객체에 대 한 메타데이터가 포함될 수 있다. 어노테이션 작업 결과물의 메타데이터에는 특정된 객체의 카테고리 (category), 객체가 2D 이미지의 화각에 의해 잘려진 비율(truncation), 객체가 다른 객체 또는 물체에 의해 가 려진 비율(occlusion), 객체의 트래킹 아이디(tracking identifier), 이미지가 촬영된 시각, 이미지가 촬영된 날의 기상 조건 등이 포함될 수 있으며, 이에 한정되는 것은 아니다. 이와 같은, 어노테이션 작업 결과물은JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것A_09_도 아니다. 학습 데이터 검증 장치는 수신된 어노테이션 작업 결과물을 검수할 수 있다. 이를 위하여, 학습 데이터 검 증 장치는 어노테이션 작업 결과물을 대상으로 스크립트(script)를 이용하여 검수를 수행할 수 있다. 여기 서, 스크립트는 어노테이션 작업 결과물을 대상으로 사전에 설정된 목표 품질의 부합 여부 또는 데이터 유효성 여부를 검증하기 위한 코드이다. 그리고, 학습 데이터 검증 장치는 어노테이션 장치들로부터 수신된 어노테이션 작업 결과물 및 검수 결과를 학습 데이터 생성 장치에 전송할 수 있다. 상술한 바와 같은 특징을 가지는, 학습 데이터 검증 장치는 어노테이션 장치 및 학습 데이터 생성 장 치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라 A_09_도 허용될 수 있다. 예를 들어, 학습 데이터 검증 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 학습 장치는 학습 데이터 생성 장치로부터 제공된 3D 점군 데이터를 통해 인공지능 (AI)을 기계 학습하는데 사용될 수 있는 장치이다. 이와 같은, 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라A_09_도 허용될 수 있다. 예를 들어, 학습 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것 은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치, 하나 이상의 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것A_09_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 구체적으로 설명하A_09_도록 한다. A_09_도 3은 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성A_09_도이다. A_09_도 3에 A_09_도시된 바와 같이, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력 부, 포인트 생성부, 객체 특정부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치 또는 학습 데이터 검증 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 어노테이션의 작업 결과물에 대한 좌표 및 객체의 속성 정 보가 포함될 수 있다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 3D 점군 데이터에서 식별하고자 하는 객체의 속성, 어노테이션 규칙 등이 포함될 수 있으나, 이에 한정되 는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해 사용자로부터 신호를 입력 받거나, 연산 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 3D 점군 데이터를 출력할 수 있다. 입출력부(21 0)는 어노테이션 작업을 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 다음 구성으로, 포인트 생성부는 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 설정된 적어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포 인트(guide point)들을 생성할 수 있다. 구체적으로, 포인트 생성부는 3D 점군 데이터에서 객체를 횡단하는 평면인 적어A_09_도 하나의 포인트 보 드(point board)를 생성할 수 있다. 여기서, 포인트 보드는 3D 점군 데이터에 사전 설정된 3D 직교좌표계(orthogonal coordinate)에서의 좌표 평면 이 될 수 있다. 예를 들어, 포인트 보드는 x축, y축 및 z축을 포함하는 직교좌표계의 xy 좌표 평면, yz 좌표 평면 및 xz 좌표 평면이 될 수 있다. 즉, 포인트 생성부는 어노테이션을 수행하는 작업자의 제어에 따라 객체의 특정 지점을 선택받으면, 선택 받은 지점을 원점으로 하는 3D 직교좌표계를 생성하고, 생성된 3D 직교좌표계에서의 각 좌표축 쌍을 기초로 생 성된 좌표 평면을 포인트 보드로 생성할 수 있다. 그리고, 포인트 생성부는 작업자의 제어에 따라 적어A_09_도 하나의 포인트 보드를 평행 이동시켜 복수 개 의 추가 포인트 보드들을 생성할 수 있다. 예를 들어, 포인트 생성부는 작업자로부터 최초 객체의 최상단에 위치한 점을 선택받고, 선택받은 점을 원 점으로 하는 3D 직교좌표계를 생성할 수 있다. 이후, 포인트 생성부는 포인트 보드를 z축 방향으로 평행 이동시킬 수 있으며, 최종적으로 객체의 최하단까지 포인트 보드를 이동시킬 수 있다. 하지만 이에 한정된 것은 아니고, 포인트 생성부는 각 포인트 보드와 평행하며 객체를 횡단하는 일정 간격 이격된 복수 개의 추가 포인트 보드들을 생성할 수 있다. 예를 들어, 포인트 생성부는 작업자로부터 객체를 선택받을 수 있으며, 객체가 선택되면, 선택받은 객체의 외곽선을 식별할 수 있다. 그리고, 포인트 생성부는 사전 설정된 3D 직교좌표계를 기준으로 좌표 평면인 포인트 보드를 생성하되, 식별된 객체의 외곽선을 기초로 각 포인트 보드와 평행하며 객체를 횡단하는 일정 간 격 이격된 복수 개의 추가 포인트 보드들을 생성할 수 있다. 포인트 생성부는 포인트 보드를 생성함에 있어서 객체의 외곽선을 기준으로 포인트 보드의 크기를 설정함 으로써, 작업자가 포인트 보드 상에 가이드 포인트들을 용이하게 입력할 수 있A_09_도록 지원할 수 있다. 즉, 포인트 생성부는 생성된 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 객체 를 횡단하는 가상선을 생성하고, 생성된 가상선 중 가장 큰 폭을 갖는 가상선을 기준으로 생성된 적어A_09_도 하나의 포인트 보드 각각의 크기를 설정할 수 있다. 이후, 포인트 생성부는 적어A_09_도 하나의 포인트 보드 내에서 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받을 수 있다. 구체적으로, 포인트 생성부는 적어A_09_도 하나의 포인트 보드 각각에서의 객체를 특정하기 위한 복수 개 의 가이드 포인트들을 선택받되, 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 식별 된 외곽선으로부터 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시킬 수 있다. 또한, 포인트 생성부는 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 식별된 객 체의 외곽선을 따라 일정 간격 이격된 복수 개의 가이드 포인트들을 자동 생성하고, 식별된 객체의 외곽선으로 부터 생성된 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시킬 수 있다. 즉, 후술할 객체 특정부가 선택된 복수 개의 가이드 포인트들 연결하여 어노테이션을 수행함에 있어서, 작 업자로부터 충분히 가이드 포인트를 입력받지 않은 상태로 복수 개의 가이드 포인트들을 연결하는 경우에는 3D 모델링 된 영역 내에 객체에 대한 점군이 모두 포함되지 않는 문제가 발생될 수 있다. 이에 따라, 포인트 생성부는 작업자가 선택한 가이드 포인트들을 객체로부터 이격시킴으로써, 3D 모델링 된 영역 내에 객체의 점군이 완전히 포함될 수 있A_09_도록 지원할 수 있다. 예를 들어, 포인트 생성부는 식별된 객체의 중심점 추정하고, 추정된 중심점을 기준으로 해당 가이드 포인 트를 연결하는 가상선을 형성하고, 형성된 가상선을 기준으로 객체의 외곽으로부터 가이드 포인트를 사전 설정 된 거리만큼 이격시킬 수 있다. 다음 구성으로, 객체 특정부는 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위 한 어노테이션(annotation)을 수행할 수 있다. 구체적으로, 객체 특정부는 선택받은 복수 개의 가이드 포인트들을 연결하여, 3D 점군 데이터에서 객체의 점군 기초로 3D 모델링을 수행하여, 특정을 위한 객체의 점군을 포함하는 3D 모델을 생성할 수 있다. 예를 들어, 객체 특정부는 가이드 포인트들을 삼각형으로 연결하여 공간을 분할하되, 삼각형들의 내각의 최소값이 최대가 되A_09_도록 분할하여 생성된 삼각형을 기초로 3D 모델링을 수행할 수 있다. 즉, 객체 특정부는 들로네 삼각분할(delaunay triangulation)을 통해 객체를 3D 모델링할 수 있다. 다시 말하면, 객체 특정부는 선택된 복수 개의 가이드 포인트들을 모두 포함하는 최초 삼각형(super triangle) 을 구성하고, 최초 삼각형 안에서 각 점을 대상으로 세 점을 포함하는 원 안에 다른 점이 존재하지 않으면, 그 삼각형을 델로니 삼각형으로 추가할 수 있다. 이때, 최초 삼각형은 모든 델로니 삼각망 구성 과정이 수행되면 삭제할 수 있다. 또한, 객체 특정부는 3D 점군 데이터 내의 점군의 밀A_09_도를 기초로 객체에 해당하는 점군을 식별하고, 식별된 점군 중 3D 모델링 된 영역 외곽에 위치하는 점을 추출하고, 추출된 점을 기준으로 3D 모델링을 재수행 할 수 있다. 예를 들어, 객체 특정부는 3D 모델링 된 영역 외곽에 위치하는 점에 가이드 포인트를 생성하고, 생성된 가 이드 포인트를 기초로 3D 모델링을 재수행할 수 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. A_09_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성A_09_도이다. A_09_도 4에 A_09_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리 지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(280a)에 따른 명 령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명의 실시예들 에 따른 방법이 구현된 소프트웨어(280a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 , 학습 데이터 검증 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치 는 어노테이션 장치의 동작에 필요한 데이터를 입력 받거나, 학습 결과물을 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각 의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이 션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스 (resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어 (280b)를 저장할 수 있다. 또한, 스토리지는 본 발명의 실시예들에 따른 방법의 수행에 필요한 정보들을저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 학습 방법을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point grou p)를 수신하는 단계, 프로세서가, 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하 고, 적어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포인트(guide point)들을 생성하는 단계 및 프로세서가, 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노테이션(annotation)을 수행하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로 그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. A_09_도 4에 A_09_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수A_09_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하A_09_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하A_09_도록 구성될 수 있으며, 그 역A_09_도 마찬가지이다. A_09_도 5는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서A_09_도이다. A_09_도 5를 참조하면, S100 단계에서 어노테이션 장치는 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신할 수 있다. 다음으로, S200 단계에서 어노테이션 장치는 3D 점군 데이터에서 객체를 횡단하는 평면인 적어A_09_도 하나의 포인트 보드(point board)를 생성할 수 있다. 여기서, 포인트 보드는 3D 점군 데이터에 사전 설정된 3D 직교좌표계(orthogonal coordinate)에서의 좌표 평면 이 될 수 있다. 즉, 어노테이션 장치는 어노테이션을 수행하는 작업자의 제어에 따라 객체의 특정 지점을 선택받으면, 선택받은 지점을 원점으로 하는 3D 직교좌표계를 생성하고, 생성된 3D 직교좌표계에서의 각 좌표축 쌍을 기초로 생성된 좌표 평면을 포인트 보드로 생성할 수 있다. 이때, 어노테이션 장치는 적어A_09_도 하나의 포인트 보드를 생성하되, 각 포인트 보드와 평행하며 객체를 횡단 하는 일정 간격 이격된 복수 개의 추가 포인트 보드들을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 어노테이션 장치는 작업자의 제어에 따라 상기 적어A_09_도 하나의 포인트 보드를 평행 이동시켜 복수 개의 추가 포인트 보드들을 생성할 수A_09_도 있다. 또한, 어노테이션 장치는 생성된 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 객체 를 횡단하는 가상선을 생성하고, 생성된 가상선 중 가장 큰 폭을 갖는 가상선을 기준으로 생성된 적어A_09_도 하나의 포인트 보드 각각의 크기를 설정할 수 있다. 다음으로, S300 단계에서 어노테이션 장치는 적어A_09_도 하나의 포인트 보드 내에서 객체를 특정하기 위한 복 수 개의 가이드 포인트들을 선택받을 수 있다. 구체적으로, 어노테이션 장치는 적어A_09_도 하나의 포인트 보드들 각각에서의 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받되, 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 식별된 외곽선으로부터 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시킬 수 있다. 또한, 어노테이션 장치는 적어A_09_도 하나의 포인트 보드 각각에서의 객체의 외곽선을 식별하고, 식별된 객체 의 외곽선을 따라 일정 간격 이격된 복수 개의 가이드 포인트들을 자동 생성하고, 식별된 객체의 외곽선으로부 터 생성된 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시킬 수 있다. 그리고, S400 단계에서 어노테이션 장치는 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정 하기 위한 어노테이션(annotation)을 수행할 수 있다. 구체적으로, 어노테이션 장치는 선택받은 복수 개의 가이드 포인트들을 연결하여, 3D 점군 데이터에서 객체의 점군 기초로 3D 모델링을 수행하여, 특정을 위한 객체의 점군을 포함하는 3D 모델을 생성할 수 있다. 예를 들어, 어노테이션 장치는 가이드 포인트들을 삼각형으로 연결하여 공간을 분할하되, 삼각형들의 내각의 최 소값이 최대가 되A_09_도록 분할하여 생성된 삼각형을 기초로 3D 모델링을 수행할 수 있다. 또한, 어노테이션 장치는 3D 점군 데이터 내의 점군의 밀A_09_도를 기초로 객체에 해당하는 점군을 식별하고, 식별된 점군 중 3D 모델링 된 외곽에 위치하는 점을 추출하고, 추출된 점을 기준으로 3D 모델링을 재수행할 수 있다. 이때, 어노테이션 장치는 3D 모델링 된 영역의 외곽에 위치하는 점에 가이드 포인트를 생성하고, 생성된 가이드 포인트를 기초로 3D 모델링을 재수행할 수 있다. 또한, 어노테이션 장치는 3D 모델링 된 영역 내에 위치하는 객체의 점군의 색상을 변경하여 작업자가 용이하게 식별하A_09_도록 할 수 있다. A_09_도 6 내지 A_09_도 10은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시A_09_도이다. A_09_도 6 내지 A_09_도 10을 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 3D 점군 데이터에서 객 체를 횡단하는 평면인 적어A_09_도 하나의 포인트 보드(point board)를 생성할 수 있다. 여기서, 포인트 보드는 3D 점군 데이터에 사전 설정된 3D 직교좌표계(orthogonal coordinate)에서의 좌표 평면 이 될 수 있다. 즉, 어노테이션 장치는 어노테이션을 수행하는 작업자의 제어에 따라 객체의 특정 지점을 선택받으면, 선택받은 지점을 원점으로 하는 3D 직교좌표계를 생성하고, 생성된 3D 직교좌표계에서의 각 좌표축 쌍을 기초로 생성된 xy 좌표 평면(a), yz 좌표 평면(b) 및 xz 좌표 평면(c)을 포인트 보드로 생성할 수 있다. A_09_도 6에 A_09_도시된 바와 같이, 어노테이션 장치는 포인트 보드(a, b, c)가 생성되면, 생성된 포인트 보드 (a, b, c) 중 적어A_09_도 하나에 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받을 수 있다. 예를 들어, 작업자는 3D 점군 데이터의 뷰 포인트를 변경하면서, 포인트 보드(a, b, c) 중 하나를 선택할 수 있 다. 그리고, 작업자는 포인트 보드(a, b, c) 중 하나를 선택한 상태로, 선택된 포인트 보드 상에서 특정 지점을지정하게 되면, 해당 포인트 보드 상에 가이드 포인트를 생성할 수 있다. 이때, 어노테이션 장치는 작업자의 제어에 따라 포인트 보드(a, b, c)를 평행 이동시킬 수 있다. 즉, A_09_도 7 에 A_09_도시된 바와 같이, 어노테이션 장치는 작업자의 제어에 따라 포인트 보드(a, b, c)를 z축 방향으로 평 행 이동시킬 수 있다. 즉, 작업자는 xy 좌표 평면에 해당하는 포인트 보드(a)를 객체(object)의 최상단에 위치시킨 상태로 가이드 포 인트들을 생성하고, 객체의 하부로 포인트 보드(a)를 평행 이동시키면서 가이드 포인트들을 추가로 생성할 수 있다. 최종적으로, A_09_도 8에 A_09_도시된 바와 같이, 작업자는 객체의 최하단에 포인트 보드(a)를 위치시킨 후 가 이드 포인트들을 생성할 수 있다. A_09_도 9에 A_09_도시된 바와 같이, 어노테이션 장치는 작업자의 가이드 포인트 생성 작업이 완료되면, 선택받 은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노테이션(annotation)을 수행할 수 있 다. 구체적으로, 어노테이션 장치는 가이드 포인트들을 삼각형으로 연결하여 공간을 분할하되, 삼각형들의 내각의 최소값이 최대가 되A_09_도록 분할할 수 있다. 그리고, A_09_도 10에 A_09_도시된 바와 같이, 어노테이션 장치는 작업자로부터 선택받은 가이드 포인트들을 삭 제하고, 생성된 삼각형을 기초로 외부면을 형성함으로써 3D 모델링을 수행할 수 있다. 이상과 같이, 본 명세서와 A_09_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에A_09_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 A_09_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하 고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니되고 예시적인 것으로 고려되 어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내 에서의 모든 변경은 본 발명의 범위에 포함된다. A_09_부호의 설명 100 : 학습 데이터 생성 장치 200 : 어노테이션 장치 300 : 학습 데이터 검증 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 포인트 생성부 220 : 객체 특정부 225 : 저장부 A_09_청구범위 A_09_청구항 1 어노테이션 장치가, 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하는 단계; 상기 어노테이션 장치가, 상기 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 상기 설정된 적어A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포인트(guide point)들을 생성하는 단계; 및 상기 어노테이션 장치가, 상기 생성한 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노 테이션(annotation)을 수행하는 단계; 를 포함하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 2 제1 항에 있어서, 상기 가이드 포인트들을 생성하는 단계는 상기 3D 점군 데이터에서 상기 객체를 횡단하는 평면인 적어A_09_도 하나의 포인트 보드(point board)를 생성하 는 단계; 및 상기 적어A_09_도 하나의 포인트 보드 내에서 상기 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받 는 단계;를 포함하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 3 제2 항에 있어서, 상기 포인트 보드는 상기 3D 점군 데이터에 사전 설정된 3D 직교좌표계(orthogonal coordinate)에서의 좌표 평면인 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 4 제3 항에 있어서, 상기 포인트 보드를 생성하는 단계는 어노테이션을 수행하는 작업자의 제어에 따라 상기 객체의 특정 지점을 선택받으면, 상기 선택받은 지점을 원점 으로 하는 상기 3D 직교좌표계를 생성하고, 상기 생성된 3D 직교좌표계에서의 각 좌표축 쌍을 기초로 생성된 좌 표 평면을 포인트 보드로 생성하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 5 제4 항에 있어서, 상기 포인트 보드를 생성하는 단계는 상기 적어A_09_도 하나의 포인트 보드를 생성하되, 각 포인트 보드와 평행하며 상기 객체를 횡단하는 일정 간격 이격된 복수 개의 추가 포인트 보드들을 생성하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 6 제4 항에 있어서, 상기 포인트 보드를 생성하는 단계는 상기 작업자의 제어에 따라 상기 적어A_09_도 하나의 포인트 보드를 평행 이동시켜 복수 개의 추가 포인트 보드 들을 생성하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 7 제4 항에 있어서, 상기 가이드 포인트들을 선택받는 단계는 상기 적어A_09_도 하나의 포인트 보드들 각각에서의 상기 객체를 특정하기 위한 복수 개의 가이드 포인트들을 선택받되, 상기 적어A_09_도 하나의 포인트 보드 각각에서의 상기 객체의 외곽선을 식별하고, 상기 식별된 외곽 선으로부터 상기 복수 개의 가이드 포인트들을 사전 설정된 거리만큼 이격시키는 것을 특징으로 하는, 어노테이 션 방법. A_09_청구항 8 제1 항에 있어서, 상기 어노테이션을 수행하는 단계는 상기 가이드 포인트들을 삼각형으로 연결하여 공간을 분할하되, 삼각형들의 내각의 최소값이 최대가 되A_09_도 록 분할하여 생성된 삼각형을 기초로 3D 모델링을 수행하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 9 제8 항에 있어서, 상기 어노테이션을 수행하는 단계는 상기 3D 점군 데이터 내의 점군의 밀A_09_도를 기초로 상기 객체에 해당하는 점군을 식별하고, 상기 식별된 점 군 중 상기 3D 모델링 된 영역의 외곽에 위치하는 점을 추출하고, 상기 추출된 점을 기준으로 3D 모델링을 재수 행하는 것을 특징으로 하는, 어노테이션 방법. A_09_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 라이다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하는 단계; 상기 프로세서가, 상기 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 상기 적어 A_09_도 하나의 뷰 포인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포인트(guide point)들을 생성하는 단계; 및 상기 프로세서가, 상기 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노테이 션(annotation)을 수행하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 235, "content": "A_09_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 236, "content": "A_09_요약 본 발명은 인공지능을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점 군 데이터에 포함된 객체를 특정하기 위한 어노테이션 방법을 제안한다. 상기 방법은 어노테이션 장치가, 라이 다(lidar)로부터 수집된 3D 점군 데이터(3D point group)를 수신하는 단계, 상기 어노테이션 장치가, 상기 3D 점군 데이터를 적어A_09_도 하나의 뷰 포인트(view point)로 설정하고, 상기 설정된 적어A_09_도 하나의 뷰 포 인트 각각에서 객체(object)를 특정하기 위한 복수 개의 가이드 포인트(guide point)들을 생성하는 단계 및 상 기 어노테이션 장치가, 상기 선택받은 복수 개의 가이드 포인트들을 기초로 객체(object)를 특정하기 위한 어노 테이션(annotation)을 수행하는 단계를 포함할 수 있다. A_09_대표A_09_도 A_09_도 10 A_09_도면 A_09_도 1 A_09_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 237, "content": "A_09_도 3 A_09_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 238, "content": "A_09_도 5 A_09_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 239, "content": "A_09_도 7 A_09_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 240, "content": "A_09_도 9 A_09_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 241, "content": "D_01_발명의 설명 D_01_발명의 명칭 컬러 정보를 이용한 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램 {Method for learning data classification using color information, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 242, "content": "D_01_기술분야 *본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 정제에 관한 것이다. 보다 상세하게 는, 인공지능(AI)의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 컬 러 정보를 이용한 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. D_01_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_01_도 학습(supervised learning), 비지D_01_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다.각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_01_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_01_도와 운전차가 차량을 제어하는 정D_01_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_01_도 자동화(high automation), 그리고 레벨 5단계는 완전 자동 화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_01_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_01_도 한다. 그리고, 어노테이션 작업 결과물에 해당되 는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작업 은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작 업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_01_도에 의존하여 산출되고 있는 문제점이 있었 다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다.그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_01_선행기술문헌 D_01_특허문헌 (특허문헌 1) 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 243, "content": "D_01_발명의 내용 D_01_해결하고자 하는 과제 본 발명의 일 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 컬러 정보를 이용한 학습 데이터 분류 방법을 제공하는 것이다. 본 발명의 다른 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제 할 수 있는, 컬러 정보를 이용한 학습 데이터 분류 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램 을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 244, "content": "D_01_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집 된 데이터 중 불필요한 데이터를 정제할 수 있는 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공 지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_01_도 하나 의 학습 데이터 수집 장치에 요청하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_01_도 하나의 학습 데 이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수신한 이미지들의 컬러 정보를 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계; 를 포함하는 것을 특징으로 한다. 구체적으로, 상기 컬러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값 인 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들 중 상기 컬러 정보의 유사D_01_도가 사전 설정된 값보다 높은 이미지들 중 적어D_01_도 하나를 노이즈 이미지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들을 사전 설정된 해상D_01_도로 리샘플링(resamping)하고, 상기 리샘플링 된 이미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 상기 이미지들 간 유사D_01_도를 평가하 는 것을 특징으로 한다. 상기 컬러 정보를 추출하는 단계는 상기 적어D_01_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 상기 이미지들의 파일명 및 컬러 정보를 저장하는 것을 특징으로 한다. 상기 분류하는 단계는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 상기 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 상기 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 상기 나열된 이미지들을 사전 설정된 개수로 그 룹핑(grouping) 한 시퀀스 데이터를 생성하고, 상기 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류하는 것을 특징으로 한다. 상기 컬러 정보를 추출하는 단계는 상기 시퀀스 데이터의 상기 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 상기 이미지들의 유사D_01_도를 평가하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들 중 유사D_01_도가 사전 설정된 값보다 높은 이미지들의 선명D_01_도 (sharpness)를 산출하고, 상기 산출된 선명D_01_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미 지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_01_도를 산출하고, 상기 산출된 유 사D_01_도를 기준으로 상기 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정하는 것을 특징으로 한 다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집 된 데이터 중 불필요한 데이터를 정제할 수 있는 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서 (processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서 가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어 D_01_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 상기 프로세서가, 상기 적어D_01_도 하나의 학습 데이 터 수집 장치로부터 이미지들을 수신하는 단계, 상기 프로세서가, 상기 수신한 이미지들의 컬러 정보를 추출하 는 단계 및 상기 프로세서가, 상기 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실 행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_01_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 245, "content": "D_01_발명의 효과 본 발명의 실시예들에 따르면, 인공지능 학습을 위하여 복수의 학습 데이터 수집 장치로부터 수신한 이미지를 대상으로, 컬러 정보를 기초로 노이즈 이미지를 분류함으로써, 중복되어 업로드 되는 이미지를 효과적으로 정제 할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 246, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 247, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 248, "content": "D_01_도면의 간단한 설명 D_01_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_01_도이다. D_01_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_01_도이다. D_01_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_01_도이다. D_01_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_01_도이다. D_01_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_01_도이다. D_01_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_01_도이다. D_01_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_01_도이다. D_01_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_01_도이다. D_01_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_01_도이다. D_01_도 10 내지 D_01_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_01_도이다. D_01_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_01_도이다. D_01_도 18 내지 D_01_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_01_도이 다. D_01_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_01_도이다. D_01_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_01_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 249, "content": "D_01_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_01_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_01_도하게 포괄적인 의미로 해석되거나, 과D_01_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_01_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_01_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_01_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_01_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_01_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_01_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_01_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_01_도면은 본 발명의 사상을 쉽게 이해할 수 있D_01_도록 하기 위한 것일 뿐, 첨부된 D_01_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_01_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_01_ 도 확장되는 것으로 해석되어야 한다. 한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_01_도에 의존하여 산출되고 있는 문제점이 있었다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_01_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_01_도이다. D_01_도 1에 D_01_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_01_도 감지 센서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_01_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이 미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_01_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한정 되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라D_01_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_01_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_01_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_01_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다.그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_01_도 하나의 샘플 데이터를 수신하고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_01_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_01_도 하나의 프로젝트를 추출할 수 있다. 학습 데 이터 생성 장치는 추출된 적어D_01_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_01_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크 스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정 되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_01_도 2 및 D_01_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_01_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용자 의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수 있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_01_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_01_도 4 및 D_01_도 5를 참조하여 후술하D_01_도록 한 다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_01_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_01_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_01_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을 기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_01_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_01_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_01_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_01_도이다. D_01_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데이 터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_01_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_01_도 감지 센서로부터 감지된 데이터를 수신할 수 있다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_01_도 하나의 샘플 데이터를 수신할 수 있다. 통신 부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행하 기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_01_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다. 다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_01_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘 플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데 이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정 되는 것D_01_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_01_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적 어D_01_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_01_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위 하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_01_도 를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_01_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상 이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_01_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_01_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment)를 비교하여 유사D_01_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_01_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치 에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_01_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_01_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인 공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처리를 수행할 수 있다. 즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이 러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다. 데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_01_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수 행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_01_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검 출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수 의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_01_도 있다. 그리 고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_01_도 하나의 학습 데 이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가이 드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_01_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보를 샘플 이 미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_01_도록 할 수 있다. 또한, 데이터 수집부는 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 이때, 데이터 수집부는 적어D_01_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_01_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_01_도가 사전 설정된 값보다 높은 이미지들 중 적어D_01_도 하나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_01_도로 리샘플링(resamping)하고, 리샘플링 된 이 미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_01_도를 평가할 수 있 다. 또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이 즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록 되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_01_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_01_도가 사전 설정된 값보다 높은 이미지들의 선명D_01_도 (sharpness)를 산출하고, 산출된 선명D_01_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_01_도가 가장 높은 이미지를 제 외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_01_도를 산출하고, 산출된 유사 D_01_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_01_도가 사전 설정된 값보다 높 은 경우, 해당 이미지를 수집한 차량의 속D_01_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_01_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_01_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_01_도, 픽셀에 대한 RGB 값 및 컬 러 코드 값 중 적어D_01_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지 들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_01_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_01_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_01_도가 사전 설정된 값 보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_01_도를 비교하고, 전후 이미지 사이의 유사D_01_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_01_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_01_도가 낮은 경우, 특 정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_01_도로 리샘플링하고, 리샘플링 된 이 미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_01_도를 평가할 수 있 다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_01_도를 통해 특정 이미지가 과속 방지턱을 넘 는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지 들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_01_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_01_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_01_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_01_도를 비교하고, 유사D_01_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들 로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다.또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_01_도를 기초로 노이즈 이미지를 분류하 고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_01_도를 비교하고, 제1 이미 지와 연속된 제2 이미지 사이의 유사D_01_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제 2 이미지의 유사D_01_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격하게 변 화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_01_도를 비교하고, 유사D_01_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬 영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변화 되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_01_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_01_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데이 터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매칭 하고, 매칭된 이미지 사이의 유사D_01_도를 비교하여 유사D_01_도가 사전 설정된 값보다 낮은 경우, 해당 이미 지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_01_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_01_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별하 기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_01_도를 평가하여 이미지의 양단부를 연결하는 객체 를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_01_도 중 적어D_01_도 하나를 기준으 로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다. 또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를 기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_01_도로, 건물, 가이드레일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이 의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_01_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_01_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_01_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_01_도 하나 의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_01_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하 는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_01_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프로 세서가, 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_01_도 하나의 샘플 데이터를 인공지능 학습 장치 로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비 교하고, 샘플 데이터와의 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_01_도 하나의 프 로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_01_도 하나의 프로젝트를 기초로 수행 예정인 어노 테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_01_도 3에 D_01_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_01_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_01_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_01_도록 구성될 수 있으며, 그 역D_01_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_01_도록 한다. D_01_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_01_도이다. D_01_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저장 부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다.또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_01_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_01_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_01_도, 경D_01_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. *객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객 체의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. *다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_01_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역 에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제 하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽선으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_01_도 하나의 점을 선택받고, 선택받은 적어D_01_도 하나의 점을 기초로 상기 경계선 을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_01_도 하나의 점을 연결하는 선을 생성하 고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_01_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하 고, 식별된 적어D_01_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로 밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_01_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다. 일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_01_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할 수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_01_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_01_도이다. D_01_도 5에 D_01_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_01_도 5에 D_01_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_01_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_01_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_01_도록 구성될 수 있으며, 그 역D_01_도 마찬가지이다.이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_01_도록 한다. D_01_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_01_도이다. D_01_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_01_도가 사전 설정된 값보다 높은 이미지들 중 적어D_01_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이 미지들의 유사D_01_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_01_도가 사전 설정된 값보다 높은 이미지들의 선명D_01_도 (sharpness)를 산출하고, 산출된 선명D_01_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_01_도를 산출하고, 산출된 유 사D_01_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_01_도록 한다. D_01_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_01_도이다. D_01_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어 D_01_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_01_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_01_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_01_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_01_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미 지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사D_01_도 가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이 미지와 파일 확장자 또는 이미지 해상D_01_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_01_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분 류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_01_도를 비교하 고, 전후 이미지 사이의 유사D_01_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사 D_01_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_01_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_01_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_01_도 정보에 포함된 과속 방지 턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_01_도 정보에 포함된 커 브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미지 로 분류할 수 있다. *또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_01_도를 비교하고, 유사D_01_ 도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미 지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_01_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_01_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_01_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제2 이미지의 유사D_01_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_01_도를 비교하고, 유사D_01_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_01_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_01_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_01_도 중 적어D_01_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_01_도록 한다. D_01_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_01_도이다. D_01_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위한 적어D_01_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_01_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_01_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사 D_01_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_01_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_01_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_01_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_01_도 하나의 프로젝트를 기초로 수행 예정인 어 노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_01_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_01_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전 체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_01_도록 한다. D_01_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_01_도이고, D_01_도 10 내지 D_01_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_01_도이다. 먼저, D_01_도 10에 D_01_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공지능 (AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_01_도 11에 D_01_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복 수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_01_도 하나의 점을 선 택받고, 선택받은 적어D_01_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_01_도 12에 D_01_도시된 바와 같이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_01_도 13에 D_01_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_01_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선으로 지정할 수 있다. 다음으로, D_01_도 14에 D_01_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계 선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선 과 구분되D_01_도록 출력할 수 있다. 다음으로, D_01_도 15에 D_01_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하 고, 식별된 적어D_01_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지 를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_01_도 16에 D_01_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제2 객 체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_01_도록 한다. D_01_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_01_도이고, D_01_도 18 내 지 D_01_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_01_도이다. 먼저, D_01_도 18에 D_01_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_01_도 19에 D_01_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_01_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_01_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_01_도 20에 D_01_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영 역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_01_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_01_도이다. D_01_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_01_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_01_도가 사전 설정된 값보다 높 은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_01_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_01_도가 사전 설정된 값보다 높은 이미지들의 선명D_01_도 (sharpness)를 산출하고, 산출된 선명D_01_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_01_도가 높게 판단된 경우, 특정 기준을 통해 제1 이 미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_01_도가 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. D_01_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_01_도이다. D_01_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미 지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_01_도를 비교하여, 노이즈 이미지를 분류 할 수 있다. 즉, (A)에 D_01_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_01_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정 점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_01_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_01_도를 기준으로 제 1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_01_도 중 적어D_01_도 하나를 기준으로 사전 설정 된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 이상과 같이, 본 명세서와 D_01_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_01_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_01_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_01_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_01_청구범위 D_01_청구항 1 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집을 적어D_01_도 하나의 학습 데이터 수집 장치에 요청하는 단계; 상기 학습 데이터 생성 장치가, 상기 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단 계; 상기 학습 데이터 생성 장치가, 상기 수신한 이미지들의 컬러 정보를 추출하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계; 를 포함하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 2 제1 항에 있어서, 상기 컬러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값인 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 3 제2 항에 있어서, 상기 분류하는 단계는 상기 이미지들 중 상기 컬러 정보의 유사D_01_도가 사전 설정된 값보다 높은 이미지들 중 적어D_01_도 하나를 노이즈 이미지로 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 4 제1 항에 있어서, 상기 컬러 정보를 추출하는 단계는 상기 적어D_01_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 상기 이미지들 의 파일명 및 컬러 정보를 저장하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 5 제4 항에 있어서, 상기 분류하는 단계는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 상기 동일한 파일명을 갖는 이미지 중 적어D_01_도 하나를 노이즈 이미지로 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 6 제3 항에 있어서, 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 상기 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀 스 데이터를 생성하고, 상기 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지 를 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 7 제6 항에 있어서, 상기 컬러 정보를 추출하는 단계는 상기 시퀀스 데이터의 상기 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 8 제7 항에 있어서, 상기 분류하는 단계는 상기 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 상기 이미지들의 유사D_01_도를 평가하 는 것을 특징으로 하는 데이터 분류 방법. D_01_청구항 9 제8 항에 있어서, 상기 분류하는 단계는 상기 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_01_도를 산출하고, 상기 산출된 유사D_01_도를 기준으로 상기 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정하는 것을 특징으로 하는, 데이터 분류 방법. D_01_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수 집을 적어D_01_도 하나의 학습 데이터 수집 장치에 요청하는 단계; 상기 프로세서가, 상기 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계; 상기 프로세서가, 상기 수신한 이미지들의 컬러 정보를 추출하는 단계; 및 상기 프로세서가, 상기 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 250, "content": "D_01_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 251, "content": "D_01_요약 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 방 법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습 (machine learning)을 위한 이미지의 수집을 적어D_01_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_01_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수신한 이미지들의 컬러 정보를 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 포함할 수 있다. D_01_대표D_01_도 D_01_도 21 D_01_도면 D_01_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 252, "content": "D_01_도 2 D_01_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 253, "content": "D_01_도 4 D_01_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 254, "content": "D_01_도 6 D_01_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 255, "content": "D_01_도 8 D_01_도 9"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 256, "content": "D_01_도 10 D_01_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 257, "content": "D_01_도 12 D_01_도 13"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 258, "content": "D_01_도 14 D_01_도 15"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 259, "content": "D_01_도 16 D_01_도 17"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 260, "content": "D_01_도 18 D_01_도 19"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 261, "content": "D_01_도 20 D_01_도 21 D_01_도 22"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 262, "content": "D_02_발명의 설명 D_02_발명의 명칭 수집 환경 기반 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for learning data classification based on collection environment, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 263, "content": "D_02_기술분야 *본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 정제에 관한 것이다. 보다 상세하게 는, 인공지능(AI)의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 수 집 환경 기반 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다.D_02_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_02_도 학습(supervised learning), 비지D_02_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_02_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_02_도와 운전차가 차량을 제어하는 정D_02_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_02_도 자동화(high automation), 그리고 레벨 5단계는 완전 자동 화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_02_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_02_도 한다. 그리고, 어노테이션 작업 결과물에 해당되 는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작업 은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_02_도에 의존하여 산출되고 있는 문제점이 있었 다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다. 그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_02_선행기술문헌 D_02_특허문헌 (특허문헌 1) 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 264, "content": "D_02_발명의 내용 D_02_해결하고자 하는 과제 본 발명의 일 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 수집 환경 기반 학습 데이터 분류 방법을 제공하는 것이다. 본 발명의 다른 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제 할 수 있는, 수집 환경 기반 학습 데이터 분류 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 265, "content": "D_02_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집 된 데이터 중 불필요한 데이터를 정제할 수 있는 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공 지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집 조건을 포함하는 가 이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상기 학습 데이터 생성 장치 가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단 계를 포함할 수 있다. 구체적으로, 상기 가이드 정보는 상기 수집 조건에 따른 샘플 이미지를 포함하고, 상기 이미지 정보는 파일 확 장자, 이미지 해상D_02_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적 어D_02_도 하나를 포함하고, 상기 분류하는 단계는 상기 샘플 이미지의 파일 확장자, 이미지 해상D_02_도, 픽셀 에 대한 RGB 값 및 컬러 코드 값 중 적어D_02_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 상기 추출된 샘플 이미지 정보를 상기 이미지들로부터 추출된 이미지 정보와 비교하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 연속된 복수의 이미지를 사전 설정된 개수로 그 룹핑(grouping) 한 시퀀스 데이터를 생성하고, 상기 시퀀스 데이터 별로 노이즈 이미지를 분류하는 것을 특징으 로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_02_도를 비교하고, 상기 전후 이미지 사이의 유사D_02_도가 사전 설정된 값보다 높되, 상기 전후 이미지와 상기 특정 이미지 사이 의 유사D_02_도가 사전 설정된 값보다 낮은 경우, 상기 특정 이미지를 노이즈 이미지로 판단하는 것을 특징으로한다. 상기 분류하는 단계는 상기 이미지들을 사전 설정된 해상D_02_도로 리샘플링(resamping)하고, 상기 리샘플링 된 이미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 상기 이미지들 간 유사D_02_도를 평가하 는 것을 특징으로 한다. 상기 추출하는 단계는 상기 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들 각각에 포함된 객체를 검출하고, 상기 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류하는 것을 특징으로 한다. 상기 수신하는 단계는 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 상기 이미지들과 함께 상기 이미 지들 각각의 메타 정보를 함께 수신하는 것을 특징으로 한다. 상기 메타 정보는 상기 이미지들 각각의 촬영 시점에서 상기 학습 데이터 수집 장치의 위치 정보 및 속D_02_도 정보를 포함하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 과속 방 지턱의 위치 정보를 기준으로 상기 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이 미지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 커브 (curve)길의 위치 정보를 기준으로 상기 메타 정보와 비교하여, 상기 커브길의 위치에서 생성된 이미지를 노이 즈 이미지로 분류하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터에서 연속된 이미지 사이의 유사D_02_도를 비교하고, 유사D_02_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 상기 검출된 이미지들이 커브길에서 촬영된 이미 지들로 판단하고, 상기 검출된 이미지들을 노이즈 이미지로 분류하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집 된 데이터 중 불필요한 데이터를 정제할 수 있는 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory) 및 상기 메모리에 상주된 명령어를 처리하는 프로세서 (processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서 가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수집 조건을 포함하는 가 이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 상기 프로세서가, 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 프로세서가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상기 프로세서가, 상기 이미지 정보를 상기 가이드 정보 와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨 터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_02_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 266, "content": "D_02_발명의 효과 본 발명의 실시예들에 따르면, 인공지능 학습을 위하여 복수의 학습 데이터 수집 장치로부터 수신한 이미지를 대상으로, 추출한 이미지 정보를 수집 조건을 포함하는 가이드 정보와 비교하여 노이즈 이미지를 분류함으로써, 수집 환경에 따른 불량 데이터를 효과적으로 정제할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 267, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 268, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 269, "content": "D_02_도면의 간단한 설명 D_02_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_02_도이다. D_02_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_02_도이다. D_02_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_02_도이다. D_02_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_02_도이다. D_02_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_02_도이다. D_02_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_02_도이다. D_02_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_02_도이다. D_02_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_02_도이다. D_02_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_02_도이다. D_02_도 10 내지 D_02_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_02_도이다. D_02_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_02_도이다. D_02_도 18 내지 D_02_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_02_도이 다. D_02_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_02_도이다. D_02_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_02_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 270, "content": "D_02_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_02_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_02_도하게 포괄적인 의미로 해석되거나, 과D_02_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_02_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_02_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_02_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_02_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_02_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_02_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_02_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_02_도면은 본 발명의 사상을 쉽게 이해할 수 있D_02_도록 하기 위한 것일 뿐, 첨부된 D_02_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_02_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_02_ 도 확장되는 것으로 해석되어야 한다. 한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다.이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_02_도에 의존하여 산출되고 있는 문제점이 있었다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_02_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_02_도이다. D_02_도 1에 D_02_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_02_도 감지 센서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_02_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이 미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_02_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한정 되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라D_02_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_02_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다. 그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_02_도 하나의 샘플 데이터를 수신하고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_02_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_02_도 하나의 프로젝트를 추출할 수 있다. 학습 데 이터 생성 장치는 추출된 적어D_02_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_02_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크 스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정 되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_02_도 2 및 D_02_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_02_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용자 의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수 있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_02_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨 (instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_02_도 4 및 D_02_도 5를 참조하여 후술하D_02_도록 한 다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_02_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_02_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_02_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을 기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_02_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_02_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_02_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_02_도이다. D_02_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데이 터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_02_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_02_도 감지 센서로부터 감지된 데이터를 수신할 수 있다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_02_도 하나의 샘플 데이터를 수신할 수 있다. 통신 부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행하 기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_02_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다. 다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_02_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘 플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데 이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정 되는 것D_02_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_02_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적 어D_02_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_02_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위 하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_02_도 를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_02_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상 이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_02_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_02_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment)를 비교하여 유사D_02_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_02_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치 에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_02_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_02_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인 공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처리를 수행할 수 있다. 즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이 러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다.데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_02_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수 행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_02_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검 출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수 의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비 용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_02_도 있다. 그리 고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_02_도 하나의 학습 데 이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가이 드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_02_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보를 샘플 이 미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_02_도록 할 수 있다. 또한, 데이터 수집부는 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 이때, 데이터 수집부는 적어D_02_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_02_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_02_도가 사전 설정된 값보다 높은 이미지들 중 적어D_02_도 하나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_02_도로 리샘플링(resamping)하고, 리샘플링 된 이 미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_02_도를 평가할 수 있 다. 또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_02_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_02_도 하나를 노이 즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록 되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_02_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_02_도가 사전 설정된 값보다 높은 이미지들의 선명D_02_도 (sharpness)를 산출하고, 산출된 선명D_02_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_02_도가 가장 높은 이미지를 제 외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_02_도를 산출하고, 산출된 유사 D_02_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_02_도가 사전 설정된 값보다 높 은 경우, 해당 이미지를 수집한 차량의 속D_02_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_02_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_02_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_02_도, 픽셀에 대한 RGB 값 및 컬 러 코드 값 중 적어D_02_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지 들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_02_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_02_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_02_도가 사전 설정된 값 보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_02_도를 비교하고, 전후 이미지 사이의 유사D_02_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_02_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_02_도가 낮은 경우, 특 정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_02_도로 리샘플링하고, 리샘플링 된 이 미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_02_도를 평가할 수 있 다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_02_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지 들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_02_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_02_도를 비교하고, 유사D_02_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들 로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_02_도를 기초로 노이즈 이미지를 분류하 고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_02_도를 비교하고, 제1 이미 지와 연속된 제2 이미지 사이의 유사D_02_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제 2 이미지의 유사D_02_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격하게 변 화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_02_도를 비교하고, 유사D_02_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬 영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변화 되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_02_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_02_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데이 터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매칭 하고, 매칭된 이미지 사이의 유사D_02_도를 비교하여 유사D_02_도가 사전 설정된 값보다 낮은 경우, 해당 이미 지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_02_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_02_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별하 기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_02_도를 평가하여 이미지의 양단부를 연결하는 객체 를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_02_도 중 적어D_02_도 하나를 기준으 로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다. 또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_02_도로, 건물, 가이드레일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이 의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_02_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_02_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_02_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_02_도 하나 의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하 는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다.본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프로 세서가, 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_02_도 하나의 샘플 데이터를 인공지능 학습 장치 로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비 교하고, 샘플 데이터와의 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_02_도 하나의 프 로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_02_도 하나의 프로젝트를 기초로 수행 예정인 어노 테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_02_도 3에 D_02_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_02_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_02_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_02_도록 구성될 수 있으며, 그 역D_02_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_02_도록 한다.D_02_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_02_도이다. D_02_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저장 부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다. 또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_02_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_02_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_02_도, 경D_02_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. *객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객 체의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. *다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_02_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역 에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제 하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽선 으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_02_도 하나의 점을 선택받고, 선택받은 적어D_02_도 하나의 점을 기초로 상기 경계선 을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_02_도 하나의 점을 연결하는 선을 생성하 고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_02_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하 고, 식별된 적어D_02_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로 밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_02_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다. 일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_02_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할 수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다.이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_02_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_02_도이다. D_02_도 5에 D_02_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_02_도 5에 D_02_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors),DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_02_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_02_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_02_도록 구성될 수 있으며, 그 역D_02_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_02_도록 한다. D_02_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_02_도이다. D_02_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_02_도가 사전 설정된 값보다 높은 이미지들 중 적어D_02_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_02_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_02_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이 미지들의 유사D_02_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_02_도가 사전 설정된 값보다 높은 이미지들의 선명D_02_도 (sharpness)를 산출하고, 산출된 선명D_02_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_02_도를 산출하고, 산출된 유 사D_02_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_02_도록 한다. D_02_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_02_도이다. D_02_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어 D_02_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_02_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_02_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_02_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_02_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미 지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사D_02_도 가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이 미지와 파일 확장자 또는 이미지 해상D_02_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_02_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분 류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_02_도를 비교하 고, 전후 이미지 사이의 유사D_02_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사 D_02_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_02_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_02_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 과속 방지 턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 커 브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미지 로 분류할 수 있다. *또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_02_도를 비교하고, 유사D_02_ 도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_02_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_02_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_02_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제2 이미지의 유사D_02_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_02_도를 비교하고, 유사D_02_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_02_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_02_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_02_도 중 적어D_02_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_02_도록 한다. D_02_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_02_도이다. D_02_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위한 적어D_02_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_02_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_02_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사D_02_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_02_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_02_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_02_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_02_도 하나의 프로젝트를 기초로 수행 예정인 어 노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습 데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_02_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_02_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전 체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_02_도록 한다. D_02_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_02_도이고, D_02_도 10 내지 D_02_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_02_도이다. 먼저, D_02_도 10에 D_02_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공지능 (AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_02_도 11에 D_02_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복 수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_02_도 하나의 점을 선 택받고, 선택받은 적어D_02_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_02_도 12에 D_02_도시된 바와 같이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_02_도 13에 D_02_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_02_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선으로 지정할 수 있다. 다음으로, D_02_도 14에 D_02_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계 선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선 과 구분되D_02_도록 출력할 수 있다. 다음으로, D_02_도 15에 D_02_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하 고, 식별된 적어D_02_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지 를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_02_도 16에 D_02_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제2 객 체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_02_도록 한다. D_02_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_02_도이고, D_02_도 18 내 지 D_02_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_02_도이다. 먼저, D_02_도 18에 D_02_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_02_도 19에 D_02_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_02_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고,작업자의 제어에 따라 복수의 점 중 적어D_02_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_02_도 20에 D_02_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영 역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_02_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_02_도이다. D_02_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_02_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_02_도가 사전 설정된 값보다 높 은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_02_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_02_도가 사전 설정된 값보다 높은 이미지들의 선명D_02_도 (sharpness)를 산출하고, 산출된 선명D_02_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_02_도가 높게 판단된 경우, 특정 기준을 통해 제1 이 미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_02_도가 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. D_02_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_02_도이다. D_02_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미 지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_02_도를 비교하여, 노이즈 이미지를 분류 할 수 있다. 즉, (A)에 D_02_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_02_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정 점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_02_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_02_도를 기준으로 제 1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_02_도 중 적어D_02_도 하나를 기준으로 사전 설정 된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 이상과 같이, 본 명세서와 D_02_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_02_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_02_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_02_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_02_청구범위 D_02_청구항 1 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하는 단계; 상기 학습 데이터 생성 장치가, 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단 계; 상기 학습 데이터 생성 장치가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 수집 환경에 따른 노 이즈 이미지를 분류하는 단계; 를 포함하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 2 제1 항에 있어서, 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 연속된 복수의 이미지를 사전 설정된 개수로 그룹핑(grouping) 한 시퀀 스 데이터를 생성하고, 상기 시퀀스 데이터 별로 노이즈 이미지를 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 3 제2 항에 있어서, 상기 분류하는 단계는 상기 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_02_도를 비교하고, 상기 전후 이미지 사이 의 유사D_02_도가 사전 설정된 값보다 높되, 상기 전후 이미지와 상기 특정 이미지 사이의 유사D_02_도가 사전 설정된 값보다 낮은 경우, 상기 특정 이미지를 노이즈 이미지로 판단하는 것을 특징으로 하는, 데이터 분류 방 법. D_02_청구항 4 제2 항에 있어서, 상기 추출하는 단계는 상기 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 5 제4 항에 있어서, 상기 분류하는 단계는 상기 이미지들 각각에 포함된 객체를 검출하고, 상기 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 6 제5 항에 있어서, 상기 수신하는 단계는 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 상기 이미지들과 함께 상기 이미지들 각각의 메타 정보 를 함께 수신하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 7 제6 항에 있어서, 상기 메타 정보는 상기 이미지들 각각의 촬영 시점에서 상기 학습 데이터 수집 장치의 위치 정보 및 속D_02_도 정보를 포함하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 8 제7 항에 있어서, 상기 분류하는 단계는 상기 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 과속 방지턱의 위치 정보를 기 준으로 상기 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 9 제7 항에 있어서, 상기 분류하는 단계는 상기 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_02_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 상기 메타 정보와 비교하여, 상기 커브길의 위치에서 생성된 이미지를 노이즈 이미지로 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_02_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수집 장치에 전송하는 단계; 상기 프로세서가, 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계; 상기 프로세서가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계; 및 상기 프로세서가, 상기 이미지 정보를 상기 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하 는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 271, "content": "D_02_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 272, "content": "D_02_요약 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 방 법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습 (machine learning)을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적어D_02_도 하나의 학습 데이터 수 집 장치에 전송하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_02_도 하나의 학습 데이터 수집 장치로부 터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 추출한 이미지 정보를 상기 가이드 정보 와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 포함할 수 있다. D_02_대표D_02_도 D_02_도 7 D_02_도면 D_02_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 273, "content": "D_02_도 2 D_02_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 274, "content": "D_02_도 4 D_02_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 275, "content": "D_02_도 6 D_02_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 276, "content": "D_02_도 8 D_02_도 9"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 277, "content": "D_02_도 10 D_02_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 278, "content": "D_02_도 12 D_02_도 13"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 279, "content": "D_02_도 14 D_02_도 15"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 280, "content": "D_02_도 16 D_02_도 17"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 281, "content": "D_02_도 18 D_02_도 19"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 282, "content": "D_02_도 20 D_02_도 21 D_02_도 22"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 283, "content": "D_03_발명의 설명 D_03_발명의 명칭 물리적 요인 기반 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for learning data classification based physical factor, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 284, "content": "D_03_기술분야 *본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 정제에 관한 것이다. 보다 상세하게 는, 인공지능(AI)의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 물 리적 요인 기반 학습 데이터 분류 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. D_03_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_03_도 학습(supervised learning), 비지D_03_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_03_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_03_도와 운전차가 차량을 제어하는 정D_03_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_03_도 자동화(high automation), 그리고 레벨 5단계는 완전 자동 화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_03_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_03_도 한다. 그리고, 어노테이션 작업 결과물에 해당되 는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작업은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작 업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_03_도에 의존하여 산출되고 있는 문제점이 있었 다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다. 그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_03_선행기술문헌 D_03_특허문헌 (특허문헌 1) 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 285, "content": "D_03_발명의 내용 D_03_해결하고자 하는 과제 본 발명의 일 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 물리적 요인 기반 학습 데이터 분류 방법을 제공하는 것이다. 본 발명의 다른 목적은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제 할 수 있는, 물리적 요인 기반 학습 데이터 분류 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 286, "content": "D_03_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 물리적 요인 기반 학습 데이터 분류 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상 기 학습 데이터 생성 장치가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 상기 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류하는 단계를 포함할 수 있다. 구체적으로, 상기 가이드 정보는 상기 수집 조건에 따른 샘플 이미지를 포함하고, 상기 이미지 정보는 파일 확 장자, 이미지 해상D_03_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적 어D_03_도 하나를 포함하고, 상기 분류하는 단계는 상기 샘플 이미지의 파일 확장자, 이미지 해상D_03_도, 픽셀 에 대한 RGB 값 및 컬러 코드 값 중 적어D_03_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 상기 추출된 샘플 이미지 정보를 상기 이미지들로부터 추출된 이미지 정보와 비교하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 연속된 복수의 이미지를 사전 설정된 개수로 그 룹핑(grouping) 한 시퀀스 데이터를 생성하고, 상기 시퀀스 데이터 별로 노이즈 이미지를 분류하는 것을 특징으 로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터 별 연속된 이미지의 유사D_03_도를 기초로 노이즈 이미지를 분류하고, 상기 분류된 노이즈 이미지 각각의 오류 종류를 추정하는 것을 특징으로 한다.상기 분류하는 단계는 상기 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 제1 이미지와 연 속된 제2 이미지 사이의 유사D_03_도가 사전 설정된 값보다 낮고, 상기 제2 이미지와 연속된 제3 이미지와 상기 제2 이미지의 유사D_03_도가 사전 설정된 값보다 높은 경우, 상기 시퀀스 데이터에 포함된 이미지를 촬영한 카 메라의 카메라 앵글(camera angle)이 변경된 오류로 판단하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 유사D_03_도가 사 전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 상기 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 추정된 오류 종류를 상기 노이즈 이미지 각각의 메타 정보에 포함시키는 것을 특징 으로 한다. 상기 분류하는 단계는 GPS(global positioning system) 좌표를 기초로 상기 수신한 이미지들 및 사전 저장된 이 미지를 매칭하고, 상기 매칭된 이미지 사이의 유사D_03_도를 비교하여, 노이즈 이미지를 분류하는 것을 특징으 로 한다. 상기 분류하는 단계는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값 의 유사D_03_도를 기준으로 상기 제1 변 및 상기 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 상기 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출하는 것을 특징으로 한다. 상기 분류하는 단계는 상기 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_03_도 중 적어D_03_도 하나를 기준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 상기 매칭된 이미지 각각에 포함된 객체를 식별하고, 상기 식별된 객체의 위치 변화 값을 기초로 상기 노이즈 이미지 를 분류하는 것을 특징으로 하는 것을 특징으로 한다. 상기 수신하는 단계는 상기 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신하고, 상기 분류하는 단계는 상기 3D 점군 데이터에 포함된 거리 정보를 기초로 상기 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 상기 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 물리적 요인 기반 학습 데이터 분류 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어 를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로 그램은 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 상기 프로세 서가, 상기 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 프로세서가, 상 기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상기 프로세서가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 상기 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하 는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_03_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 287, "content": "D_03_발명의 효과 본 발명의 실시예들에 따르면, 인공지능 학습을 위하여 복수의 학습 데이터 수집 장치로부터 수신한 이미지를 대상으로, 추출한 이미지 정보를 수집 조건을 포함하는 가이드 정보와 비교하여 노이즈 이미지를 분류함으로써, 수집 장치의 오류에 따른 불량 데이터를 효과적으로 정제할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 288, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 289, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 290, "content": "D_03_도면의 간단한 설명 D_03_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_03_도이다. D_03_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_03_도이다. D_03_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_03_도이다. D_03_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_03_도이다. D_03_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_03_도이다. D_03_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_03_도이다. D_03_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_03_도이다. D_03_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_03_도이다. D_03_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_03_도이다. D_03_도 10 내지 D_03_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_03_도이다. D_03_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_03_도이다. D_03_도 18 내지 D_03_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_03_도이 다. D_03_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_03_도이다. D_03_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_03_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 291, "content": "D_03_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_03_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_03_도하게 포괄적인 의미로 해석되거나, 과D_03_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_03_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_03_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_03_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_03_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_03_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_03_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_03_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_03_도면은 본 발명의 사상을 쉽게 이해할 수 있D_03_도록 하기 위한 것일 뿐, 첨부된 D_03_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_03_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_03_ 도 확장되는 것으로 해석되어야 한다.한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_03_도에 의존하여 산출되고 있는 문제점이 있었다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_03_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_03_도이다. D_03_도 1에 D_03_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_03_도 감지 센서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_03_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이 미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_03_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라D_03_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_03_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다. 그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_03_도 하나의 샘플 데이터를 수신하고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_03_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_03_도 하나의 프로젝트를 추출할 수 있다. 학습 데 이터 생성 장치는 추출된 적어D_03_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_03_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크 스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정 되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_03_도 2 및 D_03_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_03_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용자 의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수 있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_03_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨 (instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_03_도 4 및 D_03_도 5를 참조하여 후술하D_03_도록 한 다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_03_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_03_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_03_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을 기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_03_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_03_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_03_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_03_도이다. D_03_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데이 터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_03_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_03_도 감지 센서로부터 감지된 데이터를 수신할 수 있다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_03_도 하나의 샘플 데이터를 수신할 수 있다. 통신 부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행하 기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_03_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다. 다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_03_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘 플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데 이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정 되는 것D_03_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_03_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적 어D_03_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_03_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위 하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_03_도 를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_03_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상 이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_03_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_03_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment)를 비교하여 유사D_03_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_03_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치 에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_03_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_03_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인 공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별처리를 수행할 수 있다. 즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이 러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다. 데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_03_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수 행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_03_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검 출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수 의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비 용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_03_도 있다. 그리 고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_03_도 하나의 학습 데 이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가이 드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_03_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보를 샘플 이 미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_03_도록 할 수 있다. 또한, 데이터 수집부는 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 이때, 데이터 수집부는 적어D_03_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_03_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_03_도가 사전 설정된 값보다 높은 이미지들 중 적어D_03_도 하나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_03_도로 리샘플링(resamping)하고, 리샘플링 된 이 미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_03_도를 평가할 수 있 다.또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_03_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_03_도 하나를 노이 즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록 되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_03_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_03_도가 사전 설정된 값보다 높은 이미지들의 선명D_03_도 (sharpness)를 산출하고, 산출된 선명D_03_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_03_도가 가장 높은 이미지를 제 외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_03_도를 산출하고, 산출된 유사 D_03_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_03_도가 사전 설정된 값보다 높 은 경우, 해당 이미지를 수집한 차량의 속D_03_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_03_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_03_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_03_도, 픽셀에 대한 RGB 값 및 컬 러 코드 값 중 적어D_03_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지 들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_03_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_03_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_03_도가 사전 설정된 값 보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_03_도를 비교하고, 전후 이미지 사이의 유사D_03_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_03_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_03_도가 낮은 경우, 특 정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_03_도로 리샘플링하고, 리샘플링 된 이 미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_03_도를 평가할 수 있 다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_03_도를 통해 특정 이미지가 과속 방지턱을 넘 는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지 들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_03_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_03_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_03_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_03_도를 비교하고, 유사D_03_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들 로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_03_도를 기초로 노이즈 이미지를 분류하 고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 제1 이미 지와 연속된 제2 이미지 사이의 유사D_03_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제 2 이미지의 유사D_03_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격하게 변 화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 유사D_03_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬 영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변화 되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_03_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_03_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데이 터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매칭 하고, 매칭된 이미지 사이의 유사D_03_도를 비교하여 유사D_03_도가 사전 설정된 값보다 낮은 경우, 해당 이미 지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_03_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_03_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별하 기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_03_도를 평가하여 이미지의 양단부를 연결하는 객체 를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_03_도 중 적어D_03_도 하나를 기준으 로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다. 또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를 기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_03_도로, 건물, 가이드레일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이 의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_03_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_03_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_03_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_03_도 하나 의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하 는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프로 세서가, 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_03_도 하나의 샘플 데이터를 인공지능 학습 장치 로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비 교하고, 샘플 데이터와의 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_03_도 하나의 프 로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_03_도 하나의 프로젝트를 기초로 수행 예정인 어노 테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_03_도 3에 D_03_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_03_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_03_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_03_도록 구성될 수 있으며, 그 역D_03_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_03_도록 한다. D_03_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_03_도이다. D_03_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저장 부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다. 또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_03_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_03_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_03_도, 경D_03_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. *객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객 체의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는것은 아니다. 다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. *다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_03_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역 에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제 하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽선 으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_03_도 하나의 점을 선택받고, 선택받은 적어D_03_도 하나의 점을 기초로 상기 경계선 을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_03_도 하나의 점을 연결하는 선을 생성하 고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_03_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하 고, 식별된 적어D_03_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로 밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_03_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다.일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_03_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할 수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_03_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_03_도이다. D_03_도 5에 D_03_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_03_도 5에 D_03_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_03_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_03_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_03_도록 구성될 수 있으며, 그 역D_03_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_03_도록 한다. D_03_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_03_도이다. D_03_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_03_도가 사전 설정된 값보다 높은 이미지들 중 적어D_03_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_03_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_03_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들의 유사D_03_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_03_도가 사전 설정된 값보다 높은 이미지들의 선명D_03_도 (sharpness)를 산출하고, 산출된 선명D_03_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_03_도를 산출하고, 산출된 유 사D_03_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_03_도록 한다. D_03_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_03_도이다. D_03_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어 D_03_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_03_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_03_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_03_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_03_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미 지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사D_03_도 가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이 미지와 파일 확장자 또는 이미지 해상D_03_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_03_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분 류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_03_도를 비교하 고, 전후 이미지 사이의 유사D_03_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사 D_03_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_03_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_03_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_03_도 정보에 포함된 과속 방지 턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_03_도 정보에 포함된 커 브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미지 로 분류할 수 있다. *또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_03_도를 비교하고, 유사D_03_ 도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미 지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_03_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_03_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제2 이미지의 유사D_03_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 유사D_03_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_03_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_03_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_03_도 중 적어D_03_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_03_도록 한다. D_03_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_03_도이다. D_03_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위한 적어D_03_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_03_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_03_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사 D_03_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_03_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_03_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_03_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_03_도 하나의 프로젝트를 기초로 수행 예정인 어 노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습 데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_03_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_03_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전 체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_03_도록 한다. D_03_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_03_도이고, D_03_도 10 내지 D_03_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_03_도이다. 먼저, D_03_도 10에 D_03_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공지능 (AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_03_도 11에 D_03_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복 수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_03_도 하나의 점을 선 택받고, 선택받은 적어D_03_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_03_도 12에 D_03_도시된 바와 같이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_03_도 13에 D_03_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_03_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선으로 지정할 수 있다.다음으로, D_03_도 14에 D_03_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계 선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선 과 구분되D_03_도록 출력할 수 있다. 다음으로, D_03_도 15에 D_03_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하 고, 식별된 적어D_03_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지 를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_03_도 16에 D_03_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제2 객 체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_03_도록 한다. D_03_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_03_도이고, D_03_도 18 내 지 D_03_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_03_도이다. 먼저, D_03_도 18에 D_03_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_03_도 19에 D_03_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다.또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_03_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_03_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_03_도 20에 D_03_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영 역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_03_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_03_도이다. D_03_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_03_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_03_도가 사전 설정된 값보다 높 은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_03_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_03_도가 사전 설정된 값보다 높은 이미지들의 선명D_03_도 (sharpness)를 산출하고, 산출된 선명D_03_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_03_도가 높게 판단된 경우, 특정 기준을 통해 제1 이 미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_03_도가 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. D_03_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_03_도이다. D_03_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미 지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_03_도를 비교하여, 노이즈 이미지를 분류 할 수 있다. 즉, (A)에 D_03_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_03_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정 점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_03_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_03_도를 기준으로 제 1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_03_도 중 적어D_03_도 하나를 기준으로 사전 설정 된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 이상과 같이, 본 명세서와 D_03_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_03_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_03_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_03_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_03_청구범위 D_03_청구항 1 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계; 상기 학습 데이터 생성 장치가, 상기 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단 계; 상기 학습 데이터 생성 장치가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 상기 학습 데이터 수 집 장치의 물리적 요인에 따른 노이즈 이미지를 분류하는 단계; 를 포함하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 2 제1 항에 있어서, 상기 분류하는 단계는 상기 이미지들을 시간 순으로 나열하고, 연속된 복수의 이미지를 사전 설정된 개수로 그룹핑(grouping) 한 시퀀 스 데이터를 생성하고, 상기 시퀀스 데이터 별로 노이즈 이미지를 분류하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 3 제2항에 있어서, 상기 분류하는 단계는 상기 시퀀스 데이터 별 연속된 이미지의 유사D_03_도를 기초로 노이즈 이미지를 분류하고, 상기 분류된 노이즈 이미지 각각의 오류 종류를 추정하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 4 제3 항에 있어서, 상기 분류하는 단계는 상기 시퀀스 데이터 중 연속된 이미지 사이의 유사D_03_도를 비교하고, 제1 이미지와 연속된 제2 이미지 사이의 유사D_03_도가 사전 설정된 값보다 낮고, 상기 제2 이미지와 연속된 제3 이미지와 상기 제2 이미지의 유사D_03_ 도가 사전 설정된 값보다 높은 경우, 상기 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글 (camera angle)이 변경된 오류로 판단하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 5 제3 항에 있어서, 상기 분류하는 단계는 상기 추정된 오류 종류를 상기 노이즈 이미지 각각의 메타 정보에 포함시키는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 6 제1 항에 있어서, 상기 분류하는 단계는 GPS(global positioning system) 좌표를 기초로 상기 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 상기 매칭된 이미지 사이의 유사D_03_도를 비교하여, 노이즈 이미지를 분류하는 것을 특징으로 하는, 데이터 분류 방 법. D_03_청구항 7 제6 항에 있어서, 상기 분류하는 단계는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_03_도를 기준 으로 상기 제1 변 및 상기 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 상기 제1 변 및 상기 제2 변 으로부터 각각 식별된 두 정점을 연결한 선분을 추출하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 8 제7 항에 있어서, 상기 분류하는 단계는 상기 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_03_도 중 적어D_03_도 하나를 기준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 9 제6 항에 있어서, 상기 추출하는 단계는 상기 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 상기 매칭된 이미지 각각에 포함된 객체를 식별하고, 상기 식별된 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류하는 것을 특징 으로 하는 것을 특징으로 하는, 데이터 분류 방법. D_03_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계; 상기 프로세서가, 상기 적어D_03_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계; 상기 프로세서가, 상기 수집 조건과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계; 및 상기 프로세서가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 상기 이미지들 중 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로 그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 292, "content": "D_03_요약서 D_03_요약 본 발명은 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는, 물리적 요인 기반 학습 데이터 분류 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적어 D_03_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 상기 학습 데이터 생성 장치가, 상기 적어D_03_도 하 나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 상기 학습 데이터 생성 장치가, 상기 수집 조건 과 대응하는 이미지 정보를 상기 이미지들로부터 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 상기 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지 를 분류하는 단계를 포함할 수 있다. D_03_대표D_03_도 D_03_도 22 D_03_도면 D_03_도 1 D_03_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 293, "content": "D_03_도 3 D_03_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 294, "content": "D_03_도 5 D_03_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 295, "content": "D_03_도 7 D_03_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 296, "content": "D_03_도 9 D_03_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 297, "content": "D_03_도 11 D_03_도 12"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 298, "content": "D_03_도 13 D_03_도 14"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 299, "content": "D_03_도 15 D_03_도 16"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 300, "content": "D_03_도 17 D_03_도 18"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 301, "content": "D_03_도 19 D_03_도 20"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 302, "content": "D_03_도 21 D_03_도 22"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 303, "content": "D_04_발명의 설명 D_04_발명의 명칭 학습 데이터 생성에 관한 작업 비용 예측 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램 {Method for estimate job cost of work on generating training data, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 304, "content": "D_04_기술분야 *본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 설계에 관한 것이다. 보다 상세하게 는, 인공지능(AI) 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는, 학습 데이터 생성에 관한 작업 비용 예측 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로 그램에 관한 것이다.D_04_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_04_도 학습(supervised learning), 비지D_04_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_04_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_04_도와 운전차가 차량을 제어하는 정D_04_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_04_도 자동화(high automation), 그리고 레벨 5단계는 완전 자동 화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_04_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_04_도 한다. 그리고, 어노테이션 작업 결과물에 해당되 는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작업 은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_04_도에 의존하여 산출되고 있는 문제점이 있었 다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다. 그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_04_선행기술문헌 D_04_특허문헌 (특허문헌 1) 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 305, "content": "D_04_발명의 내용 D_04_해결하고자 하는 과제 본 발명의 일 목적은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으 로 예측할 수 있는, 학습 데이터 생성에 관한 작업 비용 예측 방법을 제공하는 것이다. 본 발명의 다른 목적은 일 목적은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비 용을 합리적으로 예측할 수 있는, 학습 데이터 생성에 관한 작업 비용 예측 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 306, "content": "D_04_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관 한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는, 학습 데이터 생성에 관한 작업 비용 예측 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 수 신하는 단계, 상기 학습 데이터 생성 장치가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데 이터와 비교하고, 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어 D_04_도 하나의 프로젝트를 추출하는 단계 및 상기 학습 데이터 생성 장치가, 상기 추출된 적어D_04_도 하나의 프로젝트를 기초로 상기 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작 업 비용을 예측하는 단계를 포함할 수 있다. 구체적으로, 상기 추출하는 단계는 상기 기존 데이터를 대상으로, 상기 기존 데이터를 구성하고 있는 하나 이상 의 분해 구성요소를 식별하고, 상기 기존 데이터로부터 식별된 분해 구성요소와 상기 샘플 데이터로부터 식별된 분해 구성요소를 서로 비교하는 것을 특징으로 한다. 상기 샘플 데이터는 상기 어노테이션 작업의 대상이 되는 샘플 이미지에 해당되고, 상기 추출하는 단계는 상기 샘플 이미지를 대상으로 상기 어노테이션 작업을 수행한 후, 상기 어노테이션 작업에 의해 상기 샘플 이미지로 부터 특정된 객체(object)의 클래스(class)와, 상기 객체를 특정하기 위해 사용된 툴(tool)을 상기 분해 구성요 소로 식별하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 인공지능 학습 장치로부터 상기 샘플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 상기 입력받은 가중치를 고려하여, 상기 기존 이미지와의 유사D_04_도를 평가하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 샘플 이미지의 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 상기 샘플 데이 터에 포함된 객체를 검출하고, 상기 객체의 RGB(Red, Green, Blue) 값을 상기 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_04_도를 평가하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 상기 샘플 이미지 와 비교하고, 상기 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 상기 인공지 능 학습 장치에 전송하고, 상기 인공지능 학습 장치로부터 상기 복수의 대표 이미지 중 하나를 선택받는 것을 특징으로 한다. 상기 대표 이미지는 상기 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 상기 기 수행된 복 수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_04_도가 가장 높은 이미지인 것을 특징으로 한다. 상기 추출하는 단계는 상기 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 상 기 인공지능 학습 장치에 전송하되, 상기 대표 이미지에 포함된 객체를 식별하고, 상기 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 상기 식별된 객체를 비식별(de-identify) 처리하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처리를 수행하는 것을 특징으로 한다. 상기 기밀 정보는 상기 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 상기 학습 데이터 생성 장치에 의해 미리 설정되는 것을 특징으로 한다. 상기 예측하는 단계는 상기 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력받고, 상기 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 상기 전체 작업 비용을 예측하는 것을 특징으로 한다. 상기 추출하는 단계는 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복 수의 프로젝트를 추출하고, 상기 예측하는 단계는 상기 복수의 프로젝트의 작업 비용 평균값을 상기 전체 작업 비용으로 예측하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관 한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어를 처리하는 프 로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프 로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신하는 단계, 상기 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출 하는 단계 및 상기 프로세서가, 상기 추출된 적어D_04_도 하나의 프로젝트를 기초로 상기 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_04_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 307, "content": "D_04_발명의 효과 본 발명의 실시예들에 따르면, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출함에 있어, 기 수행된 프로젝트의 작업 비용을 기준으로 전체 작업 비용을 예측함으로써, 수행 예정 인 프로젝트의 전체 작업 비용을 보다 합리적으로 예측할 수 있게 된다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 308, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 309, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 310, "content": "D_04_도면의 간단한 설명 D_04_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_04_도이다. D_04_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_04_도이다. D_04_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_04_도이다. D_04_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_04_도이다. D_04_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_04_도이다. D_04_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_04_도이다. D_04_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_04_도이다. D_04_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_04_도이다. D_04_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_04_도이다. D_04_도 10 내지 D_04_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_04_도이다. D_04_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_04_도이다. D_04_도 18 내지 D_04_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_04_도이 다. D_04_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_04_도이다. D_04_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_04_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 311, "content": "D_04_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_04_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_04_도하게 포괄적인 의미로 해석되거나, 과D_04_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_04_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_04_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_04_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_04_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_04_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_04_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_04_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_04_도면은 본 발명의 사상을 쉽게 이해할 수 있D_04_도록 하기 위한 것일 뿐, 첨부된 D_04_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_04_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_04_ 도 확장되는 것으로 해석되어야 한다.한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_04_도에 의존하여 산출되고 있는 문제점이 있었다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_04_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_04_도이다. D_04_도 1에 D_04_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_04_도 감지 센서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_04_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이 미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_04_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한정 되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라할지라D_04_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_04_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_04_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_04_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다. 그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 수신하고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_04_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출할 수 있다. 학습 데 이터 생성 장치는 추출된 적어D_04_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_04_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크 스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정 되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_04_도 2 및 D_04_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_04_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용자 의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_04_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨 (instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_04_도 4 및 D_04_도 5를 참조하여 후술하D_04_도록 한 다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_04_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_04_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_04_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_04_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_04_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_04_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_04_도이다. D_04_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데이 터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_04_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_04_도 감지 센서로부터 감지된 데이터를 수신할 수 있다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_04_도 하나의 샘플 데이터를 수신할 수 있다. 통신 부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행하 기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_04_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다. 다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘 플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데 이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정 되는 것D_04_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적 어D_04_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_04_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위 하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_04_도 를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_04_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상 이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_04_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_04_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment)를 비교하여 유사D_04_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치 에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_04_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인 공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처리를 수행할 수 있다. 즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다. 데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_04_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수 행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_04_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검 출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수 의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비 용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_04_도 있다. 그리 고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_04_도 하나의 학습 데 이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가이 드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_04_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보를 샘플 이 미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_04_도록 할 수 있다. 또한, 데이터 수집부는 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 이때, 데이터 수집부는 적어D_04_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_04_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_04_도가 사전 설정된 값보다 높은 이미지들 중 적어D_04_도 하나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_04_도로 리샘플링(resamping)하고, 리샘플링 된 이 미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_04_도를 평가할 수 있 다. 또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_04_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_04_도 하나를 노이 즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_04_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_04_도가 사전 설정된 값보다 높은 이미지들의 선명D_04_도 (sharpness)를 산출하고, 산출된 선명D_04_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_04_도가 가장 높은 이미지를 제 외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_04_도를 산출하고, 산출된 유사 D_04_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_04_도가 사전 설정된 값보다 높 은 경우, 해당 이미지를 수집한 차량의 속D_04_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_04_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_04_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_04_도, 픽셀에 대한 RGB 값 및 컬 러 코드 값 중 적어D_04_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지 들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_04_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_04_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_04_도가 사전 설정된 값 보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_04_도를 비교하고, 전후 이미지 사이의 유사D_04_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_04_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_04_도가 낮은 경우, 특 정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_04_도로 리샘플링하고, 리샘플링 된 이 미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_04_도를 평가할 수 있 다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_04_도를 통해 특정 이미지가 과속 방지턱을 넘 는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지 들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_04_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_04_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_04_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_04_도를 비교하고, 유사D_04_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들 로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_04_도를 기초로 노이즈 이미지를 분류하 고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_04_도를 비교하고, 제1 이미 지와 연속된 제2 이미지 사이의 유사D_04_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제 2 이미지의 유사D_04_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격하게 변 화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_04_도를 비교하고, 유사D_04_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬 영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변화 되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_04_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_04_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데이 터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매칭 하고, 매칭된 이미지 사이의 유사D_04_도를 비교하여 유사D_04_도가 사전 설정된 값보다 낮은 경우, 해당 이미 지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_04_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_04_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별하 기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_04_도를 평가하여 이미지의 양단부를 연결하는 객체 를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_04_도 중 적어D_04_도 하나를 기준으 로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다.또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를 기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_04_도로, 건물, 가이드레일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이 의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_04_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_04_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_04_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_04_도 하나 의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_04_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하 는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_04_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프로 세서가, 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 인공지능 학습 장치 로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비 교하고, 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프 로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_04_도 하나의 프로젝트를 기초로 수행 예정인 어노 테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_04_도 3에 D_04_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_04_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_04_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_04_도록 구성될 수 있으며, 그 역D_04_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_04_도록 한다. D_04_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_04_도이다. D_04_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저장 부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다. 또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_04_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_04_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_04_도, 경D_04_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. *객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객 체의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다.*다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_04_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역 에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제 하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽선 으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_04_도 하나의 점을 선택받고, 선택받은 적어D_04_도 하나의 점을 기초로 상기 경계선 을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_04_도 하나의 점을 연결하는 선을 생성하 고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_04_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하 고, 식별된 적어D_04_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로 밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_04_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다. 일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_04_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할 수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_04_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_04_도이다. D_04_도 5에 D_04_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_04_도 5에 D_04_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_04_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_04_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_04_도록 구성될 수 있으며, 그 역D_04_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_04_도록 한다. D_04_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_04_도이다. D_04_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_04_도가 사전 설정된 값보다 높은 이미지들 중 적어D_04_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_04_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_04_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이 미지들의 유사D_04_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_04_도가 사전 설정된 값보다 높은 이미지들의 선명D_04_도 (sharpness)를 산출하고, 산출된 선명D_04_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다.또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_04_도를 산출하고, 산출된 유 사D_04_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_04_도록 한다. D_04_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_04_도이다. D_04_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어 D_04_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_04_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_04_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_04_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_04_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미 지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사D_04_도 가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이 미지와 파일 확장자 또는 이미지 해상D_04_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_04_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분 류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_04_도를 비교하 고, 전후 이미지 사이의 유사D_04_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사 D_04_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_04_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_04_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_04_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_04_도 정보에 포함된 과속 방지 턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_04_도 정보에 포함된 커 브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미지 로 분류할 수 있다.*또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_04_도를 비교하고, 유사D_04_ 도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미 지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_04_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_04_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_04_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제2 이미지의 유사D_04_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_04_도를 비교하고, 유사D_04_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_04_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_04_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_04_도 중 적어D_04_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_04_도록 한다. D_04_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_04_도이다. D_04_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사 D_04_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_04_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_04_도 하나의 프로젝트를 기초로 수행 예정인 어 노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습 데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_04_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전 체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_04_도록 한다. D_04_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_04_도이고, D_04_도 10 내지 D_04_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_04_도이다. 먼저, D_04_도 10에 D_04_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공지능 (AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_04_도 11에 D_04_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복 수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_04_도 하나의 점을 선 택받고, 선택받은 적어D_04_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_04_도 12에 D_04_도시된 바와 같이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_04_도 13에 D_04_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_04_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선으로 지정할 수 있다. 다음으로, D_04_도 14에 D_04_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계 선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선 과 구분되D_04_도록 출력할 수 있다.다음으로, D_04_도 15에 D_04_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하 고, 식별된 적어D_04_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지 를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_04_도 16에 D_04_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제2 객 체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_04_도록 한다. D_04_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_04_도이고, D_04_도 18 내 지 D_04_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_04_도이다. 먼저, D_04_도 18에 D_04_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_04_도 19에 D_04_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다.그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_04_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_04_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_04_도 20에 D_04_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영 역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_04_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_04_도이다. D_04_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_04_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_04_도가 사전 설정된 값보다 높 은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_04_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_04_도가 사전 설정된 값보다 높은 이미지들의 선명D_04_도 (sharpness)를 산출하고, 산출된 선명D_04_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_04_도가 높게 판단된 경우, 특정 기준을 통해 제1 이 미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_04_도가 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. D_04_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_04_도이다. D_04_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미 지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_04_도를 비교하여, 노이즈 이미지를 분류 할 수 있다. 즉, (A)에 D_04_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_04_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정 점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_04_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_04_도를 기준으로 제 1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_04_도 중 적어D_04_도 하나를 기준으로 사전 설정 된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 이상과 같이, 본 명세서와 D_04_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_04_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_04_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_04_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_04_청구범위 D_04_청구항 1 학습 데이터 생성 장치가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 수신하는 단계; 상기 학습 데이터 생성 장치가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하 고, 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출하는 단계; 및 상기 학습 데이터 생성 장치가, 상기 추출된 적어D_04_도 하나의 프로젝트를 기초로 상기 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계; 를 포함하는 것을 특 징으로 하는, 작업 비용 예측 방법. D_04_청구항 2 제1 항에 있어서, 상기 추출하는 단계는 상기 기존 데이터를 대상으로, 상기 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소를 식별하고, 상기 기존 데이터로부터 식별된 분해 구성요소와 상기 샘플 데이터로부터 식별된 분해 구성요소를 서로 비교하는 것 을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 3 제2 항에 있어서, 상기 샘플 데이터는 상기 어노테이션 작업의 대상이 되는 샘플 이미지에 해당되고, 상기 추출하는 단계는 상기 샘플 이미지를 대상으로 상기 어노테이션 작업을 수행한 후, 상기 어노테이션 작업에 의해 상기 샘플 이미 지로부터 특정된 객체(object)의 클래스(class)와, 상기 객체를 특정하기 위해 사용된 툴(tool)을 상기 분해 구 성요소로 식별하는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 4 제1 항에 있어서, 상기 추출하는 단계는 상기 샘플 이미지의 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 상기 샘플 데이터에 포함된 객체를 검 출하고, 상기 객체의 RGB(Red, Green, Blue) 값을 상기 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사 D_04_도를 평가하는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 5 제1 항에 있어서, 상기 추출하는 단계는 상기 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 상기 샘플 이미지와 비교하고, 상기 샘 플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 상기 인공지능 학습 장치에 전송 하고, 상기 인공지능 학습 장치로부터 상기 복수의 대표 이미지 중 하나를 선택받는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 6 제5 항에 있어서, 상기 대표 이미지는 상기 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 상기 기 수행된 복수의 프로젝트 각각 을 수행하기 위하여 수신한 샘플 이미지와 유사D_04_도가 가장 높은 이미지인 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 7 제5 항에 있어서, 상기 추출하는 단계는 상기 샘플 이미지와의 유사D_04_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 상기 인공지능 학습 장치 에 전송하되, 상기 대표 이미지에 포함된 객체를 식별하고, 상기 식별된 객체의 클래스가 기밀 정보로 사전 등 록된 경우, 상기 식별된 객체를 비식별(de-identify) 처리하는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 8 제1 항에 있어서, 상기 예측하는 단계는 상기 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력받고, 상기 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 상기 전체 작업 비용을 예측하는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 9 제1 항에 있어서, 상기 추출하는 단계는 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하 고, 상기 예측하는 단계는 상기 복수의 프로젝트의 작업 비용 평균값을 상기 전체 작업 비용으로 예측하는 것을 특징으로 하는, 작업 비용 예측 방법. D_04_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신하 는 단계; 상기 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 상기 샘플 데이터와의 유사D_04_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출 하는 단계; 및 상기 프로세서가, 상기 추출된 적어D_04_도 하나의 프로젝트를 기초로 상기 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계; 를 실행시키기 위하여, 기록매체 에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 312, "content": "D_04_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 313, "content": "D_04_요약 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는, 학습 데이터 생성에 관한 작업 비용 예측 방법을 제안한다. 상기 방법은 학습 데이터 생성 장치가, 인 공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로 젝트를 수행하기 위한 적어D_04_도 하나의 샘플 데이터를 수신하는 단계, 상기 학습 데이터 생성 장치가, 기 수 행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 상기 샘플 데이터와의 유사D_04_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_04_도 하나의 프로젝트를 추출하는 단계 및 상기 학 습 데이터 생성 장치가, 상기 추출된 적어D_04_도 하나의 프로젝트를 기초로 상기 수행 예정인 어노테이션 작업 과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 포함할 수 있다. D_04_대표D_04_도 D_04_도 8 D_04_도면 D_04_도 1 D_04_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 314, "content": "D_04_도 3 D_04_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 315, "content": "D_04_도 5 D_04_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 316, "content": "D_04_도 7 D_04_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 317, "content": "D_04_도 9 D_04_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 318, "content": "D_04_도 11 D_04_도 12"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 319, "content": "D_04_도 13 D_04_도 14"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 320, "content": "D_04_도 15 D_04_도 16"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 321, "content": "D_04_도 17 D_04_도 18"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 322, "content": "D_04_도 19 D_04_도 20"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 323, "content": "D_04_도 21 D_04_도 22"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 324, "content": "D_06_발명의 설명 D_06_발명의 명칭 경계선 지정을 통한 어노테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for annotation using boundary designation, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 325, "content": "D_06_기술분야 *본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 가공에 관한 것이다. 보다 상세하게 는, 인공지능(AI) 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다.D_06_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_06_도 학습(supervised learning), 비지D_06_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_06_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_06_도와 운전차가 차량을 제어하는 정D_06_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_06_도 자동화(high automation), 그리고 레벨 5단계는 완전 자동 화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_06_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_06_도 한다. 그리고, 어노테이션 작업 결과물에 해당되 는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작업 은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_06_도에 의존하여 산출되고 있는 문제점이 있었 다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다. 그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_06_선행기술문헌 D_06_특허문헌 (특허문헌 1) 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 326, "content": "D_06_발명의 내용 D_06_해결하고자 하는 과제 본 발명의 일 목적은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용 이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제공하는 것이다. 본 발명의 다른 목적은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 327, "content": "D_06_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정 할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제안한다. 상기 방법은 어노테이션 장치가, 작업자의 제어 에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 어노테이션 장치가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계 및 상기 어노테이션 장치가, 상기 지정 된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 포함할 수 있 다. 구체적으로, 상기 외곽선을 지정하는 단계는 상기 작업자로부터 상기 복수의 객체를 포함하는 외곽선을 따라 복 수의 점을 입력 받고, 상기 복수의 점을 연결하여, 상기 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 이미지의 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 복수의 객체를 선택받고, 상기 추출된 엣지를 기초로 상기 선택받은 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 상기 작업자로부터 상기 복수의 점군 중 복수의 객체를 선택받고, 상기 식별된 복수의 점군을 기초로 상기 선택받은 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하 고, 상기 바운딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 상기 배경을 삭제하여 상기 복수의 객체의 외곽선을 지정하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 추출된 엣지를 기초로 상기 복수의 객체에 대한 외곽선 내에서 경계선을 식별하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 작업자로부터 상기 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력받고, 상기 입력받은 복수의 점을 연결하여 상기 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 상기 복수의 점군 사이의 경계선을 상기 복수의 객체 사이의 경계선으로 지정하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 상 기 인식된 객체의 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 상기 작업자로부터 상기 복 수의 점군 중 하나의 객체를 선택받고, 상기 선택받은 객체의 점군을 기초로 상기 경계선을 생성하는 것을 특징 으로 한다. 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부의 엣지를 추출하고, 상기 추출된 엣지를 기초로 적어 D_06_도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 하나의 객체를 선택받고, 상기 추출 된 엣지를 기초로 상기 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 상 기 작업자의 제어에 따라 상기 복수의 점 중 적어D_06_도 하나의 점을 이동시켜 상기 경계선을 수정하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정 할 수 있는, 경계선 지정을 통한 어노테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제 안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서 (processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서 가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업 의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 프로세 서가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계 및 상기 프로세서가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_06_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 328, "content": "D_06_발명의 효과 본 발명의 실시예들에 따르면, 어노테이션을 수행함에 있어서, 이미지 내에 포함된 서로 중첩되어 배치되는 복 수의 객체를 대상으로, 복수의 객체 전체에 대한 외곽선을 지정한 후에, 각 객체의 경계선을 지정하여 각 객체 를 구분함으로써, 중첩되어 배치되는 개체를 용이하게 식별할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 329, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 330, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 331, "content": "D_06_도면의 간단한 설명 D_06_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_06_도이다. D_06_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_06_도이다. D_06_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_06_도이다. D_06_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_06_도이다. D_06_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_06_도이다. D_06_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06_도이다. D_06_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06_도이다. D_06_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_06_도이다. D_06_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06_도이다. D_06_도 10 내지 D_06_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06_도이다. D_06_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06_도이다. D_06_도 18 내지 D_06_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06_도이 다. D_06_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06_도이다. D_06_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 332, "content": "D_06_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_06_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_06_도하게 포괄적인 의미로 해석되거나, 과D_06_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_06_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_06_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_06_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_06_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_06_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_06_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_06_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_06_도면은 본 발명의 사상을 쉽게 이해할 수 있D_06_도록 하기 위한 것일 뿐, 첨부된 D_06_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_06_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_06_ 도 확장되는 것으로 해석되어야 한다. 한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다.이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_06_도에 의존하여 산출되고 있는 문제점이 있었다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_06_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_06_도이다. D_06_도 1에 D_06_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_06_도 감지 센서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_06_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이 미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_06_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한정 되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라D_06_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_06_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_06_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_06_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조 건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다. 그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_06_도 하나의 샘플 데이터를 수신하고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_06_ 도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06_도 하나의 프로젝트를 추출할 수 있다. 학습 데 이터 생성 장치는 추출된 적어D_06_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_06_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크 스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정 되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_06_도 2 및 D_06_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_06_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용자 의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수 있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_06_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨 (instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_06_도 4 및 D_06_도 5를 참조하여 후술하D_06_도록 한 다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_06_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_06_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_06_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을 기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_06_도 허용될 수 있다. 예를 들어, 인공지능 학습 장 치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_06_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_06_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_06_도이다. D_06_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데이 터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_06_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_06_도 감지 센서로부터 감지된 데이터를 수신할 수 있다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_06_도 하나의 샘플 데이터를 수신할 수 있다. 통신 부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행하 기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_06_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다. 다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_06_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘 플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데 이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정 되는 것D_06_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적 어D_06_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_06_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위 하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_06_도 를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_06_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상 이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_06_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_06_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment)를 비교하여 유사D_06_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_06_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치 에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_06_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_06_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인 공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처리를 수행할 수 있다. 즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이 러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다.데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_06_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관 련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수 행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_06_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검 출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수 의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비 용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_06_도 있다. 그리 고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_06_도 하나의 학습 데 이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가이 드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_06_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보를 샘플 이 미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_06_도록 할 수 있다. 또한, 데이터 수집부는 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 이때, 데이터 수집부는 적어D_06_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_06_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_06_도가 사전 설정된 값보다 높은 이미지들 중 적어D_06_도 하나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_06_도로 리샘플링(resamping)하고, 리샘플링 된 이 미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_06_도를 평가할 수 있 다. 또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06_도 하나를 노이 즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록 되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_06_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_06_도가 사전 설정된 값보다 높은 이미지들의 선명D_06_도 (sharpness)를 산출하고, 산출된 선명D_06_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_06_도가 가장 높은 이미지를 제 외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_06_도를 산출하고, 산출된 유사 D_06_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_06_도가 사전 설정된 값보다 높 은 경우, 해당 이미지를 수집한 차량의 속D_06_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_06_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_06_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_06_도, 픽셀에 대한 RGB 값 및 컬 러 코드 값 중 적어D_06_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지 들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_06_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_06_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_06_도가 사전 설정된 값 보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_06_도를 비교하고, 전후 이미지 사이의 유사D_06_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_06_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_06_도가 낮은 경우, 특 정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_06_도로 리샘플링하고, 리샘플링 된 이 미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_06_도를 평가할 수 있 다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_06_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지 들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_06_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_06_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_06_도를 비교하고, 유사D_06_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들 로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_06_도를 기초로 노이즈 이미지를 분류하 고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06_도를 비교하고, 제1 이미 지와 연속된 제2 이미지 사이의 유사D_06_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제 2 이미지의 유사D_06_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격하게 변 화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06_도를 비교하고, 유사D_06_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬 영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변화 되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_06_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데이 터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매칭 하고, 매칭된 이미지 사이의 유사D_06_도를 비교하여 유사D_06_도가 사전 설정된 값보다 낮은 경우, 해당 이미 지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_06_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별하 기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_06_도를 평가하여 이미지의 양단부를 연결하는 객체 를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_06_도 중 적어D_06_도 하나를 기준으 로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다. 또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_06_도로, 건물, 가이드레일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이 의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_06_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_06_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_06_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_06_도 하나 의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_06_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하 는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다.본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_06_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프로 세서가, 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_06_도 하나의 샘플 데이터를 인공지능 학습 장치 로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비 교하고, 샘플 데이터와의 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06_도 하나의 프 로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_06_도 하나의 프로젝트를 기초로 수행 예정인 어노 테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행시키기 위 하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_06_도 3에 D_06_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_06_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_06_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_06_도록 구성될 수 있으며, 그 역D_06_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_06_도록 한다.D_06_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_06_도이다. D_06_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저장 부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다. 또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_06_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_06_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_06_도, 경D_06_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. *객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객 체의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. *다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_06_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역 에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제 하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽선 으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_06_도 하나의 점을 선택받고, 선택받은 적어D_06_도 하나의 점을 기초로 상기 경계선 을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_06_도 하나의 점을 연결하는 선을 생성하 고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_06_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하 고, 식별된 적어D_06_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로 밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_06_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다. 일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_06_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할 수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다.이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_06_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_06_도이다. D_06_도 5에 D_06_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_06_도 5에 D_06_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors),DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_06_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_06_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_06_도록 구성될 수 있으며, 그 역D_06_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_06_도록 한다. D_06_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06_도이다. D_06_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_06_도가 사전 설정된 값보다 높은 이미지들 중 적어D_06_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이 미지들의 유사D_06_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_06_도가 사전 설정된 값보다 높은 이미지들의 선명D_06_도 (sharpness)를 산출하고, 산출된 선명D_06_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_06_도를 산출하고, 산출된 유 사D_06_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_06_도록 한다. D_06_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06_도이다. D_06_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장 치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어 D_06_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들 을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_06_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_06_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_06_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_06_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미 지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사D_06_도 가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이 미지와 파일 확장자 또는 이미지 해상D_06_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_06_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분 류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_06_도를 비교하 고, 전후 이미지 사이의 유사D_06_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사 D_06_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_06_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_06_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_06_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06_도 정보에 포함된 과속 방지 턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06_도 정보에 포함된 커 브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미지 로 분류할 수 있다. *또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_06_도를 비교하고, 유사D_06_ 도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이미지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_06_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_06_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지와 제2 이미지의 유사D_06_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06_도를 비교하고, 유사D_06_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_06_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_06_도 중 적어D_06_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_06_도록 한다. D_06_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_06_도이다. D_06_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이 션 작업과 관련된 프로젝트를 수행하기 위한 적어D_06_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부터 샘 플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사D_06_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_06_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_06_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_06_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_06_도 하나의 프로젝트를 기초로 수행 예정인 어 노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습 데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_06_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_06_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전 체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_06_도록 한다. D_06_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06_도이고, D_06_도 10 내지 D_06_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06_도이다. 먼저, D_06_도 10에 D_06_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공지능 (AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_06_도 11에 D_06_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복 수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_06_도 하나의 점을 선 택받고, 선택받은 적어D_06_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_06_도 12에 D_06_도시된 바와 같이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_06_도 13에 D_06_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_06_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선으로 지정할 수 있다. 다음으로, D_06_도 14에 D_06_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계 선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선 과 구분되D_06_도록 출력할 수 있다. 다음으로, D_06_도 15에 D_06_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하 고, 식별된 적어D_06_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지 를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_06_도 16에 D_06_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제2 객 체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_06_도록 한다. D_06_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06_도이고, D_06_도 18 내 지 D_06_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06_도이다. 먼저, D_06_도 18에 D_06_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_06_도 19에 D_06_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생성 할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고,작업자의 제어에 따라 복수의 점 중 적어D_06_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_06_도 20에 D_06_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영 역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_06_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06_도이다. D_06_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_06_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_06_도가 사전 설정된 값보다 높 은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_06_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_06_도가 사전 설정된 값보다 높은 이미지들의 선명D_06_도 (sharpness)를 산출하고, 산출된 선명D_06_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_06_도가 높게 판단된 경우, 특정 기준을 통해 제1 이 미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_06_도가 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. D_06_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06_도이다. D_06_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미 지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06_도를 비교하여, 노이즈 이미지를 분류 할 수 있다. 즉, (A)에 D_06_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정 점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_06_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06_도를 기준으로 제 1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_06_도 중 적어D_06_도 하나를 기준으로 사전 설정 된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 이상과 같이, 본 명세서와 D_06_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_06_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_06_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_06_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_06_청구범위 D_06_청구항 1 어노테이션 장치가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션 (annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계; 상기 어노테이션 장치가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계; 및 상기 어노테이션 장치가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계; 를 포함하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 2 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 상기 작업자로부터 상기 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 상기 복수의 점을 연결 하여, 상기 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 3 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 상기 이미지의 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 복수의 객체를 선택받고, 상기 추출된 엣지를 기초로 상기 선택받은 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 4 제3 항에 있어서, 상기 경계선을 지정하는 단계는 상기 추출된 엣지를 기초로 상기 복수의 객체에 대한 외곽선 내에서 경계선을 식별하는 것을 특징으로 하는, 어 노테이션 방법. D_06_청구항 5 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 상기 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 상기 바운딩 박스 내측 영역 에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 상기 배 경을 삭제하여 상기 복수의 객체의 외곽선을 지정하는 것을 특징으로 하는, 어노테이션 방법.D_06_청구항 6 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 작업자로부터 상기 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력받고, 상기 입력받은 복수 의 점을 연결하여 상기 경계선을 생성하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 7 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유 사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 상기 인식된 객체의 경계선을 생성 하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 8 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부의 엣지를 추출하고, 상기 추출된 엣지를 기초로 적어D_06_도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 하나의 객체를 선택받고, 상기 추출된 엣지를 기초로 상기 경계선을 생 성하는 것을 특징으로 하는, 어노테이션 방법, D_06_청구항 9 제8 항에 있어서, 상기 경계선을 지정하는 단계는 상기 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 상기 작업자의 제어에 따라 상기 복수의 점 중 적어D_06_도 하나의 점을 이동시켜 상기 경계선을 수정하는 것을 특징으로 하는, 어노테이션 방법. D_06_청구항 10 메모리(memory); 송수신기(transceiver); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합되어, 상기 프로세서가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션 (annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계; 상기 프로세서가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계; 및 상기 프로세서가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식 별하는 단계; 를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 333, "content": "D_06_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 334, "content": "D_06_요약 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제안한다. 상기 방법은 어노테이션 장치가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학 습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 어노테이션 장치가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선 을 지정하는 단계 및 상기 어노테이션 장치가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 포함할 수 있다. D_06_대표D_06_도 D_06_도 17 D_06_도면 D_06_도 1"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 335, "content": "D_06_도 2 D_06_도 3"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 336, "content": "D_06_도 4 D_06_도 5"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 337, "content": "D_06_도 6 D_06_도 7"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 338, "content": "D_06_도 8 D_06_도 9"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 339, "content": "D_06_도 10 D_06_도 11"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 340, "content": "D_06_도 12 D_06_도 13"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 341, "content": "D_06_도 14 D_06_도 15"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 342, "content": "D_06_도 16 D_06_도 17"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 343, "content": "D_06_도 18 D_06_도 19"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 344, "content": "D_06_도 20 D_06_도 21 D_06_도 22"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 345, "content": "D_06-1_발명의 설명 D_06-1_발명의 명칭 경계선 지정을 통한 어노테이션 방법{Method for annotation using boundary designation}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 346, "content": "D_06-1_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 기계 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 인공지능(AI) 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이하 게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법에 관한 것이다. D_06-1_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습은 학습용 데이터의 형태에서 따라, 지D_06-1_도 학습(supervised learning), 비지D_06-1_도 학습 (unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장, 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 메타데이터(meta data)를 입력하고 어노테이션(annotatio n)을 수행하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_06-1_도구를 활용하여 설정된 목표 품질 에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_06-1_도와 운전차가 차량을 제어하는 정D_06-1_도에 따라 비자 동화부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자 협회(SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차 기술자협회(SAE)가 분류한 6단계에 따르면, 레벨 0단계는 비자동화(no automation), 레벨 1단계는 운전자 보조 (driver assistance), 레벨 2단계는 부분 자동화(partial automation), 레벨 3단계는 조건부 자동화 (conditional automation), 레벨 4단계는 고D_06-1_도 자동화(high automation), 그리고 레벨 5단계는 완전 자 동화(full automation) 단계이다. 차량의 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메 커니즘을 통해 수행된다. 현재 여러 기업체들은 자율주행 메커니즘 중에서 인지 및 경로 계획을 인공지능(AI)을 이용하여 구현하기 위해 개발 중에 있다. 그리고, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사 용되는 데이터는 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 이러한, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 차량에 설치된 다양 한 종류의 센서들에 의해 수집된다. 예를 들어, 차량의 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터들은 차량에 고정 설치된 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서 (ultrasonic sensor) 및 GPS(Global Positioning System) 등에 의해 획득, 촬영 또는 감지된 데이터들이 될 수 있으며, 이에 한정되는 것D_06-1_도 아니다. 일반적으로, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데 이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 한편, 데이터 가공 단계의 어노테이션 작업은 이미지 속에 포함된 객체에 대하여 바운딩 박스(bounding box), 폴리곤(polygon) 등으로 객체를 식별하고, 식별된 객체의 속성 정보를 입력하여 진행된다. 이와 같은 어노테이 션 작업은 데이터 라벨링(data labeling)이라 지칭되기D_06-1_도 한다. 그리고, 어노테이션 작업 결과물에 해당 되는 데이터셋(dataset)은 JSON(Java Script Object Notation) 파일 형태로 산출된다. 이러한, 어노테이션 작 업은 적게는 몇 천개에서 많게는 수 백만개에 이르는 많은 수의 데이터를 대상으로 이루어지므로, 어노테이션 작업을 수행하는 작업자 또한 많은 수의 인원으로 이루어진다. 따라서, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데 이터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_06-1_도에 의존하여 산출되고 있는 문제점이 있었다. 또한, 어노테이션 작업 중 폴리곤 기법은 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성 하여 객체를 식별하는 방법이다. 이러한 폴리곤 기법은 자동차나 사람과 같은 비정형 객체의 윤곽을 정밀하게 선택할 수 있어, 객체의 크기와 형태를 정확하게 인식할 수 있는 장점이 있다. 그러나, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. D_06-1_선행기술문헌 D_06-1_특허문헌 대한민국 공개특허공보 제10-2020-0042629호, ‘인공지능 학습을 위한 모바일 기기의 터치 기반 어노테이션과 이미지 생성 방법 및 그 장치’, (2020.04.24. 공개)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 347, "content": "D_06-1_발명의 내용 D_06-1_해결하고자 하는 과제 본 발명의 일 목적은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용 이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 348, "content": "D_06-1_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정 할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제안한다. 상기 방법은 어노테이션 장치가, 작업자의 제어 에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 어노테이션 장치가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계 및 상기 어노테이션 장치가, 상기 지정 된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 포함할 수 있 다. 구체적으로, 상기 외곽선을 지정하는 단계는 서로 중첩되어 배치되는 적어D_06-1_도 제1 객체와 제2 객체를 포 함하는 복수의 객체의 전체 외곽선을 지정하는 것을 특징으로 하고, 상기 경계선을 지정하는 단계는 상기 지정 된 외곽선 내에서 상기 제1 객체와 상기 제2 객체 사이의 중첩된 경계선을 지정하는 것을 특징으로 하며, 상기 경계선은, 상기 제1 객체의 제2 객체와 중첩되는 부분의 외곽선을 형성하는 동시에, 상기 제2 객체의 제1 객체 와 중첩되는 부분의 외곽선을 형성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 작업자로부터 상기 복수 개의 객체를 포함하는 외곽선을 따라 복수 개의 점을 입력 받고, 상기 복수 개의 점을 연결하여, 상기 복수 개의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 이미지의 엣지(edge)를 추출하고, 상기 추출된 엣지를 기초로 적어D_06-1_ 도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 복수의 객체를 선택받고, 상기 추출된 엣 지를 기초로 상기 선택받은 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 상기 작업자로부터 상기 복수의 점군 중 복수의 객체를 선택받고, 상기 식별된 복수의 점군을 기초로 상기 선택받은 복수의 객체에 대한 외곽선을 생성하는 것을 특징으로 한다. 상기 외곽선을 지정하는 단계는 상기 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하 고, 상기 바운딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경 (background)을 구분하고, 상기 배경을 삭제하여 상기 복수의 객체의 외곽선을 지정하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 추출된 엣지를 기초로 상기 복수의 객체에 대한 외곽선 내에서 경계선을 식별하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 작업자로부터 상기 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력받고, 상기 입력받은 복수의 점을 연결하여 상기 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 상기 복수의 점군 사이의 경계선을 상기 복수의 객체 사이의 경계선으로 지정하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 상 기 인식된 객체의 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 상기 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 상기 작업자로부터 상기 복 수의 점군 중 하나의 객체를 선택받고, 상기 선택받은 객체의 점군을 기초로 상기 경계선을 생성하는 것을 특징 으로 한다. 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부의 엣지를 추출하고, 상기 추출된 엣지를 기초로 적어 D_06-1_도 하나의 객체를 식별하고, 상기 작업자로부터 상기 식별된 객체 중 하나의 객체를 선택받고, 상기 추 출된 엣지를 기초로 상기 경계선을 생성하는 것을 특징으로 한다. 상기 경계선을 지정하는 단계는 상기 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 상 기 작업자의 제어에 따라 상기 복수의 점 중 적어D_06-1_도 하나의 점을 이동시켜 상기 경계선을 수정하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정 할 수 있는, 경계선 지정을 통한 어노테이션 방법을 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제 안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서 (processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그램은 상기 프로세서 가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업 의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 프로세 서가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선을 지정하는 단계 및 상기 프로세서가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 실행시키 기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_06-1_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 349, "content": "D_06-1_발명의 효과 본 발명의 실시예들에 따르면, 어노테이션을 수행함에 있어서, 이미지 내에 포함된 서로 중첩되어 배치되는 복 수의 객체를 대상으로, 복수의 객체 전체에 대한 외곽선을 지정한 후에, 각 객체의 경계선을 지정하여 각 객체 를 구분함으로써, 중첩되어 배치되는 개체를 용이하게 식별할 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 350, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 351, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 352, "content": "D_06-1_도면의 간단한 설명 D_06-1_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_06-1_도이다. D_06-1_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_06-1_도이다. D_06-1_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_06-1_도이다. D_06-1_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_06-1_도이다. D_06-1_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_06-1_도이다. D_06-1_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 10 내지 D_06-1_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06- 1_도이다. D_06-1_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 18 내지 D_06-1_도 20은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06- 1_도이다. D_06-1_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06-1_도이다. D_06-1_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06-1_도이다. D_06-1_도 23 및 D_06-1_도 24는 본 발명의 또 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06- 1_도이다. D_06-1_도 25 및 D_06-1_도 26은 중첩된 복수 개의 객체의 외곽선을 설명하기 위한 예시D_06-1_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 353, "content": "D_06-1_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_06-1_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이 해되는 의미로 해석되어야 하며, 과D_06-1_도하게 포괄적인 의미로 해석되거나, 과D_06-1_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못 하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따 라 해석되어야 하며, 과D_06-1_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여 러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_06-1_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어 야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_06-1_도 제1 구성 요소로 명 명될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_06-1_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_06-1_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다 고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_06-1_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_06-1_도면 부호에 관계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_06-1_도면은 본 발명의 사상을 쉽게 이해할 수 있D_06-1_도록 하기 위한 것일 뿐, 첨부된 D_06-1_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어 서는 아니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_06-1_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_06-1_도 확장되는 것으로 해석되어야 한다. 한편, 학습 데이터의 수집은 프로젝트(project) 단위로 진행된다. 이때, 각 프로젝트의 컨트롤 타워는 데이터 수집을 위한 장치가 설치된 복수의 차량으로부터 데이터를 수신하게 된다. 이때, 복수의 차량으로부터 수집된 데이터를 업로드 받는 과정에서는 각 차량으로부터 동일한 이미지가 중복되 어 업로드 되거나, 서로 다른 차량으로부터 동일한 이미지가 중복되어 업로드 되는 문제점이 있었다. 또한, 각 차량의 관리자는 컨트롤 타워로부터 수집 조건이 명시된 가이드에 따라 데이터를 수집하고, 수집된 데 이터를 업로드하게 된다. 이때, 각 차량의 관리자의 주관인 관점, 수집 환경, 수집 장치의 오류 둥에 따라, 기 준에 적합하지 않은 데이터들이 무작위로 업로드 되는 문제점이 있었다. 위와 같은 문제점을 방지하기 위하여, 컨트롤 타워는 업로드 된 데이터를 수동으로 검수하기 위한 검수자를 배 치하고 있다. 그러나, 검수자를 배치함에 따른 불필요한 리소스 낭비를 방지할 수 방안이 필요한 실정이다. 또한, 수많은 작업자가 수많은 어노테이션 작업을 수행해야하는 프로젝트의 전체 작업 비용을 산출하는 것은 어 려운 작업이다. 종래에는 어노테이션 작업에 관한 프로젝트의 전체 작업 비용은 단순히 작업 대상이 되는 데이 터의 개수, 또는 담당자의 직관에 따라 예측된 작업의 난이D_06-1_도에 의존하여 산출되고 있는 문제점이 있었 다. 그리고, 이미지 내에 객체가 중첩되어 배치되는 경우, 폴리곤 기법으로 각각의 객체를 식별하기 위해서는 이미 지를 확대하여 경계선의 동일한 지점에 이중으로 점을 생성해야 한다. 이때, 경계선의 동일한 지점에 정확히 점 을 생성하지 않는 경우, 중첩된 객체와 객체 사이에 공간이 발생되는 문제점이 있었다. 이러한 한계를 극복하고자, 본 발명은 인공지능의 기계 학습용 데이터를 수집하고, 수집된 데이터 중 불필요한 데이터를 정제할 수 있는 다양한 수단을 제안하고자 한다. 또한, 본 발명은 인공지능 학습용 데이터의 어노테이션 작업에 관한 프로젝트의 전체 작업 비용을 합리적으로 예측할 수 있는 다양한 수단을 제안하고자 한다. 그리고, 본 발명은 인공지능 학습용 데이터를 어노테이션함에 있어서, 이미지 속에 포함된 중첩된 객체를 용이 하게 지정할 수 있는 다양한 수단을 제공하고자 한다. D_06-1_도 1은 본 발명의 일 실시예에 따른 인공지능 학습 시스템의 구성D_06-1_도이다. D_06-1_도 1에 D_06-1_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 복수의 학습 데 이터 수집 장치(100a, 100b, …, 100n; 100), 학습 데이터 생성 장치, 복수의 어노테이션 장치(300a, 300b, …, 300n; 300) 및 인공지능 학습 장치를 포함하여 구성될 수 있다. 이와 같은, 일 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것 에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 수집 장치는 자율주행에 사용될 수 있는 인공지능(AI) 을 기계 학습시키기 위한 데이터를 수집하기 위하여, 차량에 설치된 라이다(lidar), 카메라(camera), 레이더 (radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_06-1_도 감지 센 서 중 하나 이상으로부터 실시간으로 데이터를 수집하는 장치이다. 이러한, 학습 데이터 수집 장치는 인공지능의 기계 학습을 위한 데이터의 수집을 학습 데이터 생성 장치 로부터 요청받을 수 있다. 이때, 학습 데이터 수집 장치는 데이터의 수집 조건을 포함하는 가이드 정 보를 학습 데이터 생성 장치로부터 수신할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체(object)의 클래스(class), 데이터 확장자(filename extension), 이미 지 해상D_06-1_도(resolution) 등을 포함할 수 있다. 이때, 학습 데이터 수집 장치는 가이드 정보를 샘플 이미지를 통해 제공받을 수 있다. 이러한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터를 획득, 촬영 또는 감지하는 센서의 종류에는 라이다(lidar), 카메라(camera), 레이더(radar), 초음파 센서(ultrasonic sensor), 레인 센서(rain sensor), 위치 측정 센서 및 속D_06-1_도 감지 센서 중 하나 이상이 포함될 수 있으나, 이에 한 정되는 것은 아니다. 또한, 학습 데이터 수집 장치의 제어 대상이자, 차량에 설치되어 기계 학습용 데이터 를 획득, 촬영 또는 감지하는 센서는 종류별로 하나씩 구비되는 것으로 한정되지 아니하며, 동일한 종류의 센서라 할지라D_06-1_도 복수 개로 구비될 수 있다. 다음 구성으로, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지능(AI)을 기계 학습시 키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 특징적으로, 본 발명의 일 실시예에 따른 학습 데이터 생성 장치는 인공지능(AI)의 기계 학습(machine learning)을 위한 이미지의 수집을 적어D_06-1_도 하나의 학습 데이터 수집 장치에 요청하고, 적어D_06-1_ 도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수신 한 이미지들의 컬러 정보를 추출하고, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_06-1_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_06-1_ 도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류할 수 있다. 또한, 본 발명의 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능의 기계 학습을 위한 이미지의 수 집 조건을 포함하는 가이드 정보를 적어D_06-1_도 하나의 학습 데이터 수집 장치에 전송하고, 적어D_06-1_ 도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있다. 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하고, 추출한 이미지 정보를 가이드 정보와 비교하여, 학습 데이터 수집 장치의 물리적 요인에 따른 노이즈 이미지를 분류할 수 있다. 그리고, 본 발명의 또 다른 실시예에 따른 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테이션(annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_06-1_도 하나의 샘플 데이터를 수신하 고, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 상기 샘플 데이터와 비교하고, 샘플 데이터와의 유사 D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06-1_도 하나의 프로젝트를 추출할 수 있다. 학습 데이터 생성 장치는 추출된 적어D_06-1_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라 면 어떠한 장치라D_06-1_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워 크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한 정되는 것은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치에 대한 구체적인 구성 및 동작에 대해서는 추후 D_06-1_도 2 및 D_06-1_도 3을 참조하여 설명하기로 한다. 다음 구성으로, 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 을 수행하는데 사용될 수 있는 장치이다. 이와 같은, 어노테이션 장치의 사용자는 라벨러(labeler), 리뷰어(reviewer), 인스펙터(inspector) 및 트 레이니(trainee)로 구분될 수 있다. 여기서, 라벨러는 이미지를 대상으로 어노테이션 작업을 수행하는 자에 해당된다. 리뷰어는 상기 어노테이션 작 업이 수행된 이미지를 시각적으로 검증하는 자에 해당된다. 인스펙터는 상기 어노테이션 작업 결과물을 스크립 트(script)를 이용하여 검증하는 자에 해당된다. 그리고, 트레이니는 상기 어노테이션 작업을 수행하기 위한 교 육을 받는 자에 해당된다. 구체적으로, 어노테이션 장치는 라벨러에 해당되는 사용자의 제어에 따라 다음과 같이 어노테이션 작업을 수행할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 학습 데이터 생성 장치로부터 수신된 이미지를 출력할 수 있다. 어노테이션 장치는 사용자의 제어에 따라, 툴을 선택할 수 있다. 여기서, 툴(tool)은 이미지 속에 포함되 어 있는 하나 이상의 객체를 특정하기 위한 D_06-1_도구이다. 어노테이션 장치는 선택된 툴을 이용한 사용 자의 제어에 따라, 좌표를 입력 받을 수 있다. 어노테이션 장치는 입력된 좌표를 기초로 객체를 식별할 수있다. 한편, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자가 이미지 내에 포함된 객체의 외곽선을 따라 복수의 점을 생성하여 객체를 식별하는 폴리곤(polygon) 기법을 통해 이미지 속에 포함된 하나 이상의 객체를 식별할 수 있다. 하지만, 이에 한정된 것은 아니고 어노테이션 장치는 바운딩 박스(bounding box), 폴리라 인(polyline), 포인트(point), 큐보이드(cuboid), 시맨틱 세그멘티이션(semantic segmentation) 등의 기법을 사용할 수D_06-1_도 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어노 테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정 하고, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하고, 경계선을 상기 제2 객체의 외곽선의 일 부로 설정할 수 있다. 어노테이션 장치는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 또한, 본 발명의 다른 실시예에 따른 어노테이션 장치는 작업자의 제어에 따라, 인공지능 학습을 위한 어 노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하고, 지정 된 외곽선 내에서 복수의 객체 사이의 경계선을 지정할 수 있다. 어노테이션 장치는 지정된 경계선을 기준 으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 어노테이션 장치는 특정된 객체의 속성 정보를 설정할 수 있다. 여기서, 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체의 속성 정보에는 어노테이션의 종 류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부(truncated), 대분류, 소분류 또는 상위 레벨 (instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다. 어노테이션 장치는 사용자에 의해 설정된 객체의 위치 및 크기에 따른 좌표와, 설정된 속성 정보를 포함하 여 어노테이션의 작업 결과물을 생성할 수 있다. 이와 같은, 작업 결과물은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 어노테이션 장치는 생성된 어노테이션 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있 다. 한편, 어노테이션 장치와 관련한 구체적인 설명은 D_06-1_도 4 및 D_06-1_도 5를 참조하여 후술하D_06- 1_도록 한다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이 용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_06-1_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_06-1_도 있다. 다음 구성으로, 인공지능 학습 장치는 인공지능 학습용 데이터를 기초로, 인공지능의 기계 학습을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 인공지능 학습 장치는 수행 예정인 프로젝트와 관련된 샘플 데이터를 학습 데이터 생성 장치 에 전송할 수 있다. 여기서, 샘플 데이터는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련 된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한정되는 것D_06-1_도 아니다. 인공지능 학습 장치는 학습 데이터 생성 장치로부터 수행 예정인 프로젝트를 수행하기 위하여 요구되 는 전체 작업 비용을 수신할 수 있다. 인공지능 학습 장치는 수신된 전체 작업 비용을 출력할 수 있다. 이러한, 전체 작업 비용은 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사 이에 프로젝트 수행과 관련된 계약을 체결하는데 활용될 수 있다. 인공지능 학습 장치의 운영 주체와 학습 데이터 생성 장치의 운영 주체 사이에 프로젝트 수행과 관련 된 계약이 체결된 이후, 인공지능 학습 장치는 학습 데이터 생성 장치로부터 패키징 된 어노테이션 작업 결과물을 수신할 수 있다. 그리고, 인공지능 학습 장치는 수신된 어노테이션 작업 결과물을기반으로, 인공지능(AI)의 기계 학습을 수행할 수 있다. 이와 같은, 인공지능 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_06-1_도 허용될 수 있다. 예를 들어, 인공지능 학습 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 상술한 바와 같은, 하나 이상의 학습 데이터 수집 장치, 학습 데이터 생성 장치, 어노테이션 장치 및 인공지능 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_06-1_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunicatio n)가 포함될 수 있으나, 이에 한정되는 것은 아니다. 이하, 상술한 바와 같은, 학습 데이터 생성 장치의 구성에 대하여 보다 구체적으로 설명하기로 한다. D_06-1_도 2는 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 논리적 구성D_06-1_도이다. D_06-1_도 2를 참조하면, 학습 데이터 생성 장치는 통신부, 입출력부, 데이터 설계부, 데 이터 수집부, 데이터 전처리부, 데이터 납품부 및 저장부를 포함하여 구성될 수 있다. 이와 같은, 학습 데이터 생성 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므 로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리 적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 구체적으로, 통신부는 인공지능(AI)의 기계 학습을 위한 이미지의 수집 조건을 포함하는 가이드 정보를 적 어D_06-1_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 또한, 통신부는 학습 데이터 수집 장치로부터, 카메라(camera)에 의해 촬영된 이미지, 라이다(lida r)로부터 획득된 점군 데이터, 위치 측정 센서 및 속D_06-1_도 감지 센서로부터 감지된 데이터를 수신할 수 있 다. 또한, 통신부는 어노테이션 작업의 대상이 되는 하나 이상의 이미지를 어노테이션 장치에 전송할 수 있다. 또한, 통신부는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 또한, 통신부는 인공지능 학습 장치로부터 적어D_06-1_도 하나의 샘플 데이터를 수신할 수 있다. 통 신부는 데이터 설계부에 의해 예측되거나, 또는 사용자로부터 입력된, 수행 예정인 프로젝트를 수행 하기 위하여 요구되는 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 사용자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자로부터 학습 데이터 수집 장치의 수집 조건을 포함하는 가이드 정보를 입력받을 수 있다. 가이드 정보에는 학습 목적, 학습 기간, 학습에 필요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 이미지의 해상D_06-1_도, 이미지의 확장자 등이 포함될 수 있으나, 이에 한정되는 것은 아니 다. 또한, 입출력부는 사용자로부터 샘플 데이터를 입력 받을 수 있다. 또한, 입출력부는 사용자로부터 분해 구성요소, 가중치 및 가이드 정보를 입력 받을 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물 을 구성하고 있는 요소들 중에서, 프로젝트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴(tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 입출력부는 데이터 설계부에 의해 예측된, 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 출력할 수 있다. 입출력부는 사용자로부터 수정된 전체 작업 비용을 입력 받을 수 있다.다음 구성으로, 데이터 설계부는 수행 예정인 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예 측하여, 인공지능 학습 장치에 제공할 수 있다. 구체적으로, 데이터 설계부는 인공지능 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_06-1_도 하나의 샘플 데이터를 인공지능 학습 장치로부터 수신할 수 있다. 여기서, 샘플 데이터는 인공지능(AI) 학습을 위하여 수행 예정인 어노테이션 작업과 관련된 샘플이다. 이와 같은, 샘플 데이터는 어노테이션 작업의 대상이 되는 이미지이거나, 또는 어노테이션 작업 결과물이 될 수 있으며, 이에 한 정되는 것D_06-1_도 아니다. 데이터 설계부는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이 터와의 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06-1_도 하나의 프로젝트를 추출 할 수 있다. 이때, 데이터 설계부는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성요소 를 식별할 수 있다. 여기서, 분해 구성요소는 어노테이션 작업 결과물을 구성하고 있는 요소들 중에서, 프로젝 트의 전체 작업 비용을 예측하기 위해 사용되는 요소이다. 예를 들어, 분해 구성요소에는 클래스(class)와 툴 (tool)이 포함될 수 있으나, 이에 한정되는 것은 아니다. 일 예를 들어, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 데이터 설계부는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 데이터 설계부는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다 또한, 데이터 설계부는 샘플 데이터와의 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06-1_도 하나의 프로젝트를 추출할 수 있다. 이때, 데이터 설계부는 인공지능 학습 장치로부터 샘플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력받은 가중치를 고려하여, 기존 이미지와의 유사 D_06-1_도를 평가할 수 있다. 예를 들어, 인공지능 학습 장치에 의해 객체의 클래스와, 객체를 특정하기 위하여 사용된 툴 중 객체의 클래스에 더 높은 가중치를 부여하는 경우, 데이터 설계부는 객체의 클래스를 중점적으로 유사한 기존 데이터를 추출하고, 해당 기존 데이터가 포함된 프로젝트를 추출할 수 있다. 데이터 설계부는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포함된 객체 를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유사D_06-1_ 도를 평가할 수 있다. 예를 들어, 데이터 설계부는 검출된 객체의 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 히스토그램을 생성 하고, 생성된 RGB 히스토그램을 비교하여 유사D_06-1_도를 산출할 수 있다. 여기서 RGB 히스토그램은 이미지에 서 각 원색(RGB)의 밝기 분포를 나타내는 그래프이다. 예를 들어, RGB 히스토그램은 가로축이 컬러의 밝기 레벨 을 표시하며, 세로축이 컬러의 밝기 레벨에 할당된 픽셀 수로 표시되고, 좌측으로 치우친 픽셀 수가 많을수록 색상이 어둡고 덜 선명하게 표현되며, 우측으로 치운 친 픽셀 수가 많을수록 색상이 더 밝고 진하게 표현될 수 있다. 이와 같이, 데이터 설계부는 RGB 히스토그램을 통해 샘플 이미지에 포함된 객체 및 기존 이미지에 포함된 객체의 색상의 채D_06-1_도와 계조 상태, 화이트 밸런스의 성향 등을 비교하여 유사D_06-1_도를 산출할 수 있다. 하지만, 이에 한정된 것은 아니고, 데이터 설계부는 추출된 객체의 엣지에 대한 모멘트(moment) 를 비교하여 유사D_06-1_도를 산출할 수 있다. 또한, 데이터 설계부는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지와 비교하고, 샘플 이미지와의 유사D_06-1_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장 치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 여기서, 대표 이미지는 기 수행된 복수의 프로젝트를 수행하는 과정에서 수집된 이미지 중 기 수행된 복수의 프로젝트 각각을 수행하기 위하여 수신한 샘플 이미지와 유사D_06-1_도가 가장 높은 이미지가 될 수 있다. 이때, 데이터 설계부는 샘플 이미지와의 유사D_06-1_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정 보로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있 다. 데이터 설계부는 기밀 정보로 등록된 클래스에 해당하는 객체를 블러링(blurring) 처리하여 비식별 처 리를 수행할 수 있다.즉, 각 프로젝트별로 수집된 이미지에는 기밀 정보가 포함될 수 있다. 여기서, 기밀 정보는 각 프로젝트를 의뢰 한 기업으로부터 지정된 각 기업의 기밀 정보이거나, 얼굴, 자동차번호판 등의 개인 정보가 포함될 수 있다. 이 러한, 기밀 정보는 기 수행된 복수의 프로젝트 각각을 요청한 인공지능 학습 장치로부터 설정되거나, 학습 데이터 생성 장치에 의해 미리 설정될 수 있다. 데이터 설계부는 기밀 정보로 지정된 객체의 일부를 블러링(blurring) 처리하여 비식별 처리를 수행하되, 식별된 객체에서 랜드 마크(land mark)를 추출하고, 추출된 랜드 마크에 블러링 처리를 수행할 수 있다. 예를 들어, 데이터 설계부는 식별된 객체가 사람일 경우, 사람의 랜드 마크에 해당하는 눈, 코, 입을 추출하고, 추출된 눈, 코, 입만 선택적으로 블러링 처리할 수 있다. 또한, 데이터 설계부는 추출된 적어D_06-1_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 데이터 설계부는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 비용 및 데이 터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 예를 들어, 데이터 설계부는 추출된 적어D_06-1_도 하나의 프로젝트에 대한 데이터 수량 및 작업 비용을 검출한다. 그리고, 데이터 설계부는 수행 예정인 프로젝트의 데이터 수량을 입력 받고, 추출된 프로젝트의 데이터 수량 및 작업 비용과 비례하여, 입력받은 데이터 수량에 따른 작업 비용을 가감하여 전체 작업 비용을 예측할 수 있다. 또한, 데이터 설계부는 샘플 데이터와 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복 수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 데이터 설계부는 예측된 전체 작업 비용을 입출력부를 통하여 출력할 수 있다. 데이터 설계 부는 입출력부를 통해 입력된 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_06-1_도 있다. 그 리고, 데이터 설계부는 예측 또는 수정된 전체 작업 비용을 통신부를 통해 인공지능 학습 장치 에 전송할 수 있다. 다음 구성으로, 데이터 수집부는 인공지능 학습 장치의 운영 주체와 프로젝트 수행과 관련된 계약이 체결되면, 해당 프로젝트를 위하여 인공지능(AI) 학습용 데이터를 수집할 수 있다. 구체적으로, 데이터 수집부는 인공지능의 기계 학습을 위한 이미지의 수집을 적어D_06-1_도 하나의 학습 데이터 수집 장치에 요청할 수 있다. 이를 위해, 데이터 수집부는 이미지의 수집 조건을 포함하는 가 이드 정보를 학습 데이터 수집 장치에 전송할 수 있다. 여기서, 가이드 정보는 수집 조건인 객체의 클래스, 데이터 확장자, 이미지 해상D_06-1_도 등을 포함할 수 있다. 이때, 데이터 수집부는 가이드 정보 를 샘플 이미지를 통해 제공할 수 있다. 즉, 데이터 수집부는 프로젝트 계약 당시 수신한 샘플 이미지를 학습 데이터 수집 장치에 전송하여, 학습 데이터 수집 장치 수집 조건을 인지하D_06-1_도록 할 수 있 다. 또한, 데이터 수집부는 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신할 수 있 다. 이때, 데이터 수집부는 적어D_06-1_도 하나의 학습 데이터 수집 장치 각각에 식별자(identifie r)를 부여하고, 부여된 식별자별로 수신한 이미지들을 저장할 수 있다. 다음 구성으로, 데이터 정제부는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬러 정보는 픽 셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 데이터 정제부 는 적어D_06-1_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자(identifier)를 기준으로 이미지들 의 파일명 및 컬러 정보를 저장부에 저장할 수 있다. 데이터 정제부는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류할 수 있다. 즉, 데이터 정 제부는 이미지들 중 컬러 정보의 유사D_06-1_도가 사전 설정된 값보다 높은 이미지들 중 적어D_06-1_도 하 나를 노이즈 이미지로 분류할 수 있다. 이때, 데이터 정제부는 이미지들을 사전 설정된 해상D_06-1_도로 리샘플링(resamping)하고, 리샘플링 된 이미지들의 동일한 좌표에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_06-1_도를 평가할 수 있다. 또한, 데이터 정제부는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06-1_도 하나를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 식별자 가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06-1_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추출 할 수 있다. 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이미지들 의 유사D_06-1_도를 평가할 수 있다. 여기서, 엣지는 이미지 안에서 픽셀의 값이 급격하게 변하는 곳이다. 이러한, 데이터 정제부는 이미지를 미분한 그레디언트(gradient) 벡터의 크기로 엣지를 판단할 수 있다. 예를 들어, 데이터 정제부는 소벨 엣 지 검출(sobel edge detection) 알고리즘, 케니 엣지 검출(canny edge detection) 알고리즘 등의 엣지 추출 알 고리즘을 통해 이미지 상의 엣지를 추출할 수 있다. 또한, 데이터 정제부는 이미지들 중 유사D_06-1_도가 사전 설정된 값보다 높은 이미지들의 선명D_06-1_도 (sharpness)를 산출하고, 산출된 선명D_06-1_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 유사한 이미지가 복수개 존재하는 경우, 특정 기준을 통해 복수의 이미지 중 하나를 제외한 나머지 이미지 를 제거해야 한다. 이를 위해, 데이터 정제부는 선정된 이미지들 중 선명D_06-1_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지로 분류하여 삭제할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_06-1_도를 산출하고, 산출된 유사 D_06-1_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 즉, 데이터 정제부는 하나의 시퀀스 데이터에 연속된 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 높은 경우, 해당 이미지를 수집한 차량의 속D_06-1_도로 높은 것으로 판단하고, 해당 시퀀스 데이터 별 초당 프 레임 수를 결정하여, 시퀀스 데이터 내에 포함된 이미지의 부피를 줄일 수 있다. 또한, 데이터 정제부는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이 미지 정보는 파일 확장자, 이미지 해상D_06-1_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드 (color code) 값 중 적어D_06-1_도 하나를 포함할 수 있다. 여기서, 데이터 정제부는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수 집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 데이터 정제부는 샘플 이미지의 파일 확장자, 이미지 해상D_06-1_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_06-1_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이 미지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 데이터 정제부는 샘플 이미지와 유사D_06-1_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지 로 분류할 수 있다. 즉, 데이터 정제부는 샘플 이미지와 파일 확장자 또는 이미지 해상D_06-1_도가 상이하 거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_06-1_도가 사전 설 정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 데이터 정제부는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑 (grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 데이터 정제부는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_06-1_도를 비교하고, 전후 이미 지 사이의 유사D_06-1_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 즉, 데이터 정제부는 특정 이미지의 전후 이미지를 비교하여 특정 이미지 만 유사D_06-1_도가 낮은 경우, 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지로 판단하고, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 여기서, 데이터 정제부는 이미지들을 사전 설정된 해상D_06-1_도로 리샘플링하고, 리샘플링 된이미지들의 동일한 위치에 존재하는 픽셀의 컬러 정보를 각각 비교하여 이미지들 간 유사D_06-1_도를 평가할 수 있다. 또한, 데이터 정제부는 이미지들 각각에 포함된 객체의 엣지를 추출하고, 이미지들 각각에 포함된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 데이터 정제부는 이미지에 포함된 객체의 움직임의 정D_06-1_도를 통해 특정 이미지가 과속 방지턱을 넘는 과정에서 촬영된 이미지 인지를 판단할 수 있다. 또한, 데이터 정제부는 통신부를 통해 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미 지들 각각의 메타 정보를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이 터 수집 장치의 위치 정보 및 속D_06-1_도 정보를 포함할 수 있다. 학습 데이터 수집 장치로부터 제공받은 메타 정보를 활용하여, 데이터 정제부는 학습 데이터 수집 장 치가 이동한 경로를 포함하는 지D_06-1_도 정보에 포함된 과속 방지턱의 위치 정보를 기준으로 메타 정보 와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06-1_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 유사D_06- 1_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이 미지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 별 연속된 이미지의 유사D_06-1_도를 기초로 노이즈 이미지를 분류 하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 제1 이 미지와 연속된 제2 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미지 와 제2 이미지의 유사D_06-1_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카 메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 급격 하게 변화된 후 변화된 상태의 이미지가 지속적으로 수집되는 경우, 이미지가 변화된 이후의 데이터를 카메라 앵글이 변경된 오류에 따른 노이즈 데이터로 판단할 수 있다. 또한, 데이터 정제부는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 유사D_06-1_도 가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 즉, 데이터 정제부는 이미지가 지속적으로 변 화되는 경우, 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 데이터 정제부는 추정된 오류에 해당하는 데이터를 삭제하거나, 추정된 오류의 종류를 메타 정보에 포함시 켜 검수자가 확인할 수 있D_06-1_도록 할 수 있다. 또한, 데이터 정제부는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06-1_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, 데 이터 정제부는 기 수행된 프로젝트 중 해당 위치에서 이전에 수집된 이미지와, 현재 수집된 이미지들을 매 칭하고, 매칭된 이미지 사이의 유사D_06-1_도를 비교하여 유사D_06-1_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 데이터 정제부는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06-1_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 즉, 데이터 정제부는 이미지 내에서 D_06-1_도로, 방음벽, 가드레일 등의 정적으로 존재하는 객체를 식별 하기 위하여, 이미지 내에서 양단부에 존재하는 픽셀의 유사D_06-1_도를 평가하여 이미지의 양단부를 연결하는 객체를 식별할 수 있다. 그리고, 이미지의 양단부를 연결하는 객체를 정적으로 존재하는 객체로 인식할 수 있다. 데이터 정제부는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_06-1_도 중 적어D_06-1_도 하나를 기 준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 데이터 정제부는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추출 된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이즈 이미지를 분류할 수 있다. 또한, 데이터 정제부는 통신부를 통해 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점 군(3D points group) 데이터를 더 수신할 수 있다. 데이터 정제부는 3D 점군 데이터에 포함된 거리 정보를 기초로 매칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 즉, 이미지에 포함된 객체 중에는 자동차, 자전거, 사람 등과 같은 유동 객체와, D_06-1_도로, 건물, 가이드레 일 등과 같은 정적 객체가 존재할 수 있다. 이에 따라, 데이터 정제부는 샘플 이미지와 매칭되는 이미지 사이의 정적 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 다음 구성으로, 데이터 납품부는 어노테이션 장치들에 대하여 하나 이상의 어노테이션 작업 대상물 (즉, 이미지)을 분배할 수 있다. 또한, 데이터 납품부는 어노테이션 작업 결과물을 검증한 후, 인공지능 학습 장치에 납품할 수 있다. 다음 구성으로, 저장부는 학습 데이터 생성 장치의 동작에 필요한 데이터를 저장할 수 있다. 저장부 는 인공지능(AI) 학습을 위한 데이터 설계하는데 필요한 데이터를 저장할 수 있다. 구체적으로, 저장부는 어노테이션 작업의 대상이 되는 이미지들을 저장할 수 있다. 저장부는 프로젝 트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 이하, 상술한 바와 같은 학습 데이터 생성 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보 다 구체적으로 설명한다. D_06-1_도 3은 본 발명의 일 실시예에 따른 학습 데이터 생성 장치의 하드웨어 구성D_06-1_도이다. 학습 데이터 생성 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 소프트웨어(280a)에 따른 명령어를 기초로, 학습 데이터 생성 장치(20 0)의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명에 따른 방법이 구현된 소프트웨어(280a)가 상주 (loading)될 수 있다. 송수신기는 학습 데이터 수집 장치, 어노테이션 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치는 학습 데이터 설계 장치의 동작에 필요한 데이 터를 입력 받고, 분류된 노이즈 이미지, 예측된 전체 작업 비용 등을 출력할 수 있다. 데이터 버스는 프로 세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사 이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명에 다른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이션 프로그래 밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명에 따른 방법이 구현된 소프트웨어(280b)를 저장할 수 있다. 또 한, 스토리지는 인공지능 학습용 데이터 생성 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토 리지는 프로젝트의 속성, 이미지의 속성, 작업자의 속성, 기존에 수행된 복수 개의 프로젝트에 관한 정보 및 작업자들의 풀을 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이미 지의 수집을 적어D_06-1_도 하나의 학습 데이터 수집 장치에 요청하는 단계, 프로세서가, 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수신한 이미지들의 컬러 정 보를 추출하는 단계 및 프로세서가, 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분류하는 단 계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 수 집 조건을 포함하는 가이드 정보를 적어D_06-1_도 하나의 수집 장치에 전송하는 단계, 프로세서가, 적어 D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서가, 수집 조건과 대응 하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 이미지 정보를 가이드 정보와 비교하여, 수집 환경에 따른 노이즈 이미지를 분류하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI)의 기계 학습(machine learning)을 위한 이 미지의 수집 조건을 포함하는 가이드 정보를 적어D_06-1_도 하나의 학습 데이터 수집 장치에 전송하는 단계, 프 로세서가, 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지들을 수신하는 단계, 프로세서 가, 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출하는 단계 및 프로세서가, 추출한 이미지 정 보를 상기 가이드 정보와 비교하여, 이미지들 중 수집 장치 오류에 따른 노이즈 이미지를 분류하는 단계를 실행 시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 또 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 소프트웨어(280a, 280b)는 프로세서가, 인공지능(Artificial Intelligence, AI) 학습을 위하여 수행 예정인 어노테이션 (annotation) 작업과 관련된 프로젝트를 수행하기 위한 적어D_06-1_도 하나의 샘플 데이터를 인공지능 학습 장 치로부터 수신하는 단계, 프로세서가, 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데이터와 비교하고, 샘플 데이터와의 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06-1_도 하 나의 프로젝트를 추출하는 단계 및 프로세서가, 추출된 적어D_06-1_도 하나의 프로젝트를 기초로 수행 예 정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측하는 단계를 실행 시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_06-1_도 3에 D_06-1_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또 는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_06-1_도 있다. 예컨대 기록매체는 하드 디스크, 플로 피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_06-1_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어 지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_06-1_도록 구성될 수 있으며, 그 역D_06-1_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 상세히 설명하D_06-1_도록 한 다. D_06-1_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_06-1_도이다. D_06-1_도 4를 참조하면, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력부, 저 장부, 객체 식별부, 객체 속성 설정부 및 결과물 생성부를 포함하여 구성될 수 있다. 이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 학습 데이터 생성 장치와 데이터를 송수신할 수 있다. 구체적으로, 통신부는 학습 데이터 생성 장치로부터 이미지를 수신할 수 있다. 여기서, 이미지는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지이다. 이와 같은, 이미지는 학습 데이터 생성 장치가 설계한 데이터 가공 계획에 따라, 어노테이션 작업의 대상이 되는 이미지를 개별 적으로 수신하거나, 또는 복수 개의 이미지를 일괄적으로 수신할 수 있다. 또한, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 바운딩 박스의 좌표 및 객체의 속성 정보가 포함될 수 있 다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 폴리곤 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_06-1_도, 비트 수준, 압축 형식, 촬영 장치명, 노출 시간, ISO 감D_06-1_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_06-1_도, 경D_06- 1_도), 촬영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거 나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 객체 를 지정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 사용자 가 지정한 영역을 오버레이(overlay)하여 출력할 수 있다. 또한, 입출력부는 객체의 속성 정보를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 객체의 속성 정보는 인공지능(AI) 학습의 대상이 되는 객체의 속성을 지정하기 위한 정보이다. 이와 같은, 객체 의 속성 정보에는 어노테이션의 종류(type), 클래스 명(class), 분류 항목(tags), 객체의 잘림 여부 (truncated), 대분류, 소분류 또는 상위 레벨(instance upper)에 관한 정보가 포함될 수 있으며, 이에 한정되는 것은 아니다.다음 구성으로, 저장부는 통신부를 통해 수신된 이미지를 저장할 수 있다. 저장부는 통신부 를 통해 수신된 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성을 저장할 수 있다. 다음 구성으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이 미지에 포함된 서로 중첩되어 배치되는 복수의 객체 중 제1 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 제1 객체의 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하 여 제1 객체의 외곽선을 형성할 수 있다. 즉, 객체 식별부는 작업자가 지정한 점을 연결하여 폴리곤 (polygon) 형태의 영역을 생성할 수 있다. 이때, 객체 식별부는 작업자로부터 상기 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제 2 점 사이에 적어D_06-1_도 하나의 새로운 제3 점을 지정 받는 경우, 제1 점, 제2 점 및 상기 제3 점을 연결하 여, 제1 객체의 외곽선을 수정할 수 있다. 즉, 객체 식별부는 복수의 점을 지정받은 후에 수정이 필요한 영역에 해당하는 두개의 점을 선택받고, 두개의 점 사이에 새로운 점을 지정하는 경우, 기존에 지정했던 점을 삭제하고, 새로운 점을 기준으로 복수의 점을 연결하여 새로운 영역을 생성할 수 있다. 하지만, 이에 한정된 것은 아니고, 객체 식별부는 작업자의 제어에 따라, 복수의 점 중 임의의 점을 드레 그(drag)하여 이동시키는 경우, 이동시킨 점을 기준으로 제1 객체의 외곽선을 수정할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나의 객체 를 식별하고, 작업자로부터 하나의 점을 입력 받으면, 입력 받은 점을 포함하는 객체의 엣지를 제1 객체의 외곽 선으로 지정할 수 있다. 즉, 객체 식별부는 작업자로부터 복수의 점을 입력 받아 객체의 외곽선을 생성하지 않고, 자동으로 객체를 식별하여 객체의 외곽선을 지정할 수 있다. 즉, 엣지를 기초로 객체를 식별하는 경우, 이미지 내에 여러 개의 객체가 식별될 수 있다. 이때, 객체 식별부는 작업자가 특정 점을 선택하게 되면, 해당 점이 포함된 엣지 를 식별하고자 하는 객체의 엣지로 판단하고, 해당 객체의 엣지를 외곽선으로 인식할 수 있다. 제1 객체의 외곽선을 지정한 후에, 객체 식별부는 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정할 수 있다. 구체적으로, 객체 식별부는 제1 객체의 외곽선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 생 성된 복수의 점 중 적어D_06-1_도 하나의 점을 선택받고, 선택받은 적어D_06-1_도 하나의 점을 기초로 상기 경 계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, 객체 식별부는 복수의 점 중 임의 의 두개의 점을 선택받고, 선택받은 두개의 점 사이에 존재하는 적어D_06-1_도 하나의 점을 연결하는 선을 생성 하고, 생성된 선을 경계선으로 지정할 수 있다. 여기서, 객체 식별부는 복수의 점 중 임의의 제1 점 및 제2 점을 선택받고, 제1 점 및 제2 점 사이에 존재 하는 제3 점을 선택받는 경우, 제1 점, 제2 점 및 제3 점을 연결하는 선을 생성할 수 있다. 즉, 두개의 점을 선 택받는 경우, 선택받은 두개의 점을 기준으로 객체의 외곽선을 이루는 두개의 선이 존재한다. 이에 따라, 객체 식별부는 두개의 점 사이에 다른 한점을 선택받아, 경계선을 명확히 인식할 수 있다. 경계선을 지정한 후에 객체 식별부는 경계선을 제2 객체의 외곽선의 일부로 설정할 수 있다. 이때, 객체 식별부는 경계선이 제1 객체의 외곽선과 구분되D_06-1_도록 색상을 달리하여 출력할 수 있다. 또한, 객체 식별부는 설정된 경계선을 기준으로 제2 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나의 객체를 식별 하고, 식별된 적어D_06-1_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체를 제외한 나머지 객체의 엣지 를 제2 객체의 외곽선으로 지정할 수 있다. 또한, 객체 식별부는 작업자로부터 제2 객체의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체의 외곽선을 형성할 수 있다. 그리고, 객체 식별부는 제1 객체의 외곽선 및 제2 객체의 외곽선의 너비를 합산한 너비로 경계선의 너비를 변경하고, 너비가 변경된 경계선을 인접(adjacent)한 두개의 선으로 구분하고, 구분된 두개의 선 각각을 제1 객 체의 외곽선 및 제2 객체의 외곽선과 연결할 수 있다. 즉, 객체 식별부는 입력받은 하나의 경계선을 서로밀착되어 배치되는 두개의 경계선으로 생성하고, 생성된 두개의 경계선을 각 객체에 연결할 수 있다. 한편, 객체 식별부는 상술한 방법 이외에 하기와 같이 객체를 식별할 수 있다. 구체적으로, 객체 식별부는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서 로 중첩되어 배치되는 복수의 객체의 외곽선을 지정할 수 있다. 이때, 객체 식별부는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나의 객체 를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선 택받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 객체 식별부는 통신부를 통해 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 수 신할 수 있다. 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 객체 식별부는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 복수의 객체에 대한 외곽선을 지정한 후에, 객체 식별부는 지정된 외곽선 내에서 복수의 객체 사이의 경계 선을 지정할 수 있다. 여기서, 객체 식별부는 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 객체 식별부는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입 력받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 객체 식별부는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체 의 경계선을 생성할 수 있다. 또한, 객체 식별부는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체 를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 객체 식별부는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나 의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생 성할 수 있다. 이때, 객체 식별부는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_06-1_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 경계선을 지정한 후에, 객체 식별부는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. 다음 구성으로, 객체 속성 설정부는 입출력부를 통해 작업자로부터 객체의 속성 정보를 설정하기 위 한 제어 신호를 입력받을 수 있다. 객체 속성 설정부는 작업자의 제어에 의해 추천 정보의 목록 중에서 하나의 정보가 선택되면, 선택된 정보 에 대응하는 객체의 유형에 따라 피드백(feedback)을 제공할 수 있다. 일 실시예로, 객체 속성 설정부는 선택된 정보에 대응하는 객체의 유형에 따라 서로 다르게 설정된 색상 또는 투명D_06-1_도를 반영하여, 객체 내부의 영역과 관련된 사용자 인터페이스(User Interface, UI)를 변경할수 있다. 다음 구성으로, 결과물 생성부는 어노테이션의 작업 결과물을 생성하여, 학습 데이터 생성 장치에 전 송할 수 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_06-1_도 5는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_06-1_도이다. D_06-1_도 5에 D_06-1_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 350), 메모리(Memory, 355), 송수신기(Transceiver, 360), 입출력장치(Input/output device, 365), 데이터 버스(Bus, 370) 및 스토리 지(Storage, 375)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 어노테이션 방법이 구현된 소프트웨어(380a)에 따른 명령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 어노테이션 방법이 구현된 소프트웨 어(380a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 와 데이터를 송수신할 수 있다. 입출력장치는 어노테이션 장치의 동작에 필요한 데이터를 입력 받고, 이미지를 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(180a)의 실행을 위해 필요한 애플리케이션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스(resource) 파일 등 을 저장할 수 있다. 스토리지는 어노테이션 방법이 구현된 소프트웨어(380b)를 저장할 수 있다. 또한, 스 토리지는 어노테이션 방법의 수행에 필요한 정보들을 저장할 수 있다. 특히, 스토리지는 어노테이션 작업의 대상이 되는 이미지를 저장하는 데이터베이스를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구 현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배 치되는 복수의 객체 중 제1 객체의 외곽선을 지정하는 단계, 프로세서가, 제1 객체와 중첩되어 배치된 제2 객체 사이의 경계선을 지정하는 단계, 프로세서가, 경계선을 제2 객체의 외곽선의 일부로 설정하는 단계 및 프로세서가, 경계선을 기준으로 제2 객체의 외곽선을 지정하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 본 발명의 다른 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 어노테이션 방법을 구현하기 위한 소프트웨어(380a, 380b)는 프로세서가 작업자의 제어에 따라, 인공지능학습을 위한 어노테 이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 프 로세서가, 지정된 외곽선 내에서 복수의 객체 사이의 경계선을 지정하는 단계 및 프로세서가, 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별하는 단계를 실행시키기 위하여, 기 록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다.D_06-1_도 5에 D_06-1_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또 는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_06-1_도 있다. 예컨대 기록매체는 하드 디스크, 플로 피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_06-1_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어 지는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_06-1_도록 구성될 수 있으며, 그 역D_06-1_도 마찬가지이다. 이하, 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하D_06-1_도록 한다. D_06-1_도 6은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 6을 참조하면, 먼저 S110 단계에서 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수집 장치에 이미지 수집을 요청할 수 있다. 다음으로, S120 단계에서 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지 들을 수신할 수 있다. 다음으로, S130 단계에서 학습 데이터 생성 장치는 수신한 이미지들의 컬러 정보를 추출할 수 있다. 여기서, 컬 러 정보는 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 또는 컬러 코드(color code) 값이 될 수 있다. 여기서, 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수집 장치 각각에 부여된 식별자 (identifier)를 기준으로 이미지들의 파일명 및 컬러 정보를 저장할 수 있다. 다음으로, S140 단계에서 학습 데이터 생성 장치는 이미지들 사이의 컬러 정보를 기준으로 노이즈 이미지를 분 류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지들 중 컬러 정보의 유사D_06-1_도가 사전 설정된 값보다 높 은 이미지들 중 적어D_06-1_도 하나를 노이즈 이미지로 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 동일한 식별자에 동일한 파일명을 갖는 이미지가 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06-1_도 하나를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 식별자가 상이하나 동일한 파일명이 복수개로 존재하는 경우, 동일한 파일명을 갖는 이미지 중 적어D_06-1_도 하나를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 동일한 식별자에서 같은 이름의 파일이 중복되어 등록되거나, 다른 식별자로부터 동일한 파일이 중복되어 등록되는 경우를 사전 방지할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 포함된 이미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 생성된 시퀀스 데이터의 이미지들 각각에 포함된 객체의 엣지(edge)를 추 출할 수 있다. 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 엣지 변화량을 기준으로 이 미지들의 유사D_06-1_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 중 유사D_06-1_도가 사전 설정된 값보다 높은 이미지들의 선명D_06-1_ 도(sharpness)를 산출하고, 산출된 선명D_06-1_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지 로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별로 연속된 이미지 사이의 유사D_06-1_도를 산출하고, 산출된 유사D_06-1_도를 기준으로 시퀀스 데이터 별 초당 프레임 수(frame per second)를 결정할 수 있다. 그리고, S150 단계에서 학습 데이터 생성 장치는 S140 단계에서 분류된 노이즈 이미지를 리스트화 하여 출력하 거나, 노이즈 이미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하D_06-1_도록 한다. D_06-1_도 7은 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 7을 참조하면, 먼저, S210 단계에서 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수 집 장치에 이미지 수집을 요청할 수 있다. 이때, 학습 데이터 생성 장치는 수집 조건을 포함하는 가이드 정보를 적어D_06-1_도 하나의 학습 데이터 수집 장치에 전송할 수 있다. 다음으로, S220 단계에서 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지 들을 수신할 수 있다. 다음으로, S230 단계에서 학습 데이터 생성 장치는 수집 조건과 대응하는 이미지 정보를 이미지들로부터 추출할 수 있다. 여기서, 이미지 정보는 파일 확장자, 이미지 해상D_06-1_도, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값 중 적어D_06-1_도 하나를 포함할 수 있다. 다음으로, S240 단계에서 학습 데이터 생성 장치는 추출한 이미지 정보를 가이드 정보와 비교하여, 수집 환경 또는 학습 데이터 수집 장치의 오류에 따른 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 샘플 이미지의 파일 확장자, 이미지 해상D_06-1_도, 픽셀에 대한 RGB 값 및 컬러 코드 값 중 적어D_06-1_도 하나를 포함하는 샘플 이미지 정보를 추출하고, 추출된 샘플 이미지 정보를 이미지들로부터 추출된 이미지 정보와 비교할 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와 유사 D_06-1_도가 사전 설정된 값보다 낮은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 샘플 이미지와 파일 확장자 또는 이미지 해상D_06-1_도가 상이하거나, 픽셀(pixel)에 대한 RGB(Red, Green, Blue) 값 및 컬러 코드(color code) 값의 유사D_06-1_도가 사전 설정된 값보다 낮은 경우, 해당 이미지를 노이 즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹 핑(grouping) 한 시퀀스 데이터를 생성하고, 시퀀스 데이터 별로 노이즈 이미지를 분류할 수 있다. 이때, 학습 데이터 생성 장치는 시퀀스 데이터 중 특정 이미지를 대상으로 전후 이미지의 유사D_06-1_도를 비교 하고, 전후 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 높되, 전후 이미지와 상기 특정 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 낮은 경우, 특정 이미지를 노이즈 이미지로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들 각각에 포함된 객체의 엣지(edge)를 추출하고, 이미지들 각각에 포함 된 객체를 검출하고, 검출된 객체의 위치 변화 값이 사전 설정된 값보다 높은 이미지를 노이즈 이미지로 분류할 수 있다. 즉, 학습 데이터 생성 장치는 이미지에 포함된 객체의 움직임의 정D_06-1_도를 통해 특정 이미지가 과 속 방지턱을 넘는 과정에서 촬영된 이미지인지를 판단할 수 있다. 또한, 학습 데이터 생성 장치는 적어D_06-1_도 하나의 학습 데이터 수집 장치로부터 이미지들 각각의 메타 정보 를 함께 수신할 수 있다. 여기서, 메타 정보는 이미지들 각각의 촬영 시점에서 학습 데이터 수집 장치의 위치 정보 및 속D_06-1_도 정보를 포함할 수 있다. 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06-1_도 정보에 포함된 과속 방 지턱의 위치 정보를 기준으로 메타 정보와 비교하여, 과속 방지턱의 위치에서 촬영된 이미지를 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 학습 데이터 수집 장치가 이동한 경로를 포함하는 지D_06-1_도 정보에 포함된 커브(curve)길의 위치 정보를 기준으로 메타 정보와 비교하여, 커브길의 위치에서 생성된 이미지를 노이즈 이미 지로 분류할 수 있다.또한, 학습 데이터 생성 장치는 시퀀스 데이터에서 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 유사D_06- 1_도가 사전에 설정된 값보다 낮은 이미지가 연속적으로 검출된 경우, 검출된 이미지들이 커브길에서 촬영된 이 미지들로 판단하고, 검출된 이미지들을 노이즈 이미지로 분류할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 별 연속된 이미지의 유사D_06-1_도를 기초로 노이즈 이미지를 분 류하고, 분류된 노이즈 이미지 각각의 오류 종류를 추정할 수 있다. 구제적으로, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 제1 이미지와 연속된 제2 이미지 사이의 유사D_06-1_도가 사전 설정된 값보다 낮고, 제2 이미지와 연속된 제3 이미 지와 제2 이미지의 유사D_06-1_도가 사전 설정된 값보다 높은 경우, 시퀀스 데이터에 포함된 이미지를 촬영한 카메라의 카메라 앵글(camera angle)이 변경된 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 시퀀스 데이터 중 연속된 이미지 사이의 유사D_06-1_도를 비교하고, 유사D_06- 1_도가 사전에 설정된 값보다 낮은 이미지가 사전 설정된 개수를 초과하는 경우, 시퀀스 데이터에 포함된 이미 지를 촬영한 카메라의 결속 불량에 따른 오류로 판단할 수 있다. 또한, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이미지들 및 사전 저장 된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06-1_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 각 이미지의 제1 변(side)을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이 의 RGB 값의 유사D_06-1_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나의 정점(vertex)을 식별하고, 제1 변 및 상기 제2 변으로부터 각각 식별된 두 정점을 연결한 선분을 추출할 수 있다. 학습 데이터 생성 장치는 매칭된 이미지 각각에서 추출된 선분의 길이 및 각D_06-1_도 중 적어D_06-1_도 하나를 기준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다. 하지만 이에 한정된 것은 아니고, 학습 데이터 생성 장치는 매칭된 이미지 각각에서 엣지(edge)를 추출하고, 추 출된 엣지를 기초로 매칭된 이미지 각각에 포함된 객체를 식별하고, 식별된 객체의 위치 변화 값을 기초로 노이 즈 이미지를 분류할 수 있다. 또한, 학습 데이터 생성 장치는 이미지들과 동시에 획득된 라이다(lidar)를 통해 획득된 3D 점군(3D points group) 데이터를 더 수신할 수 있다. 학습 데이터 생성 장치는 3D 점군 데이터에 포함된 거리 정보를 기초로 매 칭된 이미지 각각에 검출된 객체의 유형이 유동 객체 또는 정적 객체 인지 여부를 판단하고, 검출된 객체 중 정 적 객체의 위치 변화 값을 기초로 상기 노이즈 이미지를 분류할 수 있다. 그리고, S250 단계에서 학습 데이터 생성 장치는 분류된 노이즈 이미지를 리스트화 하여 출력하거나, 노이즈 이 미지로 분류된 이미지를 삭제할 수 있다. 이하, 본 발명의 일 실시예에 따른 작업 비용 예측 방법에 대하여 설명하D_06-1_도록 한다. D_06-1_도 8는 본 발명의 일 실시예에 따른 작업 비용 예측 방법을 설명하기 위한 순서D_06-1_도이다. D_06-1_도 8을 참조하면, S310 단계에서 학습 데이터 생성 장치는 인공지능 학습을 위하여 수행 예정인 어노테 이션 작업과 관련된 프로젝트를 수행하기 위한 적어D_06-1_도 하나의 샘플 데이터를 인공지능 학습 장치로 부터 수신할 수 있다. 다음으로, S320 단계에서 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트에 포함된 기존 데이터를 샘플 데 이터와 비교하고, 샘플 데이터와의 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 적어D_06- 1_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 기존 데이터를 대상으로, 기존 데이터를 구성하고 있는 하나 이상의 분해 구성 요소를 식별할 수 있다. 구체적으로, 샘플 데이터가 어노테이션 작업 대상이 되는 이미지에 해당되는 경우, 학습 데이터 생성 장치는 샘 플 데이터에 해당되는 이미지를 대상으로, 사용자의 제어에 따라 어노테이션 작업을 수행할 수 있다. 그리고, 학습 데이터 생성 장치는 어노테이션 작업에 의해 이미지로부터 특정된 객체의 클래스와, 객체를 특정하기 위하 여 사용된 툴을 샘플 데이터의 분해 구성요소로 식별할 수 있다또한, 학습 데이터 생성 장치는 샘플 데이터와의 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함 된 적어D_06-1_도 하나의 프로젝트를 추출할 수 있다. 이때, 학습 데이터 생성 장치는 인공지능 학습 장치로부 터 샘플 이미지의 분해 구성요소에 대한 가중치를 입력 받고, 입력 받은 가중치를 고려하여, 기존 이미지와의 유사D_06-1_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 샘플 데이터에 포 함된 객체를 검출하고, 객체의 RGB(Red, Green, Blue) 값을 기존 데이터에 포함된 객체의 RGB 값과 비교하여 유 사D_06-1_도를 평가할 수 있다. 또한, 학습 데이터 생성 장치는 기 수행된 복수의 프로젝트 별로 각각 사전 저장된 대표 이미지를 샘플 이미지 와 비교하고, 샘플 이미지와의 유사D_06-1_도가 사전 설정된 값보다 높은 복수의 대표 이미지를 인공지능 학습 장치에 전송하고, 인공지능 학습 장치로부터 복수의 대표 이미지 중 하나를 선택받을 수 있다. 이때, 학습 데이터 생성 장치는 샘플 이미지와의 유사D_06-1_도가 사전 설정된 값보다 높은 복수의 대표 이미지 를 인공지능 학습 장치에 전송하되, 대표 이미지에 포함된 객체를 식별하고, 식별된 객체의 클래스가 기밀 정보 로 사전 등록된 경우, 식별된 객체를 비식별(de-identify) 처리하여 인공지능 학습 장치에 전송할 수 있다. 그리고, S330 단계에서 학습 데이터 생성 장치는 추출된 적어D_06-1_도 하나의 프로젝트를 기초로 수행 예정인 어노테이션 작업과 관련된 프로젝트를 수행하기 위하여 요구되는 전체 작업 비용을 예측할 수 있다. 이때, 학습 데이터 생성 장치는 수행 예정인 어노테이션 작업과 관련된 프로젝트의 데이터 수량을 입력 받고, 추출된 프로 젝트의 비용 및 데이터 수량을 고려하여, 전체 작업 비용을 예측할 수 있다. 또한, 학습 데이터 생성 장치는 샘플 데이터와 유사D_06-1_도가 사전 설정된 값보다 높은 기존 데이터가 포함된 복수의 프로젝트를 추출하고, 추출된 복수의 프로젝트의 작업 비용 평균값을 수행 예정인 프로젝트의 전체 작업 비용으로 예측할 수 있다. 그리고, 학습 데이터 생성 장치는 예측된 전체 작업 비용을 출력할 수 있다. 학습 데이터 생성 장치는 사용자의 제어에 따라, 전체 작업 비용을 수정할 수D_06-1_도 있다. 그리고, 학습 데이터 생성 장치는 예측 또는 수정된 전체 작업 비용을 인공지능 학습 장치에 전송할 수 있다. 이하, 본 발명의 일 실시예에 따른 어노테이션 방법에 대하여 설명하D_06-1_도록 한다. D_06-1_도 9는 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06-1_도이고, D_06-1_도 10 내지 D_06-1_도 16은 본 발명의 일 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06-1_도이다. 먼저, D_06-1_도 10에 D_06-1_도시된 바와 같이, S410 단계에서 어노테이션 장치는 작업자의 제어에 따라, 인공 지능(AI) 학습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복 수의 객체(A, B) 중 제1 객체(A)의 외곽선을 지정할 수 있다. 이때, D_06-1_도 11에 D_06-1_도시된 바와 같이, 어노테이션 장치는 작업자로부터 제1 객체(A)의 외곽선을 따라 복수의 점(point)을 입력 받고, 복수의 점을 연결하여 제1 객체(A)의 외곽선을 형성할 수 있다. 즉, 어노테이션 장치는 작업자가 지정한 점을 연결하여 폴리곤(polygon) 형태의 영역을 생성할 수 있다. 다음으로, S420 단계에서 어노테이션 장치는 제1 객체(A)의 외곽선을 지정한 후에, 제1 객체(A)와 중첩되어 배 치된 제2 객체(A) 사이의 경계선(borderline)을 지정할 수 있다. 구체적으로, 어노테이션 장치는 제1 객체(A)의 외곽선을 따라 생성된 복수의 점 중 적어D_06-1_도 하나의 점을 선택받고, 선택받은 적어D_06-1_도 하나의 점을 기초로 경계선을 지정할 수 있다. 이때, 사용자에게 경계선에 해당하는 모든 점을 입력받는 것이 아니고, D_06-1_도 12에 D_06-1_도시된 바와 같 이, 어노테이션 장치는 복수의 점 중 임의의 두개의 점을 선택받고, D_06-1_도 13에 D_06-1_도시된 바와 같이, 선택받은 두개의 점 사이에 존재하는 적어D_06-1_도 하나의 점을 연결하는 선을 생성하고, 생성된 선을 경계선 으로 지정할 수 있다. 다음으로, D_06-1_도 14에 D_06-1_도시된 바와 같이, S430 단계에서 어노테이션 장치는 경계선을 지정한 후에 경계선을 제2 객체(B)의 외곽선의 일부로 설정할 수 있다. 이때, 어노테이션 장치는 경계선이 제1 객체(A)의 외곽선과 구분되D_06-1_도록 출력할 수 있다. 다음으로, D_06-1_도 15에 D_06-1_도시된 바와 같이, S440 단계에서 어노테이션 장치는 설정된 경계선을 기준으 로 제2 객체(B)의 외곽선을 지정할 수 있다. 이때, 어노테이션 장치는 작업자로부터 제2 객체(B)의 외곽선을 따라 복수의 점을 입력 받고, 경계선 및 복수의 점을 연결하여 제2 객체(B)의 외곽선을 형성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나의 객체를 식별 하고, 식별된 적어D_06-1_도 하나의 객체 중 경계선을 포함하는 객체 중 제1 객체(A)를 제외한 나머지 객체의 엣지를 제2 객체(B)의 외곽선으로 지정할 수 있다. 그리고, D_06-1_도 16에 D_06-1_도시된 바와 같이, S450 단계에서 어노테이션 장치는 지정된 제1 객체(A) 및 제 2 객체(B)를 각각 식별할 수 있다. 이하, 본 발명의 다른 실시예에 따른 어노테이션 방법에 대하여 설명하D_06-1_도록 한다. D_06-1_도 17은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 순서D_06-1_도이고, D_06-1_도 18 내지 D_06-1_도 21은 본 발명의 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06-1_도이다. 먼저, D_06-1_도 18에 D_06-1_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체(A, B)의 외곽선을 지정할 수 있다. 이때, 복수의 객체의 외곽선은 경계선을 제외한 외곽선이 될 수 있다. 이때, 어노테이션 장치는 작업자로부터 복수의 객체를 포함하는 외곽선을 따라 복수의 점을 입력 받고, 복수의 점을 연결하여, 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지의 엣지(edge)를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나의 객체를 식별하고, 작업자로부터 식별된 객체 중 복수의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 제외한 선택 받은 복수의 객체에 대한 외곽선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다(lidar)로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리(depth)가 일정 범위(range)를 갖는 복수의 점군을 식별하고, 작업자로부터 복수 의 점군 중 복수의 객체를 선택받고, 식별된 복수의 점군을 기초로 선택받은 복수의 객체에 대한 외곽선을 생성 할 수 있다. 그리고, 어노테이션 장치는 복수의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 바운 딩 박스 내측 영역에서 객체의 엣지를 추출하고, 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 배경을 삭제하여 복수의 객체의 외곽선을 지정할 수 있다. 다음으로, D_06-1_도 19에 D_06-1_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내에서 복수의 객체 사 이의 경계선을 지정할 수 있다. 여기서, 어노테이션 장치는 S510 단계에서 추출된 엣지를 기초로 복수의 객체에 대한 외곽선 내에서 경계선을 식별할 수 있다. 또한, 어노테이션 장치는 작업자로부터 복수의 객체에 대한 외곽선 내부에 위치한 복수의 점을 입력 받고, 입력 받은 복수의 점을 연결하여 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 복수의 점군 사이의 경계선을 복수의 객체 사이의 경계선으로 지정할 수 있다. 또한, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로, 사 전 설정된 값보다 유사한 RGB 값을 갖는 그룹을 생성하고, 생성된 그룹을 각각 객체로 인식하고, 인식된 객체의 경계선을 생성할 수 있다. 또한, 어노테이션 장치는 이미지와 동시에 라이다로부터 획득된 점군 데이터를 기초로, 지정된 외곽선 내부에 존재하는 점들 중 거리가 일정 범위를 갖는 복수의 점군을 식별하고, 작업자로부터 복수의 점군 중 하나의 객체를 선택받고, 선택받은 객체의 점군을 기초로 경계선을 생성할 수 있다. 그리고, 어노테이션 장치는 지정된 외곽선 내부의 엣지를 추출하고, 추출된 엣지를 기초로 적어D_06-1_도 하나 의 객체를 식별하고, 작업자로부터 식별된 객체 중 하나의 객체를 선택받고, 추출된 엣지를 기초로 경계선을 생 성할 수 있다. 이때, 어노테이션 장치는 생성된 경계선을 따라 사전 설정된 간격을 갖는 복수의 점을 생성하고, 작업자의 제어에 따라 복수의 점 중 적어D_06-1_도 하나의 점을 이동시켜 경계선을 수정할 수 있다. 그리고, D_06-1_도 20에 D_06-1_도시된 바와 같이, 어노테이션 장치는 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 복수의 객체를 각각 식별할 수 있다. D_06-1_도 21은 본 발명의 일 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06-1_도이다. D_06-1_도 21을 참조하면, 학습 데이터 생성 장치는 이미지들을 시간 순으로 나열하고, 나열된 이미지들을 사전 설정된 개수로 그룹핑(grouping) 한 시퀀스 데이터(sequence data)를 생성하고, 시퀀스 데이터 별로 포함된 이 미지들의 컬러 정보를 각각 비교하여 노이즈 이미지를 분류할 수 있다. 구체적으로, 학습 데이터 생성 장치는 연속하는 제1 이미지(image A) 및 제2 이미지(image B) 각각의 RGB 값에 대한 컬러 히스토그램을 생성하고, 생성된 컬러 히스토그램을 기준으로 제1 이미지(image A) 및 제2 이미지 (image B)의 유사D_06-1_도를 판단할 수 있다. 학습 데이터 생성 장치는 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_06-1_도가 사전 설정된 값보다 높은 경우, 제1 이미지(image A) 및 제2 이미지(image B) 중 적어D_06-1_도 하나를 노이즈 이미지로 판단할 수 있다. 이때, 학습 데이터 생성 장치는 이미지들 중 유사D_06-1_도가 사전 설정된 값보다 높은 이미지들의 선명D_06-1_ 도(sharpness)를 산출하고, 산출된 선명D_06-1_도가 가장 높은 이미지를 제외한 나머지 이미지를 노이즈 이미지 로 분류할 수 있다. 즉, 제1 이미지(image A) 및 제2 이미지(image B)의 유사D_06-1_도가 높게 판단된 경우, 특정 기준을 통해 제1 이미지(image A) 및 제2 이미지(image B) 중 하나를 제거해야 한다. 이를 위해, 학습 데이터 생성 장치는 선정 된 제1 이미지(image A) 및 제2 이미지(image B) 중 선명D_06-1_도가 높은 이미지를 제외한 나머지 이미지를 노 이즈 이미지로 분류하여 삭제할 수 있다. D_06-1_도 22는 본 발명의 다른 실시예에 따른 데이터 분류 방법을 설명하기 위한 예시D_06-1_도이다. D_06-1_도 22를 참조하면, 학습 데이터 생성 장치는 GPS(global positioning system) 좌표를 기초로 수신한 이 미지들 및 사전 저장된 이미지를 매칭하고, 매칭된 이미지 사이의 유사D_06-1_도를 비교하여, 노이즈 이미지를 분류할 수 있다. 즉, (A)에 D_06-1_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지의 제1 변을 구성하는 픽셀들 과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06-1_도를 기준으로 제1 변 및 제2 변으로부터 각각 하나 의 정점(point A, B)을 식별할 수 있다. 이때, 식별된 각각 하나의 정점은 정적 객체인 가이드 레일이 될 수 있 다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, B)을 연결한 선분(line A)을 추출할 수 있다. 그리고, (B)에 D_06-1_도시된 바와 같이, 학습 데이터 생성 장치는 사전 저장된 이미지와 동일한 위치에 존재하 는 이미지에서 제1 변을 구성하는 픽셀들과 제2 변을 구성하는 픽셀들 사이의 RGB 값의 유사D_06-1_도를 기준으 로 제1 변 및 제2 변으로부터 각각 하나의 정점(point A, C)을 식별할 수 있다. 학습 데이터 생성 장치는 제1 변 및 제2 변으로부터 각각 식별된 두 정점(point A, C)을 연결한 선분(line B)을 추출할 수 있다. 그리고, 학습 데이터 생성 장치는 추출된 선분의 길이 및 각D_06-1_도 중 적어D_06-1_도 하나를 기준으로 사전 설정된 오차 범위를 벗어나는 경우, 카메라 앵글이 변경된 오류로 판단할 수 있다.D_06-1_도 23 및 D_06-1_도 24는 본 발명의 또 다른 실시예에 따른 어노테이션 방법을 설명하기 위한 예시D_06- 1_도이다. D_06-1_도 23 및 D_06-1_도 24를 참조하면, 본 발명의 또 다른 실시예에 따른 어노테이션 장치는 외곽선에 포함 된 두 점과 작업자의 제어에 따라 입력된 복수 개의 점(point)들을 연속적으로 연결하여 설정된 복수 개의 간선 을 포함시켜 경계선(borderline)을 지정할 수 있다. 객체 사이의 경계선을 지정함에 있어서, 어노테이션 장치는 작업자로부터 입력된 복수 개의 점들을 연결하는 간 선을 기초로 객체 사이의 경계선을 지정하되, 객체의 엣지를 기초로 작업자로부터 지정된 경계선 사이의 점에 대한 보간(interpolation)을 수행할 수 있다. 구체적으로, D_06-1_도 24에 D_06-1_도시된 바와 같이, 어노테이션 장치는 지정된 외곽선 내부에 위치한 픽셀 (pixel)의 RGB(Red, Green, Blue) 값을 기초로 지정된 외곽선 내부에 위치한 객체의 엣지(edge)를 추출하고, 간선(line)의 중심점을 기준으로 간선과 수직한 가상선을 생성할 수 있다. 그리고, 어노테이션 장치는 생성된 가상선과 추출된 엣지가 교차하는 지점(c)으로부터 중심점 사이의 거리(x)가 사전 설정된 값보다 큰 간선에 포함된 점을 보간할 수 있다. 하지만, 이에 한정된 것은 아니고, 어노테이션 장치는 복수 개의 간선 각각의 길이가 사전 설정된 값을 초과하 는 간선에 포함된 점을 보간할 수D_06-1_도 있다. 즉, 어노테이션 장치는 간선의 길이를 기준으로, 길이가 사전 설정된 값보다 긴 간선에 대하여 보간이 필요한 간선으로 판단하고, 해당 간선에 포함된 점에 대한 보간을 수행 할 수 있다. 일 실시예로, 어노테이션 장치는 가상선과 추출된 엣지가 교차하는 지점(c)에 보간을 위한 신규 점을 추가할 수 있다. 또한, 어노테이션 장치는 사용자의 프로파일을 기초로 작업자로부터 입력된 점에 대한 보정을 수행할 수 있다. 여기서, 사용자의 프로파일은 작업자가 점을 입력하기 위한 입력 D_06-1_도구 및 입력 D_06-1_도구를 사용하기 위한 작업자의 주 사용손에 대한 정보를 포함할 수 있다. 구체적으로, 어노테이션 장치는 작업자의 프로파일을 획득하고, 획득된 프로파일을 기초로 복수 개의 점들을 입 력하기 위한 입력 D_06-1_도구 및 입력 D_06-1_도구를 사용하기 위한 주 사용손을 식별할 수 있다. 예를 들어, 어노테이션 장치는 해당 작업자가 입력 D_06-1_도구로 마우스(mouse)를 사용하고, 오른손을 주 사용 손으로 사용하는 것으로 식별할 수 있다. 어노테이션 장치는 입력 D_06-1_도구 및 주 사용손에 따른 사전 저장된 보정 테이블로부터 식별된 입력 D_06-1_ 도구 및 주 사용손에 대응되는 보정 값을 획득하고, 획득된 보정 값을 기초로 작업자로부터 입력된 복수 개의 점들을 일괄적으로 보정할 수 있다. 예를 들어, 작업자가 마우스를 사용하고 오른손을 주 사용손으로 사용하는 경우, 해당 작업자에 의해 입력된 점 은 마우스의 버튼을 입력하는 과정에서 왼쪽 아래 방향으로 치우치는 경향을 보인다. 이에 따라, 어노테이션 장치는 프로파일에 따른 보정 테이블을 사전 저장하고, 식별된 프로파일에 따라 입력된 복수 개의 점들을 일괄적으로 보정할 수 있다. 또한, 어노테이션 장치는 작업자를 대상으로 누적 기록된 보정 히스토리를 기초로 작업자로부터 입력된 복수 개 의 점들을 일괄적으로 보정할 수 있다. 예를 들어, 어노테이션 장치는 프로파일에 따른 보정 테이블을 기초로 입력된 복수 개의 점들을 보정하되, 누적 기록된 보정 히스토리를 기초로 가중치를 부여하여 보정 강D_06-1_도를 조절할 수 있다. 그리고, 어노테이션 장치는 식별된 객체의 유형에 대응되는 3D 모델을 추출하여 작업자로부터 입력된 복수 개의 점을 검증할 수 있다. 구체적으로, 어노테이션 장치는 식별된 객체의 외곽선을 기초로 객체의 유형을 추정하고, 추정된 객체의 유형별 로 사전에 저장된 3D 모델을 추출할 수 있다. 예를 들어, 어노테이션 장치는 식별된 객체의 외곽선의 형태를 기초로 객체의 유형을 특정 자동차로 추정하고, 특정 자동차에 해당하는 3D 모델을 추출할 수 있다. 어노테이션 장치는 추출된 3D 모델을 3D 회전시켜 객체의 촬영 방향을 식별하고, 식별된 촬영 방향에 대응하여 사전에 설정된 객체 별 필수 구성요소를 식별할 수 있다. 예를 들어, 어노테이션 장치는 추출된 3D 모델을 3D 회전시키면서 식별된 객체와의 외곽선에 대한 유사D_06- 1_도가 사전 설정된 값보다 높은 시점을 객체의 촬영 방향으로 식별할 수 있으며, 식별된 객체가 자동차의 경우, 타이어, 본네트, 사이드 미러, 트렁크 등을 필수 구성요소로 식별할 수 있다. 이후, 어노테이션 장치는 식별된 객체의 폐쇄 영역 내에 필수 구성요소가 존재하는지 판단하여 작업자의 제어에 따라 입력된 점을 검증할 수 있다. 이때, 어노테이션 장치는 식별된 객체에 필수 구성요소가 존재하지 않는 경우, 해당 영역에 특정 기호를 표시하 여 해당 영역을 작업자가 직관적으로 인지하여 해당 영역에 입력된 점을 보정할 수 있D_06-1_도록 할 수 있다. 예를 들어, 어노테이션 장치는 3D 모델에서 식별된 필수 구성요소에 대한 외곽선을 인식된 객체에 이식하되 색 상 또는 굵기를 달리하여 표시할 수 있다. D_06-1_도 25 및 D_06-1_도 26은 중첩된 복수 개의 객체의 외곽선을 설명하기 위한 예시D_06-1_도이다. 먼저, D_06-1_도 25에 D_06-1_도시된 바와 같이, S510 단계에서 어노테이션 장치는 인공지능(AI) 학습을 위한 어노테이션 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수 개의 객체의 외곽선을 지정할 수 있다. 이때, 복수 개의 객체의 외곽선은 경계선(borderline)을 제외한 외곽선이 될 수 있다. 이하에서는, 제1 객체(A), 제2 객체(B) 및 제3 객체(C)의 적어D_06-1_도 일부가 서로 중첩되어 배치된 것을 예 로 설명하기로 한다. 다음으로, 어노테이션 장치는 지정된 외곽선 내에서 제1 객체(A), 제2 객체(B) 및 제3 객체(C) 사이의 중첩된 경계선(borderline)을 지정할 수 있다. D_06-1_도 25를 참조하여 구체적으로 설명하면, 어노테이션 작업의 대상이 되는 이미지에 서로 중첩된 제1 객체 (A), 제2 객체(B), 제3 객체(C)가 존재할 경우, S510 단계는 제1 객체(A), 제2 객체(B), 제3 객체(C)를 포함하 는 굵은 선(외곽선)을 지정하고, S520 단계는 굵은 선 내에서 제1 객체(A), 제2 객체(B), 제3 객체(C)의 중첩된 얇은 선(경계선, borderline)을 지정할 수 있다. 여기서, 얇은 선(경계선, borderline)은, D_06-1_도 26에 D_06-1_도시된 바와 같이, 제1 객체(A)와 제3 객체 (C)가 중첩되는 부분의 쇄선 경계선(BL1), 제1 객체(A)와 제2 객체(B)가 중첩되는 부분의 실선 경계선(BL2), 제 2 객체(B)와 제3 객체(C)가 중첩되는 부분의 점선 경계선(BL3)으로 지정될 수 있으며, 이와 같이 지정된 각 경 계선(BL1, BL2, BL3) 중 하나는 적어D_06-1_도 두 개의 객체의(A,B,C) 외곽선으로 형성된다. 즉, 쇄선 경계선(BL1)은, 제1 객체(A)의 외관선을 형성하는 동시에 제3 객체(C)의 외곽선을 형성할 수 있고, 실 선 경계선(BL2)은, 제1 객체(A)의 외곽선을 형성하는 동시에 제2 객체(B)의 외곽선을 형성할 수 있으며, 점선 경계선(BL3)은, 제2 객체(B)의 외곽선을 형성하는 동시에 제3 객체(C)의 외곽선을 형성할 수 있다. 이를 통해, 본 발명은 폴리곤 기법으로 복수의 객체의 사이에 중첩된 경계선을 복수 개 생성(예, 제1 객체(A)와 제2 객체(B)의 경계선인 실선 경계선(BL2)를 각 객체의 외곽선으로 두 번 지정)했을 경우 두 번 지정된 실선 경 계선(BL2)이 일치하지 않는 문제점을 해결할 수 있다. 이상과 같이, 본 명세서와 D_06-1_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실시예 외에D_06-1_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_06-1_도면에서 특정 용 어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미 에서 사용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에 서 제한적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합 리적해석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다.D_06-1_부호의 설명 100 : 학습 데이터 수집 장치 200 : 학습 데이터 생성 장치 300 : 어노테이션 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 데이터 설계부 220 : 데이터 수집부 225 : 데이터 정제부 230 : 데이터 납품부 235 : 저장부 305 : 통신부 310 : 입출력부 315 : 저장부 320 : 객체 식별부 325 : 객체 속성 설정부 330 : 결과물 생성부 D_06-1_청구범위 D_06-1_청구항 1 어노테이션 장치가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학습을 위한 어노테이션 (annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수 개의 객체를 포함하는 외곽선 을 지정하는 단계; 상기 어노테이션 장치가, 상기 지정된 외곽선 내에 포함된 객체들을 서로 구분하기 위한 상기 복수 개의 객체들 사이의 경계선을 지정하는 단계; 및 상기 어노테이션 장치가, 상기 지정된 경계선을 기준으로 구획된 복수 개의 영역별로 상기 복수 개의 객체들을 각각 식별하는 단계; 를 포함하되, D_06-1_청구항 2 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 서로 중첩되어 배치되는 적어D_06-1_도 제1 객체와 제2 객체를 포함하는 복수의 객체의 전체 외곽선을 지정하는 것을 특징으로 하고, 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내에서 상기 제1 객체와 상기 제2 객체 사이의 중첩된 경계선을 지정하는 것을 특징으로 하 며, 상기 경계선은, 상기 제1 객체의 제2 객체와 중첩되는 부분의 외곽선을 형성하는 동시에, 상기 제2 객체의 제1 객체와 중첩되는 부분의 외곽선을 형성하는 것을 특징으로 하는, 어노테이션 방법. D_06-1_청구항 3 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 외곽선에 포함된 두 점과 상기 작업자의 제어에 따라 입력된 복수 개의 점들을 연속적으로 연결하여 설정 된 복수 개의 간선을 포함시켜 상기 경계선을 지정하는 것을 특징으로 하는, 어노테이션 방법.D_06-1_청구항 4 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 상기 작업자로부터 상기 복수 개의 객체를 포함하는 외곽선을 따라 복수 개의 점을 입력 받고, 상기 복수 개의 점을 연결하여, 상기 복수 개의 객체에 대한 외곽선을 생성하는 것을 특징으로 하는, 어노테이션 방법. D_06-1_청구항 5 제1 항에 있어서, 상기 외곽선을 지정하는 단계는 상기 복수 개의 객체를 포함하는 일부 영역을 바운딩 박스(bounding box)로 설정하고, 상기 바운딩 박스 내측 영역에서 객체의 엣지를 추출하고, 상기 추출된 엣지를 기준으로 객체(object)와 배경(background)을 구분하고, 상기 배경을 삭제하여 상기 복수 개의 객체의 외곽선을 지정하는 것을 특징으로 하는, 어노테이션 방법. D_06-1_청구항 6 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 지정된 외곽선 내부에 위치한 픽셀(pixel)의 RGB(Red, Green, Blue) 값을 기초로 상기 지정된 외곽선 내부 에 위치한 객체의 엣지를 추출하고, 상기 복수 개의 간선 각각의 중심점을 기준으로 간선과 수직한 가상선을 생 성하며, 상기 생성된 가상선과 상기 추출된 엣지가 교차하는 지점으로부터 상기 중심점 사이의 거리가 사전 설 정된 값보다 큰 간선에 포함된 점을 보간(interpolation)하는 것을 특징으로 하는, 어노테이션 방법. D_06-1_청구항 7 제6 항에 있어서, 상기 경계선을 지정하는 단계는 상기 가상선과 상기 추출된 엣지가 교차하는 지점에 보간을 위한 신규 점을 추가하는, 어노테이션 방법. D_06-1_청구항 8 제6 항에 있어서, 상기 경계선을 지정하는 단계는 상기 복수 개의 간선 각각의 길이가 사전 설정된 값을 초과하는 간선에 포함된 점을 보간하는 것을 특징으로 하 는, 어노테이션 방법. D_06-1_청구항 9 제1 항에 있어서, 상기 경계선을 지정하는 단계는 상기 작업자의 프로파일을 획득하고, 상기 획득된 프로파일을 기초로 상기 복수 개의 점들을 입력하기 위한 입 력 D_06-1_도구 및 상기 입력 D_06-1_도구를 사용하기 위한 주 사용손을 식별하고, 보정 테이블로부터 상기 식 별된 입력 D_06-1_도구 및 상기 주 사용손에 대응되는 보정 값을 획득하고, 상기 획득된 보정 값을 기초로 작업 자로부터 입력된 복수 개의 점들을 일괄적으로 보정하는 것을 특징으로 하는, 어노테이션 방법. D_06-1_청구항 10 제1 항에 있어서, 상기 식별하는 단계는 상기 식별된 객체의 외곽선을 기초로 객체의 유형을 추정하고, 상기 추정된 객체의 유형별로 사전에 저장된 3D 모델을 추출하고, 상기 추출된 3D 모델을 3D 회전시켜 객체의 촬영 방향을 식별하고, 상기 식별된 촬영 방향에 대응하여 사전에 설정된 객체 별 필수 구성요소를 식별하고, 상기 식별된 객체의 폐쇄 영역 내에 필수 구성요소 가 존재하는지 판단하여 상기 작업자의 제어에 따라 입력된 점을 검증하는 것을 특징으로 하는, 어노테이션 방법."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 354, "content": "D_06-1_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 355, "content": "D_06-1_요약 본 발명은 이미지 속에 포함된 중첩된 객체를 용이하게 지정할 수 있는, 경계선 지정을 통한 어노테이션 방법을 제안한다. 상기 방법은 어노테이션 장치가, 작업자의 제어에 따라, 인공지능(Artificial Intelligence, AI) 학 습을 위한 어노테이션(annotation) 작업의 대상이 되는 이미지에 포함된 서로 중첩되어 배치되는 복수의 객체의 외곽선을 지정하는 단계, 상기 어노테이션 장치가, 상기 지정된 외곽선 내에서 상기 복수의 객체 사이의 경계선 을 지정하는 단계 및 상기 어노테이션 장치가, 상기 지정된 경계선을 기준으로 구획된 복수의 영역을 기준으로 상기 복수의 객체를 각각 식별하는 단계를 포함할 수 있다. D_06-1_대표D_06-1_도 D_06-1_도 23 D_06-1_도면 D_06-1_도 1 D_06-1_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 356, "content": "D_06-1_도 3 D_06-1_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 357, "content": "D_06-1_도 5 D_06-1_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 358, "content": "D_06-1_도 7 D_06-1_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 359, "content": "D_06-1_도 9 D_06-1_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 360, "content": "D_06-1_도 11 D_06-1_도 12"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 361, "content": "D_06-1_도 13 D_06-1_도 14"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 362, "content": "D_06-1_도 15 D_06-1_도 16"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 363, "content": "D_06-1_도 17 D_06-1_도 18"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 364, "content": "D_06-1_도 19 D_06-1_도 20"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 365, "content": "D_06-1_도 21 D_06-1_도 22 D_06-1_도 23"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 366, "content": "D_06-1_도 24 D_06-1_도 25"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 367, "content": "D_06-1_도 26"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 368, "content": "D_07_발명의 설명 D_07_발명의 명칭 객체 검출 방법 및 이를 실행하기 위하여 기록매체에 기록된 컴퓨터 프로그램{Method for object detection, and computer program recorded on record-medium for executing method thereof}"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 369, "content": "D_07_기술분야 본 발명은 인공지능(Artificial Intelligence, AI) 학습용 데이터의 가공에 관한 것이다. 보다 상세하게는, 인 공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션(annotation) 작업과 관련하여, 라이다(lidar)에 의해 획 득된 3D 점군 데이터와, 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출하기 위한 방법 및 이를 실행하 기 위하여 기록매체에 기록된 컴퓨터 프로그램에 관한 것이다. D_07_발명의 배경이 되는 기술 인공지능(AI)은 인간의 학습능력, 추론능력 및 지각능력 등의 일부 또는 전부를 컴퓨터 프로그램을 이용하여 인 공적으로 구현하는 기술을 의미한다. 인공지능(AI)과 관련하여, 기계 학습(machine learning)은 다수의 파라미 터로 구성된 모델을 이용하여 주어진 데이터로 파라미터를 최적화하는 학습을 의미한다. 이와 같은, 기계 학습 은 학습용 데이터의 형태에서 따라, 지D_07_도 학습(supervised learning), 비지D_07_도 학습(unsupervised learning) 및 강화 학습(reinforcement learning)으로 구분된다. 일반적으로, 인공지능(AI) 학습용 데이터의 설계는 데이터 구조의 설계, 데이터의 수집, 데이터의 정제, 데이터 의 가공, 데이터의 확장 및 데이터의 검증 단계로 진행된다. 각각의 단계에서 대하여 보다 구체적으로 설명하면, 데이터 구조의 설계는 온톨로지(ontology) 정의, 분류 체계 의 정의 등을 통해 이루어진다. 데이터의 수집은 직접 촬영, 웹 크롤링(web crawling) 또는 협회/전문 단체 등 을 통해 데이터를 수집하여 이루어진다. 데이터 정제는 수집된 데이터 내에서 중복 데이터를 제거하고, 개인 정 보 등을 비식별화하여 이루어진다. 데이터의 가공은 어노테이션(annotation)을 수행하고, 메타데이터(metadat a)를 입력하여 이루어진다. 데이터의 확장은 온톨로지 매핑(mapping)을 수행하고, 필요에 따라 온톨로지를 보완 하거나 확장하여 이루어진다. 그리고, 데이터의 검증은 다양한 검증 D_07_도구를 활용하여 설정된 목표 품질에 따른 유효성을 검증하여 이루어진다. 한편, 차량의 자율주행(automatic driving)은 차량 스스로 판단하여 주행할 수 있는 시스템을 의미한다. 이와 같은, 자율주행은 시스템이 주행에 관여하는 정D_07_도와 운전자가 차량을 제어하는 정D_07_도에 따라 비자동화 부터 완전 자동화까지 점진적인 단계로 구분될 수 있다. 일반적으로, 자율주행의 단계는 국제자동차기술자협회 (SAE(Society of Automotive Engineers) International)에서 분류한 6단계의 레벨로 구분된다. 국제자동차기술 자협회가 분류한 6단계에 따르면, 레벨 0단계는 비자동화, 레벨 1단계는 운전자 보조, 레벨 2단계는 부분 자동 화, 레벨 3단계는 조건부 자동화, 레벨 4단계는 고D_07_도 자동화, 그리고 레벨 5단계는 완전 자동화 단계이다. 자율주행은 인지(perception), 측위(localization), 경로 계획(path planning) 및 제어(control)의 메커니즘을 통해 수행된다. 그리고, 다양한 기업체들은 자율주행 메커니즘에서 인지 및 경로 계획을 인공지능(AI)을 이용하 여 구현하기 위해 개발 중에 있다. 상술한 바와 같은, 자율주행에 사용될 수 있는 인공지능(AI)의 기계 학습에 사용되는 데이터는 적게는 몇 천개 에서, 많게는 수 백만개에 이르는 많은 수로 이루어진다. 따라서, 많은 수의 데이터를 보다 용이하게 가공할 수 있는 다양한 수단들이 요구되고 있는 실정이다. D_07_선행기술문헌 D_07_특허문헌 대한민국 등록특허공보 제10-2230144호, ‘인공 지능 심층 학습 타겟 탐지 및 속D_07_도 퍼텐셜 필드 알고리즘 기반 장애물 회피 및 자율 주행 방법 및 장치’, (2021.03.15. 공고)"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 370, "content": "D_07_발명의 내용 D_07_해결하고자 하는 과제 본 발명의 일 목적은 인공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터와 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출하기 위한 방법을 제공하는 것 이다. 본 발명의 또 다른 목적은 인공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터와 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출하기 위한 방법을 실행하 기 위하여 기록매체에 기록된 컴퓨터 프로그램을 제공하는 것이다. 본 발명의 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 371, "content": "D_07_과제의 해결 수단 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 인공지능(AI)을 기계 학습하기 위한 데이터의 어 노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터와 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출하기 위한 방법을 제안한다. 상기 방법은 어노테이션 장치가, 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group) 및 카메라(camera)를 통해 획득된 복수의 이미지(image) 중 적어 D_07_도 하나를 수신하는 단계 및 상기 어노테이션 장치가, 상기 수신된 점군 데이터 또는 상기 복수의 이미지 내에 포함된 적어D_07_도 하나 이상의 객체(object)를 검출하는 단계를 포함할 수 있다. 구체적으로, 상기 객체를 검출하는 단계는, 상기 어노테이션 장치가, 상기 3D 점군 데이터(3D point group)를 사전 설정된 뷰 포인트(view point)로 설정하고, 상기 사전 설정된 뷰 포인트의 3D 점군 데이터 내에서 적어 D_07_도 하나 이상의 객체(object)를 특정하는 단계 및 상기 어노테이션 장치가, 상기 특정된 객체에 대응되 D_07_도록 상기 특정된 객체를 포함하는 큐보이드(cuboid)를 생성하는 단계; 를 포함하는 것을 특징으로 한다. 상기 객체를 특정하는 단계는 상기 3D 점군 데이터의 탑-뷰(top-view)를 투영하고, 상기 탑-뷰로 투영된 3D 점 군 데이터 내에서 객체에 대응하는 큐보이드의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개 의 꼭지점 및 객체의 방향을 선택받는 것을 특징으로 한다. 상기 큐보이드를 생성하는 단계는 상기 선택받은 두 개의 꼭지점 및 상기 객체의 방향을 기초로 상기 큐보이드 의 밑면을 생성하고, 상기 밑면에 사전 설정된 높이 값을 부여하여 상기 큐보이드를 생성하는 것을 특징으로 한 다. 상기 객체를 특정하는 단계는 상기 3D 점군 데이터의 프론트-뷰(front-view)를 투영하고, 상기 프론트-뷰로 투 영된 3D 점군 데이터 내에서 객체의 전면에 대응하는 큐보이드의 전면에 포함된 네 개의 꼭지점 중 서로 대향하 는 두개의 꼭지점을 선택받는 단계 및 상기 3D 점군 데이터의 사이드-뷰(side-view)를 투영하고, 상기 선택받은 두개의 꼭지점 중 하나로부터 상기 객체의 측면 폭에 대응하는 하나의 포인트를 선택받는 단계를 포함하는 것을 특징으로 한다. 상기 두개의 꼭지점을 선택받는 단계는 사전 설정된 기준 방향을 기초로 상기 3D 점군 데이터의 프론트-뷰를 투 영하되, 상기 프론트-뷰로 투영된 3D 점군 데이터의 뷰 포인트를 보정하기 위한 보정 값을 입력 받는 것을 특징 으로 한다. 상기 두개의 꼭지점을 선택받는 단계는 상기 선택받은 두개의 꼭지점을 기초로 상기 객체의 전면 폭 및 전면 높 이를 구성하는 박스를 생성하되, 상기 객체에 해당하는 점군 중 상기 라이다와 가장 가까이에 위치하는 포인트 를 식별하고, 상기 식별된 포인트를 기준으로 상기 박스를 생성하는 것을 특징으로 한다. 상기 큐보이드를 생성하는 단계는 상기 생성된 박스를 상기 선택받은 하나의 포인트까지 연장하여 큐보이드를 생성하는 것을 특징으로 한다. 상기 큐보이드를 생성하는 단계 이후에 상기 3D 점군 데이터의 밀D_07_도를 기초로 상기 객체의 엣지(edge)를 검출하고, 상기 검출된 엣지와의 간격이 최소화되D_07_도록 상기 큐보이드의 크기 및 방향 중 적어D_07_도 하나 를 재조정하는 것을 특징으로 한다. 상기 큐보이드를 생성하는 단계 이후에 상기 3D 점군 데이터와 대응하는 이미지를 추출하고, 상기 추출된 이미 지에 포함된 객체의 엣지(edge)를 추출하고, 상기 추출된 엣지에 의한 폐쇄 영역(enclosure)을 하나 이상 추출 하고, 상기 추출된 폐쇄 영역을 통해 상기 객체를 식별하고, 상기 식별된 객체와의 간격이 최소화되D_07_도록 상기 큐보이드의 크기 및 방향 중 적어D_07_도 하나를 재조정하는 것을 특징으로 한다. 상기 객체를 검출하는 단계는 사전 설정된 기계 학습(machine learning)된 인공지능을 통해 점군의 밀D_07_도를 기초로 사전 설정된 거리 이내에 위치한 적어D_07_도 하나의 객체를 특정하고, 상기 특정된 적어D_07_도 하나의 객체를 구성하는 점군 중 상기 라이다와 가장 가까운 점을 기준으로 큐보이드를 생성하는 것을 특징으로 한다. 상기 객체를 검출하는 단계는 상기 수신된 복수의 이미지를 시간 순으로 배열하고, 상기 배열된 복수의 이미지 각각에 포함된 적어D_07_도 하나의 객체에 순차적으로 바운딩 박스를 생성하되, 연속된 이미지 사이의 연관성을 기초로 상기 적어D_07_도 하나의 객체를 추적하여 각 이미지의 객체를 검출하는 것을 특징으로 한다. 상기 객체를 검출하는 단계는 연속된 이미지 사이에 검출된 동일한 객체 사이의 거리를 추정하고, 상기 추정된 동일한 객체 사이의 거리 및 촬영된 시간을 기초로 상기 객체의 속D_07_도를 추정하고, 상기 추정된 객체의 속 D_07_도를 기초로 각 이미지의 객체를 추적하는 것을 특징으로 한다. 상기 객체를 검출하는 단계는 상기 시간 순으로 배열된 복수의 이미지를 사전 설정된 개수로 그룹핑(grouping) 하고, 각 그룹에 포함된 이미지 중 첫번째 이미지를 출력하여 바운딩 박스를 입력받고, 상기 입력받은 바운딩 박스를 기초로 객체를 추적하여 각 그룹별로 나머지 이미지에 대한 바운딩 박스를 생성하는 것을 특징으로 한다. 상술한 바와 같은 기술적 과제를 달성하기 위하여, 본 발명은 객체 검출 방법을 실행하기 위하여 기록매체에 기 록된 컴퓨터 프로그램을 제안한다. 상기 컴퓨터 프로그램은 메모리(memory); 및 상기 메모리에 상주된 명령어를 처리하는 프로세서(processor)를 포함하여 구성된 컴퓨팅 장치와 결합될 수 있다. 그리고, 상기 컴퓨터 프로그 램은 상기 프로세서가, 상기 프로세서가, 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group) 및 카메라(camera)를 통해 획득된 복수의 이미지(image) 중 적어D_07_도 하나를 수신하는 단계 및 상기 프로세서가, 상기 수신된 점군 데이터 또는 상기 복수의 이미지 내에 포함된 적어D_07_도 하나 이상의 객 체(object)를 검출하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 D_07_도면들에 포함되어 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 372, "content": "D_07_발명의 효과 본 발명의 실시 예들에 따르면, 3D 점군 데이터를 사전 설정된 뷰 포인트로 투영하고, 투영된 뷰 포인트 상에서 큐보이드를 생성함으로써, 어노테이션 작업에 수행되는 시간을 대폭 감소시킬 수 있다. 또한, 본 발명의 실시 예들에 따르면, 시간 순으로 배열된 복수의 이미지 각각에 포함된 적어D_07_도 하나의 객 체에 순차적으로 바운딩 박스를 생성하되, 연속된 이미지 사이의 연관성을 기초로 객체를 추적하여 각 이미지의 객체를 검출함으로써, 별D_07_도의 학습 데이터가 불필요하며 검출 정확D_07_도를 높일 수 있다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 373, "content": "본 발명의 효과들은 이상에서 언급한 효과로 제한되지 아니하며, 언급되지 않은 또 다른 효과들은 청구범위의"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 374, "content": "기재로부터 본 발명이 속한 기술분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 375, "content": "D_07_도면의 간단한 설명 D_07_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성D_07_도이다. D_07_도 3은 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_07_도이다. D_07_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_07_도이다. D_07_도 5는 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 검출 방법을 설명하기 위한 순서D_07_도이 다. D_07_도 6은 본 발명의 일 실시예에 따른 이미지를 이용한 객체 검출 방법을 설명하기 위한 순서D_07_도이다. D_07_도 7 내지 D_07_도 13은 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 검출 방법을 설명하기 위 한 예시D_07_도이다. D_07_도 14는 본 발명의 일 실시예에 따른 이미지를 이용한 객체 검출 방법을 설명하기 위한 예시D_07_도이다."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 376, "content": "D_07_발명을 실시하기 위한 구체적인 내용 본 명세서에서 사용되는 기술적 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하 려는 의D_07_도가 아님을 유의해야 한다. 또한, 본명세서에서 사용되는 기술적 용어는 본 명세서에서 특별히 다 른 의미로 정의되지 않는 한, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해 되는 의미로 해석되어야 하며, 과D_07_도하게 포괄적인 의미로 해석되거나, 과D_07_도하게 축소된 의미로 해석 되지 않아야 한다. 또한, 본 명세서에서 사용되는 기술적인 용어가 본 발명의 사상을 정확하게 표현하지 못하는 잘못된 기술적 용어일 때에는, 당업자가 올바르게 이해할 수 있는 기술적 용어로 대체되어 이해되어야 할 것이다. 또한, 본 발명에서 사용되는 일반적인 용어는 사전에 정의되어 있는 바에 따라, 또는 전후 문맥상에 따라 해석되어야 하며, 과D_07_도하게 축소된 의미로 해석되지 않아야 한다. 또한, 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"가지다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않을 수D_07_도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 또한, 본 명세서에서 사용되는 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으 면서 제1 구성 요소는 제2 구성 요소로 명명될 수 있고, 유사하게 제2 구성 요소D_07_도 제1 구성 요소로 명명 될 수 있다. 어떤 구성 요소가 다른 구성 요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성 요소 에 직접적으로 연결되어 있거나 또는 접속되어 있을 수D_07_도 있지만, 중간에 다른 구성 요소가 존재할 수 D_07_도 있다. 반면에, 어떤 구성 요소가 다른 구성 요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있다고 언급된 때에는, 중간에 다른 구성 요소가 존재하지 않는 것으로 이해되어야 할 것이다. 이하, 첨부된 D_07_도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하되, D_07_도면 부호에 관 계없이 동일하거나 유사한 구성 요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한 다. 또한, 본 발명을 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있 다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 D_07_도면은 본 발명의 사상을 쉽게 이해할 수 있D_07_도록 하기 위한 것일 뿐, 첨부된 D_07_도면에 의해 본 발명의 사상이 제한되는 것으로 해석되어서는 아 니 됨을 유의해야 한다. 본 발명의 사상은 첨부된 D_07_도면 외에 모든 변경, 균등물 내지 대체물에 까지D_07_ 도 확장되는 것으로 해석되어야 한다. 한편, 인공지능(AI) 학습용 데이터를 생성하기 위한 이미지의 가공은 적게는 몇 천개에서, 많게는 수 백만개에 이르는 많은 수의 이미지를 대상으로 이루어진다. 따라서, 종래의 수단들에 따르면, 많은 수의 데이터를 보다 용이하게 가공할 수 있는 다양한 수단들이 요구되고 있는 실정이다. 이러한 한계를 극복하고자, 본 발명은 인공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하 여, 라이다에 의해 획득된 3D 점군 데이터와 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출할 수 있는 다양한 수단들을 제안하고자 한다. D_07_도 1 및 2는 본 발명의 다양한 실시예에 따른 인공지능 학습 시스템을 나타낸 구성D_07_도이다. D_07_도 1에 D_07_도시된 바와 같이, 본 발명의 일 실시예에 따른 인공지능 학습 시스템은 학습 데이터 생성 장 치, 하나 이상의 어노테이션 장치(200-1, 200-2, …, 200-n; 200), 학습 데이터 검증 장치 및 학습 장치를 포함하여 구성될 수 있다. 또한, D_07_도 2에 D_07_도시된 바와 같이, 본 발명의 다른 실시예에 따른 인공지능 학습 시스템은 하나 이상의 어노테이션 장치(200-a, 200-b, …, 200-m; 200)와 학습 데이터 검증 장치(300-a, 300-b, …, 300-m; 300)가 하나로 이루어진 복수 개의 그룹(Group-a, Group-b …, Group-m), 학습 데이터 생성 장치 및 학습 장치 를 포함하여 구성될 수 있다. 이와 같은, 다양한 실시예에 따른 인공지능 학습 시스템의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소 가 실제 물리적 환경에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 학습 데이터 생성 장치는 차량의 자율주행에 사용될 수 있는 인공지 능(AI)을 기계 학습시키기 위한 데이터를 설계 및 생성하는데 사용될 수 있는 장치이다. 이와 같은, 학습 데이터 생성 장치는 기본적으로 학습 데이터 검증 장치와 구분되는 장치이나, 실제 물리적 환경에서 학습 데이터 생성 장치와 학습 데이터 검증 장치가 하나의 장치로 통합되어 구현될 수D_07_도 있다. 구체적으로, 학습 데이터 설계 장치는 학습 장치로부터 인공지능(AI) 학습과 관련된 프로젝트의 속성 을 수신할 수 있다. 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지능(AI) 학습을 위한 데이터 구조의 설계, 수집된 데이터의 정제, 데이터의 가공, 데이터의 확장 및 데이터의 검 증을 수행할 수 있다. 우선적으로, 학습 데이터 설계 장치는 인공지능(AI) 학습을 위한 데이터 구조를 설계할 수 있다. 예를 들 어, 학습 데이터 설계 장치는 사용자의 제어 및 수신된 프로젝트의 속성을 기초로, 인공지능(AI) 학습을 위한 온톨로지(ontology), 인공지능(AI) 학습을 위한 데이터의 분류 체계를 정의할 수 있다. 학습 데이터 설계 장치는 설계된 데이터 구조를 기초로, 인공지능(AI) 학습을 위한 데이터를 수집할 수 있 다. 이를 위하여, 학습 데이터 설계 장치는 외부로부터 3D 점군 데이터 및 2D 이미지들을 입력 받거나, 웹 크롤링(web crawling)을 수행하여 3D 점군 데이터 및 2D 이미지들을 수집하거나, 또는 외부 기관의 장치로부터 3D 점군 데이터 및 2D 이미지들을 다운로드 할 수 있다. 여기서, 3D 점군 데이터는 차량에 고정 설치된 라이다(lidar)에 의해 획득된 데이터이다. 차량에 고정 설치된 라이다는 레이저 펄스를 발사하고, 차량 주위에 위치하는 객체들에 의해 반사되어 돌아온 빛을 감지하여, 차량 주위에 대한 3차원 영상에 해당하는 3D 점군 데이터를 생성할 수 있다. 즉, 3D 점군 데이터를 구성하는 점군은 라이다에 의해 3차원 공간으로 발사된 레이저 펄스를 반사시킨 점(point)들의 집합을 의미한다. 그리고, 2D 이미지는 차량에 고정 설치된 복수 개의 카메라에 의해 촬영된 이미지이다. 자율주행을 위하여 하나 의 차량에는 다수 개의 카메라가 고정 설치되어, 차량 주위에 대한 2차원 이미지를 각각 촬영할 수 있다. 예를 들어, 하나의 차량에 6개의 카메라가 설치될 수 있으나, 이에 한정되지 않는다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들 중에서 중복되거나 또는 극히 유사한 데 이터를 제거할 수 있다. 학습 데이터 생성 장치는 수집된 3D 점군 데이터 및 2D 이미지들에 포함된 개인정 보를 비식별화(de-identification)할 수 있다. 학습 데이터 생성 장치는 수집 및 정제된 3D 점군 데이터 및 2D 이미지들을 복수 개의 어노테이션 장치 에 분배하여 전송할 수 있다. 이 경우, 학습 데이터 생성 장치는 어노테이션 장치의 작업자(즉, 라벨러)에 대하여 사전에 할당된 양에 따라 3D 점군 데이터 및 2D 이미지들을 분배할 수 있다. 학습 데이터 생성 장치는 어노테이션 장치로부터 직접 어노테이션 작업 결과물을 수신하거나, 또는 학습 데이터 검증 장치로부터 어노테이션 작업 결과물 및 검수 결과를 수신할 수 있다. 학습 데이터 생성 장치는 수신된 어노테이션 작업 결과물을 패키징(packaging)하여 인공지능(AI) 학습용 데이터를 생성할 수 있다. 그리고, 학습 데이터 생성 장치는 생성된 인공지능(AI) 학습용 데이터를 학습 장치에 전송할 수 있다. 이와 같은 특징을 가지는, 학습 데이터 생성 장치는 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라D_07_도 허용될 수 있다. 예를 들어, 학습 데이터 생성 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server) 와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 어노테이션 장치는 학습 데이터 생성 장치로부터 제공된 이미지에 대하여 어노테이션 작업을 수행하 는데 사용될 수 있는 장치이다. 구체적으로, 어노테이션 작업은 큐보이드(cuboid) 또는 바운딩 박스(bounding box)를 설정하고, 객체의 속성 정 보를 포함하는 클래스(class) 정보를 입력하는 과정을 포함할 수 있다. 특히, 본 발명의 일 실시예에 따른 어노테이션 장치는 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group) 및 카메라(camera)를 통해 획득된 복수의 이미지(image) 중 적어D_07_도 하나 를 수신하고, 수신된 점군 데이터 또는 복수의 이미지 내에 포함된 적어D_07_도 하나 이상의 객체(object)를 검 출할 수 있다. 어노테이션 장치에 관한 구체적인 내용은 이하 D_07_도 3 및 D_07_도 4를 참조하여 후술하D_07_도록 한다. 이와 같은, 어노테이션 장치는 학습 데이터 생성 장치 또는 학습 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_07_도 허용될 수 있다. 예를 들어, 어노테이션 장치는 데스크탑(desktop), 워크스테이션(workstation) 또는 서버(server)와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되지 아니하고, 스마트폰(smart phone), 랩탑 (laptaop), 태블릿(tablet), 패블릿(phablet), 휴대용 멀티미디어 재생장치(Portable Multimedia Player, PMP), 개인용 휴대 단말기(Personal Digital Assistants, PDA) 또는 전자책 단말기(E-book reader)과 같은 이 동식 컴퓨팅 장치 중 어느 하나가 될 수D_07_도 있다. 다음 구성으로, 학습 데이터 검증 장치는 인공지능(AI) 학습용 데이터를 검증하는데 사용될 수 있는 장치 이다. 즉, 학습 데이터 검증 장치는 어노테이션 장치에 의해 생성된 어노테이션 작업 결과물이 사전 에 설정된 목표 품질에 부합하는지 여부, 또는 어노테이션 작업 결과물이 인공지능(AI) 학습에 유효한지 여부를 검증할 수 있는 장치이다. 구체적으로, 학습 데이터 검증 장치는 어노테이션 장치로부터 어노테이션 작업 결과물을 수신할 수 있다. 여기서, 어노테이션 작업 결과물은 3D 점군 데이터 및 2D 이미지들로부터 특정된 객체의 좌표와, 이미지 또는 객체에 대한 메타데이터가 포함될 수 있다. 어노테이션 작업 결과물의 메타데이터에는 특정된 객체의 카테 고리(category), 객체가 2D 이미지의 화각에 의해 잘려진 비율(truncation), 객체가 다른 객체 또는 물체에 의 해 가려진 비율(occlusion), 객체의 트래킹 아이디(tracking identifier), 이미지가 촬영된 시각, 이미지가 촬 영된 날의 기상 조건 등이 포함될 수 있으며, 이에 한정되는 것은 아니다. 이와 같은, 어노테이션 작업 결과물 은 JSON(Java Script Object Notation) 파일 형식을 가질 수 있으나, 이에 한정되는 것D_07_도 아니다. 학습 데이터 검증 장치는 수신된 어노테이션 작업 결과물을 검수할 수 있다. 이를 위하여, 학습 데이터 검 증 장치는 어노테이션 작업 결과물을 대상으로 스크립트(script)를 이용하여 검수를 수행할 수 있다. 여기 서, 스크립트는 어노테이션 작업 결과물을 대상으로 사전에 설정된 목표 품질의 부합 여부 또는 데이터 유효성 여부를 검증하기 위한 코드이다. 그리고, 학습 데이터 검증 장치는 어노테이션 장치들로부터 수신된 어노테이션 작업 결과물 및 검수 결과를 학습 데이터 생성 장치에 전송할 수 있다. 상술한 바와 같은 특징을 가지는, 학습 데이터 검증 장치는 어노테이션 장치 및 학습 데이터 생성 장 치와 데이터를 송수신하고, 송수신된 데이터를 기초로 연산을 수행할 수 있는 장치라면 어떠한 장치라 D_07_도 허용될 수 있다. 예를 들어, 학습 데이터 검증 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 학습 장치는 학습 데이터 생성 장치로부터 제공된 이미지를 통해 인공지능(AI)을 기 계 학습하는데 사용될 수 있는 장치이다. 이와 같은, 학습 장치는 학습 데이터 생성 장치와 데이터를 송수신하고, 송수신된 데이터를 이용하여 연산을 수행할 수 있는 장치라면 어떠한 장치라D_07_도 허용될 수 있다. 예를 들어, 학습 장치는 데스크탑, 워크스테이션 또는 서버와 같은 고정식 컴퓨팅 장치 중 어느 하나가 될 수 있으나, 이에 한정되는 것 은 아니다. 상술한 바와 같은, 학습 데이터 생성 장치, 하나 이상의 어노테이션 장치, 학습 데이터 검증 장치 및 학습 장치는 장치들 사이에 직접 연결된 보안회선, 공용 유선 통신망 또는 이동 통신망 중 하나 이상이 조합된 네트워크를 이용하여 데이터를 송수신할 수 있다. 예를 들어, 공용 유선 통신망에는 이더넷(ethernet), 디지털가입자선(x Digital Subscriber Line, xDSL), 광동 축 혼합망(Hybrid Fiber Coax, HFC), 광가입자망(Fiber To The Home, FTTH)가 포함될 수 있으나, 이에 한정되 는 것D_07_도 아니다. 그리고, 이동 통신망에는 코드 분할 다중 접속(Code Division Multiple Access, CDMA), 와이드 밴드 코드 분할 다중 접속(Wideband CDMA, WCDMA), 고속 패킷 접속(High Speed Packet Access, HSPA), 롱텀 에볼루션(Long Term Evolution, LTE), 5세대 이동통신(5th generation mobile telecommunication)가 포함 될 수 있으나, 이에 한정되는 것은 아니다. 이하, 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성에 대하여 구체적으로 설명하D_07_도록 한다. D_07_도 3은 본 발명의 일 실시예에 따른 어노테이션 장치의 논리적 구성D_07_도이다. D_07_도 3에 D_07_도시된 바와 같이, 본 발명의 일 실시예에 따른 어노테이션 장치는 통신부, 입출력 부, 큐보이드 생성부, 바운딩 박스 생성부 및 저장부를 포함하여 구성될 수 있다.이와 같은, 어노테이션 장치의 구성 요소들은 기능적으로 구분되는 요소들을 나타낸 것에 불과하므로, 둘 이상의 구성 요소가 실제 물리적 환경에서는 서로 통합되어 구현되거나, 하나의 구성 요소가 실제 물리적 환경 에서는 서로 분리되어 구현될 수 있을 것이다. 각각의 구성 요소에 대하여 설명하면, 통신부는 어노테이션의 작업 결과물을 학습 데이터 생성 장치 또는 학습 데이터 검증 장치에 전송할 수 있다. 여기서, 작업 결과물은 작업자의 제어에 따라 설정된 큐보이드의 좌표, 바운딩 박스의 좌표 및 객체의 속성 정 보가 포함될 수 있다. 또한, 작업 결과물은 JSON 파일 형식을 가질 수 있으나, 이에 한정되는 것은 아니다. 그리고, 통신부는 학습 데이터 생성 장치로부터 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성 을 수신할 수 있다. 여기서, 프로젝트의 속성에는 인공지능(AI)의 학습과 관련된 프로젝트에 대한 학습 목적, 학습 기간, 학습에 필 요한 이미지의 수, 이미지에서 식별하고자 하는 객체의 속성, 바운딩 박스 설정 규칙 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 이미지의 속성에는 이미지의 파일명, 이미지의 크기(너비, 높이), 해상D_07_도, 비트 수준, 압축 형식, 촬영 장 치명, 노출 시간, ISO 감D_07_도, 초점 거리, 조리개 개방 값, 촬영 장소 좌표(GPS 위D_07_도, 경D_07_도), 촬 영 시각 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 작업자의 속성에는 작업자의 명칭, 식별번호, 할당된 작업량, 작업에 따른 비용, 작업 결과 평가 등이 포함될 수 있으나, 이에 한정되는 것은 아니다. 또한, 큐보이드 또는 바운딩 박스 설정 규칙은 프로젝트의 속성, 이미지의 속성 또는 작업자의 속성에 따라, 작 업자가 이미지 내의 객체에 큐보이드 또는 바운딩 박스를 설정하는 과정에서 지켜야 하는 규칙이다. 이러한, 큐 보이드 또는 바운딩 박스 설정 규칙에는 영역의 최소 크기 값, 객체의 외곽선으로부터의 최대 이격 픽셀 수 등 이 포함될 수 있으나, 이에 한정되는 것은 아니다. 다음 구성으로, 입출력부는 사용자 인터페이스(UI)를 통해 사용자로부터 신호를 입력 받거나, 연산 결과를 외부로 출력할 수 있다. 구체적으로, 입출력부는 사용자 인터페이스(User Interface, UI)를 통해 작업자로부터 신호를 입력 거나, 또는 연산된 결과를 외부로 출력할 수 있다. 여기서, 작업자는 어노테이션 작업을 수행하는 자를 의미한다. 이와 같은, 작업자는 사용자, 수행자, 라벨러 또 는 데이터 라벨러 등으로 지칭될 수 있으며, 이에 한정되는 것은 아니다. 구체적으로, 입출력부는 어노테이션 작업의 대상이 되는 이미지를 출력할 수 있다. 입출력부는 바운 딩 박스를 설정하기 위한 제어 신호를 작업자로부터 입력 받을 수 있다. 그리고, 입출력부는 이미지 위에 바운딩 박스를 오버레이(overlay)하여 출력할 수 있다. 여기서, 큐보이드 또는 바운딩 박스는 이미지 속에 포함된 객체들 중에서 인공지능(AI) 학습의 대상이 되는 객 체를 특정하기 위한 영역이다. 이와 같은, 바운딩 박스는 직육면체, 사각형(rectangle) 또는 다각형(polygon) 형상을 가질 수 있으며, 이에 한정되는 것은 아니다. 다음 구성으로, 큐보이드 생성부는 3D 점군 데이터(3D point group)를 사전 설정된 뷰 포인트(viewpoin t)로 설정하고, 사전 설정된 뷰 포인트의 3D 점군 데이터 내에서 적어D_07_도 하나 이상의 객체(object)를 특정 할 수 있다. 구체적으로, 큐보이드 생성부는 3D 점군 데이터의 탑-뷰(top-view)를 투영하고, 탑-뷰로 투영된 3D 점군 데이터 내에서 객체에 대응하는 큐보이드의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지점 및 객체의 방향을 선택받을 수 있다. 예를 들어, 큐보이드 생성부는 입출력부를 통해 카메라를 통해 촬영된 이미지를 출력하고, 출력된 이 미지와 매칭되는 라이다를 통해 획득된 3D 점군 데이터를 함께 출력할 수 있다. 이때, 큐보이드 생성부는 3D 점군 데이터를 탑-뷰(top-view), 프론트-뷰(front-view) 및 사이드-뷰(side-view)로 구분하여 출력할 수 있 다. 큐보이드 생성부는 작업자로부터 3D 점군 데이터의 탑-뷰 상에서 두 개의 점을 선택받을 수 있다. 이때, 두 개의 점은 객체에 대응하는 큐보이드의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭 지점이 될 수 있다. 하지만, 이에 한정된 것은 아니고, 큐보이드 생성부는 작업자로부터 하나의 점을 선택받고, 작업자의 드래 그(drag) 동작에 의해 선택받은 점으로부터 확장된 박스를 생성할 수 있다. 큐보이드 생성부는 작업자로부터 생성된 박스의 방향을 선택받을 수 있다. 예를 들어, 큐보이드 생성부 는 작업자가 박스를 선택한 상태에서 드래그 동작에 의해 박스를 특정 지점을 중심으로 회전시킬 수 있다. 큐보이드 생성부는 박스 내에 포함된 3D 점군 데이터의 밀D_07_도를 기초로 객체의 엣지(edge)를 검출하고, 검출된 엣지와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방향 중 적어D_07_도 하나를 재조정 할 수 있다. 다음으로, 큐보이드 생성부는 특정된 객체에 대응되D_07_도록 특정된 객체를 포함하는 큐보이드(cuboid)를 생성할 수 있다. 이때, 큐보이드 생성부는 선택받은 두 개의 꼭지점 및 객체의 방향을 기초로 큐보이드의 밑면을 생성하고, 밑면에 사전 설정된 높이 값을 부여하여 큐보이드를 생성할 수 있다. 예를 들어, 큐보이드 생성부는 어노테이션 작업을 위한 객체의 종류를 미리 선택받고, 선택받은 객체의 종 류에 따라 해당 객체의 높이 값을 미리 설정할 수 있다. 그리고, 큐보이드 생성부는 작업자로부터 선택받 은 박스를 밑면으로 하여 높이 값을 부여함으로써 큐보이드를 완성시킬 수 있다. 다른 실시예로, 큐보이드 생성부는 3D 점군 데이터의 프론트-뷰(front-view)를 투영하고, 프론트-뷰로 투 영된 3D 점군 데이터 내에서 객체의 전면에 대응하는 큐보이드의 전면에 포함된 네 개의 꼭지점 중 서로 대향하 는 두개의 꼭지점을 선택받을 수 있다. 예를 들어, 큐보이드 생성부는 입출력부를 통해 카메라를 통해 촬영된 이미지를 출력하고, 출력된 이 미지와 매칭되는 라이다를 통해 획득된 3D 점군 데이터를 함께 출력할 수 있다. 이때, 큐보이드 생성부는 3D 점군 데이터를 탑-뷰(top-view), 프론트-뷰(front-view) 및 사이드-뷰(side-view)로 구분하여 출력할 수 있 다. 큐보이드 생성부는 프론트-뷰로 투영된 3D 점군 데이터 내에서 두 개의 점을 선택받을 수 있다. 이때, 두 개의 점은 객체에 대응하는 큐보이드의 전면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지 점이 될 수 있다. 하지만, 이에 한정된 것은 아니고, 큐보이드 생성부는 작업자로부터 하나의 점을 선택받고, 작업자의 드래 그(drag) 동작에 의해 선택받은 점으로부터 확장된 박스를 생성할 수 있다. 이때, 큐보이드 생성부는 사전 설정된 기준 방향을 기초로 3D 점군 데이터의 프론트-뷰를 투영하되, 프론 트-뷰로 투영된 3D 점군 데이터의 뷰 포인트를 보정하기 위한 보정 값을 입력 받을 수 있다. 또한, 큐보이드 생성부는 선택받은 두개의 꼭지점을 기초로 객체의 전면 폭 및 전면 높이를 구성하는 박스 (box)를 생성하되, 객체에 해당하는 점군 중 라이다와 가장 가까이에 위치하는 포인트를 기준으로 박스를 생성 할 수 있다. 즉, 입출력부를 통해 출력되는 점군 데이터의 각 뷰 포인트는 라이다를 기준으로 출력된다. 이에 따라, 큐 보이드 생성부는 객체에 해당하는 점군 중 라이다와 가장 가까이에 위치하는 포인트를 기준으로 박스를 생 성함으로써, 박스가 객체의 전면에 위치하D_07_도록 할 수 있다. 이때, 큐보이드 생성부는 프론트 뷰에서 생성된 박스를 탑-뷰 및 사이드-뷰에 각각 표시할 수 있다. 즉, 프론트 뷰에서 생성된 박스는 탑-뷰 및 사이드-뷰에서 선(line) 형태로 표시될 수 있다. 그리고, 큐보이드 생성부는 3D 점군 데이터의 사이드-뷰(side-view)를 투영하고, 선택받은 두개의 꼭지점 중 하나로부터 객체의 측면 폭에 대응하는 하나의 포인트를 선택받을 수 있다. 즉, 큐보이드 생성부는 작 업자로부터 사이드-뷰 상의 점군 데이터에서 객체의 후면에 대응하는 하나의 점을 선택받을 수 있다. 이를 통해, 큐보이드 생성부는 객체의 후면에 해당하는 지점을 식별할 수 있다. 다음으로, 큐보이드 생성부는 생성된 박스를 선택받은 하나의 포인트까지 연장하여 객체를 포함하는 큐보 이드를 생성할 수 있다.여기서, 큐보이드 생성부는 3D 점군 데이터의 밀D_07_도를 기초로 객체의 엣지(edge)를 검출하고, 검출된 엣지와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방향 중 적어D_07_도 하나를 재조정할 수 있다. 구체적으로, 큐보이드 생성부는 3D 점군 데이터와 대응하는 이미지를 추출하고, 추출된 이미지에 포함된 객체의 엣지(edge)를 추출하고, 추출된 엣지에 의한 폐쇄 영역(enclosure)을 하나 이상 추출하고, 추출된 폐쇄 영역을 통해 객체를 식별하고, 식별된 객체와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방향 중 적어 D_07_도 하나를 재조정할 수 있다. 또 다른 실시예로, 큐보이드 생성부는 사전 설정된 기계 학습(machine learning)된 인공지능을 통해 점군 의 밀D_07_도를 기초로 사전 설정된 거리 이내에 위치한 적어D_07_도 하나의 객체를 특정하고, 특정된 적어 D_07_도 하나의 객체를 구성하는 점군 중 라이다와 가장 가까운 점을 기준으로 큐보이드를 생성할 수 있다. 다음 구성으로, 바운딩 박스 생성부는 수신된 복수의 이미지를 시간 순으로 배열하고, 배열된 복수의 이미 지 각각에 포함된 적어D_07_도 하나의 객체에 순차적으로 바운딩 박스를 생성하되, 연속된 이미지 사이의 연관 성을 기초로 적어D_07_도 하나의 객체를 추적하여 각 이미지의 객체를 검출할 수 있다. 즉, 자율 주행을 위한 인공 지능 기계학습에 사용되는 데이터 셋은 차량에 탑재되는 카메라에 의해 연속적으로 촬영된 이미지이기 때문에, 연속되는 이미지 사이의 유사D_07_도가 높은 경우가 많다. 이에 따라, 바운딩 박스 생성부는 연속된 이미지 사이의 연관성을 기초로 각 이미지의 객체를 검출할 수 있다. 이를 위해, 바운딩 박스 생성부는 시간 순으로 배열된 복수의 이미지 중 첫번째 이미지에 포함된 객체를 작업자로부터 선택받을 수 있다. 예를 들어, 바운딩 박스 생성부는 작업자로부터 이미지 상에 두 개의 점 을 선택받고, 선택받은 두 개의 점을 통해 객체를 포함하는 바운딩 박스를 생성할 수 있다. 그리고, 바운딩 박스 생성부는 시간 순으로 배열된 복수의 이미지 중 두번째 이미지에서 객체를 검출하여 바운딩 박스를 생성하되, 첫번째 이미지에서 생성된 바운딩 박스를 기반으로 객체를 검출할 수 있다. 이와 같이, 바운딩 박스 생성부는 시간 순으로 배열된 복수의 이미지 중 이전 촬영된 이미지를 참조하여 객체를 검출할 수 있다. 이때, 바운딩 박스 생성부는 연속된 이미지 사이에 검출된 동일한 객체 사이의 거리를 추정하고, 추정된 동일한 객체 사이의 거리 및 촬영된 시간을 기초로 객체의 속D_07_도를 추정하고, 추정된 객체의 속D_07_도를 기초로 각 이미지의 객체를 추적할 수 있다. 예를 들어, 연속된 제1 이미지, 제2 이미지 및 제3 이미지가 존재하고, 제1 이미지 및 제2 이미지를 참조하여 제3 이미지에 포함된 객체를 검출하는 경우, 바운딩 박스 생성부는 제1 이미지에서 공통 객체를 특정하기 위해 설정된 바운딩 박스의 넓이와 제2 이미지에서 공통 객체를 특정하기 위하여 설정된 바운딩 박스의 넓이를 비교하여, 제1 이미지로부터 특정된 공통 객체와 제2 이미지로부터 특정된 공통 객체 사이의 이격 거리를 추정 할 수 있다. 바운딩 박스 생성부는 제1 이미지의 촬영 시각과 제2 이미지의 촬영 시각 사이의 이격 시간을 산출하고, 이격 거리 및 이격 시간을 이용하여 공통 객체의 이동 속D_07_도를 산출할 수 있다. 바운딩 박스 생성부는 산출된 이동 속D_07_도를 기초로, 제3 이미지 내에서 공통 객체가 위치할 것으로 추 정되는 영역을 식별할 수 있다. 또한, 바운딩 박스 생성부는 연속된 이미지에 포함된 객체를 순차적으로 검출하되, 추적중인 객체의 추적 이 불가능한 경우 입출력부를 통해 추적 불가능에 따른 알람을 출력할 수 있다. 즉, 바운딩 박스 생성부는 최초 첫번째 이미지에 포함된 객체에 대한 바운딩 박스를 작업자로부터 입력받 고, 두번째 이미지부터는 적어D_07_도 하나의 이전 이미지들을 통해 자동으로 객체를 검출할 수 있다. 이때, 바 운딩 박스 생성부는 추적중인 객체의 추적이 불가능한 것으로 판단되는 경우 해당 이미지에서 검출을 중지 하고, 작업자로부터 해당 이미지에 대한 바운딩 박스를 입력받을 수 있다. 이후, 바운딩 박스 생성부는 새 롭게 입력받은 바운딩 박스를 기초로 객체를 추적하여 바운딩 박스를 생성할 수 있다. 다른 실시예로, 바운딩 박스 생성부는 시간 순으로 배열된 복수의 이미지를 사전 설정된 개수로 그룹핑 (grouping)하고, 각 그룹에 포함된 이미지 중 첫번째 이미지를 작업자에게 제공할 수 있다. 예를 들어, 바운딩 박스 생성부는 시간 순으로 배열된 복수의 이미지를 10개 단위로 그룹핑하고, 각 그룹 의 첫번째 이미지만 작업자에게 제공할 수 있다. 이를 통해, 바운딩 박스 생성부는 작업자에게 입력받은 이미지를 기초로 단위별로 나머지 이미지의 객체를 추적하여, 각 이미지의 추적된 객체에 대한 바운딩 박스를 생성할 수 있다. 또한, 바운딩 박스 생성부는 큐보이드 생성부로부터 생성된 큐보이드를 기반으로 생성된 바운딩 박스 를 보정할 수 있다. 즉, 바운딩 박스 생성부는 3D 점군 데이터로부터 특정된 객체와 이미지로부터 특정된 객체가 서로 대응할 수 있D_07_도록 큐보이드를 기반으로 바운딩 박스의 위치 또는 크기를 조정할 수 있다. 반 대로 바운딩 박스 생성부는 생성된 바운딩 박스를 기반으로 3D 점군 데이터 내에 설정된 큐보이드의 위치 또는 크기를 조정할 수D_07_도 있다. 이하, 상술한 바와 같은 어노테이션 장치의 논리적 구성요소를 구현하기 위한 하드웨어에 대하여 보다 구 체적으로 설명한다. D_07_도 4는 본 발명의 일 실시예에 따른 어노테이션 장치의 하드웨어 구성D_07_도이다. D_07_도 4에 D_07_도시된 바와 같이, 어노테이션 장치는 프로세서(Processor, 250), 메모리(Memory, 255), 송수신기(Transceiver, 260), 입출력장치(Input/output device, 265), 데이터 버스(Bus, 270) 및 스토리 지(Storage, 275)를 포함하여 구성될 수 있다. 프로세서는 메모리에 상주된 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(280a)에 따른 명 령어를 기초로, 어노테이션 장치의 동작 및 기능을 구현할 수 있다. 메모리에는 본 발명의 실시예들 에 따른 방법이 구현된 소프트웨어(280a)가 상주(loading)될 수 있다. 송수신기는 학습 데이터 생성 장치 , 학습 데이터 검증 장치 및 인공지능 학습 장치와 데이터를 송수신할 수 있다. 입출력장치 는 어노테이션 장치의 동작에 필요한 데이터를 입력 받거나, 학습 결과물을 출력할 수 있다. 데이터 버스는 프로세서, 메모리, 송수신기, 입출력장치 및 스토리지와 연결되어, 각각 의 구성 요소 사이가 서로 데이터를 전달하기 위한 이동 통로의 역할을 수행할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어(280a)의 실행을 위해 필요한 애플리케이 션 프로그래밍 인터페이스(Application Programming Interface, API), 라이브러리(library) 파일, 리소스 (resource) 파일 등을 저장할 수 있다. 스토리지는 본 발명의 실시예들에 따른 방법이 구현된 소프트웨어 (280b)를 저장할 수 있다. 또한, 스토리지는 본 발명의 실시예들에 따른 방법의 수행에 필요한 정보들을 저장할 수 있다. 본 발명의 일 실시예에 따르면, 메모리에 상주되거나 또는 스토리지에 저장된 학습 방법을 구현하기 위한 소프트웨어(280a, 280b)는 프로세서가 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데 이터(3D point group) 및 카메라(camera)를 통해 획득된 복수의 이미지(image) 중 적어D_07_도 하나를 수신하 는 단계 및 프로세서가, 수신된 점군 데이터 또는 복수의 이미지 내에 포함된 적어D_07_도 하나 이상의 객 체(object)를 검출하는 단계를 실행시키기 위하여, 기록매체에 기록된 컴퓨터 프로그램이 될 수 있다. 보다 구체적으로, 프로세서는 ASIC(Application-Specific Integrated Circuit), 다른 칩셋(chipset), 논 리 회로 및/또는 데이터 처리 장치를 포함할 수 있다. 메모리는 ROM(Read-Only Memory), RAM(Random Access Memory), 플래쉬 메모리, 메모리 카드, 저장 매체 및/또는 다른 저장 장치를 포함할 수 있다. 송수신기 는 유무선 신호를 처리하기 위한 베이스밴드 회로를 포함할 수 있다. 입출력장치는 키보드 (keyboard), 마우스(mouse), 및/또는 조이스틱(joystick) 등과 같은 입력 장치 및 액정표시장치(Liquid Crystal Display, LCD), 유기 발광 다이오드(Organic LED, OLED) 및/또는 능동형 유기 발광 다이오드(Active Matrix OLED, AMOLED) 등과 같은 영상 출력 장치 프린터(printer), 플로터(plotter) 등과 같은 인쇄 장치를 포 함할 수 있다. 본 명세서에 포함된 실시 예가 소프트웨어로 구현될 경우, 상술한 방법은 상술한 기능을 수행하는 모듈(과정, 기능 등)로 구현될 수 있다. 모듈은 메모리에 상주되고, 프로세서에 의해 실행될 수 있다. 메모리 는 프로세서의 내부 또는 외부에 있을 수 있고, 잘 알려진 다양한 수단으로 프로세서와 연결될 수 있다. D_07_도 4에 D_07_도시된 각 구성요소는 다양한 수단, 예를 들어, 하드웨어, 펌웨어(firmware), 소프트웨어 또 는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 일 실시예는 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field ProgrammableGate Arrays), 프로세서, 콘트롤러, 마이크로 콘트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 일 실시예는 이상에서 설명된 기능 또는 동작들을 수행하는 모듈, 절차, 함수 등의 형태로 구현되어, 다양한 컴퓨터 수단을 통하여 판독 가능한 기록매체에 기록 될 수 있다. 여기서, 기록매체는 프로그램 명령, 데이터 파일, 데이터 구조 등을 단독으로 또는 조합하여 포함 할 수 있다. 기록매체에 기록되는 프로그램 명령은 본 발명을 위하여 특별히 설계되고 구성된 것들이거나 컴퓨 터 소프트웨어 당업자에게 공지되어 사용 가능한 것일 수D_07_도 있다. 예컨대 기록매체는 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기 -광 매체(Magneto-Optical Media), 및 롬(ROM), 램(RAM), 플래시 메모리 등과 같은 프로그램 명령을 저장하고 수행하D_07_도록 특별히 구성된 하드웨어 장치를 포함한다. 프로그램 명령의 예에는 컴파일러에 의해 만들어지 는 것과 같은 기계어 코드뿐만 아니라 인터프리터 등을 사용해서 컴퓨터에 의해서 실행될 수 있는 고급 언어 코 드를 포함할 수 있다. 이러한, 하드웨어 장치는 본 발명의 동작을 수행하기 위해 하나 이상의 소프트웨어로서 작동하D_07_도록 구성될 수 있으며, 그 역D_07_도 마찬가지이다. D_07_도 5는 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 검출 방법을 설명하기 위한 순서D_07_도이 다. D_07_도 5를 참조하면, 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 방법에 따르면, S110 단계에서 어노테이션 장치는 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group)를 수신할 수 있다. 다음으로, S120 단계에서 어노테이션 장치는 3D 점군 데이터를 사전 설정된 뷰 포인트(viewpoint)로 설정하고, 사전 설정된 뷰 포인트의 3D 점군 데이터 내에서 적어D_07_도 하나 이상의 객체(object)를 특정할 수 있다. 구체적으로, 어노테이션 장치는 3D 점군 데이터의 탑-뷰(top-view)를 투영하고, 탑-뷰로 투영된 3D 점군 데이터 내에서 객체에 대응하는 큐보이드의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지점 및 객체의 방향을 선택받을 수 있다. 다른 실시예로, 어노테이션 장치는 3D 점군 데이터의 프론트-뷰(front-view)를 투영하고, 프론트-뷰로 투영된 3D 점군 데이터 내에서 객체의 전면에 대응하는 큐보이드의 전면에 포함된 네 개의 꼭지점 중 서로 대향하는 두 개의 꼭지점을 선택받고, 3D 점군 데이터의 사이드-뷰(side-view)를 투영하고, 선택받은 두개의 꼭지점 중 하나 로부터 객체의 측면 폭에 대응하는 하나의 포인트를 선택받을 수 있다. 이때, 어노테이션 장치는 사전 설정된 기준 방향을 기초로 3D 점군 데이터의 프론트-뷰를 투영하되, 프론트-뷰 로 투영된 3D 점군 데이터의 뷰 포인트를 보정하기 위한 보정 값을 입력 받을 수 있다. 그리고, S130 단계에서 어노테이션 장치는 특정된 객체에 대응되D_07_도록 특정된 객체를 포함하는 큐보이드 (cuboid)를 생성할 수 있다. 구체적으로, 어노테이션 장치는 선택받은 두 개의 꼭지점 및 객체의 방향을 기초로 큐보이드의 밑면을 생성하고, 밑면에 사전 설정된 높이 값을 부여하여 큐보이드를 생성할 수 있다. 이때, 어노테이션 장치는 선택받은 두개의 꼭지점을 기초로 객체의 전면 폭 및 전면 높이를 구성하는 박스(bo x)를 생성하되, 객체에 해당하는 점군 중 라이다와 가장 가까이에 위치하는 포인트를 기준으로 박스를 생성할 수 있다. 다른 실시예로, 어노테이션 장치는 생성된 박스를 선택받은 하나의 포인트까지 연장하여 객체를 포함하는 큐보 이드를 생성할 수 있다. 여기서, 어노테이션 장치는 3D 점군 데이터의 밀D_07_도를 기초로 객체의 엣지(edge)를 검출하고, 검출된 엣지 와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방향 중 적어D_07_도 하나를 재조정할 수 있다. 구체적으로, 어노테이션 장치는 3D 점군 데이터와 대응하는 이미지를 추출하고, 추출된 이미지에 포함된 객체의 엣지(edge)를 추출하고, 추출된 엣지에 의한 폐쇄 영역(enclosure)을 하나 이상 추출하고, 추출된 폐쇄 영역을통해 객체를 식별하고, 식별된 객체와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방향 중 적어D_07_도 하 나를 재조정할 수 있다. 또 다른 실시예로, 어노테이션 장치는 S120 단계에서 작업자로부터 객체를 특정 받지 않고, 사전 설정된 기계 학습(machine learning)된 인공지능을 통해 점군의 밀D_07_도를 기초로 사전 설정된 거리 이내에 위치한 적어 D_07_도 하나의 객체를 특정하고, 특정된 적어D_07_도 하나의 객체를 구성하는 점군 중 라이다와 가장 가까운 점을 기준으로 큐보이드를 생성할 수 있다. D_07_도 6은 본 발명의 일 실시예에 따른 이미지를 이용한 객체 검출 방법을 설명하기 위한 순서D_07_도이다. D_07_도 6을 참조하면, 본 발명의 일 실시예에 따른 이미지를 이용한 객체 검출 방법은 D_07_도 5를 참조하면, 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 방법에 따르면, S110 단계에서 어노테이션 장치는 차량에 고정 설치된 카메라를 통해 획득된 복수의 이미지(image)를 수신할 수 있다. 다음으로, S120 단계에서 어노테이션 장치는 수신한 복수의 이미지를 시간 순으로 배열할 수 있다. 그리고, S130 단계에서 어노테이션 장치는 배열된 복수의 이미지 각각에 포함된 적어D_07_도 하나의 객체에 순 차적으로 바운딩 박스를 생성하되, 연속된 이미지 사이의 연관성을 기초로 적어D_07_도 하나의 객체를 추적하여 각 이미지의 객체를 검출할 수 있다. 이때, 어노테이션 장치는 연속된 이미지 사이에 검출된 동일한 객체 사이의 거리를 추정하고, 추정된 동일한 객 체 사이의 거리 및 촬영된 시간을 기초로 객체의 속D_07_도를 추정하고, 추정된 객체의 속D_07_도를 기초로 각 이미지의 객체를 추적할 수 있다. D_07_도 7 내지 D_07_도 13은 본 발명의 일 실시예에 따른 점군 데이터를 이용한 객체 검출 방법을 설명하기 위 한 예시D_07_도이다. D_07_도 7에 D_07_도시된 바와 같이, 어노테이션 장치는 작업자로부터 3D 점군 데이터의 탑-뷰 상에서 두 개의 점(Point A, Point B)을 선택받을 수 있다. 이때, 두 개의 점(Point A, Point B)은 객체에 대응하는 큐보이드 의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지점이 될 수 있다. 하지만, 이에 한정된 것은 아니고, 어노테이션 장치는 작업자로부터 하나의 점(Point A)을 선택받고, 작업자의 드래그(drag) 동작에 의해 선택받은 점으로부터 확장된 박스를 생성할 수 있다. 그리고, D_07_도 8에 D_07_도시된 바와 같이, 어노테이션 장치는 작업자로부터 생성된 박스의 방향을 선택받을 수 있다, 예를 들어, 어노테이션 장치는 작업자가 박스를 선택한 상태에서 드래그 동작에 의해 박스를 특정 지 점을 중심으로 회전시킬 수 있다. 다음으로, D_07_도 9에 D_07_도시된 바와 같이, 어노테이션 장치는 박스 내에 포함된 3D 점군 데이터의 밀D_07_ 도를 기초로 객체의 엣지(edge)를 검출하고, 검출된 엣지와의 간격이 최소화되D_07_도록 큐보이드의 크기 및 방 향 중 적어D_07_도 하나를 재조정할 수 있다. 그리고, 어노테이션 장치는 특정된 객체에 대응되D_07_도록 특정된 객체를 포함하는 큐보이드(cuboid)를 생성할 수 있다. 이때, 어노테이션 장치는 선택받은 두 개의 꼭지점 및 객체의 방향을 기초로 큐보이드의 밑면을 생성하고, 밑면 에 사전 설정된 높이 값을 부여하여 큐보이드를 생성할 수 있다. 예를 들어, 어노테이션 장치는 어노테이션 작업을 위한 객체의 종류를 미리 선택받고, 선택받은 객체의 종류에 따라 해당 객체의 높이 값을 미리 설정할 수 있다. 그리고, 어노테이션 장치는 작업자로부터 선택받은 박스를 밑면으로 하여 높이 값을 부여함으로써 큐보이드를 완성시킬 수 있다. 다른 실시예로, D_07_도 10에 D_07_도시된 바와 같이, 어노테이션 장치는 입출력부를 통해 카메라를 통해 촬영 된 이미지(a)를 출력하고, 출력된 이미지(a)와 매칭되는 라이다를 통해 획득된 3D 점군 데이터(b)를 함께 출력 할 수 있다. 이때, 어노테이션 장치는 3D 점군 데이터를 탑-뷰(c), 프론트-뷰(d) 및 사이드-뷰(e)로 구분하여 출력할 수 있다.다음으로, D_07_도 11에 D_07_도시된 바와 같이, 어노테이션 장치는 프론트-뷰(d)로 투영된 3D 점군 데이터 내 에서 두 개의 점(point A, point B)을 선택받을 수 있다. 이때, 두 개의 점(point A, point B)은 객체에 대응 하는 큐보이드의 전면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지점이 될 수 있다. 이때, 어노테이션 장치는 사전 설정된 기준 방향을 기초로 3D 점군 데이터의 프론트-뷰(d)를 투영하되, 프론트- 뷰(d)로 투영된 3D 점군 데이터의 뷰 포인트를 보정하기 위한 보정 값을 입력 받을 수 있다. 즉, 입출력부를 통해 출력되는 점군 데이터의 각 뷰 포인트는 라이다를 기준으로 출력된다. 이에 따라, 투영된 프론트-뷰(d)는 객체의 방향과는 일치하지 않을 수 있다. 따라서, 어노테이션 장치는 작업자로부터 뷰 포인트 조정을 위한 보정 값을 입력받을 수 있다. 예를 들어, 어노테이션 장치는 작업자의 드래그 동작을 통해 뷰 포인 트를 조정할 수 있다. 또한, 어노테이션 장치는 선택받은 두개의 꼭지점을 기초로 객체의 전면 폭 및 전면 높이를 구성하는 박스(bo x)를 생성하되, 객체에 해당하는 점군 중 라이다와 가장 가까이에 위치하는 포인트를 기준으로 박스를 생성할 수 있다. 이에 따라, 어노테이션 장치는 다른 뷰 포인트에서D_07_도 생성된 박스가 객체의 전면에 위치하D_07_ 도록 할 수 있다. 이때, D_07_도 11에 D_07_도시된 바와 같이, 어노테이션 장치는 프론트 뷰(d)에서 생성된 박스를 탑-뷰(c) 및 사이드-뷰(e)에 각각 표시할 수 있다. 즉, 프론트 뷰에서 생성된 박스는 탑-뷰(c) 및 사이드-뷰(e)에서 선 (line) 형태로 표시될 수 있다. 그리고, D_07_도 12에 D_07_도시된 바와 같이, 어노테이션 장치는 3D 점군 데이터의 사이드-뷰(e)를 투영하고, 선택받은 두개의 꼭지점 중 하나로부터 객체의 측면 폭에 대응하는 하나의 포인트(point C)를 선택받을 수 있다. 즉, 어노테이션 장치는 작업자로부터 사이드-뷰(e) 상의 점군 데이터에서 객체의 후면에 대응하는 하나의 점을 선택받을 수 있다. 이를 통해, 어노테이션 장치는 객체의 후면에 해당하는 지점을 식별할 수 있다. 이후, D_07_도 13에 D_07_도시된 바와 같이, 어노테이션 장치는 생성된 박스를 선택받은 하나의 포인트까지 연 장하여 객체를 포함하는 큐보이드를 생성할 수 있다. 즉, 어노테이션 장치는 생성된 박스를 전면으로 포함하는 3D 형상의 큐보이드를 생성할 수 있다. D_07_도 14는 본 발명의 일 실시예에 따른 이미지를 이용한 객체 검출 방법을 설명하기 위한 예시D_07_도이다. D_07_도 14의 (a)에 D_07_도시된 바와 같이, 어노테이션 장치는 수신된 복수의 이미지를 시간 순으로 배열할 수 있다. 이후 (b)에 D_07_도시된 바와 같이, 어노테이션 장치는 시간 순으로 배열된 복수의 이미지를 사전 설정된 개수 로 그룹핑(grouping)하고, 각 그룹에 포함된 이미지 중 첫번째 이미지를 작업자에게 제공할 수 있다. 예를 들어, 시간 순으로 배열된 복수의 이미지를 10개 단위로 그룹핑하고, 각 그룹의 첫번째 이미지만 작업자에 게 제공할 수 있다. 그리고, (c)에 D_07_도시된 바와 같이, 어노테이션 장치는 작업자에게 입력받은 이미지를 기초로 단위별로 나머 지 이미지의 객체를 추적하여, 각 이미지의 추적된 객체에 대한 바운딩 박스를 생성할 수 있다. 이상과 같이, 본 명세서와 D_07_도면에는 본 발명의 바람직한 실시예에 대하여 개시하였으나, 여기에 개시된 실 시예 외에D_07_도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하 는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다. 또한, 본 명세서와 D_07_도면에서 특정 용어들이 사용되었으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사 용된 것이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 따라서, 상술한 상세한 설명은 모든 면에서 제한 적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적해 석에 의해 선정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다. D_07_부호의 설명 100 : 학습 데이터 생성 장치 200 : 어노테이션 장치 300 : 학습 데이터 검증 장치 400 : 인공지능 학습 장치 205 : 통신부 210 : 입출력부 215 : 큐보이드 생성부 220 : 바운딩 박스 생성부 225 : 저장부 D_07_청구범위 D_07_청구항 1 어노테이션 장치가, 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group) 및 카메 라(camera)를 통해 획득된 복수의 이미지(image) 중 적어D_07_도 하나를 수신하는 단계; 및 상기 어노테이션 장치가, 상기 수신된 점군 데이터 또는 상기 복수의 이미지 내에 포함된 적어D_07_도 하나 이 상의 객체(object)를 검출하는 단계; 를 포함하는 것을 특징으로 하는 객체 검출 방법. D_07_청구항 2 제1 항에 있어서, 상기 객체를 검출하는 단계는, 상기 어노테이션 장치가, 상기 3D 점군 데이터(3D point group)를 사전 설정된 뷰 포인트(view point)로 설정하 고, 상기 사전 설정된 뷰 포인트의 3D 점군 데이터 내에서 적어D_07_도 하나 이상의 객체(object)를 특정하는 단계; 및 상기 어노테이션 장치가, 상기 특정된 객체에 대응되D_07_도록 상기 특정된 객체를 포함하는 큐보이드(cuboid) 를 생성하는 단계; 를 포함하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 3 제2 항에 있어서, 상기 객체를 특정하는 단계는 상기 3D 점군 데이터의 탑-뷰(top-view)를 투영하고, 상기 탑-뷰로 투영된 3D 점군 데이터 내에서 객체에 대응 하는 큐보이드의 밑면에 포함된 네 개의 꼭지점(vertex) 중 서로 대향하는 두 개의 꼭지점 및 객체의 방향을 선 택받는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 4 제3항에 있어서, 상기 큐보이드를 생성하는 단계는 상기 선택받은 두 개의 꼭지점 및 상기 객체의 방향을 기초로 상기 큐보이드의 밑면을 생성하고, 상기 밑면에 사전 설정된 높이 값을 부여하여 상기 큐보이드를 생성하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 5 제2 항에 있어서, 상기 객체를 특정하는 단계는 상기 3D 점군 데이터의 프론트-뷰(front-view)를 투영하고, 상기 프론트-뷰로 투영된 3D 점군 데이터 내에서 객 체의 전면에 대응하는 큐보이드의 전면에 포함된 네 개의 꼭지점 중 서로 대향하는 두개의 꼭지점을 선택받는 단계; 및 상기 3D 점군 데이터의 사이드-뷰(side-view)를 투영하고, 상기 선택받은 두개의 꼭지점 중 하나로부터 상기 객 체의 측면 폭에 대응하는 하나의 포인트를 선택받는 단계; 를 포함하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 6 제5 항에 있어서, 상기 두개의 꼭지점을 선택받는 단계는 사전 설정된 기준 방향을 기초로 상기 3D 점군 데이터의 프론트-뷰를 투영하되, 상기 프론트-뷰로 투영된 3D 점 군 데이터의 뷰 포인트를 보정하기 위한 보정 값을 입력 받는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 7 제2 항에 있어서, 상기 큐보이드를 생성하는 단계 이후에 상기 3D 점군 데이터의 밀D_07_도를 기초로 상기 객체의 엣지(edge)를 검출하고, 상기 검출된 엣지와의 간격이 최소화되D_07_도록 상기 큐보이드의 크기 및 방향 중 적어D_07_도 하나를 재조정하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 8 제2 항에 있어서, 상기 큐보이드를 생성하는 단계 이후에 상기 3D 점군 데이터와 대응하는 이미지를 추출하고, 상기 추출된 이미지에 포함된 객체의 엣지(edge)를 추출하 고, 상기 추출된 엣지에 의한 폐쇄 영역(enclosure)을 하나 이상 추출하고, 상기 추출된 폐쇄 영역을 통해 상기 객체를 식별하고, 상기 식별된 객체와의 간격이 최소화되D_07_도록 상기 큐보이드의 크기 및 방향 중 적어D_07_ 도 하나를 재조정하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 9 제1 항에 있어서, 상기 객체를 검출하는 단계는 상기 수신된 복수의 이미지를 시간 순으로 배열하고, 상기 배열된 복수의 이미지 각각에 포함된 적어D_07_도 하 나의 객체에 순차적으로 바운딩 박스를 생성하되, 연속된 이미지 사이의 연관성을 기초로 상기 적어D_07_도 하 나의 객체를 추적하여 각 이미지의 객체를 검출하는 것을 특징으로 하는, 객체 검출 방법. D_07_청구항 10 제9 항에 있어서, 상기 객체를 검출하는 단계는 연속된 이미지 사이에 검출된 동일한 객체 사이의 거리를 추정하고, 상기 추정된 동일한 객체 사이의 거리 및 촬영된 시간을 기초로 상기 객체의 속D_07_도를 추정하고, 상기 추정된 객체의 속D_07_도를 기초로 각 이미지의 객체를 추적하는 것을 특징으로 하는, 객체 검출 방법."}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 377, "content": "D_07_요약서"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 378, "content": "D_07_요약 본 발명은 인공지능(AI)을 기계 학습하기 위한 데이터의 어노테이션 작업과 관련하여, 라이다에 의해 획득된 3D 점군 데이터와 카메라에 의해 촬영된 이미지 내에 포함된 객체를 검출하기 위한 방법을 제안한다. 상기 방법은 어노테이션 장치가, 차량에 고정 설치된 라이다(lidar)를 통해 획득된 3D 점군 데이터(3D point group) 및 카메 라(camera)를 통해 획득된 복수의 이미지(image) 중 적어D_07_도 하나를 수신하는 단계 및 상기 어노테이션 장 치가, 상기 수신된 점군 데이터 또는 상기 복수의 이미지 내에 포함된 적어D_07_도 하나 이상의 객체(object)를 검출하는 단계를 포함할 수 있다. D_07_대표D_07_도 D_07_도 1 D_07_도면 D_07_도 1 D_07_도 2"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 379, "content": "D_07_도 3 D_07_도 4"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 380, "content": "D_07_도 5 D_07_도 6"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 381, "content": "D_07_도 7 D_07_도 8"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 382, "content": "D_07_도 9 D_07_도 10"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 383, "content": "D_07_도 11 D_07_도 12"}
{"patent_id": "10-2023-0066082", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 384, "content": "D_07_도 13 D_07_도 14"}
{"patent_id": "10-2023-0066082", "section": "도면", "subsection": "도면설명", "item": 1, "content": "76_도 1은 본 발명의 일 실시예에 따른 무인 결제 시스템의 구성76_도이다. 76_도 2는 본 발명의 다른 실시예에 따른 무인 결제 시스템의 구성76_도이다. 76_도 3은 본 발명의 일 실시예에 따른 결제 제어 서버의 논리적 구성76_도이다. 76_도 4는 본 발명의 일 실시예에 따른 결제 제어 서버의 하드웨어 구성76_도이다. 76_도 5는 본 발명의 일 실시예에 따라 식별되는 고객의 행동을 설명하기 위한 예시76_도이다. 76_도 6은 본 발명의 일 실시예에 따른 구성요소가 배치된 매장을 설명하기 위한 예시76_도이다. 76_도 7은 본 발명의 일 실시예에 따른 운반 기구를 설명하기 위한 예시76_도이다. 76_도 8은 본 발명의 일 실시예에 따른 무인 결제 장치를 설명하기 위한 예시76_도이다. 76_도 9는 본 발명의 일 실시예에 따른 결제 프로세스의 과정들을 설명하기 위한 예시76_도이다. 76_도 10은 본 발명의 일 실시예에 따른 무인 결제 방법을 설명하기 위한 순서76_도이다. 76_도 11은 본 발명의 일 실시예에 따라 고객 및 상품을 관리하는 단계를 설명하기 위한 순서76_도이다. 76_"}
