{"patent_id": "10-2023-0026906", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0049128", "출원번호": "10-2023-0026906", "발명의 명칭": "포인트 클라우드를 생성하는 방법 및 전자 장치", "출원인": "삼성전자주식회사", "발명자": "김동찬"}}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치가 포인트 클라우드(point cloud)를 생성하는 방법에 있어서,상기 전자 장치(200)의 적어도 하나의 센서(210)로부터, 객체에 대응하는 제1 센싱 데이터를 획득하는 단계(S310);상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득하는 단계 (S320);적어도 하나의 심층 신경망 모델(231)을 이용하여, 상기 제1 포인트 클라우드에서 적어도 하나의 기 정의된 물리 법칙의 위반을 나타내는 이상치 포인트들을 식별하는 단계 (S330); 및상기 이상치 포인트들에 기초하여, 상기 객체에 대한 재촬영 위치 가이드를 제공하는 단계 (S340)를 포함하는,방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 방법은, 상기 재촬영 위치 가이드를 제공한 후, 상기 적어도 하나의 센서(210)로부터, 상기 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획득하는 단계; 및상기 제1 센싱 데이터 및 상기 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득하는 단계를 포함하는방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는,재촬영이 필요한 카메라 위치를 제1 형태(1311)로 시각화한 상기 재촬영 위치 가이드를 상기 전자 장치(200)의디스플레이(263)에 표시하는 단계;상기 제2 센싱 데이터를 획득한 후, 상기 제2 센싱 데이터가 상기 재촬영이 필요한 카메라 위치에서 촬영된 것인지 결정하는 단계; 및상기 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정한 것에 기초하여, 상기 제1 형태를 재촬영이 불필요한 카메라 위치를 나타내는 제2 형태(1321)로 변경하여 상기 재촬영 위치 가이드를 상기 전자 장치(200)의 디스플레이(263)에 표시하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는,상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 위치를 추정하는단계;상기 추정된 위치에 기초하여, 상기 객체와 상기 적어도 하나의 센서 사이의 거리를 추정하는 단계;상기 추정된 거리, 상기 적어도 하나의 센서의 화각, 및 상기 제1 센싱 데이터 중 적어도 하나에 기초하여, 상기 객체에 대응하는 픽셀 해상도를 획득하는 단계;상기 픽셀 해상도가 제1 임계 값 미만인지 여부를 결정하는 단계; 및공개특허 10-2024-0049128-3-상기 픽셀 해상도가 상기 제1 임계 값 미만인 경우, 상기 추정된 거리보다 더 가까운 거리에서 상기 객체를 촬영할 것을 지시하는 상기 재촬영 위치 가이드를 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는,상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서(210)의 제1 위치를추정하는 단계;상기 추정된 제1 위치에 기초하여, 상기 식별된 부분에 대응하는 제2 센싱 데이터를 획득하기 위한 상기 적어도하나의 센서(210)의 제2 위치를 추정하는 단계;상기 제1 위치 및 상기 제2 위치를 비교하여 상기 적어도 하나의 센서의 위치 조정 값을 획득하는 단계;상기 위치 조정 값이 제2 임계 값을 초과하는지 여부를 결정하는 단계; 및상기 위치 조정 값이 제2 임계 값을 초과하는 경우, 상기 제2 위치에서 상기 객체를 촬영할 것을 지시하는 상기재촬영 위치 가이드를 제공하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서, 상기 제1 포인트 클라우드에서 이상치 포인트들을 식별하는 단계는,상기 제1 포인트 클라우드에 기초하여 상기 객체에 대응하는 멀티 뷰 이미지들을 획득하고, 상기 멀티 뷰 이미지들 각각에 대응하는 카메라 정보를 획득하는 단계;상기 멀티 뷰 이미지들로 구성되는 동영상을 상기 적어도 하나의 심층 신경망 모델(231)에 적용하여, 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하는 단계; 및상기 추론 결과에 기초하여, 상기 객체의 형상 중에서 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 객체의 형상 중에서 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계는,상기 동영상에서 상기 추론 결과에 대응하는 히트 맵을 획득하는 단계;상기 히트 맵 및 상기 카메라 정보에 기초하여, 상기 객체의 형상 중 상기 제1 포인트 클라우드에서 상기 누락된 포인트들을 추정하는 단계;상기 추정 결과의 신뢰도 값이 제3 임계 값 미만인지 여부를 결정하는 단계; 및상기 신뢰도 값이 제3 임계 값 미만인 경우, 상기 멀티 뷰 이미지들을 추가 획득하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항 및 제7항 중 어느 한 항에 있어서, 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된물리 법칙을 위반하는지 여부를 추론하는 단계는,상기 동영상 중 제1 프레임에서 객체를 지각하는 단계;상기 지각 결과에 기초하여 상기 제1 프레임의 다음 프레임인 제2 프레임의 예측 값을 획득하는 단계;상기 제2 프레임의 예측 값과 상기 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는지 여부를 결정하는 단계; 및공개특허 10-2024-0049128-4-상기 오차가 상기 제4 임계 값보다 크면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 것으로 결정하고, 상기 오차가 상기 제4 임계 값보다 작거나 같으면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 것으로 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서,상기 적어도 하나의 기 정의된 물리 법칙은, 객체 지속성(object persistence), 견고성(solidity), 불변성(unchangeableness), 및 방향 관성(directional inertia) 중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서,상기 적어도 하나의 심층 신경망 모델(231)은, 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 영상 시퀀스 및 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터(training data)로 이용하여, 상기 영상 시퀀스에서의 객체의 움직임이 상기 적어도 하나의 기 정의된 물리법칙을 위반하는지 여부를 추론하도록 학습된, 방법."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "포인트 클라우드(point cloud)를 생성하는 전자 장치(200)에 있어서,적어도 하나의 센서(210);하나 이상의 인스트럭션을 저장하는 메모리(280); 및상기 메모리(280)에 저장된 상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프로세서(270)를 포함하되,상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 적어도 하나의 센서(210)로부터, 객체에 대응하는 제1 센싱 데이터를 획득하고,상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득하고,적어도 하나의 심층 신경망 모델(231)을 이용하여, 상기 제1 포인트 클라우드에서 적어도 하나의 기 정의된 물리 법칙의 위반을 나타내는 이상치 포인트들을 식별하고,상기 이상치 포인트들에 기초하여, 상기 객체에 대한 재촬영 위치 가이드를 제공하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 재촬영 위치 가이드를 제공한 후, 상기 적어도 하나의 센서(210)로부터, 상기 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획득하고,상기 제1 센싱 데이터 및 상기 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 전자 장치(200)는,디스플레이(263)를 더 포함하고,공개특허 10-2024-0049128-5-상기 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,재촬영이 필요한 카메라 위치를 제1 형태(1311)로 시각화한 상기 재촬영 위치 가이드를 상기 디스플레이(263)에표시하고;상기 제2 센싱 데이터를 획득한 후, 상기 제2 센싱 데이터가 상기 재촬영이 필요한 카메라 위치에서 촬영된 것인지 결정하고; 및상기 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정한 것에 기초하여, 상기 제1 형태를 재촬영이 불필요한 카메라 위치를 나타내는 제2 형태(1321)로 변경하여 상기 재촬영 위치 가이드를 상기 디스플레이(263)에 표시하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항 내지 제13항 중 어느 한 항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 위치를 추정하고,상기 추정된 위치에 기초하여, 상기 객체와 상기 적어도 하나의 센서 사이의 거리를 추정하고,상기 추정된 거리, 상기 적어도 하나의 센서의 화각, 및 상기 제1 센싱 데이터 중 적어도 하나에 기초하여, 상기 객체에 대응하는 픽셀 해상도를 획득하고,상기 픽셀 해상도가 제1 임계 값 미만인지 여부를 결정하고,상기 픽셀 해상도가 상기 제1 임계 값 미만인 경우, 상기 추정된 거리보다 더 가까운 거리에서 상기 객체를 촬영할 것을 지시하는 상기 재촬영 위치 가이드를 제공하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항 내지 제14항 중 어느 한 항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서(210)의 제1 위치를추정하고,상기 추정된 제1 위치에 기초하여, 상기 식별된 부분에 대응하는 제2 센싱 데이터를 획득하기 위한 상기 적어도하나의 센서(210)의 제2 위치를 추정하고,상기 제1 위치 및 상기 제2 위치를 비교하여 상기 적어도 하나의 센서의 위치 조정 값을 획득하고,상기 위치 조정 값이 제2 임계 값을 초과하는지 여부를 결정하고,상기 위치 조정 값이 제2 임계 값을 초과하는 경우, 상기 제2 위치에서 상기 객체를 촬영할 것을 지시하는 상기재촬영 위치 가이드를 제공하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제15항 중 어느 한 항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 제1 포인트 클라우드에 기초하여 상기 객체에 대응하는 멀티 뷰 이미지들을 획득하고, 상기 멀티 뷰 이미지들 각각에 대응하는 카메라 정보를 획득하고,상기 멀티 뷰 이미지들로 구성되는 동영상을 상기 적어도 하나의 심층 신경망 모델(231)에 적용하여, 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하고,공개특허 10-2024-0049128-6-상기 추론 결과에 기초하여, 상기 객체의 형상 중에서 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 동영상에서 상기 추론 결과에 대응하는 히트 맵을 획득하고,상기 히트 맵 및 상기 카메라 정보에 기초하여, 상기 객체의 형상 중 상기 제1 포인트 클라우드에서 상기 누락된 포인트들을 추정하고,상기 추정 결과의 신뢰도 값이 제3 임계 값 미만인지 여부를 결정하고,상기 신뢰도 값이 제3 임계 값 미만인 경우, 상기 멀티 뷰 이미지들을 추가 획득하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항 및 제17항 중 어느 한 항에 있어서, 상기 적어도 하나의 프로세서(270)는, 상기 하나 이상의 인스트럭션을 실행함으로써,상기 동영상 중 제1 프레임에서 객체를 지각하고,상기 지각 결과에 기초하여 상기 제1 프레임의 다음 프레임인 제2 프레임의 예측 값을 획득하고,상기 제2 프레임의 예측 값과 상기 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는지 여부를 결정하고,상기 오차가 상기 제4 임계 값보다 크면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 것으로 결정하고, 상기 오차가 상기 제4 임계 값보다 작거나 같으면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 것으로 결정하는, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항 내지 제18항 중 어느 한 항에 있어서,상기 적어도 하나의 심층 신경망 모델(231)은, 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 영상 시퀀스 및 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터로 이용하여, 상기 영상 시퀀스에서의 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하도록 학습된, 전자 장치."}
{"patent_id": "10-2023-0026906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수있는 기록매체."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 포인트 클라우드를 생성하는 방법 및 전자 장치에 관한 것이다. 본 개시의 실시예에 따른 포인트 클라 우드를 생성하는 방법은, 전자 장치의 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득하는 단계, 제1 센싱 데이터에 기초하여 객체에 대응하는 제1 포인트 클라우드를 획득하는 단계, 적어도 하나의 심층 신경망 모델을 이용하여, 제1 포인트 클라우드에서 적어도 하나의 기 정의된 물리 법칙의 위반을 나타내는 이상 치 포인트들을 식별하는 단계, 및 이상치 포인트들에 기초하여, 객체에 대한 재촬영 위치 가이드를 제공하는 단 계를 포함한다."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시의 일 실시예는 포인트 클라우드를 생성하는 방법 및 전자 장치에 관한 것이다."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "과거 스마트 폰과 같은 사용자 기기에 탑재된 이미지 센서는 그 크기가 작아서 해상도가 낮았고, 광학식 이미지 안정화 기술이 발전되지 않아 블러 현상이 빈번하게 발생하였으며, 깊이 정보를 측정하는 기술이 없었다. 최근에는 LiDAR 센서, ToF 센서, RGB-D 센서와 같은 다양한 센서를 탑재한 사용자 기기들(예컨대, 스마트 폰)이 출시되고 있으며, 이러한 센서들을 이용하여 촬영된 이미지들에서 객체의 포인트 클라우드를 생성하는 방법이 연구되고 있다. 사용자 기기에서 다양한 센서들이 탑재되면서 포인트 클라우드 생성 기술의 응용 분야가 확대되 고 있다. 예를 들어, 포인트 클라우드는 3D 스캐닝, 3D 프린팅, 가상 현실, 증강 현실 및 자율주행 등의 분야에 서 널리 사용되고 있다."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시예에 있어서, 전자 장치가 포인트 클라우드(point cloud)를 생성하는 방법이 제공될 수 있다. 상기 방법 은, 상기 전자 장치의 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득하 는 단계를 포함할 수 있다. 상기 방법은, 적어도 하나의 심층 신경망 모델을 이용하여, 상기 제1 포인트 클라우 드에서 적어도 하나의 기 정의된 물리 법칙의 위반을 나타내는 이상치 포인트들을 식별하는 단계를 포함할 수 있다. 상기 방법은, 이상치 포인트들에 기초하여, 상기 객체에 대한 재촬영 위치 가이드를 제공하는 단계를 포 함할 수 있다. 일 실시예에 있어서, 전자 장치가 포인트 클라우드를 생성하는 전자 장치가 제공될 수 있다. 상기 전자 장치는, 적어도 하나의 센서를 포함할 수 있다. 상기 전자 장치는, 하나 이상의 인스트럭션을 저장하는 메모리를 포함할 수 있다. 상기 전자 장치는, 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프 로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서 는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으 로써, 적어도 하나의 심층 신경망 모델을 이용하여, 상기 제1 포인트 클라우드에서 적어도 하나의 기 정의된 물 리 법칙의 위반을 나타내는 이상치 포인트들을 식별할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이 상의 인스트럭션을 실행함으로써, 상기 이상치 포인트들에 기초하여, 상기 객체에 대한 재촬영 위치 가이드를 제공할 수 있다. 일 실시예에 있어서, 상기 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체가 제공될 수 있다."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서, \"a, b 또는 c 중 적어도 하나\" 표현은 \" a\", \" b\", \" c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 또한, 본 명세서에서 사용되는 '제1' 또는 '제2' 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소 프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 또한, 각각의 도면에서 사용된 도면 부호는 각각의 도면을 설명하기 위한 것일 뿐, 상이한 도면들 각각 에서 사용된 상이한 도면 부호가 상이한 요소를 나타내기 위한 것은 아니다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 개념도이다. 도 1을 참조하면, 일 실시예에 따른 전자 장치는 센서(예컨대, 이미지 센서) 및 디스플레이를 포함하는 장 치일 수 있다. 전자 장치는 센서를 통해 센싱 데이터(예컨대, 정지 이미지 데이터, 동영상 데이터 등)를 획득하고, 디스플레이를 통해 센싱 데이터를 출력하는 장치일 수 있다. 예를 들어, 스마트 TV, 스마트 폰, 태블 릿 PC, 랩탑 PC, 등을 포함할 수 있으나, 이에 한정되는 것은 아니다. 전자 장치는 센서 및 디스플레이를 포함하는 다양한 종류 및 형태의 장치로 구현될 수 있다. 또한, 전자 장치는 오디오를 출력하기 위한 스피커를 포함할 수도 있다. 전자 장치의 구체적인 구성, 동작, 및 기능은 도 2a 및 2b에서 좀 더 구체적으로 설명한다. 일 실시예에 있어서, 전자 장치의 사용자는 전자 장치의 센서를 이용하여 객체를 촬영할 수 있 다. 설명의 편의를 위해, 객체는 의자의 형태로 도시되었으나, 객체의 종류는 이에 한정되지 않는다. 전자 장치는 객체의 적어도 일부를 포함하는 센싱 데이터를 획득할 수 있다. 일 실시예에 있어서, 전자 장치는 센서로부터 객체에 대응하는 제1 센싱 데이터를 획득할 수 있다. 전자 장치는 제1 센싱 데이터에 기초하여 객체에 대응하는 제1 포인트 클라우드를 획득할 수 있 다. 본 개시에서, 포인트 클라우드는 3차원 좌표계에서 객체의 적어도 일부에 대응하는 포인트들의 집합을 나타낼 수 있다. 포인트 클라우드는 포인트 클라우드 이미지로 표현될 수도 있다. 제1 포인트 클라우드는 객체의 전체 또는 일부에 대응할 수 있다. 일 실시예에 있어서, 제1 포인트 클라우드에서, 객체의 일부 영역에 대응하는 포인트들이 누락된 부분이 존재할 수 있다. 해당 포인트들은, 누락된 포인트들로 지칭될 수도 있다. 예를 들어, 객체를 센싱(또는 촬영)한 센서의 위 치 및/또는 각도에 따라, 객체의 전체에 대응하는 제1 포인트 클라우드가 생성(또는 획득)되지 못할 수 있다. 예를 들어, 제1 포인트 클라우드의 누락된 부분은, 정반사(specular reflection), 신호 흡 수, 다른 객체에 의한 폐색(occlusion), 객체의 자기 폐색(self-occlusion), 및 블라인드 스팟(blind spot) 중 적어도 하나에 의해 발생할 수 있으나, 이에 한정되지 않는다. 전자 장치는 심층 신경망 모델을 이용하여, 제1 포인트 클라우드에서 이상치 포인트들을 식별할 수 있다. 일 실시예에 있어서, 이상치 포인트들은 적어도 하나의 기 정의된 물리 법칙의 위반을 나타낼 수 있다. 일 실시예에 있어서, 전자 장치는 하나의 심층 신경망 모델을 이용하여 제1 포인트 클라우드에서 하 나의 기 정의된 물리 법칙의 위반을 나타내는 이상치 포인트들을 식별할 수 있다. 일 실시예에 있어서, 전자 장 치는 하나의 심층 신경망 모델을 이용하여 제1 포인트 클라우드에서 복수의 기 정의된 물리 법칙들 각각의 위반을 나타내는 이상치 포인트들을 식별할 수 있다. 일 실시예에 있어서, 전자 장치는 복수의 심 층 신경망 모델들을 이용하여 제1 포인트 클라우드에서 복수의 심층 신경망 모델들 각각에 대응하는 기 정 의된 물리 법칙의 위반을 나타내는 이상치 포인트들을 식별할 수 있다. 본 개시에서, 심층 신경망 모델은 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 심층 신경망 모 델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 심층 신경망 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 복수의 가중치들이 갱신될 수 있다. 예를 들어, 심층 신경 망 모델은 CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등을 포함할 수 있으나, 전술한 예에 한정되지 않는다. 본 개시에서, 물리 법칙은 인간의 상식 관점에서 직관적으로 이해될 수 있는 직관적 물리학을 나타낼 수 있다. 예를 들어, 정지 이미지 또는 동영상에 표현되는 객체의 움직임이 인간의 상식 관점에서 이해되지 않는 경우, 해당 객체의 움직임은 물리 법칙을 위반한 것이다. 예를 들어, 물리 법칙은 객체 지속성(object persistence), 견고성(solidity), 불변성(unchangeableness), 및/또는 방향 관성(directional inertia) 등을 포함할 수 있으 나, 이제 한정되는 것은 아니다. 일 실시예에 있어서, 제1 포인트 클라우드의 누락된 포인트들에 대응하는 누락된 부분의 존재로 인해, 제1 포인트 클라우드는 적어도 하나의 기 정의된 물리 법칙을 위반한 것으로 식별될 수 있다. 예를 들어, 의자 객체의 다리가 앞에 두 개만 있는 경우, 의자가 뒤 쪽으로 기울어져야 하는데, 의자가 똑바로 서 있 는 것은 물리 법칙(예컨대, 중력)에 맞지 않는 것으로 식별될 수 있다. 전자 장치는, 이상치 포인트들에 기초하여, 객체에 대한 재촬영 위치 가이드를 제공할 수 있다. 예를 들어, 전자 장치는 재촬영 위치 가이드를 디스플레이를 통해 출력(또는 표시)할 수 있다. 예를 들어, 재촬영 위치 가이드는 객체를 다른 위치 또는 각도에서 센싱(또는 촬영)하도록 지시하는 메시 지, 이미지 및/또는 오디오 등을 포함할 수 있다. 예를 들어, 재촬영 위치 가이드는 전자 장치에 설치된 특정 애플리케이션(예컨대, 3D 모델링 애플리케이션)을 통해서 제공될 수 있다. 전자 장치의 사용자는 재촬영 위치 가이드를 참조하여 제1 센싱 데이터에 대응하는 촬영 각도와 다른 각도에서 객체의 적어도 일부를 촬영할 수 있다. 일 실시예에 있어서, 전자 장치는 센서로부터 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획 득할 수 있다. 전자 장치는 제1 센싱 데이터 및 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득할 수 있다. 제2 포인트 클라우드는, 제1 포인트 클라우드에서 누락된 부분에 대응하는 누락된 포인트들의 집합을 포함하는 완전한 포인트 클라우드일 수 있다. 도 2a 및 2b는 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 구현하는 전자 장치를 보여주는 블록 도이다. 전자 장치의 구성, 동작, 및 기능은 도 1의 전자 장치의 구성, 동작, 및 기능에 대응할 수 있다. 설명의 편의를 위해, 도 1에서 설명한 내용과 중복되는 내용은 생략한다. 도 2a를 참조하면, 일 실시예에 따른 전자 장치는 센서, 메모리, 및 프로세서를 포함할 수 있다. 그러나, 도 2a에 도시된 구성요소가 필수 구성요소인 것은 아니며, 전자 장치는 구성요소를 생략하 거나, 추가 구성요소를 더 포함할 수 있다. 예를 들어, 일 실시예에 따른 전자 장치는, 도 2b에 도시된 바 와 같이, 적어도 하나의 센서, 통신 인터페이스, 사용자 인터페이스, 프로세서, 메모리 를 포함할 수 있다. 센서는 물리량을 계량하거나 감지함으로써, 계량 또는 감지된 정보를 전기 신호(또는 센싱 데이터)로 변환 할 수 있다. 예를 들어, 센서는 외부 장면의 정지 이미지 또는 동영상의 적어도 하나의 프레임을 촬영하기 위한 적어도 하나의 카메라 또는 이미지 센서를 포함할 수 있다. 예를 들어, 센서는 터치 입력을 위한 적 어도 하나의 버튼, 제스처 센서, 자이로스코프, 자이로 센서, 기압 센서, 자기 센서, 자력계, 가속도 센서, 가 속도계, 그립 센서, 근접 센서, RGB 센서, 생체물리 센서, 온도 센서, 습도 센서, 조도 센서, 자외선 센서, 근 전도 센서, 뇌파 센서, 심전도 센서, 적외선 센서, 초음파 센서, 홍채 센서, 또는 지문 센서 중 적어도 하나를 포함할 수 있으나, 본 개시는 이에 한정되지 않는다. 일 실시예에 있어서, 센서는 객체를 포함하는 장면을 촬영할 수 있다. 센서는 객체에 대응하는 포인 트 클라우드를 생성하기 위한 센싱 데이터를 생성할 수 있다. 예를 들어, 센서는 이미지 센서, LiDAR 센서, RGB-D 센서, 깊이 센서, ToF(time of flight) 센서, 초음파 센서, 레이다 센서, 및 스테레오 카메라 중 적어도 하나로 구성될 수 있으나, 본 개시는 이에 한정되지 않는다. 센서는 센싱 데이터를 포인트 클라우 드 생성 모듈에 전달할 수 있다. 일 실시예에 있어서, 센싱 데이터는 적어도 하나의 스틸 이미지 또는 동 영상일 수 있다. 통신 인터페이스는 전자 장치와 외부의 다른 전자 장치(미도시) 또는 서버(미도시) 사이의 유선 또는 무선 통신 채널의 수립 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 일 실시 예에 있어서, 통신 인터페이스는 유선 또는 무선 통신을 통해 외부의 다른 전자 장치(미도시) 또는 서버(미도시)로부터 데이 터를 수신하거나 외부의 다른 전자 장치(미도시) 또는 서버(미도시)로 데이터를 송신할 수 있다. 몇몇 실시 예 들에 있어서, 통신 인터페이스는 무선 통신 모듈(예컨대, 셀룰러 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예컨대, LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있고, 그 중 어느 하나의 통신 모듈을 이용하여 적어도 하나의 네트워크(예컨대, 근거리 통신 네트워크(예컨대, 블루투스, WiFi direct 또는 IrDA(infrared data association)) 또는 원거리 통신 네트워크(예컨대, 셀룰러 네트워크, 인터넷, 또는 컴퓨터 네트워크(예컨대, LAN 또는 WAN)))를 통하여 외부의 다른 전자 장치(미도시) 또는 서버(미도시)와 통신할 수 있다. 사용자 인터페이스는 입력 인터페이스 및 출력 인터페이스를 포함할 수 있다. 입력 인터페이스는, 사용자로부터의 입력(이하에서, 사용자 입력)을 수신하기 위한 것이다. 입력 인터페이 스는 키 패드(key pad), 돔 스위치 (dome switch), 터치 패드(접촉식 정전 용량 방식, 압력식 저항막 방식, 적외선 감지 방식, 표면 초음파 전도 방식, 적분식 장력 측정 방식, 피에조 효과 방식 등), 조그 휠, 조 그 스위치 중 적어도 하나일 수 있으나, 이에 한정되는 것은 아니다. 입력 인터페이스는 음성 인식 모듈을 포함할 수 있다. 예를 들어, 전자 장치는 마이크로폰을 통해 아 날로그 신호인 음성 신호를 수신하고, ASR(Automatic Speech Recognition) 모델을 이용하여 음성 부분을 컴퓨터 로 판독 가능한 텍스트로 변환할 수 있다. 전자 장치는 자연어 이해(Natural Language Understanding, NLU) 모델을 이용하여 변환된 텍스트를 해석하여, 사용자의 발화(utterance) 의도를 획득할 수 있다. 여기서ASR 모델 또는 NLU 모델은 인공지능 모델일 수 있다. 인공지능 모델은 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지능 전용 프로세서에 의해 처리될 수 있다. 인공지능 모델은 학습을 통해 만들어 질 수 있 다. 여기서, 학습을 통해 만들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이 터들을 이용하여 학습됨으로써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 복수의 신경망 레이어들 각각은 복수의 가중치들(weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경망 연산을 수행한다. 언어적 이해는 인간의 언어/문자를 인식하고 응용/처리하는 기술로서, 자연어 처리(Natural Language Processing), 기계 번역(Machine Translation), 대화 시스템(Dialog System), 질의 응답(Question Answering), 음성 인식/합성(Speech Recognition/Synthesis) 등을 포함한다. 출력 인터페이스는 오디오 신호 또는 비디오 신호의 출력을 위한 것으로, 예컨대 디스플레이 또는 스 피커 등이 포함될 수 있다. 일 실시예에 의하면, 전자 장치는 디스플레이를 통해서 전자 장치와 관련된 정보를 표시해 줄 수 있다. 예를 들어, 전자 장치는 센서의 센싱 데이터를 시각화한 이미지들을 디스플레이에 표 시할 수 있다. 예를 들어, 전자 장치는 재촬영 위치 가이드를 디스플레이에 표시할 수 있다. 디스플레이와 터치패드가 레이어 구조를 이루어 터치 스크린으로 구성되는 경우, 디스플레이는 출력 장치 이외에 입력 장치로도 사용될 수 있다. 디스플레이는 액정 디스플레이(liquid crystal display), 박 막 트랜지스터 액정 디스플레이(thin film transistor-liquid crystal display), 발광 다이오드(LED, light- emitting diode), 유기 발광 다이오드(organic light-emitting diode), 플렉시블 디스플레이(flexible display), 3차원 디스플레이(3D display), 전기영동 디스플레이(electrophoretic display) 중에서 적어도 하나 를 포함할 수 있다. 그리고 전자 장치의 구현 형태에 따라 디스플레이를 2개 이상 포함할 수도 있다. 스피커는 통신 인터페이스로부터 수신되거나 메모리에 저장된 오디오 데이터를 출력할 수 있다. 또한, 스피커는 전자 장치에서 수행되는 기능과 관련된 음향 신호를 출력할 수 있다. 프로세서는 AP(application processor), CPU(central processing unit) 또는 GPU(graphic processing unit)와 같은 범용 프로세서와 소프트웨어의 조합을 통해 구현될 수도 있다. 전용 프로세서의 경우, 본 개시의 실시예를 구현하기 위한 메모리를 포함하거나, 외부 메모리를 이용하기 위한 메모리 처리부를 포함할 수 있다. 프로세서는 복수의 프로세서로 구성될 수도 있다. 이 경우, 전용 프로세서들의 조합으로 구현될 수도 있고, AP, CPU 또는 GPU와 같은 다수의 범용 프로세서들과 소프트웨어의 조합을 통해 구현될 수도 있다. 일 실시예에 있어서, 프로세서는, 인공 지능(AI) 프로세서를 탑재할 수도 있다. 인공 지능(AI) 프로세서는, 인공 지능(AI)을 위한 전용 하드웨어 칩 형태로 제작될 수도 있고, 기존의 범용 프로세서(예: CPU 또는 application processor) 또는 그래픽 전용 프로세서(예: GPU)의 일부로 제작되어 전자 장치에 탑재될 수도 있다. 예를 들어, 인공 지능 프로세서는 적어도 하나의 심층 신경망 모델과 관련된 학습 및/또는 추 론에 필요한 데이터 처리를 수행할 수 있다. 본 개시에 따른 인공지능과 관련된 기능은 프로세서와 메모리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등 과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인 공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기 정의된 동작 규칙 또는 인 공지능 모델(예컨대, 심층 신경망 모델)에 따라, 입력 데이터를 처리하도록 제어한다. 또는, 하나 또는 복수의 프로세서가 인공지능 전용 프로세서인 경우, 인공지능 전용 프로세서는, 특정 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 기 정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만 들어진다는 것은, 기본 인공지능 모델이 학습 알고리즘에 의하여 다수의 학습 데이터들을 이용하여 학습됨으로 써, 원하는 특성(또는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미 한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버 및/ 또는 시스템을 통해 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도 형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 메모리는, 프로세서의 처리 및 제어를 위한 프로그램을 저장할 수도 있고, 입/출력되는 데이터들을 저장할 수도 있다. 메모리는 적어도 하나의 심층 신경 망 모델을 저장할 수도 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 또한, 전자 장치는 인터넷(internet)상에서 저장 기능을 수행하는 웹 스토리지(web storage) 또는 클라우드 서버를 운영할 수도 있 다. 일 실시예에 있어서, 메모리에는 프로세서에 의하여 처리되거나 처리될 예정인 데이터, 펌웨어, 소프 트웨어, 및 프로세스 코드 등이 저장될 수 있다. 일 실시 예에 있어서, 메모리에는 포인트 클라우드 생성 모듈, 물리 법칙 위반 식별 모듈, 적어도 하나의 심층 신경망 모델, 및 재촬영 위치 가이드 제 공 모듈 중 적어도 하나에 대응되는 데이터 및 프로그램 코드들이 저장될 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터를 수신할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데 이터에 기초하여 포인트 클라우드를 생성(또는 획득)할 수 있다. 포인트 클라우드 생성 모듈은 포인트 클 라우드를 생성하기 위한 데이터 처리를 수행할 수 있다. 예를 들어, 포인트 클라우드 생성 모듈은 센싱 데 이터의 노이즈를 제거하거나 및/또는 필터링할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터에 기초 하여 장면의 객체와 센서 간의 거리 및/또는 각도 등의 정보를 추출할 수 있다. 포인트 클라우드 생성 모듈 은 추출된 정보를 3차원 좌표계의 포인트들을 포함하는 데이터로 변환할 수 있다. 포인트 클라우드 생성 모듈은 변환된 데이터, 즉 포인트 클라우드를 물리 법칙 위반 식별 모듈에 전달할 수 있다. 일 실시예에 있어서, 포인트 클라우드 생성 모듈은 센싱 데이터에 기초하여 객체 검출(object detection) 을 수행할 수 있다. 포인트 클라우드 생성 모듈은 검출된 객체에 대응하는 포인트 클라우드를 생성할 수 있다. 일 실시예에 있어서, 센서는 이미지 센서(예컨대, RGB 센서)를 포함할 수 있다. 센서는 객체에 대응 하는 센싱 데이터(예컨대, 컬러 이미지)를 획득할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터에 기초하여 포인트 클라우드를 생성하기 위해, 2D의 센싱 데이터로부터 장면(또는 객체)의 깊이를 추정할 수 있다. 예를 들어, 포인트 클라우드 생성 모듈은 vSLAM(visual simultaneous localization and mapping) 알고리즘을 이용하여 장면의 깊이를 추정할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터 및 추정된 깊이를 이용하여 포인트 클라우드를 생성할 수 있다. 일 실시예에 있어서, 센서는 스테레오 카메라(예컨대, 두 개의 이미지 센서들)를 포함할 수 있다. 예를 들 어, 두 개의 이미지 센서들은 일정한 간격으로 전자 장치에 배치될 수 있다. 두 개의 이미지 센서들 각각 은 동일한 시간에 객체를 촬영할 수 있다. 두 개의 이미지 센서들 각각은 객체에 대응하는 컬러 이미지를 획득 할 수 있다. 포인트 클라우드 생성 모듈은 두 개의 컬러 이미지들로부터 장면의 깊이를 추정할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터 및 추정된 깊이를 이용하여 포인트 클라우드를 생성할 수 있다. 일 실시예에 있어서, 센서는 장면 또는 객체의 깊이 값을 측정하는 센서(예컨대, LiDAR 센서, RGB-D 센서, 깊이 센서, ToF(time of flight) 센서, 초음파 센서, 레이다 센서 등)를 포함할 수 있다. 센서는 깊이 값 을 포함하는 센싱 데이터를 획득할 수 있다. 포인트 클라우드 생성 모듈은 센싱 데이터에 기초하여 포인트 클라우드를 생성할 수 있다. 일 실시예에 있어서, 센서는 두 종류 이상의 센서들의 조합으로 구성될 수 있다. 포인트 클라우드 생성 모 듈은 두 종류 이상의 센서들에 의해 획득되는 센싱 데이터들을 시간 동기화할 수 있다. 일 실시예에 있어 서, 센서는 이미지 센서와 ToF 센서로 구성될 수 있다. 포인트 클라우드 생성 모듈은 이미지 센서에 의해 획득된 컬러 이미지와 ToF 센서에 의해 획득된 깊이 값을 이용하여 포인트 클라우드를 생성할 수 있다. 일 실시예에 있어서, 센서는 RGB-D 센서와 LiDAR 센서로 구성될 수 있다. 포인트 클라우드 생성 모듈은 RGB-D 센서와 LiDAR 센서 각각에 의해 획득된 센싱 데이터에 기초하여 포인트 클라우드를 생성할 수 있다. 일 실시예에 따르면, 더 많은 종류의 센서들을 이용함으로써, 정확도가 높은 포인트 클라우드가 생성될 수 있다. 일 실시예에 있어서, 센싱 데이터가 복수의 스틸 이미지들이거나 동영상인 경우, 포인트 클라우드 생성 모듈 은 복수의 스틸 이미지들 또는 동영상의 복수의 프레임들을 이용하여, 다양한 위치와 각도에 대응하는 포인트 클라우드들을 정합할 수 있다. 물리 법칙 위반 식별 모듈은 적어도 하나의 심층 신경망 모델을 포함할 수 있다. 물리 법칙 위반 식 별 모듈은 포인트 클라우드를 수신할 수 있다. 물리 법칙 위반 식별 모듈은 적어도 하나의 심층 신경 망 모델을 이용하여, 포인트 클라우드에서 이상치 포인트들을 식별할 수 있다. 일 실시예에 있어서, 물리 법칙 위반 식별 모듈은 포인트 클라우드에 기초하여 객체에 대응하는 멀티 뷰 이미지들을 생성할 수 있다. 본 개시에서, 멀티 뷰 이미지들은 동일한 객체에 대한 여러 시점에서의 이미지들을 의미할 수 있다. 일 실시예에 있어서, 물리 법칙 위반 식별 모듈은 포인트 클라우드를 이용하여 객체에 대한 3D 모델링을 수행할 수 있다. 예를 들어, 물리 법칙 위반 식별 모듈은 포인트 클라우드에 기초하여 객체에 대응하는 메 쉬를 생성할 수 있다. 물리 법칙 위반 식별 모듈은 생성된 메쉬에 대해 텍스처링 및 UV 매핑을 수행할 수 있다. 일 실시예에 있어서, 물리 법칙 위반 식별 모듈은 포인트 클라우드에 기초한 3D 모델(예컨대, 메쉬)을 렌 더링할 수 있다. 예를 들어, 객체를 360도 회전하여 2D 이미지들을 생성하는 방식으로 렌더링이 수행될 수 있다. 물리 법칙 위반 식별 모듈은 렌더링 결과에 기초하여 멀티 뷰 이미지들을 생성할 수 있다. 물리 법 칙 위반 식별 모듈은 멀티 뷰 이미지들을 카메라를 여러 각도로 회전하여 객체를 촬영한 동영상으로 재구 성할 수 있다. 일 실시예에 있어서, 물리 법칙 위반 식별 모듈은 멀티 뷰 이미지들 각각에 대응하는 카메라 정보를 획득 할 수 있다. 예를 들어, 카메라 정보는 카메라 행렬을 포함할 수 있다. 예를 들어, 카메라 행렬은 카메라의 내 부 파라미터(예컨대, 광학 중심, 렌즈 왜곡, 초점 거리 등) 및 외부 파라미터(카메라의 위치, 방향 등)를 포함 할 수 있다. 물리 법칙 위반 식별 모듈은 멀티 뷰 이미지들로 구성되는 동영상을 적어도 하나의 심층 신경망 모델(23 1)에 적용하여, 동영상에서 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론할 수 있다. 객체의 움직임은 이전 프레임과 현재 프레임에서의 객체의 변화에 대응할 수 있다. 일 실시예에 있어 서, 적어도 하나의 심층 신경망 모델은 복수의 물리 법칙들 각각에 대응하는 복수의 심층 신경망 모델들을 포함할 수 있다. 물리 법칙 위반 식별 모듈은 동영상을 복수의 심층 신경망 모델들 각각에 입력하여, 동영 상에서 객체의 움직임이 복수의 물리 법칙들 각각을 위반하는지 여부를 추론할 수 있다. 일 실시예에 있어서, 적어도 하나의 심층 신경망 모델은, 적어도 하나의 기 정의된 물리 법칙을 위반하는 영상 시퀀스 및 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터로 이용하여, 영상 시퀀스에서의 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부 를 추론하도록 학습될 수 있다. 예를 들어, 적어도 하나의 심층 신경망 모델은 적어도 하나의 기 정의된 물리 법칙을 위반하는 경우, 물리 법칙 위반을 나타내는 데이터를 출력하고, 적어도 하나의 기 정의된 물리 법 칙을 위반하지 않는 경우, 물리 법칙 위반하지 않음을 나타내는 데이터를 출력하도록 학습될 수 있다. 일 실시 예에 있어서, 데이터 셋에는, 적어도 하나의 기 정의된 물리 법칙을 위반하는 영상 시퀀스 및 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스의, 물리 법칙 위반 여부가 레이블링되어 있을 수 있다. 적어도 하나의 심층 신경망 모델은, 데이터 셋을 학습함으로써 적어도 하나의 심층 신경망 모델의 신경망 레 이어들의 가중치 값들을 갱신할 수 있다. 일 실시예에 있어서, 전자 장치 또는 외부 전자 장치(미도시)는 하나의 물리 법칙의 위반 여부를 추론하기 위해, 하나의 물리 법칙(예컨대, 객체 지속성)에 대응하는 데이터 셋을 학습 데이터로 이용하여, 영상 시퀀스에 서의 객체의 움직임이 하나의 심층 신경망 모델이 해당 물리 법칙(예컨대, 객체 지속성)을 위반하는지 여부를 추론하도록 학습할 수 있다. 일 실시예에 있어서, 하나의 심층 신경망 모델은 하나의 물리 법칙의 위반 여부를 추론할 수 있다. 예를 들어, 제1 심층 신경망 모델은 제1 물리 법칙의 위반 여부를 추론하고, 제2 심층 신경망 모델은 제2 물리 법칙 위반 여부를 추론하고, 제k 심층 신경망 모델은 제k 물리 법칙 위반 여부를 추론할 수 있다. 이 경우, 학습 장치(예 컨대, 전자 장치 또는 외부 전자 장치)에 의해 학습할 심층 신경망 모델을 사용자 입력 또는 제조사의 설 정에 기초하여 선정할 수 있다. 선정된 심층 신경망 모델에 대응되는 물리 법칙을 위반한 영상 시퀀스 및 위반 하지 않은 영상 시퀀스가 시뮬레이션을 통해 생성됨으로써, 데이터 셋이 확보될 수 있다. 물리 법칙 위반 식별 모듈은, 동영상에서 추론 결과에 대응하는 히트 맵(heat map)을 획득할 수 있다. 물 리 법칙 위반 식별 모듈은 히트 맵 및 카메라 정보에 기초하여, 객체의 형상 중 포인트 클라우드에서 누락된 포인트들을 추정할 수 있다. 본 개시에서, 히트 맵은 동영상의 프레임들 각각에서 물리 법칙을 위반한 부분 에 대응하는 포인트들의 집합을 나타낼 수 있다. 재촬영 위치 가이드 제공 모듈은 포인트 클라우드에서 적어도 하나의 기 정의된 물리 법칙을 위반한 부분 (즉, 이상치 포인트들)에 기초하여, 객체에 대한 재촬영 위치 가이드를 제공할 수 있다. 재촬영 위치 가이드 제 공 모듈은 전자 장치 또는 외부 전자 장치를 이용하여 사용자에게 재촬영이 필요한 객체의 적어도 일 부에 대한 정보를 전달할 수 있다. 예를 들어, 재촬영 위치 가이드 제공 모듈은 재촬영 위치 가이드를 메 시지, 이미지, 및/또는 오디오를 통해 사용자에게 전달할 수 있다. 도 3은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 흐름도이다. 도 1 내지 2b에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 및 2b를 참조하여, 도 3을 설명한다. 도 3을 참조하면, 전자 장치가 포인트 클라우드를 생성하는 방법은 단계 S310 내지 S340을 포함할 수 있다. 일 실시 예에 있어서, 단계 S310 내지 S340은 전자 장치 또는 전자 장치의 프로세서에 의 해 수행될 수 있다. 본 개시에 따른 전자 장치가 포인트 클라우드를 생성하는 방법은 도 3에 도시된 바에 한정되지 않으며, 도 3에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 3에 도시되지 않은 단계를 더 포함 할 수도 있다. 단계 S310에서, 전자 장치의 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득할 수 있다. 적어도 하나의 센서는 사용자 입력 또는 제조사의 설정에 기초하여 객체를 포함하는 장면을 센싱 (또는 촬영)할 수 있다. 일 실시예에 있어서, 제1 센싱 데이터는 적어도 하나의 스틸 이미지 또는 복수의 프레 임들을 포함하는 동영상을 포함할 수 있다. 일 실시예에 있어서, 제1 센싱 데이터는 2차원 좌표 값, 깊이 값, 및/또는 색상 값을 포함할 수 있다. 단계 S320에서, 전자 장치는 제1 센싱 데이터에 기초하여 객체에 대응하는 제1 포인트 클라우드를 획득할 수 있다. 예를 들어, 전자 장치는 제1 센싱 데이터에 기초하여 제1 포인트 클라우드를 생성할 수 있다. 예 를 들어, 전자 장치는 통신 인터페이스를 통해 외부 전자 장치에 제1 센싱 데이터를 전송하고, 외부 전자 장치로부터 제1 포인트 클라우드를 수신할 수 있다. 단계 S330에서, 전자 장치는 적어도 하나의 심층 신경망 모델을 이용하여, 제1 포인트 클라우드에서 이상 치 포인트들을 식별할 수 있다. 일 실시예에 있어서, 이상치 포인트들은 적어도 하나의 기 정의된 물리 법칙의 위반을 나타낼 수 있다. 단계 S340에서, 전자 장치는 이상치 포인트들에 기초하여, 객체에 대한 재촬영 위치 가이드를 제공할 수 있다. 전자 장치는 이미지 및/또는 텍스트를 디스플레이에 표시하거나, 스피커를 통해 오디오를 출력함으로써 재촬영 위치 가이드를 제공할 수 있다. 도 4a는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 개념도이다. 도 1 내지 3에서 설명 한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3을 참조하여, 도 4a를 설명한다. 도 4a를 참조하면, 전자 장치의 적어도 하나의 센서와 객체 간의 거리로 인해, 객체를 표 현하는 제1 센싱 데이터 중 객체에 대응하는 픽셀 해상도가 낮을 수 있다. 픽셀 해상도가 낮은 경우, 객체에 대 응하는 포인트 클라우드를 생성하는 과정에서, 객체의 적어도 일부에 대응하는 포인트 클라우드가 누락될 수 있 다. 전자 장치는 픽셀 해상도가 미리 정의된 임계 값보다 작은 경우, 제1 센싱 데이터에서의 적어도 하나의 센 서와 객체 간의 거리보다 더 가까운 거리에서 객체를 촬영할 것을 지시하는 재촬영 위치 가이드를 제 공할 수 있다. 예를 들어, 전자 장치는 \"더 가까운 거리에서 촬영해주세요.\"와 같은 텍스트 메시지(402a) 를 디스플레이에 표시할 수 있다. 예를 들어, 전자 장치는 \"더 가까운 거리에서 촬영해주세요.\"와 같 은 오디오(402b)를 스피커를 통해 외부로 출력할 수 있다. 도 4b는 도 4a의 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 1 내지 4a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 4a를 참조하여, 도 4b를 설명한다.도 4b를 참조하면, 도 3의 단계 S340은 단계 S410 내지 S450을 포함할 수 있다. 일 실시 예에 있어서, 단계 S410 내지 S450은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S340의 세부 단계들은 도 4b에 도시된 바에 한정되지 않으며, 도 4b에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 4b에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S410에서, 전자 장치는 제1 센싱 데이터에 기초하여 제1 센싱 데이터를 획득한 적어도 하나의 센서 의 위치를 추정할 수 있다. 예를 들어, 전자 장치는 단일의 이미지 또는 복수의 이미지들에 기초하여 적어도 하나의 센서의 위치를 결정할 수 있다. 전자 장치는 단일의 이미지 또는 복수의 이미지들에서 특징점들을 추정할 수 있다. 전자 장치는 추정된 특징점들 및/또는 적어도 하나의 센서의 스펙(예컨 대, 초점 거리 등)을 이용하여 적어도 하나의 센서의 위치를 추정할 수 있다. 단계 S420에서, 전자 장치는 추정된 위치에 기초하여, 객체와 적어도 하나의 센서 사이의 거리 를 추정할 수 있다. 예를 들어, 전자 장치는 픽셀 크기, 스테레오 비전(stereo vision), 및 레이캐스팅 (ray casting) 중 적어도 하나의 방식을 이용하여 객체와 적어도 하나의 센서 사이의 거리를 추정할 수 있으나, 본 개시는 이러한 방식에 한정되지 않는다. 단계 S430에서, 전자 장치는 추정된 거리, 적어도 하나의 센서의 화각, 및 제1 센싱 데이터 중 적어 도 하나에 기초하여, 객체에 대응하는 픽셀 해상도를 획득할 수 있다. 일 실시예에 있어서, 전자 장치는 추정된 거리, 적어도 하나의 센서의 화각, 및 제1 센싱 데이터의 해상도에 기초하여 객체에 대응하는 픽셀 해상도를 획득할 수 있다. 본 개시에서, 해상도는 센싱 데이터(예컨대, 이미지)에 포함되는 전체 픽셀 수를 나 타낼 수 있다. 본 개시에서, 픽셀 해상도는 객체가 센싱 데이터(예컨대, 이미지) 상에서 차지하는 픽셀 수를 나 타낼 수 있다. 단계 S440에서, 전자 장치는 픽셀 해상도가 제1 임계 값 미만인지 여부를 결정할 수 있다. 픽셀 해상도가 제1 임계 값 이상인 경우(아니오), 절차는 종료된다. 픽셀 해상도가 제1 임계 값 미만인 경우(예), 절차는 종료 되지 않고 단계 S450으로 이동한다. 단계 S450에서, 전자 장치는 추정된 거리보다 더 가까운 거리에서 객체를 촬영할 것을 지시하는 재촬 영 위치 가이드를 제공할 수 있다. 도 5a는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 개념도이다. 도 1 내지 3에서 설명 한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3을 참조하여, 도 5a를 설명한다. 도 5a를 참조하면, 객체 중 적어도 일부에 대응하는 폐색, 객체를 구성하는 물질, 객체의 구조 및/또는 외부 노이즈(예컨대, 직사광선) 등과 같은 이유로, 객체의 일부 영역에 대응하는 포인트 클 라우드의 누락된 부분이 존재할 수 있다. 포인트 클라우드의 누락된 부분을 채우기 위해, 객체의 일부 영역을 특정 위치(또는 방향, 각도 등)(A1)에서 촬영한 센싱 데이터가 요구될 수 있다. 전자 장치는 특정 위치(A1)에서 객체를 촬영할 것을 지시하는 재촬영 위치 가이드를 제공할 수 있다. 예를 들어, 전자 장치는 \"다음과 같은 각도에서 촬영해주세요.\"와 같은 텍스트 메시지(505a)를 디스플레이(26 3)에 표시할 수 있다. 예를 들어, 전자 장치는 \"다음과 같은 각도에서 촬영해주세요.\"와 같은 오디오 (505b)를 스피커를 통해 외부로 출력할 수 있다. 예를 들어, 전자 장치는 촬영 필요 각도를 시각화한 이미지를 디스플레이에 표시할 수 있다. 도 5b는 도 5a의 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 1 내지 3, 및 5a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3, 및 5a를 참조하여, 도 5b를 설명한다. 도 5b를 참조하면, 도 3의 단계 S340은 단계 S510 내지 S550을 포함할 수 있다. 일 실시 예에 있어서, 단계 S410 내지 S450은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S340의 세부 단계들은 도 5b에 도시된 바에 한정되지 않으며, 도 4b에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 5b에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S510에서, 전자 장치는 제1 센싱 데이터에 기초하여 제1 센싱 데이터를 획득한 적어도 하나의 센서 의 제1 위치를 추정할 수 있다. 예를 들어, 전자 장치는 단일의 이미지 또는 복수의 이미지들에 기초하여 적어도 하나의 센서의 위치를 결정할 수 있다. 전자 장치는 단일의 이미지 또는 복수의 이미지 들에서 특징점들을 추정할 수 있다. 전자 장치는 추정된 특징점들 및/또는 적어도 하나의 센서의 스 펙(예컨대, 초점 거리 등)을 이용하여 적어도 하나의 센서의 제1 위치를 추정할 수 있다. 단계 S520에서, 전자 장치는 추정된 제1 위치에 기초하여, 식별된 부분에 대응하는 제2 센싱 데이터를 획 득하기 위한 적어도 하나의 센서의 제2 위치를 추정할 수 있다. 예를 들어, 제2 센싱 데이터는 객체의 일부 영 역에 대응할 수 있다. 예를 들어, 제2 위치는 객체의 일부 영역을 촬영하기 위한 특정 위치(A1)에 대 응할 수 있다. 단계 S530에서, 전자 장치는 제1 위치 및 제2 위치를 비교하여 적어도 하나의 센서의 위치 조정 값을 획득할 수 있다. 예를 들어, 위치 조정 값은 제1 위치와 제2 위치 간의 거리 및 각도 차이 중 적어도 하나를 나 타낼 수 있다. 단계 S540에서, 전자 장치는 위치 조정 값이 제2 임계 값을 초과하는지 여부를 결정할 수 있다. 위치 조정 값이 제2 임계 값 이하인 경우(아니오), 절차는 종료된다. 위치 조정 값이 제2 임계 값을 초과한 경우(예), 절 차는 종료되지 않고 단계 S550으로 이동한다. 단계 S550에서, 전자 장치는 제2 위치에서 객체를 촬영할 것을 지시하는 재촬영 위치 가이드를 제공할 수 있다. 일 실시예에 있어서, 도 4b에서 설명된 S340의 세부 단계들과, 도 5b에서 설명된 S340의 세부 단계들은 전자 장 치에 의해 병렬적으로 수행될 수 있다. 그러나, 본 개시는 이에 한정되지 않으며, 도 4b에서 설명된 S340 의 세부 단계들과, 도 5b에서 설명된 S340의 세부 단계들 중 적어도 하나가 전자 장치에 의해 수행되거나, 직렬적으로 수행될 수 있다. 도 6a는 일 실시예에 따른 포인트 클라우드에서 물리 법칙을 위반하는 부분을 식별하는 방법을 보여주는 흐름도 이다. 도 1 내지 3에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3을 참조하 여, 도 6a를 설명한다. 도 6a를 참조하면, 도 3의 단계 S330은 단계 S610 내지 S630을 포함할 수 있다. 일 실시 예에 있어서, 단계 S610 내지 S630은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S330의 세부 단계들은 도 6a에 도시된 바에 한정되지 않으며, 도 6a에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 6a에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S610에서, 전자 장치는 제1 포인트 클라우드에 기초하여 객체에 대응하는 멀티 뷰 이미지들을 생성하 고, 멀티 뷰 이미지들 각각에 대응하는 카메라 정보를 획득할 수 있다. 예를 들어, 카메라 정보는 3차원 좌표계 에서의 카메라 위치 및 카메라의 초점 거리를 포함할 수 있다. 전자 장치는 멀티 뷰 이미지들을 생성하기 위해, 제1 포인트 클라우드에 기초하여 객체에 대응하는 메쉬를 생성하고, 메쉬를 2D 이미지들로 렌더링할 수 있다. 단계 S620에서, 전자 장치는 멀티 뷰 이미지들로 구성되는 동영상을 적어도 하나의 심층 신경망 모델(23 1)에 적용하여, 동영상에서 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론할 수 있다. 적어도 하나의 심층 신경망 모델은 적어도 하나의 기 정의된 물리 법칙을 위반하는 영상 시퀀스 및 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터로 이 용하여, 영상 시퀀스에서의 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하도 록 미리 학습된 모델일 수 있다. 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하지 않은 경우(아 니오), 절차는 종료된다. 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반한 경우(예), 절차는 종료 되지 않고 단계 S630으로 이동한다. 단계 S630에서, 전자 장치는 추론 결과에 기초하여 제1 포인트 클라우드에서 누락된 포인트들을 추정할 수 있다. 제1 포인트 클라우드에서 누락된 포인트들은 적어도 하나의 심층 신경망 모델의 출력에서 역산함으 로써 추정될 수 있다. 단계 S630의 세부 단계들은 도 7a에서 좀 더 상세하게 설명한다. 도 6b는 일 실시예에 따른 멀티 뷰 이미지들을 생성하고 카메라 정보를 획득하는 방법을 보여주는 개념도이다. 도 1 내지 3, 및 6a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 및 2b를 참조하 여, 도 6b를 설명한다. 도 6b를 참조하면, 전자 장치는 카메라 정보 세트(CCamera)에 대응하는 카메라들에 의해 생성된 멀티 뷰 이 미지들을 생성할 수 있다. 카메라 정보 세트(Ccamera)는 M 개의 카메라들 각각의 카메라 정보(C1~CM)를 포함할 수 있다. 여기서, M은 자연수일 수 있다. 예를 들어, 카메라 정보(Cm)는 3차원 좌표계에서의 카메라의 위치(xm, ym, zm)를 포함할 수 있다. 카메라 정보(Cm)는 카메라의 초점 거리(f)를 포함할 수 있다. 여기서, m은 특정 방향, 각 도, 위치를 갖는 카메라의 고유 값이며, M보다 같거나 작은 자연수일 수 있다. 설명의 편의를 위해 7 개의 시점 들에서 촬영된 것으로 해석되는 이미지들만을 도시하였으나, 시점들의 개수는 이에 한정되지 않는다. 전자 장치는 멀티 뷰 이미지들을 카메라의 연속적인 회전에 따른 동영상으로 구성할 수 있다. 따라서, 동 영상의 프레임들 각각은 카메라의 연속적이거나 및/또는 선형적인 회전에 따른 이미지일 수 있다. 도 7a는 일 실시예에 따른 포인트 클라우드에서 누락된 포인트들을 추정하는 방법을 보여주는 흐름도이다. 도 1 내지 3, 및 도 6a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3, 및 도 6a 을 참조하여, 도 7a를 설명한다. 도 7a를 참조하면, 도 3의 단계 S630은 단계 S710 내지 S730을 포함할 수 있다. 일 실시 예에 있어서, 단계 S710 내지 S730은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따 른 단계 S630의 세부 단계들은 도 7a에 도시된 바에 한정되지 않으며, 도 7a에 도시된 단계 중 어느 하나를 생 략할 수도 있고, 도 7a에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S710에서, 도 6a의 단계 S620 이후, 전자 장치는, 동영상에서 객체의 움직임이 적어도 하나의 기 정 의된 물리 법칙을 위반하는지 여부를 추론한 결과에 대응하는 히트 맵을 획득할 수 있다. 전자 장치는 동 영상의 프레임들 각각에 대응하는 히트 맵을 획득할 수 있다. 예를 들어, 전자 장치는 적어도 하나의 심층 신경망 모델의 출력으로부터 히트 맵을 추출할 레이어(예컨대, 마지막 레이어)를 결정할 수 있다. 전자 장 치는 사용자 또는 제조사의 설정에 따라 레이어를 결정할 수 있다. 전자 장치는 결정된 레이어에서 활성화 값을 추출할 수 있다. 전자 장치는 추출된 활성화 값에 기초하여 히트 맵을 생성할 수 있다. 예를 들어, 전자 장치는 활성화 값이 미리 정의된 임계 값을 초과하는 포인트들로 히트 맵을 구성할 수 있다. 단계 S720에서, 전자 장치는 히트 맵 및 카메라 정보에 기초하여, 객체의 형상 중 제1 포인트 클라우드에 서 누락된 포인트들을 추정할 수 있다. 일 실시예에 있어서, 전자 장치는 히트 맵 및 카메라 정보에 기초 하여 동영상의 프레임 상의 포인트들을 3차원 좌표계의 포인트들로 변환할 수 있다. 단계 S730에서, 전자 장치는 추정 결과의 신뢰도(confidence) 값이 제3 임계 값 미만인지 여부를 결정할 수 있다. 여기에서, 신뢰도 값은 변환된 3차원 좌표계의 포인트들이 히트 맵에 대응하는 정도를 나타낼 수 있다. 예를 들어, 동영상의 프레임들 중 어느 하나의 히트 맵에 기초하여 추정된 누락된 포인트들과 동영상의 프레임들 중 다른 하나의 히트 맵에 기초하여 추정된 누락된 포인트들이 동일하거나 유사한 것으로 판단되는 경 우, 해당 부분의 신뢰도 값은 높을 수 있다. 일 실시예에 있어서, 동일 또는 유사에 대한 판단은 미리 정의된 임계 값을 기준으로 수행될 수 있다. 신뢰도 값이 제3 임계 값 미만인 경우(예), 절차는 단계 S610으로 이동한다. 따라서, 신뢰도 값이 제3 임계 값 미만인 경우, 신뢰도 값이 낮은 것으로 판단된 영역에 대응되는 시점에서의 멀티 뷰 이미지들을 추가 생성할 수 있다. 신뢰도 값이 제3 임계 값 이상인 경우(아니오), 절차는 단계 S340으로 이동한다. 도 7b는 일 실시예에 따른 멀티 뷰 이미지들의 히트 맵들을 보여주는 개념도이다. 도 1 내지 3, 6b, 및 7a에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 및 2b 및 6b를 참조하여, 도 7b를 설명한 다. 도 7b를 참조하면, 전자 장치는 동영상의 프레임들(즉, 멀티 뷰 이미지들) 각각에 대응하는 히트 맵을 획 득할 수 있다. 히트 맵 세트(PHeatMap)는 동영상의 프레임들에 대응하는 히트 맵들(P1~PN)을 포함할 수 있다. N은동영상을 구성하는 프레임들의 개수일 수 있다. N은 자연수일 수 있다. 일 실시예에 있어서, N은 M과 동일할 수 있다. 히트 맵(Pn)은 동영상 프레임에서의 좌표 값들의 집합으로 구성될 수 있다. 카메라 정보(C1~C7)는 히트 맵 들(P1~P7)에 대응할 수 있다. 도 7b에 도시된 예시적인 동영상의 프레임들을 살펴보면, 동영상은 객체(즉, 사람)을 여러 시점에서 촬영한 이 미지들로 구성될 수 있다. 히트 맵들(P1~P7) 각각은 동영상의 프레임 내에 표현되는 객체의 형상 중 누락된 포인 트들의 집합일 수 있다. 따라서, 전자 장치는 적어도 하나의 심층 신경망 모델을 이용하여 동영상에 서(또는 동영상 내의 객체의 움직임에서) 기 정의된 물리 법칙을 위반한 부분이 있는 것으로 추론할 수 있다. 도 8a 내지 8d는 일 실시예에 따른 물리 법칙의 종류를 설명하기 위한 개념도이다. 도 8a를 참조하면, 제1 물리 법칙은 객체 지속성을 포함할 수 있다. 본 개시에서, 객체 지속성은 객체가 일정한 시간 동안 모양, 크기, 위치, 및 운동 상태 등을 유지하는 성질을 나타낼 수 있다. 객체 지속성은 시간의 흐름 에 따라 관찰되는 물체의 상태가 변하지 않음을 나타낼 수 있다. 제1 시나리오는 객체 지속성을 위반하지 않는 예시적인 시나리오를 나타낸다. 제1 시나리오의 첫 번 째 영상 시퀀스는, 판이 쓰러지면서 큐브 객체를 덮는 상황에 대응한다. 쓰러진 판 아래에 큐브 객체가 존재하 므로, 판이 바닥에 완전히 닿지 않는다. 제1 시나리오의 두번째 영상 시퀀스는, 판이 쓰러지나 큐브 객체 가 존재하지 않는 상황에 대응한다. 쓰러진 판 아래에 아무 것도 존재하지 않으므로, 판이 바닥에 완전히 닿는 다. 제2 시나리오는 객체 지속성을 위반하는 예시적인 시나리오를 나타낸다. 제2 시나리오의 첫 번째 영 상 시퀀스는, 판이 쓰러지면서 큐브 객체를 덮는 상황에 대응한다. 쓰러진 판 아래에 큐브 객체가 존재하므로, 판이 바닥에 완전히 닿지 않아야 하지만, 판이 바닥에 완전 닿는다. 제2 시나리오의 두번째 영상 시퀀스는, 판이 쓰러지나 큐브 객체가 존재하지 않는 상황에 대응한다. 쓰러진 판 아래에 아무 것도 존재하지 않으므로, 판이 바닥에 완전히 닿아야 하지만, 판이 바닥에 완전히 닿지 않는다. 도 8a와 함께, 도 2a 및 2b를 참조하면, 제1 시나리오는 객체 지속성을 위반하지 않는 학습 데이터 셋에 포함되는 영상 시퀀스의 일 예시일 수 있다. 제2 시나리오는 객체 지속성을 위반하는 학습 데이터 셋에 포 함되는 영상 시퀀스의 일 예시일 수 있다. 도 8b를 참조하면, 제2 물리 법칙은 견고성을 포함할 수 있다. 본 개시에서, 견고성은 단단한 물체들은 서로 통 과할 수 없고, 공간이 비어있는 경우 물체가 통과하는 성질을 나타낼 수 있다. 제3 시나리오는 견고성을 위반하지 않는 예시적인 시나리오를 나타낸다. 제3 시나리오의 첫 번째 영 상 시퀀스는, 긴 큐브 객체가 속이 비어있는 통에 들어가는 상황에 대응한다. 속이 비어있는 통으로 큐브 객체 가 통과하고, 큐브 객체는 통의 단단한 바닥에 닿게 되어 더 이상 통과하지 못한다. 제3 시나리오의 두번 째 영상 시퀀스는, 짧은 큐브 객체가 속이 비어있는 통에 들어가는 상황에 대응한다. 속이 비어있는 통으로 큐 브 객체가 통과하고, 큐브 객체는 통의 단단한 바닥에 닿게 된다. 그러나, 큐브 객체가 길이가 짧아 현재 각도 에서 비어있는 통 안에 들어간 큐브 객체의 모습은 보이지 않는다. 제4 시나리오는 견고성을 위반하는 예시적인 시나리오를 나타낸다. 제4 시나리오의 첫 번째 영상 시 퀀스는, 긴 큐브 객체가 속이 비어있는 통에 들어가는 상황에 대응한다. 속이 비어있는 통으로 큐브 객체가 통 과하고, 큐브 객체는 통의 단단한 바닥에 닿게 되어 더 이상 통과하지 못해야 하지만, 큐브 객체는 통의 단단한 바닥을 통과한다. 제4 시나리오의 두번째 영상 시퀀스는, 짧은 큐브 객체가 속이 비어있는 통에 들어가는 상황에 대응한다. 속이 비어있는 통으로 큐브 객체가 통과하고, 큐브 객체는 통의 단단한 바닥에 닿음으로써 현 재 각도에서 비어있는 통 안에 들어간 큐브 객체의 모습은 보이지 않아야 하지만, 큐브 객체는 비어있는 통을 충분히 통과하지 못한다. 도 8b와 함께, 도 2a 및 2b를 참조하면, 제3 시나리오는 견고성을 위반하지 않는 학습 데이터 셋에 포함되 는 영상 시퀀스의 일 예시일 수 있다. 제4 시나리오는 견고성을 위반하는 학습 데이터 셋에 포함되는 영상 시퀀스의 일 예시일 수 있다. 도 8c를 참조하면, 제3 물리 법칙은 불변성을 포함할 수 있다. 본 개시에서, 불변성은 객체의 형태가 변하지 않 는 성질을 나타낼 수 있다. 예를 들어, 형태는 색상, 모양, 및/또는 형상을 포함할 수 있다.제5 시나리오는 불변성을 위반하지 않는 예시적인 시나리오를 나타낸다. 제5 시나리오의 첫 번째 영 상 시퀀스는, 세 개의 큐브 객체들이 판으로 완전 가려졌다가 다시 보여지는 상황에 대응한다. 세 개의 큐브 객 체들이 판으로 완전히 가려지더라도, 다시 보여지는 세 개의 큐브 객체들의 형태는 동일하다. 제4 시나리오 의 두번째 영상 시퀀스는, 첫번째 영상 시퀀스 대비 세 개의 큐브 객체들의 배치 순서가 다른 상황에 대응 한다. 세 개의 큐브 객체들이 판으로 완전히 가려지더라도, 다시 보여지는 세 개의 큐브 객체들의 형태는 동일 하다. 제6 시나리오는 불변성을 위반하는 예시적인 시나리오를 나타낸다. 제6 시나리오의 첫 번째 영상 시 퀀스는, 세 개의 큐브 객체들이 판으로 완전 가려졌다가 다시 보여지는 상황에 대응한다. 세 개의 큐브 객체들 이 판으로 완전히 가려지더라도, 다시 보여지는 세 개의 큐브 객체들의 형태는 동일해야함에도, 판으로 완전히 가려지기 전의 큐브 객체들과 다시 보여지는 세 개의 큐브 객체들의 색상이 상이하다. 제6 시나리오의 두 번째 영상 시퀀스는, 첫번째 영상 시퀀스 대비 세 개의 큐브 객체들의 배치 순서가 다른 상황에 대응한다. 세 개의 큐브 객체들이 판으로 완전히 가려지더라도, 다시 보여지는 세 개의 큐브 객체들의 형태는 동일해야함에도, 판으로 완전히 가려지기 전의 큐브 객체들과 다시 보여지는 세 개의 큐브 객체들의 색상이 상 이하다. 도 8c와 함께, 도 2a 및 2b를 참조하면, 제5 시나리오는 불변성을 위반하지 않는 학습 데이터 셋에 포함되 는 영상 시퀀스의 일 예시일 수 있다. 제6 시나리오는 불변성을 위반하는 학습 데이터 셋에 포함되는 영상 시퀀스의 일 예시일 수 있다. 도 8d를 참조하면, 제4 물리 법칙은 방향 관성을 포함할 수 있다. 본 개시에서, 방향 관성은 움직이는 객체가 관성의 방향성을 가진 채 적절하게 운동하는 성질을 나타낼 수 있다. 제7 시나리오는 방향 관성을 위반하지 않는 예시적인 시나리오를 나타낸다. 제7 시나리오의 첫 번째 영상 시퀀스는, 구형 객체가 특정 방향으로 운동하다가 고정된 큐브 객체에 충돌하여 방향을 전환하여 운동 방 향을 유지하는 상황에 대응한다. 구형 객체가 고정된 큐브 객체에 충돌한 각도에 비추어 볼 때, 충돌 후 구형 객체의 운동 방향은 적절하다. 제7 시나리오의 두번째 영상 시퀀스는, 첫번째 영상 시퀀스 대비 충돌 전후 의 운동 방향이 서로 반대인 상황에 대응한다. 구형 객체가 고정된 큐브 객체에 충돌한 각도에 비추어 볼 때, 충돌 후 구형 객체의 운동 방향은 적절하다. 제8 시나리오는 방향 관성을 위반하는 예시적인 시나리오를 나타낸다. 제8 시나리오의 첫 번째 영상 시퀀스는, 구형 객체가 특정 방향으로 운동하다가 고정된 큐브 객체에 충돌하여 방향을 전환하여 운동 방향을 유지하는 상황에 대응한다. 구형 객체가 고정된 큐브 객체에 충돌한 각도에 비추어 볼 때, 충돌 후 구형 객체의 운동 방향은 부적절하다. 제8 시나리오의 두번째 영상 시퀀스는, 첫번째 영상 시퀀스 대비 충돌 전후의 운 동 방향이 서로 반대인 상황에 대응한다. 구형 객체가 고정된 큐브 객체에 충돌한 각도에 비추어 볼 때, 충돌 후 구형 객체의 운동 방향은 부적절하다. 도 8d와 함께, 도 2a 및 2b를 참조하면, 제7 시나리오는 방향 관성을 위반하지 않는 학습 데이터 셋에 포 함되는 영상 시퀀스의 일 예시일 수 있다. 제8 시나리오는 방향 관성을 위반하는 학습 데이터 셋에 포함되 는 영상 시퀀스의 일 예시일 수 있다. 도 9는 일 실시예에 따른 심층 신경망의 구조를 보여주는 개념도이다. 심층 신경망 모델의 구성, 동작, 및 기능은 도 2a 및 2b의 적어도 하나의 심층 신경망 모델의 구성, 동작, 및 기능에 대응할 수 있다. 설명의 편의를 위해, 도 1 내지 8에서 설명한 내용과 중복되는 내용은 생략한다. 일 실시예에 있어서, 심층 신경망 모델은 지각(perception) 모듈 및 역학(dynamics) 모듈을 포 함할 수 있다. 지각 모듈은 제1 동작, 제2 동작, 및 제3 동작을 수행할 수 있다. 역학 모 듈은 제4 동작을 수행할 수 있다. 제1 동작은 입력 데이터의 전처리 동작일 수 있다. 제1 동작을 살펴보면, 지각 모듈은 이미지 (x)를 수신할 수 있다. 예를 들어, 이미지(x)는 동영상 중 하나의 프레임일 수 있다. 예를 들어, 이미지(x)는 도6a 내지 7b에서 설명한 멀티 뷰 이미지들 중 하나일 수 있다. 일 실시예에 있어서, 지각 모듈은 이미지 (x)에 기초하여 객체들 각각에 대응하는 세그멘테이션 마스크(segmentation mask)()를 생성할 수 있다. 지각 모듈은 이미지(x) 및 세그멘테이션 마스크()에 대한 성분별 곱(elementwise product)을 수행함으 로써, 객체들 각각의 가시 부분만을 포함하는 이미지 세트()를 출력할 수 있다. 제2 동작은 인코더 모듈() 및 디코더 모듈()의 학습 동작일 수 있다. 제2 동작을 살펴보면, 지각 모듈은 이미지 세트()와 세그멘테이션 마스크() 중 원본 한 쌍(예컨대, , )을, 인코더 모듈()을 이용하여, 객체 코드()로 인코딩할 수 있다. 지각 모듈은 객체 코드()를, 디코더 모듈 ()을 이용하여, 재구성된 한 쌍(, )으로 디코딩할 수 있다. 원본 한 쌍(예컨대, , )과 재구성 된 한 쌍(, ) 간 불일치(discrepancy) 정도는, 객체 코드()가 이미지-마스크 한 쌍의 유용한 정보를 나타낼 수 있도록, 인코더 모듈()의 파라미터들과 디코더 모듈()의 파라미터들을 학습하는 데 이용될 수 있 다. 일 실시예에 있어서, 인코더 모듈() 및 디코더 모듈()은 오토인코더(auto-encoder) 구조를 가질 수 있 다. 제3 동작은 학습된 심층 신경망 모델을 이용하여 객체 코드들을 출력하는 동작일 수 있다. 제3 동작 을 살펴보면, 지각 모듈은 원본 쌍들(, )을, 학습된 인코더 모듈들()을 이용하여, 객체 코드들()로 인코딩할 수 있다. 일 실시예에 있어서, 지각 모듈은 객체 코드들()를, 학습된 디코 더 모듈들()을 이용하여, 재구성된 쌍들(, )로 디코딩할 수 있다. 일 실시예에 있어서, 객체 코드들 중 하나는 동영상 중 하나의 프레임에서의 하나의 객체에 대응할 수 있다. 제4 동작은 학습된 심층 신경망 모델을 이용하여 동영상의 다음 프레임의 객체 코드를 예측(또는 추 론)하는 동작일 수 있다. 예를 들어, 동영상은 도6a 내지 7b에서 설명한 멀티 뷰 이미지들로 구성되는 동영상일 수 있다. 역학 모듈은 제t 시점 프레임의 원본 쌍들(, ) 에 대응하는 제t 시점 프레임의 객체 코드들()을 획득할 수 있다. 객체 버퍼에는, 제1 시점 내지 제t-1 시점 프레임들의 객체 코드들이 저장되 어 있을 수 있다. 역학 모듈은 객체 버퍼에 저장된 제1 시점 내지 제t-1 시점 프레임들의 객체 코드들과, 제t 시점 프레임들의 객체 코드들()에 기초하여 제t+1 시점 프레임의 객체 코드들을 예측(또는 추론)할 수 있다. 일 실시예에 있어서, 역학 모듈은 이전 시점의 프레임들과 현재 프레임의 객체 코드들에 기초하여 다음 프레임의 객체 코드를 추론하도록 학습될 수 있다. 일 실시예에 있어서, 역학 모듈은 객체 메모리 및 인터랙션 네트워크를 포함할 수 있다. 예를 들어, 객체 메모리는 LSTM(long short-term memroy)을 포함할 수 있다. 예를 들어, 객체 메모리는 객체마다 대응되는 슬롯들을 포함할 수 있다. 객체 메모리에는 예측된 객체 코드들이 저장될 수 있다. 인 터랙션 네트워크는 제1 시점 내지 제t-1 시점 프레임들의 객체 코드들, 제t 시점 프레임들의 객체 코드들 (), 객체 메모리에 저장된 객체 코드들(즉, 직전에 예측된 객체 코드들) 간의 인터랙션을 계산할 수 있다. 역학 모듈은 계산된 인터랙션에 기초하여 제t+1 시점 프레임의 객체 코드들을 예측(또는 추론)할 수 있다. 일 실시예에 있어서, 전자 장치 또는 외부 전자 장치(미도시)는 사용자 입력 및/또는 제조사의 설정에 기 초하여 복수의 물리 법칙들 중 하나를 선택할 수 있다. 전자 장치 또는 외부 전자 장치(미도시)는 선택된 물리 법칙을 위반하는 영상 시퀀스 및/또는 선택된 물리 법칙을 위반하지 않는 영상 시퀀스를 심층 신경망 모델 에 적용하여, 영상 시퀀스의 연속되는 프레임들 각각에 대응하는 객체 코드들을 획득하도록 학습시킬 수 있다. 전자 장치 또는 외부 전자 장치(미도시)는 객체 코드들을 심층 신경망 모델에 적용하여, 연속되는 프레임 들의 다음 프레임에 대응하는 객체 코드들을 예측하도록 학습시킬 수 있다. 일 실시예에 있어서, 전자 장치 또는 외부 전자 장치(미도시)는 예측된 객체 코드들과 연속되는 프레임들 의 다음 프레임에 대응하는 실제 객체 코드들을 비교함으로써, 물리 법칙 위반 여부를 추론할 수 있다. 도 10은 일 실시예에 따른 심층 신경망의 동작을 보여주는 흐름도이다. 도 1 내지 3, 및 도 9에서 설명한 내용 과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3, 및 도 9를 참조하여, 도 10을 설명한다. 도 10을 참조하면, 도 6의 단계 S620은 단계 S1010 내지 S1040b를 포함할 수 있다. 일 실시 예에 있어서, 단계 S1010 내지 S1040b는 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따른 단계 S620의 세부 단계들은 도 10에 도시된 바에 한정되지 않으며, 도 10에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 10에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S1010에서, 전자 장치는 동영상 중 제1 프레임에서 객체를 지각(perceiving)할 수 있다. 본 개시에서, 객체(또는 물체) 지각은 동영상 내의 객체에 대응하는 움직임을 지각하는 동작을 의미할 수 있다. 단 계 S1010에서 수행되는 전자 장치의 동작은 지각 모듈의 동작에 대응될 수 있다. 예를 들어, 전자 장 치는 제1 프레임에 기초하여 세그멘테이션 마스크를 생성할 수 있다. 전자 장치는 제1 프레임 및 제1 프레임에 대응하는 세그멘테이션 마스크에 기초하여 제1 프레임의 객체를 지각할 수 있다. 예를 들어, 지각 결 과는 도 9의 제t 시점의 객체 코드를 나타낼 수 있다. 일 실시예에 있어서, 전자 장치는 제1 프레임과 함 께, 제1 프레임의 적어도 하나의 이전 프레임에 기초하여 객체를 지각할 수 있다. 단계 S1020에서, 전자 장치는 지각 결과에 기초하여 제1 프레임의 다음 프레임인 제2 프레임의 예측 값을 획득(또는 추론)할 수 있다. 제2 프레임의 예측 값을 획득하는 과정에서, 전자 장치는 제2 프레임의 이전 프레임들 중 적어도 하나를 이용할 수 있다. 단계 S1020에서 수행되는 전자 장치의 동작은 역학 모듈(92 0)의 동작에 대응될 수 있다. 예를 들어, 제2 프레임의 예측 값은 도 9의 제t+1 시점의 객체 코드들을 예측한 데이터를 나타낼 수 있다. 단계 S1030에서, 전자 장치는 제2 프레임의 예측 값과 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는지 여부를 결정할 수 있다. 예를 들어, 제2 프레임의 실제 값은 도 9의 제t+1 시점의 실제 객체 코드들 을 나타낼 수 있다. 예를 들어, 제2 프레임의 실제 값은 영상 시퀀스의 제t+1 시점의 프레임을 학습된 인코더 모듈()에 입력하여 획득된 객체 코드들을 나타낼 수 있다. 일 실시예에 있어서, 제2 프레임의 예측 값과 제2 프레임의 실제 값 간의 오차는, 예측된 제2 프레임의 객체 코드들에 대응하는 값들과 실제 제2 프레임의 객체 코드들에 대응하는 값들의 차이로부터 계산될 수 있다. 제2 프레임의 예측 값과 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는 경우(예), 절차는 단계 S1040a로 이동한다. 제2 프레임의 예측 값과 제2 프레임의 실제 값 간의 오차가 제4 임계 값 이하인 경우(아니 오), 절차는 단계 S1040b로 이동한다. 단계 S1040a에서, 전자 장치는 동영상에서 객체의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반한 것으로 결정할 수 있으며, 절차는 단계 S340로 이동한다. 단계 S1040b에서, 전자 장치는 동영상에서 객체 의 움직임이 적어도 하나의 기 정의된 물리 법칙을 위반하지 않은 것으로 결정할 수 있으며, 절차는 종료된다. 도 11은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 흐름도이다. 도 1 내지 3에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3을 참조하여, 도 11을 설명한다. 도 11을 참조하면, 포인트 클라우드를 생성하는 방법은, 단계 S340 이후에 단계 S1110 및 S1120을 포함할 수 있 다. 일 실시 예에 있어서, 단계 S1110 및 S1120은 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따른 포인트 클라우드를 생성하는 방법은 도 11에 도시된 바에 한정되지 않으며, 도 11에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 11에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S1110에서, 전자 장치는 재촬영 위치 가이드를 제공한 후, 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획득할 수 있다. 일 실시예에 있어서, 전자 장치의 사용자는 재촬영 위치 가이드에 대응하는 지 시에 따라, 객체를 더 가까이서 촬영하거나, 제1 센싱 데이터를 획득한 방향과 다른 방향에서 촬영할 수 있다. 제2 센싱 데이터는 사용자가 추가로 촬영한 이미지에 대응할 수 있다. 단계 S1120에서, 전자 장치는 제1 센싱 데이터 및 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획 득할 수 있다. 일 실시예에 있어서, 전자 장치는 제1 포인트 클라우드 및 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득할 수 있다. 제2 포인트 클라우드는 객체를 표현하는 완전한 포인트 클라우드일 수있다. 도 12는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 1 내지 3, 및 도 11 에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 내지 3, 및 도 11를 참조하여, 도 12를 설명한다. 도 12를 참조하면, 도 3의 단계 S340은 단계 S1210 내지 S1230을 포함할 수 있다. 일 실시 예에 있어서, 단계 S1210 내지 S1230는 전자 장치 또는 전자 장치의 프로세서에 의해 수행될 수 있다. 본 개시에 따른 단계 S340의 세부 단계들은 도 12에 도시된 바에 한정되지 않으며, 도 12에 도시된 단계 중 어느 하나를 생략할 수도 있고, 도 12에 도시되지 않은 단계를 더 포함할 수도 있다. 단계 S1210에서, 전자 장치는 재촬영이 필요한 카메라 위치를 제1 형태로 시각화한 재촬영 위치 가이드를 전자 장치의 디스플레이에 표시할 수 있다. 전자 장치의 사용자는 재촬영 위치 가이드에 따라, 재촬영이 필요한 카메라 위치에서 객체를 촬영할 수 있다. 단계 S1110에서, 전자 장치는 재촬영 위치 가이 드에 대응하는 제2 센싱 데이터를 획득할 수 있다. 단계 S1220에서, 재촬영 위치 가이드에 대응하는 제2 센싱 데이터를 획득한 후, 전자 장치는 제2 센싱 데 이터가 재촬영이 필요한 카메라 위치에서 촬영된 것인지를 결정할 수 있다. 단계 S1230에서, 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정한 것에 기초하여, 전자 장치는 제1 형태를 재촬영이 불필요한 카메라 위치를 나타내는 제2 형태로 변경하여 재촬영 위치 가이드를 전자 장치 의 디스플레이에 표시할 수 있다. 예를 들어, 제2 형태는, 제1 형태의 형상, 모양, 및 색채 중 적어도 하 나가 변경된 형태일 수 있다. 단계 S1120에서, 전자 장치는 제1 센싱 데이터 및 제2 센싱 데이터에 기초하 여 제2 포인트 클라우드를 획득할 수 있다. 도 13a 내지 13b는 도 12의 재촬영 위치 가이드를 제공하는 방법을 예시적으로 보여주는 개념도이다. 도 1 내지 3, 11, 및 12에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a, 2b, 및 12를 참조하여, 도 6b를 설명한다. 도 13a를 참조하면, 전자 장치는 재촬영 위치 가이드를 디스플레이에 표시할 수 있다. 재촬영 위치 가이드는 촬영 목표인 객체에 대응하는 이미지를 표시할 수 있다. 재촬영 위치 가이드는 3차 원 좌표계에서 카메라 위치를 표시하는 복수의 컴포넌트들을 포함할 수 있다. 복수의 컴포넌트들은 객체에 대응 하는 이미지들을 둘러싸는 형태(예컨대, 반구 형태)로 표시될 수 있다. 복수의 컴포넌트들 중 재촬영이 필요한 적어도 하나의 컴포넌트는 제1 형태로 시각화될 수 있다. 예를 들어, 제1 형태는 제1 색상으로 표 현될 수 있다. 도 13b를 참조하면, 전자 장치의 사용자는 제1 형태에 대응하는 카메라 위치에서 객체를 촬영할 수 있다. 전자 장치는 적어도 하나의 센서로부터 제2 센싱 데이터를 획득할 수 있다. 전자 장치는 제2 센싱 데이터가 재촬영이 필요한 카메라 위치에서 촬영된 것인지 결정할 수 있다. 도 13c를 참조하면, 전자 장치는 재촬영 위치 가이드를 디스플레이에 표시할 수 있다. 제2 센 싱 데이터가 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정된 경우, 전자 장치는 제1 형태로 시각화된 컴포넌트는 제2 형태로 변경하여 재촬영 위치 가이드를 제공할 수 있다. 예를 들어, 제2 형태는 제1 색상과 다른 제2 색상으로 표현될 수 있다. 도 14는 일 실시예에 따른 가상 공간에서 가구 배치를 변경하기 위한 사용자 인터페이스를 보여주는 개념도이다. 도 1 내지 13c에서 설명한 내용과 중복되는 내용은 생략한다. 설명의 편의를 위해, 도 2a 및 2b를 참조하여 도 14를 설명한다. 도 14를 참조하면, 전자 장치는 가상 공간을 디스플레이에 표시할 수 있다. 예를 들어, 가상 공간은 전자 장치에 설치된 가구 배치 애플리케이션을 통해 제공될 수 있다. 예를 들어, 가상 공간 은 증강 현실, 가상 현실, 메타버스 공간 등으로 표현될 수 있다.전자 장치는 현실 공간의 객체(예컨대, 의자)와 동일한 객체 이미지를 가상 공간 상에 배치하기 위 해 적어도 하나의 센서(예컨대, 카메라)를 이용하여 객체의 이미지를 획득할 수 있다. 예를 들어, 전자 장치 는 객체에 대응하는 포인트 클라우드를 획득하고, 포인트 클라우드를 신경망 모델에 적용하여 물리 법칙에 위반되는 부분이 있는지를 식별할 수 있다. 전자 장치는, 포인트 클라우드에 기 정의된 물리 법칙의 위반 을 나타내는 이상치 포인트들이 있는 경우, 사용자에게 객체를 다시 촬영하도록 재촬영 위치 가이드를 제공할 수 있다. 포인트 클라우드에 기 정의된 물리 법칙의 위반을 나타내는 이상치 포인트들이 없는 경우, 전자 장치 는 해당 객체를 3D 모델링한 이미지를 가상 공간 상에 배치할 수 있다. 전자 장치는 가상 공간 내의 객체(예컨대, 의자)를 선택하는 사용자 입력을 수신할 수 있다. 전자 장치는 사용자 입력에 기초하여, 객체에 대응하는 3차원 바운딩 박스를 디스플레이 에 표시할 수 있다. 일 실시예에 있어서, 전자 장치는 3차원 바운딩 박스를 가상 공간의 다른 위치로 이동시키는 사용자 입력을 수신할 수 있다. 전자 장치는 객체가 3차원 바운딩 박스와 함께, 가상 공간의 다른 위치로 이동되는 애니메이션을 디스플레이에 표시할 수 있다. 일 실시예에 있어서, 전자 장치는 사용자 입력에 기초하여, 선택된 객체와 동일한 종류이되 다른 형 태를 갖는 객체들을 포함하는 리스트를 포함하는 윈도우를 디스플레이에 표시할 수 있다. 일 실시예에 있어서, 리스트에 포함된 객체들 중 적어도 하나는 도 1 내지 13c에서 설명한 제1 포인트 클라 우드 또는 제2 포인트 클라우드에 대응되는 객체일 수 있다. 일 실시예에 있어서, 윈도우는 가상 공간이 표시되는 윈도우와 중첩될 수 있다. 예를 들어, 가상 공간이 표시되는 윈도우가 축소되고, 축소되어 남은 공간에 윈도우가 표시될 수 있다. 예를 들어, 가상 공간이 표시되는 윈도우가 백그라운드로 이동되고, 디스플레이 전체에 윈도우가 표시될 수 있다. 일 실시예에 있어서, 전자 장치는 리스트의 어느 객체를 선택하는 사용자 입력을 수신할 수 있다. 가상 공간 내의 사용자 입력에 대응하는 객체는 리스트에서 선택된 객체로 변경되어 표시될 수 있다. 본 개시는, 포인트 클라우드 생성 방법에 있어서, 심층 신경망 모델을 이용하여 물리 법칙 위반 여부를 추론하 고, 추론 결과에 기초하여 누락된 포인트들을 추정함으로써 재촬영 위치 가이드를 제공하는, 포인트 클라우드 생성 방법을 제시하고자 한다. 본 개시에서 이루고자 하는 기술적 과제는, 이상에서 언급한 것으로 제한되지 않으며, 언급되지 않은 또 다른"}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해 될 수 있을 것이다. 일 실시예에 있어서, 전자 장치가 포인트 클라우드(point cloud)를 생성하는 방법이 제공될 수 있다. 상기 방법 은, 상기 전자 장치의 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득하 는 단계를 포함할 수 있다. 상기 방법은, 적어도 하나의 심층 신경망 모델을 이용하여, 상기 제1 포인트 클라우 드에서 이상치 포인트들을 식별하는 단계를 포함할 수 있다. 상기 이상치 포인트들은, 적어도 하나의 기 정의된 물리 법칙의 위반을 나타낼 수 있다. 상기 방법은, 상기 이상치 포인트들에 기초하여, 상기 객체에 대한 재촬영 위치 가이드를 제공하는 단계를 포함할 수 있다. 일 실시예에 따르면, 물리 법칙의 위반을 나타내는 이상치 포 인트들을 식별함으로써 포인트 클라우드에서 누락된 포인트들을 빠르고 정확하게 추정할 수 있다. 일 실시예에 따르면, 포인트 클라우드에서 누락된 포인트들을 가이드함으로써, 추가적인 센싱 데이터를 효과적으로 얻을 수 있다. 일 실시예에 있어서, 상기 방법은, 상기 재촬영 위치 가이드를 제공한 후, 상기 적어도 하나의 센서로부터, 상 기 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획득하는 단계를 포함할 수 있다. 상기 방법은, 상기 제1 센싱 데이터 및 상기 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득하는 단계를 포함할 수 있다. 일 실시예에 따르면, 일 실시예에 따르면, 재촬영 위치 가이드에 따라 획득된 추가적인 센싱 데이터를 이용하여, 완전한 포인트 클라우드를 효과적으로 생성할 수 있다.일 실시예에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는, 재촬영이 필요한 카메라 위치를 제1 형태로 시각화한 상기 재촬영 위치 가이드를 상기 전자 장치의 디스플레이에 표시하는 단계를 포함할 수 있다. 상기 재 촬영 위치 가이드를 제공하는 단계는, 상기 제2 센싱 데이터를 획득한 후, 상기 제2 센싱 데이터가 상기 재촬영 이 필요한 카메라 위치에서 촬영된 것인지 결정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하 는 단계는, 상기 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정한 것에 기초하여, 상기 제1 형태를 재촬 영이 불필요한 카메라 위치를 나타내는 제2 형태로 변경하여 상기 재촬영 위치 가이드를 상기 전자 장치의 디스 플레이에 표시하는 단계를 포함할 수 있다. 일 실시예에 따르면, 재촬영 위치 가이드를 직관적인 UI/UX로 사용 자에게 제공됨으로써 추가 센싱 데이터를 효과적으로 획득할 수 있다. 일 실시예에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 위치를 추정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 추정된 위치에 기초하여, 상기 객체와 상기 적어도 하나의 센서 사이의 거리를 추정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 추정된 거리, 상기 적어도 하나의 센서의 화각, 및 상기 제1 센싱 데이터 중 적어도 하나에 기초하여, 상기 객체에 대응하는 픽셀 해상도 를 획득하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 픽셀 해상도가 제1 임 계 값 미만인지 여부를 결정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 픽 셀 해상도가 상기 제1 임계 값 미만인 경우, 상기 추정된 거리보다 더 가까운 거리에서 상기 객체를 촬영할 것 을 지시하는 상기 재촬영 위치 가이드를 제공하는 단계를 포함할 수 있다. 일 실시예에 따르면, 픽셀 해상도를 빠르고 정확하게 파악할 수 있다. 일 실시예에 있어서, 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 제1 위치를 추정하는 단계를 포함할 수 있다. 상기 재촬영 위 치 가이드를 제공하는 단계는, 상기 추정된 제1 위치에 기초하여, 상기 식별된 부분에 대응하는 제2 센싱 데이 터를 획득하기 위한 상기 적어도 하나의 센서의 제2 위치를 추정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 제1 위치 및 상기 제2 위치를 비교하여 상기 적어도 하나의 센서의 위치 조정 값을 획득하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 위치 조정 값이 제2 임계 값을 초과하는지 여부를 결정하는 단계를 포함할 수 있다. 상기 재촬영 위치 가이드를 제공하는 단계는, 상기 위치 조정 값이 제2 임계 값을 초과하는 경우, 상기 제2 위치에서 상기 객체를 촬영할 것을 지시하는 상기 재촬영 위치 가이드를 제공하는 단계를 포함할 수 있다. 일 실시예에 따르면, 추가 촬영이 필요한 카메라 위치 를 빠르고 정확하게 파악할 수 있다. 일 실시예에 있어서, 상기 제1 포인트 클라우드에서 이상치 포인트들을 식별하는 단계는, 상기 제1 포인트 클라 우드에 기초하여 상기 객체에 대응하는 멀티 뷰 이미지들을 획득하고, 상기 멀티 뷰 이미지들 각각에 대응하는 카메라 정보를 획득하는 단계를 포함할 수 있다. 상기 제1 포인트 클라우드에서 이상치 포인트들을 식별하는 단 계는, 상기 멀티 뷰 이미지들로 구성되는 동영상을 상기 적어도 하나의 심층 신경망 모델에 적용하여, 상기 동 영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하는 단계를 포함할 수 있다. 상기 제1 포인트 클라우드에서 이상치 포인트들을 식별하는 단계는, 상기 추론 결과에 기초하 여, 상기 객체의 형상 중에서 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계를 포함할 수 있 다. 일 실시예에 따르면, 포인트 클라우드로부터 동영상을 생성함으로써, 일관적인 조건 하에서, 심층 신경망 모델이 물리 법칙을 위반하였는지를 판단할 수 있다. 일 실시예에 있어서, 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계는, 상기 동영상에서 상기 추론 결과에 대응하는 히트 맵을 획득하는 단계를 포함할 수 있다. 상기 제1 포인트 클라우드에서 누락된 포인 트들을 추정하는 단계는, 상기 히트 맵 및 상기 카메라 정보에 기초하여, 상기 객체의 형상 중 상기 제1 포인트 클라우드에서 상기 누락된 포인트들을 추정하는 단계를 포함할 수 있다. 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계는, 상기 추정 결과의 신뢰도 값이 제3 임계 값 미만인지 여부를 결정하는 단계를 포 함할 수 있다. 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정하는 단계는, 상기 신뢰도 값이 제3 임계 값 미만인 경우, 상기 멀티 뷰 이미지들을 추가 획득하는 단계를 포함할 수 있다. 일 실시예에 따르면, 심층 신 경망 모델을 이용한 추론의 신뢰성을 높일 수 있다. 일 실시예에 있어서, 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하 는지 여부를 추론하는 단계는, 상기 동영상 중 제1 프레임에서 객체를 지각하는 단계를 포함할 수 있다. 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하는 단계 는, 상기 지각 결과에 기초하여 상기 제1 프레임의 다음 프레임인 제2 프레임의 예측 값을 획득하는 단계를 포함할 수 있다. 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론하는 단계는, 상기 제2 프레임의 예측 값과 상기 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는지 여부를 결정하는 단계를 포함할 수 있다. 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나 의 기 정의된 물리 법칙을 위반하는지 여부를 추론하는 단계는, 상기 오차가 상기 제4 임계 값보다 크면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 것으로 결정하고, 상기 오차가 상기 제4 임계 값보다 작거나 같으면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 것으로 결정하는 단계를 포함할 수 있다. 일 실시예에 따르면, 물리 법칙 위반 여부에 대한 추론의 신뢰성을 높일 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 기 정의된 물리 법칙은, 객체 지속성(object persistence), 견고성 (solidity), 불변성(unchangeableness), 및 방향 관성(directional inertia) 중 적어도 하나를 포함할 수 있으 나, 본 개시는 이에 한정되지 않는다. 일 실시예에 있어서, 적어도 하나의 심층 신경망 모델이 위반 여부를 추론하는 물리 법칙은, 여러 물리 법칙들 중 사용자 또는 제조사의 설정에 따라 미리 정의될 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 심층 신경망 모델은, 상기 적어도 하나의 기 정의된 물리 법칙을 위반 하는 영상 시퀀스 및 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터로 이용하여, 상기 영상 시퀀스에서의 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법 칙을 위반하는지 여부를 추론하도록 학습될 수 있다. 일 실시예에 있어서, 상기 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체가 제공될 수 있다. 일 실시예에 있어서, 전자 장치가 포인트 클라우드를 생성하는 전자 장치가 제공될 수 있다. 상기 전자 장치는, 적어도 하나의 센서를 포함할 수 있다. 상기 전자 장치는, 하나 이상의 인스트럭션을 저장하는 메모리를 포함할 수 있다. 상기 전자 장치는, 상기 메모리에 저장된 상기 하나 이상의 인스트럭션을 실행하는 적어도 하나의 프 로세서를 포함할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 적어도 하나의 센서로부터, 객체에 대응하는 제1 센싱 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서 는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 센싱 데이터에 기초하여 상기 객체에 대응하는 제1 포인트 클라우드를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으 로써, 적어도 하나의 심층 신경망 모델을 이용하여, 상기 제1 포인트 클라우드에서 이상치 포인트들을 식별할 수 있다. 상기 이상치 포인트들은, 적어도 하나의 기 정의된 물리 법칙의 위반을 나타낼 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 이상치 포인트들에 기초하여, 상기 객 체에 대한 재촬영 위치 가이드를 제공할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 재촬 영 위치 가이드를 제공한 후, 상기 적어도 하나의 센서로부터, 상기 객체의 적어도 일부에 대응하는 제2 센싱 데이터를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 센싱 데이터 및 상기 제2 센싱 데이터에 기초하여 제2 포인트 클라우드를 획득할 수 있다. 일 실시예에 있어서, 상기 전자 장치는, 디스플레이를 포함할 수 있다. 상기 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 재촬영이 필요한 카메라 위치를 제1 형태로 시각화한 상기 재촬 영 위치 가이드를 상기 디스플레이에 표시할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스 트럭션을 실행함으로써, 상기 제2 센싱 데이터를 획득한 후, 상기 제2 센싱 데이터가 상기 재촬영이 필요한 카 메라 위치에서 촬영된 것인지 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 재촬영이 필요한 카메라 위치에서 촬영된 것으로 결정한 것에 기초하여, 상기 제1 형태를 재촬영이 불필요한 카메라 위치를 나타내는 제2 형태로 변경하여 상기 재촬영 위치 가이드를 상기 디스플레이에 표시할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 위치를 추정할 수 있다. 상 기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 추정된 위치에 기초하여, 상 기 객체와 상기 적어도 하나의 센서 사이의 거리를 추정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 추정된 거리, 상기 적어도 하나의 센서의 화각, 및 상기 제1 센싱 데이터 중 적어도 하나에 기초하여, 상기 객체에 대응하는 픽셀 해상도를 획득할 수 있다. 상기 적어도 하나의 프 로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 픽셀 해상도가 제1 임계 값 미만인지 여부를 결 정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 픽셀 해상 도가 상기 제1 임계 값 미만인 경우, 상기 추정된 거리보다 더 가까운 거리에서 상기 객체를 촬영할 것을 지시 하는 상기 재촬영 위치 가이드를 제공할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 센싱 데이터에 기초하여 상기 제1 센싱 데이터를 획득한 상기 적어도 하나의 센서의 제1 위치를 추정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 추정된 제1 위치에 기초하여, 상기 식별된 부분에 대응하는 제2 센싱 데이터를 획득하기 위한 상기 적어도 하나의 센서의 제2 위치 를 추정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 위치 및 상기 제2 위치를 비교하여 상기 적어도 하나의 센서의 위치 조정 값을 획득할 수 있다. 상기 적어도 하 나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 위치 조정 값이 제2 임계 값을 초과하는 지 여부를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상 기 위치 조정 값이 제2 임계 값을 초과하는 경우, 상기 제2 위치에서 상기 객체를 촬영할 것을 지시하는 상기 재촬영 위치 가이드를 제공할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제1 포인트 클라우드에 기초하여 상기 객체에 대응하는 멀티 뷰 이미지들을 획득하고, 상기 멀티 뷰 이미지들 각각 에 대응하는 카메라 정보를 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 멀티 뷰 이미지들로 구성되는 동영상을 상기 적어도 하나의 심층 신경망 모델에 적용하여, 상기 동영상에서 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는지 여부를 추론할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 추론 결과에 기 초하여, 상기 객체의 형상 중에서 상기 제1 포인트 클라우드에서 누락된 포인트들을 추정할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 동영 상에서 상기 추론 결과에 대응하는 히트 맵을 획득할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상 의 인스트럭션을 실행함으로써, 상기 히트 맵 및 상기 카메라 정보에 기초하여, 상기 객체의 형상 중 상기 제1 포인트 클라우드에서 상기 누락된 포인트들을 추정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상 의 인스트럭션을 실행함으로써, 상기 추정 결과의 신뢰도 값이 제3 임계 값 미만인지 여부를 결정할 수 있다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 신뢰도 값이 제3 임계 값 미만인 경우, 상기 멀티 뷰 이미지들을 추가 획득할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 동영 상 중 제1 프레임에서 객체를 지각하고, 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행 함으로써, 상기 지각 결과에 기초하여 상기 제1 프레임의 다음 프레임인 제2 프레임의 예측 값을 획득할 수 있 다. 상기 적어도 하나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 제2 프레임의 예측 값 과 상기 제2 프레임의 실제 값 간의 오차가 제4 임계 값을 초과하는지 여부를 결정할 수 있다. 상기 적어도 하 나의 프로세서는, 상기 하나 이상의 인스트럭션을 실행함으로써, 상기 오차가 상기 제4 임계 값보다 크면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하는 것으로 결정하고, 상기 오차가 상기 제4 임계 값보다 작거나 같으면 상기 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 것으로 결정할 수 있다. 일 실시예에 있어서, 상기 적어도 하나의 심층 신경망 모델은, 상기 적어도 하나의 기 정의된 물리 법칙을 위반 하는 영상 시퀀스 및 상기 적어도 하나의 기 정의된 물리 법칙을 위반하지 않는 영상 시퀀스를 포함하는 데이터 셋을 학습 데이터로 이용하여, 상기 영상 시퀀스에서의 객체의 움직임이 상기 적어도 하나의 기 정의된 물리 법 칙을 위반하는지 여부를 추론하도록 학습될 수 있다.. 한편, 본 개시의 실시예들은 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨 터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독 가 능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 컴퓨터 또는 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2023-0026906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상기 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으 로 해석되어야 한다.도면 도면1 도면2a 도면2b 도면3 도면4a 도면4b 도면5a 도면5b 도면6a 도면6b 도면7a 도면7b 도면8a 도면8b 도면8c 도면8d 도면9 도면10 도면11 도면12 도면13a 도면13b 도면13c 도면14"}
{"patent_id": "10-2023-0026906", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 개념도이다. 도 2a 및 2b는 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 구현하는 전자 장치를 보여주는 블록도이 다. 도 3은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 흐름도이다. 도 4a는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 개념도이다. 도 4b는 도 4a의 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 5a는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 개념도이다. 도 5b는 도 5a의 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 6a는 일 실시예에 따른 포인트 클라우드에서 물리 법칙을 위반하는 부분을 식별하는 방법을 보여주는 흐름도 이다.도 6b는 일 실시예에 따른 멀티 뷰 이미지들을 생성하고 카메라 정보를 획득하는 방법을 보여주는 개념도이다. 도 7a는 일 실시예에 따른 포인트 클라우드에서 누락된 포인트들을 추정하는 방법을 보여주는 흐름도이다. 도 7b는 일 실시예에 따른 멀티 뷰 이미지들의 히트 맵들을 보여주는 개념도이다. 도 8a 내지 8d는 일 실시예에 따른 물리 법칙의 종류를 설명하기 위한 개념도이다. 도 9는 일 실시예에 따른 심층 신경망의 구조를 보여주는 개념도이다. 도 10은 일 실시예에 따른 심층 신경망의 동작을 보여주는 흐름도이다. 도 11은 일 실시예에 따른 포인트 클라우드를 생성하는 방법을 보여주는 흐름도이다. 도 12는 일 실시예에 따른 재촬영 위치 가이드를 제공하는 방법을 보여주는 흐름도이다. 도 13a 내지 13c는 도 12의 재촬영 위치 가이드를 제공하는 방법을 예시적으로 보여주는 개념도이다. 도 14는 일 실시예에 따른 가상 공간에서 가구 배치를 변경하기 위한 사용자 인터페이스를 보여주는 개념도이다."}
