{"patent_id": "10-2024-0020737", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0050678", "출원번호": "10-2024-0020737", "발명의 명칭": "wav2vec2.0의 은닉 표현을 활용한 분리 기반 음성 변조 장치 및 방법", "출원인": "프레리스쿠너 주식회사", "발명자": "임재민"}}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "메모리; 및상기 메모리에 저장된 명령어를 실행하는 적어도 하나의 프로세서를 포함하고,상기 프로세서는음성 인식 모델을 이용하여 타겟 화자의 음성 및 소스 화자의 음성 각각에 대하여 서로 다른 레이어 가중치를적용하여 모든 레이어에 대한 가중합 표현을 각각 출력하고,상기 각각의 가중합 표현으로부터 상기 타겟 화자의 음성 정보 및 상기 소스 화자의 언어 정보를 추출하고,상기 음성 정보 및 상기 언어 정보를 입력으로하여 변조된 음성을 합성하여 출력하는분리 기반 음성 변조 장치."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 음성 인식 모델은 제1 모델부 및 제2 모델부를 포함하고,상기 프로세서는상기 타겟 화자의 음성으로부터 화자 가중치를 적용하여 상기 제1 모델부의 모든 레이어에 대한 제1 가중합 표현을 출력하고,상기 소스 화자의 음성으로부터 콘텐츠 가중치를 적용하여 상기 제2 모델부의 모든 레이어에 대한 제2 가중합표현을 출력하는분리 기반 음성 변조 장치."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 프로세서는상기 제1 가중합 표현에 기반하여 상기 타겟 화자의 음성 정보를 추출하고, 상기 제2 가중합 표현에 기반하여상기 소스 화자의 언어 정보를 추출하는분리 기반 음성 변조 장치."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서，상기 화자 가중치는 화자 식별(speaker identification： SID) 모델을 통해 하기 수학식 2 및 3을 만족하면서크로스 엔트로피 손실(cross entropy loss)인 H 값을 최소화시키는 값으로 사전 학습된 분리 기반 음성 변조 장치.공개특허 10-2025-0050678-3-[수학식 2][수학식 3]여기서，u는 화자의 랜덤 변수，는 확률 분포의 실제값(ground-truth), 는 SID 모델의 예측값,는 최적화된 화자 가중치를 각각 의미한다"}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서，상기 콘텐츠 가중치는 자동 음성 인식(automatic speech recognition：ASR) 모델을 통해 하기 수학식 5를 만족하면서 CTC 손실(loss)인 값을 최소화시키는 값으로 사전 학습된 분리 기반 음성 변조 장치.[수학식 5]여기서, S는 입력 오디오의 실제값 전사(ground-truth transcript), 는 ASR모델의 입력 오디오에 대한 예측라벨 시퀀스, 는 최적화된 콘텐츠 가중치를 각각 의미한다."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 프로세서는 상기 음성 정보의 평균 및 표준편차 인 화자 임베딩 및 상기 언어 정보를 표현한 콘텐츠 임베딩을 합성하여 음성 변조 출력값을 생성하는분리 기반 음성 변조 장치."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "분리 기반 음성 변조 장치의 동작 방법에 있어서,음성 인식 모델을 이용하여 타겟 화자의 음성 및 소스 화자의 음성 각각에 대하여 서로 다른 레이어 가중치를적용하여 모든 레이어에 대한 가중합 표현을 각각 출력하는 동작,상기 각각의 가중합 표현으로부터 상기 타겟 화자의 음성 정보 및 상기 소스 화자의 언어 정보를 추출하는동작, 및상기 음성 정보 및 상기 언어 정보를 입력으로하여 변조된 음성을 합성하여 출력하는 동작을 포함하는 분리 기반 음성 변조 장치의 동작 방법.공개특허 10-2025-0050678-4-청구항 8 제7항에 있어서,상기 음성 인식 모델은 제1 모델부 및 제2 모델부를 포함하고,상기 타겟 화자의 음성으로부터 화자 가중치를 적용하여 상기 제1 모델부의 모든 레이어에 대한 제1 가중합 표현을 출력하는 동작, 및상기 소스 화자의 음성으로부터 콘텐츠 가중치를 적용하여 상기 제2 모델부의 모든 레이어에 대한 제2 가중합표현을 출력하는 동작을 포함하는분리 기반 음성 변조 장치의 동작 방법."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제1 가중합 표현에 기반하여 상기 타겟 화자의 음성 정보를 추출하는 동작, 및상기 제2 가중합 표현에 기반하여 상기 소스 화자의 언어 정보를 추출하는 동작을 포함하는분리 기반 음성 변조 장치의 동작 방법."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서，상기 화자 가중치는 화자 식별(speaker identification： SID) 모델을 통해 하기 수학식 2 및 3을 만족하면서크로스 엔트로피 손실(cross entropy loss)인 H 값을 최소화시키는 값으로 사전 학습된 분리 기반 음성 변조 장치의 동작 방법.[수학식 2][수학식 3]여기서，u는 화자의 랜덤 변수，는 확률 분포의 실제값(ground-truth), 는 SID 모델의 예측값,는 최적화된 화자 가중치를 각각 의미한다"}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서，상기 콘텐츠 가중치는 자동 음성 인식(automatic speech recognition：ASR) 모델을 통해 하기 수학식 5를 만족하면서 CTC 손실(loss)인 값을 최소화시키는 값으로 사전 학습된 분리 기반 음성 변조 장치의 동작방법.[수학식 5]공개특허 10-2025-0050678-5-여기서, S는 입력 오디오의 실제값 전사(ground-truth transcript), 는 ASR모델의 입력 오디오에 대한 예측라벨 시퀀스, 는 최적화된 콘텐츠 가중치를 각각 의미한다."}
{"patent_id": "10-2024-0020737", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제7항에 있어서,상기 음성 정보의 평균 및 표준편차 인 화자 임베딩 및 상기 언어 정보를 표현한 콘텐츠 임베딩을 합성하여 음성 변조 출력값을 생성하는 동작을 더 포함하는분리 기반 음성 변조 장치의 동작 방법."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "wav2vec 2.0의 은닉 표현을 활용한 분리 기반 음성 변조 장치 및 방법이 개시된다. 본 발명의 분리 기반 음성 변 조 장치는，wav2vec 2.0 모델을 기반으로 학습되어 타겟 화자의 음성을 입력 받아 화자 가중치를 적용하여 모든 레이어에 대한 가중합을 출력하는 제1 모델부; wav2vec 2.0 모델을 기반으로 학습되어 소스 화자의 음성을 입력 (뒷면에 계속)"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 음성 변조 장치 및 방법에 관한 것으로서，보다 상세하게는 wav2vec 2.0 모델 기반으로 모든 레이어 의 은닉 표현을 활용하고 사전 학습된 레이어 가중치를 활용하여 분리 성능이 향상된 분리 기반 음성 변조 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자기지도학습(Self-Supervised Learning； SSL)의 표현(representations) 의 사용이 최근 유행하고 있다. 음성 처리 분야에서 지금까지 SSL 표현의 다양한 타입들이 개발되었다(예를 들어，APC[문헌 1], CPC[문헌 2], HuBERT[문헌 3] 및 wav2vec 2.0[문헌 4]). 특히，wav2vec 2.0은 강력한 음성 처리 능력을 가진 것으로 널리 알 려져 있다. Wav2vec 2.0의 성공과 함께，연구자들은 wav2vec 2.0의 층별 특성의 이해에 관심을 가져왔다[문헌 5-10], 연구 자들은 음성 특성(speech characteristics)이 wav2vec 2.0의 은닉 중에 균일하게 분포하지 않음을 보고하였다. 문헌 7(Pasad et al.)은 wav2vec 2.0의 층별 표현은 음향-언어 계층(acoustic-linguistic hierarchy)을 따르는 것을 보여준다. 문헌 10(Baevski et al.)은 wav2vec 2.0을 이용한 음소 분류(phoneme classification)를 수행 하여 중-상(mid- high) 레이어 표현들이 더 좋은 성능을 달성하는 것을 보여준다. 문헌 5(Fan et al.)는 낮은 레이어(low-layer) 표현들이 t-SNE 가시화에서 화자-레벨 클러스터 (speaker-level clusters)를 형성하는 것을 보여준다. 위의 선행 연구는 wav2vec 표현들의 최종 출력의 사용을 제시하였고 (즉，wav2vec 2.0의 마지막 레이어의 표현 만을 사용)，이러한 방법이 일반적으로 사용되었지만 모든 타입의 음성 임무(speech task)에 대해서 최적의 선 택은 아니었다. 문헌 8 및 9는 음성 인식 임무에서 wav2vec 은닉 표현의 중첩(aggregation of wav2vec hidden representations)을 사용하는 것이 마지막 층의 표현을 사용하는 것보다 좋은 결과를 나타내는 것을 보여주었다. 최근에는 wav2vec 2.0이 음성 변조(Voice Conversion； VC) 임무， 특히 any-to-any VC(문헌 11 및 12)에도 채 택되고 있다. 음성 변조는 소스 화자(source speaker)에 의한 발음의 음색(timbre of an utterance)을 언어/콘 텐츠 정보의 손실 없이 타겟 화자(target speaker)의 그것으로 변환하는 임무이다. 심지어 그러한 화자 정보는 학습 데이터셋에서는 보이지 않을 수도 있다. 하지만，현존하는 wav2vec 기반 음성 변조 제안들(문헌 11 및 1 2)은 wav2vec 표현들의 마지막 레이어만 사용한다. 같은 사전 학습된 표현들이 화자 및 콘텐츠 인코더 모두의 입력으로 취해지기 때문에 변환된 발음의 화자 유사성은 다소 불만족스럽다. 더욱이 어텐션 메커니즘(attention mechanism)의 특성 때문에，음성 변조된 발음의 화자 유사성은 주어지는 타겟 화자의 발음의 수에 의존하게 된 다. 분리 기반의 음성 변조(Disentanglement-based Voice Conversion) 방식은 도 1에 도시된 바와 같이，훈련 시 한 화자의 발화 음성에서 화자 음색 정보 및 언어적 정보를 분리하도록 훈련한다. 추론 시(즉，음성 변조시)에 는 위에서 훈련한 모델을 활용하여 서로 다른 두 화자의 발화에서 목적 화자의 음성으로부터 음색 정보를，입력 화자의 음성에서는 언어 정보를 취득하고，두 정보를 이용하여 음성 변조된 발화를 합성한다. 분리 기반의 음성 변조 장치에서는 입력 발화 음성에서 음색 정보와 언어 정보가 서로 독립적으로 분리되는 것 이 중요하다. 만약，잘못된 훈련으로 인해 언어 정보에 화자의 음색 정보가 포함되어 있다면，음성 변조시 합성 된 음성은 입력 화자와 목적 화자의 음성이 뒤섞인 발화로 합성되기 때문에 변조된 음성은 목적 화자와의 유사 도뿐만 아니라 음성 명료도 또한 저하되게 된다. 이에 기존 연구들은 손실 함수(loss function)에 다양한 제한 항을 두어 두 정보가 서로 독립적으로 분리하는 방법을 제안해 왔다. 그러나 해당 기술들은 다양한 제한 항의 최적의 가중치를 찾는데 많은 비용이 요구되는 한 계를 가졌다. 문헌 8 및 9는 기존 wav2vec 2.0의 은닉 표현을 레이어 가중치 합을 활용하여 음성 관련 작업(음성 인식， 화자 인식， 발화 언어 인식 등)을 수행하는 종래 기술을 소개하고 있다. 이러한 종래기술은 목적하는 음성 관련 작 업을 수행하는 모델과 함께 레이어 가중치를 함께 학습시킨다. 결과적으로 목적 음성 작업을 가장 잘 수행할 수 있도록， 레이어 가중치는 wav2vec 2.0의 은닉 표현에서 필요한 정보를 추출할 수 있도록 학습된다. 문헌 8 및 9의 방식을 분리 기반 음성 변조에 적용하게 되면， 화자 음색 정보 및 언어 정보 인코더 이전의 wav2vec 레이어 가중치도 함께 학습된다. 그러나， 문헌 8 및 9에서 상정하는 음성 관련 작업과 다르게， 분리 기반 음성 변조는 동시에 두 가지 음성 관련 작업을 수행한다고 볼 수 있다. 즉， 동시에 화자 음색 정보뿐만 아니라 언어적인 정보 또한 인코딩을 요구한다. 실험 결과에 비추어 볼 때， 레이어 가중치가 함께 학습되는 것으로 인해 학습된 가중치는 각 정보를 잘 추출할 수 있도록 학습되지 않았고 이에 따라 모델은 두 정보를 서로 독립적으로 분리하는데 실패했다. 선행기술문헌 비특허문헌 (비특허문헌 0001) （비특허문헌 0001） Y.-A. Chung and J. Glass, \"Generative pre-training for speech with autoregressive predictive coding,\" in ICASSP 2020- 2020 IEEE International Conference on Acoustics, Speech and Signal Processing （ICASSP）. IEEE, 2020， pp. 3497-3501. (비특허문헌 0002) （비특허문헌 0002） A. v. d. Oord, Y. Li, and 0. Vinyals, \" Representation learning with contrastive predictive coding,\" arXiv preprint arXiv：1807.03748, 2018. (비특허문헌 0003) （비특허문헌 0003） W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \"Hubert: Self-supervised speech representation learning by masked prediction of hidden units,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451-3460, 2021. (비특허문헌 0004) （비특허문헌 0004） A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \" wav2vec 2.0: A framework for self-supervised learning of speech representations,\" Advances in neural information processing systems, vol. 33, pp. 12 449-12 460, 2020 (비특허문헌 0005) （비특허문헌 0005） Z. Fan, M. Li, S. Zhou, and B. Xu, \"Exploring wav2vec 2.0 on speaker verification and language identification,\" arXiv preprint arXiv:2012.06185, 2020. (비특허문헌 0006) （비특허문헌 0006） J. Shah, Y. K. Singla, C. Chen, and R. R. Shah, \" What all do audio transformer models hear? probing acoustic representations for language delivery and its structure,\" arXiv preprint arXiv:2101.00387, 2021. (비특허문헌 0007) （비특허문헌 0007） A. Pasad, J.-C. Chou, and K. Livescu, \"Layer-wise analysis of a self-supervised speech representation.\" (비특허문헌 0008) （비특허문헌 0008） S.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, (비특허문헌 0009) Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin et al., \" Superb: Speech processing universal performance benchmark,\"arXiv preprint arXiv：2105.01051, 2021. (비특허문헌 0010) （비특허문헌 0009） X. Chang, T. Maekaku, P. Guo, J. Shi, Y.-J. Lu, A. S. Subramanian, T. Wang, S.-w. Yang, Y. Tsao, H.-y. Lee et al., \"An exploration of self-supervised pretrained representations for end-to-end speech recognition,\" in 2021 IEEE Automatic Speech Recognition and Understanding Workshop （ASRU）. IEEE, 2021, pp. 228-235. (비특허문헌 0011) （비특허문헌 0010） A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \" Unsupervised speech recognition,\" Advances in Neural Information Processing Systems, vol. 34, pp. 27 826-27 839, 2021. (비특허문헌 0012) （비특허문헌 0011） Y. Y. Lin, C.-M. Chien, J.-H. Lin, H.-y. Lee, and L.-s. Lee \"Fragmentvc: Any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing（ICASSP）. IEEE, 2021, pp. 5939-5943. (비특허문헌 0013) （비특허문헌 0012）J.-h. Lin, Y. Y. Lin, C.-M. Chien, and H.-y. Lee, \"S2vc: A framework for any-to-any voice conversion with self-supervised pretrained representations,\" arXiv preprint arXiv:2104.02901, 2021. (비특허문헌 0014) （비특허문헌 0013） Y.-H. Chen, D.-Y. Wu, T.-H. Wu, and H.-y. Lee, \" Again-vc： A one-shot voice conversion using activation guidance and adaptive instance normalization,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing （ICASSP）. IEEE, 2021, pp. 5954-5958. (비특허문헌 0015) （비특허문헌 0014） D.-Y. Wu, Y.-H. Chen, and H.-Y. Lee, \"Vqvc+: One- shot voice conversion by vector quantization and u-net architecture,\" arXiv preprint arXiv:2206.04154, 2020. (비특허문헌 0016) （비특허문헌 0015） K. Qian, Y. Zhang, S. Chang, X. Yang, and M. Hasegawa-Johnson, \"Autovc: Zero-shot voice style transfer with only autoencoder loss,\" in International Conference on Machine Learning. PMLR, 2019, pp. 5210-5219. (비특허문헌 0017) （비특허문헌 0016） J.-c. Chou, C.-c. Yeh, and H.-y. Lee, \"One-shot voice conversion by separating speaker and content representations with instance normalization,\" arXiv preprint arXiv:1904.05742, 2019. (비특허문헌 0018) （비특허문헌 0017） D.-Y. Wu and H.-y. Lee, \"One-shot voice conversion by vector quantization,\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing （ICASSP）. IEEE, 2020, pp. 7734- 7738. (비특허문헌 0019) （비특허문헌 0018） D. Wang, L. Deng, Y. T. Yeung, X. Chen, X. Liu, and H. Meng,\"Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion,\" arXiv preprint arXiv:2106.10132, 2021. (비특허문헌 0020) （비특허문헌 0019） Q. Wang, X. Zhang, J. Wang, N. Cheng, and J. Xiao, \"Drvc: A framework of any-to-any voice conversion with self-supervised learning,\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing （ICASSP）. IEEE, 2022, pp. 3184- 3188. (비특허문헌 0021) （비특허문헌 0020） B. Nguyen and F. Cardinaux, \"Nvc-net: End-to-end adversarial voice conversion,\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing （ICASSP）. IEEE, 2022, pp. 7012-7016. (비특허문헌 0022) （비특허문헌 0021） \"Facebook Al research sequence-to-sequence toolki t,\" https://github.com/facebookresearch/fairseq. (비특허문헌 0023) （비특허문헌 0022） X. Huang and S. Belongie, \"Arbitrary style transfer in real-time with adaptive instance normalization,\" in Proceedings of the IEEE international conference on computervision, 2017, pp. 2111-1510. (비특허문헌 0024) （비특허문헌 0023)Z. Liu and B. Mak, \"Cross-lingual multi-speaker text-to-speech synthesis for voice cloning without using parallel corpus for unseen speakers,\" arXiv preprint arXiv:1911.11601, 2019. (비특허문헌 0025) （비특허문헌 0024） H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \"Libritts: A corpus derived from librispeech for text-to-speech,n arXiv preprint arXiv:1904.02882, 2019. (비특허문헌 0026) （비특허문헌 0025） J. Lorenzo-Trueba, T. Drugman, J. Latorre, T. Merritt, B. Putrycz, R. Barra-Chicote, A. Moinet, and V. Aggarwal, \"Towards achieving robust universal neural vocoding,\" arXiv preprint arXiv：1811.06292, 2018. (비특허문헌 0027) （비특허문헌 0026）J. Kominek and A. W. Black, \"The emu arctic speech databases,\"in Fifth ISCA workshop on speech synthesis, 2004. (비특허문헌 0028) （비특허문헌 0027）\"Resemblyzer: a python package to analyze and compare voices with deep learning,\" https://github.com/resemble-ai/Resemblyzer. (비특허문헌 0029) （비특허문헌 0028）\"Silero models： pre-trained enterprise-grade STT /TTS models and benchmarks.\" https://github.com/snakers4/silero-models."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 위와 같은 문제를 해결하기 위한 것으로，사전 학습된 레이어 가중치를 활용하여 분리 성능이 향상된 분리 기반 음성 변조 장치 및 방법을 제공하는 것을 목적으로 한다. 또한，본 발명은 wav2vec 2.0의 모든 층의 은닉 표현을 사용하여 음성 변조 성능이 향상된 분리 기반 음성 변조 장치 및 방법을 제공하는 것을 목적으로 한다. 본 명세서의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 명세서의 다른 목적 및 장 점들은 하기의 설명에 의해서 이해될 수 있고, 본 명세서의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 명세서의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 과제를 해결하기 위한 본 발명의 일실시예에 따른 wav2vec 2.0의 은닉 표현을 활용한 분리 기반 음성 변조 장치는，메모리 및 상기 메모리에 저장된 명령어를 실행하는 적어도 하나의 프로세서를 포함하고, 상기 프로세 서는 음성 인식 모델을 이용하여 타겟 화자의 음성 및 소스 화자의 음성 각각에 대하여 서로 다른 레이어 가중 치를 적용하여 모든 레이어에 대한 가중합 표현을 각각 출력하고, 상기 각각의 가중합 표현으로부터 상기 타겟 화자의 음성 정보 및 상기 소스 화자의 언어 정보를 추출하고, 상기 음성 정보 및 상기 언어 정보를 입력으로하 여 변조된 음성을 합성하여 출력한다. 또한, 본 발명의 일 실시예에서 상기 음성 인식 모델은 제1 모델부 및 제2 모델부를 포함하고, 상기 프로세서는 상기 타겟 화자의 음성으로부터 화자 가중치를 적용하여 상기 제1 모델부의 모든 레이어에 대한 제1 가중합 표 현을 출력하고, 상기 소스 화자의 음성으로부터 콘텐츠 가중치를 적용하여 상기 제2 모델부의 모든 레이어에 대 한 제2 가중합 표현을 출력한다. 또한, 본 발명의 일 실시예에서 상기 프로세서는 상기 제1 가중합 표현에 기반하여 상기 타겟 화자의 음성 정보 를 추출하고, 상기 제2 가중합 표현에 기반하여 상기 소스 화자의 언어 정보를 추출한다. 또한, 본 발명의 일 실시예에서 상기 화자 가중치는 화자 식별(speaker identification： SID) 모델을 통해 하 기 수학식 2 및 3을 만족하면서 크로스 엔트로피 손실(cross entropy loss)인 H 값을 최소화시키는 값으로 사전 학습된다. [수학식 2]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "[수학식 3]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "여기서，u는 화자의 랜덤 변수， 는 확률 분포의 실제값(ground-truth), 는 SID 모델의 예측값, 는 최적화된 화자 가중치를 각각 의미한다 또한, 본 발명의 일 실시예에서 상기 콘텐츠 가중치는 자동 음성 인식(automatic speech recognition：ASR) 모 델을 통해 하기 수학식 5를 만족하면서 CTC 손실(loss)인 값을 최소화시키는 값으로 사전 학습된다. [수학식 5]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 4, "content": "여기서, S는 입력 오디오의 실제값 전사(ground-truth transcript), 는 ASR모델의 입력 오디오에 대한 예측 라벨 시퀀스, 는 최적화된 콘텐츠 가중치를 각각 의미한다. 또한, 본 발명의 일 실시예에서 상기 프로세서는 상기 음성 정보의 평균 및 표준편차 인 화자 임베딩 및 상기 언어 정보를 표현한 콘텐츠 임베딩을 합성하여 음성 변조 출력값을 생성한다. 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 동작 방법은 음성 인식 모델을 이용하여 타겟 화자의 음성 및 소스 화자의 음성 각각에 대하여 서로 다른 레이어 가중치를 적용하여 모든 레이어에 대한 가중합 표현 을 각각 출력하는 동작, 상기 각각의 가중합 표현으로부터 상기 타겟 화자의 음성 정보 및 상기 소스 화자의 언 어 정보를 추출하는 동작, 및 상기 음성 정보 및 상기 언어 정보를 입력으로하여 변조된 음성을 합성하여 출력 하는 동작을 포함한다. 또한, 본 발명의 일 실시예에서 상기 음성 인식 모델은 제1 모델부 및 제2 모델부를 포함하고, 상기 타겟 화자 의 음성으로부터 화자 가중치를 적용하여 상기 제1 모델부의 모든 레이어에 대한 제1 가중합 표현을 출력하는 동작, 및 상기 소스 화자의 음성으로부터 콘텐츠 가중치를 적용하여 상기 제2 모델부의 모든 레이어에 대한 제2 가중합 표현을 출력하는 동작을 포함한다. 또한, 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 동작 방법은 상기 제1 가중합 표현에 기반하여 상기 타겟 화자의 음성 정보를 추출하는 동작, 및 상기 제2 가중합 표현에 기반하여 상기 소스 화자의 언어 정 보를 추출하는 동작을 포함한다. 또한, 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 동작 방법은 상기 화자 가중치는 화자 식별 (speaker identification： SID) 모델을 통해 하기 수학식 2 및 3을 만족하면서 크로스 엔트로피 손실(cross entropy loss)인 H 값을 최소화시키는 값으로 사전 학습된다. [수학식 2] [수학식 3]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 5, "content": "여기서，u는 화자의 랜덤 변수， 는 확률 분포의 실제값(ground-truth), 는 SID 모델의 예측값, 는 최적화된 화자 가중치를 각각 의미한다 또한, 본 발명의 일 실시예에서 상기 콘텐츠 가중치는 자동 음성 인식(automatic speech recognition：ASR) 모 델을 통해 하기 수학식 5를 만족하면서 CTC 손실(loss)인 값을 최소화시키는 값으로 사전 학습된다. [수학식 5]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 6, "content": "여기서, S는 입력 오디오의 실제값 전사(ground-truth transcript), 는 ASR모델의 입력 오디오에 대한 예측 라벨 시퀀스, 는 최적화된 콘텐츠 가중치를 각각 의미한다. 또한, 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 동작 방법은 상기 음성 정보의 평균 및 표준편 차 인 화자 임베딩 및 상기 언어 정보를 표현한 콘텐츠 임베딩을 합성하여 음성 변조 출력값을 생성하는 동작을 더 포함한다."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일실시예에 따른 wav2vec 2.0 기반 분리 기반 음성 변조 장치 및 방법은，사전 학습된 레이어 가중치 를 활용함으로써 모델 학습시 레이어 가중치가 함께 학습됨으로써 분리 성능이 저하되는 문제를 해결함으로써 분리 성능을 향상시키고，이로 인해 음성 변조 성능을 크게 향상시킬 수 있다. 또한，본 발명은 wav2vec 2.0의 마지막 레이어만을 활용하는 것이 아닌 모든 레이어에 대한 사전 학습된 레이어 가중치를 적용한 가중합을 적용함으로써 음성 변조 성능을 크게 향상시킬 수 있다. 본 발명에 따른 효과는 이상에서 예시된 내용에 의해 제한되지 않으며，더욱 다양한 효과들이 본 명세서 내에 포함되어 있다."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 각 도면을 설명하면서 유사한 참조부호를 유사한 구성요소에 대해 사용하였다. 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어 들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있 고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재된 항 목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가지는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 이하, 첨부된 도면을 참조하여 본 발명의 바람직한 실시예를 상세하게 설명한다. 도 1은 종래기술에 따른 분리 기반 음성 변조 장치의 학습 방법을 설명하는 도면이고, 도 2는 종래기술에 따른 분리 기반 음성 변조 장치의 동작 방법을 설명하는 도면이다.도 1 및 도 2를 참조하면, 종래의 분리 기반 음성 변조(Disentangled-based Voice Conversion) 장치는 훈련시，한 화자의 발화 음성에서 화자 음색 정보 및 언어 적 정보를 분리하도록 훈련된다. 다음 추론시(즉，음성 변조시)，위에서 훈련된 모델을 활용하여 서로 다른 두 화자의 발화에서 목적 화자의 음성으로부터 음색 정보를，입력 화자의 음성에서는 언어 정보를 취득하고，취득 한 두 정보를 이용하여 음성 변조된 발화를 합성한다. 이와 같은 분리 기반의 음성 변조 장치에서는 입력 발화 음성에서 음색 정보와 언어 정보가 서로 독립적으로 분 리되는 것이 중요하다. 위에서 언급한 바와 같이，잘못된 훈련으로 인해 언어 정보에 화자의 음색 정보가 포함 되면 음성 변조 시 합성된 음성은 입력 화자와 목적 화자의 음성이 뒤섞인 발화로 합성되어 변조된 음성은 목적 화자와의 유사도뿐만 아니라 음성 명료도 또한 저하된다. 상기와 같은 문제를 해결하기 위해서, 본 발명은 any-to-any VC를 위하여 wav2vec 2.0의 모든 레이어의 은닉 표 현을 활용하는 wav2vec 2.0 기반 음성 변조(Wav2vec_VC)를 제안한다. 현존하는 분리 기반 모델과 달리 본 발명의 음성 변조 방법은 어떤 명시적 제약 조건(any explicit constraint) 없이 오직 재구성 손실 (reconstruction loss)을 사용하여 학습된다. 본 발명의 음성 변조 방법은 사전 학습된 레이어 가중치를 이용하여 wav2vec 2.0의 은닉 표현으로부터 화자 및 콘텐츠 정보를 구분한다. 더욱 구체적으로, 본 발명의 분리 기반 음성 변조 방법에서 레이어 가중치들은 화자 및 콘텐츠 인코더에서 필요 한 정보를 추출할 수 있도록 사전 학습된다. 즉, 사전 학습된 레이어 가중치를 이용하여 모든 은닉 표현의 가중 합(weighted-sum)을 획득함으로써, 화자 및 콘텐츠 인코더는 화자 및 콘텐츠 정보의 잠재 표현(latent representations)을 보다 더 효율적으로 학습한다. 다시 말하면, 사전 학습된 레이어 가중치는 구분을 위한 암시적 제약조건(implicit constraint)으로 동작한다. 본 발명의 분리 기반 음성 변조 장치는 공개적으로 이용 가능한 13개의 레이어로 구성되는 wav2vec 2.0 베이스 모델을 사용하여 Wav2vec-VC 모델을 구축하였으며, 실험 결과 본 발명의 음성 변조 장치는 화자 식별 수용율 (speaker verification acceptance rate)이 95%를 초과하였으며, 단어 오류율(word error rate)은 22% 미만을 나타냈으며, 이는 화자 유사성(speaker similarity) 및 음성 명료도(speech intelligibility)의 관점 모두에 있어서 SOTA(State-of-the-art) VC모델을 능가하는 것을 보여주었다. 도 3은 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 블록도이다. 도면을 참조하면, 분리 기반 음성 변조 장치는 메모리, 프로세서, 데이터 베이스, 네트워크 인 터페이스 및 시스템 버스 를 포함한다. 프로세서는 메모리와 같은 컴퓨터-판독 가능 매체 내에 저장되는 컴퓨터 명령어들을 실행하도록 구성된 다. 예컨대, 프로세서는 중앙 처리 장치(Central Processing Unit; CPU)이거나, 그래픽 처리 장치 (Graphics Processing Unit; GPU), 인공지능 프로세서(Neural Processing Unit; NPU), 텐서 프로세서(Tensor Processing Unit; TPU) 또는 신호 처리 장치 (Digital Signal Processor; DSP)일 수 있으며, 이는 예시적인 것으로 실시예의 한정 없이 컴퓨터 명령어들을 실행하는 모든 기기를 포함할 수 있다. 데이터 베이스는 체계적으로 구조화된 데이터의 집합을 저장하고 관리하는 저장 장치로, 수신한 화자의 음 성 신호와 관련된 데이터를 저장하거나, 음성 인식 모델 학습을 위한 학습용 음성 데이터를 저장한다. 도면에서 데이터 베이스는 음성 변조 장치 내부에 위치한 것으로 도시되었으나, 반드시 이에 한정되는 것은 아니 며, 외부에 별도로 구비되어 음성 변조 장치와 통신하며 데이터를 관리할 수 있다. 네트워크 인터페이스는 분리 기반 음성 변조 장치가 네트워크 내에서 데이터를 송신 및 수신하는 것을 허용하도록 구성된다. 네트워크 인터페이스는 하나 이상의 네트워크 인터페이스 카드(Network Interface Card; NIC)를 포함할 수 있다. 시스템 버스는 선형 구조의 네트워크 토폴리지로서, 단일 라인인 중앙 버스를 통해 모든 노드(Node)가 다중 으로 연결되는 구조를 갖는다. 중앙 버스를 통해 각 노드의 신호 및 데이터는 양방향으로 전송되며 연결된 모든 구성이 서로 통신할 수 있다. 메모리는 데이터 및 명령어들을 저장한다. 도시된 바와 같이, 다양한 실시예에서, 메모리는 음성 수신 모듈, 가중합 산출 모듈, 부호화 모듈, 복호화 모듈을 포함한다. 음성 수신 모듈은 복수의 화자로부터 음성을 수신한다. 구체적으로, 음성 수신 모듈은 안테나 또는 마이크로폰을 포함하여 외부에서 들어오는 음성 신호를 수신하고 수신된 음성 신호의 파형을 Mel scale로 다운 스케일링하여 멜 스펙트로그램(Mel Spectrogram)으로 변환한다. 한편, 음성은 발화자(Source speaker)의 음성인 소스 음성 및 음성 변조의 대상이 되는 화자인 타겟 화자 (target speaker)의 음성을 포함할 수 있고, 소스 음성 및 타겟 음성 각각으로부터 변환된 멜 스펙트로그램은 모두 음성 인식 모델에 입력된다. 여기서, 음성 인식 모델은 사전 학습된 인공지능 모델로서 예컨대, wav2vec 2.0모델일 수 있으며, 반드시 이에 한정되는 것은 아니나 이하에서는 설명의 편의를 위해 음성 인식 모델은 wav2vec 2.0모델임을 전제로 하여 설명 하도록 한다. 가중합 산출 모듈은 음성 인식 모델을 이용하여 타겟 화자의 음성 및 소스 화자의 음성 각각에 대하여 서 로 다른 레이어 가중치를 적용하여 모든 레이어에 대한 가중합 표현을 각각 출력한다. 상세하게, 음성 인식 모 델은 제1 모델부 및 제2 모델부를 포함하고, 레이어 가중치는 화자 가중치 및 콘텐츠 가중치를 포함할 수 있다. 즉, 가중합 산출 모듈은 타겟 화자의 음성으로부터 화자 가중치를 적용하여 상기 제1 모델부에 포함된 모 든 레이어에 대한 제1 가중합을 산출하고, 제1 가중합이 적용된 제1 가중합 표현을 출력한다. 마찬가지로, 가중합 산출 모듈은 소스 화자의 음성으로부터 콘텐츠 가중치를 적용하여 제2 모델부에 포함 된 모든 레이어에 대한 제2 가중합을 산출하고, 제2 가중합이 적용된 제2 가중합 표현을 출력한다. 상술한 바와 같이 화자 가중치 및 콘텐츠 가중치가 상이한 값을 가짐에 따라 제1 가중합과 제2 가중합도 서로 상이한 값을 가질 수 있으며, 화자 가중치 및 콘텐츠 가중치 각각은 음성 인식에 최적화되도록 사전 학습된 고정값으로 정의될 수 있다. 화자 가중치 및 콘텐츠 가중치의 최적화에 대한 사전 학습 방법은 후술하여 상세히 설명하도록 한다. 부호화 모듈은 인코더(Encoder)를 통해 가중합 산출 모듈에서 산출된 가중합이 적용된 가중합 표현으 로부터 특정 정보를 추출한다. 구체적으로, 인코더는 화자 인코더(Speaker Encoder) 및 콘텐츠 인코더(Content Encoder)를 포함하며, 부호화 모듈은 화자 인코더(Speaker Encoder)를 통해 제1 가중합 표현으로부터 타겟 화자의 음성 정보를 추출하고, 콘텐츠 인코더(Content Encoder)를 통해 제2 가중합 표현으로부터 소스 화자의 언어 정보를 추출한다. 복호화 모듈은 음성 정보 및 언어 정보를 입력으로 하여 음성을 합성한다. 상세하게, 복호화 모듈은 디코더(Decoder)를 이용하여 음성을 합성하고, 변조된 음성을 외부로 출력한다. 도 4는 본 발명의 일실시예에 따른 wav2vec 2.0의 은닉 표현을 활용한 분리 기반 음성 변조 장치의 개략 구성도 이다. 도 4를 참조하면，본 발명의 분리 기반 음성 변조 장치는 제1 모델부，제2 모델부，화자 인코더(31 0)，콘텐츠 인코더 및 디코더를 포함하여 구성된다. 제1 모델부는 wav2vec 2.0 모델을 기반으로 학습되어 타겟 화 자(target speaker)의 음성을 입력 받아 화 자 가중치를 적용하여 모든 레이어에 대한 가중합을 출력하는 역할을 한다. 타겟 화자의 발음(utterance)이 먼저 wav2vec 2.0 모델에 입력된다. 은닉 레이어를 포함한 wav2vec 레이어들로 부터의 모든 결과 표현들이 가중합된다. 이때，레이어 가중치의 값들은 사전 학습 단계에서 사전 정의되어 사용 된다. 특히，가중치들은 화자 식별 성능을 높이도록 모든 은닉 레이어들에 대하여 가중되도록 학습되며，이것은 화자 가중(speaker weighting)으로 정의된다. 제2 모델부는 wav2vec 2.0 모델을 기반으로 학습되어 소스 화 자(target speaker)의 음성을 입력 받아 콘 텐츠 가중치를 적용하여 모든 레이어에 대한 가중합을 출력하는 역할을 한다. 소스 화자의 발음 역시 wav2vec 2.0 모델에 입력된다. 마찬가지로 wav2vec 레이어의 모든 출력 표현은 가중합되 며，이때 레이어 가중치의 값은 사전 학습 단계에서 사전 정의된다. 이때，가중치들은 음성 인식 성능을 높이도 록 모든 은닉 레이어들에 대하여 가중되도록 학습되며， 이것은 콘텐츠 가중(contents weighting)으로 정의된다. 화자 인코더는 제1 모델부의 출력값을 수신하여 타겟 화자의 음성 정보를 추출하는 역할을 하며，콘 텐츠 인코더는 제2 모델부의 출력값을 수신하여 소스 화자의 언어 정보를 추출하는 역할을 한다. 구체적으로，제1 모델부의 타겟 화자의 화자 가중합된 표현은 화자 인코더의 입력으로 취해지고，제2 모델부의 소스 화자의 콘텐츠 가중합된 표현은 콘텐츠 인코더의 입력으로 취해진다. 화자 인코더는 화자 정보(speaker information)를 화자 표현(speaker information)으로 인코딩하고， 콘 텐츠 인코더는 언어 정보(linguistic information)를 콘텐츠 표현(contents representation)으로 인코딩 한다. 디코더는 화자 인코더의 음성 정보 출력값 및 콘텐츠인 코더의 언어 정보 출력값을 이용하여 변 조된 음성을 합성하여 출력하는 역할을 한다. 도 5는 본 발명의 분리 기반 음성 변조 장치가 사전 학습을 통하여 레이어 가중치를 결정하는 방법을 설명하는 개념도이다. 본 발명의 음성 변조 장치는 도 5에 도시된 바와 같이，화자 가중을 위한 화자 식별(speaker identification) 및 콘텐츠 가중을 위한 음성 인식(speech recognition)을 통한 학습을 통하여 wav2vec layer 가중치를 결정한다. 이때，화자 가중합을 위한 화자 가중치의 결정을 위한 학습은 제1 모델부를 통하여 수행될 수 있으며，콘 텐츠 가중합을 위한 콘텐츠 가중치의 결정을 위한 학습은 제2 모델부를 통하여 수행될 수 있다. 도 5에 도시된 바와 같이, 제1 모델부는 화자 분류부를 포함하며, 화자 분류부는 프레임-레벨 임베딩(frame-level embedding)을 위한 평균-풀링(mean-pooling) 및 무속의 교차 엔트로피 손실(cross entropyloss)을 가지는 선형 변환(linear transformation)을 사용한다. 또한, 제2 모델부는 음성 인식부를 포함하며, 음성 인식부는 CTC 손실(CTC loss)을 가지는 2 레 이어 및 1014 유닛의 BLSTM(Bidirectional LSTM)을 사용한다. 화자 분류부 및 음성 인식부는 공통적으로 상기 임무 관 련된 레이어(task-related layers)의 앞에 하나의 선형 레이어(linear layer) 및 임시 컨볼루션 레이어(temporal convolution layer)를 가진다. - 화자 가중을 위한 레이어 가중치 (Layer weights for speaker weighting) n 개의 레이어를 가지는 wav2vec 2.0으로부터 i 번째 레이어의 표현 시퀀스를 로 정의하고，화자 식별 (speaker identification： SID) 모델 즉，화자 분류부를 통해 학습된 레이어 가중치를 로 정의하면，화자 분류부로 입력되는 가중합 표현 는 아래의 수학식 1로 정의될 수 있다. [수학식 1]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이때，교차 엔트로피 손실(cross entropy loss)인 를 최소화시킴으로써 최적화된 를 결정할 수 있 으며 이를 수학식으로 표현하면 아래 수학 식 2 및 3과 같다. [수학식 2]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[수학식 3]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "상기 수학식 2 및 3에서，u는 화자의 랜덤 변수， 는 확률 분포의 실제값(ground-truth)， 는 SID 모델의 예측값, 는 최적화된 화자 가중치를 의미한다. - 콘텐츠 가중을 위한 레이어 가중치(Layer weights for content weighting) n 개의 레이어를 가지는 wav2vec 2.0으로부터 i 번째 레이어의 표현 시퀀스를 로 정의하고，자동 음성 인식 (automatic speech recognition： ASR) 모듈 즉，음성 인식부를 통해 학습된 레이어 가중치를 로 정의하면，음성 인식부로 입력되는 가중합 표현 는 아래의 수학식 4로 정의될 수 있다.[수학식 4]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "이때，CTC 손실(CTC loss)인 를 최소화시킴으로써 최적화된 를 결정할 수 있으며 이를 수학식으로 표현하면 아래 수학식 5와 같다. [수학식 5]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "이때， 는 입력 오디오의 실제값 전사(ground-truth transcript), 는 음성 인식부로부터의 오디오에 대한 예측 라벨 시퀀스(predicted label sequence), 는 최적화된 콘텐츠 가중치이다. 여기에 특정 언어 모델이 적용되지는 않았다. 도 6은 본 발명의 분리 기반 음성 변조 장치의 화자 인코더，콘텐츠 인코더 및 디코더의 개략 구성도이다. 도 6에서，ConvlD 블록은 스킵 연결(skip connection)이 적용된 단일 임시 컨볼루션 레이어(single temporal convolution layer), ELU 활성화 함수 및 배치 정규화(batch normalization) 레이어를 가진다. 선형 블록 (Linear Block)은 단일 선형 레이어(single linear layer), ReLU 활성화 함수，배치 정규화 레이어 및 다른 선 형 레이어를 포함하여 구성된다. 이하 각 모듈에 대하여 개략적으로 설명한다. 화자 인코더는 선형 블록(Linear Block), ConvlD 블록 및 통계 풀링(Statistic pooling) 블록을 포함하여 구성된다. 콘텐츠 인코더는 선형 블록(Linear Block), ConvlD 블록，인스턴스정규화(InstanceNormlD) 블록 및 보틀 넥(Bottleneck) 블록을 포함하여 구성된다. 디코더는 선형 블록(Linear Block), 인스턴스정규화 (InstanceNormlD) 블록，적응인스턴스정규화(AdalN) 블록，ConvlD 블록，GRU(Gated Recurrent Units) 블록 및 선형 블록을 포함하여 구성된다. 본 발명의 음성 변조 장치는 시불변 정보를 추출하기 위해서 시간 차원에서 화자 인코더의 표현의 평균 및 표준편차인 를 계산하고，이것을 화자 임베딩(speaker embeddings)으로 사용한다. 즉, 화자 임베딩은 디 코더 에서 콘텐츠 특성(content features)과 결합된다. 또한, 콘텐츠 인코딩을 위하여 본 발명은 인스턴스정규화 (InstanceNormlD) 및 보틀넥(Bottleneck) 레이어를 사 용하여 콘텐츠 임베딩으로부터 화자-종속 정보(speaker-dependent information)를 제거한다. 디코더는 화자 인코더의 출력 표현의 평균 및 표준편차인 을 화자 임베딩으로，콘텐츠 인코더 의 출력을 콘텐츠 임베딩으로 입력 받아 이들을 합성하여 음성 변조 출력값을 생성하는 역할을 한다. 디코더는 각 인코더로부터의 화자 및 콘텐츠 임베딩을 결합하기 위하여，연결(concatenation)보다 스타일 전송(style transfer)에 더 효과적인 것으로 알려진 적응적 인스턴스 정규화(adaptive instance normalization) 블록 즉，적응인스턴스정규화(AdalN) 블록을 사용한다. 본 발명의 음성 변조 장치는 손실 함수로 오직 재구성 오차 (reconstruction errors)만을 사용하여，재구성 오 차를 최소화시킴으로써 wav2vec 2.0의 동결 레이어(frozen layers) 및 사전 학습된 레이어 가중치를 제외한 네 트워크를 한 번에 업데이트할 수 있다.본 발명은 상기와 같은 구조를 채택하여 화자 가중치 및 콘텐츠 가중치를 사전 학습 단계에서 미리 결정하고， 화장 가중치 및 콘텐츠 가중치가 결정된 상태에서 소스 화자 및 타겟 화자의 데이터를 입력받아 학습을 수행하 여 음성 변조 성능을 극대화시킬 수 있다."}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "이하에서는 상기 본 발명의 구조 및 방법에 따른 실험예를 참고로 하여 본 발명의 효과에 대하여 설명한다. - 실험 셋업(Experiment Setup) 본 발명은 사전 학습되었지만 특정 임무에 대한 인 튜닝이 되지 않은 공개적으로 이용 가능한 wav2vec 2.0 베이 스 모델을 사용하여 wav2vec 2.0 모델 기반 음성 변조 모델(Wav2vec-VC)을 생성하였다. - 학습 셋업(Training Setup) (데이터셋) 학습은 CSTR VCTK corpus를 기초로 다양한 악센트를 가지는 109명의 영어 화자의 44시간의 음성 데 이터를 통하여 수행되었다. 학습을 위하여, wav2vec 2.0으로부터의 출력 표현 시퀀스의 길이에 매칭하기 위하여 80 차원 멜스펙트로그램(80-dim mel-spectrogram)이 채택되었다. 화자/콘텐츠 가중 즉, SID/ASR 모델의 사전 학습을 위하여, LibriTTS train-clean-100, train-clean-360 및 VCTK의 세 가지 다른 음성 데이터 셋(speech dataset)이 사용되었다. LibriTTS train-clean-360에서 각 화자에 대하여，40개의 발음을 랜덤하게 샘플링하였다. 결과적인 +34K 개의 발음(그것들의 전사와 함께)은 학습 및 검증에 각각 80% 및 20%의 비율로 그룹핑되었다. 실험에서 모든 발음은 16kHz로 재 샘플링되어 wav2vec 2.0의 입력으로 사용되었다. (모델 구성(Model configuration)) 및 를 가지는 아담 옵티마이저(Adam optimizer)를 사용하여 SID/ASR 모델뿐 만 아니라 Wav2vec-VC 모델을 최적화하였다. 본 발명의 Wav2vec_VC 모 델은 전체 200K의 스텝에 대하여 배치 사이즈 32 및 학습율 5.10-4을 기초로 학습되었다. 학습율은 학습율 스케 줄러에 의하여 40K의 스텝마다 절반으로 줄어든다. SIDASR 모델의 학습을 위하여，배치 사이즈 8，학습 단계 320K，스케줄러 없이 학습율 3.10-4로 설정되었다. 디코딩된 멜스펙트로그램으로부터 파형을 재구성하기 위하여，유니버설 뉴럴 보코딩(Universal neural vocoding)이 채택되었으며，이것은 LibriTTS train-clean-360 데이터셋의 전체에 대하여 250K 단계 및 배치 사 이즈 32로 학습된다. - 평가 셋팅(Evaluation Setting) (테스트 시나리오) 종래기술의 문헌 11 및 12에서와 같이 s2s 및 u2u의 두 가지 다른 시나리오에서 평가하였다. s2s(seen-to-seen)는 VCTK 학습 데이터셋에서 보여지는 화자 사이의 음성 변조를 의미하며，u2u(unseen-to- unseen)는 보이지 않는(unseen) CMU Arctic 데이터셋(종래기술 문헌 26 참조)에서 화자들 사이의 음성 변조를 의미한다. 각 시나리오에 대하여 랜덤하게 306개의 소스-타겟 화자 쌍을 선택하였다. (메트릭스(Metrics)) 화자 인식 수용율(speaker verification acceptance rate： SVAR)을 이용하여 타겟 화자 의 발음과 변조된 발음 사이의 화자 유사성을 수량화하였다. 이것은 화자 인식 장치(speaker verification system)이 변조된 발음의 화자가 타겟 화자로 수용되는 정보를 나타낸다. 즉，SVAR이 높을수 록 화자 유사성이 강해진다. 화자가 동일한지 아닌지를 결정하기 위해 즉，SVAR 측정을 위해 발 음 세그먼트의 화자 임베딩을 생성하는데 널 리 사용되는 오픈 소스 화자 인식 API 인 Resemblyer(종래기술 문헌 27 참조)가 사용되었다. 변조된 발음의 품질을 평가하기 위하여，WER(metric of word error rare)가 사용되었다. 이것은 소스 화자의 ASR을 통한 기록과 변조된 발음 사이의 차이를 나타낸다. 본 평가에서 Silero 모델(종래기술의 문헌 28 참조)이 사용되며，이것은 다양한 방언，소음 등에 강인한 엔터프 라이즈 등급의 전사(transcription)를 제공하는 오픈소스 음성 인식 API이다. (음성 변조 모델 비교) 실험에서 본 발명에 따른 Wav2vec-VC 모델과 현존하는 any-to-any VC 모델인 AutoVC(종 래기술 문헌 15 참조)，AdaIN-VC(종래기술 문헌 16 참조) 및 VQVC+(종래기술 문헌 14 참조)를 비교하였다. - 실험 결과(Experiment Results) 사전 학습된 레이어 가중치 화자/콘텐츠 가중을 위한 적절한 레이어 가중치를 찾기 위해서， SID/ASR 모델을 이용하여 학습을 수행하였다. 도 7 및 도 8은 유효 데이터셋 상에서 각 모델이 최고의 성능을 달성했을 때의 레이어 가중치 결과를 나타내는 도면이다. 도 7 및 도 8의 실험 결과는 아래의 사항을 보여준다. 화자 가중에 대해 대부분의 가중치는 wav2vec 2.0의 낮은 레이어들 특히，레이어 #1，#3 및 #4에 주어졌다. 콘텐츠 가중에 대해서는 대부분의 가중치는 중-고 레이어(mid- high layers) 즉，레이어 #8, #9 및 #10에 주어졌다. 이러한 결과들은 다른 데이터셋에 대하여 상당히 일관성이 있었다. 사전 학습된 가중치들을 이용한 검증 테스트에서，SID 및 ASR 모델을 화자 식별 정확도(speaker identification accuracy)가 98%를 초과하였으며， WER는 23% 미만이었다. 본 발명은 사전 학습되었지만 특정 임무에 대한 파인 튜닝이 되지 않은 공개적으로 이용 가능한 wav2vec 2.0 베 이스 모델을 사용하여 wav2vec 2.0 모델 기반 음성 변조 모델(Wav2vec-VC)을 생성하였다. 음성 변조 성능 결과 상기 사전 학습된 결과에 기초하여，화자/콘텐츠 가중에 대한 레이어 가중치를 설정하고 다음의 실험에서 레이 어 가중치를 사용하였다. (레이어 가중치 효과) 레이어 가중치의 영향을 확인하기 위하여， wav2vec 2.0을 사용하여 세 가지 가능한 접근 방법의 음성 변조 성능을 측정하였으며，세 가지 방법은 아래와 같다. wav2vec 2.0의 마지막 출력 표현만을 사용하는 베이스라인 방법 (single로 불림) 일정한 가중치를 가지는 모든 레이어 표현을 사용하는 평균 방 법(uniform으로 불림) 본 발명이 제안하는 사전 학습된 화자/콘텐츠 가중을 채택하는 Wav2vec-VC 방법 아래 표 1은 같은 모델 아키텍처에 대하여 세 가지 방법을 테스트했을 때 SVAR 및 WER 결과를 나타낸다. 본 발 명의 Wav2vec-VC 방법은 다른 두 가지 방법보다 일관되게 더 좋은 SVAR/WER을 보여준다. 특히，u2u 시나리오에 서 본 발명은 거의 96%의 SVAR을 제공한다. 참고로， 다른 두 가지 방법에서는 SVAR은 67% 및 87%이다.[표 1]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "[표 2]"}
{"patent_id": "10-2024-0020737", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "상기 표 2는 타겟의 화자 임베딩 및 그것의 변환된 발음 간의 평균 코인 유사성(Mean cosine similarity)을 나 타낸다. (화자/콘텐츠 임베딩의 가시화) 가시화의 목적으로 s2s 및 u2u 시나리오에서 18명의 다른 화자로부터 랜덤하게 발음을 샘플링하였다. 도 9는 화자 및 콘텐츠 임베딩의 결과를 보여준다. 구체적으로，t-SNE 가시화를 통해 표현되었고，임베딩은 다 른 화자에 대하여 다른 색상으로 표시되었다. Wav2vec-VC 화자 임베딩의 좌표들은 같은 화자의 발음들이 타이트하게 함께 군집되었고(tightly clustered together), 다른 화자들의 클러스터들은 서로 떨어져 있음을 확인할 수 있다. 한편，Wav2vec-VC의 콘텐츠 임베딩은 화자 정보를 대부분 제외하고는 화자 레벨의 클러스터는 형성하지 않았다. (참조 음성변조 모델과의 비교) 표 1은 SVAR 및 대묘모에 대해서 본 발 명의 Wav2vec-VC는 참조 음성변조 모델 을 능가하고 있음을 보여준다. 흥미롭게도 다른 두 개의 wav2vec 기반 모델 즉，single 및 uniform 모델은 비슷 하게 WER의 즉 면에서 참조 SOTA VC 모델에 대하여 우수하였다. 이것은 wav2vec 2.0 표현은 종래의 로그 멜스펙 트로그램 대비 언어학적 정보의 잠재 표현(latent representation s)을 더 잘 학습할 수 있음을 암시한다. 한편，SVAR에 대해서，AdaIN-VC가 u2u에서 uniform 방법 대비 좋은 결과를 획득하였다. 이는 AdaIN-VC가 제약 조건으로서 Kullback-Leiber 다이버전스 (divergence)를 사용하여 발음으로부터 화자 정보를 구분하는 반면， uniform 방법은 구분을 위한 명백한 제약(explicit constraint)을 가지고 있지 않기 때문이다. 본 발명의 Wav2vec-VC의 개선된 SVAR은 사전 학습된 레이어 가중치가 제약 조건으로 동작하여 화자 유사성을 향 상시키는 것을 증명한다. 상기와 같은 구조 및 방법을 통하여 본 발명의 분리 기반 음성 변조 장치 및 방법은，모델 학습시 사전학습된 가중치를 기반으로 학습하기 때문에 최적의 제안 항 가중치 조합을 찾을 필요가 없기 때문에 종래 기술 대비 학 습 시간을 대폭 줄일 수 있을 뿐만 아니라 음성 변조 성능을 크게 향상시킬 수 있다. 또한，본 발명의 음성 변조 장치 및 방법은 wav2vec 2.0의 모든 레이어의 은닉 표현에 레이어 가중치를 적용하 여 모두 활용함으로써 음성의 언어적인 정보를 최대한 활용할 수 있어 변환된 음성의 명료도가 크게 향상되는 효과를 가진다. 또한，본 발명의 음성 변조 장치 및 방법은 변환하는 소스 화자와 변환하고자하는 타겟 화자의 발화 데이터 수 가 변환 성능에 영향을 주지 않으므로 변환 대상 화자의 데이터 수가 부족한 상황에서도 고성능의 음성 변환 결 과를 발취할 수 있다. 이상과 같이 본 발명에 대해서 예시한 도면을 참조로 하여 설명하였으나, 본 명세서에 개시된 실시 예와 도면에 의해 본 발명이 한정되는 것은 아니며, 본 발명의 기술사상의 범위 내에서 통상의 기술자에 의해 다양한 변형이 이루어질 수 있음은 자명하다. 아울러 앞서 본 발명의 실시 예를 설명하면서 본 발명의 구성에 따른 작용 효과 를 명시적으로 기재하여 설명하지 않았을지라도, 해당 구성에 의해 예측 가능한 효과 또한 인정되어야 함은 당 연하다."}
{"patent_id": "10-2024-0020737", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래기술에 따른 분리 기반 음성 변조 장치의 학습 방법을 설명하는 도면이다. 도 2는 종래기술에 따른 분리 기반 음성 변조 장치의 동작 방법을 설명하는 도면이다. 도 3은 본 발명의 일 실시예에 따른 분리 기반 음성 변조 장치의 블록도이다. 도 4는 본 발명의 일실시예에 따른 wav2vec 2.0의 은닉 표현을 활용한 분리 기반 음성 변조 장치의 개략 구성도 이다. 도 5는 본 발명의 분리 기반 음성 변조 장치가 사전 학습을 통하여 레이어 가중치를 결정하는 방법을 설명하는 개념도이다. 도 6은 본 발명의 분리 기반 음성 변조 장치의 화자 인코더，콘텐츠 인코더 및 디코더의 개략 구성도이다. 도 7 및 도 8은 각각 본 발명의 분리 기반 음성 변조 장치가 사전 학습을 통하여 결정된 화자 가중(Speaker weighting) 및 콘텐츠 가중(Content weighting)을 위한 레이어 가중치의 일 예이다. 도 9는 본 발명의 분리 기반 음성 변조 장치의 화자 및 콘텐츠 임베딩의 t-SNE 가시화의 일 예를 나타내는 도면이다."}
