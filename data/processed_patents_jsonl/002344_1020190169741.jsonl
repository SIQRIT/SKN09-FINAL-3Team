{"patent_id": "10-2019-0169741", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0078060", "출원번호": "10-2019-0169741", "발명의 명칭": "사용자와의 인터랙션 중 인터럽션", "출원인": "삼성전자주식회사", "발명자": "천재민"}}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇에 있어서,구동부;카메라; 및상기 카메라를 통해 획득된 이미지에서 식별된 제1 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 상기 제1 사용자로부터 수신된 사용자 명령에 대응되는 동작을 수행하는 프로세서;를 포함하며,상기 프로세서는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽션이 발생하는지 판단하고, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 상기 로봇이 상기 인터럽션에 대한 피드백 모션을수행하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 로봇의 전면이 상기 제1 사용자를 향하고 상기 제2 사용자에 반하는(against) 상기 피드백 모션을 수행하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 로봇의 전면에 배치되며, 사용자 명령에 대응되는 터치 조작을 입력받기 위한 터치 디스플레이;를 더 포함하고,상기 프로세서는,상기 로봇의 전면이 상기 제1 사용자를 향하고 상기 로봇의 측면 또는 후면이 상기 제2 사용자를 향하는 위치로상기 로봇이 이동하는 상기 피드백 모션을 수행하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 프로세서는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 상기 제2 사용자로부터 사용자 명령이수신되는 경우, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,복수의 마이크;를 더 포함하고,상기 프로세서는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 제2 사용자의 방향을 판단하고, 공개특허 10-2021-0078060-3-상기 복수의 마이크 중 상기 제2 사용자의 방향에 대응되는 마이크로부터 사용자 음성이 수신되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,터치 디스플레이;를 더 포함하고,상기 프로세서는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 터치 디스플레이에 입력된 터치조작이 상기 제2 사용자에 의한 터치 조작인 것으로 판단되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 프로세서는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 제2 사용자가 상기 로봇과 상기제1 사용자 사이에 위치한 것으로 판단되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,터치 디스플레이;를 더 포함하고,상기 프로세서는,상기 제1 사용자로부터 사용자 명령이 수신되면, 상기 제1 사용자를 인터랙션 대상으로 설정하고, 상기 제1 사용자가 상기 인터랙션 대상으로 설정된 상기 인터랙션 세션 동안 상기 제1 사용자로부터 수신된 사용자 명령에대응되는 유저 인터페이스를 상기 터치 디스플레이의 화면에 표시하고,기설정된 제1 시간 동안, 상기 제1 사용자로부터 사용자 명령이 수신되지 않거나 또는 상기 제1 사용자가 상기로봇을 향하지 않는 경우, 상기 터치 디스플레이의 화면을 비활성화하고, 상기 로봇이 상기 제1 사용자로부터기설정된 거리 내에 위치하도록 상기 구동부를 제어하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 프로세서는,상기 터치 디스플레이의 화면이 비활성화된 상태에서, 상기 제2 사용자의 인터럽션이 발생한 것으로 판단되는경우, 상기 인터랙션 세션이 진행 중임을 알리는 화면을 상기 터치 디스플레이에 표시하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 프로세서는,상기 기설정된 제1 시간 보다 긴 기설정된 제2 시간 동안, 상기 제1 사용자로부터 사용자 명령이 수신되지 않거나 또는 상기 제1 사용자가 상기 로봇을 향하지 않는 경우, 상기 인터랙션 세션을 종료하고,상기 제1 사용자에 대한 상기 인터랙션 세션이 종료된 이후, 상기 카메라를 통해 획득된 이미지에서 식별된 제3사용자로부터 사용자 명령이 수신되면, 상기 제3 사용자를 인터랙션 대상으로 설정하는, 로봇."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "로봇의 동작 방법에 있어서,공개특허 10-2021-0078060-4-카메라를 통해 획득된 이미지에서 식별된 제1 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 상기 제1 사용자로부터 수신된 사용자 명령에 대응되는 동작을 수행하는 단계;상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽션이 발생하는지 판단하는 단계; 및상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 상기 인터럽션에 대한 피드백 모션을 수행하는 단계;를 포함하는 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 피드백 모션을 수행하는 단계는,상기 로봇의 전면이 상기 제1 사용자를 향하고 상기 제2 사용자에 반하는(against) 상기 피드백 모션을 수행하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 로봇은, 상기 로봇의 전면에 배치되며, 사용자 명령에 대응되는 터치 조작을 입력받기 위한 터치 디스플레이를 포함하고,상기 피드백 모션을 수행하는 단계는,상기 로봇의 전면이 상기 제1 사용자를 향하고 상기 로봇의 측면 또는 후면이 상기 제2 사용자를 향하는 위치로이동하는 상기 피드백 모션을 수행하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 인터럽션이 발생하는지 판단하는 단계는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 상기 제2 사용자로부터 사용자 명령이수신되는 경우, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 로봇은, 복수의 마이크를 포함하고,상기 인터럽션이 발생하는지 판단하는 단계는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 제2 사용자의 방향을 판단하고,상기 복수의 마이크 중 상기 제2 사용자의 방향에 대응되는 마이크로부터 사용자 음성이 수신되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 로봇은 터치 디스플레이를 포함하고,상기 인터럽션이 발생하는지 판단하는 단계는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 터치 디스플레이에 입력된 터치조작이 상기 제2 사용자에 의한 터치 조작인 것으로 판단되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는, 동작 방법.공개특허 10-2021-0078060-5-청구항 17 제11항에 있어서,상기 인터럽션이 발생하는지 판단하는 단계는,상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지를 기반으로 상기 제2 사용자가 상기 로봇과 상기제1 사용자 사이에 위치한 것으로 판단되면, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단하는, 동작방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항에 있어서,상기 로봇은, 터치 디스플레이를 포함하고,상기 동작 방법은,상기 제1 사용자로부터 사용자 명령이 수신되면, 상기 제1 사용자를 인터랙션 대상으로 설정하는 단계;상기 제1 사용자가 상기 인터랙션 대상으로 설정된 상기 인터랙션 세션 동안 상기 제1 사용자로부터 수신된 사용자 명령에 대응되는 유저 인터페이스를 상기 터치 디스플레이의 화면에 표시하는 단계; 및기설정된 제1 시간 동안, 상기 제1 사용자로부터 사용자 명령이 수신되지 않거나 또는 상기 제1 사용자가 상기로봇을 향하지 않는 경우, 상기 터치 디스플레이의 화면을 비활성화하는 단계;를 더 포함하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 터치 디스플레이의 화면이 비활성화된 상태에서, 상기 제2 사용자의 인터럽션이 발생한 것으로 판단되는경우, 상기 인터랙션 세션이 진행 중임을 알리는 화면을 상기 터치 디스플레이에 표시하는 단계;를 더포함하는, 동작 방법."}
{"patent_id": "10-2019-0169741", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서,상기 기설정된 제1 시간 보다 긴 기설정된 제2 시간 동안, 상기 제1 사용자로부터 사용자 명령이 수신되지 않거나 또는 상기 제1 사용자가 상기 로봇을 향하지 않는 경우, 상기 인터랙션 세션을 종료하는 단계; 및상기 제1 사용자에 대한 상기 인터랙션 세션이 종료된 이후, 상기 카메라를 통해 획득된 이미지에서 식별된 제3사용자로부터 사용자 명령이 수신되면, 상기 제3 사용자를 인터랙션 대상으로 설정하는 단계;를 더 포함하는,동작 방법."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "로봇이 개시된다. 본 로봇은, 구동부, 카메라, 카메라를 통해 획득된 이미지에서 식별된 제1 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 제1 사용자로부터 수신된 사용자 명령에 대응되는 동작을 수행하는 프로세 서를 포함하며, 프로세서는, 인터랙션 세션 동안, 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽션이 발생하는지 판단하고, 제2 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 로봇이 인터럽션에 대 한 피드백 모션을 수행하도록 구동부를 제어할 수 있다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 사용자 명령을 기반으로 적어도 하나의 동작을 수행하는 로봇에 관한 것이다. 보다 상세하게는, 특정 사용자와의 인터랙션 중 다른 사용자의 인터럽션을 방어하는 로봇에 관한 것이다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래 불특정 다수의 사용자를 대상으로 인터랙션 기반 서비스를 제공하는 로봇은, 특정 사용자와의 인터랙션 중 다른 사용자의 개입(ex. attention hijacking)에 취약했다. 특히, 공공 장소에서 사용되거나 및/또는 다수의 사용자에게 이용될 수 있는 로봇의 경우, 특정 사용자와의 인 터랙션 세션이 명확히 종료되기도 전에 다른 사용자와의 인터랙션이 불가피하게 시작될 수도 있다는 문제가 있 었다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 일 사용자와의 인터랙션 세션 동안 다른 사용자의 인터럽션을 방지하는 전자 장치를 제공한다. 본 개시는, 일 사용자와의 인터랙션 세션 동안 다른 사용자에 의한 인터럽션이 발생한 경우, 현재 진행 중인 인 터랙션 세션을 유지하면서 인터럽션을 방지할 수 있는 피드백 모션을 제공하는 전자 장치를 제공한다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 로봇은, 구동부, 카메라, 상기 카메라를 통해 획득된 이미지에서 식별된 제1 사용 자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 상기 제1 사용자로부터 수신된 사용자 명령에 대응되는 동 작을 수행하는 프로세서를 포함하며, 상기 프로세서는, 상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽션이 발생하는지 판단하고, 상기 제2 사용자에 의한 인터럽션이 발 생한 것으로 판단되면, 상기 로봇이 상기 인터럽션에 대한 피드백 모션을 수행하도록 상기 구동부를 제어한다. 본 개시의 일 실시 예에 따른 로봇의 동작 방법은, 카메라를 통해 획득된 이미지에서 식별된 제1 사용자가 인터 랙션 대상으로 설정된 인터랙션 세션 동안, 상기 제1 사용자로부터 수신된 사용자 명령에 대응되는 동작을 수행 하는 단계, 상기 인터랙션 세션 동안, 상기 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽 션이 발생하는지 판단하는 단계, 상기 제2 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 상기 인터럽션에 대한 피드백 모션을 수행하는 단계를 포함한다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따른 로봇은, 특정 사용자와의 인터랙션 세션이 진행 중인 경우 다른 사용자의 인터럽션을 방지한 결 과, 인터랙션 세션을 통해 서비스를 제공받던 기존 사용자에게 마지막까지 정확한 서비스를 제공할 수 있다는 효과가 있다. 본 개시에 따른 로봇은, 인터랙션 세션이 진행 중인 상태에서 인터랙션의 대상이 아닌 사용자로부터 사용자 명 령 등이 수신된 경우, 피드백 모션 및/또는 다양한 출력을 통해 이미 다른 사용자의 인터랙션 세션이 진행 중이 라는 점을 해당 사용자에게 인지시킬 수 있다는 효과가 있다."}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기 술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소를 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 도 1a 내지 도 1c는 본 개시에 따른 로봇의 동작을 개략적으로 설명하기 위한 도면들이다. 도 1a를 참조하면, 로봇은 카메라 및/또는 센서 등을 통해 식별된 사용자를 인터랙션 대상으로 설정하 고, 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안 사용자에게 하나 이상의 서비스를 제공할 수 있다. 구체적으로로, 로봇은 터치, 음성, 모션 등 다양한 형태로 수신되는 사용자의 명령을 기반으 로 하나 이상의 동작을 수행할 수 있다. 한편, 도 1b를 참조하면, 사용자와의 인터랙션 세션 중에, 다른 사용자로부터 로봇으로 사용자 명 령이 입력될 수 있다. 이 경우, 로봇은 사용자와의 인터랙션에 대한 다른 사용자의 인터럽션이 발 생한 것으로 판단할 수 있다. 이때, 도 1c를 참조하면, 로봇은 사용자의 인터럽션을 회피 또는 방지하기 위한 피드백 모션을 수행할 수 있다. 피드백 모션은 인터랙션 세션 중 다른 사용자의 인터럽션에 대한 거부 내지는 외면을 표현하기 위한 로봇의 모션을 의미한다. 구체적으로, 도 1c를 참조하면, 로봇은 사용자를 피해서 이동할 수 있으며, 사용자에게 더 가까워 질 수 있다. 여기서, 로봇의 측면 또는 후면이 사용자를 향하고 로봇의 전면이 사용자를 향 하도록 로봇의 위치가 변경될 수도 있다. 이렇듯, 본 개시의 로봇은 인터랙션 대상이 아닌 다른 사용자의 인터럽션에 대해 피드백 모션을 제공함으 로써, 인터랙션 대상인 사용자와의 인터랙션이 용이한 상태를 유지하면서도 다른 사용자의 인터럽션을 외면하는의도를 비언어적으로 표현할 수 있다. 이하 도면들을 통해, 본 개시에 따른 로봇의 구성 및 동작을 보다 상세하게 설명한다. 도 2a는 본 개시의 일 실시 예에 따른 로봇의 구성을 설명하기 위한 블록도이다. 도 2a를 참조하면, 로봇 은 구동부, 카메라, 프로세서 등을 포함할 수 있다. 구동부는 로봇의 모션 내지는 이동을 제어하기 위한 구성이다. 이를 위해, 구동부는 로봇 의 이동 수단을 제어할 수 있다. 이밖에, 구동부는 로봇의 물리적 움직임을 구현하는 기계적 구성과 전기적으로 연결되어 해당 구성을 구동/제어할 수 있다. 도 1a의 로봇을 예로 들면, 구동부는 도 1a의 로봇의 바디 아래에 있는 바퀴, 도 1a의 로봇 의 바디 위에 부착된 헤드의 회전을 제어하는 기계적 구성 등을 제어할 수 있다. 이밖에, 만약 로봇에 팔 이나 다리와 같은 별도 구성이 포함된 경우라면, 구동부는 팔 및 다리의 움직임을 구동하도록 구현될 수도 있다. 카메라는 로봇의 주변의 이미지를 획득하기 위한 구성으로, RGB 카메라, 뎁스 카메라, RGB-D(Depth) 카메라 등을 포함할 수 있다. 카메라는 스테레오 카메라 또는 3D 카메라로 구현될 수도 있다. 로봇은 카메라를 통해 로봇 주변에 대한 RGB 이미지뿐만 아니라 뎁스 이미지도 획득할 수 있다. 로봇에는 복수의 카메라가 포함될 수 있다. 프로세서는 로봇에 포함된 구성들을 제어하기 위한 구성이다. 프로세서는 CPU(Central Processing Unit), AP(Application Processor) 등과 같은 범용 프로세서, GPU(Graphic Processing Unit), VPU(Vision Processing Unit) 등과 같은 그래픽 전용 프로세서 또는 NPU(Neural Processing Unit)와 같은 인공 지능 전용 프로세서 등으로 구현될 수 있다. 또한, 프로세서는 SRAM 등의 휘발성 메모리를 포함할 수 있다. 프로세서는 카메라를 통해 획득된 이미지에서 식별된 사용자를 인터랙션의 대상(: 상대방)으로 설정 할 수 있다. 인터랙션은 로봇과 사용자 간의 의사소통을 의미할 수 있다. 사용자와 인터랙션을 수행하기 위해, 로봇 은 사용자 명령을 수신하기 위한 사용자 입력부, 다양한 정보를 출력하기 위한 디스플레이 및 스피커 등을 구비할 수 있으나 이에 한정되지는 않는다. 일 예로, 카메라를 통해 획득된 이미지에서 일 사용자가 식별되면, 프로세서는 해당 사용자를 인터랙 션의 대상으로 설정할 수 있다. 이때, 프로세서는 객체를 식별하도록 학습된 인공지능 모델에 이미지를 입 력함으로써 해당 사용자를 식별할 수도 있다. 일 예로, 사용자로부터 적어도 하나의 사용자 명령이 수신되면, 카메라를 통해 획득된 이미지를 통해 해당 사용자를 식별할 수 있다. 이때, 프로세서는 해당 사용자를 인터랙션의 대상으로 설정할 수도 있다. 사용 자 명령은, 터치 조작, 음성, 모션 등 다양한 형태로 로봇에 수신될 수 있다. 이때, 프로세서는 센서(라이다 센서, 초음파 센서 등) 및 카메라를 통해 사용자 명령을 입력한 사용 자를 검출 및 식별할 수 있다. 구체적으로, 사용자 명령이 수신된 경우, 센서를 통해 근접한 객체를 검출하고, 검출된 객체의 방향에서 촬영된 이미지를 통해 해당 객체(: 사용자)를 식별할 수 있다. 예로, 로봇의 터치 디스플레이에 터치 조작이 입력되는 경우, 카메라를 통해 획득된 이미지를 통해 사용자의 손이 터치 디스플레이를 터치하는 모션이 감지된다면, 해당 사용자를 인터랙션 대상으로 설정할 수 있 다. 예로, 로봇의 마이크에 음성이 수신되는 경우, 음성이 수신된 마이크가 향하는 방향을 기준으로 기설 정된 시야각 범위 내에 대해 촬영된 이미지를 이용하여 사용자를 식별하고, 해당 사용자를 인터랙션 대상으로 설정할 수 있다. 한편, 카메라를 통해 획득된 이미지를 통해 일 사용자가 식별된 이후, 식별된 사용자로부터 적어도 하나의 사용자 명령이 수신되는 경우에 해당 사용자를 인터랙션의 대상으로 설정할 수도 있다. 이렇듯, 사용자가 인터랙션의 대상으로 설정되면, 해당 사용자와의 인터랙션 세션이 시작될 수 있다. 인터랙션 세션은, 로봇이 인터랙션의 대상으로 설정된 사용자로부터 적어도 하나의 사용자 명령을 수신하 고, 수신된 사용자 명령을 기반으로 적어도 하나의 동작을 수행하기 위한 기간을 의미한다. 인터랙션 세션은,로봇이 사용자와 대화 내지는 정보를 주고받기 위한 기간을 의미할 수도 있다. 따라서, 인터랙션 세션 동안, 프로세서는 인터랙션 대상으로 설정된 사용자로부터 수신된 사용자 명령에 대응되는 동작을 수행할 수 있다. 그 결과, 인터랙션 대상인 사용자에게 하나 이상의 서비스가 제공될 수 있다. 서비스는, 로봇의 기능에 따라 달라질 수 있다. 예를 들어, 로봇이 안내 로봇이라면, 사용자 명령에 따라 요청된 위치에 대한 정보를 디스플레이하거나 또는 해당 위치로 로봇이 직접 이동하는 서비스를 제공 할 수 있다. 서비스의 종류는 로봇의 용도에 따라 다양할 수 있기 때문에 상술한 예에 한정되는 것은 아니 다. 인터랙션 세션 동안, 프로세서는 로봇의 전면이 사용자를 향하도록 구동부를 제어할 수 있다. 로봇의 전면에는 사용자 명령에 대응되는 터치 조작을 입력받기 위한 터치 디스플레이가 구비될 수 있다. 또한, 프로세서는 인터랙션 세션 동안 로봇의 위치가 사용자로부터 기설정된 거리 내에 위치하도록 구동부를 제어할 수도 있다. 본 개시의 일 실시 예에 따른 프로세서는, 인터랙션 세션 동안, 카메라를 통해 획득된 이미지에서 식 별된 다른 사용자에 의한 인터럽션이 발생하는지 판단할 수 있다. 인터럽션은 현재 진행 중인 인터랙션 세션의 인터랙션 및 서비스 제공에 방해 또는 저해가 되는 모든 요소를 의 미할 수 있다. 구체적으로, 일 사용자와의 인터랙션 세션 중 다른 사용자의 사용자 명령 또는 끼어듬 등이 인터 럽션의 예들에 해당할 수 있으나, 이것들에만 한정되는 것은 아니다. 다른 사용자는, 인터랙션의 대상으로 설정된 사용자가 아닌 사용자를 의미할 수 있다. 프로세서는 카메라 를 이용하여 인터랙션의 대상인 사용자와 인터랙션의 대상이 아닌 사용자를 구분할 수 있다. 이때, 프로세 서는 인터랙션의 대상인 사용자의 얼굴이 포함된 이미지로부터 특징 정보를 출력할 수 있으며, 특징 정보 를 기반으로, 인터랙션의 대상인 사용자의 얼굴을 실시간으로 인식할 수 있다. 이때, 특징 정보를 비교하여 얼 굴 인식을 수행하도록 학습된 인공지능 모델이 이용될 수 있다. 그 결과, 카메라를 통해 획득된 이미지 내 에서 인터랙션의 대상인 사용자와 인터랙션의 대상이 아닌 사용자가 구분될 수 있다. 또한, 프로세서는 인터랙션의 대상인 사용자의 위치를 센서(라이다 센서, 초음파 센서 등)를 통해 추적할 수 있으며, 인터랙션의 대상인 사용자가 아닌 다른 사용자 역시 센서를 통해 검출할 수 있다. 인터랙션 세션 동안 다른 사용자로부터 적어도 하나의 사용자 명령이 수신되는 경우, 프로세서는 다른 사 용자에 의해 인터럽션이 발생한 것으로 판단할 수 있다. 관련하여, 도 3 내지 도 5를 통해 구체적인 예들을 후 술한다. 인터랙션 세션 동안, 인터랙션 대상으로 설정된 사용자와 로봇 사이에 다른 사용자가 위치한 것으로 판단 된 경우에도, 프로세서는 인터럽션이 발생한 것으로 판단할 수 있다. 관련하여, 도 6을 통해 구체적인 예 를 후술한다. 이밖에도, 인터랙션의 대상인 사용자와의 인터랙션의 효율성을 저해하는 다양한 이벤트가 인터럽션에 포함되는 것으로 정의될 수 있으며, 정의된 인터럽션에 따라 로봇의 구성 내지는 프로세서의 동작이 통상의 기 술 상식 내에서 변형될 수 있음은 물론이다. 만약, 다른 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 프로세서는 로봇이 인터럽션에 대한 피드백 모션을 수행하도록 구동부를 제어할 수 있다. 피드백 모션은 인터랙션 세션 중 다른 사용자의 인터럽션에 대한 거부 내지는 외면을 표현하기 위한 로봇 의 모션을 의미한다. 피드백 모션은, 현재 인터랙션의 대상으로 설정된 사용자에 대한 집중을 표현하는 모션을 의미할 수도 있다. 예를 들어, 인터랙션 세션 동안 다른 사용자의 인터럽션이 발생한 경우, 프로세서는 로봇의 전면이 인터럭션의 대상으로 설정된 사용자를 향하고, 인터럽션을 발생시킨 상술한 다른 사용자에 반하는(against) 피 드백 모션을 수행하도록 구동부를 제어할 수 있다. 구체적으로, 프로세서는 로봇의 전면이 인터럭션의 대상으로 설정된 사용자를 향하고 로봇의 측면 또는 후 면이 인터럽션을 발생시킨 사용자를 향하는 위치로 로봇이 이동하는 피드백 모션을 수행하도록 구동부 를 제어할 수 있다.또한, 프로세서는 로봇이 인터럽션 발생 전보다 인터럭션의 대상으로 설정된 사용자에게 더 가까이 다가가도록 구동부를 제어할 수 있다. 이때, 프로세서는 로봇이 인터럽션을 발생시킨 사용자를 피해서 주행하도록 구동부를 제어할 수도 있다. 이밖에도, 프로세서는 인터럽션에 대한 거부를 표현하기 위한 다양한 피드백 모션을 수행하도록 구동부 를 제어할 수 있다. 그 결과, 다른 사용자의 인터럽션에도 불구하고, 로봇이 인터랙션의 대상으로 설정된 사용자의 사용자 명 령을 수신하고 사용자에게 서비스를 제공할 수 있는 최적의 상황이 유지될 수 있다. 도 2b는 본 개시의 일 실시 예에 따른 로봇의 기능적 구성을 설명하기 위한 블록도이다. 도 2b에 도시된 각 모 듈들은 로봇의 메모리에 소프트웨어 형태로 저장되어 프로세서에 의해 선택적으로 실행될 수 있다. 도 2b에 도시된 각 모듈들은 로봇상에 회로 형태로 구현되어 프로세서의 제어를 받을 수도 있다. 또 한, 각 모듈들은 소프트웨어 및 하드웨어가 결합된 형태로 구현될 수도 있다. 도 2b를 참조하면, 로봇은 인터랙션 세션 관리 모듈, 서비스 모듈, 인터럽션 판단 모듈, 인터럽션 피드백 모듈 등을 포함할 수 있다. 인터랙션 세션 관리 모듈은, 적어도 한 명의 사용자가 인터랙션의 대상으로 설정된 경우, 해당 사용자에 대한 인터랙션 세션의 시작/유지/종료를 설정하기 위한 구성이다. 일 예로, 카메라를 통해 획득된 이미지에서 일 사용자가 식별된 경우, 인터랙션 관리 모듈은 해당 사 용자와의 인터랙션 세션을 시작할 수 있다. 이후, 인터랙션 세션의 종료에 대한 기설정된 이벤트가 발생하면, 인터랙션 관리 모듈은 인터랙션 세션을 종료할 수 있다. 종료에 대한 기설정된 이벤트는, 해당 사용자가 요청한 서비스의 제공이 종료되는 경우, 해당 사용자로부터 일 정 시간 이상 동안 사용자 명령이 수신되지 않는 경우 등을 포함할 수 있다. 또한, 종료와 관련된 이벤트는, 해당 사용자가 일정 시간 이상 동안 로봇을 바라보지 않는 것으로 식별된 경우를 포함할 수 있다. 이 경우, 프로세서는 카메라를 통해 식별된 이미지를 기반으로 사용자의 시 선 방향을 인식할 수 있다. 서비스 모듈은 수신된 사용자 명령에 따라 적어도 하나의 동작을 수행하기 위한 모듈이다. 서비스 모듈 의 구성 내지 동작은 로봇의 기능에 따라 달라질 수 있다. 구체적으로, 서비스 모듈은, 인터랙션 세션 동안, 인터랙션의 대상을 설정된 사용자로부터 수신되는 사용 자 명령을 기반으로 적어도 하나의 동작을 수행하도록 로봇을 제어할 수 있다. 인터럽션 판단 모듈은 인터랙션 세션 동안 인터랙션의 대상으로 설정된 사용자 외의 다른 사용자의 인터럽 션이 발생하는지 여부를 판단하기 위한 모듈이다. 프로세서의 동작으로 상술하였듯, 인터럽션 판단 모듈은 인터랙션 세션 동안 수신된 다른 사용자의 사용자 명령 또는 인터랙션 세션 동안 다른 사용자의 위치 등을 기반으로, 인터럽션의 발생 여부를 판단할 수 있다. 구체적인 실시 예는 도 3 내지 도 6을 통해 후술한다. 한편, 인터럽션 피드백 모듈은 인터랙션 세션 도중에 인터럽션이 발생한 경우, 로봇이 상술한 피드백 모션을 수행하도록 구동부를 제어하기 위한 모듈이다. 일 예로, 인터럽션 피드백 모듈은 로봇의 메모리상에 기저장된 모션 정보를 기반으로 구동부를 제어할 수 있다. 이하 도 3 내지 도 6을 통해 로봇이 인터럽션의 발생 여부를 판단하는 구체적인 실시 예들을 설명한다. 도 3은 본 개시의 로봇이, 카메라를 통해 획득된 이미지를 기반으로 사용자 명령을 입력한 주체를 파악하여, 인 터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면이다. 도 3을 참조하면, 로봇은 터치 조작을 입력받도록 구현된 터치 디스플레이 및 카메라를 포함할 수 있다. 도 3은, 인터랙션의 대상이 사용자로 설정된 인터랙션 세션이 진행 중인 상황을 가정한다. 도 3을 참조하면, 터치 조작이 입력된 경우, 프로세서는 카메라를 통해 획득된 이미지를 이용하 여 터치 조작의 주체를 판단할 수 있다.구체적으로, 프로세서는 이미지로부터 사용자 및 사용자를 각각 식별하고, 수신된 터치 조 작이 사용자 및 사용자 중 누구의 터치 조작인지 식별할 수 있다. 도 3을 참조하면, 프로세서는 카메라를 통해 획득된 이미지를 기반으로 사용자 및 사용자 각각의 신체 일부분(311, 312)이 로봇(또는 로봇의 터치 디스플레이)에 가까워진다는 점을 식별 할 수 있다. 구체적으로, 프로세서는 카메라를 통해 획득된 RGB 이미지 및 RGB 이미지에 대응되는 픽셀 별 뎁스 정보를 기반으로, 이미지 내에서 사용자를 구성하는 영역에 포함되며 로봇의 터치 디스플레이 에 접근하는 일부분을 식별할 수 있다. 이 경우, 프로세서는 사용자로부터 터치 조작이 입 력된 것으로 식별할 수 있다. 한편, 프로세서는 센서(라이다 센서, 초음파 센서 등)를 통해 수신된 센싱 데이터를 기반으로, 사용자 및/또는 사용자의 신체 일부분이 로봇에 가까워진다는 점을 식별할 수도 있다. 센싱 데이터에 따라, 터치 조작이 입력된 때에 사용자의 신체 일부분이 터치 디스플레이에 근접한 것으로 식별되는 경우, 프로세서는 사용자로부터 터치 조작이 입력된 것으로 식별할 수 있다. 이렇듯, 입력된 터치 조작이 인터랙션의 대상으로 설정되지 않은 사용자의 터치 조작이라는 점이 식별되면, 프로세서는 인터럽션이 발생한 것으로 식별할 수 있다. 한편, 도 3에서는 로봇의 전면에 위치한 카메라로부터 획득된 이미지만이 도시되었으나, 그밖에 로봇의 다양한 측면에 부착된 하나 이상의 카메라를 통해 사용자가 식별될 수 있음은 물론이다. 도 4a 내지 도 4b는 본 개시의 로봇이 터치 디스플레이를 통해 수신된 터치 조작의 입력 모양을 기반으로 인터 럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면들이다. 도 4a 및 도 4b는, 도 3의 로봇의 터치 디스플레이에 입력된 터치의 영역들을 도시한다. 도 4a를 참조하면, 일 사용자가 인터랙션의 대상으로 설정된 상태에서, 해당 사용자로부터 복수의 터치 조작 (touch 1, 2, 3)이 터치 디스플레이에 입력될 수 있다. 도 4a의 복수의 터치 조작은, 카메라를 통해 획득된 이미지를 기반으로, 인터랙션의 대상인 사용자만이 로 봇에 근접해있다고 판단된 상태에서 입력된 터치 조작들일 수 있다. 이때, 프로세서는 각 터치들의 영역들(410, 420, 430)의 장축들(411, 421, 431)을 식별할 수 있다. 그리 고, 프로세서는 장축들(411, 421, 431)의 방향(각도)을 식별할 수 있다. 이후, 인터랙션의 대상이 아닌 다른 사용자도 로봇에 근접할 수 있다. 이때, 프로세서는 카메라(12 0)를 통해 획득된 이미지를 기반으로, 인터랙션의 대상인 사용자 외에 다른 사용자도 로봇에 근접한 것으 로 판단할 수 있다. 이 경우, 프로세서는 인터랙션의 대상으로 설정된 사용자의 장축들(411, 421, 431) 각각의 방향으로부터 기설정된 각도 이상 벗어나는 터치 조작에 대해서는, 인터랙션 대상이 아닌 다른 사용자의 터치 조작인 것으로 식별할 수 있다. 관련하여, 도 4b는 카메라를 통해 획득된 이미지를 기반으로, 인터랙션의 대상인 사용자에 더하여 인터랙 션의 대상이 아닌 다른 사용자 역시 로봇에 근접한 것으로 식별된 경우를 가정한다. 이 경우, 프로세서는 수신된 터치 조작들(touch 4, 5) 각각의 영역들(440, 450)을 식별하고, 영역들(440, 450) 각각의 장축들(441, 451)의 방향(각도)을 식별할 수 있다. 도 4b를 참조하면, touch 4의 영역의 장축의 방향이, 인터랙션의 대상인 사용자의 터치 영역들(410, 420, 430)의 장축들(411, 421, 431)의 방향과 90도 가량의 큰 차이가 나므로, 프로세서는 touch 4를 인터 랙션의 대상이 아닌 다른 사용자의 터치 조작으로 식별할 수 있다. 그 결과, 프로세서는 인터럽션이 발생했다고 판단할 수 있다. 이때, 프로세서는 인터랙션의 대상인 사용자의 터치 조작들(touch 1, 2, 3, 5)을 기반으로 적어도 하나의 기능을 수행하되, 다른 사용자의 터치 조작(touch 4)을 기반으로는 어떠한 기능도 수행하지 않을 수 있다. 즉, 인터럽션을 발생시킨 다른 사용자의 터치 조작은 차단될 수 있다.도 5는 본 개시의 로봇이 복수의 마이크를 통해 수신된 음성을 기반으로 인터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면이다. 도 5는 로봇 및 사용자들(510, 520)을 위에서 바라본 상황을 도시한다. 도 5에 도시된 로봇은 로봇 의 단면도로 이해될 수 있다. 도 5는 사용자가 인터랙션의 대상으로 설정된 인터랙션 세션이 진행 중인 상황을 전제로 한다. 또한, 도 5 는 프로세서가 카메라 및/또는 센서(라이다 센서, 초음파 센서 등)를 통해 사용자들(510, 520)의 방 향을 식별한 상황을 전제로 한다. 도 5를 참조하면, 로봇은 복수의 마이크(160-1, 2, 3, 4, 5)를 포함할 수 있다. 다만, 마이크의 배치 상태 나 수가 도 5의 실시 예에 한정되지 않음은 물론이다. 도 5를 참조하면, 프로세서는 복수의 마이크(160-1, 2, 3, 4, 5) 중 사용자의 방향에 대응되는 마이 크(160 - 2, 3)로부터 음성이 수신되면, 사용자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 구체적으로, 복수의 마이크 중(160-1, 2, 3, 4, 5) 중 사용자 방향에 위치한 마이크들(160-2, 3)을 통해 가장 크게 수신되는 음성이 있는 것으로 식별되는 경우, 프로세서는 해당 음성이 사용자의 음성인 것으로 식별할 수 있다. 또는, 인터랙션의 대상인 사용자를 향하는 마이크들(160-1, 5)을 통해 사용자의 음성이 수신되 고 있는 상태에서 사용자의 음성이 마이크(160-2) 내지는 마이크(160-3)를 통해 동시에 수신되는 경 우, 프로세서는 사용자 외에 사용자의 음성도 수신되고 있음을 식별할 수 있다. 또는, 프로세서는 사용자의 음성으로부터 적어도 하나의 특징 정보를 추출하여 사용자의 음성에 대한 모델을 생성할 수 있다. 이후, 생성된 모델과 매칭되지 않는 특징 정보를 가지는 음성이 수신 되면, 프로세서는 사용자가 아닌 다른 사용자의 음성이 수신되고 있음을 식별할 수 있다. 이렇듯, 인터랙션 대상이 아닌 사용자로부터 음성이 수신되는 경우, 프로세서는 사용자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 이때, 프로세서는 인터랙션의 대상인 사용자의 음성을 기반으로 적어도 하나의 기능을 수행하되, 사용자의 음성을 기반으로는 어떠한 기능도 수행하지 않을 수 있다. 즉, 인터럽션을 발생 시킨 사용자의 음성 입력은 차단될 수 있다. 도 6은 본 개시의 로봇이 카메라를 통해 획득된 이미지를 기반으로 인터랙션 대상이 아닌 사용자의 위치를 판단 하여 인터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면이다. 도 6은 인터랙션의 대상이 사용자로 설정된 인터랙션 세션이 진행 중인 상황을 가정한다. 도 6을 참조하면, 프로세서는 카메라를 통해 획득된 이미지를 기반으로, 인터랙션 대상이 아닌 사용자가 로봇과 사용자 사이에 위치한 것으로 판단할 수 있다. 이 경우, 프로세서는 사용 자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 한편, 만약 로봇이 카메라 내지는 모션 센서를 기반으로 인식된 사용자 모션을 기반으로 적어도 하나 의 기능을 수행하도록 구현된 경우, 프로세서는 인터랙션 대상인 사용자의 모션에 대응되는 적어도 하나의 기능을 수행하되, 인터랙션의 대상이 아닌 사용자의 모션에 대응하여서는 어떠한 기능도 수행하지 않을 수도 있다. 한편, 도 3, 도 4a 내지 도 4b, 도 5, 도 6의 실시 예는 서로 독립적으로 구현될 수도 있으나, 그 중 두 가지 이상의 실시 예가 함께 수행된 결과 인터럽션 발생 여부가 판단될 수 있음은 물론이다. 도 7a 내지 도 7c는 본 개시의 로봇이 인터랙션 세션을 시작한 이후 사용자의 상황 및 인터럽션 발생 여부에 따 라 동작하는 구체적인 일 예를 설명하기 도면들이다. 도 7a를 참조하면, 사용자로부터 사용자 명령이 수신되면, 프로세서는 사용자를 인터랙션 대상 으로 설정할 수 있다. 도 7a의 경우, 사용자 명령은 터치 조작이지만, 이밖에 음성이나 모션 등도 물론 가능하 다. 또한, 도 7a와 달리, 사용자의 사용자 명령 수신 여부와 무관하게, 카메라를 통해 획득된 이미지에서 사용자가 식별되기만 하면 곧바로 사용자가 인터랙션 대상으로 설정될 수도 있다. 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 프로세서는 사용자로부터 수신된 사용 자 명령에 대응되는 유저 인터페이스를 터치 디스플레이의 화면에 표시하거나 또는 청각적으로 제공(ex; 스피커 이용)할 수 있다. 다만, 도 7b와 같이, 인터랙션 세션 중 인터랙션 대상인 사용자가 잠시 한 눈을 팔거나 다른 개인 용무를 수행하는 경우가 발생할 수 있다. 관련하여, 인터랙션 세션 동안, 로봇은 사용자 명령이 수신되지 않는 상태가 유지되는 시간 및/또는 사용 자의 시선 방향이 로봇을 향하지 않는 상태가 유지되는 시간 등을 실시간으로 식별할 수 있다. 시선 방향을 식별하기 위한 구체적인 예로, 프로세서는 카메라를 통해 획득된 이미지(ex. RGB 이미지)를 이용하여 사용자의 머리를 인식할 수 있다. 그리고, 프로세서는 머리에서 눈, 코, 귀, 턱 등 얼 굴의 방향과 관련된 지점들을 식별할 수 있다. 그리고, 프로세서는 식별된 지점들 간의 방향 관계를 기반 으로 사용자의 시선 방향을 인식할 수 있다. 이때, 프로세서는 얼굴 이미지로부터 시선 방향을 검출하도록 학습된 별도의 인공지능 모델을 이용할 수도 있다. 도 7b를 참조하면, 기설정된 제1 시간 동안, 사용자로부터 사용자 명령이 수신되지 않거나 또는 사용자 가 로봇을 향하지 않는 경우, 프로세서는 터치 디스플레이의 화면을 비활성화할 수 있다. 사용자가 로봇을 바라보지 않는 상황에서 불필요하게 유저 인터페이스가 디스플레이되어 있는 경우, 사용자의 명령 내지는 신상에 대한 정보가 의도치 않게 다른 사용자에게 공개될 수 있다는 문제가 있다. 따라서, 프로세서는 사용자가 로봇을 향하지 않는 경우 화면을 비활성화함으로써 불필요한 개인 정보의 공개를 방지할 수 있다. 이때, 프로세서는 로봇이 사용자로부터 기설정된 거리 내에 위치하도록 구동부를 제 어할 수 있다. 그 결과, 비록 사용자는 잠시 다른 용무를 수행하더라도 아직 사용자와의 인터랙션이 진행 중이라는 점이 사용자 및 다른 사용자들에게 명확히 표현될 수 있다. 터치 디스플레이의 화면이 비활성화된 상태에서, 인터랙션 대상인 사용자가 아닌 다른 사용자에 의해 인터럽션이 발생한 경우, 프로세서는 피드백 모션을 수행하도록 구동부를 제어할 수 있음은 물론, 인 터랙션 세션이 진행 중임을 알리는 화면을 터치 디스플레이에 표시할 수도 있다. 관련하여, 도 7c는 도 7b의 상태에서 다른 사용자에 의해 인터럽션이 발생한 상황을 가정한다. 도 7c를 참조하면, 인터럽션이 발생하면, 프로세서는 계속해서 로봇이 사용자와의 거리(705 이 내)를 유지하도록 구동부를 제어하면서, 피드백 모션을 수행할 수 있다. 아울러, 프로세서는 '현재 다른 사용자가 사용 중입니다'라는 안내문이 적힌 유저 인터페이스를 표시하도록 터치 디스플레이를 제어 할 수 있다. 또한, 프로세서는 '현재 다른 사용자가 사용 중입니다'라는 음성을 출력하도록 로봇 의 스피커를 제어할 수도 있다. 한편, 상술한 기설정된 제1 시간 보다 긴 기설정된 제2 시간 동안, 인터랙션의 대상인 사용자로부터 사용 자 명령이 수신되지 않거나 또는 사용자가 로봇을 향하지 않는 경우, 프로세서는 사용자와 의 인터랙션 세션을 종료할 수 있다. 이 경우, 프로세서는 카메라를 통해 획득된 이미지에서 식별된 또다른 사용자를 인터랙션 대상으로 설정하여 새로운 인터랙션 세션을 시작할 수 있다. 구체적으로, 식별된 사용자로부터 사용자 명령이 수신되면, 프로세서는 해당 사용자를 새로운 인터랙션 대상으로 설정할 수 있다. 도 8은 본 개시의 다양한 실시 예들에 따른 로봇의 상세한 구성을 설명하기 위한 블록도이다. 도 8을 참조하면, 로봇은 구동부, 카메라 및 프로세서 외에 센서, 디스플레이, 마이크, 스피커, 메모리 등을 더 포함할 수 있다. 센서는 로봇의 주변에 대한 다양한 정보를 획득하기 위한 구성이다. 센서는 라이다 센서, 초음 파 센서, 가속도 센서, 자이로 센서 등을 포함할 수 있으며, 이밖에도 다양한 센서들이 로봇에 구비될 수 있다. 일 예로, 프로세서는 라이다 센서 또는 초음파 센서를 통해 수신된 센싱 데이터를 기반으로 근접한 위치에 존재하는 사용자의 존재를 검출하고, 검출된 사용자의 방향으로 카메라를 통해 촬영된 이미지로부터 해당 사용자를 식별할 수 있다. 이때, 프로세서는 사용자의 얼굴을 포함하는 이미지의 특징 정보를 기반으로 해당 사용자를 추적할 수 있 다. 구체적으로, 프로세서는 사용자의 얼굴을 포함하는 이미지의 특징 정보를 추출한 뒤, 추후 연속적으로 획득되는 이미지내 하나 이상의 영역을 특징 정보와 비교하여 해당 사용자의 위치 및 방향을 연속적으로 추적할 수 있다. 특징 정보는, 얼굴 인식을 위해 이미지로부터 특징 정보를 추출 내지는 다른 이미지와 매핑하도록 학 습된 하나 이상의 인공지능 모델의 출력에 따라 정의될 수 있다. 디스플레이는 다양한 컨텐츠 내지는 유저 인터페이스를 시각적으로 출력하기 위한 구성이다. 디스플레이 는 사용자 명령을 터치 형태로 수신하기 위해 터치 디스플레이 형태로 구현될 수 있다. 로봇에는 하나 이상의 디스플레이가 구비될 수 있다. 일 예로, 로봇이 바퀴 등의 이동 수단이 구비된 바디 및 바디상에 부착된 헤드로 구성되는 경우, 헤드의 전면 및 바디의 전면에 각각 디스플레이가 구비될 수 있으나 이에 한정되지 않는다. 마이크는 외부로부터 오디오 신호를 입력받기 위한 구성이다. 로봇은 하나 이상의 마이크를 구비할 수 있다. 로봇은 마이크를 통해 수신되는 사용자 음성을 기반으로 적어도 하나의 동작을 수행할 수도 있다. 구 체적으로, 프로세서는 음향 모델 및 언어 모델 등을 이용하여 사용자 음성을 텍스트로 변환한 뒤, 변환된 텍스트를 기반으로 사용자 음성에 대응되는 사용자 명령을 식별할 수 있다. 그리고, 프로세서는 식별된 사 용자 명령을 기반으로 동작할 수 있다. 스피커는 오디오 신호를 출력하기 위한 구성이다. 프로세서는 다양한 정보 내지는 유저 인터페이스를 음성 형태로 출력하도록 스피커를 제어할 수 있다. 메모리는 로봇의 구성요소들의 전반적인 동작을 제어하기 위한 운영체제(OS: Operating System) 및 로봇의 구성요소와 관련된 적어도 하나의 인스트럭션 또는 데이터를 저장하기 위한 구성이다. 프로세서는 메모리에 저장된 적어도 하나의 인스트럭션을 실행함으로써 상술할 다양한 실시 예들에 따른 동작을 수행할 수 있다. 메모리는 ROM, 플래시 메모리 등의 비휘발성 메모리를 포함할 수 있으며, DRAM 등으로 구성된 휘발성 메모 리를 포함할 수 있다. 또한, 메모리는 하드 디스크, SSD(Solid state drive) 등을 포함할 수도 있다. 도 8을 참조하면, 메모리는 상술한 인터랙션 세션 관리 모듈, 서비스 모듈, 인터럽션 판단 모듈 , 인터럽션 피드백 모듈 외에도 객체 인식 모듈, 구동 제어 모듈 등을 더 포함할 수 있다. 객체 인식 모듈은 카메라를 통해 획득된 이미지로부터 적어도 하나의 객체(ex. 사용자/사람)를 식별 하기 위한 구성이다. 객체 인식 모듈은 객체를 식별하도록 학습된 인공지능 모델을 이용할 수 있다. 구동 제어 모듈은 구동부를 통해 로봇의 모션을 제어하기 위한 구성이다. 구동 제어 모듈 은 인터랙션 대상인 사용자에게 제공되는 서비스에 포함되는 모션 내지는 상술한 피드백 모션이 로봇에 의 해 수행되도록 구동부를 제어할 수 있다. 이하 도 9 내지 도 10을 통해서는 본 개시에 따른 로봇의 동작 방법을 설명한다. 도 9는 본 개시의 일 실시 예에 따른 로봇의 동작 방법을 설명하기 위한 블록도이다. 도 9를 참조하면, 본 동작 방법은, 제1 사용자가 인터랙션 대상으로 설정된 인터랙션 세션 동안, 제1 사용자로 부터 수신된 사용자 명령에 대응되는 동작을 수행할 수 있다(S910). 이때, 제1 사용자는 카메라를 통해 획득된 이미지로부터 식별될 수 있다. 제1 사용자와의 인터랙션 세션 동안, 본 동작 방법은, 카메라를 통해 획득된 이미지에서 식별된 제2 사용자에 의한 인터럽션이 발생하는지 판단할 수 있다(S920). 일 예로, 카메라를 통해 획득된 이미지에서 식별된 제2 사용자로부터 사용자 명령이 수신되는 경우, 제2 사용자 에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 일 예로, 로봇이 복수의 마이크를 포함하는 경우, 카메라를 통해 획득된 이미지를 기반으로 제2 사용자의 방향 을 판단하고, 복수의 마이크 중 제2 사용자의 방향에 대응되는 마이크로부터 사용자 음성이 수신되면, 제2 사용 자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 일 예로, 로봇이 터치 디스플레이를 포함하는 경우, 카메라를 통해 획득된 이미지를 기반으로 터치 디스플레이 에 입력된 터치 조작이 제2 사용자에 의한 터치 조작인 것으로 판단되면, 제2 사용자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 일 예로, 카메라를 통해 획득된 이미지를 기반으로 제2 사용자가 로봇과 제1 사용자 사이에 위치한 것으로 판단 되면, 제2 사용자에 의한 인터럽션이 발생한 것으로 판단할 수 있다. 그리고, 제2 사용자에 의한 인터럽션이 발생한 것으로 판단되면, 본 동작 방법은 인터럽션에 대한 피드백 모션 을 수행할 수 있다(S930). 구체적으로, 로봇의 전면이 제1 사용자를 향하고 제2 사용자에 반하는(against) 피드백 모션을 수행할 수 있다. 로봇의 전면에 배치되며 사용자 명령에 대응되는 터치 조작을 입력받기 위한 터치 디스플레이가 로봇에 구비된 경우, 로봇의 전면이 제1 사용자를 향하고 로봇의 측면 또는 후면이 제2 사용자를 향하는 위치로 이동하는 피드 백 모션을 수행할 수도 있다. 도 10은 본 개시의 로봇이 인터랙션 세션 동안 인터럽션에 대한 피드백 모션을 수행하는 일 예를 설명하기 위한 알고리즘이다. 도 10을 참조하면, 카메라 및/또는 센서를 통해 식별된 사용자로부터 사용자 명령이 수신된 경우(S1010 - Y), 해당 사용자를 인터랙션 대상으로 설정하고 해당 사용자와의 인터랙션 세션을 시작할 수 있다(S1020). 인터랙션 세션 동안, 사용자로부터 수신되는 사용자 명령에 따라 동작을 수행할 수 있다(S1030). 구체적인 예로, 사용자로부터 수신된 사용자 명령에 대응되는 유저 인터페이스를 터치 디스플레이의 화면에 표시할 수 있 다. 인터랙션 세션이 유지되는 동안, 인터랙션 대상이 아닌 다른 사용자의 인터럽션이 발생했는지 여부를 판단할 수 있다(S1060). 만약, 인터럽션이 발생한 경우(S1060 - Y), 인터럽션을 거부 내지는 외면하기 위한 피드백 모션을 수행할 수 있다(S1070). 한편, 기설정된 제1 시간 동안, 인터랙션의 대상인 사용자로부터 사용자 명령이 수신되지 않거나 또는 사용자가 로봇을 향하지 않는 경우, 터치 디스플레이의 화면을 비활성화할 수도 있다. 여기서, 인터랙션 대상이 아닌 다 른 사용자의 인터럽션이 발생한 것으로 판단되는 경우, 인터랙션 세션이 진행 중임을 알리는 화면을 터치 디스 플레이에 표시할 수 있다. 인터랙션 종료 이벤트가 발생한 경우(S1040), 인터랙션 세션을 종료할 수 있다(S1050). 인터랙션 종료 이벤트는, 사용자 명령에 대응되는 동작이 종료된 경우, 사용자 명령이 일정 시간 이상 수신되지 않는 경우, 사 용자의 시선 방향이 일정 시간 이상 로봇을 향하지 않는 경우 등을 포함할 수 있다. 예로, 상술한 기설정된 제1 시간 보다 긴 기설정된 제2 시간 동안, 인터랙션의 대상인 사용자로부터 사용자 명 령이 수신되지 않거나 또는 사용자가 로봇을 향하지 않는 경우, 인터랙션 세션을 종료할 수 있다. 인터랙션 세션이 종료된 이후, 본 동작 방법은, 카메라를 통해 획득된 이미지에서 식별된 새로운 사용자로부터 사용자 명령이 수신되면, 새로운 사용자를 인터랙션 대상으로 설정하고 새로운 인터랙션 세션을 시작할 수 있다. 도 9 내지 도 10을 통해 상술한 동작 방법은, 도 2 및 도 8을 통해 도시 및 설명한 로봇을 통해 구현될 수 있다. 다만, 도 9 내지 도 10을 통해 상술한 동작 방법 중 적어도 일부는 로봇과 통신 가능한 외부 장치(ex. 서 버 장치, 단말 장치 등)를 통해 수행될 수도 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(Programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processor), 제어기(controller), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessor), 기타 기능 수행을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구 현에 의하면 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 로봇에서의 처리동작을 수행하기 위한 컴퓨터 명령어 (computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium)에 저 장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 로봇의 처리 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체가 아니 라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 구체적으로는, 상 술한 다양한 어플리케이션 또는 프로그램들은 CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등과 같은 비일시적 판독 가능 매체에 저장되어 제공될 수 있다. 또한, 이상에서는 본 발명의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 발명은 상술한 특정의 실시"}
{"patent_id": "10-2019-0169741", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "예에 한정되지 아니하며, 청구범위에서 청구하는 본 발명의 요지를 벗어남이 없이 당해 발명이 속하는 기술분야 에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 발명의 기술적 사상이나 전망으로부터 개별적으로 이해돼서는 안 될 것이다."}
{"patent_id": "10-2019-0169741", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a 내지 도 1c는 본 개시에 따른 로봇의 동작을 개략적으로 설명하기 위한 도면들, 도 2a는 본 개시의 일 실시 예에 따른 로봇의 구성을 설명하기 위한 블록도, 도 2b는 본 개시의 일 실시 예에 따른 로봇의 기능적 구성을 설명하기 위한 블록도, 도 3은 본 개시의 로봇이 카메라를 통해 획득된 이미지를 기반으로 사용자 명령의 주체를 파악하여 인터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면, 도 4a 내지 도 4b는 본 개시의 로봇이 터치 디스플레이를 통해 수신된 터치 조작의 입력 모양을 기반으로 인터 럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면들, 도 5는 본 개시의 로봇이 복수의 마이크를 통해 수신된 음성을 기반으로 인터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면, 도 6은 본 개시의 로봇이 카메라를 통해 획득된 이미지를 기반으로 인터랙션 대상이 아닌 사용자의 위치를 판단 하여 인터럽션의 발생 여부를 판단하는 일 예를 설명하기 위한 도면, 도 7a 내지 도 7c는 본 개시의 로봇이 인터랙션 세션을 시작한 이후 사용자의 상황 및 인터럽션 발생 여부에 따 라 동작하는 구체적인 일 예를 설명하기 도면들, 도 8은 본 개시의 다양한 실시 예들에 따른 로봇의 상세한 구성을 설명하기 위한 블록도, 도 9는 본 개시의 일 실시 예에 따른 로봇의 동작 방법을 설명하기 위한 블록도, 그리고 도 10은 본 개시의 로봇이 인터랙션 세션 동안 인터럽션에 대한 피드백 모션을 수행하는 일 예를 설명하기 위한 알고리즘이다."}
