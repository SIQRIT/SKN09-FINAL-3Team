{"patent_id": "10-2019-0087640", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0010139", "출원번호": "10-2019-0087640", "발명의 명칭": "차량의 주변 상황 확인 방법", "출원인": "엘지전자 주식회사", "발명자": "신동헌"}}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "차량 내부의 연결 가능한 적어도 하나의 클라이언트를 확인하는 단계;상기 확인된 클라이언트와 통신을 수행하여, 상기 클라이언트의 성능의 결정 기준에 기반하여 상기 클라이언트의 성능을 확인하는 단계;상기 확인된 클라이언트의 성능에 기반하여 상기 클라이언트에 대응하는 알고리즘을 할당하는 단계; 및차량의 센서를 통해 영상 데이터를 획득하는 경우, 상기 알고리즘에 따른 상기영상 데이터의 처리 결과로 상기클라이언트로부터 획득된 정보를 기반으로 상기 차량의 주변 상황을 확인하는 단계를 포함하는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 주변 상황을 확인하는 단계는,상기 획득된 영상 데이터를 상기 클라이언트에 전송하고, 상기 클라이언트에 할당된 상기 알고리즘에 의한 상기영상 데이터의 인식 결과를 기반으로 상기 차량의 주변 상황을 확인하는 단계를 포함하는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 주변 상황을 확인하는 단계는,복수의 알고리즘에 의한 상기 영상 데이터 인식 결과가 서로 상이한 경우, 상기 영상 데이터에서 관심 영역에대해 상기 복수의 알고리즘과 다른 알고리즘에 의한 인식 결과를 추가로 고려하여 상기 차량의 주변 상황을 확인하는 것을포함하는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 복수의 알고리즘은, 상기 영상 데이터에 나타난 객체를 확인하기 위한 인식 율이 서로 다른,차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서,상기 관심 영역은, 상기 영상 데이터에 대해 상기 복수의 알고리즘이 서로 다른 인식 결과를 나타내는 영역을포함하는,공개특허 10-2021-0010139-3-차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 클라이언트의 성능의 결정 기준은, 상기 클라이언트의 전송 지연 및 처리 시간 중 적어도 하나를포함하는,차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 차량의 주변 상황을 확인하는 단계는,상기 클라이언트의 전송 지연과 처리 시간이 상기 센싱 주기 보다 긴 경우, 영상 데이터의 인식 결과를 보정하여 상기 차량의 주변 상황을 확인하는 것을포함하는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 차량의 주변 상황을 결정하는 단계는,상기 클라이언트의 성능이 미리 설정된 기준 이하인 경우, 상기 클라이언트로 관심 영역에 대한 정보를 전송하는 단계; 및상기 클라이언트로 전송된 영상 데이터의 상기 관심 영역에 대해 상기 클라이언트에 대응하는 알고리즘의 인식결과를 기반으로 상기 차량의 주변 상황을 확인하는 것을포함하는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 관심 영역은, 상기 영상 데이터에서 상기 차량의 진행 방향을 기반으로 결정되는, 차량의 주변 상황 확인 방법."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서,상기 차량의 주변 상황을 확인하는 단계는,상기 차량 주변의 환경에 적합한 알고리즘의 처리 결과에 상기 알고리즘에 대응하는 가중치를 적용하여, 상기차량의 주변 상황을 확인하는 것을 포함하는, 차량의 주변 상황 확인 방법.공개특허 10-2021-0010139-4-청구항 11 제1항내지 제10항의 방법을 컴퓨터에서 실행시키기 위한 인스트럭션을 기록한 컴퓨터로 읽을 수 있는 비휘발성기록매체."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "영상 데이터를 획득하는 센서;차량 내부의 클라이언트와 수행된 통신에 따라 결정 기준에 기반하여 확인된 상기 클라이언트의 성능에 기반하여 상기 클라이언트에 대응하는 알고리즘을 할당하는 컨트롤러; 및상기 알고리즘에 따른 상기 센서를 통해 획득된 영상 데이터의 처리 결과로 상기 클라이언트로부터 획득된 정보를 기반으로 상기 차량의 주변 상황을 확인하는 프로세서;를 포함하는, 차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 프로세서는,상기 차량의 센서를 통해 획득된 영상 데이터가 상기 클라이언트로 전송된 경우, 상기 클라이언트의 알고리즘에의한 상기 영상 데이터의 인식 결과를 기반으로 상기 차량의 주변 상황을 확인하는,차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 프로세서는,복수의 알고리즘에 의한 상기 영상 데이터 인식 결과가 서로 상이한 경우, 상기 영상 데이터에서 관심 영역에대해 상기 복수의 알고리즘과 다른 알고리즘에 의한 인식 결과를 추가로 고려하여 상기 차량의 주변 상황을 확인하는, 차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 복수의 알고리즘은, 상기 영상 데이터에 나타난 객체를 확인하기 위한 인식 율이 서로 다른,차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서,상기 관심 영역은, 상기 영상 데이터에 대해 상기 복수의 알고리즘이 서로 다른 인식 결과를 나타내는 영역을포함하는,공개특허 10-2021-0010139-5-차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 클라이언트의 성능의 결정 기준은, 상기 클라이언트의 전송 지연 및 처리 시간 중 적어도 하나를포함하는,차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 프로세서는,상기 클라이언트의 전송 지연과 처리 시간이 상기 센싱 주기 보다 긴 경우, 영상 데이터의 인식 결과를 보정하여 상기 차량의 주변 상황을 확인하는,차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17항에 있어서,상기 프로세서는,상기 클라이언트의 성능이 미리 설정된 기준 이하인 경우, 상기 클라이언트로 전송된 영상 데이터의 상기 관심영역에 대해 상기 클라이언트에 대응하는 상기 알고리즘의 인식 결과를 기반으로 상기 차량의 주변 상황을 확인하는,차량."}
{"patent_id": "10-2019-0087640", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제12항에 있어서,상기 프로세서는,상기 차량 주변의 환경에 적합한 알고리즘의 처리 결과에 상기 알고리즘에 대응하는 가중치를 적용하여, 상기차량의 주변 상황을 확인하는,차량."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "차량의 주변 상황 결정 방법이 개시된다. 차량의 주변 상황 결정 방법은 차량 내부의 연결 가능한 적어도 하나의 클라이언트를 확인하는 단계, 확인된 클라이언트와 통신을 수행하여 클라이언트의 성능의 결정 기준에 기반하여 클라이언트의 성능을 확인하는 단계, 확인된 클라이언트의 성능에 기반하여 상기 클라이언트에 대응하는 알고리 즘을 할당하는 단계, 차량의 센서를 통해 영상 데이터를 획득하는 경우 알고리즘에 따른 영상 데이터의 처리 결 과로 상기 클라이언트로부터 획득된 정보를 기반으로 차량의 주변 상황을 확인하는 단계를 포함할 수 있다. 본 발명의 차량은 인공지능(Artificial Intelligence) 모듈, 드론(Unmmaned Aerial Vehicle, UAV), 로봇, 증강 현 실(Augmented Reality, AR) 장치, 가상 현실(Virtual Reality, VR) 장치, 5G서비스와 관련된 장치 등과 연계될 수 있다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 연산장치가 차량의 주변 상황을 확인하고, 확인 결과에 따라 관련된 정보를 제공하는 방법에 관한 것으로, 구체적으로 차량과 클라이언트 간의 영상 데이터를 공유하여 차량의 주변 상황을 확인하는 방법에 관한 것이다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "차량에 장착된 ADAS(Advanced Driver Assistance System) 모듈의 성능에 따라 구동 가능한 알고리즘의 종류가 상이할 수 있다. 또는 기 장착된 ADAS 모듈의 성능이 좋지 않아 구동 가능한 알고리즘의 종류가 제한될 수 있다. 만약 차량과 클라이언트 간의 영상 데이터를 공유하여 클라이언트에 의한 영상 데이터의 인식 결과를 기 반으로 차량의 주변 상황을 결정할 수 있다면, 차량에 장착된 ADAS 모듈의 성능이 좋지 않더라도 차량은 주변 상황을 정확히 인식할 수 있다. 따라서, 차량과 클라이언트 간의 영상 데이터를 공유하여 차량의 주변 상황을 확인하는 기술이 필요하다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "개시된 실시 예들은 차량과 클라이언트 간의 영상 데이터를 공유하여 클라이언트에 의한 영상 데이터의 인식 결 과를 기반으로 차량의 주변 상황을 확인할 수 있는 기술을 개시한다. 본 실시 예가 이루고자 하는 기술적 과제 는 상기된 바와 같은 기술적 과제들로 한정되지 않으며, 이하의 실시 예 들로부터 또 다른 기술적 과제들이 유 추될 수 있다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시 예에 따른, 차량의 주변 상황 결정 방법은 차량 내부의 연결 가능한 적어도 하나의 클라이언 트를 확인하는 단계, 확인된 클라이언트와 통신을 수행하여 클라이언트의 성능의 결정 기준에 기반하여 클라이 언트의 성능을 확인하는 단계, 확인된 클라이언트의 성능에 기반하여 상기 클라이언트에 대응하는 알고리즘을 할당하는 단계, 차량의 센서를 통해 영상 데이터를 획득하는 경우 알고리즘에 따른 영상 데이터의 처리 결과로 상기 클라이언트로부터 획득된 정보를 기반으로 차량의 주변 상황을 확인하는 단계를 포함할 수 있다. 본 발명의 다른 일 실시 예에 따른, 차량은 영상 데이터를 획득하는 센서, 차량 내부의 클라이언트와 수행된 통 신에 따라 결정 기준에 기반하여 확인된 클라이언트의 성능에 기반하여 알고리즘을 할당하는 컨트롤러, 및 상기 알고리즘에 따른 상기 센서를 통해 획득된 영상 데이터의 처리 결과로 상기 클라이언트로부터 획득된 정보를 기 반으로 차량의 주변 상황을 확인하는 프로세서를 포함할 수 있다. 기타 실시 예들의 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세서의 실시 예에 따르면 아래와 같은 효과가 하나 혹은 그 이상 있다. 첫째, 차량에 장착된 ADAS 모듈의 성능이 좋지 않더라도, 차량과 연결 가능한 클라이언트 간의 영상 데이터를 공유하여 클라이언트에 의한 영상 데이터의 인식 결과를 기반으로 차량의 주변 상황을 결정할 수 있는 효과가 있다. 둘째, 차량과 연결 가능한 클라이언트의 성능이 좋지 않더라도, 영상 데이터의 일부인 관심 영역에 대한 클라이 언트의 인식 결과를 기반으로 차량의 주변 상황을 결정할 수 있는 효과가 있다. 셋째, 클라이언트의 성능에 기반하여 알고리즘을 할당하므로, 알고리즘에 의한 인식 결과를 종합하여 차량의 주 변 상황을 보다 정확하게 결정할 수 있는 효과가 있다. 넷째, 차량의 센서가 차량 주변의 환경을 센싱한 경우, 환경에 적합한 알고리즘의 처리 결과에 가중치를 적용하 여 차량의 주변 상황을 보다 정확하게 결정할 수 있는 효과가 있다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "발명의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 청구범위의 기재 로부터 당해 기술 분야의 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 실시 예를 첨부된 도면을 참조하여 상세하게 설명한다. 실시 예를 설명함에 있어서 본 발명이 속하는 기술 분야에 익히 알려져 있고 본 발명과 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 발명의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되었다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 각 도면에서 동일한 또는 대응하는 구성요소에 는 동일한 참조 번호를 부여하였다. 본 발명의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 발명은 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시 예들은 본 발명의 개시가 완전하도록 하고, 본 발명이"}
{"patent_id": "10-2019-0087640", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "속하는 기술분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 발명은 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 이 때, 처리 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행 될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가 능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들 을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트 럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재 되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하 다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 이 때, 본 실시 예에서 사용되는 '~부'라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구성요소를 의미하며, '~부'는 어떤 역할들을 수행한다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아 니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재 생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저 들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공 되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 또한, 본 명세서에서 인공 지능(AI, Artificial Intelligence)은 인공적인 지능 또는 이를 만들 수 있는 방법론 을 연구하는 분야를 의미하며, 머신 러닝(기계 학습, Machine Learning)은 인공 지능 분야에서 다루는 다양한 문제를 정의하고 그것을 해결하는 방법론을 연구하는 분야를 의미한다. 머신 러닝은 어떠한 작업에 대하여 꾸준 한 경험을 통해 그 작업에 대한 성능을 높이는 알고리즘으로 정의하기도 한다. 인공 신경망(ANN: Artificial Neural Network)은 머신 러닝에서 사용되는 모델로써, 시냅스의 결합으로 네트워 크를 형성한 인공 뉴런(노드)들로 구성되는, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경 망은 다른 레이어의 뉴런들 사이의 연결 패턴, 모델 파라미터를 갱신하는 학습 과정, 출력 값을 생성하는 활성 화 함수(Activation Function)에 의해 정의될 수 있다. 인공 신경망은 입력 층(Input Layer), 출력 층(Output Layer), 그리고 선택적으로 하나 이상의 은닉 층(Hidden Layer)을 포함할 수 있다. 각 층은 하나 이상의 뉴런을 포함하고, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅 스를 포함할 수 있다. 인공 신경망에서 각 뉴런은 시냅스를 통해 입력되는 입력 신호들, 가중치, 편향에 대한 활성 함수의 함수 값을 출력할 수 있다. 모델 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편향 등이 포함된 다. 그리고, 하이퍼 파라미터는 머신 러닝 알고리즘에서 학습 전에 설정되어야 하는 파라미터를 의미하며, 학습 률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함된다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수 는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표로 이용될 수 있다. 머신 러닝은 학습 방식에 따라 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 분류할 수 있다. 지도 학습은 학습 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시키는 방법을 의미하며, 레이블이란 학습 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과 값)을 의미할 수 있다. 비지도 학습은 학습 데이터에 대한 레이블이 주어지지 않는 상태에서 인공 신경망을 학습시키 는 방법을 의미할 수 있다. 강화 학습은 어떤 환경 안에서 정의된 에이전트가 각 상태에서 누적 보상을 최대화 하는 행동 혹은 행동 순서를 선택하도록 학습시키는 학습 방법을 의미할 수 있다. 인공 신경망 중에서 복수의 은닉 층을 포함하는 심층 신경망(DNN: Deep Neural Network)으로 구현되는 머신 러 닝을 딥 러닝(심층 학습, Deep Learning)이라 부르기도 하며, 딥 러닝은 머신 러닝의 일부이다. 이하에서, 머신 러닝은 딥 러닝을 포함하는 의미로 사용된다. 또한, 본 명세서에서 자율 주행은 스스로 주행하는 기술을 의미하며, 자율 주행 차량은 사용자의 조작 없이 또 는 사용자의 최소한의 조작으로 주행하는 차량(Vehicle)을 의미한다. 예컨대, 자율 주행에는 주행중인 차선을 유지하는 기술, 어댑티브 크루즈 컨트롤과 같이 속도를 자동으로 조절 하는 기술, 정해진 경로를 따라 자동으로 주행하는 기술, 목적지가 설정되면 자동으로 경로를 설정하여 주행하 는 기술 등이 모두 포함될 수 있다. 여기서 차량은 내연 기관 만을 구비하는 차량, 내연 기관과 전기 모터를 함께 구비하는 하이브리드 차량, 그리 고 전기 모터만을 구비하는 전기 차량을 모두 포괄하며, 자동차뿐만 아니라 기차, 오토바이 등을 포함할 수 있다. 이때, 자율 주행 차량은 자율 주행 기능을 가진 로봇으로 볼 수 있다. 도 1은 본 발명의 일 실시 예에 따른 AI 장치를 나타낸다. AI 장치는 TV, 프로젝터, 휴대폰, 스마트폰, 데스크 탑 컴퓨터, 노트북, 디지털방송용 단말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 태블릿 PC, 웨어러블 장치, 셋 톱 박스(STB), DMB 수신기, 라디오, 세탁기, 냉장고, 데스크 탑 컴퓨터, 디지털 사이니지, 로봇, 차량 등과 같은, 고정형 기기 또는 이동 가능한 기기 등으로 구현될 수 있다. 도 1을 참조하면, 단말기는 통신부, 입력부, 러닝 프로세서, 센싱부, 출력부, 메모리 및 프로세서 등을 포함할 수 있다. 통신부는 유무선 통신 기술을 이용하여 다른 AI 장치(100a 내지 100e)나 AI 서버 등의 외부 장치들과 데이터를 송수신할 수 있다. 예컨대, 통신부는 외부 장치들과 센서 정보, 사용자 입력, 학습 모델, 제어 신호 등을 송수신할 수 있다. 이때, 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블 루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있다. 입력부는 다양한 종류의 데이터를 획득할 수 있다. 이때, 입력부는 영상 신호 입력을 위한 카메라, 오디오 신호를 수신하기 위한 마이크로폰, 사용자로부터 정보를 입력 받기 위한 사용자 입력부 등을 포함할 수 있다. 여기서, 카메라나 마이크로폰을 센서로 취급하여, 카메라나 마이크로 폰 로부터 획득한 신호를 센싱 데이터 또는 센서 정보라고 할 수도 있다. 입력부는 모델 학습을 위한 학습 데이터 및 학습 모델을 이용하여 출력을 획득할 때 사용될 입력 데이터 등을 획득할 수 있다. 입력부는 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 프로세서 또는 러닝 프로세서는 입력 데이터에 대하여 전처리로써 입력 특징점(input feature)을 추출할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학습 된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 어떠한 동작을 수행하기 위한 판단의 기초로 이용될 수 있다. 이때, 러닝 프로세서는 AI 서버의 러닝 프로세서과 함께 AI 프로세싱을 수행할 수 있다. 이때, 러닝 프로세서는 AI 장치에 통합되거나 구현된 메모리를 포함할 수 있다. 또는, 러닝 프로세서 는 메모리, AI 장치에 직접 결합된 외부 메모리 또는 외부 장치에서 유지되는 메모리를 사용하 여 구현될 수도 있다. 센싱부는 다양한 센서들을 이용하여 AI 장치 내부 정보, AI 장치의 주변 환경 정보 및 사용자 정보 중 적어도 하나를 획득할 수 있다. 이때, 센싱부에 포함되는 센서에는 근접 센서, 조도 센서, 가속도 센서, 자기 센서, 자이로 센서, 관성 센 서, RGB 센서, IR 센서, 지문 인식 센서, 초음파 센서, 광 센서, 마이크로폰, 라이다, 레이더 등이 있다. 출력부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시킬 수 있다. 이때, 출력부에는 시각 정보를 출력하는 디스플레이부, 청각 정보를 출력하는 스피커, 촉각 정보를 출력하 는 햅틱 모듈 등이 포함될 수 있다. 메모리는 AI 장치의 다양한 기능을 지원하는 데이터를 저장할 수 있다. 예컨대, 메모리는 입력 부에서 획득한 입력 데이터, 학습 데이터, 학습 모델, 학습 히스토리 등을 저장할 수 있다. 메모리 는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타 입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, RandomAccess Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 프로세서는 데이터 분석 알고리즘 또는 머신 러닝 알고리즘을 사용하여 결정되거나 생성된 정보에 기초하 여, AI 장치의 적어도 하나의 실행 가능한 동작을 결정할 수 있다. 그리고, 프로세서는 AI 장치(10 0)의 구성 요소들을 제어하여 결정된 동작을 수행할 수 있다. 이를 위해, 프로세서는 러닝 프로세서 또는 메모리의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 상기 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행 하도록 AI 장치의 구성 요소들을 제어할 수 있다. 이때, 프로세서는 결정된 동작을 수행하기 위하여 외부 장치의 연계가 필요한 경우, 해당 외부 장치를 제 어하기 위한 제어 신호를 생성하고, 생성한 제어 신호를 해당 외부 장치에 전송할 수 있다. 프로세서는 사용자 입력에 대하여 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 사용자의 요구 사 항을 결정할 수 있다. 이때, 프로세서는 음성 입력을 문자열로 변환하기 위한 STT(Speech To Text) 엔진 또는 자연어의 의도 정 보를 획득하기 위한 자연어 처리(NLP: Natural Language Processing) 엔진 중에서 적어도 하나 이상을 이용하여, 사용자 입력에 상응하는 의도 정보를 획득할 수 있다. 이때, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 적어도 일부가 머신 러닝 알고리즘에 따라 학습된 인 공 신경망으로 구성될 수 있다. 그리고, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 러닝 프로세서 에 의해 학습된 것이나, AI 서버의 러닝 프로세서에 의해 학습된 것이거나, 또는 이들의 분산 처리에 의해 학습된 것일 수 있다. 프로세서는 AI 장치의 동작 내용이나 동작에 대한 사용자의 피드백 등을 포함하는 이력 정보를 수집 하여 메모리 또는 러닝 프로세서에 저장하거나, AI 서버 등의 외부 장치에 전송할 수 있다. 수 집된 이력 정보는 학습 모델을 갱신하는데 이용될 수 있다. 프로세서는 메모리에 저장된 응용 프로그램을 구동하기 위하여, AI 장치의 구성 요소들 중 적어 도 일부를 제어할 수 있다. 나아가, 프로세서는 상기 응용 프로그램의 구동을 위하여, AI 장치에 포 함된 구성 요소들 중 둘 이상을 서로 조합하여 동작시킬 수 있다. 도 2는 본 발명의 일 실시 예에 따른 AI 서버를 나타낸다. 도 2를 참조하면, AI 서버는 머신 러닝 알고리즘을 이용하여 인공 신경망을 학습시키거나 학습된 인공 신 경망을 이용하는 장치를 의미할 수 있다. 여기서, AI 서버는 복수의 서버들로 구성되어 분산 처리를 수행 할 수도 있고, 5G 네트워크로 정의될 수 있다. 이때, AI 서버는 AI 장치의 일부의 구성으로 포함되어, AI 프로세싱 중 적어도 일부를 함께 수행할 수도 있다. AI 서버는 통신부, 메모리, 러닝 프로세서 및 프로세서 등을 포함할 수 있다. 통신부는 AI 장치 등의 외부 장치와 데이터를 송수신할 수 있다. 메모리는 모델 저장부를 포함할 수 있다. 모델 저장부는 러닝 프로세서을 통하여 학습 중 인 또는 학습된 모델(또는 인공 신경망, 231a)을 저장할 수 있다. 러닝 프로세서는 학습 데이터를 이용하여 인공 신경망(231a)을 학습시킬 수 있다. 학습 모델은 인공 신경 망의 AI 서버에 탑재된 상태에서 이용되거나, AI 장치 등의 외부 장치에 탑재되어 이용될 수도 있다. 학습 모델은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 학습 모델의 일부 또는 전부가 소프트웨어로 구현되는 경우 학습 모델을 구성하는 하나 이상의 명령어(instruction)는 메모리 에 저장될 수 있다. 프로세서는 학습 모델을 이용하여 새로운 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기 초한 응답이나 제어 명령을 생성할 수 있다.도 3은 본 발명의 일 실시 예에 따른 AI 시스템을 나타낸다. 도 3을 참조하면, AI 시스템은 AI 서버, 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스마트폰 (100d) 또는 가전(100e) 중에서 적어도 하나 이상이 클라우드 네트워크와 연결된다. 여기서, AI 기술이 적 용된 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스마트폰(100d) 또는 가전(100e) 등을 AI 장치(100a 내지 100e)라 칭할 수 있다. 클라우드 네트워크는 클라우드 컴퓨팅 인프라의 일부를 구성하거나 클라우드 컴퓨팅 인프라 안에 존재하는 네트워크를 의미할 수 있다. 여기서, 클라우드 네트워크는 3G 네트워크, 4G 또는 LTE(Long Term Evolution) 네트워크 또는 5G 네트워크 등을 이용하여 구성될 수 있다. 즉, AI 시스템을 구성하는 각 장치들(100a 내지 100e, 200)은 클라우드 네트워크를 통해 서로 연결될 수 있다. 특히, 각 장치들(100a 내지 100e, 200)은 기지국을 통해서 서로 통신할 수도 있지만, 기지국을 통하지 않 고 직접 서로 통신할 수도 있다. AI 서버는 AI 프로세싱을 수행하는 서버와 빅 데이터에 대한 연산을 수행하는 서버를 포함할 수 있다. AI 서버는 AI 시스템을 구성하는 AI 장치들인 로봇(100a), 자율 주행 차량(100b), XR 장치(100c), 스 마트폰(100d) 또는 가전(100e) 중에서 적어도 하나 이상과 클라우드 네트워크을 통하여 연결되고, 연결된 AI 장치들(100a 내지 100e)의 AI 프로세싱을 적어도 일부를 도울 수 있다. 이때, AI 서버는 AI 장치(100a 내지 100e)를 대신하여 머신 러닝 알고리즘에 따라 인공 신경망을 학습시킬 수 있고, 학습 모델을 직접 저장하거나 AI 장치(100a 내지 100e)에 전송할 수 있다. 이때, AI 서버는 AI 장치(100a 내지 100e)로부터 입력 데이터를 수신하고, 학습 모델을 이용하여 수신한 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기초한 응답이나 제어 명령을 생성하여 AI 장치 (100a 내지 100e)로 전송할 수 있다. 또는, AI 장치(100a 내지 100e)는 직접 학습 모델을 이용하여 입력 데이터에 대하여 결과 값을 추론하고, 추론 한 결과 값에 기초한 응답이나 제어 명령을 생성할 수도 있다. 이하에서는, 상술한 기술이 적용되는 AI 장치(100a 내지 100e)의 다양한 실시 예들을 설명한다. 여기서, 도 3에 도시된 AI 장치(100a 내지 100e)는 도 1에 도시된 AI 장치의 구체적인 실시 예로 볼 수 있다. 본 실시 예에 따른 자율 주행 차량(100b)은 AI 기술이 적용되어, 이동형 로봇, 차량, 무인 비행체 등으로 구현 될 수 있다. 자율 주행 차량(100b)은 자율 주행 기능을 제어하기 위한 자율 주행 제어 모듈을 포함할 수 있고, 자율 주행 제 어 모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩을 의미할 수 있다. 자율 주행 제어 모듈은 자율 주 행 차량(100b)의 구성으로써 내부에 포함될 수도 있지만, 자율 주행 차량(100b)의 외부에 별도의 하드웨어로 구 성되어 연결될 수도 있다. 자율 주행 차량(100b)은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 자율 주행 차량(100b)의 상 태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계 획을 결정하거나, 동작을 결정할 수 있다. 여기서, 자율 주행 차량(100b)은 이동 경로 및 주행 계획을 결정하기 위하여, 로봇(100a)과 마찬가지로, 라이다, 레이더, 카메라 중에서 적어도 하나 이상의 센서에서 획득한 센서 정보를 이용할 수 있다. 특히, 자율 주행 차량(100b)은 시야가 가려지는 영역이나 일정 거리 이상의 영역에 대한 환경이나 객체는 외부 장치들로부터 센서 정보를 수신하여 인식하거나, 외부 장치들로부터 직접 인식된 정보를 수신할 수 있다. 자율 주행 차량(100b)은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수 행할 수 있다. 예컨대, 자율 주행 차량(100b)은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있고, 인 식된 주변 환경 정보 또는 객체 정보를 이용하여 주행 동선을 결정할 수 있다. 여기서, 학습 모델은 자율 주행 차량(100b)에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 자율 주행 차량(100b)은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, AI 서 버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 자율 주행 차량(100b)은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적어도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로 와 주행 계획에 따라 자율 주행 차량(100b)을 주행 시킬 수 있다. 맵 데이터에는 자율 주행 차량(100b)이 주행하는 공간(예컨대, 도로)에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예컨대, 맵 데이터에는 가로등, 바위, 건물 등의 고정 객체들과 차량, 보행자 등의 이 동 가능한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위 치 등이 포함될 수 있다. 또한, 자율 주행 차량(100b)은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거나 주행할 수 있다. 이때, 자율 주행 차량(100b)은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 응답을 결정하여 동작을 수행할 수 있다. 도 4는 본 발명의 일 실시 예에 따른 주변 상황을 확인하는 차량을 나타낸 도면이다. 차량은 센서, 컨트롤러, 프로세서를 포함할 수 있다. 이때, 센서는 차량에 내장된 적 어도 하나의 기기를 포함할 수 있다. 예를 들면, 센서는 카메라, LIDAR와 같은 차량의 주변 상황을 센싱하 는 기기를 포함할 수 있다. 따라서, 센서는 카메라를 이용하여 차량의 주변 상황에 대한 영상 데이터를 획 득할 수 있고, 또는 센서는 LIDAR를 이용하여 차량의 주변 객체와의 거리에 관한 정보를 획득할 수 있다. 클라이언트는 차량에 내장되어 있지 않지만 차량과 연결 가능한 기기를 포함할 수 있다. 예를 들면, 클라이언트는 모바일 폰, 셀룰러 폰, 스마트 폰, 퍼스널 컴퓨터, 태블릿 컴퓨터, 웨어러블 기기, 노트북, 넷북, 휴대 정보 단말기(personal digital assistant, PDA), 디지털 카메라, 퍼스널 멀티미디어 플레이어(personal multimedia player, PMP), 전자 북(E-book) 등을 포함할 수 있다. 이때, 클라이언트는 차량과 유/무선 통 신을 통해 연결될 수 있다. 예를 들면, 클라이언트는 차량과 5G 통신, WLAN(Wireless LAN), WiFi(Wireless Fidelity) Direct, DLNA(Digital Living Network Alliance), Wibro(Wireless broadband), Wimax(World Interoperability for Microwave Access), HSDPA(High Speed Downlink Packet Access), GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), WCDMA, 3GPP LTE(Long Term Evolution), 3GPP LTE-A(LTE Advanced) 등의 무선 통신 기술을 통해 연결되거나, 또는 블루투스 (Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra Wideband), ZigBee, NFC(Near Field Communication) 등의 근거리 통신을 통해 연결될 수 있다. 이하, 클라이언트와 차량이 5G 무선 통신을 통해 연결됨을 전제로 하나, 본 발명의 권리 범위는 5G 통신을 통해 연결되는 것으로 한정되지 않는다. 구체적으로, 클라이언트 1은 스마트 폰에 해당하고, 클라이언트 2는 태 블릿에 해당하며, 클라이언트 3은 디지털 카메라에 해당할 수 있고, 각각의 클라이언트는 차량과 5G 통신 을 통해 연결될 수 있다. 클라이언트는 차량의 센서 또는 컨트롤러와 데이터를 송수신하여 통신을 수행할 수 있다. 구체 적으로, 센서에 의해 센싱된 데이터가 클라이언트와 센서 사이 또는 클라이언트와 컨트롤러 사 이에서 송수신될 수 있다. 예를 들면, 센서에 의해 센싱된 영상 데이터가 클라이언트 1과 센서 사이 또는 클라이언트 1과 컨트롤러 사이에서 송수신될 수 있거나, 또는 센서에 의해 센싱된 영상 데이터 가 클라이언트 2와 센서 사이 또는 클라이언트 2와 컨트롤러 사이에서 송수신될 수 있거나, 또는 센 서에 의해 센싱된 영상 데이터가 클라이언트 3과 센서 사이 또는 클라이언트 3과 컨트롤러 사이 에서 송수신될 수 있다. 이때, 컨트롤러는 데이터 송수신에 따른 클라이언트의 성능을 확인할 수 있다. 여기서, 클라이언트의 성능 은 클라이언트의 전송 지연과 처리 시간을 고려하여 결정될 수 있다. 예를 들면, 클라이언트 1의 전송 지연과 처리 시간, 클라이언트 2의 전송 지연과 처리 시간, 클라이언트 3의 전송 지연과 처리 시간을 고려하여, 각각의 클라이언트의 성능이 확인될 수 있다. 클라이언트의 전송 지연과 처리 시간을 고려하여 성능이 확인된 경우, 컨트롤러는 각각의 클라이언트에 적 합한 알고리즘을 할당할 수 있다. 예를 들면, 클라이언트의 전송 지연과 처리 시간을 고려할 때, 클라이언트 2/ 클라이언트 1/클라이언트 3 순으로 성능이 좋은 경우, 상대적으로 성능이 좋은 클라이언트 2에 시간이 많이 소 비되는 알고리즘 B가 할당될 수 있고, 상대적으로 성능이 나쁜 클라이언트 3에 시간이 적게 소비되는 알고리즘C가 할당될 수 있다. 각각의 클라이언트는 할당된 알고리즘을 이용하여, 센서에 의해 센싱된 데이터를 처리할 수 있다. 프로세 서는 각각의 클라이언트에서 처리된 결과를 종합하여, 차량의 주변 상황을 확인할 수 있다. 예를 들 면, 각각의 알고리즘이 할당된 클라이언트 1 ~ 클라이언트 3은 센싱된 데이터를 처리할 수 있고, 프로세서(43 0)은 클라이언트 1 ~ 클라이언트 3의 처리 결과를 기반으로 차량의 주변 상황을 확인할 수 있다. 보다 구체적으 로, 프로세서은 각각의 클라이언트의 처리 결과를 기반으로 차량의 진행 방향의 동일 차선 또는 인근 차선에 위치한 객체(예를 들면, 차량, 오토바이, 사람 등), 신호 정보를 결정할 수 있다. 실시 예에 따르면, 차량은 내장된 모듈의 성능에 따라 구동 가능한 알고리즘의 차이가 발생하고, 이로 인 해 차량의 ADAS(Advanced Driver Assistance Systems)와 같은 기술에 의한 주변 상황 인식의 정확성이 낮 을 수 있다. 만약, 차량에 내장된 모듈의 성능이 좋지 않아 구동 가능한 알고리즘의 인식률이 낮을 경우, 차량의 ADAS와 같은 기술의 신뢰도가 낮을 수 있다. 이때, 차량에 내장된 모듈뿐만 아니라 연결 가능 한 클라이언트를 이용하여 클라이언트에 의한 처리 결과를 종합할 경우, 차량의 ADAS와 같은 기술에 의한 주변 상황 인식의 정확성이 향상될 수 있다. 여기서, ADAS는 차량의 운전 중 발생할 수 있는 상황 가운데 일부를 차량 스스로 인지하고 상황을 판단, 제어하는 기술이다. 도 5는 본 발명의 일 실시 예에 따른 클라이언트에 할당된 알고리즘에 의해 영상 데이터를 처리하는 것을 나타 낸 도면이다. 차량은 센서를 통해 센싱된 데이터를 획득할 수 있다. 센싱된 데이터는 카메라를 통해 획득된 영상 데이터를 포 함할 수 있다. 영상 데이터는 차량의 진행 방향인 전방에 대한 영상 데이터이거나, 또는 차량의 후방에 대한 영 상 데이터이거나, 또는 차량의 측방에 대한 영상 데이터를 포함할 수 있다. 차량은 기본적으로 내재된 알고리즘에 의해 영상 데이터에 포함된 차량/차선/사람과 같은 객체를 인식할 수 있 다. 또한, 차량은 인식된 객체의 위치/속도를 이용하여, 차량과 인식된 객체에 따른 주변 상황을 인식할 수 있 다. 예를 들면, 차량은 인식된 객체가 사람인 경우, 차량의 속도 및 차량과 사람 간의 거리를 기반으로 충돌 가 능성과 같은 주변 상황을 인식할 수 있다. 또는 차량은 인식된 객체가 차선인 경우, 차량과 차선 간의 간격을 기반으로 차선 이탈 가능성과 같은 주변 상황을 인식할 수 있다. 만약, 차량에 내재된 센서 및 알고리즘의 성능이 낮을 경우, 차량이 인식한 주변 상황에 대한 신뢰도는 낮을 수 있다. 따라서, 차량은 연결 가능한 클라이언트를 확인하고, 클라이언트에 의한 처리 결과를 종합하여 차량의 주 변 상황을 인식할 수 있다. 이하, 일례인 3개의 클라이언트에 기초하여 기재하지만, 본 발명의 권리 범위가 3개 의 클라이언트로 한정되는 것은 아니다. 차량은 연결 가능한 클라이언트 1 ~ 클라이언트 3을 확인할 수 있다. 이때, 클라이언트 1은 스 마트 폰일 수 있고, 클라이언트 2은 태블릿일 수 있고, 클라이언트 3은 네비게이션일 수 있다. 차량 은 클라이언트 1인 스마트 폰, 클라이언트 2 태블릿, 클라이언트 3 네비게이션과 데이터를 송수 신할 수 있고, 각각의 클라이언트의 전송 지연과 처리 시간을 고려하여 성능을 판단할 수 있다. 이때, 차량은 각각의 클라이언트의 성능에 대응하는 테이블을 저장할 수 있고, 테이블을 이용하여 데이터 송수신 과정을 생략 할 수 있다. 만약, 클라이언트 1인 스마트 폰과 클라이언트 3인 네비게이션의 경우 이전에 데이터 송수신에 따 른 성능이 판단된 경우, 차량은 저장된 테이블에 기록된 데이터를 이용하여 각 클라이언트의 성능을 판단할 수 있다. 또한, 차량은 테이블에 기록이 없는 클라이언트 2인 태블릿과 데이터 송수신을 통해 클라이언트 2의 성능 을 판단할 수 있다. 이때, 클라이언트 2와 관련된 기록이 테이블에 업데이트되어 저장될 수 있다. 차량은 각각의 판단된 클라이언트의 성능에 기초하여, 각각 서로 다른 알고리즘을 클라이언트에 할당할 수 있다. 예를 들면, 클라이언트 2/클라이언트 1/클라이언트 3 순으로 성능이 좋은 경우, 차량은 알고리즘 연산에 상대적으로 많은 시간이 소모되는 알고리즘 B를 클라이언트 2에 할당할 수 있고, 또한 알고리즘 연산에 상대적 으로 적은 시간이 소모되는 알고리즘 C를 클라이언트 3에 할당할 수 있다. 영상 데이터는 차량의 센서에 의해 센싱된 데이터의 일례로서, 각각의 알고리즘이 할당된 각각의 클라이언 트로 전송될 수 있다. 이때, 각각의 클라이언트로 전송되는 영상 데이터은 동일한 영상 데이터 이거나, 서 로 다른 영상 데이터일 수 있다. 즉, 차량에 내장된 센서가 복수 개인 경우, 영상 데이터는 서로 다른 데이터일 수 있다. 이때, 차량은 서로 다른 센서에 의해 획득되는 영상 데이터 간의 차이를 사전에 확인할 수 있다. 만약, 서로 다른 센서에 의해 획득되는 영상 데이터의 개수가 확인된 클라이언트의 개수보다 적은 경우 메인 영상 데이터가 보다 많은 클라이언트로 전송될 수 있다. 예를 들면, 차량에 내장된 동일한 센서에 의해 획득된 동 일한 영상 데이터가 클라이언트 1, 클라이언트 2, 클라이언트 3으로 전송될 수 있고, 각각의 클라이언트 1 ~ 클 라이언트 3에 의한 동일한 영상 데이터의 처리 결과를 기반으로, 차량은 주변 상황을 결정할 수 있다. 다른 예 를 들면, 차량에 내장된 다른 센서에 의해 2개의 서로 다른 영상 데이터가 획득된 경우, 메인 영상 데이터는 상 대적으로 성능이 좋은 클라이언트 2/클라이언트 1로 전송될 수 있고, 서브 영상 데이터는 상대적으로 성능이 좋 지 않은 클라이언트 3로 전송될 수 있다. 이때, 차량은 메인 영상 데이터와 서브 영상 데이터 간의 차이를 사전 에 확인하여, 주변 상황을 결정할 때 차이를 반영할 수 있다. 영상 데이터를 수신한 클라이언트는 할당된 알고리즘을 기반으로 영상 데이터로부터 객체 및/또는 주변 상황을 인식할 수 있다. 예를 들면, 클라이언트 1은 영상 데이터로부터 동일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2대, 왼쪽 옆 차선에 오토바이 1대를 인식할 수 있다. 또한, 클라이언트 2은 영상 데이터로부터 동 일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2대, 왼쪽 옆 차선에 오토바이 0대를 인식할 수 있다. 또한, 클 라이언트 3은 영상 데이터로부터 동일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2대, 왼쪽 옆 차선에 오 토바이 1대를 인식할 수 있다. 여기서, 트럭, 승용차, 오토바이는 영상 데이터를 통해 인식된 객체의 일례를 나 타낸다. 차량은 인식된 객체의 위치 및/또는 속도를 기반으로 차량과 인식된 객체에 의한 충돌 가능성과 같은 주변 상황을 인식할 수 있다. 도 6은 본 발명의 일 실시 예에 따른, 클라이언트의 성능을 판단할 때 이용되는 패킷을 나타낸 도면이다. 센서와 클라이언트 또는 컨트롤러와 클라이언트는 데이터인 패킷을 송수신할 수 있고, 패킷의 송수신 결과를 기반으로 클라이언트의 성능이 확인될 수 있다. 패킷은 센싱된 데이터(예를 들면, 영상 데이터)와 헤더를 포함할 수 있다. 여기서, 헤더는 전송시작시간을 포함할 수 있다. 전송시작시간은 센서 또는 컨트롤러에서 클라이언트로 패킷의 전송 이 시작되는 시간을 나타낼 수 있다. 또한, 헤더는 처리시간을 포함할 수 있다. 처리시간은 각 각의 클라이언트 마다 상이한 시간으로서, 처리 시간은 클라이언트가 영상 데이터에 포함된 객체를 인식할 때, 소비되는 시간을 나타낼 수 있다. 종료 시간은 패킷이 클라이언트로 전송된 이후, 전송 지연과 처리 시간을 고려하여 결정될 수 있다. 즉, 종료 시간은 전송 시작 시간과 전송 지연, 처리 시간을 더한 시간일 수 있다. 또한, IMU(Inertial Measurement Unit)는 영상 데이터에서 인식된 객체와 관련된 정보를 나타낸다. 여기서, 객체와 관련된 정보는, 영상 데이터에서 객체의 X좌표, Y좌표, 폭(width), 높이(height)뿐만 아니 라 이에 기반한 변화 량(예를 들면, 속도)을 포함할 수 있다. 또한, 관심 영역(Region of Interest, ROI)은 컨트롤러에 의해 할당되는 영역일 수 있다. 구체적으로, 관심 영 역은 영상 데이터에서 차량의 진행 방향과 관련된 영역일 수 있다. 예를 들면, 관심 영역은 영상 데이터 에서 차량의 동일 차선 방향과 관련된 전방 영역일 수 있다. 또는, 관심 영역은 영상 데이터에 대해 각각의 클라이언트가 서로 다른 인식 결과를 나타내는 영역일 수 있다. 예를 들면, 차량의 왼쪽 차선의 오토바 이에 대해 각각의 클라이언트가 다른 인식 결과를 나타낼 경우, 해당 영역이 관심 영역으로 설정될 수 있다. 또 한 실시 예에서 관심 영역은 차량의 운행 속도, 운행 정보 및 경로 안내 정보 중 적어도 하나를 기반으로 결정 될 수 있다. 일 실시 예에 따르면, 패킷의 송수신 결과를 기반으로 각각의 클라이언트에 할당될 알고리즘이 결정될 수 있다. 예를 들면, 패킷의 송수신 결과, 클라이언트 1의 경우 전송 지연 없이 처리 시간 20msec가 소요되고, 클라이언트 2의 경우 전송 지연 2msec, 처리 시간 10msec가 소요될 수 있다. 이때, 컨트롤러는 클라 이언트 2의 성능이 클라이언트 1의 성능 보다 좋은 것으로 판단하여, 클라이언트 2에 연산 량이 상대적으로 많 은 알고리즘 B를 할당하고 클라이언트 1에 연산 량이 상대적으로 적은 알고리즘 A를 할당할 수 있다. 도 7은 본 발명의 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면이다. 전술한 도 6의 과정을 통해 클라이언트 1과 클라이언트 2의 성능이 판단되었고, 각각의 클라이언트에 대응하는 알고리즘 A 및 알고리즘 B가 할당되었음을 전제로 한다. 여기서, 알고리즘은 영상 데이터에 나타난 객체 마다 인식 율이 아래의 표 1과서로 같이 다를 수 있다. Fast R- CNN, SSD512, ResNet는 영상 데이터로부터 객체를 인식하는 알고리즘의 일례에 해당하며, 본 발명에 적용 가능 한 알고리즘은 이에 한정되지 않는다. 표 1과 같이, 각각의 알고리즘 마다 객체 인식 율이 모두 상이하며, 전체 평균 인식 율이 높더라도 모든 객체에 대해 인식 율이 높지 않을 수 있다. 이는 알고리즘이 gray, rgb, hsv와 같은 다양한 포맷을 이용하여 영상 데이터로부터 객체를 인식하기 때문에, 이용되는 포맷에 따라 객체의 인식 율이 상이할 수 있다. 예를 들면, 표 1을 통해 객체(person)에 대한 인식 율은 SSD512 알고리즘이 가장 좋으며, 객체(Mbike)에 대한 인식 율은 Resnet 알고리즘이 가장 좋은 것을 확인할 수 있다. 표 1 Bike Bird Bus Car Mbike Person Fast R-CNN 78.4 70.8 77.8 71.6 80.8 72.0 SSD512 82.3 75.8 81.7 81.5 84.3 83.3 ResNet 81.6 77.2 78.6 76.6 84.8 80.7 알고리즘 A가 할당된 클라이언트 1이 처리한 영상 데이터에 대한 인식 결과는 왼쪽 옆 차선에 오토바이 1 대, 동일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2대, 사람 없음일 수 있다. 또한, 알고리즘 B가 할당된 클 라이언트 2가 처리한 영상 데이터에 대한 인식 결과는 왼쪽 옆 차선에 오토바이 없음, 동일 차선에 트럭 1 대, 오른쪽 옆 차선에 승용차 2대, 사람 없음일 수 있다. 또한, 차량에 내장된 기본 알고리즘에 의해 영상 데이 터에 대한 인식 결과은 왼쪽 옆 차선에 오토바이 1대, 동일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2 대, 사람 없음일 수 있다. 이때, 각각의 알고리즘의 객체 인식 율이 표 1과 같이 상이할 수 있으므로, 영상 데 이터로부터 인식된 객체에 대한 결과가 상이할 수 있다. 따라서, 클라이언트 1은 왼쪽 옆 차선에 오토바이 1대 가 존재한다고 판단하였고, 클라이언트 2는 왼쪽 옆 차선에 오토바이가 없다고 판단하였고, 차량은 왼쪽 옆 차 선에 오토바이 1대가 존재한다고 판단하였다. 이때, 차량은 서로 다른 인식 결과를 나타내는 영역을 관심 영역으로 설정할 수 있다. 차량은 관심 영역에 대해, 다른 알고리즘에 의한 인식 결과를 종합하여, 최종 인식 결과을 판단할 수 있다. 예를 들면, 차량은 다른 인식 결과를 나타내는 왼쪽 옆 차선 영역을 관심 영역으로 설정할 수 있고, 차량은 관심 영역에 대해 다른 알고리즘에 의한 인식 결과를 종합하여 최종 인식 결과로서 왼쪽 옆 차선에 오토바이 1대, 동일 차선에 트 럭 1대, 오른쪽 옆 차선에 승용차 2대, 사람 없음을 도출할 수 있다. 또한, 차량은 관심 영역의 객체에 대해 인 식율이 상대적으로 높은 알고리즘의 결과에 가중치를 반영하여, 최종 인식 결과을 판단할 수 있다. 예를 들면, 차량은 다른 인식 결과를 나타내는 왼쪽 옆 차선 영역을 관심 영역으로 설정할 수 있고, 차량은 오토바이 에 대해 높은 인식 율을 나타내는 알고리즘 A의 인식 결과에 가중치를 부여하여, 최종 인식 결과로서 왼쪽 옆 차선에 오토바이 1대, 동일 차선에 트럭 1대, 오른쪽 옆 차선에 승용차 2대, 사람 없음을 도출할 수 있 다. 도 8은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 차량의 센서에 의한 센싱 주기는 센서의 성능에 따라 결정될 수 있다. 예를 들면, 센서가 33msec 주기로 영상을 센싱 할 수 있다면, 센서는 33msec, 66msec, 99msec, 132msec 마다 영상을 센싱할 수 있다. 즉, 센서는 33msec 에 영상 데이터 1을 센싱 하고, 66msec에 영상 데이터 2를 센싱 하고, 99msec에 영상 데이터 3을 센싱 하고, 132msec에 영상 데이터 4를 센싱 할 수 있다. 실시 예에서 센싱 주기는 예시적으로 언급된 것으로 구체적인 수 치들의 상대적인 관계 기반으로 다른 값을 적용할 수도 있다. 만약, 센서의 센싱 주기 보다 클라이언트의 전송 지연과 처리 시간이 짧은 경우, 보정 없이 클라이언트에 의한 영상 데이터의 인식 결과를 기반으로 차량의 주변 상황이 결정될 수 있다. 예를 들면, 클라이언트 1인 태블릿의 전송 지연과 처리 시간이 20msec이고 센서의 센싱 주기가 33msec인 경우, 차량은 보정 없이 클라이언트에 의한 영상 데이터의 인식 결과를 기반으로 차량의 주변 상황을 결정할 수 있다. 만약, 센서의 센싱 주기 보다 클라이언트의 전송 지연과 처리 시간이 긴 경우, 클라이언트에 의한 영상 데이터 의 인식 결과가 보정되어 차량의 주변 상황이 결정될 수 있다. 예를 들면, 클라이언트 2인 스마트 폰의 전송 지 연과 처리 시간이 60msec이고 센서의 센싱 주기가 33msec인 경우, 영상 데이터 1을 스마트 폰이 처리한 인식 결 과를 차량은 93msec에 인식 결과를 확인할 수 있고 또한 영상 데이터 2를 스마트 폰이 처리한 인식 결과를 차량은 126msec에 확인할 수 있고 또한 영상 데이터 3을 스마트 폰이 처리한 인식 결과를 차량은 159msec에 확인할 수 있다. 따라서, 차량은 영상 데이터 1에 대한 스마트 폰의 인식 결과를 기반으로, 영상 데이터 2에 대한 스마 트 폰의 인식 결과를 추정할 수 있다. 구체적으로, 차량은 영상 데이터 1에 대한 스마트 폰의 인식 결과에 포함 된 객체의 X좌표, Y좌표, 폭(width), 높이(height) 및/또는 변화율에 기반하여 영상 데이터 2에 포함된 객체와 관련된 정보를 추정할 수 있다. 예를 들면, 영상 데이터 1에 대한 스마트 폰의 인식 결과에 포함된 왼쪽 차선의 차량의 X좌표 20, Y좌표 300, 폭 100, 높이 50 및/또는 변화율에 기반하여, 영상 데이터 2에 포함된 왼쪽 차선 의 차량의 X좌표 24, Y좌표 285, 폭 107, 높이 54를 추정할 수 있다. 이때, 영상 데이터에 대한 클라이언트의 인식 결과는 필터나 뉴럴 네트워크의 적용에 의해 보정될 수 있다. 예를 들면, 필터는 칼만 필터가 적용될 수 있으며, 뉴럴 네트워크는 딥 뉴럴 네트워크가 적용될 수 있다. 이와 같이, 클라이언트의 성능이 센싱 주기 이하 인 경우, 차량은 영상 데이터에 대한 클라이언트의 인식 결과를 보정하여 객체를 추정할 수 있고, 추정된 결과 를 반영하여 차량의 주변 상황이 결정될 수 있다. 도 9는 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 클라이언트의 성능이 미리 설정된 기준(예를 들면, 센싱 주기) 이하라면, 차량은 관심 영역을 설정할 수 있다. 즉, 클라이언트의 전송 지연 및/또는 처리 시간을 고려한 결과 미리 설정된 기준 이하인 경우, 차량은 영상 데 이터 전부가 아닌 일부인 관심 영역에 대해 클라이언트의 처리를 요청할 수 있다. 이때, 영상 데이터 전부가 아 닌 일부인 관심 영역에 대한 클라이언트의 성능은 미리 설정된 기준 이상일 수 있다. 따라서, 영상 데이터에 대 한 스마트 폰의 처리 시간이 줄어들 수 있다. 예를 들면, 클라이언트인 스마트 폰의 성능이 미리 설정된 기준 이하라고 판단된 경우, 차량은 관심 영역으로서 차량의 진행 방향과 관련된 영역을 설정할 수 있고, 스마트 폰 은 할당된 알고리즘에 기반하여 영상 데이터 전부가 아닌 일부인 관심 영역에 대한 객체를 인식할 수 있다. 즉, 스마트 폰은 차량의 진행 방향과 관련된 영역에 대해, 왼쪽 차선/동일 차선/오른쪽 차선의 차량 및/또는 신호 정보와 관련된 정보를 인식할 수 있다. 또한, 복수의 클라이언트의 성능이 미리 설정된 기준 이하인 경우, 차량은 복수의 관심 영역을 설정할 수 있다. 차량은 영상 데이터에 대한 관심 영역 1, 관심 영역 2를 설정할 수 있다. 클라이언트 1은 관심 영역 1에 대한 처리를 요청 받을 수 있고, 클라이언트 2는 관심 영역 2에 대한 처리를 요청 받을 수 있다. 예를 들면, 차량의 진행 방향의 왼쪽 차선/동일 차선/오른쪽 차선과 관련된 영역이 관심 영역 1로 설정될 수 있고, 차량의 진행 방 향의 오른쪽 2번째 차선과 관련된 영역 및 신호등이 표시되는 영역이 관심 영역 2로 설정될 수 있다. 따라서, 클라이언트 1은 차량 전방의 왼쪽 차선/동일 차선/오른쪽 차선과 관련된 관심 영역 1에 포함된 객체를 인식할 수 있고, 클라이언트 2는 차량 전방의 오른쪽 2번째 차선/신호등과 관련된 관심 영역 2에 포함된 객체를 인식할 수 있다. 실시 예에 따르면, 클라이언트의 성능이 미리 설정된 기준 이하라도, 차량은 관심 영역(Region of Interest, ROI)를 설정하여 해당 영역에 대한 클라이언트의 인식 결과를 기반으로 주변 상황을 결정할 수 있다. 도 10은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 차량과 연결된 클라이언트가 서로 다른 인식 결과를 도출한 경우, 차량은 해당 영역을 관심 영역으로 설정할 수 있다. 이때, 관심 영역은 영상 데이터의 전체 영역이 아닌 일부 영역으로서, 클라이언트가 관심 영역을 처리하 는 시간이 줄어들 수 있고 또한 객체에 대한 인식 율이 향상될 수 있다. 예를 들면, 클라이언트 1에 의해 오른 쪽 2번째 차선에 차량이 존재한다고 판단되었지만, 클라이언트 2에 의해 오른쪽 2번재 차선에 차량이 존재하지 않는다고 판단될 수 있다. 이와 같이 서로 다른 인식 결과가 도출된 오른쪽 2번째 차선은 관심 영역으로 설정될 수 있다. 차량은 관심 영역의 위치 및/또는 서로 다른 결과가 도출된 객체의 위치를 나타내는 정보를 클라이언트 1 및/또 는 클라이언트 2로 전송할 수 있다. 클라이언트 1 또는 클라이언트 2는 관심 영역으로 설정된 영역 및 객체의 위치에 대해 다시 인식 결과를 도출할 수 있다. 차량은 다시 처리된 인식 결과에 기반하여, 오른쪽 2번째 차선 에 차량이 존재하는지 유무에 대한 판단을 할 수 있다. 또는, 차량은 관심 영역의 위치 및/또는 서로 다른 결과가 도출된 객체의 위치를 나타내는 정보를 클라이언트 1, 클라이언트 2와 다른 클라이언트 3으로 전송할 수 있다. 클라이언트 3은 관심 영역으로 설정된 영역 및 객체 의 위치에 대해 인식 결과를 도출할 수 있다. 차량은 클라이언트 3에서 처리된 인식 결과에 기반하여, 오른쪽 2 번째 차선에 차량이 존재하는지 유무에 대한 판단을 할 수 있다. 도 11은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 클라이언트에 할당되는 알고리즘은 차량 주변의 환경에 따라 객체 별 인식 율이 상이할 수 있다. 여기서, 차량 주변의 환경은 일례로서, 낮/저녁/눈/비와 같은 경우를 포함하며, 본 발명의 권리 범위가 이에 한정되지 않는다. 예를 들면, 알고리즘 A는 차량 주변의 환경이 저녁일 때 객체 별 인식 율이 상대적으로 높을 수 있다. 또한, 알고리즘 B는 차량 주변의 환경이 비 내릴 때 객체 별 인식 율이 상대적으로 높을 수 있다. 또한, 알고리 즘 C는 차량 주변의 환경이 눈이 올 때 객체 별 인식 율이 상대적으로 높을 수 있다. 또한, 알고리즘 D는 차량 주변의 환경이 낮일 때 객체 별 인식 율이 상대적으로 높을 수 있다. 구체적으로, 차량의 센서가 조도/비/눈을 감지하여 환경을 판단한 경우, 차량은 복수의 알고리즘 중에서 환경에 적합한 알고리즘을 클라이언트에 우선 순위로 할당할 수 있다. 예를 들면, 차량 주변의 환경이 비 내리는 환경 인 경우, 차량은 비 내릴 때 객체 별 인식 율이 상대적으로 높은 순서로 알고리즘을 클라이언트에 할당할 수 있 다. 차량은 각각의 클라이언트가 처리한 인식 결과를 기반으로 차량의 주변 상황을 결정할 수 있다. 이때, 인식 결 과가 상이한 경우, 차량은 환경에 적합한 알고리즘의 인식 결과에 가중치를 적용하여 주변 상황을 결정할 수 있 다. 즉, 가중치가 적용된 인식 결과에 상대적으로 높은 신뢰도가 인정될 수 있다. 예를 들면, 클라이언트 1에 알고리즘 A가 할당되어 있고, 클라이언트 2에 알고리즘 B가 할당되었고, 차량의 주 변 환경은 조도가 낮은 밤 12시를 전제로 한다. 이때, 조도가 낮은 밤일 때 알고리즘 A의 객체 별 인식 율이 알 고리즘 B의 객체 별 인식 율 보다 높을 수 있다. 만약, 클라이언트 1에 의해 오토바이 1대, 트럭 1대, 승용차 2 대, 사람 없음이 인식 결과로서 도출될 수 있다. 또한, 클라이언트 2에 의해 오토바이 없음, 트럭 1대, 승용차 2대, 사람 없음이 인식 결과로서 도출될 수 있다. 또한, 차량에 내장된 알고리즘에 의해 오토바이 1대, 트럭 1대, 승용차 2대, 사람 없음이 인식 결과로서 도출될 수 있다. 이때, 차량은 클라이언트 1의 인식 결과에 가중치를 두어 최종 인식 결과로서 오토바이 1대, 트럭 1대, 승용차 2대, 사람 없음을 도출할 수 있다. 이때, 가중치는 적용된 알고리즘 간의 객체 별 인식 율 차이에 기초하여 결정될 수 있다. 알고리즘의 객체 별 인식 율은 사전에 결정될 수 있고, 차량은 사전에 결정된 데이터에 근거하여 가중치를 결정할 수 있다. 도 12는 본 발명의 일 실시 예에 따른, 차량에 의해 수행되는 차량의 주변 상황 확인 방법의 흐름도를 나타낸 도면이다. 단계에서, 차량은 차량 내부의 연결 가능한 적어도 하나의 클라이언트를 확인할 수 있다. 차량 내부의 유 무선 통신을 통해 차량과 연결 가능한 스마트 폰, 태블릿과 같은 클라이언트가 확인될 수 있다. 이때, 이전에 차량과 연결된 클라이언트와 관련된 정보가 저장되어 있을 수 있다. 단계에서, 차량은 확인된 클라이언트와 통신을 수행하여, 클라이언트의 성능의 결정 기준에 기반하여 상 기 클라이언트의 성능을 확인할 수 있다. 차량은 클라이언트와 패킷을 송수신하는 과정에서, 클라이언트의 전송 지연과 처리 시간을 판단할 수 있다. 만약, 이전에 차량과 연결된 클라이언트에 해당하는 경우 통신에 따른 성 능 판단하는 단계가 생략될 수 있다. 단계에서, 차량은 확인된 클라이언트의 성능에 기반하여 상기 클라이언트에 대응하는 알고리즘을 할당할 수 있다. 이때, 차량은 연결 가능한 클라이언트 모두에 알고리즘을 할당하거나, 또는 연결 가능한 클라이언트 중에서 선별된 클라이언트에 알고리즘을 할당할 수 있다. 여기서, 알고리즘은 영상 데이터에 나타난 객체 마다 인식 율이 서로 다를 수 있다. 단계에서, 차량의 센서를 통해 영상 데이터를 획득하는 경우, 상기 알고리즘에 따른 상기 영상 데이터의 처리 결과로 상기 클라이언트로부터 획득된 정보를 기반으로 차량의 주변 상황을 확인할 수 있다. 차량은 센서의 성능에 따라 결정된 센싱 주기에 기반하여, 영상 데이터를 획득할 수 있다. 예를 들면, 센서가 33msec 단위 로 영상을 센싱하는 경우, 33msec, 66msec, 99msec, 132msec일 때 영상을 획득할 수 있다. 이때, 차량의 센서 를 통해 획득된 영상 데이터를 수신한 클라이언트에 의한 인식 결과를 기반으로 차량의 주변 상황이 결정될 수 있다. 만약, 각각의 클라이언트 마다 인식 결과가 상이한 경우, 차량은 관심 영역에 대해 다른 알고리즘에 의한 인식 결과 또는 동일 알고리즘에 의한 인식 결과를 종합하여 차량 주변 상황을 결정할 수 있다. 이때, 관심 영 역은 영상 데이터에서 인식 결과가 상이한 영역을 포함할 수 있고, 관심 영역에 대한 각각의 클라이언트의 처리 로 인해 처리 속도 및/또는 인식율이 향상될 수 있다. 또는, 차량 주변의 환경에 적합한 알고리즘의 처리 결과 에 가중치를 적용하여, 차량의 주변 상황을 결정할 수 있다. 예를 들면, 알고리즘 A는 저녁일 때 객체 별 인식 율이 상대적으로 높을 수 있고, 알고리즘 B는 비 내릴 때 객체 별 인식 율이 상대적으로 높을 수 있고, 알고리 즘 C는 눈이 올 때 객체 별 인식 율이 상대적으로 높을 수 있다. 따라서, 차량 주변의 환경에 적합한 알고리즘 의 처리 결과에 가중치가 적용되어, 차량의 주변 상황이 결정될 수 있다. 또한, 클라이언트의 전송 지연과 처리 시간이 차량의 센서에 의한 센싱 주기 보다 긴 경우, 차량은 영상 데이터에 대한 인식 결과를 보정하여 차량의 주변 상황을 결정할 수 있다. 예를 들면, 성능의 결정 기준인 전송 지연과 처리 시간이 40msec이고 센서의 센싱 주기가 33msec인 경우, 차량은 영상 데이터에 대한 인식 결과를 보정하여 차량의 주변 상황을 결정할 수 있다. 각각의 실시 예에 대한 자세한 설명은 전술한 기재를 참조한다. 실시 예에 따르면, 차량에 장착된 ADAS(Advanced Driver Assistance System) 모듈의 성능에 따라 구동 가능한 알고리즘의 종류가 상이할 수 있다. 또는 기 장착된 ADAS 모듈의 성능이 좋지 않아, 구동 가능한 알고리즘의 종 류가 제한될 수 있다. 따라서, 차량에 장착된 하드웨어에 의한 알고리즘뿐만 아니라 차량과 연결 가능한 클라이 언트에 할당된 알고리즘에 의해 영상 데이터를 인식하여, 차량의 주변 상황을 보다 정확하게 결정할 수 있다. 한편, 본 명세서와 도면에는 본 발명의 바람직한 실시 예에 대하여 개시하였으며, 비록 특정 용어들이 사용되었 으나, 이는 단지 본 발명의 기술 내용을 쉽게 설명하고 발명의 이해를 돕기 위한 일반적인 의미에서 사용된 것 이지, 본 발명의 범위를 한정하고자 하는 것은 아니다. 여기에 개시된 실시 예 외에도 본 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명한 것이다."}
{"patent_id": "10-2019-0087640", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따른 AI 장치를 나타낸다. 도 2는 본 발명의 일 실시 예에 따른 AI 서버를 나타낸다. 도 3은 본 발명의 일 실시 예에 따른 AI 시스템을 나타낸다. 도 4는 본 발명의 일 실시 예에 따른 주변 상황을 결정하는 차량을 나타낸 도면이다. 도 5는 본 발명의 일 실시 예에 따른 클라이언트에 할당된 알고리즘에 의해 영상 데이터를 처리하는 것을 나타 낸 도면이다. 도 6은 본 발명의 일 실시 예에 따른, 클라이언트의 성능을 판단할 때 이용되는 패킷을 나타낸 도면이다. 도 7은 본 발명의 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면이다. 도 8은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 도 9는 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 도 10은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 도 11은 본 발명의 다른 일 실시 예에 따른, 차량과 클라이언트에 의해 주변 상황을 인식하는 것을 나타낸 도면 이다. 도 12는 본 발명의 일 실시 예에 따른, 차량에 의해 수행되는 차량의 주변 상황 결정 방법의 흐름도를 나타낸 도면이다."}
