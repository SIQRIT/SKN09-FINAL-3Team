{"patent_id": "10-2023-0047906", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0151961", "출원번호": "10-2023-0047906", "발명의 명칭": "자기지도 학습기반 통합 음성 합성 방법 및 장치", "출원인": "주식회사 수퍼톤", "발명자": "최형석"}}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "자기지도 학습 기반의 음성합성방법에 있어서,학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계; 및상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는단계를 포함하는 것을 특징으로 하는 자기지도 학습 기반의 음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 학습용 음성 신호 및 상기 합성 음성 신호에 기초하여, 상기 학습용 음성 신호 및 상기 합성 음성 신호의재구성 손실을 산출하고, 산출된 재구성 손실에 기초하여 상기 음성분석모듈 및 상기 음성합성모듈에 대한 학습을 수행하는 단계를 더 포함하는 것을 특징으로 하는 자기지도 학습 기반의 음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 학습용 음성의 음성 특징은 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징을 포함하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계는 상기 학습용 음성 신호를 복수 개의 주파수 빈의 확률분포 스펙트럼으로 변환하고, 변환된 확률분포 스펙트럼으로부터 상기 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 출력하는 단계;상기 학습용 음성 신호로부터 상기 학습용 음성에 포함된 텍스트의 발음 특징을 출력하는 단계; 및상기 학습용 음성 신호를 멜-스펙트로그램으로 변환하고, 변환된 멜-스펙트로그램부터 상기 학습용 음성의 음색특징을 출력하는 단계를 포함하는 것을 특징으로 하는 자기지도 학습 기반의 음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서,상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계는상기 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]에 기초하여, 입력 여기 신호(inputexcitation signal)를 생성하는 단계;상기 학습용 음성의 음색 특징에 기초하여, 시간-변화음색 임베딩(time-varying embedding)을 생성하는 단계;상기 학습용 음성의 발음 특징 및 상기 생성된 시간-변화음색 임베딩에 기초하여, 상기 합성 음성에 대한 프레임-레벨 컨디션을 생성하는 단계; 및상기 입력 여기 신호 및 상기 프레임-레벨 컨디션에 기초하여, 상기 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계를 포함하는 것을 특징으로 하는 자기지도 학습 기반의 음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4 항에 있어서,공개특허 10-2024-0151961-3-상기 입력 여기 신호는 아래의 수학식 1에 따르는 것을 특징으로 하는 자기지도 학습 기반의 음성합성방법. <수학식 1>(Ns는 샘플링 레이트이고, n[t]는 샘플링된 노이즈)"}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "자기지도 학습 기반의 음성합성장치에 있어서,학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈(103); 및상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는음성합성모듈(104)을 포함하는 것을 특징으로 하는 음성합성장치."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 가창음성합성방법에 있어서,합상대상노래 및 합성대상가수를 포함하는 가창음성 합성요청을 획득하는 단계;상기 가창음성 합성요청에 기초하여, 상기 합성대상가수와 연관된 음성 신호를 획득하는 단계;SVS(singing voice synthesis) 모듈에서, 상기 가창음성 합성요청 및 상기 합성대상가수와 연관된 음성 신호에기초하여, 상기 합성대상노래 및 상기 합성대상가수에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭Aap[n] 및 발음 특징을 포함하는 가창음성특징을 생성하는 단계;상기 음성분석모듈에서, 상기 획득된 합성대상가수와 연관된 음성 신호에 기초하여 상기 합성대상가수의 음색특징을 생성하는 단계; 및상기 음성합성모듈에서, 상기 가창음성특징 및 상기 음색 특징에 기초하여 상기 합성대상가수의 목소리로 상기합성대상노래를 부른 음성을 나타내는 가창음성 신호를 합성하는 단계를 포함하는 것을 특징으로 하는 자기지도학습 기반의 가창음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 7 항에 있어서, 상기 SVS 모듈은 학습용 노래, 학습용 가수 음성 및 학습용 가창 음성 특징을 포함하는 학습용 데이터세트에 의해, 입력된 합성대상노래 및 합성대상가수에 대한 가창음성특징을 출력하도록 사전에 트레이닝된 인공신경망인것을 특징으로 하는 자기지도 학습 기반의 가창음성합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 변조음성 합성방법에 있어서,공개특허 10-2024-0151961-4-음성변환의 대상이 되는 변환-전-음성을 획득하는 단계;상기 음성분석모듈에서, 상기 획득한 변환-전-음성에 기초하여, 상기 변환-전-음성에 대한 기초 주파수 F0, 주기진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 포함하는 변환-전-음성 특징을 출력하는 단계;변환음성에 대한 음성 속성을 획득하는 단계;VOD(voice design) 모듈에서, 상기 변환음성에 대한 음성속성에 기초하여, 변환음성에 대한 기초 주파수 F0 및음색 특징을 포함하는 변환음성 특징을 출력하는 단계; 및상기 음성합성모듈에서, 상기 변환-전-음성 특징 및 상기 변환음성 특징에 기초하여, 변환음성을 합성하는 단계를 포함하는 것을 특징으로 하는 자기지도 학습 기반의 변조음성 합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 9 항에 있어서, 상기 VOD 모듈은 학습용 음성속성, 학습용 기초 주파수 F0 및 학습용 음색 특징을 포함하는 학습용 데이터 세트에 의해, 입력된 음성속성에 기초하여 변환음성의 기초 주파수 F0 및 음색 특징을 출력하도록 사전에 트레이닝된인공신경망인 것을 특징으로 하는 자기지도 학습 기반의 변조음성 합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 TTS(text to speech) 합성방법에있어서,TTS 합성을 윈하는 합성대상텍스트 및 합성대상 음성주체를 획득하는 단계;상기 합성대상 음성 주체에 기초하여, 상기 합성대상 음성주체와 연관된 음성을 획득하는 단계;상기 음성분석모듈에서, 상기 합성대상 음성주체와 연관된 음성에 기초하여, 상기 합성대상 음성주체의 음색 특징을 포함하는 합성대상음성주체의 음성 특징을 출력하는 단계;TTS 모듈에서, 합성대상텍스트 및 합성대상 음성주체와 연관된 음성에 기초하여, 상기 합성대상텍스트를 상기합성대상 음성주체의 목소리로 읽은 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭Aap[n]을 포함하는 텍스트음성의 음성특징을 출력하는 단계; 및 상기 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n], 그리고 상기 합성대상 음성주체의 음색 특징에 기초하여, 텍스트 음성을 합성하는 단계를 포함하는 것을 특징으로 하는 자기지도 학습 기반의 TTS 합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서, 상기 TTS 모듈은 학습용 합성텍스트, 학습용 음성 및 학습용 음성특징을 포함하는 학습용 데이터 세트에 의해,입력된 텍스트 및 음성에 기초하여 텍스트 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 출력하도록 사전에 트레이닝된 인공신경망인 것을 특징으로 하는 자기지도 학습 기반의 TTS 합성방법."}
{"patent_id": "10-2023-0047906", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 1 항 내지 제 5 항 중 어느 한 항에 기재된 방법을 수행하는 프로그램이 기록된 컴퓨터 판독 가능한 기록매체.공개특허 10-2024-0151961-5-"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "자기지도 학습 기반의 통합 음성합성방법으로서, 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 학습용 음성 신호에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 학습용 음성에 대한 음성 특징 을 출력하고, 출력된 음성 특징을 이용하여, 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성 모듈에 대한 학습을 수행하고, 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성함으로써, 방대한 양의 음성 및 텍스트 데이터 세트를 이용하여 인공신경망을 학습시킬 필요 없이, 자기지도 학습으로 스스 로 학습한 인공신경망을 이용하여 실제 음성과 비슷한 음성을 합성할 수 있다."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "자기지도 학습기반 통합 음성 합성 방법 및 장치에 관한 것으로, 보다 구체적으로 자기지도 학습을 기반으로 학 습된 머신러닝 모델을 이용하여 음성을 합성할 수 있도록 하기 위하여 자기지도 학습기반 통합 음성 합성 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "음성합성(TTS; Text To Speech) 기술은 컴퓨터를 이용하여 텍스트로 입력된 임의의 문장을 사람의 목소리, 즉 음성신호로 생성하는 기술을 의미한다. 종래의 음성합성 기술은 음성신호를 생성할 때 미리 녹음된 한 음절의 음성신호를 결합하여 문장 전체에 대한 음성신호를 생성하는 결합형 음성합성(Concatenative TTS) 방식과 음성 의 특징이 표현된 고차원 파라미터로부터 보코더(vocoder)를 이용하여 음성신호를 생성하는 매개변수 음성합성 (Parametric TTS) 방식으로 구분된다. 종래의 결합형 음성합성 방식은 입력된 텍스트에 맞추어 미리 녹음된 단어, 음절, 음소의 음성신호를 결합하여 문장에 대한 전체 음성신호를 생성한다. 이렇게 생성된 문장에 대한 음성신호는 미리 녹음된 음성신호를 합성한 것이기 때문에 음성신호에서 문장의 억양, 운율 등이 표현되지 않아 음성 사이의 연결이 어색하고 사람의 목소 리와 이질감이 느껴지는 문제점이 있었다. 최근 인공지능(Artificial Intelligence, AI) 기술이 크게 발전하고 있다. 음성합성 분야에도 이러한 인공지능 기술이 다양하게 활용되고 있다. 인공지능 알고리즘은 기계학습(machine learning) 알고리즘을 사용할 수 있으 며, 기계학습은 크게 지도학습(Supervised Learning) 및 자기지도학습(Self-supervised Learning)으로 구별할 수 있다. 지도학습은 인공지능 기반 기계학습 모델에 대하여 학습할 데이터 및 그 데이터가 어떠한 데이터인지 에 대한 정답 레이블(Label)을 이용하여 인공지능 모델을 학습시킨다. 자기지도학습은 인공지능 기반 기계학습 모델에 대하여 학습할 데이터에 정답 레이블 없이 학습할 데이터만으로 인공지능 모델을 학습시킨다. 또한, 종래의 매개변수 음성합성 방식은 음성신호의 자연스러움을 향상시키기 위하여 기계학습을 이용한 방식으 로 발전되었으며, 방대한 양의 텍스트 및 음성 데이터를 이용하여 인공신경망을 학습시키고, 학습된 인공신경망 을 이용하여 입력된 문장의 텍스트에 대한 음성신호를 생성한다. 기계학습 기반의 매개변수 음성합성 방식은 인 공신경망을 이용하여 입력 텍스트에 대한 음성신호를 생성하기 때문에 학습된 음성신호의 음성주체의 억양, 운 율 등이 표현된 음성신호를 생성할 수 있다. 이에 따라, 결합형 음성합성 방식보다 자연스러운 음성신호를 생성 할 수 있다. 그러나, 이러한 기계학습 기반의 매개변수 음성합성 방식은 인공신경망을 학습시키기 위하여 방대 한 양의 음성 및 텍스트 데이터 세트가 필요한 단점이 존재한다. 상술한 종래의 음성합성 기술의 단점들은 가창음성 합성(SVS; Singing Voice Synthesis) 기술에서도 마찬가지이 다. 여기서, 가창음성 합성 기술은 가사 텍스트 및 악보 데이터 등을 이용하여 가창음성신호를 생성하는 기술이 다. 종래의 음성함성 기술 중 결합형 음성합성 방식은 사전에 녹음된 종류의 음소 발화만을 생성할 수 있고, 음 의 높낮이, 음의 길이, 박자 등이 자유롭게 변형이 가능한 가창음성신호를 생성할 수 없다. 이에 따라, 가창음 성 합성 분야에서는 인공신경망을 이용한 매개변수 음성합성 방식이 주로 활용되고 있다. 이러한 인공신경망 기반의 매개변수 음성합성 방식은 먼저 임의의 가수에 대한 가창음성, 해당 곡에 대한 악보 및 가사 텍스트로 인공신경망을 학습시킨다. 학습된 인공신경망은 입력된 악보 및 가사 텍스트에 기초하여 학습 된 가수의 가창음성(즉, 노래)과 음색과 창법이 유사한 가창음성 신호를 생성할 수 있다. 이러한 문제점은 매개변수 음성합성 방식뿐만 아니라 기존의 다른 음성합성 또는 가창음성 합성 방법들에서도 동일하다. 따라서, 이러한 문제점을 해결하기 위하여 자기지도 학습에 기반한 음성 또는 가창음성 합성 방법에 대한 필요 성이 대두되고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 공개특허공보 제10-2020-0015418호 (특허문헌 0002) 대한민국 등록특허공보 제10-1991733호 (특허문헌 0003) 대한민국 등록특허공보 제10-2057926호 비특허문헌 (비특허문헌 0001) Hyeong-Seok Choi, Jinhyeok Yang, Juheon Lee, and Hyeongju Kim, “NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis,” arXiv:2211.09407v1 [csSD], 17 Nov 2022."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "자기지도 학습기반 통합 음성 합성 방법 및 장치를 제공하는 데에 있다. 또한, 상기된 바와 같은 기술적 과제들 로 한정되지 않으며, 이하의 설명으로부터 또 다른 기술적 과제가 도출될 수도 있다."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 자기지도 학습 기반의 음성합성방법은 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계; 및 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음 성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부 터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계를 포함한다. 본 발명의 일 실시예에 따른 자기지도 학습 기반의 음성합성방법은 상기 학습용 음성 신호 및 상기 합성 음성 신호에 기초하여, 상기 학습용 음성 신호 및 상기 합성 음성 신호의 재구성 손실을 산출하고, 산출된 재구성 손 실에 기초하여 상기 음성분석모듈 및 상기 음성합성모듈에 대한 학습을 수행하는 단계를 더 포함한다. 상기 학습용 음성의 음성 특징은 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특 징 및 음색 특징을 포함하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계는 상기 학습용 음성 신호를 복수 개의 주파수 빈의 확률분포 스펙트럼으로 변환하고, 변환된 확률분포 스펙트럼으로부터 상기 학습용 음성 의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 출력하는 단계; 상기 학습용 음성 신호로부터 상 기 학습용 음성에 포함된 텍스트를 추출하고, 추출된 텍스트의 발음 특징을 출력하는 단계; 및 상기 학습용 음 성 신호를 멜-스펙트로그램으로 변환하고, 변환된 멜-스펙트로그램부터 상기 학습용 음성의 음색 특징을 출력하 는 단계를 포함한다. 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계는 상기 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]에 기초하여, 입력 여기 신호(input excitation signa l)를 생성하는 단계; 상기 학습용 음성의 음색 특징에 기초하여, 시간-변화음색 임베딩(time-varying embedding)을 생성하는 단계; 상기 학습용 음성의 발음 특징 및 상기 생성된 시간-변화음색 임베딩에 기초하여, 상기 합성 음성에 대한 프레임-레벨 컨디션을 생성하는 단계; 및 상기 입력 여기 신호 및 상기 프레임-레벨 컨 디션에 기초하여, 상기 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계를 포함한다. 상기 입력 여기 신호는 아래의 수학식 1에 따른다. <수학식 1>"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "(Ns는 샘플링 레이트이고, n[t]는 샘플링된 노이즈이다.) 본 발명의 다른 실시예에 따른 자기지도 학습 기반의 음성합성장치는 학습용 음성을 나타내는 학습용 음성 신호 를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈; 및 상기 출력된 음성 특징을 이용하여. 상기 학습 용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함한다. 본 발명의 또 다른 실시예에 따른 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호 에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신 호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 가창음 성합성방법은 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계; 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계; 합상대상노래 및 합성대상가수를 포함하는 가창음성 합성요청을 획득하는 단계; 상기 가창음성 합성요청에 기초 하여, 상기 합성대상가수와 연관된 음성 신호를 획득하는 단계; SVS(singing voice synthesis) 모듈에서, 상기 가창음성 합성요청 및 상기 합성대상가수와 연관된 음성 신호에 기초하여, 상기 합성대상노래 및 상기 합성대상 가수에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 포함하는 가창음성특징을 생 성하는 단계; 상기 음성분석모듈에서, 상기 획득된 합성대상가수와 연관된 음성 신호에 기초하여 상기 합성대상 가수의 음색 특징을 생성하는 단계; 및 상기 음성합성모듈에서, 상기 가창음성특징 및 상기 음색 특징에 기초하 여 상기 합성대상가수의 목소리로 상기 합성대상노래를 부른 음성을 나타내는 가창음성 신호를 합성하는 단계를 포함한다. 상기 SVS 모듈은 학습용 노래, 학습용 가수 음성 및 학습용 가창 음성 특징을 포함하는 학습용 데이터세트에 의 해, 입력된 합성대상노래 및 합성대상가수에 대한 가창음성특징을 출력하도록 사전에 트레이닝된 인공신경망이 다. 본 발명의 또 다른 실시예에 따른 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호 에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신 호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 변조음 성 합성방법은 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징 을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계; 상 기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하도록 음성합성모듈 에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단 계; 음성변환의 대상이 되는 변환-전-음성을 획득하는 단계; 상기 음성분석모듈에서, 상기 획득한 변환-전-음성 에 기초하여, 상기 변환-전-음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 포함하는 변환-전-음성 특징을 출력하는 단계; 변환음성에 대한 음성 속성을 획득하는 단계; VOD(VOice Design) 모듈에서, 상기 변환음성에 대한 음성속성에 기초하여, 변환음성에 대한 기초 주파수 F0 및 음색 특징을 포함하 는 변환음성 특징을 출력하는 단계; 및 상기 음성합성모듈에서, 상기 변환-전-음성 특징 및 상기 변환음성 특징 에 기초하여, 변환음성을 합성하는 단계를 포함한다. 상기 VOD 모듈은 학습용 음성속성, 학습용 기초 주파수 F0 및 학습용 음색 특징을 포함하는 학습용 데이터 세트 에 의해, 입력된 음성속성에 기초하여 변환음성의 기초 주파수 F0 및 음색 특징을 출력하도록 사전에 트레이닝된 인공신경망이다. 본 발명의 또 다른 실시예에 따른 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호 에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 음성분석모듈, 및 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신 호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 음성합성모듈을 포함하는 음성합성장치에서 실행되는 자기지도 학습 기반의 TTS(text to speech) 합성방법은 학습용 음성을 나타내는 학습용 음성 신호를 이용하여 상기 학습용 음성 신호에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행하고, 상기 학습용 음성에 대한 음성 특징을 출력하는 단계; 상기 출력된 음성 특징을 이용하여. 상기 학습용 음성의 음성 특징으로부터 음성 신호를 합성하 도록 음성합성모듈에 대한 학습을 수행하고, 상기 출력된 음성 특징으로부터 합성 음성을 나타내는 합성 음성 신호를 합성하는 단계; TTS 합성을 윈하는 합성대상텍스트 및 합성대상 음성주체를 획득하는 단계; 상기 합성대 상 음성 주체에 기초하여, 상기 합성대상 음성주체와 연관된 음성을 획득하는 단계; 상기 음성분석모듈에서, 상 기 합성대상 음성주체와 연관된 음성에 기초하여, 상기 합성대상 음성주체의 음색 특징을 포함하는 합성대상음 성주체의 음성 특징을 출력하는 단계; TTS 모듈에서, 합성대상텍스트 및 합성대상 음성주체와 연관된 음성에 기 초하여, 상기 합성대상텍스트를 상기 합성대상 음성주체의 목소리로 읽은 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 포함하는 텍스트음성의 음성특징을 출력하는 단계; 및 상기 텍스트 음 성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n], 그리고 상기 합성대상 음성주체의 음색 특 징에 기초하여, 텍스트 음성을 합성하는 단계를 포함한다. 상기 TTS 모듈은 학습용 합성텍스트, 학습용 음성 및 학습용 음성특징을 포함하는 학습용 데이터 세트에 의해, 입력된 텍스트 및 음성에 기초하여 텍스트 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 출 력하도록 사전에 트레이닝된 인공신경망이다. 본 발명의 또 다른 실시예에 따른 컴퓨터 판독 가능한 기록매체는 본 발명의 일 실시예에 따른 자기지도 학습 기반의 음성합성방법을 수행하는 프로그램이 기록된다."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 음성합성장치 및 음성합성방법은 자기지도 학습 기반의 인공지능을 이용하여 음성 신호를 분석 하고 음성 신호의 음성 특징을 추출하고, 추출된 음성 특징에 기초하여 다시 음성을 합성한다. 인공지능은 음성 신호의 분석 및 합성을 반복하면서 입력된 음성의 음성 특징을 출력하고, 출력된 음성 특징에 기초하여 음성을 다시 합성한다. 본 발명의 실시예들에 따르면, 인공신경망은 스스로 입력된 음성의 음성 특징을 추출하도록 학 습하고, 추출된 음성 특징에 기초하여 합성 음성을 출력하도록 학습한다. 자기지도 학습 기반의 인공지능 모델 을 이용함으로써, 음성 합성 분야에서 지도학습 기반의 인공지능 모델과 비교하여 인공신경망 모델을 학습시키 위한 방대한 양의 학습용 오디오 데이터가 필요없으며, 음성합성을 위한 인공신경망을 빠르고 용이하게 트레이 닝할 수 있다. 또한, 음성합성장치의 인공신경망인 음성분석모듈 및 음성합성모듈은 자기지도 학습시 입력된 음성 및 합성된 음성 사이의 재구성 손실을 산출하고, 산출된 재구성 손실에 기초하여 음성분석모듈 및 음성합성모듈을 학습함 으로써, 음성합성장치에 의하여 합성된 음성과 실제 음성과의 차이점을 최소화한다. 음성합성장치는 손실함수를 활용함으로써 더 자연스럽고 실제 음성과 매우 유사한 음성을 합성할 수 있다. 음성합성방법은 자기지도 학습 방식으로 트레이닝된 인공신경망을 활용하여, 임의의 가창 음성을 사용자가 원하 는 가수의 음성으로 변환할 수 있다. 이에 따라, 합성대상가수가 실제로 부르지 않은 노래이나 마치 합성대상가 수가 실제로 부른 것과 동일하거나 매우 유사하고 자연스러운 가창음성을 합성할 수 있다. 또한, 음성합성방법은 소정의 학습용 음성만으로 음성을 분석하고, 분석 결과에 기초하여 음성을 합성하는 인공 신경망을 학습시킬 수 있다. 예를 들어, 10분 이내의 소용량인 음성을 이용하여 음성합성을 위한 인공신경망을 학습시킬 수 있다. 이에 따라, 많은 양 또는 시간의 음성 데이터를 가지고 있지 않고 현재 작고한 과거의 가수, 위인의 생전 음성이 녹음된 짧은 데이터만을 이용하여 고인의 음성을 복원할 수 있다."}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 개시의 기술적 사상은 이하의 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 이하의 실시예들은 본 개시의 기술적 사상을 완전하도록"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "하고, 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 본 개시의 범주를 완전하게 알려주기 위해 제 공되는 것이며, 본 개시의 기술적 사상은 청구항의 범주에 의해 정의될 뿐이다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시 되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련 된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세 한 설명은 생략한다. 다른 정의가 없다면, 본 명세서에서 사용되는 모든 용어(기술 및 과학적 용어를 포함)는 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 공통적으로 이해될 수 있는 의미로 사용될 수 있다. 또 일반적으로 사용되 는 사전에 정의되어 있는 용어들은 명백하게 특별히 정의되어 있지 않는 한 이상적으로 또는 과도하게 해석되지 않는다. 본 명세서에서 사용된 용어는 실시예들을 설명하기 위한 것이며 본 개시를 제한하고자 하는 것은 아니 다. 본 명세서에서, 단수형은 문구에서 특별히 언급하지 않는 한 복수형도 포함한다. 또한, 본 개시의 구성요소를 설명하는 데 있어서, 제1, 제2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러 한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질이 나 차례 또는 순서 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된다 고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성 요소 사이에 또 다른 구성 요소가 \"연결\", \"결합\" 또는 \"접속\"될 수도 있다고 이해되어야 할 것이다. 본 개시에서 사용되는 \"포함한다(comprises)\" 및/또는 \"포함하는(comprising)\"은 언급된 구성요소, 단계, 동작 및/또는 소자는 하나 이상의 다른 구성요소, 단계, 동작 및/또는 소자의 존재 또는 추가를 배제하지 않는다. 이하의 본 발명의 실시예들에 대한 상세한 설명에서 기재된 용어는 다음과 같은 의미를 갖는다. “음성(voice) ”은 사람의 목소리로서 사람의 발성기관을 통하여 발생되는 소리를 의미하고, 사람의 담화(speech)뿐만 아니라 사람의 목소리로 표현된 노래인 가창음성(singing voice)도 포함한다. 소리는 파동의 하나로 목에 있는 후두의 성대가 진동하면서 발생된 진동으로 인해 전달되는 파동을 의미한다. “음성신호”는 음성주체의 음성을 나타내 는 신호를 의미한다. “음성주체”는 해당 음성을 발성한 화자(speaker)를 의미한다. “음색(timbre)”은 신체 발성기관의 구조에 의 해 물리적으로 결정되는 음성주체 고유의 목소리 특색으로 음성의 배음 구조(harmonic structure)에 의해 각 주 체 마다 구별되는 특징을 갖는다. 어느 하나의 실시예에 포함된 구성요소와, 공통적인 기능을 포함하는 구성 요소는, 다른 실시예에서 동일한 명 칭을 사용하여 설명될 수 있다. 반대되는 기재가 없는 이상, 어느 하나의 실시예에 기재된 설명은 다른 실시예 에도 적용될 수 있으며, 중복되는 범위 또는 당해 기술 분야에 속한 통상의 기술자가 자명하게 이해할 수 있는 범위 내에서 구체적인 설명은 생략될 수 있다. 이하, 본 개시의 몇몇 실시예들에 대하여 첨부된 도면에 따라 상세하게 설명한다. 본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 할 것이다. 도 1은 본 발명의 일 실시예에 따른 음성합성장치의 구성도이다. 도 1을 참고하면, 음성합성장치는 프로세 서, 입력모듈, 음성분석모듈, 음성합성모듈, 가창음성합성(SVS: Singing Voice Synthesis)모듈, 음성설계(VOD: Voice Design) 모듈, 텍스트음성변환(TTS: Text To Speech)모듈 , 출력모듈 및 스토리지를 포함한다. 음성합성장치의 프로세서는 음성합성장치의 일반적인 테스크를 처리한다. 음성합성장치의 입력모듈은 사용자로부터 합성하고자 하는 합성대상음성에 대한 음성합성요청 및 음성 합성장치에 포함된 인공신경망을 트레이닝하기 위한 학습데이터를 획득한다. 입력모듈은 사용자가 변 환을 원하는 변환음성에 대한 음성속성, 악보(musical score), 합성음성데이터의 텍스트, 합성대상음성, 합성대 상가수 등을 사용자로부터 입력받는다. 여기에서, 음성속성은 음성주체의 성별, 음성주체의 나이, 음고 등을 포 함한다. 입력모듈의 예로는 키보드, 마우스, 터치패널, 등을 들 수 있다. 음성합성장치의 음성분석모듈은 입력된 음성의 특징을 출력한다. 음성분석모듈은 입력모듈 을 통하여 입력된 음성의 특징을 출력한다. 보다 구체적으로 음성분석모듈은 입력된 음성의 기초 주파수 (fundamental frequency) F0, 주기 진폭(periodic amplitude) Ap[n], 비주기 진폭(aperiodic amplitude) Aap[n], 발음 특징(linguistic feature), 음색 특징(timbre feature)을 출력한다. 여기에서, 음성분석모듈(10 3)은 자기지도학습(unsupervised Learning) 기반으로 음성을 분석한다. 음성분석모듈에서 입력된 음성의 특징을 출력하는 과정에 대하여는 이하에서 상세하게 설명하기로 한다. 음성분석모듈은 출력된 기초 주파 수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징, 음색 특징을 음성합성모듈로 입력한다. 음성합성장치의 음성합성모듈은 음성분석모듈로부터 입력된 입력들에 기초하여 음성을 합성한다. 보다 구체적으로, 음성합성모듈은 음성분석모듈로부터 입력된 기초 주파수 F0, 주기 진폭 Ap[n], 비주 기 진폭 Aap[n], 발음 특징 및 음색 특징에 기초하여, 음성 신호를 합성한다. 음성합성모듈에서 입력된 음 성 특징에 기초하여 음성 신호를 합성하는 과정에 대하여는 이하에서 상세하게 설명하기로 한다. 음성합성장치의 SVS 모듈은 가창음성 신호를 합성하기 위하여 가창음성의 특징을 출력하도록 설정된 모듈이다. SVS 모듈은 입력된 합성대상노래 및 합성대상가수 음성에 기초하여 가창 음성 특징을 출력한다. 보다 구체적으로, SVS 모듈은 입력된 노래 및 가수 음성을 분석하여, 입력된 노래 및 가수의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징을 출력한다. 여기에서, 합성대상노래는 노래에 대한 악보 (Musical Score) 및 가사를 포함한다. SVS 모듈에서 출력되는 가창 음성 특징은 입력된 가수의 목소리로 입력된 합성대상노래를 부른 음성에 대한 음성 특징이다. 여기에서, SVS 모듈은 입력된 노래 및 가수 음성으로부터 입력된 노래 및 가수의 가창 음성 특징을 출력하 도록 사전에 트레이닝된 인공신경망이다. SVS 모듈은 학습용 노래, 학습용 가수 음성, 학습용 가창 음성 특징을 포함하는 학습용 데이터셋에 의하여 사전에 트레이닝된 인공신경망이다. SVS 모듈은 출력된 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징을 음성합성모듈(10 4)로 입력한다. 음성합성모듈는 SVS 모듈로부터 입력된 가창음성특징 및 음성분석모듈로부터 입 력된 음색 특징에 기초하여 입력된 가수의 가창음성으로 입력된 악보를 부른 음성을 합성한다. 음성합성장치의 VOD 모듈은 임의의 음성을 원하는 특색을 갖는 음성으로 변환하기 위하여, 변환음성의 특징을 출력하도록 설정된 모듈이다. VOD 모듈은 사용자가 입력한 변환음성에 대한 음성속성에 기초하여 변환음성의 특징을 출력한다. 보다 구체적으로, VOD 모듈은 입력모듈로부터 입력된 변환음성에 대한 기초 주파수 F0 및 음색 특징을 출력한다. 여기에서, 변환음성은 원래 음성으로부터 목소리, 톤, 음색, 음고 등 과 같은 목소리의 특징이 변환된 음성을 의미한다. 변환음성에 대한 음성속성은 입력모듈을 통하여 사용자 로부터 입력된 명령이다. 변환음성의 음성속성은 기본 음성의 속성에서 변환하고자 하는 음성의 속성으로, 음성주체의 성별, 음성주체의 나이, 음성의 높낮이를 포함한다. 예를 들어, 사용자가 현재 음성주체가 남성인 음성을 여성의 음성으로 변환하 고자 하는 경우, 사용자는 변환음성에 대한 음성속성을 입력할 때 음성주체의 성별을 여성으로 입력한다. 사용 자는 사용자가 변환하고자 하는 음성주체의 특색에 맞추어 변환음성의 음성속성을 입력모듈을 통하여 입력한다. 사용자는 현재 음성을 변환하고 하는 음성속성인 성별, 나이를 입력모듈로 입력할 수 있다. VOD 모듈은 입력된 변환음성에 대한 음성속성으로부터 변환 음성의 특징을 출력하도록 사전에 트레이닝된 인공신경망이다. VOD 모듈은 학습용 음성신호, 학습용 변환음성 속성, 학습용 변환음성의 기초 주파수 F0 및 학습용 변환음성의 음색 특징을 포함하는 학습용 데이터셋에 의하여 사전에 트레이닝된 인공신경망이다. 본 발명의 또 다른 실시예에 따르면, VOD 모듈은 변환전 음성 및 변환음성에 대한 음성속성에 기초하여, 변환음성에 대한 기초 주파수 F0 및 음색 특징을 출력하도록 사전에 트레이닝된 인공신경망이다. VOD 모듈은 출력된 변환음성의 특징인 기초 주파수 F0 및 음색 특징을 음성합성모듈로 입력한다. 음 성합성모듈은 VOD 모듈로부터 입력된 음색 특징 및 기초 주파수 F0, 그리고 음성분석모듈로부터 입력된 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징에 기초하여, 사용자가 입력한 변환된 특징을 갖는 변 환음성을 합성한다. 음성합성장치의 TTS 모듈은 텍스트를 음성으로 변화하기 위하여 입력된 텍스트의 특징을 출력하도록 설정된 모듈이다. TTS 모듈은 입력된 텍스트 및 음성주체(speaker)에 기초하여 대화(speech) 음성 특징을 출력한다. 보다 구체적으로, TTS 모듈은 입력된 텍스트 및 음성주체의 음성을 분석하여, 입력된 텍스트 및 음성주체의 대화 음성 특징을 출력한다. TTS 모듈은 입력된 텍스트 및 음성주체의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징을 출력한다. 여기에서, TTS 모듈은 입력된 텍스트 및 음성주체 음성으로부터 입력된 텍스트 및 음성주체의 대화 음성 특징을 출력하도록 사전에 트레이닝된 인공신경망이다. TTS 모듈은 학습용 텍스트, 학습용 음성주체 음성, 학습용 음성 특징을 포함하는 학습용 데이터셋에 의하여 사전에 트레이닝된 인공신경망이다. 음성합성장치의 출력모듈은 음성을 사용자가 들을 수 있는 청각적 신호로 변환하여 출력한다. 출력모 듈은 음성합성모듈에서 합성된 음성을 청각적 신호로 변환하여 출력한다. 출력모듈의 예로는 스 피커를 들 수 있다. 음성합성장치의 스토리지는 음성 합성을 위하여 필요한 데이터를 저장한다. 예를 들어, 스토리지(10 9)는 음성합성장치를 구성하는 인공신경망을 트레이닝하기 위한 학습용 음성 데이터 세트를 저장한다. 여기 에서, 학습용 음성 데이터 세트는 담화를 나타내는 음성 데이터뿐만 아니라 가창음성을 나타내는 가창음성 데이 터를 포함한다. 본 발명의 실시예들에 따른 음성합성장치에서, 입력모듈, 음성분석모듈 및 음성합성모듈은 프로세서와는 다른 별개의 전용 프로세서에 의하여 구현될 수 있으며, 프로세서에 의하여 수행되는 컴퓨터 프로그램의 실행에 의하여 구현될 수도 있다. 음성합성장치는 이상에서 설명된 구성요소들 외에 추가적인 구성요소를 더 포함할 수 있다. 예를 들어, 음 성합성장치는 도 1에 도시된 바와 같이, 여러 구성요소들 간에 데이터를 전송하기 위한 버스를 포함하고, 도 1에는 생략되어 있으나 각 구성요소에 구동전원을 공급하는 전원부, 인공신경망의 트레이닝을 위한 트레이닝 부, 외부 단말과 데이터, 신호를 주고 받을 수 있는 통신모듈 등의 구성요소를 더 포함할 수 있다. 이와 같이,"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본 실시예가 속하는 기술분야에서 통상의 지식을 가진 자에게 자명한 구성요소에 대한 상세한 설명은 본 실시예 의 특징을 흐려지게 함에 따라 생략하기로 한다. 이하에서는 본 발명의 일 실시예에 따른 음성합성방법을 설명 하는 과정에서 음성합성장치의 각 구성요소에 대하여 상세하게 설명하기로 한다. 도 2는 도 1에 도시된 음성합성장치에서 음성분석모듈 및 음성합성모듈을 트레이닝하는 과정을 도시한 예시도이 다. 도 2를 참고하면, 음성합성장치의 음성분석모듈은 음고 인코더(pitch encoder), 발음 인코 더(linguistic encoder) 및 음색 인코더(timbre encoder)을 포함한다. 음성합성장치의 음성합 성모듈은 사인파 노이즈 생성기(Sinusoid Noise Generator), 프레임 레벨 합성 인공신경망(frame level synthesis neural network), 시간변화음색 인공신경망(time-varying timbre neural network) 및 샘플 레벨 합성 인공신경망(sample level synthesis neural network)을 포함한다. 음성합성장치의 음성분석모듈로 입력된 음성은 음고 인코더, 발음 인코더 및 음색 인코더 로 입력된다. 음고 인코더는 음성의 음고를 분석하는 인코더이다. 음고 인코더는 음성분석모듈로 입력된 음 성에 기초하여 입력된 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]를 출력한다. 음고(pitc h)는 음의 높낮이를 의미하고, 음의 높낮이는 소리의 진동수에 의하여 결정된다. 높은 음의 소리는 높은 진동수 를 가지고, 낮은 음의 소리는 낮은 진동수를 갖는다. 음고 인코더는 CQT(Constant-Q Transform) 및 음고분석 인공신경망(pitch analysis neural network)으로 구성된다. CQT는 일정한 비율로 떨어지는 높이(즉, 음계)를 특정 주파수 대역의 스펙트럼으로 생성한다. CQT는 50 Hz에서 1,000 Hz 범위에서 64개의 주파수 빈(frequency bin)으로 구별하고, 각 주파수 빈의 확률분포 (probability distribution)를 출력한다. CQT는 출력된 각 주파수 빈의 확률분포 스펙트럼을 음고분석 인공신경 망으로 입력한다. 음고분석 인공신경망은 CQT로부터 입력된 각 주파수 빈의 확률분포 스펙트럼을 분석하고, 음고 인코더로 입력된 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]를 출력한다. 음고분석 인공신경망은 자 기지도학습 기반으로 각 주파수 빈의 확률분포 스펙트럼을 분석하고, 입력된 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]를 출력하는 인공신경망 모델이다. 이와 관련하여, 도 3은 음고 인코더의 상세 구조도이다. 보다 구체적으로, 도 3은 음고 인코더의 신경 구 조(neural architectures)를 도시한다. 도 3을 참고하면, 음고 인코더의 음고분석 인공신경망은 9개의 레 이어(layer)로 구성된다. 음고 인코더는 Conv1d, ResBlock, Reshape, GRU, Linear, ReLU, F0 Head, Pamp Head, Ap amp Head, Softmax 및 Exp.Sigmoid 레이어로 구성된다. 각 레이어에 대한 설명은 본 발명의 특징이 흐려지는 것을 방지하기 위하여 생략하기로 한다. 발음 인코더는 음성에 포함된 텍스트의 발음 특징을 분석한다. 발음 인코더는 음성분석모듈로 입력된 음성에 기초하여, 입력된 음성의 발음 특징을 출력한다. 발음 인코더는 입력된 음성에 포함된 텍 스트를 나타내는 발음기호의 특징 벡터를 추출한다. 발음 인코더는 wav2vec(waveform to vector) 및 발음분석 인공신경망(linguistic analysis neural network)으로 구성된다. wav2vec는 입력된 음성을 인식하여 심볼릭 시퀀스(symbolic sequence) 형태의 텍스트로 변환한다. wav2vec는 변환된 텍스트를 발음분석 인공신경망으로 입력한다. wav2vec는 자기지도학습 기반으로 음 성으로부터 텍스트를 인식하는 인공신경망 모델이다. 발음분석 인공신경망은 wav2vec로부터 입력된 텍스트를 분석하고, 음성에 포함된 텍스트의 발음 특징을 출력한 다. 발음분석 인공신경망은 자기지도학습 기반으로 입력된 텍스트를 분석하고, 텍스트의 발음 특징을 출력하는 인공신경망 모델이다. 이와 관련하여, 도 4는 발음 인코더의 상세 구조도이다. 보다 구체적으로, 도 4는 발음 인코더의 신경 구 도를 도시한다. 도 4를 참고하면, 발음 인코더는 6개의 레이어로 구성된다. 발음 인코더는 2개의 PreConv, 2개의 ConvGLU, Conv1d 및 L2 Normalization 레이어로 구성된다. 각 레이어에 대한 설명은 본 발명의 특징이 흐려지는 것을 방지하기 위하여 생략하기로 한다. 음색 인코더는 음성에 포함된 음성주체의 고유의 음색인 음색특징을 분석한다. 음색 인코더는 음성 분석모듈로 입력된 음성에 기초하여, 입력된 음성의 음색 특징을 출력한다. 음색 인코더는 멜-스펙트로그램(Mel-spectrogram) 변환유닛 및 음색 분석 인공신경망(timbre analysis neural network)으로 구성된다. 멜 스펙트로그램 변환 유닛은 입력된 음성을 멜-스펙트로그램으로 변환한다. 멜 스펙트로그램은 멜-스케일로 변환된 음성을 음색분석 인공신경망으로 입력한다. 음색분석 인공신경망은 멜-스펙트로그램 변환유닛으로부터 입력된 음성을 분석하고, 음성을 발화한 음성주체의 음색 특징을 출력한다. 음색분석 인공신경망은 자기지도학습 기반으로 입력된 멜-스케일의 음성을 분석하고, 음 성의 음색 특징을 출력하는 인공신경망 모델이다. 이와 관련하여, 도 5는 음색 인코더의 상세 구조도이다. 보다 구체적으로, 도 5는 음색 인코더의 신경 구 조를 도시한다. 도 5를 참고하면, 음색 인코더는 5개의 레이어로 구성된다. 음색 인코더는 ECAPA- TDNN Blocks, MFA(Multilayer Feature Aggregation), TTB(Timber Token Block), ASP(Attentive Statistical Pooling), Linear 및 L2 Normalization 레이어로 구성된다. 음색 인코더에 의하여 출력되는 음색 특징은글로벌 음색 임베딩(global timbre embedding) 및 음색 토큰(timbre token)을 포함한다. 글로벌 음색 임베딩은 음성 전체의 음색 정보를 벡터 형태로 나타낸다. 음색 토큰은 음성 파형을 나타내는 기초단위로, 음성에서 각 음운에 해당하는 음색적인 특징을 나타내고 벡터 형태로 표현된다. 각 레이어에 대한 설명은 본 발명의 특징이 흐려지는 것을 방지하기 위하여 생략하기로 한다. 음성합성모듈은 사인파 노이즈 생성기, 프레임 레벨 합성 인공신경망, 시간변화음색 인공신경 망 및 샘플 레벨 합성 인공신경망(sample level synthesis neural network)을 포함한다. 사인파 노이즈 생성기는 사인파 및 노이즈를 생성한다. 사인파 노이즈 생성기은 음성분석모듈(10 3)에 의하여 출력된 음성 특징에 기초하여 사인파를 생성한다. 보다 구체적으로, 사인파 노이즈 생성기는 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]에 기초하여 사인파 및 노이즈를 생성한다. 사인파 노이 즈 생성기에서 생성하는 사인파 및 노이즈는 각각 수학식 1 및 2와 같다. [수학식 1]"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "[수학식 2]"}
{"patent_id": "10-2023-0047906", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기에서, Ns는 샘플링 레이트이고, Ap[t]는 프레인 레벨인 Ap[n]을 샘플-레벨로 업샘플링한 값이며, Aap[t]는 프 레임 레벨인 Aap[n]을 샘플-레벨로 업샘플링한 값이다. n[t]는 샘플링된 노이즈 값이고, 보다 구체적으로 -1과 1 사이를 균등하게(uniform) 샘플링하여 뽑히는 노이즈(noise)이다. 사인파 노이즈 생성기는 생성된 사인파 및 노이즈를 더하여 음성합성모듈에 대한 입력 여기 신호 (input excitation signal) z[t] = x[t] + y[t]를 생성한다. 사인파 노이즈 생성기는 생성된 입력 여기 신호 z[t]를 샘플-레벨 합성 인공신경망으로 입력한다. 프레임 레벨 합성 인공신경망은 음성분석모듈로부터 입력된 발음 특징 및 시간변화음색 인공신경망 으로부터 입력된 시간-변화 음색 임베딩(time-varying timbre embedding)에 기초하여 샘플-레벨 합성기 를 위한 프레임-레벨 컨디션(frame level condition)을 생성한다. 프레임-레벨 컨디션은 음성의 각 프레임에서 의 음색, 발음, 감정 등의 세부적인 특징을 나타낸다. 이와 관련하여, 도 6은 프레임 레벨 합성 인공신경망의 상세 구조도이다. 보다 구체적으로, 도 6은 프레임 레벨 합성 인공신경망의 신경 구조를 도시한다. 도 6을 참고하면, 프레임 레벨 합성 인공신경망은 PreConv, 복 수 개의 ConvGLU 및 Conv1d 레이어로 구성된다. 프레임 레벨 합성 인공신경망은 자기지도 학습 기반의 신 경망으로, 상술한 레이어 간의 입력값 처리를 통하여 프레임 레벨 컨디션을 출력한다. 프레임 레벨 합성 인공신경망은 생성된 프레임 레벨 컨디션을 샘플-레벨 합성 인공신경망으로 입력 한다. 시간변화음색 인공신경망은 음성분석모듈로부터 입력된 음색특징에 기초하여 시간-변화 음색 임베딩 을 생성한다. 시간-변화 음색 임베딩은 음성의 타임 스텝마다 음색을 벡터 형태로 나타낸다. 이와 관련하여, 도 7은 시간변화음색 인공신경망의 상세 구조도이다. 보다 구체적으로, 도 7은 시간변화음색 인공신경망의 신경 구 조를 도시한다. 도 7을 참고하면, 시간변화음색 인공신경망은 Multi-Head Attention, Linear, L2 Normalization, Slerp 및 Tile 레이어로 구성된다. 시간변화음색 인공신경망은 자기지도 학습 기반의 신 경망으로, 상술한 레이어 간의 입력값 처리를 통하여 시간-변화 음색 임베딩을 출력한다. 시간변화음색 인공신경망은 생성된 시간-변화 음색 임베딩을 프레임 레벨 합성 인공신경망으로 입 력한다. 샘플-레벨 합성 인공신경망은 사인파, 노이즈 및 프레임-레벨 컨디션에 기초하여 음성을 합성한다. 본 발 명의 일 실시예에 따른 샘플-레벨 합성 인공신경망은 PWGAN(parallel wave generative adversarial network) 모델을 기반으로 한다. 샘플-레벨 합성 인공신경망은 자기지도 학습 기반의 신경망으로, 상술한레이어 간의 입력값 처리를 통하여 음성을 합성하고 출력한다. 도 8은 본 발명의 일 실시예에 따른 자기지도 학습 기반의 음성합성방법의 흐름도이다. 도 8에 도시된 자기지도 학습 기반의 음성합성방법을 수행하는 음성합성장치는 본 발명의 일 실시예에 따른 음성합성방법을 수행하기 전 에 음성합성장치의 인공신경망 중 지도학습(supervised learning) 모델인 SVS 모듈, VOD 모듈, 텍스트음성변환 모듈 각각은 학습용 데이터 세트에 의해 미리 트레이닝되어 있다고 가정한다. 도 8을 참고하면, 801 단계에서, 음성합성장치는 학습용 음성을 나타내는 학습용 음성 신호를 이용하여, 학 습용 음성에 대한 음성 특징을 출력하도록 음성합성장치의 음성분석모듈에 대한 학습을 수행하고, 학 습용 음성에 대한 음성 특징을 출력한다. 보다 구체적으로, 음성합성장치의 프로세서는 스토리지(10 9)에 저장된 학습용 음성 신호를 음성분석모듈로 입력한다. 음성분석모듈은 입력된 학습용 음성 신호 가 나타내는 학습용 음성에 대한 음성 특징을 출력하도록 음성분석모듈에 대한 학습을 수행한다. 음성합성 장치는 자기지도 학습을 통하여 음성분석모듈을 학습시킨다. 학습용 음성 신호는 임의의 음성주체에 의하여 녹음된 음성을 나타내는 신호이다. 학습용 음성은 임의의 신호에 제한되지 않고 사람의 목소리가 녹음된 음성을 학습용으로 활용할 수 있다. 음성합성장치의 프로세서는 스토리지에 저장된 학습용 음성 신호를 음성분석모듈로 입력한 다. 음성합성장치는 음성분석모듈로 하여금 입력된 학습용 음성 신호가 나타내는 음성의 특징을 출력 하도록 학습시키기 위하여 학습용 음성을 음성분석모듈로 입력한다. 음성분석모듈은 복수 개의 인코 더를 포함한다. 음성분석모듈은 입력된 학습용 음성으로부터 음성의 특징을 출력하도록 자기지도 학습을 수행한다. 보다 구체적으로, 음성분석모듈은 입력된 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징을 출력한다. 음성분석모듈은 입력된 학습용 음성 신호를 음고 인코더 , 발음 인코더 및 음색 인코더로 입력한다. 음고 인코더는 입력된 학습용 음성의 음고 특징을 출력한다. 음고 인코더는 음성을 주파수 빈의 확 률분포 스펙트럼으로 변환하는 CQT 및 음성의 음고 특징을 출력하는 음고분석 인공신경망을 포함한다. 음고분석 인공신경망은 자기지도 학습 기반의 신경망이다. 음고 인코더의 CQT는 입력된 학습용 음성을 CQT을 통하여 각 주파수 빈의 확률분포 스펙트럼으로 변환하 고, 음고분석 인공신경망은 각 주파수 빈의 확률분포 스펙트럼에 기초하여 학습용 음성의 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 생성하고 출력한다. 발음 인코더는 입력된 학습용 음성 신호로부터, 학습용 음성 신호가 나타내는 음성에 포함된 텍스트의 발 음 특징을 출력한다. 발음 인코더는 인공신경망을 이용하여 학습용 음성 신호를 분석하고, 학습용 음성에 포함된 텍스트를 나타내는 발음기호의 특징 벡터를 출력한다. 본 발명의 일 실시예에 따른 발음 인코더는 입력된 학습용 음성 신호에 포함된 학습용 음성을 심볼릭 시퀀스로 변환하고, 심볼릭 시퀀스를 학습용 음성의 발음 특징으로 변환하고 출력한다. 본 발명의 또 다른 실시예에 따른 발음 인코더는 입력된 학습용 음성 신호에 포함된 텍스트를 인식하여 추출하고, 추출된 텍스트의 발음 특징을 생성하고 출력한다. 발음 인코더는 음성 신호로부터 텍스트를 인 식하는 wav2vec 및 텍스트로부터 발음 특징을 출력하는 발음분석 인공신경망을 포함한다. 발음분석 인공신경망 은 자기지도 학습 기반의 신경망이다. 발음 인코더의 wav2vec는 입력된 학습용 음성을 심볼릭 시퀀스로 변환하고, 발음분석 인공신경망은 심볼릭 시퀀스를 학습용 음성의 발음 특징으로 변환하고 출력한다. 음색 인코더는 입력된 학습용 음성의 음성 주체의 음색 특징을 출력한다. 음색 인코더는 입력된 학 습용 음성을 멜-스펙트로그램으로 변환하고, 변환된 멜-스펙트로그램을 분석하여 음색 특징을 출력한다. 음색 인코더는 입력된 음성을 멜-스펙트로그램으로 변환하는 멜-스펙트로그램 변환유닛 및 멜-스펙트로그램으 로부터 음색 특징을 출력하는 음색분석 인공신경망을 포함한다. 음색분석 인공신경망은 자기지도 학습 기반의 신경망이다. 음색 인코더는 웨이브폼(wave form)인 음성을 멜-스펙트로그램으로 변환하고, 멜-스펙트로그램을 음색분 석 인공신경망으로 입력한다. 음색분석 인공신경망은 입력된 멜-스펙트로그램을 각 레이어를 거쳐 음색 특징으 로 변환하고 출력한다.음성분석모듈은 각 인코더에서 출력된 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징을 포함하는 학습용 음성의 음성특징을 음성합성모듈로 입력한다. 802 단계에서, 음성합성장치는 학습용 음성의 음성 특징을 이용하여, 학습용 음성의 음성 특징으로부터 음 성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 학습용 음성의 음성 특징으로부터 합성 음성 신호를 합성한다. 보다 구체적으로, 음성합성장치의 음성합성모듈은 음성분석모듈로부터 입력된 학습용 음성의 음성 특징에 기초하여 합성 음성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행한다. 음성합성장치는 자기지도 학습을 통하여 음성합성모듈을 학습시킨다. 여기에서, 합성 음성 신호는 음 성합성모듈에 의하여 합성된 합성 음성을 나타내는 신호이다. 음성합성모듈은 음성분석모듈로부터 입력된 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징에 기초하여, 음성을 합성한다. 보다 구체적으로, 음성분석모듈은 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 사인파 노이 즈 생성기로 입력하고, 발음 특징을 프레임 레벨 합성 인공신경망로 입력하고, 음색 특징을 시간변 화음색 인공신경망로 입력한다. 사인파 노이즈 생성기는 음성분석모듈로부터 입력된 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진 폭 Aap[n]에 기초하여 사인파 및 노이즈를 생성하는 학습을 수행한다. 여기에서, 사인파 노이즈 생성기는 자기지도 학습 기반 인공신경망 모델이다. 사인파 노이즈 생성기는 상술한 수학식 1 및 수학식 2에 기재 된 식에 따라 사인파 및 노이즈를 생성한다. 사인파 노이즈 생성기는 생성된 사인파 및 노이즈를 합산한 입력 여기 신호 z[t]를 샘플-레벨 합성 인공신경망으로 입력한다. 시간변화음색 인공신경망은 음성분석모듈로부터 입력된 음색 특징에 기초하여 시간-변화음색 임베딩 을 생성하는 학습을 수행한다. 여기에서, 시간변화음색 인공신경망은 자기지도 학습 기반 인공신경망 모 델이다. 시간변화음색 인공신경망은 글로벌 음색 임베딩(global timbre embedding) 및 음색 토큰(timbre token)을 포함하는 음색 특징을 복수 개의 레이어에 기반하여 시간-변화음색 임베딩을 생성한다. 시간변화음색 인공신경망은 생성된 시간-변화 음색 임베딩을 프레임 레벨 합성 인공신경망으로 입력한다. 프레임 레벨 합성 인공신경망은 음성분석모듈로부터 입력된 발음 특징 및 시간변화음색 인공신경망 로부터 입력된 시간-변화 음색 임베딩에 기초하여, 프레임-레벨 컨디션을 생성하는 학습을 수행한다. 여 기에서, 프레임 레벨 합성 인공신경망은 자기지도 학습 기반 인공신경망 모델이다. 프레임 레벨 합성 인 공신경망은 발음 특징 및 시간-변화 음색 임베딩을 복수 개의 레이어에 기반하여 프레임 레벨 컨디션을 생성한다. 프레임 레벨 합성 인공신경망은 생성된 프레임 레벨 컨디션을 샘플-레벨 합성 인공신경망 으로 입력한다. 샘플-레벨 합성 인공신경망은 입력 여기 신호 및 프레임 레벨 컨디션으로부터 음성 신호를 합성하는 학습 을 수행한다. 여기에서, 샘플-레벨 합성 인공신경망은 자기지도 학습 기반 인공신경망 모델이다. 샘플-레 벨 합성 인공신경망은 입력 여기 신호 및 프레임 레벨 컨디션에 기초하여 합성 음성 신호를 합성한다. 803 단계에서, 음성합성장치는 입력된 학습용 음성 및 합성 음성 사이의 재구성 손실(reconstruction los s)을 산출하고, 산출된 재구성 손실 및 학습용 음성으로 음성분석모듈 및 음성합성모듈에 대한 학습 을 수행한다. 음성합성장치의 프로세서는 입력된 학습용 음성과 출력된 음성 사이의 재구성 손실을 산 출한다. 재구성 손실은 멀티-스케일 스펙트럼 (MSS: Multi-Scale Spectrogram) 손실(loss), 멜-스펙트로그램 손실, 대립적 손실(adversarial loss) 및 피쳐 매칭 손실(feature matching loss)을 포함한다. MSS 손실은 로 그(log) 스케일 스펙트로그램이 아닌 선형 주파수 스케일 스펙트로그램을 이용한다. 본 발명의 다른 실시예에 따른 음성합성장치는 재구성 손실 이외에 KLD(Kullback-Leibler divergence loss), MSE(Mean Squared Error), RMSE(Root Mean Squared Error), Binary Crossentropy 등과 같은 다른 손실 함수를 사용하여 입력된 학습용 음성 및 출력된 합성 음성 사이의 차이를 산출할 수 있다. 프로세서는 산출된 재구성 손실 및 학습용 음성을 음성분석모듈로 입력한다. 음성분석모듈 및 음성합성모듈은 입력된 재구성 손실 및 학습용 음성에 기초하여 학습을 수행한다. 도 9는 본 발명의 일 실시예에 따른 자기지도학습 기반의 가창음성합성방법의 흐름도이다. 도 9에 도시된 자기 지도 학습 기반의 가창음성합성방법을 수행하는 음성합성장치는 본 발명의 일 실시예에 따른 음성합성방법을 수행하기 전에 음성합성장치의 인공신경망 중 지도학습(supervised learning) 모델인 SVS 모듈, VOD 모 듈, 텍스트음성변환 모듈 각각은 학습용 데이터 세트에 의해 미리 트레이닝되어 있다고 가정한다. 도 9를 참고하면, 901 단계에서, 음성합성장치는 학습용 음성을 나타내는 학습용 음성 신호를 이용하여, 학습용 음성에 대한 음성 특징을 출력하도록 음성합성장치의 음성분석모듈에 대한 학습을 수행하고, 학습용 음성에 대한 음성 특징을 출력한다. 음성합성장치는 자기지도 학습을 통하여 음성분석모듈을 학습시킨다. 901 단계의 음성분석모듈의 동작 및 학습 방식에 대한 구체적인 설명은 801 단계에 기재된 내 용으로 갈음하기로 한다. 902 단계에서, 음성합성장치는 학습용 음성의 음성 특징을 이용하여, 학습용 음성의 음성 특징으로부터 음 성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 학습용 음성의 음성 특징으로부터 합성 음성 신호를 합성한다. 음성합성장치는 자기지도 학습을 통하여 음성합성모듈을 학습시킨다. 902 단계의 음 성합성모듈의 동작 및 학습 방식에 대한 구체적인 설명은 802 단계에서 기재된 내용으로 갈음하기로 한다. 903 단계에서, 음성합성장치는 입력된 학습용 음성 및 합성된 음성 사이의 재구성 손실(reconstruction loss)을 산출하고, 산출된 재구성 손실 및 학습용 음성으로 음성분석모듈 및 음성합성모듈에 대한 학 습을 수행한다. 음성합성장치의 프로세서는 입력된 학습용 음성과 출력된 음성 사이의 재구성 손실을 산출한다. 프로세서는 산출된 재구성 손실 및 학습용 음성을 음성분석모듈로 입력한다. 음성분석모듈 및 음성합성모듈은 입력된 재구성 손실 및 학습용 음성에 기초하여 학습을 수행한다. 903 단계의 음 성합성장치의 재구성 손실 산출 및 학습 방식에 대한 구체적인 설명은 803 단계에 기재된 내용으로 갈음하 기로 한다. 904 단계에서, 음성합성장치는 합성대상노래 및 합성대상가수(reference singer)를 포함하는 가창음성 합성 요청을 획득한다. 음성합성장치의 입력모듈은 사용자로부터 사용자가 듣기를 원하는 합성대상노래 및 합성대상가수를 입력받는다. 입력모듈은 합성대상노래 및 합성대상가수를 SVS 모듈로 입력한다. 여기 에서, 합성대상노래는 사용자가 합성을 원하는 노래에 대한 악보 및 가사를 포함한다. 합성대상노래에 대한 악 보 및 가사를 나타내는 데이터는 음성합성장치의 스토리지에 저장된 데이터일 수 있으며, 사용자에 의 해 입력된 데이터일 수 있다. 905 단계에서, 음성합성장치는 가창음성 합성요청에 기초하여 합성대상가수와 연관된 음성 신호를 획득한다. 음성합성장치는 가창음성 합성요청에 포함된 합성대상가수와 연관된 음성 신호를 획득한다. 예를 들어, 음성합성장치의 프로세서는 스토리지로부터 합성대상가수와 연관된 음성을 검색하고, 합성 대상가수와 연관된 음성을 나타내는 음성신호를 스토리지로부터 획득한다. 다른 실시예에 따르면, 음성합 성장치의 입력모듈은 사용자로부터 합성대상가수와 연관된 음성을 획득할 수 있다. 합성대상가수와 연 관된 음성은 합성대상가수가 부른 노래를 녹음한 음성이다. 음성합성장치는 획득한 합성대상가수와 연관된 음성 신호를 음성분석모듈 및 SVS 모듈로 입력한다. 906 단계에서, 음성합성장치는 가창음성 합성요청 및 상기 합성대상가수와 연관된 음성에 기초하여, 합성대 상노래를 합성대상가수의 목소리로 부른 가창음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 포함하는 가창 음성 특징을 생성하고 출력한다. 보다 구체적으로, 음성합성장치의 SVS 모듈 은 입력된 합성대상노래 및 합성대상가수와 연관된 음성에 기초하여, 가창음성에 대한 가창 음성 특징을 생성하고 출력한다. 가창 음성 특징은 가창 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 포함한다. SVS 모듈은 입력된 합성대상노래의 악보 및 텍스트, 그리고 합성대상가수의 음 성에 기초하여, 합성대상가수의 목소리로 합성대상노래를 부른 가창 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징을 생성하고 출력한다. 여기에서, SVS 모듈은 학습용 데이터세트에 의해 사전에 트레이닝된 인공신경망이다. SVS 모듈은 학 습용 노래, 학습용 가수 및 학습용 가창 음성 특징을 포함하는 SVS 학습용 데이터 세트에 의하여, 노래 및 가수 를 입력하면, 입력된 노래 및 가수에 대한 가창 음성 특징을 출력하도록 사전에 트레이닝된 인공신경망이다. SVS 모듈은 지도학습 방식으로 트레이닝된 인공신경망이다. 907 단계에서, 음성합성장치는 합성대상가수와 연관된 음성 신호에 기초하여, 합성대상가수의 음색 특징을 생성한다. 음성합성장치의 음성분석모듈은 합성대상가수와 연관된 음성을 분석하고 합성대상가수의 음 색 특징을 생성한다. 음성분석모듈은 901 단계에서 자기지도 학습을 통하여 학습된 인공신경망이다. 음성분석모듈은 입력된 합성대상가수와 연관된 음성을 분석하여, 합성대상가수와 연관된 음성의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징을 출력한다. 여기에서, 음성분석모듈은 합성대상가수의 음색 특징만을 음성합성모듈로 입력한다. 908 단계에서, 음성합성장치는 가창 음성 특징 및 합성대상가수의 음색 특징에 기초하여, 합성대상가수의 목소리로 합성대상노래를 부른 음성을 나타내는 가창음성 신호를 합성한다. 음성합성장치의 음성합성모듈 은 SVS 모듈로부터 입력된 합성대상가수의 목소리로 합성대상노래를 부른 가창 음성에 대한 기초 주 파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징, 그리고 음성분석모듈로부터 입력된 합성대상 가수와 연관된 음성의 음색 특징에 기초하여, 가창음성을 합성한다. 음성합성모듈은 902 단계에서 자기지 도 학습을 통하여 학습된 인공신경망이다. 음성합성모듈은 합성된 가창음성 신호를 출력모듈로 입력 한다. 909 단계에서, 음성합성장치의 출력모듈은 가창음성 신호를 출력한다. 출력모듈은 가창음성 신호 를 음파로 변환하고 출력한다. 도 9에 도시된 자기지도학습 기반의 가창음성합성방법은 사전에 자기학습방식으로 학습된 음성분석모듈 및 음성합성모듈을 이용할 수 있으며, 이러한 경우 901 단계, 902 단계 및 903 단계는 생략할 수 있다. 본 발명의 상술한 실시예에 따른 가창음성 합성방법은 상술한 방식으로 사용자가 원하는 노래를 사용자가 원하 는 가수의 목소리로 부른 음성을 합성할 수 있다. 도 10은 본 발명의 다른 실시예에 따른 자기지도학습 기반의 변조음성 합성방법의 흐름도이다. 도 10에 도시된 자기지도학습 기반의 변조음성 합성방법을 수행하는 음성합성장치는 본 발명의 실시예에 따른 음성합성방법을 수행하기 전에 음성합성장치의 인공신경망 중 지도학습(supervised learning) 모델인 SVS 모듈, VOD 모듈, 텍스트음성변환 모듈 각각은 학습용 데이터 세트에 의해 미리 트레이닝되어 있다고 가정한다. 도 10은 참고하면, 1001 단계에서, 음성합성장치는 학습용 음성을 나타내는 학습용 음성 신호를 이용하여, 학습용 음성에 대한 음성 특징을 출력하도록 음성합성장치의 음성분석모듈에 대한 학습을 수행하고, 학습용 음성에 대한 음성 특징을 출력한다. 음성합성장치는 자기지도 학습을 통하여 음성분석모듈을 학습시킨다. 1001 단계의 음성분석모듈의 동작 및 학습 방식에 대한 구체적인 설명은 801 단계에 기재된 내용으로 갈음하기로 한다. 1002 단계에서, 음성합성장치는 학습용 음성의 음성 특징을 이용하여, 학습용 음성의 음성 특징으로부터 음 성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 학습용 음성의 음성 특징으로부터 합성 음성 신호를 합성한다. 음성합성장치는 자기지도 학습을 통하여 음성합성모듈을 학습시킨다. 1002 단계의 음성합성모듈의 동작 및 학습 방식에 대한 구체적인 설명은 802 단계에서 기재된 내용으로 갈음하기로 한 다. 1003 단계에서, 음성합성장치는 입력된 학습용 음성 및 합성된 음성 사이의 재구성 손실(reconstruction loss)을 산출하고, 산출된 재구성 손실 및 학습용 음성으로 음성분석모듈 및 음성합성모듈에 대한 학 습을 수행한다. 음성합성장치의 프로세서는 입력된 학습용 음성과 출력된 음성 사이의 재구성 손실을 산출한다. 프로세서는 산출된 재구성 손실 및 학습용 음성을 음성분석모듈로 입력한다. 음성분석모듈 및 음성합성모듈은 입력된 재구성 손실 및 학습용 음성에 기초하여 학습을 수행한다. 1003 단계의 음성합성장치의 재구성 손실 산출 및 학습 방식에 대한 구체적인 설명은 803 단계에 기재된 내용으로 갈음 하기로 한다. 1004 단계에서, 음성합성장치는 음성변환의 대상이 되는 변환-전-음성을 획득한다. 음성합성장치의 입 력모듈은 사용자로부터 음성변환의 대상이 되는 변환-전-음성을 획득한다. 음성합성장치는 사용자로부 터 입력모듈을 통하여 직접 변환-전-음성을 입력받을 수 있다. 또는 음성합성장치는 입력모듈을 통하여 음성변환의 대상이 되는 변환-전-음성에 대한 명령을 사용자로부터 입력받고, 스토리지로부터 변환 -전-음성을 검색하고 획득할 수 있다. 입력모듈은 획득한 변환-전-음성을 음성분석모듈로 입력한다. 1005 단계에서, 음성합성장치는 획득한 변환-전-음성에 기초하여, 변환-전-음성의 특징을 나타내는 변환-전 -음성 특징을 생성하고 출력한다. 보다 구체적으로, 음성합성장치의 음성분석모듈은 변환-전-음성으로 부터 변환-전-음성의 음성 특징을 생성하고 출력한다. 1001 단계에서 학습된 음성분석모듈은 입력된 변환- 전-음성을 분석하고, 변환-전-음성의 의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및음색 특징을 출력한다. 음성분석모듈은 출력된 변환-전-음성의 음성 특징 중 주기 진폭 Ap[n], 비주기 진 폭 Aap[n] 및 발음 특징을 음성합성모듈로 입력한다. 1006 단계에서, 음성합성장치는 변환음성에 대한 음성속성을 획득한다. 음성합성장치의 입력모듈 은 사용자로부터 사용자가 음성변환을 원하는 변환음성에 대한 음성속성을 입력받는다. 입력모듈은 입력된 음성속성을 VOD 모듈으로 입력한다. 여기에서, 변환음성에 대한 음성속성은 기본 음성의 속성 중 변환하고 자 하는 음성의 속성으로서, 음성주체(speaker)의 나이, 성별, 음고를 포함한다. 사용자는 변환하고자 하는 음 성속성을 입력한다. 예를 들어, 사용자가 현재 음성의 음성주체가 남성인 현재 음성을 여성으로 변환하고자 하 는 경우, 음성속성을 남자에서 여자로 변환하는 명령을 입력한다. 입력모듈은 변환음성에 대한 음성속성을 VOD 모듈로 입력한다. 1007 단계에서, 음성합성장치의 VOD 모듈은 입력된 변환음성에 대한 음성속성에 기초하여, 변환음성 특징을 출력한다. 변환음성 특징은 변환음성에 대한 기초 주파수 F0 및 음색 특징을 포함한다. VOD 모듈은 입력된 변환음성에 대한 음성속성에 기초하여, 사용자가 변환하고자 하는 음성으로 변환된 변환음성에 대한 기 초 주파수 F0 및 음색 특징을 생성하고 출력한다. VOD 모듈은 학습용 데이터세트에 의해 사전에 트레이닝된 인공신경망이다. VOD 모듈은 학습용 음성, 학습용 음성속성, 학습용 기초 주파수 F0 및 학습용 음색 특징을 포함하는 VOD 학습용 데이터 세트에 의하여, 변환음성에 대한 음성속성을 입력하면, 입력된 음성속성에 기초하여 변환음성에 대한 기초 주파수 F0 및 음색 특 징을 출력하도록 사전에 트레이닝된 인공신경망이다. VOD 모듈은 지도학습 방식으로 트레이닝된 인공신경 망이다. VOD 모듈은 출력된 변환음성에 대한 기초 주파수 F0 및 음색 특징을 음성합성모듈로 입력한다. 1008 단계에서, 음성합성장치는 변환음성 특징 및 변환-전-음성 특징에 기초하여, 변환음성을 합성한다. 보 다 구체적으로, 음성합성장치의 음성합성모듈는 음성분석모듈로부터 입력된 변환-전-음성의 주기 진폭 Ap[n], 비주기 진폭 Aap[n] 및 발음 특징, 그리고 VOD 모듈로부터 입력된 변환음성의 기초 주파수 F0 및 음색 특징에 기초하여 변환음성 신호를 합성한다. 음성합성모듈은 1002 단계에서 자기지도 학습을 통하 여 학습된 인공신경망이다. 음성합성모듈은 합성된 변환음성을 나타내는 변환음성 신호를 출력모듈로 입력한다. 1009 단계에서, 음성합성장치의 출력모듈은 변환음성 신호를 출력한다. 출력모듈은 변환음성 신 호를 음파로 변환하고 출력한다. 본 발명의 상술한 실시예에 따른 변조음성 합성방법은 상술한 방식으로 사용자가 원하는 음성속성을 갖도록 음 성을 변환할 수 있다. 도 10에 도시된 자기지도학습 기반의 변조음성 합성방법은 사전에 자기학습방식으로 학습된 음성분석모듈 및 음성합성모듈을 이용할 수 있으며, 이러한 경우 1001 단계, 1002 단계 및 1003 단계는 생략할 수 있다. 도 11은 본 발명의 또 다른 실시예에 따른 자기지도학습 기반의 TTS 합성방법의 흐름도이다. 도 11에 도시된 자 기지도학습 기반의 TTS 합성방법을 수행하는 음성합성장치는 본 발명의 실시예에 따른 음성합성방법을 수행하기 전에 음성합성장치의 인공신경망 중 지도학습(supervised learning) 모델인 SVS 모듈, VOD 모듈 , 텍스트음성변환 모듈 각각은 학습용 데이터 세트에 의해 미리 트레이닝되어 있다고 가정한다. 도 11을 참고하면, 1101 단계에서, 음성합성장치는 학습용 음성을 나타내는 학습용 음성 신호를 이용하여, 학습용 음성에 대한 음성 특징을 출력하도록 음성합성장치의 음성분석모듈에 대한 학습을 수행하고, 학습용 음성에 대한 음성 특징을 출력한다. 음성합성장치는 자기지도 학습을 통하여 음성분석모듈을 학습시킨다. 1101 단계의 음성분석모듈의 동작 및 학습 방식에 대한 구체적인 설명은 801 단계에 기재된 내용으로 갈음하기로 한다. 1102 단계에서, 음성합성장치는 학습용 음성의 음성 특징을 이용하여, 학습용 음성의 음성 특징으로부터 음 성 신호를 합성하도록 음성합성모듈에 대한 학습을 수행하고, 학습용 음성의 음성 특징으로부터 합성 음성 신호를 합성한다. 음성합성장치는 자기지도 학습을 통하여 음성합성모듈을 학습시킨다. 1102 단계의 음성합성모듈의 동작 및 학습 방식에 대한 구체적인 설명은 802 단계에서 기재된 내용으로 갈음하기로 한다. 1103 단계에서, 음성합성장치는 입력된 학습용 음성 및 합성된 음성 사이의 재구성 손실(reconstruction loss)을 산출하고, 산출된 재구성 손실 및 학습용 음성으로 음성분석모듈 및 음성합성모듈에 대한 학 습을 수행한다. 음성합성장치의 프로세서는 입력된 학습용 음성과 출력된 음성 사이의 재구성 손실을 산출한다. 프로세서는 산출된 재구성 손실 및 학습용 음성을 음성분석모듈로 입력한다. 음성분석모듈 및 음성합성모듈은 입력된 재구성 손실 및 학습용 음성에 기초하여 학습을 수행한다. 1103 단계의 음성합성장치의 재구성 손실 산출 및 학습 방식에 대한 구체적인 설명은 803 단계에 기재된 내용으로 갈음 하기로 한다. 1104 단계에서, 음성합성장치는 TTS 합성을 원하는 합성대상텍스트 및 합성대상 음성주체를 획득한다. 음성 합성장치의 입력모듈은 사용자로부터 TTS 합성의 대상이 되는 합성대상텍스트 및 합성대상 음성주체를 획득한다. 합성대상텍스트는 TTS 합성을 원하는 텍스트이고, 합성대상 음성주체는 합성대상텍스트를 읽는 목소 리의 주체를 의미한다. 입력모듈은 획득한 합성대상텍스트를 TTS 모듈로 입력하고, 합성대상 음성주 체를 프로세서로 입력한다. 1105 단계에서, 음성합성장치는 합성대상 음성주체에 기초하여, 합성대상 음성주체와 연관된 음성을 획득한 다. 보다 구체적으로 음성합성장치의 프로세서는 합성대상 음성주체와 연관된 음성을 스토리지에 서 검색하고 획득한다. 프로세서는 합성대상 음성주체와 연관된 음성을 음성분석모듈 및 TTS 모듈 로 입력한다. 1106 단계에서, 음성합성장치는 합성대상 음성주체와 연관된 음성에 기초하여, 합성대상 음성주체의 음성 특징을 생성하고 출력한다. 음성합성장치의 음성분석모듈은 합성대상 음성주체와 연관된 음성으로부터 합성대상 음성주체의 음성특징을 생성하고 출력한다. 1101 단계에서 학습된 음성분석모듈은 입력된 합성대 상 음성주체와 연관된 음성을 분석하고, 합성대상 음성주체와 연관된 음성의 기초 주파수 F0, 주기 진폭 Ap[n], 비주기 진폭 Aap[n], 발음 특징 및 음색 특징을 출력한다. 음성분석모듈은 출력된 합성대상 음성주체와 연 관된 음성 특징 중 음색 특징을 음성합성모듈로 입력한다. 1107 단계에서, 음성합성장치는 합성대상텍스트 및 합성대상 음성주체와 연관된 음성에 기초하여, 합성대상 텍스트를 합성대상 음성주체의 목소리로 읽은 텍스트 음성에 대한 음성특징을 출력한다. 여기에서, 텍스트 음성 은 합성대상텍스트를 합성대상 음성주체의 목소리로 읽은 음성을 의미한다. 텍스트 음성에 대한 음성특성은 텍 스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 포함한다. 음성합성장치의 TTS 모듈은 입력된 합성대상텍스트 및 합성대상 음성주체와 연관된 음성에 기초하여, 사용자가 합성하고자 하 는 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 생성하고 출력한다. TTS 모듈은 학습용 데이터세트에 의해 사전에 트레이닝된 인공신경망이다. TTS 모듈은 학습용 합성텍 스트, 학습용 음성 및 학습용 음성특징을 포함하는 TTS 학습용 데이터세트에 의하여, 합성텍스트 및 합성대상 음성주체와 연관된 음성을 입력하면 입력된 합성텍스트를 합성대상 음성주체의 목소리로 읽은 음성인 텍스트 음 성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 출력하도록 사전에 트레이닝된 인공신경망 이다. TTS 모듈은 출력된 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n]을 음성합 성모듈로 입력한다. 1108 단계에서, 음성합성장치는 텍스트 음성에 대한 음성특징 및 합성대상 음성주체의 음성 특징에 기초하 여, 텍스트 음성을 합성한다. 보다 구체적으로, 음성합성장치의 음성합성모듈은 TTS 모듈로부터 입력된 텍스트 음성에 대한 기초 주파수 F0, 주기 진폭 Ap[n] 및 비주기 진폭 Aap[n], 그리고 음성분석모듈(10 4)로부터 입력된 음색특징에 기초하여, 텍스트 음성을 나타내는 텍스트 음성 신호를 합성한다. 음성합성모듈 은 1102 단계에서 자기지도 학습을 통하여 학습된 인공신경망이다. 음성합성모듈은 합성된 텍스트 음 성 신호를 출력모듈로 입력한다. 1109 단계에서, 음성합성장치의 출력모듈은 텍스트 음성 신호를 출력한다. 출력모듈은 텍스트 음 성 신호를 음파로 변환하고 출력한다. 본 발명의 상술한 실시예에 따른 텍스트 음성 합성방법은 상술한 방식으로 사용자가 음성 합성을 원하는 텍스트 를 원하는 음성주체(화자, speaker)의 목소리로 읽은 음성을 합성할 수 있다. 도 11에 도시된 자기지도학습 기반의 TTS 합성방법은 사전에 자기학습방식으로 학습된 음성분석모듈 및 음 성합성모듈을 이용할 수 있으며, 이러한 경우 1101 단계, 1102 단계 및 1103 단계는 생략할 수 있다. 상술한 본 발명의 실시예들에 따르면, 음성합성장치 및 음성합성방법은 자기지도 학습 기반의 인공지능을 이용 하여 음성 신호를 분석하고 음성 신호의 음성 특징을 추출하고, 추출된 음성 특징에 기초하여 다시 음성을 합성 한다. 인공지능은 음성 신호의 분석 및 합성을 반복하면서 입력된 음성의 음성 특징을 출력하고, 출력된 음성 특징에 기초하여 음성을 다시 합성한다. 본 발명의 실시예들에 따르면, 인공신경망은 스스로 입력된 음성의 음 성 특징을 추출하도록 학습하고, 추출된 음성 특징에 기초하여 합성 음성을 출력하도록 학습한다. 자기지도 학 습 기반의 인공지능 모델을 이용함으로써, 음성 합성 분야에서 지도학습 기반의 인공지능 모델과 비교하여 인공 신경망 모델을 학습시키 위한 방대한 양의 학습용 오디오 데이터가 필요없으며, 음성합성을 위한 인공신경망을 빠르고 용이하게 트레이닝할 수 있다. 또한, 본 발명의 실시예들에 따르면, 음성합성장치의 인공신경망인 음성분석모듈 및 음성합성모듈은 자기지도 학습시 입력된 음성 및 합성된 음성 사이의 재구성 손실을 산출하고, 산출된 재구성 손실에 기초하여 음성분석 모듈 및 음성합성모듈을 학습함으로써, 음성합성장치에 의하여 합성된 음성과 실제 음성과의 차이점을 최소화한 다. 음성합성장치는 손실함수를 활용함으로써 더 자연스럽고 실제 음성과 매우 유사한 음성을 합성할 수 있다. 본 발명의 일 실시예에 따른 음성합성방법은 자기지도 학습 방식으로 트레이닝된 인공신경망을 활용하여, 임의 의 가창 음성을 사용자가 원하는 가수의 음성으로 변환할 수 있다. 이에 따라, 합성대상가수가 실제로 부르지 않은 노래이나 마치 합성대상가수가 실제로 부른 것과 동일하거나 매우 유사하고 자연스러운 가창음성을 합성할 수 있다. 또한, 본 발명의 실시예에 따른 음성합성방법은 소정의 학습용 음성만으로 음성을 분석하고, 분석 결과에 기초 하여 음성을 합성하는 인공신경망을 학습시킬 수 있다. 예를 들어, 10분 이내의 소용량인 음성을 이용하여 음성 합성을 위한 인공신경망을 학습시킬 수 있다. 이에 따라, 많은 양 또는 시간의 음성 데이터를 가지고 있지 않고 현재 작고한 과거의 가수, 위인의 생전 음성이 녹음된 짧은 데이터 만을 이용하여 고인의 음성을 복원할 수 있 다. 이에 더하여, 본 발명의 일 실시예에 따른 음성합성방법은 자기지도 학습 방식으로 트레이닝된 인공신경망을 활 용하여, 임의의 음성주체의 목소리로 녹음된 음성을 다른 음성 특징을 갖는 음성으로 변환할 수 있다. 예를 들 어, 남성의 음성을 여성으로 변조할 수 있으며, 젊은 사람의 음성을 나이든 사람의 음성으로 변조하는 등, 사용 자가 원하는 음성 특징을 갖는 음성으로 자유롭게 음성을 변조할 수 있다. 추가적으로, 본 발명의 일 실시예에 따른 음성합성방법은 자기지도 학습 방식으로 트레이닝된 인공신경망을 활 용하여, 임의의 텍스트를 임의의 음성주체 목소리로 녹음된 음성을 합성할 수 있다. 여기에서, 비조도 학습 방 식으로 트레이닝된 인공신경망을 이용하여 텍스트 음성을 합성함으로써, 마치 사용자가 입력한 음성주체가 직접 읽은 음성을 녹음한 것과 같이 매우 자연스러운 음성을 합성할 수 있다. 한편 상술한 본 발명의 실시예들은 컴퓨터에서 실행될 수 있는 프로그램으로 작성가능하고, 컴퓨터로 읽을 수 있는 기록매체를 이용하여 상기 프로그램을 동작시키는 범용 디지털 컴퓨터에서 구현될 수 있다. 또한, 상술한 본 발명의 실시예에서 사용된 데이터의 구조는 컴퓨터로 읽을 수 있는 기록매체에 여러 수단을 통하여 기록될 수 있다. 상기 컴퓨터로 읽을 수 있는 기록매체는 마그네틱 저장매체 (예를 들면 롬 플로피 디스크 하드디스크 등), 광학적 판독 매체(예를 들면, 시디롬, 디브이디 등)같은 저장매체를 포함한다. 컴퓨터 판독 가능한 기록매 체에는 본 발명의 실시예들에 따른 가창음성 합성 방법을 수행하는 프로그램이 기록된다. 이제까지 본 발명에 대하여 바람직한 실시예들을 중심으로 살펴보았다. 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 변형된 형태로 구현될 수 있 음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명적인 관점에서 고려되어 야 한다. 본 발명의 범위는 전술한 설명이 아니라 특허청구범위에 나타나 있으며, 그와 동등한 범위 내에 있는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0047906", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 음성합성장치의 구성도이다. 도 2는 도 1에 도시된 음성합성장치에서 음성분석모듈 및 음성합성모듈을 트레이닝하는 과정을 도시한 예시도이 다. 도 3은 음고 인코더의 상세 구조도이다. 도 4는 발음 인코더의 상세 구조도이다. 도 5는 음색 인코더의 상세 구조도이다. 도 6은 프레임 레벨 합성 인공신경망의 상세 구조도이다. 도 7은 시간변화음색 인공신경망의 상세 구조도이다. 도 8은 본 발명의 일 실시예에 따른 자기지도 학습 기반의 음성합성방법의 흐름도이다. 도 9는 본 발명의 일 실시예에 따른 자기지도학습 기반의 가창음성합성방법의 흐름도이다. 도 10은 본 발명의 다른 실시예에 따른 자기지도학습 기반의 변조음성 합성방법의 흐름도이다. 도 11은 본 발명의 또 다른 실시예에 따른 자기지도학습 기반의 TTS 합성방법의 흐름도이다."}
