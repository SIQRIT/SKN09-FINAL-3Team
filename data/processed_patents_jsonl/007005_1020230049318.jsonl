{"patent_id": "10-2023-0049318", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0102778", "출원번호": "10-2023-0049318", "발명의 명칭": "딥러닝 네트워크의 필터를 제거하는 방법 및 장치", "출원인": "충북대학교 산학협력단", "발명자": "황영배"}}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "필터를 제거하는 장치에 의해 수행되는 방법에 있어서,딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하는 단계; 및상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 필터들의 중요도를 결정하는 단계는,상기 필터들의 L1-norm 값을 계산하는 단계; 및상기 L1-norm 값에 기반하여, 상기 필터들의 중요도를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 필터들의 중요도를 결정하는 단계는,상기 필터들의 L2-norm 값을 계산하는 단계; 및상기 L2-norm 값에 기반하여, 상기 필터들의 중요도를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 필터들의 중요도를 결정하는 단계는,상기 하나 이상의 레이어 내 중심점의 위치를 설정하는 단계; 및상기 중심점의 위치에 기반하여, 상기 필터들의 중요도를 결정하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 하나 이상의 필터를 제거하는 단계는,상기 필터들의 중요도에 기반하여, 컨볼루션 레이어 내 하나 이상의 필터를 제거하는 단계; 및상기 컨볼루션 레이어 내에서 제거된 상기 하나 이상의 필터의 위치에 기반하여, 배치 정규화 레이어 내 하나이상의 필터를 제거하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 하나 이상의 필터를 제거하는 단계는,복수 개의 레이어를 이용하여 더하기 연산을 수행하는 경우, 상기 필터들의 중요도 및 상기 필터들의 위치에 기반하여 상기 복수 개의 레이어 내 복수 개의 필터를 제거하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "공개특허 10-2024-0102778-3-제1항에 있어서,상기 하나 이상의 필터를 제거하는 단계는,복수 개의 레이어를 연결(concatenate)하는 경우, 상기 필터들의 중요도 및 상기 필터들의 인덱스에 기반하여상기 복수 개의 레이어 내 복수 개의 필터를 제거하는 단계를 포함하는, 방법."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "필터를 제거하는 장치에 있어서,메모리; 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는,딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하고,상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는, 장치."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "하드웨어인 컴퓨팅 장치와 결합되어,딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하는 단계; 및상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 실행시키기 위하여 컴퓨팅 장치로 읽을 수 있는 기록매체에 저장되는, 컴퓨터프로그램."}
{"patent_id": "10-2023-0049318", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "명령어가 저장된, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는 상기 컴퓨터에 의해 실행될 때 상기 컴퓨터로 하여금,딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하는 단계; 및상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 실행하도록 하는, 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "딥러닝 네트워크의 필터를 제거하는 방법 및 장치를 개시한다. 본 개시의 일 실시예에 의하면, 필터를 제거하는 방법에 있어서, 딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하는 단계 및 상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 딥러닝 네트워크의 필터를 제거하는 방법 및 장치에 관한 것이다. 보다 구체적으로, 딥러닝 네트워크 의 계층 내 필터의 중요도를 판단하고 중요도가 낮은 필터를 제거하는 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 인공지능(Artificial Intelligence, AI) 시스템은 인간 수준의 지능을 구현하는 컴퓨터 시스템이며, 기존 규칙 기반 스마트 시스템과 달리 기계가 스스로 학습하고 판단하며 똑똑해지는 시스템이다. 인공지능 시스템은 사용 할수록 인식률이 향상되고 사용자 취향을 보다 정확하게 이해할 수 있게 되어, 기존 규칙 기반 스마트 시스템은 점차 딥러닝 기반 인공지능 시스템으로 대체되고 있다. 딥러닝 네트워크는 컴퓨터 비전 영역에서 뛰어난 성능을 보이므로 AI 산업의 핵심이다. 성능이 높은 딥러닝 네트워크는 일반적으로 구조가 복잡하고 매개변수가 많아 고 성능의 계산 장치와 큰 메모리를 가지는 저장장치가 필요하다. 모바일 환경 및 저 사양의 기기에서 딥러닝 네트 워크를 활용하기 위해서는 성능 저하를 최소화하면서 계산량과 메모리를 압축하기 위한 방법이 필요하다. 이를 위해서, 필터 프루닝(Pruning) 기법은 컨볼루션 레이어의 필터 자체를 제거하여 연산량을 줄이는 방법이다. 컨볼루션 레이어 내 필터들의 중요도가 판단되고 중요도가 낮은 필터가 프루닝된다. 중요도가 낮은 필터의 값을 0으로 두는 경우, 딥러닝 네트워크의 메모리 및 계산량은 유지된다. 이에 따라, 중요도가 낮은 필 터를 실제로 제거하여 딥러닝 네트워크를 압축할 필요가 있다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 딥러닝 네트워크의 성능을 유지하면서 필터를 제거할 수 있다. 또한, 일 실시예에 따르면, 저 사양의 기기에 고성능의 딥러닝 네트워크를 적용할 수 있다. 또한, 일 실시예에 따르면, 실제 산업 현장에서 필요한 기기의 단가를 낮추어 비용을 절감할 수 있다. 또한, 일 실시예에 따르면, 딥러닝 네트워크의 계산량 및 메모리를 압축할 수 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시에 따른, 필터를 제거하는 방법은, 딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하 는 단계 및 상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 포함할 수 있다. 본 개시에 따른, 필터를 제거하는 장치에 있어서, 메모리; 및 적어도 하나의 프로세서를 포함하고, 상기 적어도 하나의 프로세서는, 딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하고, 상기 필터들의 중 요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거할 수 있다. 본 개시에 따른, 컴퓨터 프로그램은, 하드웨어인 컴퓨팅 장치와 결합되어, 딥러닝 네트워크의 하나 이상의 레이 어 내 필터들의 중요도를 결정하는 단계 및 상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱 스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 실행시키기 위하여 컴퓨팅 장치로 읽을 수 있는 기록매체에 저장될 수 있다. 본 개시에 따른, 컴퓨터로 읽을 수 있는 기록매체는, 명령어가 저장된, 컴퓨터로 읽을 수 있는 기록매체로서, 상기 명령어는 상기 컴퓨터에 의해 실행될 때 상기 컴퓨터로 하여금, 딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정하는 단계 및 상기 필터들의 중요도, 상기 필터들의 위치 및 상기 필터들의 인덱스 중 적어도 하나에 기반하여, 상기 하나 이상의 레이어 내 하나 이상의 필터를 제거하는 단계를 실행하도록 할 수 있다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 딥러닝 네트워크의 성능을 유지하면서 필터를 제거할 수 있는 효과가 있다. 또한, 일 실시예에 따르면, 저 사양의 기기에 고성능의 딥러닝 네트워크를 적용할 수 있는 효과가 있다. 또한, 일 실시예에 따르면, 실제 산업 현장에서 필요한 기기의 단가를 낮추어 비용을 절감할 수 있는 효과가 있 다. 또한, 일 실시예에 따르면, 딥러닝 네트워크의 계산량 및 메모리를 압축할 수 있는 효과가 있다. 본 개시에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은"}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "아래의 기재로부터 본 개시가 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0049318", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 도 1은 본 개시의 일 실시예에 따른, 컨볼루션 레이어와 배치 정규화 레이어에서 필터를 제거하는 방법을 설명 하기 위한 도면이다. 딥러닝 네트워크의 레이어 내 필터를 프루닝하는 방법이 존재할 수 있다. 일 예로, FPGM(Filter Pruning via Geometric Median) 방법은 레이어 내에서 기하 중앙값을 찾고 이러한 기하 중앙값 주 위에 있는 필터들을 프루닝하는 방법에 해당할 수 있다. 이러한 방법은 이하, GM 방법으로 후술한다. 일 예로, 레이어 내 필터들의 L1-norm을 계산하고 미리 정한 비율에 따라 L1-norm 값이 작은 필터들을 프루닝하는 방법이 존재할 수 있다. 여기서, L1-norm 값은 벡터들의 각 원소들의 차이의 절대값의 합에 해당할 수 있다. 이러한 방 법은 이하, L1-norm 방법으로 후술한다. 일 예로, 레이어 내 필터들의 L2-norm을 계산하고 미리 정한 비율에 따 라 L2-norm 값이 작은 필터들을 프루닝하는 방법이 존재할 수 있다. 여기서, L2-norm 값은 벡터들의 직선거리에 해당할 수 있다. 이러한 방법은 이하, L2-norm 방법으로 후술한다. 도 1을 참조하면, GM 방법, L1-norm 방법 또는 L2-norm 방법 등을 이용하여 컨볼루션 레이어 내 필터들의 중요 도가 판단될 수 있다. 중요도가 낮은 필터들은 필터의 값이 0으로 설정되지 않고 실제로 제거될 수 있다. 배치 정규화 레이어는 컨볼루션 레이어와 활성화 레이어 사이에 위치하여 컨볼루션 레이어의 출력 값을 정규화할 수 있다. 컨볼루션 레이어의 출력 값이 정규화되는 경우, 가중치 초기 값의 영향이 낮아져 학습속도가 빨라질 수 있다. 다만, 배치 정규화 레이어에 의해 컨볼루션 레이어에서 0으로 바뀐 출력이 다른 값으로 살아날 수 있다. 이에 따라, 네트워크 압축이 방해될 수 있다. 배치 정규화 레이어 내에서 컨볼루션 레이어 내에서 제거된 필터 들의 위치와 동일한 위치에 존재하는 필터들이 제거될 수 있다. 즉, 배치 정규화 레이어 내에서 제거된 필터들 의 위치와 컨볼루션 레이어 내에서 제거된 필터들의 위치가 동일할 수 있다. 도 2는 본 개시의 일 실시예에 따른, 2개의 레이어를 더하는 경우, 필터를 제거하는 방법을 설명하기 위한 도면 이다.도 2를 참조하면, 레지듀얼 레이어에는 복수개의 레이어가 존재할 수 있다. 레지듀얼 레이어에 존재하는 각 레 이어 내 중요도가 낮은 필터들이 제거될 수 있다. 이후, 후처리하는 경우, 복수개의 레이어를 동시에 고려해야 한다. 복수개의 레이어 내 필터들을 제거한 후, 남아있는 필터들의 인덱스는 레어어 별로 다를 수 있다. 여기서, 아무런 처리 없이 복수개의 레이어끼리 더하기(add) 연산을 수행하면 필터의 위치 정보가 무시되어 성 능이 저하될 수 있다. 이에 따라, 서로 더하는 레어어들끼리 남아있는 필터의 인덱스를 고려하여 합집합 연산이 수행될 수 있다. 합집합 연산을 수행한 후, 레이어들이 서로 더해질 수 있다. 일 예로, 2개의 레이어를 더하는 경우, 첫번째 레이어 내 중요도가 낮은 필터의 위치와 두번째 레이어 내 중요 도가 낮은 필터의 위치가 동일한 경우, 해당 위치에 있는 필터들이 모두 제거될 수 있다. 첫번째 레이어 내 중 요도가 낮은 필터의 위치와 두번째 레이어 내 중요도가 낮은 필터의 위치가 동일하지 않은 경우, 해당 필터들은 제거되지 않고 필터 값이 0으로 설정될 수 있다. 즉, 첫번째 레이어 내 중요도가 낮은 필터의 위치와 두번째 레 이어 내 중요도가 낮은 필터의 위치가 동일한 경우에만, 해당 위치에 있는 중요도가 낮은 필터들이 제거될 수 있다. 이러한 합집합 연산이 수행된 후 첫번째 레이어와 두번째 레이어가 더해질 수 있다. 도 3은 본 개시의 일 실시예에 따른, 2개의 레이어를 연결하는 경우, 필터를 제거하는 방법을 설명하기 위한 도 면이다. 도 3을 참조하면, 복수개의 레이어가 연결(concatenate)될 수 있다. 복수개의 레이어 내 중요도가 낮은 필터들 이 제거될 수 있다. 필터들을 제거한 후 복수개의 레이어가 연결될 수 있다. 여기서, 필요한 필터만을 고르기 위해 인덱스가 필요할 수 있다. 이에 따라, 복수개의 레이어 내 중요도가 낮은 필터들이 제거된 후 남아있는 필 터들의 인덱스가 처리될 수 있다. 이러한 남아있는 필터들의 인덱스를 고려하여 복수개의 레이어가 연결될 수 있다. 일 예로, 2개의 레이어를 연결하는 경우, 첫번째 레이어 내 중요도가 낮은 필터들이 제거되고 남아있는 필터들 의 인덱스인 인덱스 1이 처리될 수 있다. 두번째 레이어 내 중요도가 낮은 필터들이 제거되고 남아있는 필터들 의 인덱스인 인덱스 2가 처리될 수 있다. 인덱스 1과 인덱스 2를 이용하여 2개의 레이어가 연결(concatenate)될 수 있다. 도 4는 본 개시의 일 실시예에 따른, resnet 모델의 필터를 제거한 경우, resnet 모델의 성능을 설명하기 위한 도면이다. 도 4를 참조하면, resnet 모델은 이미지 분류(image classification)를 위해 사용되는 대표적인 모델에 해당할 수 있다. resnet 모델에 GM 방법, L1-norm 방법 또는 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거될 수 있다. 일 예로, resnet101 모델에 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 resnet101 모 델이 70%로 압축된 경우, 정확도는 92.47%이고 계산량은 259.7 MFLOPs에 해당할 수 있다. 이 경우, resnet101 모델의 정확도는 resnet18 모델의 정확도인 90.77%보다 높을 수 있다. resnet101 모델의 계산량은 resnet18 모 델의 계산량인 282.71 MFLOPs보다 낮을 수 있다. 일 예로, resnet50 모델에 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 resnet50 모델이 50%로 압축된 경우, 정확도는 92.70%이고 계산량은 262.02 MFLOPs에 해당할 수 있다. 이 경우, resnet50 모델의 정확 도는 resnet18 모델의 정확도인 90.77%보다 높을 수 있다. resnet50 모델의 계산량은 resnet18 모델의 계산량인 282.71 MFLOPs보다 낮을 수 있다. 일 예로, resnet34 모델에 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 resnet34 모델이 50%로 압축된 경우, 정확도는 91.72%이고 계산량은 280.86 MFLOPs에 해당할 수 있다. 이 경우, resnet34 모델의 정확 도는 resnet18 모델의 정확도인 90.77%보다 높을 수 있다. resnet34 모델의 계산량은 resnet18 모델의 계산량인 282.71 MFLOPs보다 낮을 수 있다. 도 5는 본 개시의 일 실시예에 따른, fcn-resnet 모델의 필터를 제거한 경우, fcn-resnet 모델의 성능을 설명하 기 위한 도면이다. 도 5를 참조하면, fcn-resnet 모델은 의미적 분할(semantic segmentation)에 사용되는 모델에 해당할 수 있다. 일 예로, fcn-resnet101 모델에 L1-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 fcn-resnet101 모 델이 30%로 압축된 경우, 글로벌 정확도는 94.12%이고 계산량은 260.4 GFLOPs이고 mIoU(mean Intersection over Union)는 72.8에 해당할 수 있다. 이 경우, fcn-resnet101 모델의 글로벌 정확도는 fcn-resnet50 모델의 글로벌 정확도인 93.72%보다 높을 수 있다. fcn-resnet101 모델의 계산량은 fcn-resnet50 모델의 계산량인260.93 GFLOPs보다 낮을 수 있다. fcn-resnet101 모델의 mIoU는 fcn-resnet50 모델의 mIoU인 71.2 보다 높을 수 있다. 일 예로, fcn-resnet101 모델에 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 fcn-resnet101 모 델이 30%로 압축된 경우, 글로벌 정확도는 94.23%이고 계산량은 260.72 GFLOPs이고 mIoU는 73.2에 해당할 수 있 다. 이 경우, fcn-resnet101 모델의 글로벌 정확도는 fcn-resnet50 모델의 글로벌 정확도인 93.72%보다 높을 수 있다. fcn-resnet101 모델의 계산량은 fcn-resnet50 모델의 계산량인 260.93 GFLOPs보다 낮을 수 있다. fcn-resnet101 모델의 mIoU는 fcn-resnet50 모델의 mIoU인 71.2 보다 높을 수 있다. 도 6은 본 개시의 일 실시예에 따른, YOLOv3 모델의 필터를 제거한 경우, YOLOv3 모델의 성능을 설명하기 위한 도면이다. 도 6을 참조하면, YOLOv3 모델은 객체 분류(object classification)에 사용되는 모델에 해당할 수 있다. 일 예 로, YOLOv3 모델에 L2-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 YOLOv3 모델이 90%로 압축된 경 우, mAP(mean Average Precision)는 0.424이고 계산량은 3.252 GFLOPs에 해당할 수 있다. 이 경우, YOLOv3 모 델의 mAP는 YOLOv3-tiny 모델의 mAP인 0.370보다 높을 수 있다. YOLOv3 모델의 계산량은 YOLOv3-tiny 모델의 계산량인 5.518 GFLOPs보다 낮을 수 있다. 일 예로, YOLOv3 모델에 L1-norm 방법을 적용하여 중요도가 낮은 필터들이 제거되고 YOLOv3 모델이 90%로 압축 된 경우, mAP는 0.421이고 계산량은 3.229 GFLOPs에 해당할 수 있다. 이 경우, YOLOv3 모델의 mAP는 YOLOv3- tiny 모델의 mAP인 0.370보다 높을 수 있다. YOLOv3 모델의 계산량은 YOLOv3-tiny 모델의 계산량인 5.518 GFLOPs보다 낮을 수 있다. 일 예로, YOLOv3 모델에 GM 방법을 적용하여 중요도가 낮은 필터들이 제거되고 YOLOv3 모델이 90%로 압축된 경 우, mAP는 0.423이고 계산량은 3.248 GFLOPs에 해당할 수 있다. 이 경우, YOLOv3 모델의 mAP는 YOLOv3-tiny 모 델의 mAP인 0.370보다 높을 수 있다. YOLOv3 모델의 계산량은 YOLOv3-tiny 모델의 계산량인 5.518 GFLOPs보다 낮을 수 있다. 도 7은 본 개시의 다른 일 실시예에 따른, YOLOv3 모델의 필터를 제거한 경우, YOLOv3 모델의 성능을 설명하기 위한 도면이다. 도 7을 참조하면, 일 예로, raspberry pi 4 기기에서 YOLOv3 모델에 L1-norm 방법, L2-norm 방법 또는 GM 방법 을 적용하여 중요도가 낮은 필터들이 제거되고 YOLOv3 모델이 50%로 압축된 경우, 지연 시간(latency)이 1.5771(s)이고 계산량이 3.667 GFLOPs에 해당할 수 있다. 이 경우, YOLOv3 모델의 지연 시간은 original 모델 의 지연 시간인 5.0755(s)보다 작을 수 있다. YOLOv3 모델의 계산량은 original 모델의 계산량인 16.414 GFLOPs보다 낮을 수 있다. 일 예로, raspberry pi 4 기기에서 YOLOv3 모델에 L1-norm 방법, L2-norm 방법 또는 GM 방법을 적용하여 중요 도가 낮은 필터들이 제거되고 YOLOv3 모델이 90%로 압축된 경우, mAP는 0.423이고 지연 시간(latency)이 0.6227(s)이고 계산량이 0.812 GFLOPs에 해당할 수 있다. 이 경우, YOLOv3 모델의 지연 시간은 YOLOv3-tiny 모 델의 지연 시간인 0.6024(s)보다 크지만 차이가 거의 나지 않을 수 있다. 반면에, YOLOv3 모델의 계산량은 YOLOv3-tiny 모델의 계산량인 1.379 GFLOPs보다 낮고 YOLOv3 모델의 mAP는 YOLOv3-tiny 모델의 mAP인 0.370보 다 클 수 있다. 도 8은 본 개시의 일 실시예에 따른, 필터를 제거하는 방법을 설명하기 위한 도면이다. 도 8을 참조하면, 필터를 제거하는 장치는 딥러닝 네트워크의 하나 이상의 레이어 내 필터들의 중요도를 결정할 수 있다(S810). 필터들의 중요도를 결정하는 단계는 필터들의 L1-norm 값을 계산하는 단계 및 L1-norm 값에 기 반하여 필터들의 중요도를 결정하는 단계를 포함할 수 있다. 필터들의 중요도를 결정하는 단계는 필터들의 L2- norm 값을 계산하는 단계 및 L2-norm 값에 기반하여 필터들의 중요도를 결정하는 단계를 포함할 수 있다. 필터 들의 중요도를 결정하는 단계는 하나 이상의 레이어 내 중심점의 위치를 설정하는 단계 및 중심점의 위치에 기 반하여 필터들의 중요도를 결정하는 단계를 포함할 수 있다. 필터를 제거하는 장치는 필터들의 중요도, 필터들의 위치 및 필터들의 인덱스 중 적어도 하나에 기반하여, 하나 이상의 레이어 내 하나 이상의 필터를 제거할 수 있다(S820). 하나 이상의 필터를 제거하는 단계는 필터들의 중 요도에 기반하여 컨볼루션 레이어 내 하나 이상의 필터를 제거하는 단계 및 컨볼루션 레이어 내에서 제거된 하 나 이상의 필터의 위치에 기반하여 배치 정규화 레이어 내 하나 이상의 필터를 제거하는 단계를 포함할 수있다. 하나 이상의 필터를 제거하는 단계는 복수 개의 레이어를 이용하여 더하기 연산을 수행하는 경우, 필터들 의 중요도 및 필터들의 위치에 기반하여 복수 개의 레이어 내 복수 개의 필터를 제거하는 단계를 포함할 수 있 다. 하나 이상의 필터를 제거하는 단계는 복수 개의 레이어를 연결(concatenate)하는 경우, 필터들의 중요도 및 필터들의 인덱스에 기반하여 복수 개의 레이어 내 복수 개의 필터를 제거하는 단계를 포함할 수 있다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다. 본 명세서에 설명되는 시스템들 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어 들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 흐름도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시 예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기술 분야 에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 흐름도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수 정 및 변형하여 적용 가능할 것이므로, 흐름도는 시계열적인 순서로 한정되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8"}
{"patent_id": "10-2023-0049318", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른, 컨볼루션 레이어와 배치 정규화 레이어에서 필터를 제거하는 방법을 설명 하기 위한 도면이다. 도 2는 본 개시의 일 실시예에 따른, 2개의 레이어를 더하는 경우, 필터를 제거하는 방법을 설명하기 위한 도면 이다. 도 3은 본 개시의 일 실시예에 따른, 2개의 레이어를 연결(concatenate)하는 경우, 필터를 제거하는 방법을 설 명하기 위한 도면이다. 도 4는 본 개시의 일 실시예에 따른, resnet 모델의 필터를 제거한 경우, resnet 모델의 성능을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시예에 따른, fcn-resnet 모델의 필터를 제거한 경우, fcn-resnet 모델의 성능을 설명하 기 위한 도면이다. 도 6은 본 개시의 일 실시예에 따른, YOLOv3 모델의 필터를 제거한 경우, YOLOv3 모델의 성능을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른, YOLOv3 모델의 필터를 제거한 경우, YOLOv3 모델의 속도를 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시예에 따른, 필터를 제거하는 방법을 설명하기 위한 도면이다."}
