{"patent_id": "10-2020-0022377", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0107409", "출원번호": "10-2020-0022377", "발명의 명칭": "엣지 컴퓨팅 서비스를 이용한 영상 컨텐츠 전송 방법 및 장치", "출원인": "삼성전자주식회사", "발명자": "김지원"}}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "엣지 데이터 네트워크가 영상 컨텐츠를 전송 하는 방법에 있어서,상기 엣지 데이터 네트워크와 연결된 전자 장치로부터 제1 방위 정보를 획득하는 단계;상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정하는 단계;상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하는 단계;상기 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하는 단계;상기 전자 장치로부터 제2 부분 영상에 대응되는 제2 방위 정보를 획득하는 단계;상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하는 단계;상기 비교 결과에 기초하여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영상 또는 상기 제2 방위 정보에 대응되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하는 단계; 및상기 비교 결과에 기초하여, 상기 생성된 보상 프레임을 상기 전자 장치로 전송하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 제1 부분 영상은 복수의 프레임을 포함하는 VR 시퀀스의 1번 프레임 인덱스를 가지는 제1 VR영상 내 부분 영상이고, 상기 제2 예측 부분 영상 및 상기 제2 부분 영상은 상기 VR 시퀀스의 2번 프레임 인덱스를 가지는 제2 VR 영상 내 부분 영상들이며, 상기 제1 부분 영상, 상기 제2 예측 부분 영상, 및 상기 제2 부분 영상은 부분 영상의 위치를 나타내는 방위 정보를 포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서, 상기 제2 예측 부분 영상을 결정하는 단계는상기 제1 방위 정보 및 상기 제2 VR 영상 내 상기 제2 예측 부분 영상의 위치를 나타내는 제2 예측 방위 정보가동일해지도록, 상기 제2 예측 방위 정보를 결정하는 단계; 및상기 결정된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 제2 예측 부분 영상을 결정하는 단계는상기 제1 부분 영상 이전에 상기 전자 장치에서 재생된 부분 영상의 방위 정보 및 상기 제1 방위 정보 사이의제1 변화량을 결정하는 단계;상기 제2 예측 방위 정보 및 상기 제1 방위 정보 사이의 제2 변화량이 상기 제1 변화량과 동일해지도록 상기 제2 예측 방위 정보를 결정하는 단계; 및상기 결정된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2021-0107409-3-제2항에 있어서, 상기 제2 예측 부분 영상을 결정하는 단계는상기 제1 부분 영상 이전에 상기 전자 장치에서 재생된 부분 영상들의 방위 정보, 상기 재생된 부분 영상들에대한 버퍼 관리 정보에 기초하여 재생 패턴을 결정하는 단계;상기 결정된 재생 패턴을 기초로 사용자 별 영상 재생 모델을 생성하는 단계;상기 생성된 영상 재생 모델에 상기 제1 방위 정보를 입력함으로써, 상기 영상 재생 모델로부터 상기 제2 예측방위 정보를 획득하는 단계; 및상기 획득된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는 단계; 를 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서, 상기 제2 예측 프레임은 상기 제2 예측 부분 영상을 인코딩함으로서 생성되는 I프레임 또는 P프레임을 포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제2항에 있어서, 상기 제2 방위 정보는, 상기 제1 부분 영상이 상기 전자 장치에서 재생된 시점으로부터 기 설정된 시간이 경과한 시점 및 상기 제2 예측 부분 영상 또는 상기 제2 부분 영상이 상기 전자 장치에서 재생 될 것으로 예측되는 시점 사이에, 상기 전자장치로부터 획득되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2항에 있어서, 상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하는 단계는상기 제2 예측 방위 정보 및 상기 제2 방위 정보 내 각도 성분들 간의 차이가 기 설정된 임계값 보다 큰 경우,상기 제2 예측 방위 정보 및 상기 제2 방위 정보는 일치하지 않는 것으로 결정하는 단계; 를 더 포함하는,방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2항에 있어서, 상기 보상 프레임을 획득하는 단계는상기 제2 예측 방위 정보 및 상기 제2 방위 정보가 일치하는 것으로 결정되는 경우, 상기 제2 예측 부분 영상을인코딩함으로써 생성된 제2 예측 프레임을 참조하는 상기 보상 프레임을 획득하는 단계: 를 더 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제2항에 있어서, 상기 보상 프레임을 획득하는 단계는상기 제2 예측 방위 정보 및 상기 제2 방위 정보가 일치하지 않는 것으로 결정되는 경우, 상기 제2 부분 영상 및 상기 제1 부분 영상 사이의 제1 변위 정보 및 상기 제2 부분 영상 및 상기 제2 예측 부분 영상 사이의 제2 변위 정보를 포함하고, 상기 제2 예측 부분 영상 및 상기 제1 부분 영상을 참조하는 상기보상 프레임을 획득하는 단계; 를 더 포함하는, 방법."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "공개특허 10-2021-0107409-4-전자 장치로 영상 컨텐츠를 전송하는 엣지 데이터 네트워크에 있어서,네트워크 인터페이스;하나 이상의 인스트럭션들을 저장하는 메모리;상기 하나 이상의 인스트럭션들을 실행하는 프로세서; 를 포함하고, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,상기 엣지 데이터 네트워크와 연결된 전자 장치로부터 제1 방위 정보를 획득하고,상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정하고,상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하고,상기 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하고,상기 전자 장치로부터 제2 부분 영상에 대응되는 제2 방위 정보를 획득하고,상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하고,상기 비교 결과에 기초하여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영상 또는 상기 제2 방위 정보에 대응되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하고,상기 비교 결과에 기초하여, 상기 생성된 보상 프레임을 상기 전자 장치로 전송하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 제1 부분 영상은 복수의 프레임을 포함하는 VR 시퀀스의 1번 프레임 인덱스를 가지는 제1 VR영상 내 부분 영상이고, 상기 제2 예측 부분 영상 및 상기 제2 부분 영상은 상기 VR 시퀀스의 2번 프레임 인덱스를 가지는 제2 VR 영상 내 부분 영상들이며, 상기 제1 부분 영상, 상기 제2 예측 부분 영상, 및 상기 제2 부분 영상은 부분 영상의 위치를 나타내는 방위 정보를 포함하는 것을 특징으로 하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,상기 제1 방위 정보 및 상기 제2 VR 영상 내 상기 제2 예측 부분 영상의 위치를 나타내는 제2 예측 방위 정보가동일해지도록, 상기 제2 예측 방위 정보를 결정하고,상기 결정된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,상기 제1 부분 영상 이전에 상기 전자 장치에서 재생된 부분 영상의 방위 정보 및 상기 제1 방위 정보 사이의제1 변화량을 결정하고,상기 제2 예측 방위 정보 및 상기 제1 방위 정보 사이의 제2 변화량이 상기 제1 변화량과 동일해지도록 상기 제2 예측 방위 정보를 결정하고,상기 결정된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,공개특허 10-2021-0107409-5-상기 제1 부분 영상 이전에 상기 전자 장치에서 재생된 부분 영상들의 방위 정보, 상기 재생된 부분 영상들에대한 버퍼 관리 정보에 기초하여 재생 패턴을 결정하고,상기 결정된 재생 패턴을 기초로 사용자 별 영상 재생 모델을 생성하고,상기 생성된 영상 재생 모델에 상기 제1 방위 정보를 입력함으로써, 상기 영상 재생 모델로부터 상기 제2 예측방위 정보를 획득하고,상기 획득된 제2 예측 방위 정보에 기초하여 상기 제2 예측 부분 영상을 결정하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제12항에 있어서, 상기 제2 예측 프레임은 상기 제2 예측 부분 영상을 인코딩함으로서 생성되는 I프레임 또는 P프레임을 포함하는 것을 특징으로 하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,상기 제2 예측 방위 정보 및 상기 제2 방위 정보가 일치하는 것으로 결정되는 경우, 상기 제2 예측 부분 영상을인코딩함으로써 생성된 제2 예측 프레임을 참조하는 상기 보상 프레임을 획득하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12항에 있어서, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써,상기 제2 예측 방위 정보 및 상기 제2 방위 정보가 일치하지 않는 것으로 결정되는 경우, 상기 제2 부분 영상및 상기 제1 부분 영상 사이의 제1 변위 정보 및 상기 제2 부분 영상 및 상기 제2 예측 부분 영상 사이의 제2변위 정보를 포함하고, 상기 제2 예측 부분 영상 및 상기 제1 부분 영상을 참조하는 상기 보상 프레임을 획득하는, 엣지 데이터 네트워크."}
{"patent_id": "10-2020-0022377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "엣지 데이터 네트워크가 상기 엣지 데이터 네트워크와 연결된 전자 장치로부터 제1 방위 정보를 획득하는 단계;상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정하는 단계;상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하는 단계;상기 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하는 단계;상기 전자 장치로부터 제2 부분 영상에 대응되는 제2 방위 정보를 획득하는 단계;상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하는 단계;상기 비교 결과에 기초하여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영상 또는 상기 제2 방위 정보에 대응되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하는 단계; 및상기 비교 결과에 기초하여, 상기 생성된 보상 프레임을 상기 전자 장치로 전송하는 단계; 를 포함하는, 방법을컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "일 실시 예에 의하면, 엣지 데이터 네트워크가 영상 컨텐츠를 전송하는 방법은 상기 엣지 데이터 네트워크와 연 결된 전자 장치로부터 제1 방위 정보를 획득하는 단계; 상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결 정하는 단계; 상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하는 단계; 상기 제2 예측 부분 영 상을 인코딩함으로써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하는 단계; 상기 전자 장치로부터 제2 부 분 영상에 대응되는 제2 방위 정보를 획득하는 단계; 상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하 는 단계; 상기 비교 결과에 기초하여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영상 또는 상기 제2 방위 정보에 대응되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하는 단계; 및 상기 비교 결과에 기초하여, 상기 생성된 보상 프레임을 상기 전자 장치로 전송하는 단계; 를 포함할 수 있다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 엣지 컴퓨팅 서비스(예: MEC(multi-access edge computing) 서비스)를 이용한 영상 컨텐츠 전송 방 법 및 장치에 관한 것이다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근, 엣지 서버(edge server)를 이용하여 데이터를 전송하는 엣지 컴퓨팅(edge computing) 기술이 논의되고 있 다. 엣지 컴퓨팅 기술은, 예를 들어, MEC(Multi-access Edge Computing) 또는 포그 컴퓨팅(fog computing, FOC)을 포함할 수 있다. 엣지 컴퓨팅 기술은 전자 장치와 지리적으로 가까운 위치, 예를 들어, 기지국 내부 또 는 기지국 근처에 설치된 별도의 서버(이하, '엣지 데이터 네트워크' 또는 'MEC 서버'라 한다)를 통해 전자 장 치로 데이터를 제공하는 기술을 의미할 수 있다. 예를 들어, 전자 장치에 설치된 적어도 하나의 애플리케이션 중 낮은 지연 시간(latency)을 요구하는 애플리케이션은 외부 데이터 네트워크(data network, DN)(예: 인터넷) 에 위치한 서버를 통하지 않고, 지리적으로 가까운 위치에 설치된 엣지 서버를 통해 데이터를 송수신할 수 있다. 최근에는 엣지 컴퓨팅 기술을 이용한 서비스(이하, 'MEC 기반 서비스' 또는 'MEC 서비스'라 한다)에 관하여 논 의되고 있으며, MEC 기반 서비스를 지원하도록 전자 장치에 관한 연구 및 개발이 진행되고 있다. 예를 들면, 전 자 장치의 애플리케이션은 엣지 서버(또는 엣지 서버의 애플리케이션)와 애플리케이션 레이어(application layer) 상에서 엣지 컴퓨팅 기반 데이터를 송수신할 수 있다. MEC 기반 서비스를 지원하기 위한 연구 및 개발이 진행됨에 따라, MEC를 이용한 고해상도 영상 컨텐츠를 전자 장치로 제공하기 위한 기술들이 논의 되고 있다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 논의를 바탕으로, 본 개시는 엣지 컴퓨팅 서비스를 이용한 영상 컨텐츠 전송 방법 및 장치에 관한 것이다. 보다 상세하게는, 엣지 데이터 네트워크가 영상 컨텐츠를 전자 장치로 전송하는 방법 및 전자 장치로 영상 컨텐 츠를 제공하는 엣지 데이터 네트워크를 제공한다. 또한, 본 개시는 전자 장치가 엣지 데이터 네트워크로부터 영상 컨텐츠를 수신하는 방법 및 엣지 데이터 네트워 크로부터 영상 컨텐츠를 수신하는 전자 장치를 제공한다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "일 실시 예에 의하면, 엣지 데이터 네트워크가 영상 컨텐츠를 전송하는 방법은, 상기 엣지 데이터 네트워크와 연결된 전자 장치로부터 제1 방위 정보를 획득하는 단계; 상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정하는 단계; 상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하는 단계; 상기 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하는 단계; 상기 전자 장치로부터 제2 부분 영상에 대응되는 제2 방위 정보를 획득하는 단계; 상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교 하는 단계; 상기 비교 결과에 기초하여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영 상 또는 상기 제2 방위 정보에 대응되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하는 단계; 및 상기 비교 결과에 기초하여, 상기 생성된 보상 프레임을 상기 전자 장치로 전송하는 단계; 를 포함할 수 있 다. 본 개시의 일 실시 예에 의하면, 전자 장치로 영상 컨텐츠를 전송하는 엣지 데이터 네트워크는, 네트워크 인터 페이스; 하나 이상의 인스트럭션들을 저장하는 메모리; 상기 하나 이상의 인스트럭션들을 실행하는 프로세서; 를 포함하고, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써, 상기 엣지 데이터 네트워크와 연결된 전자 장치로부터 제1 방위 정보를 획득하고, 상기 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정하고, 상기 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하고, 상기 제2 예측 부분 영상을 인코딩함으로 써 생성된 제2 예측 프레임을 상기 전자 장치로 전송하고, 상기 전자 장치로부터 제2 부분 영상에 대응되는 제2방위 정보를 획득하고, 상기 제2 예측 방위 정보 및 상기 제2 방위 정보를 비교하고, 상기 비교 결과에 기초하 여, 상기 제1 방위 정보에 대응되는 제1 부분 영상, 상기 제2 예측 부분 영상 또는 상기 제2 방위 정보에 대응 되는 제2 부분 영상 중 적어도 둘을 이용하여 보상 프레임을 생성하고, 상기 비교 결과에 기초하여, 상기 생성 된 보상 프레임을 상기 전자 장치로 전송할 수 있다. 본 개시의 일 실시 예에 의하면, 전자 장치가 상기 전자 장치와 연결된 엣지 데이터 네트워크로부터 수신된 영 상 컨텐츠를 제공하는 방법은, 상기 엣지 데이터 네트워크로 제1 방위 정보를 전송하는 단계; 상기 엣지 데이터 네트워크로부터, 상기 제1 방위 정보에 기초하여 결정된 제2 예측 방위 정보에 대응되는 제2 예측 프레임을 획 득하는 단계; 상기 제2 예측 프레임에 대한 제2 방위 정보를 획득하는 단계; 상기 획득된 제2 방위 정보를 상기 엣지 데이터 네트워크로 전송하는 단계; 상기 제2 방위 정보 및 상기 제2 예측 방위 정보의 일치 여부에 대한 정보를 상기 엣지 데이터 네트워크로부터 획득하는 단계; 상기 제2 방위 정보 및 상기 제2 예측 방위 정보의 일 치 여부에 기초하여, 상기 엣지 데이터 네트워크로부터 보상 프레임을 수신하거나, 상기 제2 예측 프레임의 적 어도 일부를 참조하는 보상 프레임을 생성하는 단계; 상기 제2 방위 정보 및 상기 제2 예측 방위 정보의 일치 여부에 기초하여, 상기 엣지 데이터 네트워크로부터 수신된 보상 프레임 또는 상기 생성된 보상 프레임을 디코 딩하는 단계; 및 상기 디코딩된 보상 프레임을 재생하는 단계; 를 포함할 수 있다. 본 개시의 일 실시 예에 의하면, 엣지 데이터 네트워크로부터 영상 컨텐츠를 수신하는 전자 장치는, 네트워크 인터페이스; 하나 이상의 인스트럭션들을 저장하는 메모리; 상기 하나 이상의 인스트럭션들을 실행하는 프로세 서; 를 포함하고, 상기 프로세서는 하나 이상의 인스트럭션들을 실행함으로써, 상기 엣지 데이터 네트워크로 제 1 방위 정보를 전송하고, 상기 엣지 데이터 네트워크로부터, 상기 제1 방위 정보에 기초하여 결정된 제2 예측 방위 정보에 대응되는 제2 예측 프레임을 획득하고, 상기 제2 예측 프레임에 대한 제2 방위 정보를 획득하고, 상기 획득된 제2 방위 정보를 상기 엣지 데이터 네트워크로 전송하고, 상기 제2 방위 정보 및 상기 제2 예측 방 위 정보의 일치 여부에 대한 정보를 상기 엣지 데이터 네트워크로부터 획득하고, 상기 제2 방위 정보 및 상기 제2 예측 방위 정보의 일치 여부에 기초하여, 상기 엣지 데이터 네트워크로부터 보상 프레임을 수신하거나, 상 기 제2 예측 프레임의 적어도 일부를 참조하는 보상 프레임을 생성하고, 상기 제2 방위 정보 및 상기 제2 예측 방위 정보의 일치 여부에 기초하여, 상기 엣지 데이터 네트워크로부터 수신된 보상 프레임 또는 상기 생성된 보 상 프레임을 디코딩하고, 상기 디코딩된 보상 프레임을 재생할 수 있다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 실시 예는 엣지 데이터 네트워크 및 전자 장치 사이에서 영상 컨텐츠를 효과적으로 수행할 수 있는 방법 을 제공한다."}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 실시 예를 첨부된 도면을 참조하여 상세하게 설명한다. 실시 예를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. 마찬가지 이유로 첨부 도면에 있어서 일부 구성요소는 과장되거나 생략되거나 개략적으로 도시되었다. 또한, 각 구성요소의 크기는 실제 크기를 전적으로 반영하는 것이 아니다. 각 도면에서 동일한 또는 대응하는 구성요소에 는 동일한 참조 번호를 부여하였다. 본 개시의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 상세하게 후술되어 있는 실시 예들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시 예들에 한정되는 것이 아니라 서 로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시 예들은 본 개시가 완전하도록 하고, 본 개시가 속하는"}
{"patent_id": "10-2020-0022377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "기술분야에서 통상의 지식을 가진 자에게 본 개시의 범주를 완전하게 알려주기 위해 제공되는 것이며, 본 개시 는 청구항의 범주에 의해 정의될 뿐이다. 명세서 전체에 걸쳐 동일 참조 부호는 동일 구성 요소를 지칭한다. 이 때, 처리 흐름도 도면들의 각 블록과 흐름도 도면들의 조합들은 컴퓨터 프로그램 인스트럭션들에 의해 수행 될 수 있음을 이해할 수 있을 것이다. 이들 컴퓨터 프로그램 인스트럭션들은 범용 컴퓨터, 특수용 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비의 프로세서에 탑재될 수 있으므로, 컴퓨터 또는 기타 프로그램 가 능한 데이터 프로세싱 장비의 프로세서를 통해 수행되는 그 인스트럭션들이 흐름도 블록(들)에서 설명된 기능들 을 수행하는 수단을 생성하게 된다. 이들 컴퓨터 프로그램 인스트럭션들은 특정 방식으로 기능을 구현하기 위해 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를 지향할 수 있는 컴퓨터 이용 가능 또는 컴퓨터 판독 가능 메모리에 저장되는 것도 가능하므로, 그 컴퓨터 이용가능 또는 컴퓨터 판독 가능 메모리에 저장된 인스트 럭션들은 흐름도 블록(들)에서 설명된 기능을 수행하는 인스트럭션 수단을 내포하는 제조 품목을 생산하는 것도 가능하다. 컴퓨터 프로그램 인스트럭션들은 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에 탑재 되는 것도 가능하므로, 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비 상에서 일련의 동작 단계들이 수행되어 컴퓨터로 실행되는 프로세스를 생성해서 컴퓨터 또는 기타 프로그램 가능한 데이터 프로세싱 장비를수행하는 인스트럭션들은 흐름도 블록(들)에서 설명된 기능들을 실행하기 위한 단계들을 제공하는 것도 가능하 다. 또한, 각 블록은 특정된 논리적 기능(들)을 실행하기 위한 하나 이상의 실행 가능한 인스트럭션들을 포함하는 모듈, 세그먼트 또는 코드의 일부를 나타낼 수 있다. 또, 몇 가지 대체 실행 예들에서는 블록들에서 언급된 기 능들이 순서를 벗어나서 발생하는 것도 가능함을 주목해야 한다. 예컨대, 잇달아 도시되어 있는 두 개의 블록들 은 사실 실질적으로 동시에 수행되는 것도 가능하고 또는 그 블록들이 때때로 해당하는 기능에 따라 역순으로 수행되는 것도 가능하다. 이 때, 본 실시예에서 사용되는 '~부'라는 용어는 소프트웨어 또는 FPGA또는 ASIC과 같은 하드웨어 구성요소를 의미하며, '~부'는 어떤 역할들을 수행한다. 그렇지만 '~부'는 소프트웨어 또는 하드웨어에 한정되는 의미는 아 니다. '~부'는 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재 생시키도록 구성될 수도 있다. 따라서, 일 예로서 '~부'는 소프트웨어 구성요소들, 객체지향 소프트웨어 구성 요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저 들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함한다. 구성요소들과 '~부'들 안에서 제공 되는 기능은 더 작은 수의 구성요소들 및 '~부'들로 결합되거나 추가적인 구성요소들과 '~부'들로 더 분리될 수 있다. 뿐만 아니라, 구성요소들 및 '~부'들은 디바이스 또는 보안 멀티미디어카드 내의 하나 또는 그 이상의 CPU들을 재생시키도록 구현될 수도 있다. 또한 실시예에서 ‘~부’는 하나 이상의 프로세서를 포함할 수 있다. 도 1은 본 개시의 일 실시 예에 따른 네트워크 환경에서 MEC(multi-access edge computing) 기술을 설명하기 위해 개략적으로 도시하는 도면이다. 도 1을 참조하면, 본 개시의 네트워크 환경은 전자 장치, 엣지 데이터 네트워크, 클라우드 서 버 및 액세스 네트워크(access network, AN, 1100)를 포함할 수 있다. 다만, 네트워크 환경이 포함 하는 구성이 이에 제한되는 것은 아니다. 일 실시예에 따르면, 네트워크 환경에 포함되는 구성요소들 각각은 물리적인 객체(entity) 단위를 의미하 거나, 개별적인 기능(function)을 수행할 수 있는 소프트웨어 또는 모듈 단위를 의미할 수 있다. 일 실시예에 따르면, 전자 장치는 사용자에 의해 사용되는 장치를 의미할 수 있다. 예를 들면, 전자 장치 는 단말(terminal), 사용자 단말(UE, user equipment), 이동국(mobile station), 가입자국(subscriber station), 원격 단말(remote terminal), 무선 단말(wireless terminal), 또는 사용자 장치(user device)를 의 미할 수 있다. 또한, 전자 장치는 가상 현실(Virtual Reality, VR), 증강 현실(Augmented Reality, AR), 또는 혼합 현 실(Mixed Reality, MR) 중 적어도 하나를 포함하는 가상 환경에 사용자가 몰입(immersed) 하도록 하기 위한, 컨 텐츠를 제공하는 단말일 수 있다. 즉, 일 실시 예에 의하면, 전자 장치는 가상 현실, 증강 현실 또는 혼 합 현실을 위한 컨텐츠를 제공하는 헤드 마운트 디스플레이(Head Mounted Display, HMD) 또는 가상 현실 헤드셋 (Virtual Reality Headset, VRH)일 수 있다. 도 1을 참조하면, 전자 장치는 제1 애플리케이션 클라이언트(또는, 애플리케이션 클라이언트), 제2 애플리케이션 클라이언트 및 엣지 인에이블러 클라이언트(edge enabler client)(또는, MEL(MEC enabling layer))를 포함할 수 있다. 전자 장치는 MEC 서비스의 사용을 위하여 엣지 인에이블러 클라이언트 를 이용하여 필요한 작업을 수행할 수 있다. 엣지 인에이블러 클라이언트에 대한 구체적인 설명은 후 술된다. 일 실시예에 따르면, 전자 장치는 복수의 애플리케이션들을 실행할 수 있다. 예를 들면, 전자 장치(100 0)는 제1 애플리케이션 클라이언트 및 제2 애플리케이션 클라이언트를 실행할 수 있다. 복수의 애플 리케이션들은 요구되는 데이터 전송 속도, 지연 시간(또는 속도)(latency), 신뢰성(reliability), 네트워크에 접속(access)된 전자 장치의 수, 전자 장치의 네트워크 접속 주기, 또는 평균 데이터 사용량 중 적어도 하 나에 기반하여 서로 다른 네트워크 서비스를 요구(require)할 수 있다. 서로 다른 네트워크 서비스는, 예를 들 어, eMBB(enhanced mobile broadband), URLLC(ultra- reliable and low latency communication), 또는 mMTC(massive machine type communication)를 포함할 수 있다. 전자 장치의 애플리케이션 클라이언트는 전자 장치에 미리 설치된 기본 애플리케이션 또는 제 3자 가 제공하는 애플리케이션을 의미할 수 있다. 즉, 특정 응용 서비스를 위하여 전자 장치 내에서 구동되는클라이언트(client) 응용 프로그램을 의미할 수 있다. 전자 장치 내에는 여러 애플리케이션 클라이언트들 이 구동될 수 있다. 이 애플리케이션 클라이언트들 중 적어도 하나 이상은 엣지 데이터 네트워크로부터 제공되는 서비스를 사용할 수 있다. 예를 들면, 애플리케이션 클라이언트는 전자 장치에 설치되어 실행되 는 애플리케이션으로서, 엣지 데이터 네트워크를 통해 데이터를 송수신하는 기능을 제공할 수 있다. 전자 장치의 애플리케이션 클라이언트는, 하나 이상의 특정 엣지 애플리케이션들에 의해 제공된 기능을 이용하 기 위해, 전자 장치 상에서 실행되는 애플리케이션 소프트웨어를 의미할 수 있다. 일 실시예에 따르면, 전자 장치의 복수의 애플리케이션들(122, 124)은 요구되는 네트워크 서비스 타입에 기반하여 클라우드 서버와 데이터 전송을 수행하거나, 또는 엣지 데이터 네트워크와 엣지 컴퓨팅에 기반한 데이터 전송을 수행할 수 있다. 예를 들어, 제1 애플리케이션 클라이언트가 낮은 지연 시간을 요구 하지 않으면, 제1 애플리케이션 클라이언트는 클라우드 서버와 데이터 전송을 수행할 수 있다. 다른 예를 들어, 제2 애플리케이션 클라이언트가 낮은 지연 시간을 요구하면, 제2 애플리케이션 클라이언트 는 엣지 데이터 네트워크와 MEC 기반 데이터 전송을 수행할 수 있다. 일 실시예에 따르면, 전자 장치의 애플리케이션은, 애플리케이션 클라이언트(application client), 클라 이언트 애플리케이션(client application, Client App), UE 애플리케이션(UE App)으로 지칭될 수 있다. 편의를 위해, 이하, 본 개시에서는 전자 장치의 애플리케이션은 애플리케이션 클라이언트로 지칭된다. 일 실시예에 따르면, 엑세스 네트워크는 전자 장치와의 무선 통신을 위한 채널(channel)을 제공할 수 있다. 예를 들면, 엑세스 네트워크는 RAN(radio access network), 기지국(base station), 이노드비 (eNB, eNodeB), 5G 노드(5G node), 송수신 포인트(TRP, transmission/reception point), 또는 5GNB(5th generation NodeB)를 의미할 수 있다. 일 실시예에 따르면, 엣지 데이터 네트워크는 전자 장치가 MEC 서비스를 이용하기 위하여 접속하는 서버를 의미할 수 있다. 엣지 데이터 네트워크는 전자 장치와 지리적으로 가까운 위치, 예를 들어, 기지 국 내부 또는 기지국 근처에 설치될 수 있다. 일 실시예에 따르면, 엣지 데이터 네트워크는 외부 데이터 네트워크(data network, DN)(예: 인터넷)를 통하지 않고, 전자 장치와 데이터를 송수신할 수 있다. 일 실 시 예에서, MEC는 multi-access edge computing 또는 mobile-edge computing로 지칭될 수 있다. 일 실시예에 따르면, 엣지 데이터 네트워크는, MEC 호스트(host), 엣지 컴퓨팅 서버(edge computing server), 모바일 엣지 호스트(mobile edge host), 엣지 컴퓨팅 플랫폼(edge computing platform), MEC 서버 등 으로 지칭될 수 있다. 편의를 위해, 이하, 본 개시에서는 MEC 서버는 엣지 데이터 네트워크로 지칭된다. 도 1을 참조하면, 엣지 데이터 네트워크는, 제1 엣지 애플리케이션(edge application), 제2 엣지 애플리케이션 및 엣지 인에이블러 서버(또는, MEP(MEC platform))를 포함할 수 있다. 엣지 인에이블 러 서버는 엣지 데이터 네트워크에서 MEC 서비스를 제공하거나 트래픽 제어 등을 수행하는 구성으로, 엣지 인에이블러 서버에 대한 구체적인 설명은 후술된다. 일 실시예에 따르면, 엣지 데이터 네트워크는 복수의 애플리케이션들을 실행할 수 있다. 예를 들면, 엣지 데이터 네트워크는 제1 엣지 애플리케이션 및 제2 엣지 애플리케이션을 실행할 수 있다. 일 실 시예에 따르면, 엣지 에플리케이션은 MEC 서비스를 제공하는 엣지 데이터 네트워크 내 제 3자가 제공하는 응용 애플리케이션을 의미할 수 있고, 엣지 애플리케이션으로 지칭될 수도 있다. 엣지 애플리케이션은 애플리케이션 클라이언트와 관련된 데이터를 송수신하기 위하여, 애플리케이션 클라이언트와 데이터 세션을 형성하는데 이용 될 수 있다. 즉, 엣지 애플리케이션은 애플리케이션 클라이언트와 데이터 세션을 형성할 수 있다. 일 실시예에 서, 데이터 세션은, 전자 장치의 애플리케이션 클라이언트와 엣지 데이터 네트워크의 엣지 애플리 케이션이 데이터를 송수신하기 위하여 형성되는 통신 경로를 의미할 수 있다. 일 실시예에 따르면, 엣지 데이터 네트워크의 애플리케이션은, MEC 애플리케이션(MEC App), ME(MEC) App, 엣지 애플리케이션 서버(edge application server) 및 엣지 애플리케이션으로 지칭될 수 있다. 편의를 위해, 이 하, 본 개시에서는 엣지 데이터 네트워크의 애플리케이션은 엣지 에플리케이션으로 지칭된다. 이때, 애플 리케이션으로 기재되었으나, 엣지 애플리케이션은 엣지 데이터 네트워크에 존재하는 애플리케이션 서버를 의미 할 수 있다. 일 실시예에 따르면, 클라우드 서버는 애플리케이션과 관련된 컨텐츠를 제공할 수 있다. 예를 들어, 클라 우드 서버는 컨텐츠 사업자에 의하여 관리될 수 있다. 일 실시예에 따르면, 클라우드 서버는 외부 데이터 네트워크(data network, DN)(예: 인터넷)를 통해서, 전자 장치와 데이터를 송수신할 수 있다.도 1에는 도시되지 아니하였으나, 엑세스 네트워크와 엣지 데이터 네트워크 사이에 코어 네트워크 (core network, CN) 및 데이터 네트워크(data network, DN)가 존재할 수 있다. 일 실시예에 따르면, 데이터 네 트워크는 코어 네트워크 및 엑세스 네트워크를 통해, 전자 장치에게 데이터(또는 데이터 패킷)를 송수신함으로써 서비스(예: 인터넷 서비스, IMS(IP multimedia subsystem) 서비스)를 제공할 수 있다. 예를 들 어, 데이터 네트워크는 통신 사업자에 의하여 관리될 수 있다. 일 실시 예에서, 엣지 데이터 네트워크는 데이터 네트워크(예: 로컬(local) DN)를 통해 엑세스 네트워크 또는 코어 네트워크와 연결될 수 있다. 본 개시의 일 실시예에 따르면, 전자 장치에서 제1 애플리케이션 클라이언트 또는 제2 애플리케이션 클라이언트가 실행되는 경우, 전자 장치는 엑세스 네트워크를 통해 엣지 데이터 네트워크에 접속함으로써, 애플리케이션 클라이언트를 실행시키기 위한 데이터를 송수신할 수 있다. 본 개시에서는, 상술된 전자 장치, 엣지 데이터 네트워크 및 클라우드 서버사이의 영상 컨텐 츠를 스트리밍하는 방법이 제공될 수 있다. 보다 상세하게는, 전자 장치에서 재생되는 영상에 대한 사용 자의 인터랙션 정보에 기초하여, 가상 현실, 증강 현실 또는 혼합 현실을 위한 컨텐츠를 사용자에게 효과적으로 제공하기 위한 방법이 설명된다. 이하, 도 2는, 전자 장치에서 재생된 영상 컨텐츠에 대한 방위 정보에 기초하여, 엣지 데이터 네트워크가 영상 컨텐츠를 스트리밍하는 방법의 일 실시 예를 설명한다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치와 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 개략적으로 설명하기 위한 도면이다. 일 실시 예에 의하면, 전자 장치는 센싱 모듈, 네트워크 인터페이스, 프로세서 및 메모리 를 포함할 수 있다. 그러나, 전자 장치의 구성은 전술한 바에 한정되지 않고, 더 많은 구성을 포함하거나 적은 구성을 포함할 수 있다. 전자 장치는 엣지 데이터 네트워크 또는 클라우드 서버로부터 수신된 영상들을 디코딩하고, 디코딩된 영 상들을 전자 장치의 디스플레이에 표시할 수 있다. 또한, 전자 장치는 센싱 모듈을 이용하여, 재생 된 영상들에 대한 방위 정보를 획득할 수 있다. 도 4에서 후술하는 바와 같이, 방위 정보는 센싱 모듈을 이용하 여 측정된 전자 장치 사용자가 바라보는 시선의 각도 값들을 포함할 수 있다. 전자 장치는 네트워크 인터페이스 을 이용하여 방위 정보 및 현재 재생 중인 부분 영상의 프레임 인 덱스에 관한 정보를 엣지 데이터 네트워크로 전송한다. 여기서 프레임 인덱스는 프레임의 부/복호화 순서를 나타내는 정보일 수 있으나, 이에 제한되지 않고 프레임의 렌더링 순서를 나타내는 정보일 수 있다. 프로세서는 메모리내 하나 이상의 인스트럭션을 실행함으로써 전자 장치의 전반적인 동작을 제 어한다. 예를 들어, 프로세서는 메모리에 저장된 하나 이상의 인스트럭션을 실행함으로써, 센싱 모듈 , 네트워크 인터페이스 을 제어할 수 있다. 일 실시 예에 의하면, 프로세서는 엣지 데이터 네트 워크로부터, 상기 전자 장치에서 재생된 제1 부분 영상에 대한 제1 방위 정보에 기초하여 결정된 제2 예 측 부분 영상에 관한 예측프레임으로써 I프레임 또는 P프레임을 수신할 수 있다. 또한, 일 실시 예에 의하면, 프로세서는 제2 예측 부분 영상에 관한 I프레임 또는 P프레임을 디코딩할 수 있다. 프로세서는 상기 제2 예측 부분 영상을 재생하기 전 또는 제2 예측 부분 영상 재생 중, 상기 제2 부 분 영상에 대응되는 제2 방위 정보를 획득할 수 있다. 또한, 프로세서는 제2 예측 방위 정보에 기초하여 결정된 제2 예측 부분 영상 및 상기 제2 방위 정보에 대 응되는 제2 부분 영상의 비교 결과에 대한 정보를 엣지 데이터 네트워크로부터 수신할 수 있다. 또 다른 실시 예에 의하면, 프로세서는 엣지 데이터 네트워크로부터 제2 예측 부분 영상에 대응되는 제2 예측 방위 정보 및 제2 부분 영상에 대응되는 제2 예측 방위 정보의 일치 여부에 대한 정보를 상기 엣지 데이터 네트워크로부터 수신할 수도 있다. 또한, 프로세서는 제2 예측 방위 정보 및 제2 방위 정보의 일치 여부에 대한 정보에 기초하여, 상기 엣지 데이터 네트워크로부터, 제1 부분 영상 및 제2 예측 부분 영상에 관한 I프레임 또는 P프레임 중 적어도 일부를 참조하고, 제2 부분 영상에 대응되는 보상 프레임(254, 295)을 수신하거나, 상기 수신된 제2 예측 부분 영상에 관한 I프레임(미도시) 또는 P프레임을 참조하는 보상 프레임을 생성할 수 있다. 예를 들어, 엣지 데 이터 네트워크가 제2 예측 부분 영상 및 제2 부분 영상이 일치하지 않는 것으로 결정함에 따라(또는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정되는 경우), 프로세서가, 엣지 데이터 네트 워크로부터 수신된 보상 프레임을 처리하는 동작은 도 4를 참조하여 구체적으로 설명하기로 한다.또한, 프로세서는 제2 예측 방위 정보 및 제2 방위 정보의 일치 여부에 대한 정보에 기초하여, 엣지 데이 터 네트워크로부터 수신된 보상 프레임 또는 상기 생성된 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 또한, 일 실시 예에 의하면, 메모리는 엣지 데이터 네트워크로부터 수신된 인코딩된 프레임들을 디 코딩하기 위한 인스트럭션들을 저장하는 디코더 모듈, 엣지 데이터 네트워크로부터 수신된 디코딩된 영상 과 실제 전자 장치의 사용자가 바라보는 영상의 일치 여부를 결정하거나, 상기 엣지 데이터 네트워크가 미리 예 측하여 전자 장치로 전송한 예측 프레임의 예측 방위 정보와, 실제 전자 장치의 사용자의 입력에 기초하여 획득 된 방위 정보를 비교하기 위한 비교 모듈 및 엣지 데이터 네트워크로부터 수신된 예측 프레임의 예 측 방위 정보와 실제 전자 장치의 사용자가 바라보는 각도 값에 대한 정보인 방위 정보가 일치할 경우, 전자 장치의 버퍼에 미리 저장된 예측 부분 영상만을 이용하여 생성되는 보상 프레임 생성 방법을 컴퓨터에서 실행시키기 위한 인스트럭션을 저장하는 보상 프레임 생성 모듈을 포함할 수 있으나, 이에 한정되는 것은 아니다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 네트워크 인터페이스, 프로세서 및 메모리(28 0)를 포함할 수 있다. 그러나, 엣지 데이터 네트워크의 구성은 전술한 바에 한정되지 않고, 더 많은 구성을 포 함하거나, 일부 구성이 생략될 수도 있다. 엣지 데이터 네트워크는 네트워크 인터페이스을 이용하여 전자 장치로부터 프레임 인덱스 및 방위 정보를 획득하고, 엣지 데이터 네트워크에서 생성된 보상 프레임을 전자 장치로 전송할 수 있다. 프로세서는 메모리내 하나 이상의 인스트럭션을 실행함으로써 엣지 데이터 네트워크의 전반적 인 동작을 제어한다. 예를 들어, 프로세서는 전자 장치로부터 획득된 제1 방위 정보에 기초하여 , 제2 예 측 방위 정보를 결정하고, 결정된 제2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정할 수 있다. 또한, 일 실시 예에 의하면, 메모리는 엣지 데이터 네트워크가 전자 장치로 전송할 영상들을 인코딩 하기 위한 인스트럭션들을 저장하는 인코더 모듈, 엣지 데이터 네트워크가 예측한 예측 방위 정보와 실제 전자 장치의 사용자가 바라보는 각도 값을 포함하는 방위 정보의 일치 여부를 결정하기 위한 인스트럭션들을 저 장하는 비교 모듈 및 전자 장치의 사용자가 바라보는 각도 값에 대한 정보인 방위 정보에 기초하여 전자 장치의 영상 재생 패턴을 예측하는 재생 패턴 예측 모듈 및 후술하는 VR 시퀀스에 대한 전체 영상 데이터를포함하는 VR 시퀀스를 포함할 수 있으나, 이에 한정되는 것은 아니다. 도 3은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크(edge data network) 사이에서 전송되는 부분 영 상들의 관계를 나타내는 도면이다. 일 실시 예에 따른, 엣지 데이터 네트워크는 가상 현실(VR)을 위한 VR 영상 전체를 전자 장치로 전송하는 것이 아니라, 전자 장치로부터 획득된 방위 정보에 대응되는 VR 영상 내 부분 영상만을 전송함으로써, 효과적으 로 전자 장치의 사용자에게 영상 컨텐츠를 제공할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제1 VR 영상 및 제2 VR 영상에 대한 프레임을 전자 장치로 전송하는 것이 아니고, 제1 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상에 대한 프레임만을 전자 장치로 전송함으로써, 효과적으로 영상 컨 텐츠를 전자 장치로 전송할 수 있다. 일 실시 예에 의하면, 도 3에 도시된 제1 부분 영상, 제2 예측 부분 영상, 제2 부분 영상은 복수의 프레임을 포 함하는 VR 시퀀스의 각 프레임 별(302, 304, 306) 부분 영상일 수 있다. 예를 들어, 제1 부분 영상은, 제1 부분 영상을 포함하는 제1 VR 영상와 동일한 프레임 인덱스를 가지고, 제2 예측 부 분 영상 및 제2 부분 영상은 제2 예측 부분 영상 및 제2 부분 영상을 포함하는 제2 VR 영 상과 동일한 프레임 인덱스를 가질 수 있다. 또한, 일 실시 예에 의하면, VR 시퀀스는 VR 영상의 전부 또는 일부로써, VR 영상에 포함된 복수개의 프레 임들 중 전부 또는 일부의 연속되는 프레임들의 집합일 수 있다. 예를 들어, VR 시퀀스는 1번 프레임 인덱스를 가지는 제1 VR 영상, 2번 프레임 인덱스를 가지는 제2 VR 영상 및 3번 프레임 인덱스를 가지는 제3 VR 영상을 포함할 수 있다. 이때, 1번 프레임 인덱스에 대응하는 프레임, 2번 프레임 인덱스에 대응하는 프레임 및 3번 프레임 인덱스에 대응하는 프레임은 연속적인 프레임일 수 있으나, 이에 제한되지 않고, 이들 사 이에 다른 프레임이 포함될 수 있다. 또한, 제1 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상은, 전자 장치 사용자가 VR 영상 내 특정 부분 영상 을 바라보는 시선의 각도 값을 포함하는 방위 정보를 포함할 수 있다. 예를 들어, 제1 부분 영상은 제1 방위 정 보를 포함하고, 제1 방위 정보에 기초하여 제1 VR 영상 내에서 위치가 결정될 수 있다. 또한, 제2 예측 부분 영 상은 제2 예측 방위 정보를 포함하고, 제2 예측 방위 정보에 기초하여 제2 VR 영상 내에서 위치가 결정될 수 있 으며, 제2 부분 영상은 제2 방위 정보를 포함하고, 제2 방위 정보에 기초하여 제2 VR 영상 내에서 위치가 결정 될 수 있다. 즉 방위 정보는, 각 부분 영상들이 VR 영상에 대한 위치를 나타낼 수 있고, 각 부분 영상들은 방위 정보에 기초 하여 VR 영상 내에서 위치가 결정될 수 있다. 또한, 일 실시 예에 의하면, 방위 정보에 기초하여 구분될 수 있 는 VR 영상 내 부분 영상의 범위는 전자 장치의 사용자가 시청할 수 있는 표시 장치 내 디스플레이의 면 적(예컨대 뷰포트 영역의 크기)에 따라 달라질 수 있다. 또한, 일 실시 예에 의하면, 전자 장치는 센서(예컨대 3축 기울기 센서)를 이용하여 VR 영상 내 특정 부 분 영상을 응시하는 사용자 시선의 방위 정보(예컨대, Roll, Pitch, Yaw 값)를 센싱하고, 센싱된 방위 정보를 방위 정보에 대응되는 부분 영상과 매칭할 수 있다. 전자 장치는 센싱된 방위 정보를 엣지 데이터 네트워 크로 전송함으로써, 현재 전자 장치의 사용자가 VR 영상 내 어떤 부분을 바라보고 있는지에 대한 정보를 엣지 데이터 네트워크와 공유할 수 있다. 일 실시 예에 따라, 이하 도 3 내지 도 5에서 후술하는, 제1 부분 영상은 VR 시퀀스 내, 1번 프레임 인덱 스를 가지는 제1 VR 영상의 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상은, VR 시퀀스 내, 2번 프레임 인덱스를 가지는 제2 VR 영상의 부분 영상임을 가정하여 설명하기로 한다. 도 4는 본 개시의 일 실시 예에 따른 전자 장치와 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. S402에서, 엣지 데이터 네트워크는 전자 장치로부터 제1 방위 정보를 획득할 수 있다. 일 실시 예 에 의하면, 제1 방위 정보는 전자 장치에서 재생되는 제1 부분 영상의 제1 VR 영상 내 위치를 나타내는 정보만을 포함할 수 있다. 그러나, 일 실시 예에 의하면, 제1 방위 정보는 전자 장치에서 제1 부분 영상 이 재생 되기 까지 재생된 부분 영상들에 대한 복수의 방위 정보를 포함할 수 도 있다. S404에서, 전자 장치 는 제1 방위 정보를 엣지 데이터 네트워크로 전송한다. S406에서, 엣지 데이터 네트워크는 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정한다. 예를 들어, 엣지 데이터 네트워크는, 전자 장치로부터 획득되는 제1 방위 정보에 기초하여, 제2 예측 방 위 정보가 제1 방위 정보와 동일해지도록 제2 예측 방위 정보를 결정할 수도 있지만, 엣지 데이터 네트워크 는 제1 방위 정보에 포함된, 복수의 방위 정보들 사이의 변화량에 기초하여, 제2 예측 방위 정보를 결정 할 수도 있다. 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하는 방법은 도 10 내지 도 13에서 구 체적으로 설명하기로 한다. S408에서, 엣지 데이터 네트워크는 제2 예측 방위 정보에 대응하는 제2 예측 부분 영상을 결정할 수 있다. 본 명세서에서 제2 예측 부분 영상은 엣지 데이터 네트워크가 전자 장치의 사용자가 시청할 것으로 예측한 부분 영상이고, 후술하는 제2 부분 영상은, 전자 장치로부터 획득된, 실제 사용자가 바라본 부분 영상의 위치(예컨대 제2 방위 정보)에 대응되는 부분 영상을 의미한다. S410에서, 엣지 데이터 네트워크는 결정된 제2 예측 부분 영상을 인코딩함으로써 제2 예측 프레임을 생성 할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제2 예측 부분 영상을 인코딩함으로써 I프레임 또는 P프 레임을 생성할 수 있다. 상술한 I프레임, P프레임 및 후술하는 B프레임은 국제 표준화 단체 MPEG(Moving Picture Experts Group)에서 개발한 손실 압축 방식 알고리즘에 따라 영상을 압축하는데 사용되는 데이터 프레 임일 수 있다. 또한, B프레임(Bidirectional-coded frame)은 P프레임(Predictive-coded frame) 또는 I프레임 (Intra-coded frame) 중 적어도 하나를 참조함으로써, 높은 압축률로 이미지를 압축하기 위한 데이터 프레임을 의미할 수 있다. S412에서, 엣지 데이터 네트워크는 생성된 제2 예측 프레임을 전자 장치로 전송할 수 있다. 일 실 시 예에 의하면, 엣지 데이터 네트워크는 제2 예측 프레임을 전자 장치로 전송함과 함께, 전자 장 치로 전송된 제2 예측 프레임을 엣지 데이터 네트워크의 데이터 베이스에 별도로 저장해둘 수 있다. S414에서, 전자 장치는 엣지 데이터 네트워크로부터 획득된 제2 예측 프레임을 전자 장치의 버퍼에 캐싱하고, 전자 장치의 센싱 모듈을 이용하여 제2 방위 정보를 획득한다. 보다 상세하게는, 전자 장치는, 전자 장치가 제2 예측 부분 영상을 재생하려는 시점, 제1 부분 영 상을 재생 한 시점으로부터 기 설정된 시간이 경과한 시점 또는 제2 예측 부분 영상이 재생될 것으로 예측한 시 점으로부터 기 설정된 시간만큼 앞선 시점에서, 전자 장치내 센싱 모듈로부터, 제2 방위 정보를 획득할 수 있다. S416에서, 전자 장치는 획득된 제2 방위 정보를 엣지 데이터 네트워크로 전송한다. 예를 들어, 전자 장치는 제2 예측 프레임을 디코딩하고, 디코딩된 제2 예측 프레임을 이용하여 제2 예측 부분 영상을 바로 재생하는 것이 아니라, 제2 예측 부분 영상이 재생될 것으로 예측되는 시점에서, 제2 VR 영상 내 사용자가 바라보고 있는 부분 영상의 위치에 관한 정보를 제2 방위 정보로써 획득하고, 획득된 제2 방위 정 보를 엣지 데이터 네트워크로 전송함으로써, 현재 전자 장치의 버퍼에 캐싱된(cached) 제2 예측 방위 정 보에 따른 제2 예측 부분 영상이 현재 사용자가 제2 VR 영상 내에서 시청하려는 부분 영상이 맞는지 여부를 확 인하기 위해, 제2 방위 정보를 엣지 데이터 네트워크로 전송할 수 있다. S418에서, 엣지 데이터 네트워크는 S406에서 결정한 제2 예측 방위 정보 및 전자 장치로부터 획득된 제2 방위 정보가 일치하는 지 여부를 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제2 방위 정보 및 제2 예측 방위 정보 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 큰 경 우, 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정할 수 있다. 또한, 일 실시 예에 의하면, 엣지 데이터 네트워크는 제2 예측 방위 정보와 제2 방위 정보를 비교할 수도 있지만, 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상 및 제2 방위 정보에 대응되는 제2 부분 영상 자체 를 비교할 수도 있다. 예를 들어, 엣지 데이터 네트워크는 전체 프레임을 포함하는 VR 시퀀스를 데이터 베이스(Data Base, DB)에 저장할 수 있다. 엣지 데이터 네트워크는 전자 장치로부터 획득된 제2 방 위 정보를 이용하여, 데이터 베이스(DB)에 저장된 VR 시퀀스 중, 제2 부분 영상을 식별(identify)할 수 있다. 엣지 데이터 네트워크는 식별된 제2 부분 영상 및 상술한 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상의 픽셀 값들을 비교함으로써, 픽셀 값의 차이가 기 설정된 임계값 이상인 픽셀 들이 소정의 임계치 이상 존재하는 경우, 제2 부분 영상 및 제2 예측 부분 영상이 일치하는 것으로 결정할 수도 있다. S420에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는다는 비교 결과 에 대한 정보를 전자 장치로 전송한다. S422에서, 전자 장치는 엣지 데이터 네트워크로부터 수신된 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는다는 비교 결과에 대한 정보에 기초하여, 제2 예 측 부분 영상의 재생을 대기할 수 있다. S424에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정되 는 경우, 제1 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상에 기초하여, 제2 부분 영상에 대응되는 보상 프 레임을 생성할 수 있다. 보다 상세하게는, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정되는 경우, 전자 장치로부터 획득된 제2 방위 정보를 이용하여, 데이터 베이스(DB) 내 저장된 VR 시퀀스 에 액세스(access)함으로써, VR 시퀀스 내, 제2 방위 정보에 대응되는 제2 부분 영상 을 획득할 수 있다. 일 실시 예에 따라, 도 3을 참조하면, 현재 제1 부분 영상이 전자 장치에서 재생 중이므로, 엣지 데이터 네트워 크의 데이터 베이스에는 제1 부분 영상을 인코딩함으로써 생성된 제1 프레임 및 S412 단계에서 전자 장치 로 전송한 제2 예측 프레임이 이미 저장될 수 있다. 엣지 데이터 네트워크는 S412 단계에서 전자 장치로 전송한 제2 예측 프레임 및 제1 프레임 중 적어도 일부를 참조하도록, 제2 부분 영상에 대응되는 보상 프레임을 생성할 수 있다. 구체적으로, 엣지 데이터 네트워크는 제1 부분 영상에 대응되는 제1 프레임 내 영상 정보 및 제2 부분 영 상에 대응되는 제2 프레임 내 영상 정보의 차이에 대한 정보를 제1 차이 정보로 결정하고, 제2 예측 부분 영상 에 대응되는 제2 예측 프레임 내 영상 정보 및 제2 프레임 내 영상 정보의 차이에 대한 정보를 제2 차이 정보로 결정하며, 제1 차이 정보 및 제2 차이 정보를 이용하여, 제2 부분 영상에 대한 보상 프레임을 생성할 수 있다. 일 실시 예에 의하면, 제1 차이 정보는, 제1 차이 정보를 결정하는데 사용된 제1 프레임 내 블록 주소(또는 위 치) 및 제2 프레임 내 블록 주소(또는 위치)를 나타내는 제1 참조 블록 정보와 제1 차이 정보를 결정하는데 사 용된 제1 프레임 내 블록 및 제2 프레임 내 블록 간의 화소 차이에 대한 정보를 포함할 수 있다. 예를 들어, 제 1 참조 블록 정보는 움직임 벡터 정보를 포함할 수 있다. 움직임 벡터 정보는 제1 프레임 내 블록 위치와 제2 프레임 내 블록 위치의 차이를 나타내는 정보일 수 있다. 또한, 일 실시 예에 의하면, 제2 차이 정보는, 제2 차이 정보를 결정하는데 사용된 제2 프레임 내 블록 주소(또 는 위치) 및 제2 예측 프레임 내 블록 주소(또는 위치)를 나타내는 제2 참조 블록 정보와, 제2 차이 정보를 결정하는데 사용된 제2 프레임 내 블록 및 제2 예측 프레임 내 블록 간의 화소 차이에 대한 정보를 포함할 수 있 다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 제1 부분 영상, 제2 부분 영상 및 제2 예측 부분 영상 순 으로 인코더에 영상들을 입력함으로써, 제1 부분 영상에 대응되는 P프레임, 제2 부분 영상에 대응되는 보상 프 레임 및 제2 예측 부분 영상에 대응되는 P프레임을, 순서대로 인코더로부터 획득할 수 있으나, 인코더의 인코딩 순서는 이와 다를 수 있다. 즉, 엣지 데이터 네트워크 내 인코더는 제1 부분 영상을 제1 프레임으로 인 코딩하고, 제2 예측 부분 영상을 제2 예측 프레임으로 인코딩한 후, 인코딩된 제1 프레임 내 적어도 일부 및 인 코딩된 제2 예측 프레임 내 적어도 일부를 참조하도록 제2 부분 영상을 인코딩할 수 있다. S426에서, 엣지 데이터 네트워크는 S424단계에서 생성된 보상 프레임을 전자 장치로 전송한다. S428에서, 전자 장치는 엣지 데이터 네트워크로부터 획득된 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 예를 들어, 전자 장치는 보상 프레임 내 제1 참조 블록 정보 및 제2 참조 블록 정보를 획득하고, 제1 참조 블록 정보에 대응되는 제1 프레임 내 일부 블록 및 제2 참조 블록 정보에 대응 되는 제2 예측 프레임 내 일부 블록을 참조함으로써, 보상 프레임을 디코딩할 수 있다. 전자 장치는 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 전자 장치의 디스플레이에 전달함으로써, 제2 부분 영상을 전자 장치의 사용자에게 제공할 수 있다. 도 5는 본 개시의 일 실시 예에 따른 전자 장치와 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. 앞서, 도 4에서, 제2 방위 정보 및 제2 예측 방위 정보가 일치하지 않는 경우, 전자 장치와 엣지 데이터 네트워 크 간의 동작 절차를 설명하였고, 도 5를 참조하여 제2 방위 정보 및 제2 예측 방위 정보가 일치하는 경우, 전 자 장치와 엣지 데이터 네트워크 간의 동작 절차를 설명하기로 한다. 도 5의 S502 단계 내지 S516 단계는 도 4 의 S402단계 내지 S416 단계에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S518에서, 엣지 데이터 네트워크는 S506에서 결정한 제2 예측 방위 정보 및 전자 장치로부터 획득된 제2 방위 정보가 일치하는 지 여부를 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제2 방위 정보 및 제2 예측 방위 정보 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 작은 경우, 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정할 수 있다. S520에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치 한다는 비교 결과에 대 한 정보를 전자 장치로 전송한다. S522에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되는 경우, 데이터 베이스(DB) 내 저장되어 있는 제2 예측 부분 영상을 인코 딩함으로써 보상 프레임을 생성할 수 있다. 보다 상세하게는, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정 되는 경우, 제2 예측 부분 영상 및 제2 부분 영상을 동일한 영상으로 결정하고, 제2 예측 프레임의 적어도 일부 블록을 참조하는 보상 프레임을 생성할 수 있다. S524에서, 전자 장치는 제2 예측 방위 정보 및 제2 방위 정보가 일치한다는 비교 결과에 기초하여, S512단계에서 수신된 제2 예측 프레임의 영상 정보와 동일한 영상 정 보를 가지는 보상 프레임을 생성할 수 있다. 본 개시에 따른 전자 장치는, 전자 장치 내 버퍼에 캐 싱된 I프레임 또는 P프레임의 디코딩 순서에 영향을 미치지 않고도 효율적으로 사용자에게 영상 컨텐츠를 제공 하기 위해, 도 16에서 후술하는 바와 같이, 엣지 데이터 네트워크로부터 획득된 보상 프레임 및 전자 장 치가 스스로 생성한 보상 프레임만을 이용하여 영상을 재생할 수 있다. 따라서, 전자 장치는 제2 예측 방위 정보 및 제2 방위 정보가 일치한다는 비교 결과가 수신되는 경우에도, 제2 예측 프레임의 영상 정보 와 동일한 영상 정보를 포함하는 보상 프레임을 생성한다. 보다 상세하게는, 엣지 데이터 네트워크는 제2 예측 프레임 내 영상 정보 및 제2 프레임 내 영상 정보의 차이에 대한 차이 정보를 이용하여 보상 프레임을 생성할 수 있는데, 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되었으므로, 제2 예측 프레임 내 영상 정보 및 제2 프레임 내 영상 정보의 차이 값이 0으 로 결정되고, 그 결과 제2 예측 프레임과 보상 프레임은 동일한 영상 정보를 포함할 수 있다. 또한 도 4에서 전 술한 바와 같이, 제2 예측 프레임 내 영상 정보 및 제2 프레임 내 영상 정보의 차이에 대한 차이 정보는, 제2 예측 프레임 내 블록 및 제2 프레임 내 블록 간의 화소 차이에 대한 정보 외에도, 차이 정보를 결정하는데 사용 된 제2 예측 프레임 내 블록 주소(또는 위치) 및 제2 프레임 내 블록 주소(또는 위치)를 나타내는 참조 블록 정 보를 더 포함한다.S526에서, 엣지 데이터 네트워크는 생성된 보상 프레임을 폐기할 수 있다. 예를 들어, 엣지 데이터 네트 워크는 제2 예측 부분 영상 및 제2 부분 영상이 일치하는 것으로 결정되었으므로, 별도의 프레임들을 전 송할 필요가 없기 때문에, 생성된 보상 프레임을 전자 장치로 전송하지 않고, 폐기할 수 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 경우, 별도의 보상 프레임을 생성하고, 생성된 보상 프레임을 전자 장치로 전송할 필요가 없으므로, S522 단계와 달리, 별도의 보상 프레임을 생성하지 않을 수도 있다. 그러나 또 다른 실시 예에 의하면, 엣지 데이터 네트워 크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되는 경우에도, 제2 예측 프레임을 참 조하는 보상 프레임을 생성함으로써, 전자 장치 측의 프레임 디코딩 동작에 동기화될 수 있다. S528에서, 전자 장치는 S524에서 생성된 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 예를 들어, 전자 장치는 보상 프레임 내 참조 블록 정보를 획득하고, 획득된 참조 블록 정보에 기초하여 제2 예측 프레임 내 일부 블록 및 제2 프레임 내 일부 블록을 참조함으로써, 보상 프레임을 디코딩할 수 있다. 현재, 제2 예측 프레임은 보상 프레임과 동일한 영상 정보를 포함하므로, 전자 장치는 제2 예측 프레임만 을 참조하여 보상 프레임을 디코딩할 수도 있다. 전자 장치는 S524단계에서 생성된 보상 프레임을 디코딩 하고, 디코딩된 보상 프레임을 전자 장치의 디스플레이에 전달함으로써, 제2 예측 부분 영상을 전자 장치의 사 용자에게 제공할 수 있다. 이상, 전자 장치 내 버퍼에 캐싱된 I프레임 또는 P프레임의 디코딩 순서에 영향을 미치지 않고도 효율적 으로 사용자에게 영상 컨텐츠를 제공하기 위해, 제2 예측 방위 정보 및 제2 방위 정보가 일치한다는 비교 결과 가 수신되는 경우에도, S524 단계에서, 제2 예측 프레임의 영상 정보와 동일한 영상 정보를 포함하는 보상 프레 임을 생성하는 내용에 대해 설명하였으나, 이에 제한되지 않고, S524 단계에서, 전자 장치는 보상 프레임 을 별도로 생성하지 않을 수 있다. 이때, 전자 장치는 S514 단계에서 캐싱된 제2 예측 프레임을 디코딩하 고, 디코딩된 제2 예측 프레임(제2 예측 부분 영상)을 전자 장치의 디스플레이에 전달함으로써, 제2 예측 부분 영상을 재생(전자 장치의 사용자에게 제공)할 수 있다. 도 4 내지 5에서, 엣지 데이터 네트워크는 전자 장치로부터 제1 방위 정보 외에, 현재 전자 장치에 서 재생 중인 제1 부분 영상의 프레임 인덱스를 더 획득할 수 있다. 엣지 데이터 네트워크는 전자 장치 로부터 획득된 제1 부분 영상의 프레임 인덱스를 참조하여 제2 예측 프레임의 프레임 인덱스를 결정할 수 있다. 상술한 바와 같이, 전자 장치는 엣지 데이터 네트워크가 제2 예측 방위 정보 및 제2 방위 정보가 일치한다고 결정하는 경우, 엣지 데이터 네트워크가 S512단계에서 전송한 제2 예측 프레임에 대응되는 제2 예측 부분 영상과 현재 제2 방위 정보에 대응되는 제2 부분 영상이 동일한 영상이므로, 전자 장치의 버퍼에 미 리 I프레임 또는 P프레임으로 캐싱된 제2 예측 프레임을 디코딩 한 후, 디코딩된 예측 프레임을 재생할 수도 있 다. 그러나 전자 장치는 엣지 데이터 네트워크가 제2 예측 방위 정보 및 제2 방위 정보가 일치한다고 결정됨 에 따라 엣지 데이터 네트워크로부터 수신되어 버퍼에 I프레임 또는 P프레임으로 미리 캐싱되어 있는 제2 예측 프레임을 이용하여 영상을 재생할 경우, 엣지 데이터 네트워크가 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정됨에 따라 보상 프레임을 이용하여 영상을 재생하는 경우가 혼재됨으로써, 전자 장치 의 버퍼 관리 또는 프레임들의 참조 관계에 따른 디코딩 동작이 효율적으로 이루어지지 않을 수 있다. 따라서, 본 개시에 따른 전자 장치는 엣지 데이터 네트워크의 영상 예측이 성공하는 경우, 스스로 생성한 보상 프레임을 디코딩한 후, 디코딩된 보상 프레임을 버퍼에 캐싱(caching)하고, 엣지 데이터 네트워크의 영상 예측이 실패하는 경우에는 S428에서와 같이 엣지 데이터 네트워크로부터 수신된 보상 프레임을 디코 딩한후, 디코딩된 보상 프레임을 버퍼에 순차적으로 캐싱(caching)한 다음 버퍼에 저장된 보상 프레임들만을 렌 더링함으로써, 효과적으로 사용자에게 VR 영상을 제공할 수 있다. 도 6은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 사이에서 전송되는 부분 영상들의 관계를 나타내 는 참고 도면이다. 엣지 데이터 네트워크는 가상 현실(VR)을 위한 VR 영상 전체를 전자 장치로 전송하는 것이 아니라, 전자 장치로부터 획득된 방위 정보에 대응되는 VR 영상 내 부분 영상만을 전송할 수 있다. 상술한 도 3 내지 도 5에 서는, 엣지 데이터 네트워크가 제2 예측 프레임을 전자 장치로 전송하는 경우를 가정하여 설명하였으나, 이하 도 6 내지 8에서는 엣지 데이터 네트워크는 제2 예측 프레임 및 제3 예측 프레임을 함께 전자 장치로 전송하는 경우를 가정하여 설명하기로 한다. 예를 들어, 엣지 데이터 네트워크는 제1 VR 영상, 제 2 VR 영상 및 제 3 VR 영상에 대한 프레임을 전자 장치로 전송하는 것이 아니고, 제1 부분 영상, 제2 예측 부분 영상, 제3 예측 부분 영 상 및 제3 재 예측 부분 영상에 대한 프레임을 전자 장치로 전송함으로써 효과적으로 영상 컨텐츠를 전자 장치로 전송할 수 있다. 일 실시 예에 의하면, 도 6에 도시된 제1 부분 영상, 제2 예측 부분 영상, 제3 예측 부분 영상 및 제3 재-예측 부분 영상은 복수의 프레임을 포함하는 VR 시퀀스의 각 프레임 별(602, 604, 606) 부분 영상일 수 있다. 예를 들어, 제1 부분 영상은, 제1 부분 영상을 포함하는 제1 VR 영상과 동일한 프레임 인덱스를 가지고, 제2 예측 부분 영상 및 제2 부분 영상은 제2 VR 영상과 동일한 프레임 인덱스를 가질 수 있다. 또한, 제3 예측 부분 영상 및 제3 재 예측 부분 영상은 제3 VR 영상과 동일한 프레임 인덱스를 가지는 부분 영상일 수 있다. 또한, 일 실시 예에 의하면, VR 시퀀스는 VR 영상의 전부 또는 일부로써, VR 영상에 포함된 복수개의 프레 임들 중 전부 또는 일부의 연속되는 프레임들의 집합일 수 있다. 예를 들어, VR 시퀀스는 1번 프레임 인덱스를 가지는 제1 VR 영상, 2번 프레임 인덱스를 가지는 제2 VR 영상 및 3번 프레임 인덱스를 가지는 제3 VR 영상을 포함할 수 있다. 또한, 제1 부분 영상, 제2 예측 부분 영상, 제3 예측 부분 영상 및 제3 재 예측 부분 영상은, 전자 장치 사용자가 VR 영상 어떤 부분 영상을 바라보는지 여부를 결 정하기 위한 방위 정보를 포함할 수 있다. 예를 들어, 제1 부분 영상은 제1 방위 정보를 포함하고, 제1 방위 정보에 기초하여 제1 VR 영상 내에서 위치가 결정될 수 있다. 또한, 제2 예측 부분 영상은 제2 예측 방위 정보를 포함하고, 제2 예측 방위 정보에 기초하여 제2 VR 영상 내에서 위치가 결정될 수 있으며, 제3 예측 부분 영상은 제3 예측 방위 정보를 포함하고, 제3 예측 방위 정보에 기초하여 제3 VR 영상 내에서 위치가 결정될 수 있고, 제3 재 예측 부분 영상은 제3 재 예측 방위 정보에 기초하여 제3 VR 영상 내에서 위치가 결정될 수 있다. 방위 정보는, 각 부분 영상들이 VR 영상에 대한 위치를 나타낼 수 있고, 각 부분 영상들은 방위 정보에 기초하 여 VR 영상 내에서 위치가 결정될 수 있다. 일 실시 예에 의하면, VR 영상 내에서 부분 영상들의 위치를 나타내 는 방위 정보는, 전자 장치에 부착된 센싱 모듈로부터 획득되는 3축 각도 값을 포함할 수 있고, 3축 각도는 전 자 장치를 착용한 사용자의 입력에 기초하여 변경될 수 있다. 본 명세서에서 기술되는 예측 프레임, I프레임 또는 P프레임은, VR 영상 내 부분 영상들을 인코딩함으로써 생성된 데이터를 포함하는 프레임이고, VR 영상 내 부분 영상은 프레임들을 디코딩하고, 디코딩된 프레임 들을 전자 장치의 디스플레이에 표시할 경우 나타나는 영상들을 의미한다. 일 실시 예에 따라, 이하 도 6 내지 도 8에서 후술하는, 제1 부분 영상은 VR 시퀀스 내, 1번 프레임 인덱스를 가지는 제1 VR 영상의 부분 영상이고, 제2 예측 부분 영상 및 제2 부분 영상은, VR 시 퀀스 내, 2번 프레임 인덱스를 가지는 제2 VR 영상의 부분 영상이며, 제3 예측 부분 영상 및 제 3 재 예측 부분 영상은 3번 프레임 인덱스를 가지는 제3 VR 영상의 부분 영상임을 가정하여 설명하기로 한 다. 도 7은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. S702에서, 전자 장치는 제1 방위 정보를 획득한다. 일 실시 예에 의하면, 전자 장치는 전자 장치 내 센싱 모듈을 이용하여, 제1 VR 영상 내 제1 부분 영상의 위치를 나타내는 3축 각도 값을 제1 방위 정보로 획 득할 수 있다. 또 다른 실시 예에 의하면, 제1 방위 정보는 제1 부분 영상이 전자 장치에서 재생되기 전까지 재 생된 부분 영상들에 대한 복수의 방위 정보들을 포함할 수도 있다. S704에서, 전자 장치는 제1 방위 정보 를 엣지 데이터 네트워크로 전송한다. S706에서, 엣지 데이터 네트워크는 제1 방위 정보를 기초로, 제2 예측 방위 정보 및 제3 예측 방위 정보 를 결정(1차 예측)할 수 있다. 도 6 내지 도7에서는 엣지 데이터 네트워크가 2개의 예측 방위 정보(제2 예측 방위 정보 및 제3 예측 방위 정보)를 결정하는 것으로 도시되었으나, 이에 한정되는 것은 아니고, 엣지 데 이터 네트워크는 적어도 하나의 GOP 내 2이상의 부분 영상들에 대한 예측 방위 정보들을 결정할 수도 있 다. 또한, 도 6 내지 7에서는, 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상은 2번 프레임 인덱스를 가지는 제2 VR 영상 내 부분 영상, 제3 예측 부분 영상은 3번 프레임 인덱스를 가지는 제3 VR 영상 내 부분 영상인 것으로 도시되었으나, 제2 예측 부분 영상 및 제3 부분 영상은 소정의 프레임 인덱스 간격을 가지는 VR 영상 내 부분 영상일 수도 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 제1 방위 정보에 기초하여, 제2 예측 방위 정보 및 제3 예 측 방위 정보가 제1 방위 정보와 동일해지도록, 제2 예측 방위 정보 및 제3 예측 방위 정보를 결정할 수 있다. 그러나, 이에 한정되는 것은 아니며, 후술하는 도 10 내지 도 13에 기재된 엣지 데이터 네트워크가 예측 방위 정보를 결정하는 방법에 따라 제2 예측 방위 정보 및 제3 예측 방위 정보를 결정할 수도 있다. S708에서, 엣지 데이터 네트워크는 제2 예측 방위 정보에 대응하는 제2 예측 부분 영상 및 제3 예측 방위 정보에 대응하는 제3 예측 부분 영상을 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 결정한 제2 예측 방위 정보를 이용하여, 데이터 베이스(DB)에 저장된 VR 시퀀스로부터, 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 식별할 수 있고, 동일한 방법으로 제3 예측 방위 정보를 이용하여 데이터 베이스의 VR 시퀀스 중, 제3 예측 부분 영상을 식별할 수 있다. S710에서, 엣지 데이터 네트워크는 제2 예측 부분 영상 및 제3 예측 부분 영상을 인코딩함으로써 제2 예 측 프레임 및 제3 예측 프레임을 생성할 수 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크가 생성된 제2 예측 프레임 및 제3 예측 프레임은 I프레임 또는 P프레임일 수 있다. S712에서, 엣지 데이터 네트워크 는 생성된 제2 예측 프레임 및 제3 예측 프레임을 전자 장치로 전송한다. 일 실시 예에 의하면, 엣 지 데이터 네트워크는 전자 장치로 전송한 제2 예측 프레임 및 제3 예측 프레임을 엣지 데이터 네 트워크내 데이터 베이스에 별도로 저장할 수 있다. 또 다른 실시 예에 의하면, 엣지 데이터 네트워크 는 전자 장치로 전송한 제2 예측 프레임 및 제3 예측 프레임에 대응되는 제2 예측 방위 정보 및 제 3 예측 방위 정보만을 데이터 베이스에 별도로 저장할 수도 있다. S714에서, 전자 장치는 제2 예측 부분 영상을 재생하고, 제2 부분 영상에 대응되는 제2 방위 정보를 획 득할 수 있다. 보다 상세하게는, 전자 장치는 엣지 데이터 네트워크로부터 수신된 제2 예측 프레임 및 제3 예측 프레임 중, 제2 예측 프레임을 디코딩하고, 디코딩된 제2 예측 프레임을 이용하여 제2 예측 부분 영상을 재생할 수 있다. 전자 장치가 제2 예측 부분 영상을 재생하는 동안, 제3 예측 프레임은 전자 장치 의 버퍼 내 캐싱될 수 있다. 일 실시 예에 의하면, 전자 장치는 제2 예측 부분 영상이 재생 될 것으로 예측 되는 시점, 제2 예측 부분 영상을 재생 하려는 시점부터 기 설정된 시간이 경과한 시점, 또는 제2 예측 부분 영상이 재생되는 동안, 전자 장치 내 센싱 모듈로부터 제2 방위 정보를 획득할 수 있다. 즉, 전자 장치는 엣지 데이터 네트워크가 예 측한 제2 예측 부분 영상을 재생함과 함께, 현재 전자 장치의 사용자가 실제 바라보고자 하는 지점의 부 분 영상의 위치를 제2 방위 정보로써 획득할 수 있다. 일 실시 예에 의하면, 전자 장치가 획득한 제2 방위 정보가 엣지 데이터 네트워크가 예측한 제2 예 측 방위 정보와 일치하지 않을 경우, 전자 장치의 표시 장치의 뷰포트(Viewport) 영역 일부에는 영상 컨 텐츠가 표시되지 않을 수 있다. 따라서, 본 개시의 일 실시 예에 의하면, 엣지 데이터 네트워크의 영상 예측이 실패할 경우, 전자 장치의 뷰포트 일부 영역에 영상 컨텐츠가 표시되지 않는 문제를 해결하기 위 해, 엣지 데이터 네트워크는 기 설정된 전자 장치의 뷰포트 영역의 크기 보다 더 큰 크기의 예측 부분 영 상들을 인코딩함으로써 생성된 예측 프레임들을 전자 장치로 전송할 수도 있다. S716에서, 전자 장치 는 제2 방위 정보를 엣지 데이터 네트워크로 전송한다. S718에서, 엣지 데이터 네트워크는 제2 방위 정보를 기초로 제3 재 예측 방위 정보를 결정(2차 예측)할 수 있다. S720에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하는지 여부를 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정 보 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 큰 경우, 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하지 않는 것으로 결정할 수 있다. 또한, 또 다른 실시 예에 의하면, 도 4의 S418단계와 유사하게, 제3 예측 방위 정보 및 제3 재 예측 방위 정보 를 비교하는 것이 아니라, 제3 예측 방위 정보에 대응되는 제3 예측 부분 영상 및 제3 재 예측 방위 정보에 대 응되는 제3 재 예측 부분 영상 자체를 비교할 수도 있다. S722에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하지 않는다는 결과에 대한 정보를 전자 장치로 전송한다. S724에서, 전자 장치는 제3 예측 방위 정보 및 제3 재예측 방위 정보가 일치하지 않는다는 비교 결과에 기초하여, 제3 예측 부분 영상의 재생을 대기할 수 있다. S726에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하지 않는 다는 결정에 응답하여, 제2 예측 부분 영상, 제3 예측 부분 영상 및 제3 재 예측 부분 영상에 기초하여, 제3 재 예측 부분 영상에 대응되는 보상 프레임을 생성할 수 있다. 보다 상세하게는, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하지 않 는 것으로 결정되는 경우, 전자 장치로부터 획득된 제2 방위 정보에 기초하여 새로 결정한 제3 재 예측 방위 정보를 이용하여, 데이터 베이스(DB)내 VR 시퀀스로부터 제3 재 예측 부분 영상을 식별할 수 있다. 일 실시 예에 의하면, 전자 장치에서는 재2 예측 부분 영상이 재생 중이고, 엣지 데이터 네트워크 의 데이터 베이스에는 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임이 이미 저장된 상태일 수 있다. 엣지 데이터 네트워크는 S712에서 전자 장치로 전송한 제2 예측 프레임 및 제3 예측 프레임 내 적 어도 일부 블록을 참조하도록 제3 재 예측 부분 영상에 대응되는 보상 프레임을 생성할 수 있다. 구체적으로, 엣지 데이터 네트워크는 제2 예측 부분 영상에 대응되는 제2 예측 프레임 내 영상 정보 및 제3 재 예측 부분 영상에 대응되는 제3 재 예측 프레임 내 영상 정보의 차이에 대한 정보를 제3 차이 정보로 결 정하고, 제3 재 예측 부분 영상에 대응되는 제3 재 예측 프레임 내 영상 정보 및 제3 예측 부분 영상에 대응되 는 제3 예측 프레임 내 영상 정보의 차이를 제4 차이 정보로 결정하며, 제3 차이 정보 및 제4 차이 정보를 이용 하여 제3 재 예측 부분 영상에 대한 보상 프레임을 생성할 수 있다. 일 실시 예에 의하면, 제3 차이 정보는, 제3 차이 정보를 결정하는데 사용된 제2 예측 프레임 내 블록 주소(또 는 위치) 및 제3 재 예측 프레임 내 블록 주소(또는 위치)를 나타내는 제3 참조 블록 정보와, 제3 차이 정보를 결정하는데 사용된 제2 예측 프레임 내 블록 및 상기 제3 재 예측 프레임 내 블록 간의 화소 차이에 대한 정보 를 포함할 수 있다. 또한, 일 실시 예에 의하면, 제4 차이 정보는, 제4 차이 정보를 결정하는데 사용된 제3 재 예측 프레임 내 블록 주소(또는 위치) 및 제3 예측 프레임 내 블록 주소(또는 위치)를 나타내는 제4 참조 블록 정보와, 제4 차이 정 보를 결정하는데 사용된 제3 재 예측 프레임 내 블록 주소(또는 위치) 및 제3 예측 프레임 내 블록 간 화소 차 이에 대한 정보를 더 포함할 수 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 제2 예측 부분 영상, 제3 재 예측 부분 영상 및 제3 예측 부분 영상을 순서대로 인코더에 입력함으로써, 제2 예측 부분 영상에 대응되는 P프레임 및 제3 예측 부분 영상 에 대응되는 P프레임을 순서대로 인코더로부터 획득할 수 있으나, 인코더의 인코딩 순서는 이와 다를 수 있다. 즉, 엣지 데이터 네트워크내 인코더는 제2 예측 부분 영상을 제2 예측 프레임으로 인코딩하고, 제3 예측 부분 영상을 제3 예측 프레임으로 인코딩한 후, 인코딩된 제2 예측 프레임 내 적어도 일부 블록 및 인코딩된 제 3 예측 프레임 내 적어도 일부 블록을 참조하도록 제3 재 예측 부분 영상을 인코딩함으로써 보상 프레임을 생성 할 수 있다. S728에서, 엣지 데이터 네트워크는 생성된 보상 프레임을 전자 장치로 전송한다. S730에서, 전자 장치는 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 예를 들어, 전자 장치는 보상 프레임 내 제3 참조 블록 및 제4 참조 블록 정보를 식별하고, 식별된 제3 참조 블록에 대응되는 제2 예측 프레임 내 일부 블록 및 제4 참조 블록 정보에 대응되는 제3 예측 프레임 내 일부 블록을 참 조함으로써, 보상 프레임을 디코딩할 수 있다. 전자 장치는 보상 프레임을 디코딩하고, 디코딩된 보상 프 레임을 전자 장치의 표시 장치 전달함으로써 제3 재 예측 부분 영상을 재생할 수 있다. 또 다른 실시 예에 의하면, 전자 장치가 제2 예측 부분 영상을 재생 중이고, 재3 재 예측 부분 영상이 재 생될 것으로 예측되는 시점에 도달하기 전인 경우, 전자 장치는 디코딩된 보상 프레임을 전자 장치 내 버 퍼에 캐싱해둘 수도 있다. 엣지 데이터 네트워크는 제2 예측 방위 정보를 예측하고, 예측된 제2 예측 방위 정보를 전자 장치 로 전송하였으나, 제2 예측 부분 영상이 전자 장치에서 재생되는 동안, 전자 장치가 획득한 제2 방위 정 보가 제2 예측 방위 정보와 일치하지 않을 경우, 제3 예측 방위 정보에 대응되는 제3 예측 부분 영상이 전자 장 치에서 재생될 동안, 전자 장치가 획득할 제3 방위 정보 역시 제3 예측 방위 정보와 일치하지 않을 가능 성이 높다. 따라서, 도 7에서 상술한 방법에 따라, 엣지 데이터 네트워크는 제1 방위 정보에 기초하여 제3 예측 방위 정보를 결정 한 후, 새로 획득된 제2 방위 정보에 기초하여, 제3 재 예측 방위 정보를 결정함으로써, 전자 장치의 사용자가 시청하고자 하는 영상을 더 정확하게 예측할 수 있다. 도 8은 또 다른 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. 도 7에서, 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하지 않는 경우, 전자 장치와 엣지 데이터 네트 워크 간의 동작 절차를 설명하였으므로, 도 8을 참조하여 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치 하는 경우, 전자 장치와 엣지 데이터 네트워크 간의 동작 절차를 설명하기로 한다. 도 7의 S702 단계 내지 S718 단계는 도 8의 S802단계 내지 S818 단계에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S820에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하는지 여부를 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보 내 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 작은 경우, 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정할 수 있다. S822에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보의 일치 한다는 비교 결 과에 대한 정보를 전자 장치로 전송한다. S824에서, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 예측 방위 정보가 일치하는 경우, 데이터 베이스(DB) 내 기 저장되어 있는 제3 예측 부분 영상을 인코딩 함으로써 보상 프레임을 생성할 수 있다. 보다 상세하게는, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하는 것으로 결정되는 경우, 제3 예측 부분 영상을 인코딩함으로써 생성된 제3 예측 프레임의 적어도 일부 블록을 참조 하는 보상 프레임을 생성할 수 있다. S826에서, 전자 장치는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치한다는 비교 결과에 기초하 여, S812 단계에서 수신된 제3 예측 프레임의 영상 정보와 동일한 영상 정보를 가지는 보상 프레임을 생성할 수 있다. 본 개시에 따른 전자 장치는 전자 장치의 버퍼에 캐싱된 I프레임 또는 P프레임의 디코딩 순서에 영 향을 미치지 않고도 효율적으로 사용자에게 컨텐츠를 제공하기 위해, 보상 프레임만을 이용하여 영상을 재생할 수 있다. 따라서, 전자 장치는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치한다는 비교 결과가 수신되는 경우에도, 제3 예측 프레임의 영상 정보와 동일한 영상 정보를 가지는 보상 프레임을 생성한다. S826 단계는 도 5의 S524단계에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S828단계에서, 엣지 데이터 네트워크는 생성된 보상 프레임을 폐기할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 재3 재 예측 방위 정보가 일치하는 것으로 결정되었으므로, 제3 예측 부분 영상 및 재3 재 예측 부분 영상이 동일한 것으로 결정하고, 현재 제3 예측 부분 영상을 인코딩함으로써 생 성된 제3 예측 프레임이 전자 장치로 전송되어 있으므로, 별도의 보상 프레임을 전자 장치로 전송할 필요 가 없다. 따라서, 엣지 데이터 네트워크는 생성한 보상 프레임을 전자 장치로 전송하지 않고, 폐기 한다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 제3 예측 방위 정보 및 제3 재 예측 방위 정보가 일치하는 경우, 별도의 보상 프레임을 생성하지 않을 수도 있다. 그러나, 본 개시에 따른 엣지 데이터 네트워크는 전자 장치에서 수행되는 디코딩 동작에 동기화되기 위하여, 제3 예측 방위 정보 및 제3 재 예측 방위 정 보가 일치하는 경우에도, 전자 장치가 스스로 보상 프레임을 생성하는 동작과 별도로, 보상 프레임을 생 성한다. S830에서, 전자 장치는 S826단계에서 생성된 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 예를 들어, 전자 장치는 보상 프레임 내 참조 블록 정보를 획득하고, 획득된 참조 블록 정보에 기초하여 보상 프레임이 참조하는 프레임 내 일부 블록을 참조함으로써 보상 프레임을 생성할 수 있다. 현재 제 3 예측 프레임은 보상 프레임과 동일한 영상 정보를 포함하므로, 전자 장치는 제3 예측 프레임만을 참조 하여 보상 프레임을 디코딩할 수 있다. 도 9는 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 영상 컨텐츠를 스트리밍 하는 방법을 나타내는 흐 름도이다. S910에서, 엣지 데이터 네트워크는 전자 장치로부터 제1 방위 정보를 획득한다. S910은 도 4의 S402 단계 에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S920에서, 엣지 데이터 네트워크는 제1 방위 정보에 기초하여 제2 예측 방위 정보를 결정한다. S920은 도 4의 S406에 대응될 수 있다. S920은 후술하는 도 10 내지 13을 참조하여 구체적으로 설명하기로 한다. S930에서, 엣지 데이터 네트워크는 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정한다. S930 은 도 4의 S408에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S940에서, 엣지 데이터 네트워크는 제2 예측 부분 영상을 인코딩함으로써 생성된 제2 예측 프레임을 전자 장치로 전송한다. S940은 도4의 S410 내지 S420에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S950에서, 엣지 데이터 네트워크는 전자 장치로부터 제2 방위 정보를 획득한다. S950은 도 4의 414 단계 에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S960에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보를 비교한다. 도 9에는 도시되지 않았으나, S960단계 이후, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보의 비교 결과에 대한 정보를 전자 장치로 전송하는 단계를 더 수행할 수 있다. S960은 도 4의 도 4의 S418에 대응될 수 있으므 로 구체적인 설명은 생략하기로 한다. S970에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보의 비교 결과에 기초하여 보상 프레임을 생성한다. 예를 들어 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 것으로 결정되는 경우, 제1 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상에 기초하여, 제2 부분 영상 에 대응되는 보상 프레임을 생성할 수 있다. 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 경우, 엣지 데이 터 네트워크가 보상 프레임을 생성하는 구체적인 방법은 도 4의 S424에 대응될 수 있으므로 구체적인 설 명은 생략하기로 한다. 또한, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되는 경우, 제2 예측 부분 영상 및 제2 부분 영상을 동일한 영상으로 결정하고, 제2 예측 프레임의 적어도 일부 블록을 참 조하는 보상 프레임을 생성할 수 있다. 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되는 경우, 엣지 데이터 네트워크가 보상 프레임을 생성하는 방법은 S522단계에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S980에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보의 비교 결과에 기초하여 생성된 보상 프레임을 전자 장치로 전송한다. 예를 들어, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 경우, 생성된 보상 프레임을 폐기할 수 있다. 그러나, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 경우, 생성된 보상 프레임을 전자 장치로 전송할 수 있다. S980은 도 4의 S426 또는 도 5의 S526 단계에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. 도 10은 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제2 예 측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. 도 10을 참조하여, 엣지 데이터 네트워크가 제1 방위 정보에 기초하여, 제2 예측 방위 정보를 결정(S92 0)하고, 결정된 제2 예측 방위 정보에 대응되는 제2 부분 영상을 결정하는 단계((S930))의 구체적인 일 예를 설 명하기로 한다. S1010에서, 엣지 데이터 네트워크는 전자 장치로부터 획득된 제1 방위 정보와 제2 예측 방위 정보 가 동일해지도록 제2 예측 방위 정보를 결정할 수 있다. 예를 들어, 본 개시에 따른 제1 방위 정보 및 제2 예측 방위 정보는 3축 각도 성분으로써, Pitch 각도 성분, Roll 각도 성분 및 Yaw 각도 성분을 포함할 수 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 전자 장치로부터 획득된 제1 방위 정보가 Pitch 각도 성분, Roll 각도 성분 및 Yaw 각도 성분 순으로, 30도, 45도, 60도를 포함하는 경우, 제2 예측 방위 정보를 30 도, 45도 및 60도로 결정할 수 있다. S1010에서, 엣지 데이터 네트워크는 제2 예측 방위 정보를 이용하여 제2 예측 부분 영상을 결정한다. 예 를 들어, 엣지 데이터 네트워크는 전체 프레임에 대한 VR 시퀀스를 데이터 베이스에 저장할 수 있고, 제2 예측 방위 정보를 이용하여, 데이터 베이스(DB) 내 VR 시퀀스로부터 제2 예측 부분 영상을 식별할 수 있다. 일 실시 예에 의하면, 엣지 데이터 네트워크는 전자 장치로부터 제1 방위 정보 외에, 전자 장치에 서 재생 될 것으로 예측되는 VR 시퀀스 내 특정 프레임 번호를 나타내는 프레임 인덱스에 대한 정보를 더 수신 할 수도 있다. 예를 들어, 엣지 데이터 네트워크는 전자 장치로부터 획득된 프레임 인덱스에 대한 정보가 VR 시퀀스 내 2 번 프레임 인덱스를 나타내는 경우, 데이터 베이스(DB) 내 미리 저장된 VR 시퀀스의 2번 프레임 인덱스에 해당하는 제2 VR 영상을 식별하고, 식별된 제2 VR 영상 내에서, 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 식별할 수도 있다. 도 11은 본 개시의 또 다른 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제 2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. 도 11을 참조하여, 또 다른 실시 예에 따라, 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결 정된 제2 예측 방위 정보에 따라 제2 예측 부분 영상을 결정하는 방법을 설명하기로 한다. S1110에서, 엣지 데이터 네트워크는 제1 부분 영상 이전에 전자 장치에서 재생된 부분 영상들 방위 정보 및 제1 방위 정보 사이의 제1 변화량을 결정할 수 있다. 예를 들어, 본 개시에 따른 방위 정보는 3축 각도 성분을 포함할 수 있고, 제1 부분 영상 이전에 전자 장치에서 재생된 부분 영상의 방위 정보가 (x0, y0, z0)이 고, 제1 방위 정보가 (x1, y1, z1)인 경우, 제1 변화량은, (x1-x0, y1-y0, z1-z0)일 수 있다. S1120에서, 엣지 데이터 네트워크는 제2 예측 방위 정보 및 제1 방위 정보 사이의 제2 변화량이 제1 변화 량과 동일해지도록 제2 예측 방위 정보를 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제2 예측 방위 정보가 (X,Y,Z)인 경우, 제2 변화량은 (X-x1, Y-y1, Z-z1)일 수 있다. 엣지 데이터 네트워크는 제2 변화량 (X-x1, Y-y1, Z-z1)이 제1 변화량 (x1-x0, y1-y0, z1-z0)과 동일해지도록, 제2 예측 방위 정보를 결정 할 수 있다. S1130에서, 엣지 데이터 네트워크는 제2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정한다. 엣 지 데이터 네트워크가 제2 예측 방위 정보를 이용하여 데이터 베이스(DB)로부터 제2 예측 부분 영상을 식 별하는 방법은 도 4의 S408에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. 도 12는 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제2 예 측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 나타내기 위한 도면이다. 엣지 데이터 네트워크는 도 10에서 상술한 바와 같이, 제2 예측 방위 정보가 제1 방위 정보와 동일해지도 록, 제2 예측 방위 정보를 결정할 수 있다. 또한, 엣지 데이터 네트워크는 제2 예측 방위 정보를 결정하 는 방법과 유사하게, 제3 예측 방위 정보가 제2 예측 방위 정보와 동일해지도록 제3 예측 방위 정보를 결정할 수도 있다. 즉, 도 12의 고정 예측을 참조하면, 엣지 데이터 네트워크는 프레임 인덱스가 N인 VR 영상, 프레임 인덱스가 N+1인 VR 영상 및 프레임 인덱스가 N+1인 VR 영상에서 모두 동일한 방위 정보를 가지도록, 프레임 인덱스 별 부분 영상들을 예측할 수도 있다. 그러나 또 다른 실시 예에 의하면, 도 11에서 상술한 바와 같이, 엣지 데이터 네트워크는 인접한 2개 프레임 내 부분 영상들의 방위 정보의 변화량이 동일해지도록 제2 예측 방위 정보를 결정할 수도 있다. 예를 들어, 도 12 의 가속도 기반 선형 예측을 참조하면, 엣지 데이터 네트워크는 프레임 인덱스가 N-1인 VR 영상 내 부분 영상의 방위 정보 및 프레임 인덱스가 N-2인 VR 영상내 부분 영상의 방위 정보 사이의 변 화량이, 프레임 인덱스가 N인 VR 영상 내 부분 영상의 방위 정보 및 프레임 인덱스가 N-1인 VR 영상 내 부분 영상의 방위 정보 사이의 변화량과 동일해지도록, 프레임 인덱스 N내 부분 영상의 방위 정 보를 예측할 수 있다. 마찬가지로, 엣지 데이터 네트워크는 프레임 인덱스가 N+1인 VR 영상내 부분 영상의 방위 정보 및 프레임 인덱스가 N인 VR 영상내 부분 영상의 방위 정보 사이의 변화량이, 프레임 인덱스가 N+2인 VR 영상 내 부분 영상의 방위 정보 및 프레임 인덱스가 N+1인 VR 영상내 부분 영상의 방위 정보 사이의 변 화량과 동일해지도록, 프레임 인덱스 N+1인 VR 영상 및 프레임 인덱스 N+2인 VR 영상내에서 부분 영상의 방위 정보들을 예측할 수 있다. 도 13은 본 개시의 또 다른 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제 2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. S1310에서, 엣지 데이터 네트워크는, 제1 부분 영상 이전에 전자 장치에서 재생된 부분 영상들의 방위 정 보, 상기 재생된 부분 영상들에 대한 버퍼 관리 정보에 기초하여 재생 패턴을 결정할 수 있다. 예를 들어, 엣지 데이터 네트워크는 제1 부분 영상 이전에 재생된 인접한 2개 프레임 인덱스를 가지는 부분 영상들의 방위 정보의 변화량 및 상기 2개 프레임 인덱스를 가지는 부분 영상들의 버퍼 관리 정보에 기초하여 재생 패턴을 결 정할 수 있다.S1320에서, 엣지 데이터 네트워크는 재생 패턴을 기초로, 영상 재생 모델을 생성할 수 있다. 예를 들어, 영상 재생 모델은, 복수의 신경망 레이어들로 구성될 수 있고, 복수의 신경망 레이어들 각각은 복수의 가중치들 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 복수의 가중치들 간의 연산을 통해 신경 망 연산을 수행한다. 복수의 신경망 레이어들이 갖고 있는 복수의 가중치들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화 되도록 복수의 가중치들이 갱신될 수 있다. 인공 신경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다. S1330에서, 전자 장치는 영상 재생 모델에 제1 방위 정보를 입력함으로써, 영상 재생 모델로부터 제2 예 측 방위 정보를 획득할 수 있다. 예를 들어, 엣지 데이터 네트워크는 S1310단계에서 결정된 재생 패턴을 기초로 영상 재생 모델을 학습시킬 수 있고, 영상 재생 모델은, 이전 프레임 인덱스를 가지는 VR 영상 내 방위 정보가 입력되면, 다음 프레임 인덱스 또는 소정의 간격만큼 떨어진 프레임 인덱스를 가지는 VR 영상 내 방위 정보를 출력할 수 있다. 도 14는 본 개시의 일 실시 예에 따른 전자 장치가 엣지 데이터 네트워크로부터 획득된 영상 컨텐츠를 스트리밍 하는 방법을 나타내는 흐름도이다. S1410에서, 전자 장치는 엣지 데이터 네트워크로 제1 방위 정보를 전송할 수 있다. 예를 들어, 제1 방위 정보는 전자 장치에서 재생되는 제1 부분 영상의 제1 VR 영상 내 위치를 나타내는 방위 정보를 포함할 수 있다. 그러나 또 다른 실시 예에 의하면, 제1 방위 정보는 전자 장치에서 제1 부분 영상이 재생 되기 전 까지 재생된 부분 영상들에 대한 복수의 방위 정보들을 포함할 수도 있다. S1420에서, 전자 장치는 엣지 데이터 네트워크로부터 제1 방위 정보에 기초하여 결정된 제2 예측 방위 정보에 대응되는 제2 예측 프레임을 획득할 수 있다. 예를 들어, 전자 장치가 획득하는 제2 예측 프 레임은, 제2 VR 영상 내 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 인코딩함으로써 생성된 I프레임 또는 P프레임일 수 있다. 도 14에는 도시되지 않았지만, 전자 장치는 엣지 데이터 네트워크로부터 획득한 제2 예측 프레임을 전자 장치의 버퍼에 캐싱할 수 있다. S1430에서, 전자 장치는 제2 예측 프레임에 대한 제2 방위 정보를 획득할 수 있다. 예를 들어, 전자 장치 는 전자 장치가 제2 예측 부분 영상을 재생하려는 시점, 제1 부분 영상을 재생 한 시점으로부터 기 설정된 시간이 경과한 시점 또는 제2 예측 부분 영상이 재생될 것으로 예측한 시점으로부터 기 설정된 시간만큼 앞선 시점에서, 전자 장치내 센싱 모듈로부터, 제2 방위 정보를 획득할 수 있다. 그러나, 또 다른 실시 예에 의하면, 전자 장치는 제2 예측 프레임을 디코딩하고, 디코딩된 제2 예측 프레임을 이용하여 제2 예 측 부분 영상을 재생하고, 제2 예측 부분 영상이 재생되는 동안 제2 예측 프레임에 대한 제2 방위 정보를 획득 할 수도 있다. S1440에서, 전자 장치는 제2 방위 정보를 엣지 데이터 네트워크로 전송한다. S1450에서, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보의 일치 여부를 나타내는 비교 결과에 대한 정보를 엣 지 데이터 네트워크로부터 획득할 수 있다. S1460에서, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보의 일치 여부에 기초하여, 엣지 데이터 네 트워크로부터 보상 프레임을 수신하거나, 제2 예측 프레임 내 적어도 일부 블록을 참조하는 보상 프레임을 생성 할 수 있다. 예를 들어, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보가 일치하지 않는다는 비교 결 과에 대한 정보에 기초하여, 제2 예측 부분 영상의 재생을 대기하고, 제2 예측 부분 영상의 재생을 대기하는 동 안 엣지 데이터 네트워크로부터 수신된 제2 부분 영상에 대응하는 보상 프레임을 획득할 수도 있다. 그러 나 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보가 일치한다는 비교 결과에 대한 정보에 기초하여, 제2 예측 프레임의 영상 정보와 동일한 영상 정보를 가지는 보상 프레임을 생성할 수도 있다. S1470에서, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보의 일치 여부에 기초하여, 엣지 데이터 네 트워크로부터 수신된 보상 프레임을 디코딩하거나, 스스로 생성한 보상 프레임을 디코딩할 수 있다. 예를 들어, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보가 일치하지 않는다는 비교 결과에 대한 정 보에 기초하여, 엣지 데이터 네트워크로부터 수신된 보상 프레임을 디코딩할 수 있다. 전자 장치는보상 프레임 내 참조 블록 정보에 기초하여, 제1 프레임 내 일부 블록 및 제2 예측 프레임 내 일부 블록을 참조 함으로써 보상 프레임을 디코딩할 수 있다. 또 다른 실시 예에 의하면, 전자 장치는 제2 방위 정보 및 제2 예측 방위 정보가 일치한다는 비교 결과에 대한 정보에 기초하여, 스스로 생성한 보상 프레임을 디코딩한다. 현재, 전자 장치가 생성한 보상 프레임 은, 제2 예측 프레임과 동일한 영상 정보를 포함하므로, 전자 장치는 제2 예측 프레임 내 적어도 일부 블 록만을 참조함으로써 보상 프레임을 디코딩할 수 있다. S1480에서, 전자 장치는 디코딩된 보상 프레임을 재생할 수 있다. 도 15은 본 개시의 일 실시 예에 따른 전자 장치가 전자 장치의 버퍼에 저장된 보상 프레임들을 재생하는 과정 을 설명하기 위한 도면이다. 전자 장치는 적어도 하나의 버퍼를 포함할 수 있다. 예를 들어, 전자 장치는 엣지 데이터 네 트워크가 예측한 부분 영상을 인코딩함으로써 생성된 I프레임 또는 P프레임들를 버퍼에 저장할 수 있다. 본 개시에 따른 전자 장치는 엣지 데이터 네트워크로부터 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는다는 비교 결과에 대한 정보를 수신하는 경우, 제2 예측 부분 영상의 재생을 대기한 상태에서, 엣 지 데이터 네트워크로부터 보상 프레임을 획득할 수 있다. 그러나, 전자 장치가 제2 예측 방위 정보 및 제2 방위 정보가 일치한다는 비교 결과에 대한 정보를 수신 하는 경우, 전자 장치는 엣지 데이터 네트워크로부터 미리 획득된 제2 예측 프레임과 동일한 영상 정보를 가지는 보상 프레임을 생성할 수 있다. 전자 장치는 생성된 보상 프레임(1512, 1514)를, 버퍼 관리 정보에 기초하여, 엣지 데이터 네트워크 가 예측한 부분 영상들에 대한 예측 프레임들이 저장된 버퍼에 캐싱할 수 있다. 전자 장치는 디코딩하고자 하는 보상 프레임에 인접한 적어도 하나의 I프레임 또는 P프레임 내 적어도 일 부 블록을 참조함으로써, 버퍼에 캐싱된 보상 프레임들(1512, 1514)을 디코딩하고, 디코딩된 보상 프레임만을 이용하여 영상 컨텐츠를 재생할 수 있다. 전자 장치가 보상 프레임을 디코딩함에 있어, 인접한 프레임을 참조하는 구체적인 방법은 도 4의 S428 내지 도 5의 S528에 대응될 수 있으므로 구체적인 설명은 생략하기로 한 다. 본 개시에 따른 전자 장치는 버퍼에 캐싱된 I프레임 또는 P프레임을 참조함으로써, I프레임, P프레임 및 보상 프레임을 디코딩할 수 있다. 다만, 본 개시에 따른 전자 장치는 엣지 데이터 네트워크로부터 수신된 인코딩된 프레임들을 디코딩함에 있어 I프레임 또는 P프레임만을 사용할 뿐, 영상 컨텐츠를 재생함에 있 어서는, 디코딩된 프레임들 중 보상 프레임만을 이용하여 사용자에게 영상 컨텐츠를 제공할 수 있다. 본 개시에 따른 전자 장치는 디코딩된 보상 프레임만을 전자 장치내의 표시 장치에 전달함으로써 영상 컨텐츠를 제 공하기 때문에, 전자 장치의 사용자가 원하는 실시간 부분 영상 컨텐츠에 대한 정보만을 엣지 데이터 네 트워크로부터 효과적으로 획득할 수 있다. 도 16은 본 개시의 일 실시 예에 따른 클라우드 서버, 전자 장치 및 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 나타내는 흐름도이다. 일 실시 예에 의하면, 도 4 내지 도 5의 전자 장치 및 엣지 데이터 네트워크 간 동작 절차는 클라 우드 서버, 전자 장치 및 엣지 데이터 네트워크 사이에서 수행될 수도 있다. S1602에서, 전 자 장치는 제1 방위 정보를 획득한다. S1602는 도 4의 S402에 대응될 수 있으므로 구체적인 설명은 생략 하기로 한다. S1604에서, 전자 장치는 획득한 제1 방위 정보를 클라우드 서버로 전송한다. S1606에서, 클라우드 서버는 제1 방위 정보를 기초로 제2 예측 방위 정보를 결정한다. 클라우드 서버 가 제1 방위 정보를 기초로 제2 예측 방위 정보를 결정하는 방법은 도 10 내지 도 13에서 상술한 엣지 데 이터 네트워크가 제2 예측 방위 정보를 결정하는 동작에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S1608에서, 클라우드 서버는 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정한다. S1608은 도 10 내지 도 13에서 상술한 엣지 데이터 네트워크가 제2 예측 방위 정보에 대응되는 제2 예측 부분 영상을 결정하는 동작에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S1610에서, 클라우드 서버는 제2 예측 부분 영상을 인코딩함으로써 제2 예측 프레임을 생성한다. 일 실시 예에 의하면, 클라우드 서버는 제2 예측 프레임을 전자 장치로 전송함과 함께 제2 예측 프레임을클라우드 서버내 데이터 베이스에 저장할 수도 있다. S1612에서, 클라우드 서버는 제2 예측 프레임 을 전자 장치로 전송한다. S1614에서, 전자 장치는 클라우드 서버로부터 획득된 제2 예측 프레임을 전자 장치내 버퍼에 캐싱하고, 전자 장치의 센싱 모듈로부터 제2 방위 정보를 획득할 수 있다. S1614는 도 4의 414에 대응될 수 있 으므로, 구체적인 설명은 생략하기로 한다. S1618에서, 클라우드 서버는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는지 여부를 결정할 수 있다. 예를 들어, 클라우드 서버는 제2 방위 정보 및 제2 예측 방위 정보 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 큰 경우, 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않 는 것으로 결정할 수 있다. S1620에서, 클라우드 서버는 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는다는 비교 결과에 대 한 정보를 엣지 데이터 네트워크 및 전자 장치로 전송한다. S1622에서, 엣지 데이터 네트워크 는 제1 부분 영상, 제2 예측 부분 영상 및 제2 부분 영상에 기초하여, 제2 부분 영상에 대응하는 보상 프 레임을 생성할 수 있다. S1622는 도 4의 S424에 대응될 수 있으므로 구체적인 설명은 생략하기로 한다. S1624에서, 전자 장치는 클라우드 서버로부터 수신된 제2 예측 방위 정보 및 제2 방위 정보가 일치 하지 않는다는 비교 결과에 대한 정보에 기초하여, 제2 예측 부분 영상의 재생을 대기할 수 있다. S1626에서, 엣지 데이터 네트워크는 생성된 보상 프레임을 전자 장치로 전송한다. S1628에서, 전자 장치(100 0)는 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 이용하여 제2 부분 영상을 재생할 수 있다. 예를 들어, 전자 장치는 제1 부분 영상에 대응되는 제1 프레임 내 일부 블록 및 제2 예측 프레임 내 적어도 일 부 블록을 참조함으로서 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 전자 장치의 표시 장치에 전달함으 로써, 제2 부분 영상을 재생할 수 있다. 도 17은 본 개시의 일 실시 예에 따른 클라우드 서버, 전자 장치 및 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 나타내는 흐름도이다. 도 16에서, 제2 예측 방위 정보 및 제2 방위 정보가 일치하지 않는 경우, 전자 장치, 클라우드 서버 및 엣지 데이터 네트워크 간의 동작 절차를 설명하였으므로, 도 17을 참조하여 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 경우, 클라우드 서버, 전자 장치 및 엣지 데이터 네트워크간의 동작 절차를 설명하기로 한다. 도 16의 S1602 내지 S1610은 도 17의 S1702 내지 S1710에 대응될 수 있으므로 구체적 인 설명은 생략하기로 한다. S1712에서, 클라우드 서버는 제2 예측 프레임을 엣지 데이터 네트워크 및 전자 장치로 전송 한다. S1714에서, 전자 장치는 클라우드 서버로부터 수신된 제2 예측 프레임을 전자 장치 내 버퍼 에 캐싱하고, 제2 방위 정보를 획득한다. S1714는 도 4의 S414에 대응될 수 있으므로 구체적인 설명은 생략하기 로 한다. S1716에서, 전자 장치는 제2 방위 정보를 클라우드 서버로 전송한다. S1718에서, 클라우드 서버는 제2 예측 방위 정보 및 제2 방위 정보가 일치하는지 여부를 결정할 수 있다. 예를 들어, 클라우드 서버는 제2 방위 정보 및 제2 예측 방위 정보 내 각도 성분(예컨대 Pitch, Roll, Yaw) 별 차이 값의 평균이 기 설정된 임계각보다 작은 경우 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정할 수 있다. S1720에서, 클라우드 서버는 제2 예측 방위 정보 및 제2 방위 정보가 일치한다는 비교 결과에 대한 정보 를 엣지 데이터 네트워크 및 전자 장치로 전송할 수 있다. S1722에서, 엣지 데이터 네트워크 는 제2 예측 부분 영상을 이용하여 보상 프레임을 생성한다. 예를 들어, 엣지 데이터 네트워크는 제2 예측 부분 영상과 동일한 영상 정보를 가지는 보상 프레임을 생성할 수 있다. S1724에서, 전자 장치는 제2 예측 부분 영상을 이용하여 보상 프레임을 생성할 수 있다. 현재, 제2 예측 방위 정보 및 제2 방위 정보가 일치하는 것으로 결정되었으므로, 전자 장치는 제2 예측 프레임 내 적어도 일부 블록을 참조하는 보상 프레임을 생성할 수 있다. S1724는 도 5의 S524에 대응될 수 있으므로 구체적인 설 명은 생략하기로 한다. S1726에서, 엣지 데이터 네트워크는 스스로 생성한 보상 프레임을 폐기할 수 있다. S1728에서, 전자 장치 는 S1724에서 생성된 보상 프레임을 디코딩하고, 디코딩된 보상 프레임을 재생할 수 있다. 현재, 제2 예 측 프레임은 보상 프레임과 동일한 영상 정보를 포함하므로, 전자 장치는 제2 예측 프레임만을 참조하여보상 프레임을 디코딩할 수도 있다. S1728은 도 5의 S528에 대응될 수 있으므로 구체적인 설명은 생략하기로 한 다. 도 18는 본 개시의 일 실시 예에 따른 전자 장치의 블록도이다. 도 1 내지 도 17의 엣지 데이터 네트워크에 연결된 전자 장치는 도 18의 전자 장치에 대응될 수 있다. 예 를 들어, 네트워크 환경에서 전자 장치는 제1 네트워크(예: 근거리 무선 통신 네트워크)를 통 하여 전자 장치와 통신하거나, 또는 제2 네트워크(예: 원거리 무선 통신 네트워크)를 통하여 전자 장치 또는 서버와 통신할 수 있다. 일 실시예에 따르면, 전자 장치는 서버를 통하여 전 자 장치와 통신할 수 있다. 일 실시예에 따르면, 전자 장치는 프로세서, 메모리, 입력 장치, 음향 출력 장치, 표시 장치, 오디오 모듈, 센서 모듈, 인터페이스 , 햅틱 모듈, 카메라 모듈, 전력 관리 모듈, 배터리, 통신 모듈, 가입자 식별 모듈, 또는 안테나 모듈을 포함할 수 있다. 어떤 실시예에서는, 전자 장치에는, 이 구성 요소들 중 적어도 하나(예: 표시 장치 또는 카메라 모듈)이 생략되거나, 하나 이상의 다른 구성 요 소가 추가될 수 있다. 어떤 실시예에서는, 이 구성 요소들 중 일부들은 하나의 통합된 회로로 구현될 수 있다. 예를 들면, 센서 모듈(예: 지문 센서, 홍채 센서, 또는 조도 센서)은 표시 장치(예: 디스플레이)에 임베디드(embedded)된 채 구현될 수 있다. 프로세서는, 예를 들면, 소프트웨어(예: 프로그램)을 실행하여 프로세서에 연결된 전자 장치 의 적어도 하나의 다른 구성 요소(예: 하드웨어 또는 소프트웨어 구성 요소)를 제어할 수 있고, 다양한 데 이터 처리 또는 연산을 수행할 수 있다. 일 실시예에 따르면, 데이터 처리 또는 연산의 적어도 일부로서, 프로 세서는 다른 구성 요소(예: 센서 모듈 또는 통신 모듈)로부터 수신된 명령 또는 데이터를 휘 발성 메모리(volatile memory)에 로드(load)하고, 휘발성 메모리에 저장된 명령 또는 데이터를 처 리하고, 결과 데이터를 비휘발성 메모리(non-volatile memory)에 저장할 수 있다. 일 실시예에 따르면, 프로세서는, 애플리케이션 클라이언트를 실행할 수 있고, 애플리케이션 클라이언트 가 실행됨에 따라, 애플리케이션 클라이언트가 이용할 네트워크 프로토콜을 확인하기 위한 요청 메시지를 엣지 데이터 네트워크에 전송할 수 있다. 또한, 프로세서는 애플리케이션 클라이언트가 이용할 네트워크 프로토콜을 나타내는 응답 메시지를 엣지 데이터 네트워크로부터 수신할 수 있다. 프로세서는, 응 답 메시지에 기초하여 UE 애플리케이션이 이용할 네트워크 프로토콜을 업데이트할 수 있다. 프로세서는 업데이트된 네트워크 프로토콜에 대응하는 네트워크 소켓을 선택할 수 있다. 프로세서는 선택된 네트워크 소켓을 이용하여, 엣지 데이터 네트워크로부터 애플리케이션 클라이언트에 대하여 생성된 데이터를 수신 할 수 있다. 일 실시예에 따르면, 프로세서는 메인 프로세서(예: 중앙 처리 장치(CPU, central processing unit) 또는 어플리케이션 프로세서(AP, application processor)), 및 이와는 독립적으로 또는 함께 운영 가능한 보조 프로세서(예: 그래픽 처리 장치(GPU, graphic processing unit), 이미지 시그널 프로세서(ISP, image signal processor), 센서 허브 프로세서(sensor hub processor), 또는 커뮤니케이션 프로세서(CP, communication processor))를 포함할 수 있다. 추가적으로 또는 대체적으로, 보조 프로세서는 메인 프로 세서보다 저전력을 사용하거나, 또는 지정된 기능에 특화되도록 설정될 수 있다. 보조 프로세서는 메인 프로세서와 별개로, 또는 그 일부로서 구현될 수 있다. 보조 프로세서는, 예를 들면, 메인 프로세서가 인액티브(inactive)(예: 슬립(sleep)) 상태에 있는 동안 메인 프로세서를 대신하여, 또는 메인 프로세서가 액티브(active)(예: 어플리케이션 실행) 상 태에 있는 동안 메인 프로세서와 함께, 전자 장치의 구성 요소들 중 적어도 하나의 구성 요소(예: 표시 장치, 센서 모듈, 또는 통신 모듈)과 관련된 기능 또는 상태들의 적어도 일부를 제어할 수 있다. 일 실시예에 따르면, 보조 프로세서(예: 이미지 시그널 프로세서 또는 커뮤니케이션 프로세서) 는 기능적으로 관련 있는 다른 구성 요소(예: 카메라 모듈 또는 통신 모듈)의 일부로서 구현될 수 있다. 메모리는, 전자 장치의 적어도 하나의 구성 요소(예: 프로세서 또는 센서 모듈)에 의해 사용되는 다양한 데이터를 저장할 수 있다. 데이터는, 예를 들어, 소프트웨어(예: 프로그램) 및, 이와 관 련된 명령에 대한 입력 데이터 또는 출력 데이터를 포함할 수 있다. 메모리는, 휘발성 메모리 또는 비휘발성 메모리를 포함할 수 있다. 프로그램은 메모리에 소프트웨어로서 저장될 수 있으며, 예를 들면, 운영 체제(OS, operating system), 미들웨어(middleware) 또는 어플리케이션을 포함할 수 있다. 일 실시예에서, 프로 그램은, 도 1의 제1 애플리케이션 클라이언트 및 제2 애플리케이션 클라이언트를 포함할 수 있 다. 그리고, 프로그램은 도 8의 엣지 인에이블러 클라이언트를 포함할 수 있다. 입력 장치는, 전자 장치의 구성 요소(예: 프로세서)에 사용될 명령 또는 데이터를 전자 장치 의 외부(예: 사용자)로부터 수신할 수 있다. 입력 장치는, 예를 들면, 마이크, 마우스, 키보드, 또 는 디지털 펜(예: 스타일러스 펜)을 포함할 수 있다. 음향 출력 장치는 음향 신호를 전자 장치의 외부로 출력할 수 있다. 음향 출력 장치는, 예를 들면, 스피커(speaker) 또는 리시버(receiver)를 포함할 수 있다. 스피커는 멀티미디어 재생 또는 녹음 재생과 같이 일반적인 용도로 사용될 수 있고, 리시버는 착신 전화를 수신하기 위해 사용될 수 있다. 일 실시예에 따르 면, 리시버는 스피커와 별개로, 또는 그 일부로서 구현될 수 있다. 표시 장치는 전자 장치의 외부 (예: 사용자)로 정보를 시각적으로 제공할 수 있다. 표시 장치는, 예를 들면, 디스플레이, 홀로그램 장치, 또는 프로젝터 및 해당 장치를 제어하기 위한 제어 회로를 포함할 수 있다. 일 실시예에 따르면, 표시 장 치는 터치를 감지하도록 설정된 터치 회로(touch circuitry), 또는 상기 터치에 의해 발생되는 힘의 세기 를 측정하도록 설정된 센서 회로(예: 압력 센서(pressure sensor))를 포함할 수 있다. 오디오 모듈은 소리를 전기 신호로 변환시키거나, 반대로 전기 신호를 소리로 변환시킬 수 있다. 일 실시 예에 따르면, 오디오 모듈은, 입력 장치를 통해 소리를 획득하거나, 음향 출력 장치, 또는 전자 장치와 직접 또는 무선으로 연결된 외부 전자 장치(예: 전자 장치)(예: 스피커 또는 헤드폰)) 를 통해 소리를 출력할 수 있다. 센서 모듈은 전자 장치의 작동 상태(예: 전력 또는 온도), 또는 외부의 환경 상태(예: 사용자 상 태)를 감지하고, 감지된 상태에 대응하는 전기 신호 또는 데이터 값을 생성할 수 있다. 일 실시예에 따르면, 센 서 모듈은, 예를 들면, 제스처 센서(gesture sensor), 자이로 센서(gyro sensor), 기압 센서(barometer sensor), 마그네틱 센서(magnetic sensor), 가속도 센서(acceleration sensor), 그립 센서(grip sensor), 근접 센서(proximity sensor), 컬러 센서(color sensor)(예: RGB(red, green, blue) 센서), IR(infrared) 센서, 생 체 센서(biometric sensor), 온도 센서(temperature sensor), 습도 센서(humidity sensor), 조도 센서 (illuminance sensor), 또는 자율 주행차와 관련된 센서들(예: 관성 측정 센서(IMU), GPS(Global Positioning System) 센서, 카메라, LIDAR(Light Imaging Detection and Ranging), RADAR(Radio Detection and Ranging) 등)을 포함하는 센서 모듈을 포함할 수 있다. 인터페이스는 전자 장치의 외부 전자 장치(예: 전자 장치)와 직접 또는 무선으로 연결되기 위 해 사용될 수 있는 하나 이상의 지정된 프로토콜(protocol)들을 지원할 수 있다.연결 단자(connection terminal)는, 그를 통해서 전자 장치가 외부 전자 장치(예: 전자 장치)와 물리적으로 연결될 수 있는 커넥터를 포함할 수 있다. 햅틱 모듈(haptic module)은 전기적 신호를 사용자가 촉각 또는 운동 감각을 통해서 인지할 수 있는 기계적인 자극(예: 진동 또는 움직임) 또는 전기적인 자극으로 변환할 수 있다. 카메라 모듈은 정지 영상 및 동영상을 촬영할 수 있다. 일 실시예에 따르면, 카메라 모듈은 하나 이상의 렌즈들, 이미지 센서들, 이미지 시그널 프로세서들, 또는 플래시들을 포함할 수 있다. 전력 관리 모듈은 전자 장치에 공급되는 전력을 관리할 수 있다. 배터리는 전자 장치의 적어도 하나의 구성 요소에 전력을 공급할 수 있다. 통신 모듈은 전자 장치와 외부 전자 장치(예: 전자 장치, 전자 장치, 또는 서버 ) 간의 직접(예: 유선) 통신 채널 또는 무선 통신 채널의 수립, 및 수립된 통신 채널을 통한 통신 수행을 지원할 수 있다. 통신 모듈은 프로세서(예: 어플리케이션 프로세서)와 독립적으로 운영되고, 직접 (예: 유선) 통신 또는 무선 통신을 지원하는 하나 이상의 커뮤니케이션 프로세서를 포함할 수 있다. 일 실시예 에 따르면, 통신 모듈은 무선 통신 모듈(예: 셀룰러 통신 모듈, 근거리 무선 통신 모듈, 또는 GNSS(global navigation satellite system) 통신 모듈) 또는 유선 통신 모듈(예: LAN(local area network) 통신 모듈, 또는 전력선 통신 모듈)을 포함할 수 있다. 이들 통신 모듈 중 해당하는 통신 모듈은 제1 네트워크(예: 블루투스, Wi-Fi direct 또는 IrDA(infrared data association) 같은 근거리 통신 네트워 크) 또는 제2 네트워크(예: 셀룰러 네트워크, 인터넷, 또는 컴퓨터 네트워크(예: LAN 또는 WAN(wide area network))와 같은 원거리 통신 네트워크)를 통하여 외부 전자 장치와 통신할 수 있다. 이런 여러 종류의 통신모듈들은 하나의 구성 요소(예: 단일 칩)로 통합되거나, 또는 서로 별도의 복수의 구성 요소들(예: 복수 칩들) 로 구현될 수 있다. 무선 통신 모듈은 가입자 식별 모듈에 저장된 가입자 정보(예: 국제 모바일 가입자 식별자(IMSI, international mobile subscriber identity))를 이용하여 제1 네트워크 또는 제2 네트워크와 같은 통신 네트워크 내에서 전자 장치를 확인 및 인증할 수 있다. 안테나 모듈은 신호 또는 전력을 외부(예: 외부 전자 장치)로 송신하거나 외부로부터 수신할 수 있다. 상기 구성 요소들 중 적어도 일부는 주변 기기들간 통신 방식(예: 버스, GPIO(general purpose input and output), SPI(serial peripheral interface), 또는 MIPI(mobile industry processor interface))를 통해 서로 연결되고, 신호(예: 명령 또는 데이터)를 상호 간에 교환할 수 있다. 일 실시예에 따르면, 명령 또는 데이터는 제2 네트워크에 연결된 서버를 통해서 전자 장치와 외부의 전자 장치 간에 송신 또는 수신될 수 있다. 전자 장치(1702, 1704) 각각은 전자 장치와 동일 한 또는 다른 종류의 장치일 수 있다. 일 실시예에 따르면, 전자 장치에서 실행되는 동작들의 전부 또는 일부는 외부 전자 장치들(1702, 1704 또 는 1708) 중 하나 이상의 외부 장치들에서 실행될 수 있다. 예를 들면, 전자 장치가 어떤 기능이나 서비스 를 자동으로, 또는 사용자 또는 다른 장치로부터의 요청에 반응하여 수행해야 할 경우에, 전자 장치는 기 능 또는 서비스를 자체적으로 실행시키는 대신에 또는 추가적으로, 하나 이상의 외부 전자 장치들(1702, 1704) 에게 그 기능 또는 그 서비스의 적어도 일부를 수행하라고 요청할 수 있다. 상기 요청을 수신한 하나 이상의 외 부 전자 장치들(1702, 1704)은 요청된 기능 또는 서비스의 적어도 일부, 또는 상기 요청과 관련된 추가 기능 또 는 서비스를 실행하고, 그 실행의 결과를 전자 장치로 전달할 수 있다. 전자 장치는 상기 결과를, 그 대로 또는 추가적으로 처리하여, 상기 요청에 대한 응답의 적어도 일부로서 제공할 수 있다. 이를 위하여, 예를 들면, 클라우드 컴퓨팅(cloud computing), 분산 컴퓨팅(distributed computing), 또는 클라이언트-서버 컴퓨팅 (client-server computing) 기술이 이용될 수 있다."}
{"patent_id": "10-2020-0022377", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 네트워크 환경에서 MEC(multi-access edge computing) 기술을 설명하기 위해 개략적으로 도시하는 도면이다. 도 2는 본 개시의 일 실시 예에 따른 전자 장치와 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 개략적으로 설명하기 위한 도면이다. 도 3은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크(edge data network) 사이에서 전송되는 부분 영 상들의 관계를 나타내는 도면이다. 도 4는 본 개시의 일 실시 예에 따른 전자 장치와 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. 도 5는 또 다른 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. 도 6은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 사이에서 전송되는 부분 영상들의 관계를 나타내 는 참고 도면이다. 도 7은 일 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다.도 8은 또 다른 실시 예에 따른 전자 장치 및 엣지 데이터 네트워크 간의 동작 절차를 나타내는 흐름도이다. 도 9는 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 영상 컨텐츠를 스트리밍 하는 방법을 나타내는 흐 름도이다. 도 10은 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제2 예 측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. 도 11은 본 개시의 또 다른 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제 2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. 도 12는 본 개시의 일 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제2 예 측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 나타내기 위한 도면이다. 도 13은 본 개시의 또 다른 실시 예에 따른 엣지 데이터 네트워크가 제2 예측 방위 정보를 결정하고, 결정된 제 2 예측 방위 정보에 기초하여 제2 예측 부분 영상을 결정하는 방법을 구체적으로 설명하기 위한 도면이다. 도 14는 본 개시의 일 실시 예에 따른 전자 장치가 엣지 데이터 네트워크로부터 획득된 영상 컨텐츠를 스트리밍 하는 방법을 나타내는 흐름도이다. 도 15은 본 개시의 일 실시 예에 따른 전자 장치가 전자 장치의 버퍼에 저장된 보상 프레임들을 재생하는 과정 을 설명하기 위한 도면이다. 도 16은 본 개시의 일 실시 예에 따른 클라우드 서버, 전자 장치 및 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 나타내는 흐름도이다. 도 17은 본 개시의 일 실시 예에 따른 클라우드 서버, 전자 장치 및 엣지 데이터 네트워크(edge data network) 간의 동작 절차를 나타내는 흐름도이다. 도 18는 본 개시의 일 실시 예에 따른 전자 장치의 블록도이다."}
