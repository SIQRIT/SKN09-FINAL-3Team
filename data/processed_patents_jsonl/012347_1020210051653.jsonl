{"patent_id": "10-2021-0051653", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0145051", "출원번호": "10-2021-0051653", "발명의 명칭": "행동에 대한 모방학습을 수행하는 전자 장치 및 그의 동작 방법", "출원인": "한국전자통신연구원", "발명자": "최진철"}}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자의 행동을 학습하는 전자 장치의 동작 방법에 있어서,상기 사용자의 행동과 관련된 입력 데이터를 수신하는 단계;상기 입력 데이터를 처리하여 제1 행동 궤적 정보를 획득하는 단계;상기 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성하는 단계;상기 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득하는 단계;상기 제1 행동 궤적 정보 및 상기 제2 행동 궤적 정보를 샘플링하는 단계;상기 제1 행동 궤적 정보 및 상기 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하는 단계; 및상기 평가 모델을 기반으로 상기 초기 행동 정책을 업데이트하는 단계를 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 입력 데이터는 상기 전자 장치의 현재 상태에 대한 상태 데이터 및 상기 사용자가 상기 전자 장치를 제어하기 위해 입력하는 제어 데이터를 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 제1 행동 궤적 정보를 획득하는 단계는 상기 상태 데이터를 기반으로 상기 제어 데이터를 매칭시켜, 상기상태 데이터 및 상기 제어 데이터의 쌍으로 구성된 상기 제1 행동 궤적 정보를 생성하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항에 있어서,상기 초기 행동 정책을 생성하는 단계는 상기 제1 행동 궤적 정보에 대한 지도 학습(Supervised Learning)을 통해 상기 초기 행동 정책을 도출하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서,상기 제2 행동 궤적 정보를 획득하는 단계는:상기 입력 데이터로부터 상기 전자 장치의 현재 상태에 대한 상태 데이터를 획득하는 단계;상기 초기 행동 정책을 기반으로 상기 상태 데이터를 처리하여 상기 전자 장치를 제어하기 위한 자율 제어 데이터를 도출하는 단계; 및상기 상태 데이터를 기반으로 상기 자율 제어 데이터를 매칭시켜, 상기 상태 데이터 및 상기 자율 제어 데이터의 쌍으로 구성된 상기 제2 행동 궤적 정보를 생성하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1 항에 있어서,상기 샘플링하는 단계는:공개특허 10-2022-0145051-3-상기 제1 행동 궤적 정보를 추적하여 제1 데이터 집합을 생성하는 단계;상기 제1 데이터 집합을 지정된 배치 사이즈로 샘플링하여 제1 샘플 데이터를 생성하는 단계;상기 제2 행동 궤적 정보를 추적하여 제2 데이터 집합을 생성하는 단계; 및상기 제2 데이터 집합을 지정된 배치 사이즈로 샘플링하여 제2 샘플 데이터를 생성하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 평가 모델을 학습하는 단계는:상기 제1 샘플 데이터 및 상기 제2 샘플 데이터에 대해 행동 정책의 출처 및 작업의 성공 여부를 구분하는 레이블을 추가하는 단계; 및지도 학습을 이용하여 상기 레이블을 기반으로 상기 제1 샘플 데이터 및 상기 제2 샘플 데이터를 구분하도록 상기 평가 모델을 학습하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 초기 행동 정책을 업데이트하는 단계는 상기 평가 모델을 보상 함수로 이용하여 강화 학습(ReinforcementLearning)을 통해 상기 초기 행동 정책에 대한 학습을 수행하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 초기 행동 정책을 업데이트하는 단계는: 상기 학습된 행동 정책을 기반으로 제3 행동 궤적 정보를 획득하는 단계; 및상기 제3 행동 궤적 정보를 샘플링하여 제3 샘플 데이터를 생성하는 단계를 더 포함하는 전자 장치의 동작방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서,상기 제1 샘플 데이터 및 상기 제3 샘플 데이터를 기반으로 상기 평가 모델을 학습하고 상기 학습된 행동 정책을 업데이트하는 단계를 더 포함하는 전자 장치의 동작 방법."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "전자 장치의 현재 상태에 대한 상태 데이터를 획득하도록 구성된 센서;사용자로부터 입력된 제어 데이터를 기반으로 구동되도록 구성된 구동 장치; 및상기 사용자의 행동을 학습하도록 구성된 프로세서를 포함하고,상기 프로세서는:상기 상태 데이터 및 상기 제어 데이터를 수신하고, 상기 상태 데이터 및 상기 제어 데이터를 매칭시켜 제1 행동 궤적 정보를 획득하도록 구성된 데이터 처리 회로; 및상기 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성하고,상기 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득하고,상기 제1 행동 궤적 정보 및 상기 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하고, 및상기 평가 모델을 기반으로 상기 초기 행동 정책을 업데이트하도록 구성된 행동정책 학습 회로를 포함하는 전자공개특허 10-2022-0145051-4-장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 제1 행동 궤적 정보는 상기 상태 데이터 및 상기 제어 데이터의 쌍으로 구성된 행동 특징 벡터에 대한 정보를 포함하는 전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항에 있어서,상기 행동정책 학습 회로는 상기 제1 행동 궤적 정보에 대한 지도 학습을 통해 상기 초기 행동 정책을 도출하는전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항에 있어서,상기 행동정책 학습 회로는:상기 초기 행동 정책을 기반으로 상기 상태 데이터를 처리하여 상기 전자 장치를 제어하기 위한 자율 제어 데이터를 도출하고, 및상기 상태 데이터를 기반으로 상기 자율 제어 데이터를 매칭시켜 상기 상태 데이터 및 상기 자율 제어 데이터의쌍으로 구성된 상기 제2 행동 궤적 정보를 생성하는 전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11 항에 있어서,상기 행동정책 학습 회로는:상기 제1 행동 궤적 정보를 샘플링하여 제1 샘플 데이터를 생성하고,상기 제2 행동 궤적 정보를 샘플링하여 제2 샘플 데이터를 생성하고, 및상기 제1 샘플 데이터 및 상기 제2 샘플 데이터에 대해 행동 정책의 출처 및 작업의 성공 여부를 구분하는 레이블을 추가하는 전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15 항에 있어서,상기 행동정책 학습 회로는 지도 학습을 이용하여 상기 레이블을 기반으로 상기 제1 샘플 데이터 및 상기 제2샘플 데이터를 구분하도록 상기 평가 모델을 학습하는 전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16 항에 있어서,상기 행동정책 학습 회로는 상기 평가 모델을 보상 함수로 이용하여 강화 학습을 통해 상기 초기 행동 정책에대한 학습을 수행하는 전자 장치."}
{"patent_id": "10-2021-0051653", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17 항에 있어서,상기 행동정책 학습 회로는 학습된 행동 정책을 평가하여 상기 학습된 행동 정책의 성능이 기준을 충족하면 최종 행동 정책을 저장하는 전자 장치. 공개특허 10-2022-0145051-5-"}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 사용자의 행동을 학습하는 전자 장치의 동작 방법은 사용자의 행동과 관련된 입력 데이터를 수신하는 단계, 입력 데이터를 처리하여 제1 행동 궤적 정보를 획득하는 단계, 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성하는 단계, 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득하는 단계, 제 1 행동 궤적 정보 및 제2 행동 궤적 정보를 샘플링하는 단계, 제1 행동 궤적 정보 및 제2 행동 궤적 정보를 구분 하는 평가 모델을 학습하는 단계, 및 평가 모델을 기반으로 초기 행동 정책을 업데이트하는 단계를 포함한다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 모방학습 기술에 관한 것으로, 좀 더 상세하게는 전문가의 행동과 유사한 행동을 결정하는 정책을 도 출하여 행동에 대한 모방학습을 수행하는 전자 장치 및 그의 동작 방법에 관한 것이다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "초기의 로봇은 생산 현장에서 작업의 자동화 및 무인화를 등을 목적으로 반복적인 작업을 인간 대신 수행하는 역할로 한정되어 있었다. 하지만 최근에는 로봇의 활동 영역이 확장되어, 사람과 복잡한 상호작용이 필요한 안 내 로봇, 교육용 로봇 등의 서비스 로봇들이 개발되고 있으며, 제품의 다양화로 인해 공장용 로봇 역시 새로운 작업에 적용 가능한 확장성이 요구되고 있다. 또한, 홈 서비스 로봇, 나아가 자율주행 차량, 드론, 및 사물인터넷 등 사람의 작업을 대체하거나 보조할 지능 화 사물들이 고려되어, 이에 대한 연구 개발이 활발하게 진행되고 있다. 이러한 환경에서, 지능화 사물들의 확장성을 보장하기 위한 방법으로 모방학습이 연구되고 있다. 이는 지능화 사물들이 새로운 환경에서 새로운 작업의 수행을 요구 받을 때, 이를 사람의 행동을 모방하여 수행하는 방식을 의미한다. 모방학습은 학습에 기반하는 방식으로써, 수행 가능한 행동 범위를 넓힐 수 있는 확장성을 보장할 수 있다. 또한, 각 작업마다 제각각인 목표와 조건들을 고려하는 것이 아니라 이들이 이미 반영된 사람의 시연을 고려함으로써, 다양한 작업에 대해 일반화된 방법을 제시할 수 있다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 전문가의 시연 데이터를 기반으로 전문가와 유사한 행동을 결정하는 정책을 도출하는 행동 모방학습 을 위한 전자 장치 및 그의 동작 방법을 제공할 수 있다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 사용자의 행동을 학습하는 전자 장치의 동작 방법은 사용자의 행동과 관련된 입력 데이터를 수신하는 단계, 입력 데이터를 처리하여 제1 행동 궤적 정보를 획득하는 단계, 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성하는 단계, 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득하는 단계, 제1 행동 궤적 정보 및 제2 행동 궤적 정보를 샘플링하는 단계, 제1 행동 궤적 정보 및 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하는 단계, 및 평가 모델을 기반으로 초기 행동 정책을 업데이트하는 단계를 포함한 다. 일 실시 예에서, 입력 데이터는 전자 장치의 현재 상태에 대한 상태 데이터 및 사용자가 전자 장치를 제어하기 위해 입력하는 제어 데이터를 포함한다. 일 실시 예에서, 제1 행동 궤적 정보를 획득하는 단계는 상태 데이터를 기반으로 제어 데이터를 매칭시켜, 상태 데이터 및 제어 데이터의 쌍으로 구성된 제1 행동 궤적 정보를 생성하는 단계를 더 포함한다. 일 실시 예에서, 초기 행동 정책을 생성하는 단계는 제1 행동 궤적 정보에 대한 지도 학습(Supervised Learning)을 통해 초기 행동 정책을 도출하는 단계를 더 포함한다. 일 실시 예에서, 제2 행동 궤적 정보를 획득하는 단계는 입력 데이터로부터 전자 장치의 현재 상태에 대한 상태 데이터를 획득하는 단계, 초기 행동 정책을 기반으로 상태 데이터를 처리하여 전자 장치를 제어하기 위한 자율 제어 데이터를 도출하는 단계, 및 상태 데이터를 기반으로 자율 제어 데이터를 매칭시켜, 상태 데이터 및 자율 제어 데이터의 쌍으로 구성된 제2 행동 궤적 정보를 생성하는 단계를 더 포함한다. 일 실시 예에서, 샘플링하는 단계는 제1 행동 궤적 정보를 추적하여 제1 데이터 집합을 생성하는 단계, 제1 데 이터 집합을 지정된 배치 사이즈로 샘플링하여 제1 샘플 데이터를 생성하는 단계, 제2 행동 궤적 정보를 추적하 여 제2 데이터 집합을 생성하는 단계, 및 제2 데이터 집합을 지정된 배치 사이즈로 샘플링하여 제2 샘플 데이터 를 생성하는 단계를 더 포함하는 전자 장치의 동작 방법.일 실시 예에서, 평가 모델을 학습하는 단계는 제1 샘플 데이터 및 제2 샘플 데이터에 대해 행동 정책의 출처 및 작업의 성공 여부를 구분하는 레이블을 추가하는 단계, 및 지도 학습을 이용하여 레이블을 기반으로 제1 샘 플 데이터 및 제2 샘플 데이터를 구분하도록 평가 모델을 학습하는 단계를 더 포함한다. 일 실시 예에서, 초기 행동 정책을 업데이트하는 단계는 평가 모델을 보상 함수로 이용하여 강화 학습 (Reinforcement Learning)을 통해 초기 행동 정책에 대한 학습을 수행하는 단계를 더 포함한다. 일 실시 예에서, 초기 행동 정책을 업데이트하는 단계는 학습된 행동 정책을 기반으로 제3 행동 궤적 정보를 획 득하는 단계, 및 제3 행동 궤적 정보를 샘플링하여 제3 샘플 데이터를 생성하는 단계를 더 포함한다. 일 실시 예에서, 제1 샘플 데이터 및 제3 샘플 데이터를 기반으로 평가 모델을 학습하는 단계 및 학습된 행동 정책을 업데이트하는 단계를 더 포함한다. 본 개시의 일 실시 예에 따른 전자 장치는 전자 장치의 현재 상태에 대한 상태 데이터를 획득하도록 구성된 센 서, 사용자로부터 입력된 제어 데이터를 기반으로 구동되도록 구성된 구동 장치, 및 사용자의 행동을 학습하도 록 구성된 프로세서를 포함한다. 프로세서는 상태 데이터 및 제어 데이터를 수신하고, 상태 데이터 및 제어 데 이터를 매칭시켜 제1 행동 궤적 정보를 획득하도록 구성된 데이터 처리 회로, 및 제1 행동 궤적 정보를 기반으 로 초기 행동 정책을 생성하고, 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득하고, 제1 행동 궤적 정 보 및 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하고, 및 평가 모델을 기반으로 초기 행동 정책을 업데 이트하도록 구성된 행동정책 학습 회로를 포함한다. 일 실시 예에서, 제1 행동 궤적 정보는 상태 데이터 및 상기 제어 데이터의 쌍으로 구성된 행동 특징 벡터에 대 한 정보를 포함한다. 일 실시 예에서, 행동정책 학습 회로는 제1 행동 궤적 정보에 대한 지도 학습을 통해 초기 행동 정책을 도출한 다. 일 실시 예에서, 행동정책 학습 회로는 초기 행동 정책을 기반으로 상태 데이터를 처리하여 전자 장치를 제어하 기 위한 자율 제어 데이터를 도출하고, 및 상태 데이터를 기반으로 자율 제어 데이터를 매칭시켜 상태 데이터 및 자율 제어 데이터의 쌍으로 구성된 제2 행동 궤적 정보를 생성한다. 일 실시 예에서, 행동정책 학습 회로는 제1 행동 궤적 정보를 샘플링하여 제1 샘플 데이터를 생성하고, 제2 행 동 궤적 정보를 샘플링하여 제2 샘플 데이터를 생성하고, 및 제1 샘플 데이터 및 제2 샘플 데이터에 대해 행동 정책의 출처 및 작업의 성공 여부를 구분하는 레이블을 추가한다. 일 실시 예에서, 행동정책 학습 회로는 지도 학습을 이용하여 레이블을 기반으로 제1 샘플 데이터 및 제2 샘플 데이터를 구분하도록 평가 모델을 학습한다. 일 실시 예에서, 행동정책 학습 회로는 평가 모델을 보상 함수로 이용하여 강화 학습을 통해 초기 행동 정책에 대한 학습을 수행한다. 일 실시 예에서, 행동정책 학습 회로는 학습된 행동 정책을 평가하여 학습된 행동 정책의 성능이 기준을 충족하 면 최종 행동 정책을 저장한다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 전자 장치는 전문가 행동 궤적과 학습자 행동 궤적을 구분하는 평가 모델을 도출하고, 이를 기반으로 행동 정책을 학습할 수 있다. 또한, 이러한 일련의 학습 사이클을 반복함으로써, 행동 정책의 성능이 개선될 수 있다. 이로써, 전자 장치는 자율형 사물인터넷 환경에서 전문가의 행동 궤적을 모방 학습하여 사용자 의 개입 없이 전문가가 조작하는 것과 유사하게 행동하는 지능을 구축할 수 있다."}
{"patent_id": "10-2021-0051653", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서, 본 개시의 기술 분야에서 통상의 지식을 가진 자가 본 개시를 용이하게 실시할 수 있을 정도로, 본 개시의 실시 예들이 명확하고 상세하게 기재될 것이다. 이하에서, 첨부한 도면들을 참조하여, 본 개시의 바람직한 실시 예를 보다 상세하게 설명하고자 한다. 본 개시 를 설명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 유사한 구성요소에 대해서는 유사한 참조부 호가 사용되고, 그리고 유사한 구성요소에 대해서 중복된 설명은 생략된다. 도 1은 본 개시의 실시 예에 따른 전자 장치가 활용되는 환경을 나타내는 도면이다. 도 1을 참조하면, 사물(1 0)은 사용자로부터 입력 신호를 수신하고, 입력 신호를 기반으로 사용자의 행동을 모방하여 학습하는 지능화 사물 또는 자율화 사물을 포함할 수 있다. 예를 들어, 사물은 드론, 차량, 및 로봇을 포함할 수 있다. 경험과 시행착오를 통해 배울 수 있는 능력은 지능의 중요한 요소이다. 이러한 능력은 강화 학습(Reinforcement Learning)의 형태로 구체화될 수 있다. 강화 학습은 주어진 상태에 따라 최적의 행동을 선택하는 학습 방법으로, 특정 상태에서 결정한 행동의 좋고 나쁜 정도를 고려하여 특정 상태에서 최대의 보상을 받을 수 있는 정책을 수립하는 것을 목표로 한다. 그러나 현실의 많은 문제에서 행동에 따른 보상을 정확히 측정하고 모델링 하기 어려워 활용성이 떨어지는 한계가 있다. 이에 대한 대안으로, 전문가의 시연이나 행동 이력 데이터를 기반으로 특정상태에서의 행동을 마치 전문가처럼 결정해주는 모방 학습(imitation learning)이 각광받고 있다. 모방 학습의 목표는 전문가의 행동정책과 유사한 정책을 학습하여 도출하는데 있다. 모방 학습의 한 부류인 행동복제(Behavior cloning)는 회귀분석법 또는 지도 학습법을 이용하여 전문가의 각 상태에 따라 시연 데이터와 학습자의 행동이 직접적으로 매핑되는 정책을 도출 한다. 이 방식은 간단하지만 시연 데이터에 오버피팅(overfitting)되는 문제와 연속적인 행동들의 연관성을 고 려하지 않아 시간이 흐를수록 오차가 커지는 문제가 있다. 본 개시에 따른 사물은 모방 학습의 한 종류로서, 생성적 적대(Generative adversarial) 학습 기법을 기반 으로 전문가와 유사한 행동을 결정하는 정책을 도출할 수 있다. 생성적 적대 학습 기법은 행동 정책 생성자와 행동 정책 판별자가 상호 대립을 반복하면서 개선된 행동 정책을 도출하는 기법을 의미한다. 사물은 생성적 적대 학습 기법을 수행하기 위해 전자 장치를 포함할 수 있다. 전자 장치는 생성 적 적대 학습 기법을 기반으로 사용자의 행동에 대한 학습을 수행하고 사용자의 행동을 모방할 수 있다. 사용자는 전문가일 수 있고, 전자 장치는 전문가의 행동을 모방하여 사용자의 구체적인 지 시나 조작없이 스스로 행동하기 위한 행동지능을 생성할 수 있다. 실시 예에 따라, 전자 장치는 외부 서버에 구현될 수 있다. 이 경우, 전자 장치는 드론, 차량 , 및 로봇 중 적어도 어느 하나와 데이터 및 신호를 주고 받으면서 드론, 차량, 또는 로봇(1 3)이 사용자의 행동을 학습하고 모방하도록 제어할 수 있다. 실시 예에 따라, 전자 장치는 드론, 차량, 및 로봇 중 적어도 어느 하나에 구현될 수 있다. 이 경우, 전자 장치는 사용자로부터 직접 입력 신호 또는 입력 데이터를 수신하여 사용자의 행동 을 학습하고 모방할 수 있다. 설명의 편의를 위해, 이하 명세서에서, 전자 장치는 드론, 차량, 및 로봇 중 적어도 어느 하나에 구현되었음을 가정한다. 도 2는 본 개시의 실시 예에 따른 전자 장치의 블록도이다. 도 1 및 도 2를 참조하면, 전자 장치는 전원 관리부, 센서, 사용자 인터페이스, 메모리, 통신 장치, 구동 장치, 및 프로세 서를 포함할 수 있다. 전원 관리부는, 전자 장치에 전원을 공급할 수 있다. 전원 관리부는, 배터리로부터 전원을 공급 받아, 전자 장치의 각 유닛에 전원을 공급할 수 있다. 예를 들어, 전원 관리부는 SMPS(switched-mode power supply)로 구현될 수 있다. 센서는 전자 장치의 상태 데이터를 센싱할 수 있다. 상태 데이터는 전자 장치의 현재 상태에 대 한 데이터를 포함할 수 있다. 예를 들어, 센서는 속도 센서, 가속도 센서, 좌표 센서, 경사 센서, 전진/후 진 센서, 배터리 센서, 연료 센서, 스티어링 센서, 온도 센서, 습도 센서, 초음파 센서, 조도 센서, 가속 및 브레이크 센서 중 적어도 어느 하나를 포함할 수 있다. 센서는 적어도 하나의 센서에서 생성되는 신호에 기초하여, 전자 장치의 상태 데이터를 생성할 수 있 다. 예를 들어, 상태 데이터는 속도 데이터, 가속도 데이터, 공간 좌표 데이터, 기타 센싱 데이터를 포함할 수 있다. 실시 예에 따라, 센서는 전자 장치의 외부의 오브젝트를 검출할 수 있는 적어도 하나의 센서를 포함 할 수 있다. 예를 들어, 센서는 카메라, 레이다, 라이다, 초음파 센서 및 적외선 센서 중 적어도 하나를 포함할 수 있다. 센서는 적어도 하나의 센서에서 생성되는 신호에 기초하여 외부의 오브젝트에 대한 정보 를 생성할 수 있다. 사용자 인터페이스는 사용자로부터 입력 신호 수신하고, 사용자에게 전자 장치에서 생성된 정보를 제공할 수 있다. 사용자 인터페이스는 입력 장치 및 출력 장치를 포함할 수 있다. 예를 들어, 입력 장치는 터치 입력 장치, 기계식 입력 장치, 음성 입력 장치, 제스쳐 입력 장치 등을 포함할 수 있다. 예를 들어, 출력 장치는 스피커, 디스플레이, 햅틱 모듈 등을 포함할 수 있다. 메모리는 프로세서와 전기적으로 연결될 수 있다. 메모리는 유닛에 대한 기본데이터, 유닛의 동 작제어를 위한 제어데이터, 입출력되는 데이터를 저장할 수 있다. 메모리는 프로세서에서 처리된 데 이터를 저장할 수 있다. 메모리는 하드웨어적으로, ROM, RAM, EPROM, 플래시 드라이브, 하드 드라이브 중 적어도 어느 하나로 구성될 수 있다. 실시 예에 따라, 메모리는 프로세서와 일체형으로 구현되거나 프로세서의 하위 구성으로 분류될 수 있다. 통신 장치는 전자 장치 외부에 위치하는 디바이스와 신호를 교환할 수 있다. 통신 장치는 통신 을 수행하기 위해 송신 안테나, 수신 안테나, 각종 통신 프로토콜이 구현 가능한 RF(Radio Frequency) 회로 및 RF 소자 중 적어도 어느 하나를 포함할 수 있다. 구동 장치는 전자 장치의 물리적인 구동을 전기적으로 제어하는 장치일 수 있다. 예를 들어, 구동 장 치는 전자 장치의 조향, 속도, 가속도, 브레이크, 기울기, 램프, 엔진, 드론의 상승/하강, 프로펠러 의 RPM, 로봇의 팔다리 등의 동작을 제어할 수 있다. 프로세서는 전원 관리부, 센서, 사용자 인터페이스, 메모리, 통신 장치, 및 구 동 장치와 전기적으로 연결되어 신호를 교환할 수 있다. 프로세서는 ASICs (application specific integrated circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적 유닛 중 적어도 하나를 이용하여 구현될 수 있다. 프로세서는 전원 관리부, 센서 , 사용자 인터페이스, 메모리, 통신 장치, 및 구동 장치 중 적어도 하나를 제어함으 로써, 전자 장치를 구동할 수 있다. 프로세서는 사용자의 행동을 학습하기 위해, 데이터 처리 회로 및 행동정책 학습 회로를 포 함할 수 있다. 데이터 처리 회로는 모방 학습에 대한 데이터 전처리를 수행할 수 있다. 데이터 처리 회로 는 입력 신호를 통해 사용자의 행동과 관련된 입력 데이터를 수신하고, 입력 데이터를 처리하여 전문 가의 행동 궤적 정보를 획득할 수 있다. 입력 데이터는 상태 데이터 및 제어 데이터를 포함할 수 있다. 상태 데이터는 센서에 의해 획득될 수 있다. 제어 데이터는 사용자가 전자 장치를 제어하기 위한 입력 신호에 포함된 데이터일 수 있다. 예 를 들어, 제어 데이터는 구동 장치를 제어하는 신호에 포함된 조향 제어 데이터, 가속도 제어 데이터, 기 울기 제어 데이터 등을 포함할 수 있다. 실시 예에 따라, 제어 데이터는 제어 값으로 표현될 수 있다. 실시 예에 따라, 제어 데이터는 사용자의 입력 신호를 기반으로 프로세서가 생성한 제어 데이터를 포 함할 수 있다. 예를 들어, 프로세서는 입력 신호를 기반으로 전원 관리부, 사용자 인터페이스, 메모리, 통신 장치, 및 구동 장치 중 적어도 하나를 제어하기 위한 제어 신호를 생성할 수 있고, 제어 데이터는 제어 신호에 포함될 수 있다. 데이터 처리 회로는 상태 데이터 및 제어 데이터를 매칭시킬 수 있다. 데이터 처리 회로는 상태 데이 터에 따른 사용자의 제어 데이터를 매칭시켜 상태 데이터 및 제어 데이터 쌍으로 구현된 행동 특징 벡터 (behavioral feature vector)를 생성할 수 있다. 행동 궤적 정보는 일련의 행동 특징 벡터에 대한 정보를 포함할 수 있다. 데이터 처리 회로는 행동 궤적 정보를 행동정책 학습 회로에 제공할 수 있다. 데이터 처리 회로(17 5)가 획득한 행동 궤적 정보는 전문가의 행동 궤적 정보로서, 제1 행동 궤적 정보로 표현될 수 있다. 제1 행동 궤적 정보는 행동정책 학습의 기초가 될 수 있다. 실시 예에 따라, 데이터 처리 회로는 행동정책 학습 회 로의 하위 구성으로 분류될 수 있다. 행동정책 학습 회로는 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성할 수 있다. 행동정책 학습 회로는 초기 행동 정책을 기반으로 학습자의 행동 궤적 정보를 획득할 수 있다. 행동정책 학습 회로 가 획득한 학습자의 행동 궤적 정보는 제2 행동 궤적 정보로 표현될 수 있다. 행동정책 학습 회로는 제1 행동 궤적 정보 및 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하고, 평가 모델을 기반으로 개선된 행동 정책을 도출할 수 있다. 이 경우, 평가 모델은 보상 함수로서 행동 정책을 학습하는데 사용될 수 있다. 행동정 책 학습 회로의 구체적인 구성 및 동작에 대한 설명은 도 3에서 후술된다. 상술된 바와 같이, 전자 장치는 전문가의 제1 행동 궤적과 학습자의 행동 정책을 통해 생성되는 제2 행동 궤적을 구분하는 평가 모델을 학습하고, 평가 모델을 보상함수로 사용하여 학습자의 행동 정책을 학습할 수 있 다. 전자 장치는 평가 모델 및 행동 정책을 학습하는 일련의 과정을 반복함으로써, 전문가의 행동과 유사 한 개선된 행동 정책을 도출할 수 있다. 도 3은 도 2의 전자 장치의 행동정책 학습 회로의 블록도이다. 도 2 및 도 3을 참조하면, 행동정책 학습 회로 는 제1 행동 궤적 관리부, 행동정책 생성부, 제2 행동 궤적 관리부, 레이블링 처리부 , 행동정책 평가부, 및 행동정책 학습부를 포함할 수 있다. 제1 행동 궤적 관리부는 데이터 처리 회로로부터 제1 행동 궤적 정보(HT)를 수신할 수 있다. 실시 예에 따라서, 제1 행동 궤적 관리부는 제1 행동 궤적 정보(HT)를 직접 생성할 수 있다. 제1 행동 궤적 관 리부는 제1 행동 궤적 정보(HT) 추적하여 제1 데이터 집합을 생성할 수 있다. 제1 행동 궤적 정보(HT)는 상태 데이터 및 제어 데이터의 쌍으로 구성된 행동 특징 벡터에 대한 정보를 포함할 수 있고, 상태 데이터 및 제어 데이터는 시간에 따라 변화될 수 있다. 제1 행동 궤적 관리부는 상태 데이 터 및 상태 데이터에 따른 제어 데이터를 추적하여 제1 행동 궤적 정보(HT)에 대한 제1 데이터 집합을 생성할 수 있다. 제1 행동 궤적 관리부는 제1 데이터 집합을 지정된 배치 사이즈로 샘플링할 수 있다. 예를 들어, 제1 데이 터 집합은 로봇이 팔을 움직일 때의 관절 각도에 대한 일련의 데이터일 수 있고, 지정된 배치 사이즈는 행렬 사 이즈로서 10x10 사이즈 또는 샘플레이트로서 1MHz 등과 같이 샘플 데이터에 대한 정규화 사이즈를 의미할 수 있 다. 제1 행동 궤적 관리부는 제1 데이터 집합을 샘플링하여 제1 샘플 데이터(DS1)를 생성할 수 있다. 제1 샘플 데이터(DS1)는 행동정책 생성부 및 레이블링 처리부에 제공될 수 있다 행동정책 생성부는 제1 샘플 데이터(DS1)를 기반으로 초기 행동 정책(HPi)을 생성할 수 있다. 행동 정책은 상태 데이터를 입력하면 전자 장치가 취해야 할 제어 데이터를 출력하는 함수 또는 전략일 수 있다. 즉, 초기 행동 정책(HPi)은 사용자의 시연 데이터를 통해 최초로 도출된 전자 장치의 행동 함수 또는 행동 전략일 수 있다. 행동정책 생성부는 제1 샘플 데이터(DS1)에 대해 지도 학습을 수행하여 초기 행동 정책(HPi)을 도출할 수 있다. 예를 들어, 지도 학습에는 회귀분석법이 이용될 수 있다. 행동정책 생성부는 초기 행동 정책(HPi)을 제2 행동 궤적 관리부에 제공할 수 있다. 제2 행동 궤적 관리부는 초기 행동 정책(HPi)을 기반으로 제2 행동 궤적 정보를 획득할 수 있다. 제2 행동 궤적 관리부는 초기 행동 정책(HPi)을 통해 상태 데이터에 대한 자율 제어 데이터를 획득할 수 있다. 자율 제어 데이터는 전자 장치가 상태 데이터를 기준으로 사용자의 개입 없이 자율적으로 전자 장치를 제어하기 위한 데이터일 수 있다. 제2 행동 궤적 정보는 상태 데이터 및 자율 제어 데이터의 쌍으로 구성될 수 있다. 상태 데이터 및 자율 제어 데이터는 시간에 따라 변화할 수 있다. 제2 행동 궤적 관리부는 상태 데이터 및 상태 데이터에 따른 자율 제어 데이터를 추적하여 제2 행동 궤적 정보에 대한 제2 데이터 집합을 생성할 수 있다.제2 행동 궤적 관리부는 제2 데이터 집합을 지정된 배치 사이즈로 샘플링하여 정규화할 수 있다. 제2 행동 궤적 관리부는 정규화된 제2 샘플 데이터(DS2)를 생성할 수 있다. 제2 샘플 데이터(DS2)는 레이블링 처리 부에 제공될 수 있다 제2 행동 궤적 관리부는 행동정책 생성부로부터 제공된 초기 행동 정책(HPi)을 기반으로 제2 행동 궤 적 정보를 획득한 후, 행동정책 학습부로부터 제공된 학습된 행동 정책(HPl)을 기반으로 제2 행동 궤적 정 보를 업데이트할 수 있다. 제2 행동 궤적 관리부는 업데이트된 제2 행동 궤적 정보를 기반으로 샘플링하여 레이블링 처리부에 제공하는 동작을 반복할 수 있다. 제2 행동 궤적 관리부는 학습된 행동 정책(HPl)의 성능이 기준을 충족하는지 여부를 판단할 수 있다. 예를 들어, 제2 행동 궤적 관리부는 학습된 행동 정책(HPl)을 기준으로 목표함수에 대한 로스(loss) 함수 값이 일정 값 이하인 경우, 또는 학습된 행동 정책(HPl)을 기준으로 테스트 결과 기준 값 이상의 성능이 출력되는 경 우, 최종 행동 정책(HPf)을 출력할 수 있다. 최종 행동 정책(HPf)은 메모리에 저장될 수 있고, 전자 장치 의 모방 동작 또는 추론 동작에서 사용될 수 있다. 레이블링 처리부는 제1 샘플 데이터(DS1) 및 제2 샘플 데이터(DS2)를 구분하는 레이블을 추가할 수 있다. 예를 들어, 레이블은 각 샘플 데이터가 어떤 행동 정책으로 도출되었는지 여부 및 작업의 성공 여부를 구분하는 레이블을 포함할 수 있다. 각 샘플 데이터가 어떤 행동 정책으로 도출되었는지 여부는 각 샘플 데이터가 제1 행동 궤적 정보로부터 직접 도출된 샘플 데이터인지 여부, 초기 행동 정책(HPi)로부터 도출된 샘플 데이터인지 여부, 및 학습된 행동 정책 (HPl)로부터 도출된 샘플 데이터인지 여부 중 적어도 하나를 포함할 수 있다. 각 샘플 데이터에 대한 작업의 성 공 여부는 목표하는 작업에 대한 성공 또는 실패 여부를 포함할 수 있다. 레이블링 처리부는 레이블이 추가된 제1 샘플 데이터(DS1) 및 제2 샘플 데이터(DS2)를 행동정책 평가부 에 제공할 수 있다. 레이블링 처리부는 행동 정책이 학습된 후에는 제1 샘플 데이터(DS1) 및 업데이 트된 제2 행동 궤적 정보로부터 샘플링된 샘플 데이터를 구분하는 레이블을 추가할 수 있다. 행동정책 평가부는 평가 모델을 학습할 수 있다. 평가 모델은 레이블을 기반으로 제1 샘플 데이터(DS1) 및 제2 샘플 데이터(DS2)를 평가할 수 있다. 여기서 평가는 레이블을 기반으로 샘플 데이터 또는 행동 궤적 정보가 어떤 정책에 의해 도출된 것인지 여부 및 작업의 성공 여부를 구분하는 것일 수 있다. 실시 예에 따라, 평가 모델은 레이블링된 각 샘플 데이터를 입력으로 전문가 또는 학습자의 행동 정책에 의한 것인지 여부, 작업의 성공 여부 등의 확률분포를 가진 값을 출력하는 신경망 모델로 정의될 수 있다. 행동정책 평가부는 레이블을 이용하여 지도 학습의 방법으로 샘플 데이터 또는 행동 궤적 정보를 구분하는 성능이 최대화되도록 평가 모델을 학습할 수 있다. 행동정책 평가부는 학습된 평가 모델을 행동정책 학습부 에 보상함수(CF)로서 제공할 수 있다. 행동정책 학습부는 평가 모델을 보상함수(CF)로 사용하여, 강화 학습을 기반으로 행동 정책을 학습할 수 있다. 보상함수(CF)는 학습중인 행동 정책과 전문가의 행동의 유사도를 측정하는 피드백 함수일 수 있다. 예를 들어, 행동 정책의 유사도는 동일한 상태 데이터에 따른 제어 데이터 및 자율 제어 데이터의 차이를 나타낼 수 있다. 행동정책 학습부는 작업 성공률을 최대화 시키면서 코스트 함수 값 또는 로스 함수 값이 최소화되도록 목 표함수를 설정하고, 이를 최적화 시키는 행동 정책을 학습할 수 있다. 실시 예에 따라, 학습에는 폴리시 그레이 디언트(Policy Gradient) 기반의 강화 학습 기법이 사용될 수 있다. 행동정책 학습부는 학습된 행동 정책(HPl)을 제2 행동궤적 관리부(1830에 제공할 수 있다. 학습된 행동 정 책(HPl)은 제2 행동 궤적 관리부가 제2 행동 궤적 정보를 업데이트하는데 활용될 수 있다. 이후, 제2 행동 궤적 관리부, 레이블링 처리부, 행동정책 평가부, 및 행동정책 학습부는 샘플링, 레이블링, 평가 모델 학습, 및 행동 정책 학습을 포함하는 일련의 학습 사이클을 반복할 수 있고, 이로 인해 행 동 정책의 성능이 개선될 수 있다. 즉, 전자 장치는 개선된 행동 정책을 기반으로 사용자의 행동과 유 사한 행동을 사용자의 개입 없이 모방할 수 있다. 상술된 바와 같이, 전자 장치는 임의의 인공 지능 모델(행동 정책 생성자)을 통해 행동 정책을 생성 및 학 습할 수 있고, 행동 정책을 통해 행동 궤적 정보를 생성할 수 있다. 또한, 전자 장치는 평가 모델(행동 정 책 판별자)을 통해 생성된 행동 정책 또는 행동 궤적 정보를 평가할 수 있다. 즉, 전자 장치는 행동 정책생성자 및 행동 정책 판별자의 반복적인 동작을 통해 행동 정책 또는 행동 궤적 정보에 대한 생성 및 평가를 반 복함으로써, 개선된 행동 정책을 도출할 수 있다. 이로써, 전자 장치는 생성적 적대 학습 기법을 구현할 수 있다. 도 4는 본 개시의 실시 예에 따른 전자 장치의 동작 방법을 나타내는 순서도이다. 도 2 및 도 4를 참조하면, 전 자 장치는 입력 데이터를 기반으로 행동 정책을 도출할 수 있다. 입력 데이터는 전문가의 시연 데이터로 표현될 수 있고, 행동 정책은 전문가의 개입 없이 전문가의 행동과 유사한 행동을 수행하기 위한 함수 또는 전 략을 의미한다. S110 단계에서, 전자 장치는 사용자의 행동과 관련된 입력 데이터를 수신할 수 있다. 입력 데이터는 전자 장치의 현재 상태에 대한 상태 데이터 및 사용자가 전자 장치를 제어하기 위해 입력하는 제어 데이터 를 포함할 수 있다. 상태 데이터는 센서를 통해 획득될 수 있고, 제어 데이터는 전원 관리부, 사용자 인터페이스, 메모리, 통신 장치, 및 구동 장치 중 적어도 하나로부터 획득될 수 있다. S120 단계에서, 전자 장치는 입력 데이터를 처리하여 제1 행동 궤적 정보를 획득할 수 있다. 입력 데이터 의 처리는 상태 데이터를 기반으로 제어 데이터를 매칭시키는 동작을 포함할 수 있다. 전자 장치는 입력 데이터를 처리하여 상태 데이터 및 제어 데이터의 쌍으로 구성된 제1 행동 궤적 정보를 생성할 수 있다. 제1 행 동 궤적 정보는 전문가의 행동 정책으로부터 도출된 전문간의 행동 궤적 정보로 표현될 수 있다. 또한, S120 단계에서, 전자 장치는 생성된 제1 행동 궤적 정보를 샘플링할 수 있다. 제1 행동 궤적 정보는 시간에 따라 변화하고, 전자 장치는 제1 행동 궤적 정보를 추적하여 제1 데이터 집합을 생성할 수 있다. 전자 장치는 제1 데이터 집합을 지정된 배치 사이즈로 샘플링하여 정규화된 제1 샘플 데이터를 생성할 수 있다. S130 단계에서, 전자 장치는 제1 행동 궤적 정보를 기반으로 초기 행동 정책을 생성할 수 있다. 초기 행동 정책은 전문가의 시연 데이터를 기반으로 최초로 도출된 행동 정책을 의미한다. 이 경우, 인공지능 모델이 사용 될 수 있다. 실시 예에 따라, 전자 장치는 제1 행동 궤적 정보에 대한 지도 학습을 통해 초기 행동 정책을 도출할 수 있다. S140 단계에서, 전자 장치는 초기 행동 정책을 기반으로 제2 행동 궤적 정보를 획득할 수 있다. 전자 장치 는 상태 데이터를 입력으로 초기 행동 정책을 통해 자율 제어 데이터를 도출할 수 있다. 자율 제어 데이터 는 사용자의 입력 신호 없이 전자 장치가 자율적으로 전자 장치 내의 유닛들을 제어하기 위한 데이터 일 수 있다. 전자 장치는 상태 데이터를 기반으로 자율 제어 데이터를 매칭시켜, 상태 데이터 및 자율 제 어 데이터의 쌍으로 구성된 제2 행동 궤적 정보를 생성할 수 있다. 또한, S140 단계에서, 전자 장치는 생성된 제2 행동 궤적 정보를 샘플링할 수 있다. 제2 행동 궤적 정보는 제1 행동 궤적 정보와 마찬가지로 시간에 따라 변화하고, 전자 장치는 제2 행동 궤적 정보를 추적하여 제2 데이터 집합을 생성할 수 있다. 전자 장치는 제2 데이터 집합을 지정된 배치 사이즈로 샘플링하여 정규화 된 제2 샘플 데이터를 생성할 수 있다. S150 단계에서, 전자 장치는 행동 정책에 대한 학습 및 업데이트를 수행할 수 있다. 실시 예에 따라, 행동 정책에 대한 학습에는 생성적 적대 학습 기법이 이용될 수 있다. 구체적으로, 전자 장치는 임의의 인공지 능 모델을 통해 행동 정책 또는 행동 궤적 정보를 생성하고, 평가 모델을 통해 생성된 행동 정책 또는 생성된 행정 궤적 정보를 평가할 수 있다. 또한, 전자 장치는 평가 모델을 보상함수로 이용하여 행동 정책을 학습 하고, 생성 및 평가 동작을 반복적으로 수행할 수 있다. S150 단계는 예를 들어, 도 3의 제2 행동 궤적 관리부, 레이블링 처리부, 행동정책 평가부, 및 행동정책 학습부를 통해 구현될 수 있다. 실시 예에 따라, S150 단계는 프로세서 외의 별도의 뉴럴 프로세서로 구현될 수 있다. 이하, S150 단계에 대한 상세한 설명은 도 5에서 후술된다. 도 5는 도 4의 S150 단계를 구체화한 순서도이다. 도 2, 도 4, 및 도 5를 참조하면, 전자 장치는 제1 행동 궤적 정보 및 제2 행동 궤적 정보를 구분하는 평가 모델을 학습하고, 평가 모델을 기반으로 행동 정책을 업데이 트할 수 있다. S151 단계에서, 전자 장치는 각 샘플 데이터에 대한 레이블링 처리를 수행하 수 있다. 전자 장치는 제1 샘플 데이터 및 제2 샘플 데이터에 대해 행동 정책의 출처 및 작업의 성공 여부를 구분하는 레이블을 추가 할 수 있다. 행동 정책의 출처는 제1 행동 궤적 정보에 대응되는 전문가 또는 제2 행동 궤적 정보에 대응되는학습자를 포함할 수 있다. 작업의 성공 여부는 제1 행동 궤적 정보 또는 제2 행동 궤적 정보를 기반으로 수행된 작업에 대한 성공 또는 실패를 포함할 수 있다. S152 단계에서, 전자 장치는 지도 학습을 이용하여 평가 모델을 학습할 수 있다. 평가 모델은 레이블이 추 가된 제1 샘플 데이터 및 제2 샘플 데이터를 기반으로 전문가/학습자 행동 정책 여부, 작업의 성공/실패 여부 등의 확률분포를 가진 값을 출력하는 인공지능 모델일 수 있다. S153 단계에서, 전자 장치는 학습된 평가 모델을 통해 행동 정책을 평가할 수 있다. 예를 들어, 전자 장치 는 전문가 행동 정책과 학습자 행동 정책의 유사도를 평가할 수 있다. 유사도는 동일한 상태 데이터에 대 한 제어 데이터 및 자율 제어 데이터의 차이를 나타낼 수 있고, 유사도가 높을수록 차이는 줄어들 수 있다. S154 단계에서, 전자 장치는 행동 정책의 성능이 기준을 충족하는지 여부를 판단할 수 있다. 예를 들어, 전자 장치는 행동 정책을 기준으로 목표함수에 대한 로스 함수 값이 일정 값 이하인지 여부, 또는 행동 정 책을 기준으로 테스트 결과 기준 값 이상의 성능이 출력되는지 여부를 판단할 수 있다. 행동 정책의 성능이 기 준을 충족하지 못하는 경우, S156 단계가 진행될 수 있다. S156 단계에서, 전자 장치는 행동 정책을 학습할 수 있다. 전자 장치는 S152 단계에서 학습된 평가 모델을 보상 함수로 이용하여 강화 학습을 수행할 수 있다. 예를 들어 전자 장치는 작업 성공률을 최대화 시키면서 코스트 함수 값 또는 로스 함수 값이 최소화되도록 목표함수를 설정하고, 이를 최적화 시키는 행동 정 책을 학습할 수 있다. S157 단계에서, 전자 장치는 학습된 행동 정책을 기반으로 제3 행동 궤적 정보를 획득할 수 있다. 제3 행 동 궤적 정보는 학습된 행동 정책을 기반으로 업데이트된 제2 행동 궤적 정보일 수 있다. 전자 장치는 제3 행동 궤적 정보를 샘플링하여 제3 샘플 데이터를 생성할 수 있다. 이후, S151 내지 S157 단계가 반복될 수 있다. 구체적으로, 전자 장치는 제1 샘플 데이터 및 제3 샘플 데 이터에 레이블을 추가하고, 레이블을 기반으로 평가 모델을 학습하고, 행동 정책을 평가하여 기준을 충족하지 못하는 경우 행동 정책을 재학습할 수 있다. 이러한 동작들은 S154 단계에서 행동 정책의 성능이 기준을 충족한 다고 판단될 때까지 반복될 수 있다. S154 단계에서, 전자 장치는 행동 정책의 성능이 기준을 충족한다고 판단하면, S155 단계로 진행할 수 있 다. S155 단계에서, 전자 장치는 기준을 충족한 행동 정책을 최종 행동 정책으로서 저장할 수 있다. 최종 행동 정책 반복 학습 동작들을 통해 학습된 행동 정책일 수 있다. 전자 장치는 최종 행동 정책을 메모리 에 저장할 수 있다. 상술된 내용은 본 개시를 실시하기 위한 구체적인 실시 예들이다. 본 개시는 상술된 실시 예들뿐만 아니라, 단 순하게 설계 변경되거나 용이하게 변경할 수 있는 실시 예들 또한 포함될 것이다. 또한, 본 개시는 실시 예들을 이용하여 용이하게 변형하여 실시할 수 있는 기술들도 포함될 것이다. 따라서, 본 개시의 범위는 상술된 실시 예들에 국한되어 정해져서는 안 되며 후술하는 특허청구범위뿐만 아니라 이 개시의 특허청구범위와 균등한 것들 에 의해 정해져야 할 것이다."}
{"patent_id": "10-2021-0051653", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 실시 예에 따른 전자 장치가 활용되는 환경을 나타내는 도면이다. 도 2는 본 개시의 실시 예에 따른 전자 장치의 블록도이다. 도 3은 도 2의 전자 장치의 행동 정책 학습 회로의 블록도이다. 도 4는 본 개시의 실시 예에 따른 전자 장치의 동작 방법을 나타내는 순서도이다.도 5는 도 4의 S150 단계를 구체화한 순서도이다."}
