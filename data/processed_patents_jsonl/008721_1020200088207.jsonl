{"patent_id": "10-2020-0088207", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0009682", "출원번호": "10-2020-0088207", "발명의 명칭": "분산 기계 학습 방법 및 시스템", "출원인": "한국전력공사", "발명자": "윤세영"}}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "중앙 서버가 랜덤 시드를 송신하는 동작;복수의 학습 머신이 상기 랜덤 시드를 수신하고, 수신된 랜덤 시드를 기반으로 랜덤 매트릭스를 생성하는 동작;상기 복수의 학습 머신이 자신에게 할당된 학습 데이터에 기반하여 로컬 그래디언트 파라미터를 생성하는 동작;상기 복수의 학습 머신이 상기 생성된 로컬 그래디언트 파라미터와 설정 파라미터를 상기 랜덤 매트릭스를 이용하여 압축하여 압축 정보를 생성하는 동작;상기 복수의 학습 머신이 상기 압축 정보를 상기 중앙 서버로 전송하는 동작;상기 중앙 서버가 상기 복수의 학습 머신으로부터 수신한 상기 압축 정보를 취합하여 평균 압축 그래디언트 파라미터를 생성하는 동작;상기 중앙 서버가 상기 평균 압축 그래디언트 파라미터를 상기 복수의 학습 머신으로 전송하는 동작;상기 복수의 학습 머신은 상기 수신한 평균 압축 그래디언트 파라미터를 상기 랜덤 매트릭스의 트랜스포즈를 이용하여 압축 해제하여 평균 그래디언트 파라미터를 획득하는 동작; 및상기 복수의 학습 머신은 상기 획득한 평균 그래디언트 파라미터에 기초하여 인공지능 모델의 파라미터를 갱신하는 동작을 포함하는 인공지능 신경망의 분산 기계 학습 방법."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 복수의 학습 머신이 상기 평균 그래디언트 파라미터를 이용하여 상기 설정 파라미터를 갱신하는 동작을 더포함하는 인공지능 신경망의 분산 기계 학습 방법."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1 항에 있어서,상기 복수의 학습 머신이 상기 압축 정보를 생성하는 동작은,상기 로컬 그래디언트 파라미터와 상기 설정 파라미터 차에 상기 랜덤 매트릭스를 곱하여 상기 압축 정보를 생성하는 동작을 포함하는 인공지능 신경망의 분산 기계 학습 방법."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 복수의 학습 머신이 상기 평균 그래디언트 파라미터를 획득하는 동작은,상기 평균 압축 그래디언트 파라미터와 상기 랜덤 매트릭스의 트랜스포즈를 곱한 값에 상기 설정 파라미터를 더하여 상기 평균 그래디언트 파라미터를 획득하는 동작을 포함하는 인공지능 신경망의 분산 기계 학습 방법."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "공개특허 10-2022-0009682-3-컴퓨터 판독 가능 저장 매체에 있어서,컴퓨터 상에서 실행될 때, 제 1 항 내지 제 4 항 중 어느 하나의 항에 따른 분산 기계 학습 방법을 수행하는 컴퓨터 프로그램을 포함하는 컴퓨터 판독가능 저장 매체."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "중앙 서버는 랜덤 시드를 송신하고,복수의 학습 머신은 상기 랜덤 시드를 수신하고, 수신된 랜덤 시드를 기반으로 랜덤 매트릭스를 생성하고,상기 복수의 학습 머신은 자신에게 할당된 학습 데이터에 기반하여 로컬 그래디언트 파라미터를 생성하고,상기 복수의 학습 머신은 상기 생성된 로컬 그래디언트 파라미터와 설정 파라미터를 상기 랜덤 매트릭스를 이용하여 압축하여 압축 정보를 생성하고,상기 복수의 학습 머신은 상기 압축 정보를 상기 중앙 서버로 전송하고,상기 중앙 서버는 상기 복수의 학습 머신으로부터 수신한 상기 압축 정보를 취합하여 평균 압축 그래디언트 파라미터를 생성하고,상기 중앙 서버는 상기 평균 압축 그래디언트 파라미터를 상기 복수의 학습 머신으로 전송하고,상기 복수의 학습 머신은 상기 수신한 평균 압축 그래디언트 파라미터를 상기 랜덤 매트릭스의 트랜스포즈를 이용하여 압축 해제하여 평균 그래디언트 파라미터를 획득하고,상기 복수의 학습 머신은 상기 획득한 평균 그래디언트 파라미터에 기초하여 인공지능 모델의 파라미터를 갱신하는 인공지능 신경망의 분산 기계 학습 시스템."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6 항에 있어서, 상기 복수의 학습 머신은,상기 평균 그래디언트 파라미터를 이용하여 상기 설정 파라미터를 갱신하는 인공지능 신경망의 분산 기계 학습시스템."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 6 항에 있어서, 상기 복수의 학습 머신은,상기 로컬 그래디언트 파라미터와 상기 설정 파라미터 차에 상기 랜덤 매트릭스를 곱하여 상기 압축 정보를 생성하는 인공지능 신경망의 분산 기계 학습 시스템."}
{"patent_id": "10-2020-0088207", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 6 항에 있어서, 상기 복수의 학습 머신은,상기 평균 압축 그래디언트 파라미터와 상기 랜덤 매트릭스의 트랜스포즈를 곱한 값에 상기 설정 파라미터를 더하여 상기 평균 그래디언트 파라미터를 획득하는 인공지능 신경망의 분산 기계 학습 시스템."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은, 중앙 서버가 랜덤 시드를 송신하는 동작; 복수의 학습 머신이 상기 랜덤 시드를 수신하고, 수신된 랜 덤 시드를 기반으로 랜덤 매트릭스를 생성하는 동작; 상기 복수의 학습 머신이 자신에게 할당된 학습 데이터에 기반하여 로컬 그래디언트 파라미터를 생성하는 동작; 상기 복수의 학습 머신이 상기 생성된 로컬 그래디언트 파 (뒷면에 계속)"}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "다양한 실시 예는 분산 기계 학습 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능(artificial intelligence)은 컴퓨팅 장치에서 사람의 학습 능력과 추론능력, 지각능력, 자연언어의 이 해능력, 등과 같은 지능적인 동작을 수행하는 기술로 정의될 수 있다. 컴퓨팅 장치의 성능 및 컴퓨팅 장치에 이용되는 어플리케이션 개발의 수준이 점차 향상됨에 따라, 컴퓨팅 장치 에서 구현되는 인공지능의 수준도 점차 향상되고 있다. 최근 인공지능은 사람의 두뇌와 유사하게 구현된 뉴럴 네트워크(neural network)와, 그 구조를 이용하여 강화 학습을 수행하는 기법(예를 들어, 딥러닝 학습 기법)을 사용하여 구현되고 있다. 인공지능의 성능은 학습을 통해 향상될 수 있는데, 인공지능의 학습을 위해서는 다양한 학습 데이터가 요구된다. 한편, 대용량 데이터를 이용한 딥러닝 모델 개발 시에는 학습에 많은 시간이 소요된다는 한계가 있었다. 이를 해결하고자, 최근에는 분산 기계 학습이 주목받고 있다. 딥러닝 학습 과정의 분산처리 시에는, 데이터 병렬처리(data parallelism) 기법과 모델 병렬처리(model parallelism) 기법이 이용된다. 데이터 병렬처리 기법은 학습해야 하는 입력 데이터 셋(set)을 다수의 컴퓨터가 나누어 학습을 수행하는 방식을 의미하고, 모델 병렬처리 기법은 딥러닝 모델을 나누어 다수의 컴퓨터가 분할된 딥러닝 모델들에 대한 학습을 각각 수행하는 방식을 의미한다. 특히, 데이터 병렬처리 기법은 전체 학습 데이터를 대상으로 복수의 학습 머신(worker machine)들에서 각각 일 부 학습 데이터를 이용하여 학습을 진행하는 방식이므로, 네트워크에 연결된 외부 저장장치에 데이터를 저장하 고, 네트워크를 통해 학습을 수행해야만 한다. 이 경우, 학습이 진행될 때마다 학습 머신들 각각은 외부 저장장치에 빈번하게 접근하여 학습을 진행하여야 하 므로, 시스템의 성능이나 용량이 저하되는 병목현상으로 인해 학습 속도가 현저하게 떨어지는 문제점이 발생하 게 된다. 또한, 초대용량 데이터의 경우, 특정한 분산 환경에서는 전체 데이터를 학습할 수 없으므로, 학습의 효율성이 감소하게 되는 한계가 존재하게 된다."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기한 문제점들을 해결하고자 안출된 것으로서, 본 발명의 목적은 분산 기계 학습에서 요구되는 스 토리지와 분산을 감소시켜 보다 효율적인 학습을 수행하는 분산 기계 학습 방법 및 시스템을 제공하는 것이다. 본 문서에서 이루고자 하는 기술적 과제는 이상에서 언급한 기술적 과제로 제한되지 않으며, 언급되지 않은 또 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 다양한 실시 예들에 따르면 인공지능 신경망의 분산 기계 학습 방법은, 중앙 서버가 랜덤 시드를 송 신하는 동작; 복수의 학습 머신이 상기 랜덤 시드를 수신하고, 수신된 랜덤 시드를 기반으로 랜덤 매트릭스를 생성하는 동작; 상기 복수의 학습 머신이 자신에게 할당된 학습 데이터에 기반하여 로컬 그래디언트 파라미터를 생성하는 동작; 상기 복수의 학습 머신이 상기 생성된 로컬 그래디언트 파라미터와 설정 파라미터를 상기 랜덤 매트릭스를 이용하여 압축하여 압축 정보를 생성하는 동작; 상기 복수의 학습 머신이 상기 압축 정보를 상기 중 앙 서버로 전송하는 동작; 상기 중앙 서버가 상기 복수의 학습 머신으로부터 수신한 상기 압축 정보를 취합하여 평균 압축 그래디언트 파라미터를 생성하는 동작; 상기 중앙 서버가 상기 평균 압축 그래디언트 파라미터를 상 기 복수의 학습 머신으로 전송하는 동작; 상기 복수의 학습 머신은 상기 수신한 평균 압축 그래디언트 파라미터를 상기 랜덤 매트릭스의 트랜스포즈를 이용하여 압축 해제하여 평균 그래디언트 파라미터를 획득하는 동작; 및 상기 복수의 학습 머신은 상기 획득한 평균 그래디언트 파라미터에 기초하여 인공지능 모델의 파라미터를 갱신 하는 동작을 포함한다. 또한, 상기 복수의 학습 머신이 상기 평균 그래디언트 파라미터를 이용하여 상기 설정 파라미터를 갱신하는 동 작을 더 포함한다. 또한, 상기 복수의 학습 머신이 상기 압축 정보를 생성하는 동작은, 상기 로컬 그래디언트 파라미터와 상기 설 정 파라미터 차에 상기 랜덤 매트릭스를 곱하여 상기 압축 정보를 생성하는 동작을 포함한다. 또한, 상기 복수의 학습 머신이 상기 평균 그래디언트 파라미터를 획득하는 동작은, 상기 평균 압축 그래디언트 파라미터와 상기 랜덤 매트릭스의 트랜스포즈를 곱한 값에 상기 설정 파라미터를 더하여 상기 평균 그래디언트 파라미터를 획득하는 동작을 포함한다. 추가로, 상기의 기술적 과제를 해결하기 위한 본 발명의 다른 실시 예에 따른 컴퓨터 판독 가능 저장 매체는, 컴퓨터 상에서 실행될 때, 상기의 분산 기계 학습 방법을 수행하기 위한 프로그램이 기록될 수 있다. 추가로, 상기의 기술적 과제를 해결하기 위한 본 발명의 또 다른 실시 예에 따른 분산 기계 학습 시스템은, 중 앙 서버는 랜덤 시드를 송신하고, 복수의 학습 머신은 상기 랜덤 시드를 수신하고, 수신된 랜덤 시드를 기반으 로 랜덤 매트릭스를 생성하고, 상기 복수의 학습 머신은 자신에게 할당된 학습 데이터에 기반하여 로컬 그래디 언트 파라미터를 생성하고, 상기 복수의 학습 머신은 상기 생성된 로컬 그래디언트 파라미터와 설정 파라미터를 상기 랜덤 매트릭스를 이용하여 압축하여 압축 정보를 생성하고, 상기 복수의 학습 머신은 상기 압축 정보를 상 기 중앙 서버로 전송하고, 상기 중앙 서버는 상기 복수의 학습 머신으로부터 수신한 상기 압축 정보를 취합하여 평균 압축 그래디언트 파라미터를 생성하고, 상기 중앙 서버는 상기 평균 압축 그래디언트 파라미터를 상기 복 수의 학습 머신으로 전송하고, 상기 복수의 학습 머신은 상기 수신한 평균 압축 그래디언트 파라미터를 상기 랜 덤 매트릭스의 트랜스포즈를 이용하여 압축 해제하여 평균 그래디언트 파라미터를 획득하고, 상기 복수의 학습 머신은 상기 획득한 평균 그래디언트 파라미터에 기초하여 인공지능 모델의 파라미터를 갱신한다. 또한, 상기 복수의 학습 머신은 상기 평균 그래디언트 파라미터를 이용하여 상기 설정 파라미터를 갱신한다. 또한, 상기 복수의 학습 머신은 상기 로컬 그래디언트 파라미터와 상기 설정 파라미터 차에 상기 랜덤 매트릭스 를 곱하여 상기 압축 정보를 생성한다. 또한, 상기 복수의 학습 머신은 상기 평균 압축 그래디언트 파라미터와 상기 랜덤 매트릭스의 트랜스포즈를 곱 한 값에 상기 설정 파라미터를 더하여 상기 평균 그래디언트 파라미터를 획득한다."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 초대용량 학습 데이터의 학습 시간을 단축시키고, 네트워크 통신량을 감소시켜 통신비용을 절감할 수 있는 효과가 있다. 또한, 본 발명은 분산 기계 학습에서 요구되는 스토리지와 분산을 감소시켜 학습의 성능을 향상시킬 수 있는 효 과가 있다. 본 개시에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들은 아래의 기재로부터 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하 다양한 실시 예들이 첨부된 도면을 참고하여 상세히 설명된다. 도면 부호에 관계없이 동일하거나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략할 수 있다. 이하의 설명에서 사용되는 구성요소에 대한 접미사 '모듈' 또는 '부'는 명세서 작성의 용이함만이 고려되어 부 여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, '모듈' 또는 '부'는 소프트웨어 또는 FPGA(field programmable gate array) 또는 ASIC(application specific integrated circuit)과 같은 하드웨어 구성요소를 의미하나, 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '부' 또 는 '모듈'은 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생 시키도록 구성될 수도 있다. 따라서, 일 예로서 '부' 또는 '모듈'은 소프트웨어 구성요소들, 객체지향 소프트 웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드의 세그먼트들, 드라이버들, 펌웨어, 마이크로코드, 회로, 데이터, 데이 터베이스, 데이터 구조들, 테이블들, 어레이들, 및 변수들을 포함할 수 있다. 하나의 구성요소, '부' 또는 '모 듈'들 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '부' 또는 '모듈'들로 결합되거나 추가적인 구성요 소들과 '부' 또는 '모듈'들로 더 분리될 수 있다. 본 발명의 몇몇 실시 예들과 관련하여 설명되는 방법 또는 알고리즘의 단계는 프로세서에 의해 실행되는 하드웨 어, 소프트웨어 모듈, 또는 그 2 개의 결합으로 직접 구현될 수 있다. 소프트웨어 모듈은 RAM 메모리, 플래시 메모리, ROM 메모리, EPROM 메모리, EEPROM 메모리, 레지스터, 하드 디스크, 착탈형 디스크, CD-ROM, 또는 당업 계에 알려진 임의의 다른 형태의 기록 매체에 상주할 수도 있다. 예시적인 기록 매체는 프로세서에 커플링되며, 그 프로세서는 기록 매체로부터 정보를 판독할 수 있고 저장 매체에 정보를 기입할 수 있다. 다른 방법으로, 기 록 매체는 프로세서와 일체형일 수도 있다. 프로세서 및 기록 매체는 주문형 집적회로(ASIC) 내에 상주할 수도 있다. ASIC은 사용자 단말기 내에 상주할 수도 있다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 ‘연결되어’ 있다거나 ‘접속되어’ 있다고 언급된 때에는, 그 다른 구성요 소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다 고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 ‘직접 연결되어’ 있다거나 ‘직접 접속 되어’ 있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 우선 본 명세서에서 사용되는 용어들에 대하여 간략히 설명한다. 인공지능은 인간의 지능을 갖춘 컴퓨터 시스템 또는 장치이며, 인간의 지능을 기계 등에 인공적으로 구현한 것 을 의미할 수 있다. 인공 지능은 또한 지능을 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 과학 분야를 지칭하기도 한다. 인공신경망은 인공지능을 구현하는 모델 또는 알고리즘으로써, 기계학습에서 생물학의 신경망을 모사하여 모델 링한 통계학적 학습 알고리즘으로, 시냅스의 결합으로 네트워크를 형성한 인공 뉴런이 학습을 통해 시냅스의 결 합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 또는 학습 알고리즘이라 할 수 있다. 인공신경망(artificial neural network)은 입력 층, 출력 층 그리고 하나 이상의 은닉 층을 포함할 수 있다. 인공신경망의 각 층은 신경망의 뉴런에 대응하는 복수의 노드를 포함하고, 인공신경망의 한 층의 노드와 다른 층의 노드 간은 시냅스로 연결될 수 있다. 일 실시 예로 각 층의 모든 노드와 다음 층의 모든 노드가 시냅스로 연결된 인공신경망을 완전 연결된 인공신경망이라 칭할 수 있다. 인공신경망에서 각 노드는 시냅스를 통해 입력되는 입력 신호들을 받고 각 입력 신호들에 대한 가중치 및 편향 에 대한 활성 함수에 기초하여 출력 값을 생성할 수 있다. 심층신경망(deep neural network)은 입력층과 출력층 사이에 복수의 은닉층을 포함하는 인공신경망을 통칭할 수 있다. 심층신경망은 복잡한 비선형 관계들을 모델링할 수 있으며, 그 목적에 따라 다양한 구조를 가질 수 있다.예를 들면, 심층신경망 구조로, 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network, RNN), LSTM(long short term memory) 등이 있을 수 있다. 합성곱 신경망은 이미지, 동영상, 문자열과 같은 구조적 공간 데이터의 특징을 식별하여 학습함으로서 이미지, 동영상을 분류하고 식별하는데 효과적일 수 있다. 순환신경망은 내부에 순환 구조가 들어 있어 과거 시간의 학 습이 가중치와 곱해져 현재 학습에 반영될 수 있는 구조이며, 현재의 출력 결과는 과거 시간에서의 출력 결과에 영향을 받으며, 은닉 층은 일종의 메모리 기능을 수행한다. 따라서, 순차적인 데이터를 학습하여 분류 또는 예 측을 수행하는 데 효과적일 수 있다. 대용량의 데이터를 이용해 데이터의 특성과 패턴을 학습하고, 결과값을 예측하는 방법으로서, 경사하강 (gradient descent) 알고리즘과 역전파(back propagation) 알고리즘이 있다. 경사하강 알고리즘은 1차 근사값 발견용 최적화 알고리즘으로서, 함수의 기울기(gradient)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 알고리즘이고, 역전파 알고리즘은 다층 퍼셉트론 학 습에 사용되는 통계적 기법을 의미하는 것으로서, 신경망이 계산한 답과 정답 사이의 오차 정보를 획득하고, 그 오차를 줄이도록 피드백을 주어 신경망을 구성하는 개개의 가중치(weight) 또는 파라미터를 조정하는 알고리즘 일 수 있다. 이때 역전파 알고리즘에서 사용하는 오차 정보는 경사하강 알고리즘에 의하여 획득할 수 있다. 이하에서는 경사하강 알고리즘을 이용해 대용량 데이터를 학습하여 인공신경망 파라미터를 갱신하는 방법에 대 해 보다 구체적으로 상술하기로 한다. 도 1은 본 발명의 일 실시 예에 따라 대용량 학습 데이터를 데이터 병렬 처리에 기반하여 분산 학습하기 위한 시스템의 개괄도를 도시한다. 도 1을 참조하면, 대용량 학습 데이터를 분산 학습하기 위한 시스템은, 중앙 서버(server, 100)와 복수의 학습 머신(worker machine, 200)으로 구성될 수 있다. 중앙 서버는 복수의 학습 머신들이 공유하는 서버 또는 이와 유사한 장치일 수 있다. 복수의 학습 머 신은 중앙 서버와 네트워크를 통해 통신 가능하게 연결되고, 동일한 인공지능 신경망 모델을 구비하 여 분산 학습을 수행 및 처리하는 전자 장치일 수 있다. 한편, 도 1에서, 중앙 서버와 학습 머신은 별도의 구성으로 도시하였으나, 이는 예시적인 것에 불과 하고, 중앙 서버는 복수의 학습 머신 각각의 일 구성에 포함될 수 있다. 복수의 학습 머신은 중앙 서버와의 데이터 송수신을 위한 통신부 및 프로세서로 구성될 수 있다. 예를 들어, 복수의 학습 머신은 각각에 중복되지 않도록 할당된 학습 데이터를 기초로 학습된 로컬 그래디 언트 파라미터(local gradient parameter)를 생성하고, 생성된 로컬 그래디언트 파라미터를 압축하여 중앙 서버 로 전달할 수 있다. 중앙 서버는 복수의 학습 머신들로부터 전달된 각각의 로컬 그래디언트 파라미터를 취합하여 평균화 하고, 평균화된 정보를 복수의 학습 머신이 서로 공유하도록 송신할 수 있다. 즉, 복수의 학습 머신 은 중앙 서버로부터 전달된 정보를 이용하여 각각이 가지고 있는 학습시키고자하는 인공지능 신경망 모델 의 파라미터를 업데이트할 수 있다. 본 명세서에서 제시하는 분산 학습 시스템은 분산 학습 환경에서 복수의 학습 머신 각각이 전체 학 습 데이터를 사용하지 않고, 일부 학습 데이터만을 사용함으로써 야기되는 분산을 설정 파라미터를 이용하여 줄 이고, 복수의 학습 머신이 중앙 서버로 그래디언트 파라미터를 압축하여 전송함으로써, 전송되는 데 이터 량을 줄이고, 복수의 학습 머신으로부터 오는 설정 파라미터를 취합하는 중앙 서버의 스토리지 (storage)를 감소시킬 수 있다. 복수의 학습 머신은 휴대폰, 스마트폰(smart phone), 노트북 컴퓨터(laptop computer), 디지털방송용 단 말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 슬레이트 PC(slate PC), 태블릿 PC(tablet PC), 울트라북(ultrabook), 웨어러블 디바이스(wearable device, 예를 들어, 워치형 단 말기(smart watch), 글래스형 단말기(smart glass), HMD(head mounted display)) 등을 포함하는 정보통신기기 와 멀티미디어기기 및 그에 대한 응용 기기일 수 있음은 자명할 것이다. 즉, 복수의 학습 머신들은 대용량 학습 데이터를 보다 효율적으로 학습할 수 있는 전자 장치일 수 있다. 한편, 도 1에 도시되는 구성요소(100, 200)는 대용량 학습 데이터를 분산 학습하기 위한 시스템을 구성하 는 구성요소의 일 예에 불과하며, 따라서 본 명세서에서 기술되는 분산 학습을 구현하기 위해서 추가의 구성요 소가 부가될 수 있음은 명백할 것이다. 도 2는 본 발명의 일 실시 예에 따른 분산 기계 학습에서 인공 신경망 모델의 파라미터를 갱신하는 방법의 흐름 도이다. 이하에서 도 2의 적어도 일부 동작은 도 3 내지 도 5를 참조하여 설명한다. 도 3 내지 도 5는 본 발명 의 일 실시 예에 따른 분산 기계 학습에서 파라미터를 갱신하는 과정을 설명하는 예시도이다. 한편, 도 2에서 각 동작들은 순차적으로 수행될 수 있으나, 반드시 순차적으로 수행되는 것은 아니다. 예를 들 어, 각 동작들의 순서가 변경될 수도 있으며, 적어도 두 동작들이 병렬적으로 수행될 수도 있다. 도 2를 참조하면, 중앙 서버는 복수의 학습 머신들 각각에 동일한 랜덤 시드(random seed)를 제공할 수 있다(S210). 복수의 학습 머신은 S210 동작을 통해 수신된 랜덤 시드를 기반으로 랜덤 매트릭스(R)를 생성할 수 있다 (S220). 복수의 학습 머신은 동일한 난수열과 동일 랜덤 시드를 기반으로 동일한 랜덤 매트릭스(R)를 생성 할 수 있다. 복수의 학습 머신은 각각에 할당된 학습 데이터를 기반으로 로컬 그래디언트 파라미터( )를 생성할 수 있 다(S231). 복수의 학습 머신은 랜덤 매트릭스(R)을 이용하여 로컬 그래디언트 파라미터( )를 압축하여 데이터 크기를 줄일 수 있다. 복수의 학습 머신은 각각에 구비된 동일한 인공지능 신경망 모델을 이용해 각각에 할당된 학습 데이터를 학습하여 로컬 그래디언트 파라미터( )를 생성할 수 있다. 이 경우, 복수의 학습 머신에는 학습 데이터가 서로 중복되지 않도록 각각 할당될 수 있다. 도 2 및 도 3을 참조하면, 복수의 학습 머신은 S220 동작을 통해 생성된 랜덤 매트릭스(R)를 이용하여 로 컬 그래디언트 파라미터( )와 설정 파라미터( ) 정보를 압축한 압축 정보를 생성할 수 있다(S232). 예를 들어, 압축 정보는 중앙 서버에서 복수의 학습 머신 각각의 파라미터 정보를 취합하더라도, 그 정보의 크기 또는 양이 증대되지 않도록 데이터의 양을 감소시킨 정보를 의미할 수 있다. 즉, 복수의 학습 머신 각각은 압축 정보를 생성함으로써, 분산 학습에서 요구되는 스토리지를 감소시킬 수 있다. 압축 정보는 다음 수학식 1에 의해 생성될 수 있다. 수학식 1"}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1 및 도 3을 참조하면, 복수의 학습 머신은 할당된 학습 데이터에 기초하여 얻어진 로컬 그래디언 트 파라미터( )와 설정 파라미터( )의 차이에 랜덤 매트릭스(R)를 곱하여 계산된 함수의 출력 값인 압축 정보 를 획득할 수 있다. 일 실시 예에서, 학습 머신은 모두 동일한 설정 파라미터( )를 이용하거나, 또는 각각 상이한 설정 파라미 터( )를 이용할 수 있다. 설정 파라미터( )는 분산 학습에서의 분산을 감소시키기 위해 이용되는 임의의 설정 파라미터를 의미할 수 있다. 복수의 학습 머신은 S232 동작을 통해 획득한 압축 정보를 중앙 서버로 전송할 수 있다(S233). 이 경 우, 복수의 학습 머신들로부터 전달된 압축 정보들은 중앙 서버에서 수용 가능한 크기를 가질 수 있 다. 중앙 서버는 S233 동작을 통해 수신된 압축 정보를 취합하여 평균 압축 그래디언트 파라미터( )를 생성할 수 있다(S234).평균 압축 그래디언트 파라미터( )는 복수 개(예를 들어, n개)의 학습 머신으로부터 전달된 압축 정보의 평균값을 의미하며, 평균 압축 그래디언트 파라미터( )는 일반적으로 이용되는 평균값 수학식을 통해 산출될 수 있다. 중앙 서버는 다음 수학식 2에 따라 평균 압축 그래디언트 파라미터( )를 계산할 수 있다. 수학식 2"}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 2를 참조하면, 중앙 서버는 복수의 학습 머신으로부터 수신된 복수의 압축 정보를 학습 머신 의 총 개수로 나누어 평균 압축 그래디언트 파라미터( )를 산출할 수 있다. 중앙 서버는 S234 동작을 통해 생성된 평균 압축 그래디언트 파라미터( )를 복수의 학습 머신으로 전송할 수 있다(S235). 즉, 복수의 학습 머신은 동일한 평균 압축 그래디언트 파라미터( )를 공유할 수 있다. 복수의 학습 머신은 S235 동작을 통해 수신한 평균 압축 그래디언트 파라미터( )를 압축 해제하여(S236), 평균 그래디언트 파라미터( )를 획득할 수 있다(S237). 복수의 학습 머신은 랜덤 매트릭스(R)의 트랜스포즈(transpose, )를 이용하여 평균 압축 그래디언트 파 라미터( )를 압축 해제시킴으로써, 이전 크기를 가진 평균 그래디언트 파라미터( )를 획득할 수 있다. 랜덤 매트릭스(R)의 트랜스포즈(transpose, )는 랜덤 매트릭스(R)의 전치행렬로서, 랜덤 매트릭스(R)의 행과 열을 바꾼 행렬을 의미할 수 있다. 평균 그래디언트 파라미터( )는 다음 수학식 3에 의해 산출될 수 있다. 수학식 3"}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "수학식 3을 참조하면, 복수의 학습 머신은 평균 압축 그래디언트( )에 랜덤 매트릭스(R)의 트랜스포즈 ( )를 곱하여 압축 해제하고, 설정 파라미터( )를 더하여 평균 그래디언트 파라미터( )를 획득할 수 있다. 여 기서, 랜덤 매트릭스(R)과 랜덤 매트릭스의 트랜스포즈( )의 곱은 단위행렬(identity matrix)로서 정의될 수 있다. 복수의 학습 머신은 S237 동작을 통해 획득한 평균 그래디언트 파라미터( )에 기초하여 인공지능 모델의 파라미터를 갱신할 수 있다(S238). 예를 들어, 복수의 학습 머신은 분산 학습 수행에 따라 각각의 인공지능 모델의 파라미터를 갱신할 수 있 으며, 갱신된 파라미터는 아래의 수학식 4에 의해 정의될 수 있다. 수학식 4 수학식 4를 참조하면, 는 훈련 레이트(learning rate)를 의미하고, 바람직하게는 0.01 내지 0.02 값으로 설정 될 수 있다. 일 실시 예에서, 복수의 학습 머신은 이동 평균법(moving average)을 이용해 설정 파라미터( )를 업데이트 할 수 있다. 이동 평균법은 평균을 구할 수량을 설정하여 시간이 경과함에 따라 과거 데이터를 제외하고, 신규 데이터를 추가하여 평균화하는 방법으로서, 갑작스러운 데이터의 변동을 방지할 수 있다. 즉, 복수의 학습 머신은 인공지능 모델의 파라미터가 업데이트된 이후에 평균 그래디언트 파라미터( )를 기초로 이동 평균법을 이용해 설정 파라미터( )를 업데이트할 수 있다. 이동 평균법을 이용하여 갱신된 설정 파 라미터( )는 다음 수학식 5에 의해 계산될 수 있다. 수학식 5"}
{"patent_id": "10-2020-0088207", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 5를 참조하면, 는 압축률을 의미하고, 랜덤 매트릭스(R)의 열의 개수를 행의 개수로 나눈 값으로 정의 될 수 있다. 일 실시 예에서, S231 동작 내지 S238 동작은 인공지능 모델의 학습이 진행될 때마다 반복하여 수행될 수 있다. 본 발명에서는 상술한 분산 기계 학습에서의 파라미터 업데이트 방법 및 시스템을 이용하여 보다 정확한 전력 수요량을 예측할 수 있으며, 이를 통해 보다 안정적이고 효율적인 전력 제공 전략을 수립할 수 있다. 상술한 바와 같이, 본 발명은 초대용량 학습 데이터의 학습 시간을 단축시키고, 네트워크 통신량을 감소시켜 통 신비용을 절감할 수 있는 효과가 있다. 또한, 본 발명은 분산 기계 학습에서 요구되는 스토리지와 분산을 감소시켜 학습의 성능을 향상시킬 수 있는 효 과가 있다. 한편, 본 명세서에 기재된 다양한 실시예들은 하드웨어, 미들웨어, 마이크로코드, 소프트웨어 및/또는 이들의 조합에 의해 구현될 수 있다. 예를 들어, 다양한 실시예들은 하나 이상의 주문형 반도체(ASIC)들, 디지털 신호 프로세서(DSP)들, 디지털 신호 프로세싱 디바이스(DSPD)들, 프로그램어블 논리 디바이스(PLD)들, 필드 프로그램 어블 게이트 어레이(FPGA)들, 프로세서들, 컨트롤러들, 마이크로컨트롤러들, 마이크로프로세서들, 여기서 제시 되는 기능들을 수행하도록 설계되는 다른 전자 유닛들 또는 이들의 조합 내에서 구현될 수 있다. 또한, 예를 들어, 다양한 실시예들은 명령들을 포함하는 컴퓨터-판독가능한 매체에 수록되거나 인코딩될 수 있 다. 컴퓨터-판독가능한 매체에 수록 또는 인코딩된 명령들은 프로그램 가능한 프로세서 또는 다른 프로세서로 하여금 예컨대, 명령들이 실행될 때 방법을 수행하게끔 할 수 있다. 저장 매체는 컴퓨터에 의해 액세스될 수 있는 임의의 가용 매체일 수도 있다. 예를 들어, 이러한 컴퓨터-판독가능한 매체는 RAM, ROM, EEPROM, CD-ROM 또는 기타 광학 디스크 저장 매체, 자기 디스크 저장 매체 또는 기타 자기 저장 디바이스, 또는 원하는 프로그 램 코드를 컴퓨터에 의해 액세스가능한 명령들 또는 데이터 구조들의 형태로 저장하는데 이용될 수 있는 임의의 다른 매체를 포함할 수 있다. 이러한 하드웨어, 소프트웨어, 펌웨어 등은 본 명세서에 기술된 다양한 동작들 및 기능들을 지원하도록 동일한 디바이스 내에서 또는 개별 디바이스들 내에서 구현될 수 있다. 추가적으로, 본 발명에서 \"~부\"로 기재된 구성 요소들, 유닛들, 모듈들, 컴포넌트들 등은 함께 또는 개별적이지만 상호 운용가능한 로직 디바이스들로서 개별 적으로 구현될 수 있다. 모듈들, 유닛들 등에 대한 서로 다른 특징들의 묘사는 서로 다른 기능적 실시예들을 강조하기 위해 의도된 것이며, 이들이 개별 하드웨어 또는 소프트웨어 컴포넌트들에 의해 실현되어야만 함을 필 수적으로 의미하지 않는다. 오히려, 하나 이상의 모듈들 또는 유닛들과 관련된 기능은 개별 하드웨어 또는 소프트웨어 컴포넌트들에 의해 수행되거나 또는 공통의 또는 개별의 하드웨어 또는 소프트웨어 컴포넌트들 내에 통합될 수 있다. 특정한 순서로 동작들이 도면에 도시되어 있지만, 이러한 동작들이 원하는 결과를 달성하기 위해 도시된 특정한 순서, 또는 순차적인 순서로 수행되거나, 또는 모든 도시된 동작이 수행되어야 할 필요가 있는 것으로 이해되지 말아야 한다. 임의의 환경에서는, 멀티태스킹 및 병렬 프로세싱이 유리할 수 있다. 더욱이, 상술한 실시예에 서 다양한 구성요소들의 구분은 모든 실시예에서 이러한 구분을 필요로 하는 것으로 이해되어서는 안되며, 기술 된 구성요소들이 일반적으로 단일 소프트웨어 제품으로 함께 통합되거나 다수의 소프트웨어 제품으로 패키징될 수 있다는 것이 이해되어야 한다. 이상에서와 같이 도면과 명세서에서 최적 실시예가 개시되었다. 여기서 특정한 용어들이 사용되었으나, 이는 단지 본 발명을 설명하기 위한 목적에서 사용된 것이지 의미한정이나 특허청구범위에 기재된 본 발명의 범위를 제한하기 위하여 사용된 것은 아니다. 그러므로, 본 기술 분야의 통상의 지식을 가진 자라면 이로부터 다양한 변형 및 균등한 타 실시예가 가능하다는 점을 이해할 것이다. 따라서 본 발명의 진정한 기술적 보호범위는 첨 부된 특허청구범위의 기술적 사상에 의해 정해져야 할 것이다."}
{"patent_id": "10-2020-0088207", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시 예에 따라 대용량 학습 데이터를 분산 학습을 위한 시스템의 개괄도를 도시한 다. 도 2는 본 발명의 일 실시 예에 따른 분산 기계 학습에서 파라미터를 갱신하는 방법의 흐름도이다. 도 3 내지 도 5는 본 발명의 일 실시 예에 따른 분산 기계 학습에서 파라미터를 갱신하는 과정을 설명하는 예시도이다."}
