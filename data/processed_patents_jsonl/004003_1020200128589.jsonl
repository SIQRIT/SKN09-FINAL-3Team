{"patent_id": "10-2020-0128589", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0045685", "출원번호": "10-2020-0128589", "발명의 명칭": "몰입감 있는 Mixed Reality를 제공하기 위한 MR 제공 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "슈쿠르 올렉산드르"}}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "MR(Mixed Reality) 제공 장치에 있어서,카메라;비디오를 제공하는 전자 장치와 통신하기 위한 통신부; 기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 동시에 디스플레이하기 위한 광학 디스플레이부; 및상기 카메라, 상기 통신부 및 상기 광학 디스플레이부와 연결된 프로세서;를 포함하고,상기 프로세서는,상기 카메라를 통해 상기 기설정된 시야각 범위를 촬영하여 이미지를 획득하고,상기 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을식별하고,상기 통신부를 통해, 상기 위치할 수 있는 객체와 관련된 상기 의미론적 앵커 지점의 특성 정보(charicteristicinformation)를 상기 전자 장치로 전송하고,상기 비디오의 이미지 프레임에 포함된 적어도 하나의 객체 중 상기 특성 정보에 대응되는 객체가 포함된 객체영역을 상기 통신부를 통해 상기 전자 장치로부터 수신하고,상기 의미론적 앵커 지점 상에 상기 수신된 객체 영역을 디스플레이하도록 상기 광학 디스플레이부를 제어하는,MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 카메라는, 뎁스 카메라를 포함하고,상기 프로세서는,상기 카메라를 통해 획득된 이미지의 복수의 픽셀의 뎁스 정보를 획득하고,상기 획득된 뎁스 정보를 기반으로, 상기 획득된 이미지 내에서 적어도 하나의 수평면을 식별하고,상기 식별된 적어도 하나의 수평면의 넓이 및 수직 방향의 높이에 기초하여, 상기 식별된 적어도 하나의 수평면중 객체가 위치할 수 있는 의미론적 앵커 지점을 식별하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,이미지가 입력되면, 상기 입력된 이미지에 포함된 의미론적 앵커 지점 및 상기 의미론적 앵커 지점의 특성 정보를 추출하도록 훈련된 인공지능 모델을 포함하는 메모리;를 더 포함하고,상기 프로세서는,상기 카메라를 통해 획득된 이미지를 상기 인공지능 모델에 입력하여, 상기 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점을 식별하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 의미론적 앵커 지점의 특성 정보는,공개특허 10-2022-0045685-3-상기 의미론적 앵커 지점에 위치할 수 있는 객체의 종류에 대한 정보를 포함하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 프로세서는,상기 카메라를 통해 획득된 이미지에 포함된 적어도 하나의 객체를 식별하고,상기 식별된 객체의 종류를 기반으로, 상기 의미론적 앵커 지점에 위치할 수 있는 객체의 종류를 식별하는, MR제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,객체의 종류 별 수가 입력되면, 추가적으로 존재할 수 있는 객체의 종류를 출력하도록 훈련된 인공지능 모델을포함하는 메모리;를 더 포함하고,상기 프로세서는,상기 획득된 이미지로부터 식별된 객체의 수를 종류 별로 상기 인공지능 모델에 입력하여, 추가적으로 존재할수 있는 적어도 하나의 객체의 종류를 판단하고,상기 판단된 종류에 기초하여 상기 의미론적 앵커 지점에 위치할 수 있는 객체의 종류를 식별하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 프로세서는, 상기 획득된 이미지 내에서 복수의 의미론적 앵커 지점이 식별되고, 상기 전자 장치로부터 상기 복수의 의미론적 앵커 지점의 특성 정보에 대응되는 복수의 객체 영역이 수신된 경우, 상기 복수의 의미론적 앵커 지점 각각과 상기 MR 제공 장치 간의 거리 및 상기 이미지 프레임 내에서 상기 복수의 객체 영역 간의 위치 관계에 기초하여, 상기 복수의 의미론적 앵커 지점 중 상기 수신된 복수의 객체 영역 각각이 위치할 수 있는 의미론적 앵커지점들을 선택하고,상기 선택된 의미론적 앵커 지점들 각각 상에 상기 수신된 복수의 객체 영역 각각을 디스플레이하도록 상기 광학 디스플레이부를 제어하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 프로세서는,상기 획득된 이미지 내에서 복수의 의미론적 앵커 지점이 식별된 경우, 상기 이미지 내에서 상기 복수의 의미론적 앵커 지점 각각에 존재하는 객체의 종류 또는 크기를 식별하고, 상기 식별된 객체의 종류 또는 크기를 기반으로, 상기 복수의 의미론적 앵커 지점 중 상기 수신된 객체 영역이위치하기 위한 의미론적 앵커 지점을 선택하고,상기 선택된 의미론적 앵커 지점에 상기 수신된 객체 영역을 디스플레이하도록 상기 광학 디스플레이부를 제어하는, MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 프로세서는,공개특허 10-2022-0045685-4-이미지에 적어도 하나의 객체 영역을 합성하도록 훈련된 GAN(Generative Adversarial Network)에 상기 획득된이미지를 입력하여, 상기 수신된 객체 영역이 디스플레이되는 위치를 식별하고,상기 식별된 위치를 기반으로, 상기 수신된 객체 영역을 디스플레이하도록 상기 광학 디스플레이부를 제어하는,MR 제공 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "전자 장치에 있어서,비디오가 저장된 메모리;MR 제공 장치와 통신하기 위한 통신부; 및상기 메모리 및 상기 통신부와 연결된 프로세서;를 포함하고,상기 프로세서는,상기 MR 제공 장치를 통해 획득된 이미지에 포함되는 의미론적 앵커 지점의 특성 정보를 상기 통신부를 통해 상기 MR 제공 장치로부터 수신하고,상기 비디오에 포함된 이미지 프레임 내에서 상기 수신된 특성 정보에 대응되는 객체를 식별하고,상기 식별된 객체를 포함하는 객체 영역을 상기 통신부를 통해 상기 MR 제공 장치로 전송하는, 전자 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 메모리는,복수의 종류의 객체를 식별하도록 훈련된 인공지능 모델을 포함하고,상기 프로세서는,상기 복수의 종류 중 상기 특성 정보에 대응되는 종류를 선택하고,상기 이미지 프레임 내에서 상기 선택된 종류의 객체를 식별하도록 상기 인공지능 모델을 제어하는, 전자 장치."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 제공하기 위한 MR 제공 장치의 제어 방법에 있어서,카메라를 통해 상기 기설정된 시야각 범위를 촬영하여 이미지를 획득하는 단계;상기 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을식별하는 단계;상기 위치할 수 있는 객체와 관련된 상기 의미론적 앵커 지점의 특성 정보(charicteristic information)를 전자장치로 전송하는 단계;상기 전자 장치가 제공하는 비디오의 이미지 프레임에 포함된 적어도 하나의 객체 중 상기 특성 정보에 대응되는 객체가 포함된 객체 영역을 상기 전자 장치로부터 수신하는 단계; 및상기 의미론적 앵커 지점 상에 상기 수신된 객체 영역을 디스플레이하는 단계;를 포함하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 카메라는, 뎁스 카메라를 포함하고,상기 의미론적 앵커 지점을 식별하는 단계는,상기 카메라를 통해 획득된 이미지의 복수의 픽셀의 뎁스 정보를 획득하고,공개특허 10-2022-0045685-5-상기 획득된 뎁스 정보를 기반으로, 상기 획득된 이미지 내에서 적어도 하나의 수평면을 식별하고,상기 식별된 적어도 하나의 수평면의 넓이 및 수직 방향의 높이에 기초하여, 상기 식별된 적어도 하나의 수평면중 객체가 위치할 수 있는 의미론적 앵커 지점을 식별하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제12항에 있어서,상기 MR 제공 장치의 메모리는,이미지가 입력되면, 상기 입력된 이미지에 포함된 의미론적 앵커 지점 및 상기 의미론적 앵커 지점의 특성 정보를 추출하도록 훈련된 인공지능 모델을 포함하고,상기 의미론적 앵커 지점을 식별하는 단계는,상기 카메라를 통해 획득된 이미지를 상기 인공지능 모델에 입력하여, 상기 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점을 식별하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제12항에 있어서,상기 획득된 이미지에 포함된 적어도 하나의 객체를 식별하는 단계;상기 식별된 객체의 종류를 기반으로, 상기 의미론적 앵커 지점에 위치할 수 있는 객체의 종류를 판단하는단계; 및상기 판단된 객체의 종류를 기반으로, 상기 의미론적 앵커 지점의 상기 특성 정보를 생성하는 단계;를포함하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 MR 제공 장치의 메모리는,객체의 종류 별 수가 입력되면, 추가적으로 존재할 수 있는 객체의 종류를 출력하도록 훈련된 인공지능 모델을포함하고,상기 객체의 종류를 판단하는 단계는,상기 획득된 이미지로부터 식별된 객체의 수를 종류 별로 상기 인공지능 모델에 입력하여, 추가적으로 존재할수 있는 적어도 하나의 객체의 종류를 판단하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 획득된 이미지 내에서 복수의 의미론적 앵커 지점이 식별되고, 상기 전자 장치로부터 상기 복수의 의미론적 앵커 지점의 특성 정보에 대응되는 복수의 객체 영역이 수신된 경우, 상기 복수의 의미론적 앵커 지점 각각과 상기 MR 제공 장치 간의 거리 및 상기 이미지 프레임 내에서 상기 복수의 객체 영역 간의 위치 관계에 기초하여, 상기 복수의 의미론적 앵커 지점 중 상기 수신된 복수의 객체 영역 각각이 위치하기 위한 의미론적 앵커지점들을 선택하는 단계;를 더 포함하고,상기 수신된 객체 영역을 디스플레이하는 단계는,상기 선택된 의미론적 앵커 지점들 각각 상에 상기 수신된 복수의 객체 영역 각각을 디스플레이하는, MR 제공장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제12항에 있어서,공개특허 10-2022-0045685-6-상기 획득된 이미지 내에서 복수의 의미론적 앵커 지점이 식별된 경우, 상기 이미지 내에서 상기 복수의 의미론적 앵커 지점 각각에 존재하는 객체의 종류 또는 크기를 식별하는 단계; 및상기 식별된 객체의 종류 또는 크기를 기반으로, 상기 복수의 의미론적 앵커 지점 중 상기 수신된 객체 영역이위치하기 위한 의미론적 앵커 지점을 선택하는 단계;를 더 포함하고,상기 수신된 객체 영역을 디스플레이하는 단계는,상기 선택된 의미론적 앵커 지점 상에 상기 수신된 객체 영역을 디스플레이하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제12항에 있어서,이미지에 적어도 하나의 객체 영역을 합성하도록 훈련된 GAN(Generative Adversarial Network)에 상기 획득된이미지를 입력하여, 상기 수신된 객체 영역이 디스플레이되는 위치를 식별하고,상기 식별된 위치를 기반으로, 상기 수신된 객체 영역을 디스플레이하는, MR 제공 장치의 제어 방법."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "MR(Mixed Reality) 제공 장치가 개시된다. 본 MR 제공 장치는, 카메라, 비디오를 제공하는 전자 장치와 통신하기 위한 통신부, 기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 동시에 디스플레이하기 위한 광학 디스 플레이부, 프로세서를 포함한다. 프로세서는, 카메라를 통해 기설정된 시야각 범위를 촬영하여 이미지를 획득하 고, 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을 식별하고, 통신부를 통해 의미론적 앵커 지점의 특성 정보(charicteristic information)를 전자 장치로 전송하고, 비디오의 이미지 프레임에 포함되고 특성 정보에 대응되는 객체가 포함된 객체 영역을 통신부를 통해 전자 장치로부터 수신하고, 의미론적 앵커 지점 상에 수신된 객체 영역을 디스플레이하도록 광학 디스플레이부를 제어한다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는, MR(Mixed Reality) 제공 장치에 관한 것으로, 보다 상세하게는, 현실의 물리적 공간 및 비디오 컨텐 츠를 함께 제공하는 MR 제공 장치에 관한 것이다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "혼합 현실(MR. Mixed Reality)은 실제의 이미지와 가상의 이미지를 혼합하여 제공하는 개념으로, 현실의 물리적 객체와 가상의 객체가 상호 작용하는 환경을 시각적으로 제공하는 기술이다. 혼합 현실은 종종 증강 현실 (AR.Augmented Reality)과 혼용되어 사용되는 개념이기도 하다. 미래에 스마트폰을 대체할 강력한 후보로 부상하고 있는 AR/MR 제공 장치는, 주로 HMD(Head Mounted Device) 또 는 웨어러블 글래스의 형태로 개발되고 있다. 그리고, 현실의 공간과 가상의 정보를 함께 보여주기 위한 다양한 형태의 광학 디스플레이부가 이미 개발된 바 있다. 예를 들어, 미니 프로젝터의 빛을 산란(splitting)하여 복수의 광 도파관에 입력하는 기술(ex. Magic Leap One), 홀로그래픽 방식을 이용하는 기술(ex. HoloLense), 렌즈 상에 빛을 반사하는 작은 핀홀들이 배치된 핀 미 러 방식을 이용하는 기술(ex. LetinAR의 PinMR) 등 가상의 이미지를 사용자의 시야각 내 원하는 위치/뎁스로 표 시하기 위한 다양한 AR/MR 광학 기술들이 공개된 바 있다. 한편, 카메라를 통해 촬영된 현실의 이미지에 가상의 이미지를 합성한 합성 이미지를 일반적인 디스플레이를 통 해 제공하는 방식도 가능하다. 본 방식은, 상술한 광학 디스플레이부를 필요로 하지 않기 때문에, 현재 통용되 는 일반적인 스마트폰이나 태블릿 PC 등으로도 구현 가능하다(ex. 포켓몬고). 상술한 기술들을 이용함으로써, 거실의 벽에 가상의 TV 화면을 제공하는 등 가상의 비디오 컨텐츠를 실제 공간 상에 제공하는 것이 충분히 가능한 기술 수준에 이르렀다. 다만, MR 제공 장치를 이용하여 가상의 비디오 컨텐츠를 제공하는 경우, 일반적으로 HMD나 웨어러블 글래스 형 태로 제공되는 MR 제공 장치의 크기/무게/연산 속도 등의 한계 때문에 실제 TV와 비교했을 때 그 화질이 매우 떨어질 수밖에 없다. 또한, 설령 화질을 실제의 TV와 동일한 수준으로 구현할 수 있다고 하더라도, MR 제공 장치가 벽에 놓인 가상의 TV 화면를 통해 2D 이미지를 제공하는 방식이, MR 제공 장치 없이 실제 TV를 통해 비디오 컨텐츠를 제공하는 방 식보다 더 몰입감 높은 사용자 경험을 제공한다고 보기도 힘들다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 외부 전자 장치로부터 수신된 비디오 컨텐츠를 실제 환경 내에 적절히 융합하여 제공하는 MR 제공 장치를 제공한다. 구체적으로, 외부 전자 장치는, 비디오 컨텐츠에 포함된 객체가 위치할 만한 실제 위치를 식별하고, 식별된 위 치 상에, 해당 객체를 가상의 이미지로서 사용자에게 제공하는 MR 제공 장치를 제공한다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시 예에 따른 MR(Mixed Reality) 제공 장치는, 카메라, 비디오를 제공하는 전자 장치와 통신하 기 위한 통신부, 기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 동시에 디스플레이하기 위한 광학 디스플레이부, 프로세서를 포함한다. 상기 프로세서는, 상기 카메라를 통해 상기 기설정된 시야각 범위를 촬영 하여 이미지를 획득하고, 상기 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점 (semantic anchor spot)을 식별하고, 상기 통신부를 통해, 상기 위치할 수 있는 객체와 관련된 상기 의미론적 앵커 지점의 특성 정보(charicteristic information)를 상기 전자 장치로 전송하고, 상기 비디오의 이미지 프레 임에 포함된 적어도 하나의 객체 중 상기 특성 정보에 대응되는 객체가 포함된 객체 영역을 상기 통신부를 통해 상기 전자 장치로부터 수신하고, 상기 의미론적 앵커 지점 상에 상기 수신된 객체 영역을 디스플레이하도록 상 기 광학 디스플레이부를 제어한다. 본 개시의 일 실시 예에 따른 전자 장치는, 비디오가 저장된 메모리, MR 제공 장치와 통신하기 위한 통신부, 상 기 메모리 및 상기 통신부와 연결된 프로세서를 포함한다. 상기 프로세서는, 상기 MR 제공 장치를 통해 획득된 이미지에 포함되는 의미론적 앵커 지점의 특성 정보를 상기 통신부를 통해 상기 MR 제공 장치로부터 수신하고, 상기 비디오에 포함된 이미지 프레임 내에서 상기 수신된 특성 정보에 대응되는 객체를 식별하고, 상기 식별된 객체를 포함하는 객체 영역을 상기 통신부를 통해 상기 MR 제공 장치로 전송할 수 있다. 본 개시의 일 실시 예에 따라 기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 제공하기 위한 MR 제공 장치의 제어 방법은, 카메라를 통해 상기 기설정된 시야각 범위를 촬영하여 이미지를 획득하는 단계, 상기 획득 된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을 식별하는 단계, 상기 위치할 수 있는 객체와 관련된 상기 의미론적 앵커 지점의 특성 정보(charicteristic information) 를 전자 장치로 전송하는 단계, 상기 전자 장치가 제공하는 비디오의 이미지 프레임에 포함된 적어도 하나의 객 체 중 상기 특성 정보에 대응되는 객체가 포함된 객체 영역을 상기 전자 장치로부터 수신하는 단계, 상기 의미 론적 앵커 지점 상에 상기 수신된 객체 영역을 디스플레이하는 단계;를 포함한다. 본 개시의 일 실시 예에 따른 MR(Mixed Reality) 제공 장치는, 카메라, 비디오를 제공하는 전자 장치와 통신하 기 위한 통신부, 디스플레이, 프로세서를 포함한다. 상기 프로세서는, 상기 카메라를 통해 획득된 이미지 내에 서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을 식별하고, 상기 통신부 를 통해, 상기 위치할 수 있는 객체와 관련된 상기 의미론적 앵커 지점의 특성 정보(charicteristic information)를 상기 전자 장치로 전송하고, 상기 비디오의 이미지 프레임에 포함된 적어도 하나의 객체 중 상 기 특성 정보에 대응되는 객체가 포함된 객체 영역을 상기 통신부를 통해 상기 전자 장치로부터 수신하고, 상기 획득된 이미지에 포함된 상기 의미론적 앵커 지점 상에 상기 수신된 객체 영역을 합성하여 MR 이미지를 획득하 고, 상기 획득된 MR 이미지를 디스플레이하도록 상기 디스플레이를 제어한다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따른 MR 제공 장치는, 현실의 공간 상에 비디오 컨텐츠 내 객체를 함께 제공하므로, 보다 몰입감 있 는 MR 비디오 컨텐츠를 제공한다는 효과가 있다. 본 MR 제공 장치가 제공한 MR에 의해, 사용자는 현실 공간의 물건들(ex. 요리 도구, 식기 등)을 이용하여 현실 의 일을 수행(ex. 요리, 식사 등)하면서도 동시에 비디오 컨텐츠 내 객체들의 퍼포먼스를 제공받을 수 있다. 일 예로, 사용자는 요리를 하던 도중 비디오 컨텐츠를 보기 위해 가상의 TV 화면으로 고개를 돌릴 필요가 없다. 본 개시에 따른 MR 제공 장치는, 비디오 컨텐츠 전체가 아니라 비디오 컨텐츠 내에서 의미론적으로 식별된 객체 영역만을 외부 전자 장치로부터 수신하기 때문에, 비디오 컨텐츠의 스트리밍 데이터 용량을 줄이면서도 몰입감 있는 MR을 제공한다는 효과가 있다."}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에 대하여 구체적으로 설명하기에 앞서, 본 명세서 및 도면의 기재 방법에 대하여 설명한다. 먼저, 본 명세서 및 청구범위에서 사용되는 용어는 본 개시의 다양한 실시 예들에서의 기능을 고려하여 일반적 인 용어들을 선택하였다. 하지만, 이러한 용어들은 당해 기술 분야에 종사하는 기술자의 의도나 법률적 또는 기 술적 해석 및 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 일부 용어는 출원인이 임의로 선정한 용어 도 있다. 이러한 용어에 대해서는 본 명세서에서 정의된 의미로 해석될 수 있으며, 구체적인 용어 정의가 없으 면 본 명세서의 전반적인 내용 및 당해 기술 분야의 통상적인 기술 상식을 토대로 해석될 수도 있다. 또한, 본 명세서에 첨부된 각 도면에 기재된 동일한 참조번호 또는 부호는 실질적으로 동일한 기능을 수행하는 부품 또는 구성요소를 나타낸다. 설명 및 이해의 편의를 위해서 서로 다른 실시 예들에서도 동일한 참조번호 또 는 부호를 사용하여 설명한다. 즉, 복수의 도면에서 동일한 참조 번호를 가지는 구성요소를 모두 도시되어 있다 고 하더라도, 복수의 도면들이 하나의 실시 예를 의미하는 것은 아니다. 또한, 본 명세서 및 청구범위에서는 구성요소들 간의 구별을 위하여 \"제1\", \"제2\" 등과 같이 서수를 포함하는 용어가 사용될 수 있다. 이러한 서수는 동일 또는 유사한 구성요소들을 서로 구별하기 위하여 사용하는 것이며 이러한 서수 사용으로 인하여 용어의 의미가 한정 해석되어서는 안 된다. 일 예로, 이러한 서수와 결합된 구성 요소는 그 숫자에 의해 사용 순서나 배치 순서 등이 제한되어서는 안 된다. 필요에 따라서는, 각 서수들은 서로 교체되어 사용될 수도 있다. 본 명세서에서 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이 들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성 요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시의 실시 예에서 \"모듈\", \"유닛\", \"부(part)\" 등과 같은 용어는 적어도 하나의 기능이나 동작을 수행하는 구성요소를 지칭하기 위한 용어이며, 이러한 구성요소는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\", \"유닛\", \"부(part)\" 등은 각각이 개별적인 특정 한 하드웨어로 구현될 필요가 있는 경우를 제외하고는, 적어도 하나의 모듈이나 칩으로 일체화되어 적어도 하나 의 프로세서로 구현될 수 있다. 또한, 본 개시의 실시 예에서, 어떤 부분이 다른 부분과 연결되어 있다고 할 때, 이는 직접적인 연결뿐 아니라, 다른 매체를 통한 간접적인 연결의 경우도 포함한다. 또한, 어떤 부분이 어떤 구성요소를 포함한다는 의미는, 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것 을 의미한다. 도 1은 본 개시에 따른 MR 제공 장치의 동작을 개략적으로 설명하기 위한 도면이다. 도 1을 참조하면, 사용자가 HMD로 구현된 MR 제공 장치를 착용한 경우, MR 제공 장치는 사용자에 게 실제 공간의 영상을 제공할 수 있다. MR 제공 장치는 복수의 이미지 프레임으로 구성된 비디오를 제공하는 적어도 하나의 외부 전자 장치와 통신을 수행할 수 있다. 비디오는 다양한 컨텐츠에 해당할 수 있다. 예를 들어, 뉴스, 토크쇼, 콘서트, 스포츠, E-스포츠, 영화 등 다양할 수 있다. 또한, 비디오는 실시간으로 제공되는 라이브 방송일 수도 있고, 화상 통화의 상대방을 담 은 실시간 영상일 수도 있다. 이때, MR 제공 장치는 비디오의 전체를 스트리밍 받지 않고, 비디오에 포함된 일부 객체 영역만을 이미지 프레임 별로 실시간으로 수신할 수 있다. 도 1을 참조하면, MR 제공 장치는, 비디오의 각 이미지 프레임 전체가 아니라, 이미지 프레임에 포함 된 인물들(21, 22)을 각각 포함하는 객체 영역들(11, 12)만을 수신할 수 있다. 그리고, MR 제공 장치는 실제 공간 내 의자 및 특정 빈 공간 상에, 객체 영역들(11, 12) 각각을 가상의 이 미지로서 제공할 수 있다. 그 결과, 사용자는 마치 실제 공간 상에 비디오 내 인물들(21, 22)이 함께 있는 것처럼 느낄 수 있다 (10'). 이하 도면들을 통해, MR 제공 장치의 구성 및 동작을 보다 상세하게 설명한다. 도 2는 본 개시의 일 실시 예에 따른 MR 제공 장치 및 전자 장치 각각의 구성 및 동작을 설명하기 위한 블록도 이다. 도 2를 참조하면, MR 제공 장치는 카메라, 광학 디스플레이부, 통신부, 프로세서 등 을 포함할 수 있다. MR 제공 장치는 HMD, AR/MR 글래스 등 다양한 형태로 구현될 수 있다. 또한, 기술의 발전에 따라서는, 적 어도 하나의 컴퓨팅 장치와 통신 가능한 스마트 렌즈로도 구현될 수 있다. 카메라는 실제 공간을 촬영하기 위한 구성으로, 뎁스 카메라 및 RGB 카메라 중 적어도 하나를 포함할 수 있다. 뎁스 카메라는 실제 공간 상의 각 지점과 뎁스 카메라 간의 거리를 나타내는 뎁스 정보(: 뎁스 이미지 또는 뎁 스 맵)를 획득할 수 있다. 이를 위해, 뎁스 카메라는 적어도 하나의 ToF 센서를 포함할 수 있다. RGB 카메라는 RGB 이미지를 획득할 수 있다. 이를 위해, RGB 카메라는 적어도 하나의 광 센서 단자를 포함할 수 있다. 한편, 카메라는 두 개 이상의 RGB 카메라(ex. 스테레오 카메라)를 포함할 수도 있는 바, 이 경우 RGB 카메 라들을 통해 촬영된 이미지들 내 서로 대응되는 픽셀들의 위치 차이를 기반으로 뎁스 정보가 획득될 수도 있다. 광학 디스플레이부는 사용자가 바라보는 시야각 범위 내의 실제 공간과 프로세서를 통해 제공되는 가 상의 이미지를 동시에 디스플레이하기 위한 구성이다. 광학 디스플레이부가 실제 공간을 제공하는 시야각 범위는 MR 제공 장치 내 광학 디스플레이부 의 설치 구조에 따라 기설정될 수 있다. 여기서, 시야각 범위는, MR 제공 장치의 정면 방향(: 사용자의 시선 방향)을 기준으로 할 수 있다. 한편, 프로세서는 카메라를 통해 기설정된 시야각 범위를 촬영할 수 있는데, 카메라가 촬영하는 시야각 범위 역시, 광학 디스플레이부가 제공하는 시야각 범위의 기준 각도와 마찬가지로 MR 제공 장치 의 정면 방향을 기준으로 할 수 있다. 광학 디스플레이부는 빛을 산란(splitting)하여 복수의 광 도파관에 입력하는 방식, 홀로그래픽 방식, 핀 미러 방식 등 다양한 방식을 통해 가상의 이미지를 제공할 수 있다. 이를 위해, 광학 디스플레이부는 프로 젝터, 렌즈, 디스플레이, 미러 등 다양한 구성을 포함할 수 있다. 프로세서는, 광학 디스플레이부를 통해, 사용자에게 제공되는 시야각(: 실제 공간) 범위 내 다양한 위치에, 다양한 뎁스의 가상 이미지 또는 가상 정보를 디스플레이할 수 있다. 통신부를 통해, 프로세서는 외부의 전자 장치와 통신을 수행할 수 있다. 도 2를 참조하면, MR 제공 장치의 프로세서는 semantic anchor spot extractor(140. 이하 extractor 로 지칭함) 및 object positioning module 등을 포함할 수 있다. 한편, 전자 장치는 적어도 하나의 비디오를 저장/제공할 수 있는 장치이다. 전자 장치는 TV, 셋탑 박 스, 서버 등 다양한 장치로 구현될 수 있다. 도 2를 참조하면, 전자 장치는, 메모리, 통신부, 프로세서 등을 포함할 수 있다. 메모리에는 복수의 이미지 프레임으로 구성된 비디오를 포함할 수 있다. 전자 장치의 프로세서는 통신부를 통해 MR 제공 장치와 통신을 수행할 수 있다. 프로세서는, semantic object recognizer를 포함할 수 있다. 상술한 모듈들(141, 142, 231)은 각각 소프트웨어 또는 하드웨어로 구현될 수 있으며, 소프트웨어 및 하드웨어 가 결합된 형태로 구현될 수도 있다. 본 개시의 일 실시 예에 따른 MR 제공 장치의 프로세서는 카메라를 통해 기설정된 시야각 범위 를 촬영하여 실제 공간에 대한 이미지를 획득할 수 있다. Extractor는 실제 공간 상에 존재하는 의미론적 앵커 지점을 식별하기 위한 구성이다. Extractor는 카메라를 통해 실제 공간을 촬영하여 획득된 이미지 내에서 적어도 하나의 의미론적 앵커 지점을 식별할 수 있다. 의미론적 앵커 지점은, 적어도 하나의 객체가 위치할 수 있을 만한 지점으로 정의될 수 있다. 예를 들어, 의미론적 앵커 지점은, 서 있는 사람이 위치할 수 있는 바닥면, 앉은 사람이 위치할 수 있는 의자면, 식기가 위치할 수 있는 식탁면, 사무 용품이 위치할 수 있는 책상면 등 실제 공간 상에 존재하는 다양 한 수평면에 해당할 수 있다. 다만, 의미론적 앵커 지점은 반드시 수평면에 해당할 필요는 없고, 예를 들어, 실제 공간 상의 옷걸이의 경우, 옷이 위치할 수 있는 의미론적 앵커 지점이 될 수도 있다. 또한, Extractor는 의미론적 앵커 지점과 함께 정의되는 의미론적 앵커 지점의 특성 정보도 획득할 수 있 다. 의미론적 앵커 지점의 특성 정보는, 의미론적 앵커 지점에 위치할 수 있는 객체에 대한 정보이다. 구체적으로, 의미론적 앵커 지점의 특성 정보는, 의미론적 앵커 지점에 위치할 수 있는 객체의 종류에 대한 정 보를 포함할 수 있다. 여기서, 객체의 종류는, 사람, 강아지, 캐릭터(ex. 괴물) 등 움직이는 대상 뿐만 아니라, 컵, 책, TV, 나무 등 움직이지 않는 사물이나 식물도 포함할 수 있다. 또한, 객체의 종류는, 서 있는 사람, 앉아 있는 사람, 달리는 사람, 걷는 사람, 누워 있는 사람, 큰 강아지, 작은 강아지 등으로 더욱 세분화될 수도 있다. 더하여, 의미론적 앵커 지점에 위치할 수 있는 객체의 종류는, 상술한 각 객체들(사람, 나무)의 일부분(팔, 다 리, 나뭇잎)으로 더욱 세분화될 수도 있다. 일 실시 예에 따르면, 의미론적 앵커 지점의 특성 정보는, 의미론적 앵커 지점에 객체가 존재할 가능성을 객체 의 종류 별로 수치화한 적어도 하나의 벡터로 구성될 수도 있다. 의미론적 앵커 지점의 특성 정보는, 그밖에 의미론적 앵커 지점에 위치할 수 있는 객체의 크기, 모양, 수 등에 대한 정보도 포함할 수 있다. 일 실시 예에 따르면, Extractor는 카메라를 통해 촬영된 (뎁스) 이미지 내에서 수평면을 식별하고, 식별된 수평면의 수평 넓이 및 수직 높이 등을 이용하여, 해당 수평면이 의미론적 앵커 지점인지 여부를 판단할 수 있다. 일 예로, 수평면의 수평 넓이와 관련하여, 가로 길이가 40mm 이상이고 세로 길이가 40mm 이상인 경우, Extractor는 해당 수평면을 '서 있는 사람'이 위치할 수 있는 의미론적 앵커 지점으로 식별할 수 있다. 여 기서, '서 있는 사람'은 의미론적 앵커 지점의 특성 정보가 된다. 관련된 구체적인 실시 예는 도 3a 내지 도 3b 를 통해 후술한다. 한편, Extractor는 이밖에도 다양한 방식으로 의미론적 앵커 지점을 추출할 수 있는바, 도 4a 내지 도 4c 및 도 5a 내지 도 5c 등을 통해 후술한다. Extractor는 실시간으로 의미론적 앵커 지점을 식별함으로써, 이미 식별된 의미론적 앵커 지점을 추적할 수도 있다. 이때, Extractor는 카메라를 통해 촬영되는 시야각 범위 내에서 의미론적 앵커 지점의 위치를 식별할 수 있다. 카메라를 통해 촬영되는 시야각 범위의 기준 각도가 광학 디스플레이부의 시야각 범위의 기준 각도와 동일한 경우(또는 기설정된 각도 관계로 기설치된 경우), Extractor는 식별된 의미론적 앵커 지점의 위치 를 이용하여, 사용자가 광학 디스플레이부를 바라보는 시야각 범위 내의 의미론적 앵커 지점의 위치도 판 단할 수 있다. 또한, Extractor는 카메라를 통해 의미론적 앵커 지점의 뎁스 정보도 실시간으로 획득할 수 있다. 한편, 의미론적 앵커 지점 및 특성 정보가 획득된 경우, 프로세서는 통신부를 통해 외부의 전자 장치 로 의미론적 앵커 지점의 특성 정보를 전송할 수 있다. 이 경우, 전자 장치의 프로세서는 수신된 특성 정보를 이용하여, 메모리에 저장된 비디오 내 적 어도 하나의 객체를 인식할 수 있다. 구체적으로, 전자 장치의 통신부를 통해 수신된 특성 정보가 '서 있는 사람'인 경우를 가정할 수 있 다. 이 경우, 프로세서는 semantic object recognizer를 통해 비디오에 포함되는 이미지 프레임 내에서 '서 있는 사람'을 식별할 수 있다. 이를 위해, semantic object recognizer는 다양한 종류의 객체를 식별하기 위한 적어도 하나의 인공지능 모델을 이용할 수 있다. 여기서, 비록 해당 인공지능 모델이 '서 있는 사람', '앉아있는 사람', '강아지' 등을 각각 식별하기 위한 연산 을 수행하는 형태라고 하더라도, semantic object recognizer는 인공지능 모델이 '서 있는 사람'을 식별하 기 위한 연산만을 구동하도록 제어할 수 있다. 그 결과, 객체 인식을 수행하는 전자 장치의 연산량이 줄어들 수 있는바, 관련된 구체적인 실시 예는 도 6a 내지 도 6b를 통해 후술한다. 특성 정보에 따라 객체 영역이 인식되면, 프로세서는 통신부를 통해 객체 영역을 MR 제공 장치 로 전송할 수 있다. 이때, 객체 영역에 포함된 객체의 종류, 크기에 대한 정보가 함께 전송될 수 있다. 만약, 비디오의 이미지 프레 임 내에서 복수의 객체가 식별된 경우, 이미지 프레임 내의 복수의 객체의 위치 관계(ex. 거리, 방향 등)에 대 한 정보도 함께 전송될 수 있다. 객체 영역이 수신되면, MR 제공 장치의 프로세서는 object positioning module를 통해 객체 영 역의 위치를 판단할 수 있다. Object positioning module은 사용자의 시야각 범위 내 수신된 객체 영역의 위치를 판단하기 위한 모듈이 다. Object positioning module은, semantic anchor spot exractor로부터 의미론적 앵커 지점의 위치 및 뎁스 정보를 전달받을 수 있다. 여기서, 의미론적 앵커 지점의 위치는, 광학 디스플레이부의 시야각 범위 내 위치일 수 있다. 그리고, object positioning module은 의미론적 앵커 지점의 위치, 의미론적 앵커 지점의 뎁스 정보에 따 라 객체 영역의 위치 및 뎁스 정보를 판단할 수 있다. 만약, 객체 영역이 위치할 수 있는 의미론적 앵커 지점이 복수 개인 경우, object positioning module은 사용자(MR 제공 장치)와의 거리가 가장 가까운(: 뎁스가 낮은) 의미론적 앵커 지점의 위치에 따라 객체 영역의 위치를 판단할 수 있다. 만약, 복수의 객체 영역이 수신되는 경우, object positioning module은 복수의 객체 영역 간의 위치 관계 를 이용하여 복수의 객체 영역 각각의 위치를 판단할 수도 있다. 관련된 구체적인 실시 예는 도 8을 통해 후술한다. 그리고, 프로세서는 판단된 객체 영역의 위치 및 뎁스 정보에 따라 객체 영역을 디스플레이하도록 광학 디 스플레이부를 제어할 수 있다. 그 결과, MR 제공 장치는 실제 공간 상의 의미론적 앵커 지점 상에 비디오의 객체가 위치하는 장면을 사용 자에게 제공할 수 있다. 도 3a 내지 도 3c는 본 개시의 일 실시 예에 따른 MR 제공 장치가 수평면의 수평 넓이 및 높이를 기반으로 의미 론적 앵커 지점을 식별하는 동작을 설명하기 위한 도면들이다. Extractor는, 카메라를 통해 실제 공간이 촬영된 이미지 내에서 수평면을 모두 식별한 뒤, 수평 면들의 수직 높이 및 수평 넓이 등에 따른 조건을 이용하여, 수평면들 중 적어도 하나의 수평면을 의미론적 앵 커 지점으로 식별할 수 있다. 의미론적 앵커 지점이 되기 위한 수평면의 조건은, 객체의 종류 별로 다르게 기설정될 수 있다. 구체적인 예로, Extractor는, 이미지 내 수평면들 중 수직 높이가 가장 낮으며 수평 넓이가 가로 60mm 및 세로 60mm 이상인 수평면을 서 있는 사람이 위치할 수 있는 의미론적 앵커 지점으로 식별할 수 있다. 그 결과, 도 3a와 같이 Extractor는 수평면을 서 있는 사람이 위치할 수 있는 의미론적 앵커 지점으 로 식별할 수 있다. 또한, Extractor는 수평면을 앉아 있는 사람이 위치할 수 있는 의미론적 앵커 지점을 식별할 수 있다. 구체적으로, 도 3b를 참조하면, Extractor는, 수직 높이가 30mm 이상 90mm 미만이고, 수평 넓이가 가로 40mm 및 세로 40mm 이상이며, 수평면의 가장자리를 수직으로 내린 지점으로부터 20mm 이내에 가장 낮은 수평면 (바닥)이 위치하는 수평면을 앉아 있는 사람이 위치할 수 있는 의미론적 앵커 지점으로 식별할 수 있다. 상술한 과정을 거친 결과, 도 3c를 참조하면, 서 있는 사람이 위치할 수 있는 의미론적 앵커 지점이 36개 식별 되고, 앉아 있는 사람이 위치할 수 있는 의미론적 앵커 지점이 5개 식별될 수 있다. 한편, 도 3a 내지 도 3c와 같은 룰 베이스 방식 외에, Extractor는 적어도 하나의 인공지능 모델을 이용하 여 의미론적 앵커 지점을 식별할 수도 있다. 이를 위해, MR 제공 장치의 메모리는, 이미지가 입력되면, 입력된 이미지에 포함된 의미론적 앵커 지점 및 의미론적 앵커 지점의 특성 정보를 추출하도록 훈련된 인공지능 모델을 포함할 수 있다. 그리고, Extractor는, 카메라를 통해 획득된 이미지를 해당 인공지능 모델에 입력하여, 획득된 이미지 내 에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점을 식별할 수 있다. 관련하여, 도 4a는 본 개시의 일 실시 예에 따른 MR 제공 장치가 인공지능 모델을 이용하여 의미론적 앵커 지점 을 식별하는 동작을 설명하기 위한 도면이다. 도 4a를 참조하면, Extractor는, 카메라를 통해 촬영된 실제 공간의 이미지를 신경망 모델(41 0)에 입력할 수 있다. 이때, 신경망 모델은 이미지 내의 의미론적 앵커 지점을 출력할 수 있다. 구체적인 예로, 신경망 모델은 이미지에 포함되는 의미론적 앵커 지점의 히트 맵 형태로 의미론 적 앵커 지점을 출력할 수 있다. 또한, 신경망 모델은 의미론적 앵커 지점의 특성 정보를 출력할 수 있다. 이때, 특성 정보는 의미론적 앵커 지점에 위치할 가능성이 높은 적어도 하나의 객체의 종류(ex. 서 있는 사람)에 대한 정보를 포함할 수 있다. 만약, 의미론적 앵커 지점이 복수 개인 경우, 신경망 모델은 의미론적 앵커 지점의 수를 (위치할 수 있는) 객체의 종류 별로 출력할 수도 있다. 예를 들어, 서 있는 사람이 위치할 수 있는 의미론적 앵커 지점의 수는 36 개이고, 앉아 있는 사람이 위치할 수 있는 의미론적 앵커 지점의 수는 5개일 수 있다. 관련하여, 도 4b 내지 도 4c는 도 4a에서 이용되는 신경망 모델의 훈련 과정의 일 예를 설명하기 위한 도면들이 다. 본 훈련 과정은, MR 제공 장치에서 수행될 수도 있으나, 적어도 하나의 다른 외부 장치에서 수행될 수도 있음은 물론이다. 도 4b를 참조하면, 먼저 비디오로부터 적어도 하나의 객체를 인식할 수 있다(S410). 이때, 객체(ex. 서 있 는 사람)를 인식하도록 훈련된 적어도 하나의 인공지능 모델이 이용될 수 있으며, 비디오는 뎁스 정보를 포함할 수 있다. 여기서, 객체들(421-1, 2, 3)이 인식되면, 객체들(421-1, 2, 3) 각각을 구성하는 복수의 픽셀 중 가장 낮은 수 직 높이의 픽셀을 식별할 수 있다. 그리고, 식별된 픽셀과 가장 인접한 수평면을 인식할 수 있다(S420). 이때, 비디오 상에서 움직이는 객체들(421-1, 2, 3)이 모두 제외된 상태의 이미지 프레임(421') 내에서, 해당 수평면들을 인식할 수도 있다. 그리고, 도 4b를 참조하면, 이미지 프레임(421') 및 수평면의 히트 맵(402')을 훈련 데이터셋으로 하여 신 경망 모델을 훈련시킬 수 있다(S430). 마찬가지의 방식으로 수많은 이미지-히트 맵 페어를 통해 훈련된 결과, 신경망 모델은, 입력된 이미지 내 에서 객체(ex. 서 있는 사람)가 위치할 수 있는 의미론적 앵커 지점(히트 맵)을 식별할 수 있게 된다. 한편, 비록 상술한 도 3a의 실시 예를 통해서는, 가장 낮은 수평면에 위치할 수 있는 객체가 '서 있는 사람'인 것으로 단정적으로 전제하였으나, 이 밖에도 다양한 종류의 객체(ex. 강아지, 고양이 등)가 해당 수평면에 위치 할 수 있다. 일 예로, 해당 수평면은, 서 있는 사람, 강아지, 고양이 등의 다양한 객체들이 위치할 가능성이 높은 특성 정보 를 가지는 의미론적 앵커 지점으로 식별될 수도 있다. 이는, 의미론적 앵커 지점의 특성 정보가 객체의 종류 별 가능성을 수치화한 벡터 형태로 구현됨으로써 가능하다. 다만, 여기서 특성 정보에 따르면, 의미론적 앵커 지점에 위치할 수 있는 객체의 종류가 너무 많아지는 문제가 있을 수 있다. 이 경우, 전자 장치에서 수행되는 의미론적 객체 인식에 걸리는 시간이 증가할 수 있으며, 전자 장치 에서 MR 제공 장치로 수신되는 객체 영역의 수가 너무 많아질 수도 있다. 따라서, 일 실시 예에 따르면, Extractor는, 실제 공간 상에 기존재하고 있는 객체의 종류 별 수를 이용하 여, 해당 공간(: 의미론적 앵커 지점) 상에 추가적으로 위치할 수 있을 만한 객체의 종류를 예측할 수 있다. 그리고, Extractor는, 예측된 종류의 객체만을 포함하도록 의미론적 앵커 지점(: 해당 수평면)의 특성 정 보를 업데이트할 수 있다. 관련하여, 도 5a는 본 개시의 일 실시 예에 따른 MR 제공 장치가 기존재하는 객체들의 종류 별 수를 이용하여 의미론적 앵커 지점에 위치할 수 있는 객체를 예측하는 동작을 설명하기 위한 도면이다. 도 5a는, 적어도 하나의 의미론적 앵커 지점(ex. 수평면)이 이미 식별된 상황을 전제로 한다. 도 5a를 참조하면, Extractor는 object recognizer 및 object predictor를 포함할 수 있다. object recognizer는, 카메라를 통해 획득된 이미지(: 실제 공간)에 포함된 적어도 하나의 (기존재하 는) 객체를 식별할 수 있다. 이때, 다양한 종류의 객체를 식별하도록 훈련된 적어도 하나의 인공지능 모델이 이 용될 수 있다. 그리고, object predictor는 식별된 객체의 종류를 기반으로, 의미론적 앵커 지점에 위치할 수 있는 객체 의 종류를 식별(예측)할 수 있다. object predictor는, 객체의 종류 별 수가 입력되면, 추가적으로 존재할 수 있는 객체의 종류를 출력하도 록 훈련된 인공지능 모델을 이용할 수 있다. 본 인공지능 모델은 MR 제공 장치의 메모리에 저장된 것 일 수 있다. 구체적으로, object predictor는, 식별된 객체의 수를 종류 별로 인공지능 모델에 입력하여, 추가적 으로 존재할 수 있는 적어도 하나의 객체의 종류를 판단할 수 있다. 이 경우, Extractor는, 판단된 (객체의) 종류에 따라 의미론적 앵커 지점의 특성 정보를 업데이트/생성할 수 있다. 한편, 도 5b는 도 5a에서 이용되는 인공지능 모델을 훈련하기 위한 훈련 데이터를 생성하는 일 예를 설명하기 위한 도면이다. 도 5b를 참조하면, m개의 이미지(images [1 - m]) 각각에서 k개의 종류(클래스)에 대한 객체 인식이 수행될 수 있다. 그 결과, 이미지 별로, k개의 종류(클래스)의 객체에 대한 인식 결과가 종류 별 객체의 수로서 산출될 수 있다. 그리고, 산출된 각 이미지에 대한 종류 별 객체의 수에 따라 훈련 데이터인 행렬이 획득될 수 있다. 여기서, 인공지능 모델은 행렬을 훈련 데이터로 하여 훈련될 수 있다. 관련하여, 도 5c는 본 개시의 일 실시 예에 따른 MR 제공 장치가 도 5b에서 획득된 훈련 데이터를 이용하 여 인공지능 모델을 훈련시키고, 훈련된 인공지능 모델을 이용하여 객체를 예측하는 구체적인 예를설명하기 위한 블록도이다. 도 5c는 종래의 \"Market Basket Analysis\"의 개념을 이용한 것이다. Market Basket Analysis는, 어떤 상품들 (items)이 고객에 의해 빈번하게 함께 구매되는가에 대한 판단을 위한 것이다. 마찬가지로, 본 개시의 실시 예는, 어떤 객체들이 하나의 이미지(또는 실제 공간) 내에 함께 존재하는가를 판단 하게 된다. 따라서, 각 이미지 별로 함께 식별된 객체의 종류 별 수에 대한 정보를 포함하는 도 5b의 행렬은 훈련 데 이터가 될 수 있다. 도 5c에서, S501(S511-S518)은 종래의 논문인 \"A Survey of Collaborative Filtering-Based Recommnder Systems: From Traditional Methods to Hybrid Methods Based on Social Networks\"(Rui Chen, Qinhyi Hua 등)의 훈련 및 rating 과정을 그대로 차용한 것이다. 이때, S511 과정에서, \"User\"는 \"image\"로, \"item\"은 \"object(type)\"로 대체할 필요가 있다. S511의 훈련 데이 터로는, 도 5b에서 획득된 행렬이 이용될 수 있다. 그 결과, 도 5a의 인공지능 모델은, S511-S515의 과정을 통해, 이미지에 추가적으로 존재할 가능성이 높은 적어도 하나의 객체를 예측하도록 훈련될 수 있다. 그 결과, Extractor는 이미지(: 실제 공간이 촬영된)로부터 객체를 인식하고(S521), 식별된 객체의 리스트 를 획득할 수 있다(S522). 그리고, Extractor는 식별된 객체의 리스트를 모델에 입력한 결과(S516-S517), 실제 공간 상에 추가 적으로 존재할 가능성이 가장 높은 객체들(종류)의 리스트를 획득할 수 있다. 여기서, Extractor는 리스트에 따라 기식별된 의미론적 앵커 지점의 특성 정보를 정의할 수 있다. 한편, 도 6a는 본 개시의 일 실시 예에 따른 전자 장치가 특성 정보를 기반으로 비디오 내 객체를 인식하는 동 작을 설명하기 위한 도면이다. 도 6a를 참조하면, 전자 장치의 semantic object recognizer는 MR 제공 장치로부터 수신된 특 성 정보를 이용하여, 비디오에 포함된 이미지 프레임으로부터, 적어도 하나의 객체 영역을 추출할 수 있다. 구체적인 예로, 의미론적 앵커 지점에 위치할 수 있는 객체의 종류(: 특성 정보)가 서 있는 사람 및 앉아 있는 사람인 경우를 가정할 수 있다. 이 경우, 도 6a를 참조하면, semantic object recognizer는 이미지 프레임으로부터 앉아 있는 사람 을 포함하는 객체 영역 및 서 있는 사람을 포함하는 객체 영역을 각각 식별할 수 있다. 이때, semantic object recognizer는 특성 정보에 대응되는 객체를 식별하도록 훈련된 적어도 하나의 인공 지능 모델을 이용할 수 있다. 일 예로, 전자 장치의 메모리에 복수의 종류의 객체를 식별하도록 훈련된 인공지능 모델이 저장된 경 우를 가정한다. 이때, 객체 인식 방식으로는, keypoint estimation 방식, bounding box 방식(1, 2 stage 등) 이때, semantic object recognizer는, 복수의 종류 중 특성 정보(ex. 서 있는 사람, 앉아 있는 사람)에 대응되는 종류를 선택하고, 이미지 프레임 내에서 선택된 종류의 객체를 식별하도록 인공지능 모델을 제어할 수 있다. 관련하여, 도 6b는 본 개시의 일 실시 예에 따른 전자 장치가 특성 정보(: 예측된 객체 리스트)를 기반으로 비 디오 내 객체를 인식하는 동작을 설명하기 위한 도면이다. 도 6b는, 객체 인식 과정에 있어서, 일 예로써 종래의 논문인 \"CenterMask: single shot instance segmentation with point representation\"(Yuqing Wang, Zhaoliang Xu 등)의 의미론적 객체 인식 알고리즘을 차용한 것이다. 도 6b를 참조하면, semantic object recognizer는 비디오에 포함되는 이미지 프레임을 backbone network인 ConvNet에 입력할 수 있다.이때, ConvNet 이후에는 다섯 개의 헤드가 있고, 다섯 개의 헤드의 출력들(621, 622, 623, 624, 625)은 동일한 Height(H)와 Width(W)를 가지지만 채널의 수가 다르다. C는 객체의 종류(클래스)의 수이다. 그리고, S2 는 모양 벡터의 크기이다. 도 6b에서, 히트맵 헤드는 종래의 keypoint estimation pipeline에 따라 센터 포인트들 각각의 위치 및 카테고 리(: 객체의 종류)를 예측할 수 있다. 이때, 출력의 각 채널은 각 카테고리(: 객체의 종류)의 히트맵에 해당한다. 여기서, 본 개시에 따른 semantic object recognizer는, 수신된 특성 정보에 따라, 특성 정보(ex. 도 5c 의 리스트)에 포함된 객체의 종류에 매칭되는 카테고리의 히트맵만을 출력하도록 히트맵 헤드의 연산을 제 어할 수 있다. 즉, 전체 히트맵 레이어들 중, 특성 정보에 포함된 객체의 종류에 매칭되는 카테고리의 히트맵 레이어만이 연산 을 수행할 수 있고, 따라서 연산량이 줄어들 수 있다는 효과가 있다. 히트맵 헤드와 오프셋 헤드의 출력들(624, 625)은 센터 포인트의 위치를 나타낸다. 이때, 센터 포인트는 서로 다른 종류의 객체들마다 별도로 획득될 수 있다. 그리고, 모양 및 크기 헤드들은 센터 포인트의 해당 위치에서 의 Local Shapes를 예측한다. 한편, Saliency 헤드는 Global Saliency Map을 출력하는데, Global Saliency Map상에서 크롭된 객체 영역이 Local Shapes에 곱해져, 이미지 프레임 상에 각 개체를 나타내는 마스크를 형성할 수 있다. 그리고, 형성된 마스크에 따라 최종적인 객체 인식이 완성될 수 있다. 이렇듯, 의미론적 앵커 지점의 특성 정보가 객체 인식에 이용됨에 따라, 전자 장치의 객체 인식 속도가 빨 라질 수 있다. 이는, 실시간 스트리밍에 매우 긍정적인 요소에 해당한다. 한편, 비록 상술한 도 6a에서는 센터 포인트 방식에 따른 객체 인식이 이용되었으나, 도 6a는 일 예일 뿐, semantic object recognizer의 객체 인식 방식이 도 6a와 같은 센터 포인트 방식에만 한정되는 것은 아니 며, 바운딩 박스(1 size patch, multi-size patch 등) 기반 객체 인식, 엣지 포인트 기반 객체 인식 등 다양한 방식이 이용될 수 있음은 물론이다. 도 7은 본 개시의 일 실시 예에 따른 MR 제공 장치가 사용자의 시야 내에 표시될 객체 영역의 위치를 판단하는 동작을 설명하기 위한 도면이다. 도 7을 참조하면, MR 제공 장치의 Object positioning module은 inpainting module 및 synthesizer 중 적어도 하나를 포함할 수 있다. Inpainting module은 전자 장치로부터 수신된 객체 영역에 불완전한 부분이 있는 경우, 이를 보완하 기 위한 모듈이다. 구체적으로, inpainting module은 전자 장치로부터 수신된 객체 영역에 포함된 객체에서 생략된 부분 을 새롭게 생성할 수 있다. 예를 들어, 전자 장치의 비디오를 구성하는 이미지 프레임 내에서 객체(ex. 서 있는 사람)의 일부(ex. 오 른쪽 다리 아래 부분)가 다른 객체에 의해 가려진 경우, 또는 이미지 프레임 내에 객체(ex. 서 있는 사람)의 전 체가 아닌 일부 신체 부위만 나와 있는 경우를 가정할 수 있다. 이 경우, 추출된 객체 영역 내에, 객체(ex. 서 있는 사람)의 일부(ex. 오른쪽 다리 아래 부분)는 포함되지 않은 상태일 수 있다. 여기서, inpainting module은 전자 장치에서 수신된 객체 영역에 포함된 객체(ex. 서 있는 사람)의 모습이 완전한지 판단하고, 불완전한 부분(ex. 오른쪽 다리 아래 부분)을 생성하여 객체 영역을 보완할 수 있다. 그 결과, 기존에 비디오에 포함되지 않았던 객체의 일부가 객체의 기존재하는 부분들에 맞게 생성될 수 있고, 사용자는 MR 제공 장치를 통해 완전한 형태를 갖춘 가상의 객체 이미지를 제공받을 수 있게 된다. 이를 위해, inpainting module은 불완전하게 그려진 객체의 적어도 일부를 보완하기 위한 적어도 하나의 GAN을 이용할 수 있는바, 이미지 내에서 일부가 생략된 객체를 재구성하는 종래의 기술(ex. SeGAN: Segmenting and Generating the Invisible. Kiana Ehsani, Roozbeh Mottaghi 등)이 이용될 수 있다.Synthesizer는 실제 공간을 바라보는 사용자의 시야각 범위 내에서 객체 영역을 합성하기 위한 모듈이다. synthesizer는 사용자의 시야각 범위 내에 디스플레이될 객체 영역의 위치 및/또는 뎁스를 판단할 수 있다. 일 실시 예에 따르면, synthesizer는 MR 제공 장치와 의미론적 앵커 지점 간의 거리, 객체 영역의 이 미지 프레임(비디오) 내 위치 등을 이용하여 객체 영역의 위치를 판단할 수 있다. 관련하여, 도 8은, MR 제공 장치가, MR 제공 장치와 의미론적 앵커 지점 간의 거리 및 객체 영역들 간의 위치 관계를 이용하여 객체 영역들의 위치를 판단하는 동작을 설명하기 위한 도면이다. 도 8은, 서 있는 사람이 위치할 수 있는 36개의 의미론적 앵커 지점들 및 앉아있는 사람이 위치할 수 있는 5개 의 의미론적 앵커 지점들이 식별된 경우를 가정한다. 또한, 도 8은, 전자 장치로부터 수신된 (특성 정보에 대응되는) 객체 영역들(21, 22)이 각각 앉아 있는 사람 및 서 있는 사람을 포함하는 경우를 가정한다. 이미지 는 카메라를 통해 촬영된 실제 공간의 이미지이다. 여기서, synthesizer는 MR 제공 장치와의 거리가 비교적 가까운 의미론적 앵커 지점을 선택할 수 있 다. 도 8을 참조하면, synthesizer는 앉아 있는 사람이 위치할 수 있는 의미론적 앵커 지점들 중 첫 번째 의미 론적 앵커 지점의 위치를, 객체 영역의 위치로 판단할 수 있다. 또한, synthesizer는, 비디오의 이미지 프레임(ex. 도 1의 20) 내 객체 영역들(21, 22) 간의 위치 관계 (ex. 거리, 방향 등)가 유지되도록, 판단된 객체 영역의 위치를 고려하여 객체 영역의 위치를 판단할 수 있다. 그 결과, 도 8을 참조하면, synthesizer는 서 있는 사람이 위치할 수 있는 36개의 의미론적 앵커 지점 중 9번째 의미론적 앵커 지점을 객체 영역의 위치로 판단할 수 있다. 다만, 도 8은 일 예일 뿐, MR 제공 장치와 의미론적 앵커 지점 간의 거리 및/또는 객체 영역들 간의 위치 관계를 이용하는 방식이 도 8의 예에만 한정되는 것은 아니고, 통상의 기술 수준에서 다양하게 변형 가능함은 물론이다. 한편, synthesizer는, 각 의미론적 앵커 지점 상에 기존재하는 객체의 종류 및/또는 크기에 따라, 새롭게 추가될 객체 영역의 위치를 판단할 수도 있다. 이하 도 9a 및 도 9b를 통해 구체적인 예를 설명한다. 도 9a는, 실제 공간 내에서, 세 개의 의미론적 앵커 지점들(911, 912, 913)이 식별된 경우를 가정한다. 또한, 도 9a는, 의미론적 앵커 지점 상에는 노트북이 기존재하고 있고, 의미론적 앵커 지점 상 에는 연필꽂이들(922, 923)이 기존재하고 있는 상황을 가정한다. 여기서, 객체들(921, 922, 923)은 상술한 Extractor에 의해 인식된 것들일 수 있다. 이때, synthesizer는 기존재하는 객체의 종류(노트북, 연필통 등) 및 각 객체의 크기 등을 식별할 수 있다. 그 결과, 의미론적 앵커 지점들(911, 912, 913) 각각에 기존재하는 객체들의 종류 및 크기에 대한 정보가 획득될 수 있다. 그리고, 도 9b와 같이 수신된 객체 영역이 컵을 포함하는 경우, synthesizer는 컵의 크기(ex. 높이) 에 따라 적어도 하나의 의미론적 앵커 지점을 선택할 수 있다. 도 9b를 참조하면, 실제 공간 내에 기존재하는 객체들 중 의미론적 앵커 지점에 존재하는 연필꽂이들 (922, 923)의 크기/높이가 컵의 크기/높이와 가장 유사하므로, synthesizer는 의미론적 앵커 지점을 객체 영역이 위치로 선택할 수 있다. 한편, 일 실시 예에 따르면, Synthesizer는 카메라가 촬영한 실제 공간의 이미지에 객체 영역을 합성 하기 위한 GAN을 이용할 수도 있다. 관련하여, 도 10은 본 개시의 일 실시 예에 따른 MR 제공 장치가 GAN 모델을 이용하여 객체 영역의 위치를 판단 하는 동작을 설명하기 위한 도면이다. 도 10을 참조하면, synthesizer는 GAN에 해당하는 synthesizer network, target network, discriminator 등을 이용할 수 있다.Synthesizer network는 이미지에 객체 영역을 합성하여 합성 이미지를 생성하도록 훈련된 네트워크인 바, Target network를 속이도록 업데이트 된다. Target network 역시 합성 이미지를 통해 훈련될 수 있으며, Discriminator는 합성 이미지의 품질을 향상시키도록 Synthesizer netowork에 피드백을 제공할 수 있는바, 많은 수의 실제 이미지를 기반으로 훈 련될 수 있다. 다만, 도 10의 예는 종래 기술의 일 예(Learning to Generate Synthetic Data via Compositing. Shashank Tripathi, Siddhartha Chandra 등)를 이용한 것일 뿐, 이밖에도 다양한 형태/방식의 GAN이 이용될 수 있다. 한편, 일 실시 예에 따르면, Synthesizer는, 객체를 포함하지 않는 이미지가 입력되면 해당 이미지에 위치 할 수 있는 객체에 대한 정보를 포함하는 saliency map을 출력하도록 훈련된 GAN을 이용할 수도 있다. 비교적 단순한 형태(binary mask)로 객체의 위치(좌표)를 나타내는 saliency map을 이용함으로써, 실제 공간을 바라보는 사용자의 시야 상에 배치될 객체 영역의 위치가 빠르게 판단할 수 있다. 이때, GAN을 훈련시키기 위해, 객체를 포함하는 이미지 프레임과, 동일한 공간을 나타내지만 객체를 포함하지 않는 이미지 프레임이 필요하다. 이하 도 11a 내지 도 11b는 해당 GAN을 훈련시키는 과정의 일 예를 설명하기 위한 것이다. 도 11a를 참조하면, 비디오에 포함된 복수의 이미지 프레임을 순차적으로 입력받아 시공간 특징을 추출하기 위 한 Encoder network, 및 시공간 특징으로부터 객체에 대한 정보를 saliency map 형태로 추출하기 위한 ediction network를 포함하는 네트워크가 이용될 수 있다(참조: TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection. Kyle Min, Jason J. Corso). 그 결과, 객체를 포함하는 이미지 프레임 및 객체에 대한 정보를 포함하는 saliency map의 페어가 획득될 수 있 다. 더하여, 해당 이미지 프레임과 동일한 공간을 나타내지만 객체를 포함하지 않는 이미지 프레임이 필요하다. 그리고, 도 11b를 참조하면, 객체를 포함하지 않는 이미지 프레임 및 saliency map이 각각 입출력 으로서 GAN의 훈련 데이터 셋이 될 수 있다. 여기서, saliency map은, 객체를 포함하는 이미지 프레임을 도 11a와 같은 네트워크들(1110, 1120)에 입력함으로써 획득될 수 있다. 도 11a 내지 도 11b의 과정을 거친 결과, synthesizer의 GAN은, 실제 공간을 촬영한 이미지 내에 추가될 객체 영역의 위치를 판단할 수 있다. 한편, 도 12는 본 개시의 다양한 실시 예에 따른 MR 제공 장치의 상세한 구성을 설명하기 위한 블록도이다. 도 12를 참조하면, MR 제공 장치는 카메라, 광학 디스플레이부, 통신부 및 프로세서 외에도, 센서, 스피커, 사용자 입력부 등을 더 포함할 수 있다. 통신부는, 상술한 외부의 전자 장치 이외에도 다양한 외부 장치와 통신을 수행할 수 있다. 일 예로, 상술한 프로세서의 동작들 중 적어도 하나는, 통신부를 통해 MR 제공 장치와 통신할 수 있는 적어도 하나의 외부 제어 장치를 통해 수행될 수도 있다. 예를 들어, MR 제공 장치의 부피를 줄이기 위해, 상술한 프로세서의 기능을 대부분 수행하는 별도의 외부 컴퓨팅 장치가 통신부를 통해 MR 제공 장치와 연결될 수 있다. 또한, MR 제공 장치에 대한 사용자 명령을 입력하기 위한 별도의 원격 제어 장치가 있는 경우, 원격 제어 장치(ex. 사용자 모션 입력 장치)를 통해 입력되는 사용자 명령에 대한 정보 역시 통신부를 통해 수신될 수 있다. 통신부는 무선 통신 또는 유선 통신을 통해 하나 이상의 외부 장치와 통신을 수행할 수 있다. 무선 통신은 LTE(long-term evolution), LTE-A(LTE Advance), 5G(5th Generation) 이동통신, CDMA(code division multiple access), WCDMA(wideband CDMA), UMTS(universal mobile telecommunications system), WiBro(Wireless Broadband), GSM(Global System for Mobile Communications), DMA(Time Division Multiple Access), WiFi(Wi-Fi), WiFi Direct, Bluetooth, NFC(near field communication), Zigbee 등의 통신 방식 중적어도 하나를 포함할 수 있다. 유선 통신은 이더넷(Ethernet), 광 네트워크(optical network), USB(Universal Serial Bus), 선더볼트 (ThunderBolt) 등의 통신 방식 중 적어도 하나를 포함할 수 있다. 여기서, 통신부는 상술한 유무선 통신 방식에 따른 네트워크 인터페이스(Network Interface) 또는 네트워크 칩을 포함할 수 있다. 한편, 통신부는 외부 장치와 직접적으로 연결될 수도 있지만, 네트워크를 제공하는 하나 이상의 외부 서버 (ex. ISP(Internet Service Provider)) 및/또는 중계 장치를 통해서 외부 장치와 연결될 수도 있다. 네트워크는 영역 또는 규모에 따라 개인 통신망(PAN; Personal Area Network), 근거리 통신망(LAN; Local Area Network), 광역 통신망(WAN; Wide Area Network) 등일 수 있으며, 네트워크의 개방성에 따라 인트라넷 (Intranet), 엑스트라넷(Extranet), 또는 인터넷(Internet) 등일 수 있다. 한편, 통신 방식은 상술한 예에 한정되지 아니하고, 기술의 발전에 따라 새롭게 등장하는 통신 방식을 포함할 수 있다. 프로세서는 MR 제공 장치의 적어도 하나의 메모리와 연결되어 MR 제공 장치를 제어할 수 있다. 이를 위해, 프로세서는 하드웨어적으로 CPU(central processing unit), GPU(Graphic processing unit), NPU(neural processing unit) 등을 포함할 수 있으며, MR 제공 장치에 포함된 다른 구성요소들의 제어에 관한 연산이나 데이터 처리를 실행할 수 있다. 프로세서는 전자 장치에 포함된 하드웨어적 구성요소뿐만 아니라, MR 제공 장치에 포함되는 하나 이상의 소프트웨어 모듈을 제어할 수도 있으며, 프로세서가 소프트웨어 모듈을 제어한 결과가 하드웨어 구 성들의 동작으로 도출될 수도 있다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU, AP 등 과 같은 범용 프로세서, GPU, VPU 등과 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공지능 전용 프로세서일 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 기정의된 동작 규칙 또는 인공지능 모델은 학습(훈련)을 통해 만들어진 것을 특징으로 한 다. 여기서, 학습을 통해 만들어진다는 것은, 다수의 학습 데이터들에 학습 알고리즘을 적용함으로써, 원하는 특성 의 기정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버/시스템을 통해 이루어 질 수도 있다. 학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기(예컨대, 로봇)을 훈련시켜 소정의 대상 기기 스스로 결정을 내리거나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술 한 예에 한정되지 않는다. 센서는 MR 제공 장치의 주변 정보를 획득하기 위한 구성이다. 센서는 IMU(Inertial Measurement Unit) 센서, GPS(Global Position System) 센서, 지자기 센서 등 다양 한 센서를 포함할 수 있다. 프로세서는, 뎁스 카메라를 통해 획득된 뎁스 정보(ex. 라이다 센서의 데이터) 및/또는 IMU 센서의 데이터 등을 이용하여 SLAM(Simultaneous Localization and Mapping)을 수행함으로써, MR 제공 장치를 통해 사용 자가 바라보는 실제 공간의 3D 맵을 구성함과 동시에 MR 제공 장치(사용자)의 맵상 위치를 추적할 수 있다. 실제 공간 상에서 수평면(: 의미론적 앵커 지점의 후보)을 식별하는 Extractor의 상술한 동작 역시 SLAM 과정에서 함께 수행될 수 있다. 한편, 프로세서는 스테레오 카메라를 통해 획득된 이미지를 이용하여 visual SLAM을 수행할 수도 있다. 스피커는 사운드를 출력하기 위한 구성이다. 전자 장치로부터 비디오에 포함되는 오디오 신호가 수신되면, 프로세서는, 수신된 오디오 신호에 해 당하는 사운드를 출력하도록 스피커를 제어할 수 있다. 그 결과, 시각적으로 객체 영역들이 제공됨과 동시에, 청각적으로는 비디오의 사운드가 제공될 수 있다. 사용자 입력부는 사용자 명령/정보를 수신하기 위한 구성이다. 사용자 입력부는 적어도 하나의 버튼, 마이크, 터치 센서, 모션 센서 등 다양한 구성을 포함할 수 있다. 또한, MR 제공 장치가 HMD 또는 AR/MR 글래스로 구현된 경우, 사용자 입력부는 사용자의 MR 제공 장 치 착용 여부를 판단하기 위한 적어도 하나의 접촉/근접 센서를 포함할 수 있다. 예를 들어, MR 제공 장치가 착용된 상태에서, immersive mode를 활성화하기 위한 사용자 명령이 수신된 경 우, 프로세서는 전자 장치와 통신을 수행하는 한편 Extractor 및 object positing module(14 2)을 이용하여 상술한 동작들을 수행할 수 있다. 그 결과, 전자 장치가 제공하는 비디오의 적어도 일부(객체 영역, 사운드)가 MR 제공 장치를 통해 실 제 공간 상에 제공될 수 있다. 한편, 도 13은 본 개시의 다른 실시 예에 따라 디스플레이를 이용하여 MR을 제공하는 MR 제공 장치의 구성을 설 명하기 위한 블록도이다. 상술한 도면들을 통해서는, MR 제공 장치가 광학 디스플레이부를 이용하는 실시 예들만이 설명되었으 나, 광학 디스플레이부 대신 일반적인 디스플레이(120')를 이용하는 MR 제공 장치(100') 역시 본 개시의 실시 예가 될 수 있다. 본 MR 제공 장치(100')는 스마트폰, 태블릿 PC 등으로 구현될 수 있다. 본 MR 제공 장치(100')는, Extractor의 동작을 수행하고 특성 정보에 따른 객체 영역을 수신한다는 점에서 상술한 MR 제공 장치와 동일하나, 최종적으로 MR을 제공하는 과정에 있어서 차이가 있다. 구체적으로, 프로세서(140')는 의미론적 앵커 지점의 특성 정보에 따른 객체 영역이 전자 장치로부터 수신 되면, 카메라를 통해 실제 공간을 촬영한 이미지에 해당 객체 영역을 합성할 수 있다. 이때, 적어도 하나 의 GAN이 이용될 수 있다. 그리고, 프로세서(140')는 합성된 이미지를 디스플레이하도록 디스플레이(120')를 제어할 수 있다. 즉, 본 MR 제공 장치(100')는, 광학 디스플레이부를 이용하여 실제 공간 상에 가상 이미지(: 객체 영역)를 보여주는 것이 아니라, 실제 공간이 촬영된 이미지 및 가상 이미지가 합성된 합성 이미지를 생성하여 디스플레 이하는 것이다. 이 경우, 실제 공간 자체도 딜레이를 거쳐 보이게 된다는 문제가 있지만, 기존의 스마트폰 또는 태블릿 PC 등으 로도 MR을 제공할 수 있다는 장점이 있다. 한편, 도 14는 본 개시의 일 실시 예에 따른 MR 제공 장치의 제어 방법을 설명하기 위한 순서도이다. 본 MR 제 공 장치는, 기설정된 시야각 범위 내의 현실 공간 및 가상의 이미지를 제공하기 위한 것으로, 광학 디스플레이 부 및/또는 디스플레이를 포함할 수 있다. 도 14를 참조하면, 본 제어 방법은, 카메라를 통해 기설정된 시야각 범위를 촬영하여 (실제 공간의) 이미지를 획득할 수 있다(S1410). 이때, 카메라는, RGB 카메라 및/또는 뎁스 카메라를 포함할 수 있다. 그리고, 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점(semantic anchor spot)을 식별할 수 있다(S1420). 이때, 카메라를 통해 획득된 이미지의 복수의 픽셀의 뎁스 정보를 획득하고, 획득된 뎁스 정보를 기반으로, 획 득된 이미지 내에서 적어도 하나의 수평면을 식별할 수 있다. 그리고, 식별된 적어도 하나의 수평면의 넓이 및 수직 방향의 높이에 기초하여, 식별된 적어도 하나의 수평면 중 객체가 위치할 수 있는 의미론적 앵커 지점을 식별하 수 있다. 한편, MR 제공 장치의 메모리에, 이미지가 입력되면, 입력된 이미지에 포함된 의미론적 앵커 지점 및 특성 정보 를 추출하도록 훈련된 인공지능 모델이 포함된 경우를 가정할 수 있다.이 경우, 카메라를 통해 획득된 이미지를 인공지능 모델에 입력하여, 획득된 이미지 내에서 객체가 위치할 수 있는 적어도 하나의 의미론적 앵커 지점을 식별할 수 있다. 한편, 본 제어 방법은, 획득된 이미지에 포함된 적어도 하나의 객체를 식별할 수 있다. 그리고, 식별된 객체의 종류를 기반으로, 의미론적 앵커 지점에 위치할 수 있는 객체의 종류를 판단할 수 있다. 이 경우, 판단된 객체의 종류를 기반으로, 의미론적 앵커 지점의 특성 정보를 생성할 수 있다. 구체적인 예로, MR 제공 장치의 메모리에는, 객체의 종류 별 수가 입력되면, 추가적으로 존재할 수 있는 객체의 종류를 출력하도록 훈련된 인공지능 모델이 포함될 수 있다. 이때, 획득된 이미지로부터 식별된 객체의 수를 종류 별로 인공지능 모델에 입력하여, 추가적으로 존재할 수 있 는 적어도 하나의 객체의 종류를 판단할 수 있다. 상술한 실시 예들에 따라 의미론적 앵커 지점 및 그 특성 정보가 식별되면, 본 제어 방법은, 의미론적 앵커 지 점의 특성 정보(charicteristic information)를 외부의 전자 장치로 전송할 수 있다(S1420). 이때, 전자 장치는, 비디오의 이미지 프레임으로부터 특성 정보에 따른 객체를 식별 및 추적함으로써 이미지 프 레임 별로 객체 영역을 추출할 수 있다. 그리고, 전자 장치가 제공하는 비디오의 이미지 프레임에 포함된 적어도 하나의 객체 중 특성 정보에 대응되는 객체가 포함된 객체 영역을 전자 장치로부터 수신할 수 있다(S1430). 그리고, 본 제어 방법은, 의미론적 앵커 지점 상에 수신된 객체 영역을 디스플레이할 수 있다(S1440). 여기서, 본 제어 방법은, 객체 영역이 디스플레이 될 위치를 판단하고, 판단된 위치에 따라 객체 영역을 디스플 레이할 수 있다. 일 예로, 본 제어 방법은, MR 제공 장치와 의미론적 앵커 지점 간의 거리 및/또는 비디오의 이미지 프레임 내 객체 영역의 위치 정보를 이용할 수 있다. 구체적인 예로, 획득된 이미지 내에서 복수의 의미론적 앵커 지점이 식별되고, 전자 장치로부터 복수의 객체 영 역이 수신된 경우를 가정한다. 이때, 복수의 의미론적 앵커 지점 각각과 MR 제공 장치 간의 거리 및 이미지 프레임 내에서 복수의 객체 영역 간의 위치 관계에 기초하여, 복수의 의미론적 앵커 지점 중 수신된 복수의 객체 영역 각각이 위치하기 위한 의 미론적 앵커 지점들을 선택할 수 있다. 그리고, 선택된 의미론적 앵커 지점들 각각 상에 복수의 객체 영역 각각을 디스플레이할 수 있다. 복수의 의미론적 앵커 지점이 식별된 경우, 실제 공간을 촬영한 이미지 내에서 복수의 의미론적 앵커 지점 각각 에 존재하는 객체의 종류 또는 크기를 함께 식별할 수 있다. 이 경우, 식별된 객체의 종류 또는 크기를 기반으로, 복수의 의미론적 앵커 지점 중 수신된 객체 영역이 위치하 기 위한 의미론적 앵커 지점을 선택할 수 있다. 그리고, 선택된 의미론적 앵커 지점 상에 수신된 객체 영역을 디스플레이할 수 있다. 한편, 이미지에 적어도 하나의 객체 영역을 합성하도록 훈련된 GAN에 획득된 이미지(실제 공간)를 입력하여, 수 신된 객체 영역이 디스플레이되는 위치를 식별할 수도 있다. 이 경우, 식별된 위치를 기반으로, 수신된 객체 영 역을 디스플레이할 수 있다. 도 15는 본 개시의 구체적인 일 실시 예에 따른 MR 제공 장치 및 전자 장치의 제어 방법을 설명하기 위한 알고 리즘이다. 도 15는 MR 제공 장치가 HMD로 구현되고, 전자 장치가 비디오 컨텐츠를 제공하는 TV로 구현된 경우를 가정한다. 또한, 도 15는, HMD가 사용자에 착용되고, HMD와 TV가 서로 통신 가능한 상태를 가정한다. 여기서, 도 15를 참조하면, HMD 및 TV의 immersive search mode가 활성화될 수 있다(S1505). immersive search mode는, 실제 공간 및 비디오가 결합된 MR을 제공하는 immersive mode를 수행할 수 있는지 여부를 판단하기 위한 모드에 해당한다. 구체적인 예로, HMD에 입력된 사용자의 명령에 따라 HMD 및 TV의 immersive search mode가 활성화될 수 있다. 이 경우, HMD는 현재 사용자(: HMD)의 위치를 식별할 수 있다(S1510). 이때, GPS 센서가 이용되거나 또는 적어 도 하나의 중계기(ex. WiFi 공유기)가 이용될 수 있다. 또는, HMD가 주변을 촬영하여 획득된 이미지를 기저장된 다양한 위치의 이미지들과 비교함으로써, 현재 위치를 식별할 수도 있다. 그리고, 해당 장소에, 이전에 식별된 의미론적 앵커 지점이 있었는지 식별할 수 있다(S1515). 이때, HMD는 현재 위치에서 의미론적 앵커 지점이 식별된 히스토리 정보를 이용할 수 있다. 여기서, 히스토리 정보는, HMD가 식별한 의미론적 앵커 지점 및 그 특성 정보가 의미론적 앵커 지점이 식별된 위치와 매칭되어 저 장된 정보일 수 있다. 만약, 이전에 식별된 의미론적 앵커 지점이 있는 경우(S1515), HMD는 해당 의미론적 앵커 지점이 현재 이용 가 능한지 판단할 수 있다(S1520). 구체적으로, 해당 지점 상에 이미 다른 물체들이 놓여있지 않은지 식별할 수 있 다. 그리고, 의미론적 앵커 지점이 이용 가능한 경우(S1520 - Y), 해당 의미론적 앵커 지점의 특징 정보를 TV로 전 송할 수 있다(S1530). 한편, 이전에 식별된 의미론적 앵커 지점이 없었던 경우(S1515 - N) 또는 이전에 식별된 의미론적 앵커 지점이 현재 이용 가능하지 않은 경우(S1520 - N), HMD는 사용자가 바라보는 이미지(: 카메라를 통해 촬영)로부터 의미 론적 앵커 지점을 식별할 수 있다(S1525). 그리고, 식별된 의미론적 앵커 지점의 특성 정보를 TV로 전송할 수 있다(S1525). TV는, 수신된 특징 정보를 기반으로 비디오 내 객체를 식별할 수 있다(S1535). 만약 특징 정보에 맞는 객체가 비디오의 이미지 프레임 내에서 식별되지 않는 경우(S1540 - N), TV는 가능한 객 체 영역이 없음을 알리는 정보를 HMD로 전송할 수 있다. 그리고, HMD는 immersive mode를 수행할 수 없음을 알 리는 UI(User Interface)를 시각적(가상 이미지) 또는 청각적으로 제공할 수 있다(S1545). 반면, 특정 정보에 맞는 객체가 비디오의 이미지 프레임 내에서 식별된 경우(S1540 - Y), TV는 가능한 객체 영 역이 있음을 알리는 정보를 HMD로 전송할 수 있다. 이 경우, HMD 및 TV의 immersive mode가 활성화될 수 있다 (S1550). 여기서, HMD는 immersive mode의 활성화 여부를 문의하기 위한 UI를 사용자에게 제공할 수 있다. 그리고, immersive mode를 활성화하기 위한 사용자 명령이 입력되는 경우, HMD 및 TV의 immersive mode가 활성화될 수 도 있다. immersive mode가 활성화되면, TV는 이미지 프레임 별로 식별된 객체 영역을 HMD로 스트리밍할 수 있다(S1555). 그리고, HMD는, 실시간으로 수신되는 객체 영역을, 의미론적 앵커 지점 상에 가상 이미지로써 디스플레이할 수 있다(S1560). 그 결과, 실제 공간과 비디오의 객체 영역이 결합된 MR이 제공될 수 있다. 이밖에도 다양한 응용례가 가능하다. 일 실시 예로, MR 제공 장치는 모션 등으로 입력되는 사용자의 명령에 따라 실제 공간 상에서 의미론적 앵커 지 점을 선택할 수도 있다. 이 경우, 해당 의미론적 앵커 지점에 위치할 객체 역시 사용자 명령(ex. 사용자의 음성)에 따라 선택될 수도 있 다. 일 예로, 토크 쇼를 스트리밍 받고자 하는 경우, 사용자의 명령에 따라 토크 쇼 내의 인물들 각각이 위치할 지 점들이 설정될 수도 있다. 한편, 일 실시 예에 따르면, MR 제공 장치의 immersive mode가 활성화된 경우, MR 제공 장치는 제공될 객체 영 역의 크기를 사용자의 명령에 따라 다르게 설정할 수 있다. 일 예로, 토크 쇼를 스트리밍 받는 경우, MR 제공 장치는 full 또는 small 중 어느 하나를 선택받기 위한 UI를 사용자에게 제공할 수 있다. 만약, full이 선택되는 경우, MR 제공 장치는 토크 쇼 내 인물들의 객체 영역을 해당 인물들의 실제 크기로 의 미론적 앵커 지점(ex. 바닥면, 소파, 의자 등) 상에 디스플레이할 수 있다.반면, small이 선택되는 경우, MR 제공 장치는 토크 쇼 내 인물들의 객체 영역을 실제보다 훨씬 작은 크기로 의 미론적 앵커 지점(ex. 식탁, 접시 등) 상에 디스플레이할 수 있다. 이 경우, 토크 쇼 내 인물들이 아주 작은 크 기로 표현될 수 있다. 즉, 동일한 객체를 포함하는 객체 영역이라고 하더라도, 객체 영역이 제공되는 크기에 따라, 객체 영역이 위치 할 의미론적 앵커 지점이 달라질 수 있다. 한편, 상술한 MR 제공 장치 및/또는 전자 장치의 제어 방법은, 도 2, 도 12, 도 13 등을 통해 도시 및 설명한 MR 제공 장치(100 또는 100') 및/또는 전자 장치를 통해 수행될 수 있다. 또는, 상술한 MR 제공 장치 및/또는 전자 장치의 제어 방법은, MR 제공 장치(100 또는 100') 및/또는 전자 장치 외에 적어도 하나의 외부 장치를 더 포함하는 시스템을 통해 수행될 수도 있다. 상술한 실시 예들에 따르면, MR 제공 장치를 착용한 사용자는 실제 공간 상에서 다양한 일(ex. 식사, 공부, 요 리 등)을 하면서, 동시에 비디오 컨텐츠를 제공받을 수 있다. 단순히 가상의 TV를 실제 공간의 벽면에 디스플레 이하는 경우와 달리, 사용자는 시선을 돌릴 필요가 없으며 더욱 몰입감 넘치는 MR을 경험할 수 있다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합된 것 을 이용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 하드웨어적인 구현에 의하면, 본 개시에서 설명되는 실시 예들은 ASICs(Application Specific Integrated Circuits), DSPs(digital signal processors), DSPDs(digital signal processing devices), PLDs(programmable logic devices), FPGAs(field programmable gate arrays), 프로세서(processors), 제어기 (controllers), 마이크로 컨트롤러(micro-controllers), 마이크로 프로세서(microprocessors), 기타 기능 수행 을 위한 전기적인 유닛(unit) 중 적어도 하나를 이용하여 구현될 수 있다. 일부의 경우에 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 상술한 소프트웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 작동을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 MR 제공 장치 및/또는 전자 장치에서의 처리동작을 수행하기 위한 컴퓨터 명령어(computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium) 에 저장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령 어는 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 MR 제공 장치 및/또는 전 자 장치에서의 처리 동작을 상술한 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2020-0128589", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2020-0128589", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시에 따른 MR 제공 장치의 동작을 개략적으로 설명하기 위한 도면, 도 2는 본 개시의 일 실시 예에 따른 MR 제공 장치 및 전자 장치 각각의 구성 및 동작을 설명하기 위한 블록도, 도 3a 내지 도 3c는 본 개시의 일 실시 예에 따른 MR 제공 장치가 수평면의 넓이 및 높이를 기반으로 의미론적 앵커 지점을 식별하는 동작을 설명하기 위한 도면들, 도 4a는 본 개시의 일 실시 예에 따른 MR 제공 장치가 인공지능 모델을 이용하여 의미론적 앵커 지점을 식별하 는 동작을 설명하기 위한 도면, 도 4b 내지 도 4c는 도 4a에서 이용되는 인공지능 모델의 학습 과정의 일 예를 설명하기 위한 도면들, 도 5a는 본 개시의 일 실시 예에 따른 MR 제공 장치가 기존재하는 객체들의 종류 별 수를 이용하여 의미론적 앵 커 지점에 위치할 수 있는 객체를 예측하는 동작을 설명하기 위한 도면, 도 5b는 도 5a에서 이용되는 인공지능 모델을 훈련하기 위한 훈련 데이터를 생성하는 일 예를 설명하기 위한 도 면, 도 5c는 본 개시의 일 실시 예에 따른 MR 제공 장치가 도 5b에서 획득된 훈련 데이터를 이용하여 도 5a의 인공 지능 모델을 훈련시키고, 훈련된 인공지능 모델을 이용하여 객체를 예측하는 구체적인 예를 설명하기 위한 블록 도, 도 6a는 본 개시의 일 실시 예에 따른 전자 장치가 특성 정보를 기반으로 비디오 내 객체를 인식하는 동작을 설 명하기 위한 도면, 도 6b는 본 개시의 일 실시 예에 따른 전자 장치가 특성 정보(: 예측된 객체 리스트)를 기반으로 비디오 내 객 체를 인식하는 동작을 설명하기 위한 도면, 도 7은 본 개시의 일 실시 예에 따른 MR 제공 장치가 사용자의 시야 내에 표시될 객체 영역의 위치를 판단하는 동작을 설명하기 위한 도면, 도 8은 본 개시의 일 실시 예에 따른 MR 제공 장치가, MR 제공 장치와 의미론적 앵커 지점 간의 거리 및 객체 영역들 간의 위치 관계를 이용하여 객체 영역들의 위치를 판단하는 동작을 설명하기 위한 도면, 도 9a는 본 개시의 일 실시 예에 따른 MR 제공 장치가 의미론적 앵커 지점들 및 의미론적 앵커 지점들 각각에 기존재하는 객체들을 인식하는 동작을 설명하기 위한 도면, 도 9b는 본 개시의 일 실시 예에 따른 MR 제공 장치가 선택된 의미론적 앵커 지점 상에 객체 영역을 위치시키는 동작을 설명하기 위한 도면, 도 10은 본 개시의 일 실시 예에 따른 MR 제공 장치가 GAN 모델을 이용하여 객체 영역의 위치를 판단하는 동작 을 설명하기 위한 도면, 도 11a는 객체 영역의 위치를 판단하는 데에 이용되는 GAN 모델의 훈련 데이터를 생성하는 예를 설며하기 위한 도면, 도 11b는 GAN 모델의 훈련 데이터의 일 예를 설명하기 위한 도면, 도 12는 본 개시의 다양한 실시 예에 따른 MR 제공 장치의 상세한 구성을 설명하기 위한 블록도, 도 13은 본 개시의 일 실시 예에 따라 디스플레이를 이용하여 MR을 제공하는 MR 제공 장치의 구성 및 동작을 설 명하기 위한 블록도, 도 14는 본 개시의 일 실시 예에 따른 MR 제공 장치의 제어 방법을 설명하기 위한 순서도, 그리고 도 15는 본 개시의 구체적인 일 실시 예에 따른 MR 제공 장치 및 전자 장치의 제어 방법을 설명하기 위한 알고 리즘이다."}
