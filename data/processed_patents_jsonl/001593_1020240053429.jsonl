{"patent_id": "10-2024-0053429", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0058070", "출원번호": "10-2024-0053429", "발명의 명칭": "NPU를 포함하는 엣지 디바이스 그리고 동작 방법", "출원인": "주식회사 딥엑스", "발명자": "김녹원"}}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "엣지 디바이스에 포함되는 NPU(neural processing unit)로서,적어도 하나의 내부 메모리와;복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(inparallel) 처리하도록 설정가능한 복수의 PE(processing element)들과; 그리고스케줄러를 포함하고, 상기 복수의 ANN 모델들 중 제1 ANN 모델은 상시 동작되고, 제2 ANN 모델은 조건적으로 구동되고, 상기 스케줄러는 상기 제1 ANN 모델을 위한 제1 연산들을 제1 그룹의 PE들에게 할당하고, 상기 제1 연산들의 추론 결과에 기초하여 조건적으로 상기 제2 ANN 모델을 위한 제2 연산들을 제2 그룹의 PE들에게 할당함으로써 상기 엣지 디바이스의 소비 전력을 저감시키는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 엣지 디바이스는, 상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위한 제2 연산들을 수행한 추론 결과에 따라 제1 모드 또는 제2 모드로 동작되는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 스케줄러는 ANN 데이터 지역성 정보를 고려하여 할당을 수행하는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 ANN 데이터 지역성 정보는상기 적어도 하나의 내부 메모리의 크기에 대한 정보와;상기 적어도 하나의 내부 메모리의 계층(hierarchy) 구조에 대한 정보와;상기 복수의 PE들의 개수에 대한 정보와; 상기 복수의 PE들의 연산기 구조에 대한 정보와; 그리고상기 NPU에 대한 구조 정보 중에서 하나 이상을 포함하는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제1 ANN 모델을 위한 제1 연산들과 상기 제2 ANN 모델을 위한 제2 연산들은 병렬적(inparallel) 또는 시분할(time division) 방식으로 수행되는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들은 부분적으로 동일하거나, 서로 완전히 다른,NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서, 상기 스케줄러가 상기 할당들을 수행할 때, 상기 복수의 ANN들의 연산 순서에 대한 정보와 관련된 명령을 기초로 하는, NPU.공개특허 10-2024-0058070-3-청구항 8 제7항에 있어서,상기 연산 순서에 대한 정보는:레이어에 대한 정보,커널에 대한 정보,프로세싱 시간에 대한 정보, 남은 시간에 대한 정보, 그리고클럭(clock)에 대한 정보 중 하나 이상을 포함하는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서, 상기 레이어에 대한 정보는상기 제1 ANN 모델의 모든 레이어들 중에서 i번째 레이어를 나타내고,상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 i번째 레이어를 위한 연산이 시작된 이후에 시작되는,NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 커널에 대한 정보는상기 제1 ANN 모델의 모든 커널들 중에서 k번째 커널을 나타내고,상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 k번째 커널을 위한 연산이 시작된 이후에 시작되는,NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서, 상기 프로세싱 시간에 대한 정보는:상기 제1 ANN 모델을 위한 연산을 수행한 이후에 경과된 소정 시간을 나타내고,상기 제2 ANN 모델을 위한 연산은 상기 소정 시간 이후에 시작되는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제8항에 있어서, 상기 남은 시간에 대한 정보는상기 제1 ANN 모델의 연산들이 수행된 후, 완료되기까지 남은 시간을 나타내고,상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 연산들이 완료되기 전에 시작되는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제7항에 있어서, 상기 복수의 ANN들의 연산 순서에 대한 정보는 상기 적어도 하나의 내부 메모리에 저장되는,NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제7항에 있어서, 상기 제2 연산들을 제2 그룹의 PE들에게 할당하는데 사용되는 상기 명령은, 상기 복수의 ANN 모델들의 연산 순서에 대한 정보에 기초하여 상기 스케줄러에 의해서 생성되는, NPU."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "공개특허 10-2024-0058070-4-엣지 디바이스로서,시스템 버스와;상기 시스템 버스에 전기적으로 연결된 메모리와;상기 시스템 버스에 전기적으로 연결되고, 상기 시스템 버스를 통해 상기 메모리에 액세스하도록 설정되고, 애플리케이션을 위한 명령어들을 실행하도록 설정되는 CPU(central processing unit)와; 그리고상기 시스템 버스에 전기적으로 연결되는 하나 또는 복수의 NPU(neural processing units)를 포함하고,여기서 각 NPU는:적어도 하나의 내부 메모리와;복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(inparallel) 처리하도록 설정가능한 복수의 PE(processing element)들을 포함하고,상기 복수의 ANN 모델들 중 제1 ANN 모델은 상시 동작되고, 제2 ANN 모델은 조건적으로 구동되고, 상기 CPU는 제1 ANN(artificial neural network) 모델을 위한 제1 연산들을 제1 NPU 또는 상기 제1 NPU 내의제1 그룹의 PE(processing element)들에게 할당하고, 상기 제1 연산들의 추론 결과에 기초하여 조건적으로 상기제2 ANN 모델을 위한 제2 연산들을 제2 NPU 또는 제1 NPU 내의 제2 그룹의 PE들에게 할당함으로써 소비 전력을저감시키는, 엣지 디바이스."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서, 상기 제1 ANN 모델을 위한 제1 연산들과 상기 제2 ANN 모델을 위한 제2 연산들은 병렬적(inparallel) 또는 시분할(time division) 방식으로 수행되는, 엣지 디바이스."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항에 있어서, 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들은 부분적으로 동일하거나, 서로 완전히 다른,엣지 디바이스."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15항에 있어서,상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위한 제2 연산들을 수행한추론 결과에 따라 제1 모드 또는 제2 모드로 동작되는, 엣지 디바이스."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "엣지 디바이스에 장착되고, 적어도 하나의 내부 메모리와 복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(in parallel) 처리하도록 설정가능한 복수의PE(processing element)들을 포함하는 NPU(neural processing unit)의 동작 방법으로서,상기 복수의 ANN 모델들 중 제1 ANN(artificial neural network) 모델을 위한 제1 연산들을 제1 NPU 또는 상기제1 NPU 내의 제1 그룹의 PE들에게 할당하는 단계와;상기 제1 ANN 모델을 위한 제1 연산들을 수행하는 단계와; 그리고상기 제1 ANN 모델을 위한 상기 제1 연산들을 수행한 추론 결과에 기초하여 조건적으로 제2 ANN 모델을 위한 제2 연산들을 제2 NPU 또는 상기 제1 NPU 내의 제2 그룹의 PE들에게 할당하는 단계를 포함하고,상기 제1 ANN 모델을 위한 상기 제1 연산들은 상시 수행되고, 상기 제2 ANN 모델을 위한 제2 연산들은 상기 제1연산들을 수행한 추론 결과에 기초하여 조건적으로 수행됨으로써, 상기 엣지 디바이스의 소비 전력이 저감되는,방법."}
{"patent_id": "10-2024-0053429", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "공개특허 10-2024-0058070-5-제19항에 있어서,상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위한 제2 연산들을 수행한추론 결과에 따라, 상기 엣지 디바이스는 제1 모드 또는 제2 모드로 동작되는, 방법."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서의 일 개시에 따르면, 엣지 디바이스에 포함되는 NPU(neural processing unit)가 제시된다. 상기 NPU 는 적어도 하나의 내부 메모리와; 복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(in parallel) 처리하도록 설정가능한 복수의 PE(processing element)들과; 그리고 스케줄러를 포함할 수 있다. 상기 복수의 ANN 모델들 중 제1 ANN 모델은 상시 동작되고, 제2 ANN 모델은 조건적 으로 구동될 수 있다. 상기 스케줄러는 상기 제1 ANN 모델을 위한 제1 연산들을 제1 그룹의 PE들에게 할당하고, 상기 제1 연산들의 추론 결과에 기초하여 조건적으로 상기 제2 ANN 모델을 위한 제2 연산들을 제2 그룹의 PE들에 게 할당함으로써 상기 엣지 디바이스의 소비 전력을 저감시킬 수 있다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인공 신경망(artificial neural network)에 관한 것이다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있으며, 각각의 뉴런은 시냅스(Synapse)라고 불리 는 연결부위를 통해 수백에서 수천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학 적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망(Artificial Neural Network, ANN) 모 델이라고 한다. 즉, 인공 신경망은 뉴런들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이 다. 이러한 인공신경망 모델은 레이어 수에 따라 ‘단층 신경망’과 ‘다층 신경망’으로 구분하며, 일반적인 다층 신경망은 입력 레이어와 은닉 레이어, 출력 레이어로 구성되는데, 입력 레이어(input layer)은 외부의 자료 들을 받아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하며, 은닉 레이어 (hidden layer)은 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추출하여 출력층으로 전달한다. 출력 레이어(output layer)은 은닉 레이어로부터 신호를 받아 외부로 출력한다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산되며 이 합이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 보다 높은 인공 지능을 구현하기 위하여, 인공 신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망(Deep Neural Network, DNN)이라고 한다. 한편, 인공 신경망 모델은 다양한 엣지 디바이스에서 활용될 수 있고, 엣지 디바이스는 그 종류에 따라 복수의 인공 신경망 모델을 사용할 수 있다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "그러나, 복수의 인공 신경망 모델을 사용할 때에, 최적화된 방안이 존재하지 않은 문제점을 본 특허의 발명자는 인식하게 되었다. 인공 신경망 모델 별로 NPU(neural processing unit)를 별개로 두는 경우, 본 특허의 발명자는 NPU가 유휴 상태 로 존재하는 시간이 증대되어, 비효율성이 증가하는 문제점을 인식하게 되었다. 또한, 하나의 NPU를 가지고 복수의 인공 신경망 모델의 연산을 수행하는 경우, 복수의 인공 신경망 모델들 간에 효율적인 연산 순서를 정하지 않으면, 연산 처리 시간이 증대되는 문제점을 본 특허의 발명자는 인식하게 되었 다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 문제점을 해결하기 위하여, 본 명세서의 일 개시에 따르면, 엣지 디바이스에 포함되는 NPU(neural processing unit)가 제시된다. 상기 NPU는 적어도 하나의 내부 메모리와; 복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(in parallel) 처리하도록 설정가능한 복수의 PE(processing element)들과; 그리고 스케줄러를 포함할 수 있다. 상기 복수의 ANN 모델들 중 제1 ANN 모델은 상시 동작되고, 제2 ANN 모델은 조건적으로 구동될 수 있다. 상기 스케줄러는 상기 제1 ANN 모델을 위한제1 연산들을 제1 그룹의 PE들에게 할당하고, 상기 제1 연산들의 추론 결과에 기초하여 조건적으로 상기 제2 ANN 모델을 위한 제2 연산들을 제2 그룹의 PE들에게 할당함으로써 상기 엣지 디바이스의 소비 전력을 저감시킬 수 있다. 상기 엣지 디바이스는, 상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위 한 제2 연산들을 수행한 추론 결과에 따라 제1 모드 또는 제2 모드로 동작될 수 있다. 상기 스케줄러는 ANN 데이터 지역성 정보를 고려하여 할당을 수행할 수 있다. 상기 ANN 데이터 지역성 정보는: 상기 적어도 하나의 내부 메모리의 크기에 대한 정보와; 상기 적어도 하나의 내부 메모리의 계층(hierarchy) 구조에 대한 정보와; 상기 복수의 PE들의 개수에 대한 정보와; 상기 복수의 PE 들의 연산기 구조에 대한 정보와; 그리고 상기 NPU에 대한 구조 정보 중에서 하나 이상을 포함할 수 있다. 상기 제1 ANN 모델을 위한 제1 연산들과 상기 제2 ANN 모델을 위한 제2 연산들은 병렬적(in parallel) 또는 시 분할(time division) 방식으로 수행될 수 있다. 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들은 부분적으로 동일하거나, 서로 완전히 다를 수 있다. 상기 스케줄러가 상기 할당들을 수행할 때, 상기 복수의 ANN들의 연산 순서에 대한 정보와 관련된 명령을 기초 로 할 수 있다. 상기 연산 순서에 대한 정보는: 레이어에 대한 정보, 커널에 대한 정보, 프로세싱 시간에 대한 정보, 남은 시간 에 대한 정보, 그리고 클럭(clock)에 대한 정보 중 하나 이상을 포함할 수 있다. 상기 레이어에 대한 정보는: 상기 제1 ANN 모델의 모든 레이어들 중에서 i번째 레이어를 나타내고, 상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 i번째 레이어를 위한 연산이 시작된 이후에 시작될 수 있다. 상기 커널에 대한 정보는: 상기 제1 ANN 모델의 모든 커널들 중에서 k번째 커널을 나타내고, 상기 제2 ANN 모델 을 위한 연산은 상기 제1 ANN 모델의 k번째 커널을 위한 연산이 시작된 이후에 시작될 수 있다. 상기 프로세싱 시간에 대한 정보는: 상기 제1 ANN 모델을 위한 연산을 수행한 이후에 경과된 소정 시간을 나타 내고, 상기 제2 ANN 모델을 위한 연산은 상기 소정 시간 이후에 시작될 수 있다. 상기 남은 시간에 대한 정보는: 상기 제1 ANN 모델의 연산들이 수행된 후, 완료되기까지 남은 시간을 나타낼 수 있다. 상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 연산들이 완료되기 전에 시작될 수 있다. 상기 복수의 ANN들의 연산 순서에 대한 정보는 상기 적어도 하나의 내부 메모리에 저장될 수 있다. 상기 제2 연산들을 제2 그룹의 PE들에게 할당하는데 사용되는 상기 명령은, 상기 복수의 ANN 모델들의 연산 순 서에 대한 정보에 기초하여 상기 스케줄러에 의해서 생성될 수 있다. 전술한 문제점을 해결하기 위하여, 본 명세서의 일 개시에 따르면, 엣지 디바이스가 제시될 수 있다. 상기 엣지 디바이스는 시스템 버스와; 상기 시스템 버스에 전기적으로 연결된 메모리와; 상기 시스템 버스에 전기적으로 연결되고, 상기 시스템 버스를 통해 상기 메모리에 액세스하도록 설정되고, 애플리케이션을 위한 명령어들을 실 행하도록 설정되는 CPU(central processing unit)와; 그리고 상기 시스템 버스에 전기적으로 연결되는 하나 또 는 복수의 NPU(neural processing units)를 포함할 수 있다. 각 NPU는: 적어도 하나의 내부 메모리와; 복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(in parallel) 처리하도록 설정가능한 복수의 PE(processing element)들을 포함할 수 있다. 상기 복수의 ANN 모델 들 중 제1 ANN 모델은 상시 동작되고, 제2 ANN 모델은 조건적으로 구동될 수 있다. 상기 CPU는 제1 ANN(artificial neural network) 모델을 위한 제1 연산들을 제1 NPU 또는 상기 제1 NPU 내의 제1 그룹의 PE(processing element)들에게 할당하고, 상기 제1 연산들의 추론 결과에 기초하여 조건적으로 상기 제2 ANN 모 델을 위한 제2 연산들을 제2 NPU 또는 제1 NPU 내의 제2 그룹의 PE들에게 할당함으로써 소비 전력을 저감시킬 수 있다. 상기 제1 ANN 모델을 위한 제1 연산들과 상기 제2 ANN 모델을 위한 제2 연산들은 병렬적(in parallel) 또는 시 분할(time division) 방식으로 수행될 수 있다. 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들은 부분적으로 동일하거나, 서로 완전히 다를 수 있다. 상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위한 제2 연산들을 수행한 추론 결과에 따라 제1 모드 또는 제2 모드로 동작될 수 있다. 전술한 문제점을 해결하기 위하여, 본 명세서의 일 개시에 따르면, 엣지 디바이스에 장착되고, 적어도 하나의 내부 메모리와 복수의 ANN(artificial neural network) 모델들을 위한 연산을 순차적으로(in sequence) 혹은 병렬적으로(in parallel) 처리하도록 설정가능한 복수의 PE(processing element)들을 포함하는 NPU(neural processing unit)의 동작 방법이 제시된다. 상기 방법은 상기 복수의 ANN 모델들 중 제1 ANN(artificial neural network) 모델을 위한 제1 연산들을 제1 NPU 또는 상기 제1 NPU 내의 제1 그룹의 PE들에게 할당하는 단 계와; 상기 제1 ANN 모델을 위한 제1 연산들을 수행하는 단계와; 그리고 상기 제1 ANN 모델을 위한 상기 제1 연 산들을 수행한 추론 결과에 기초하여 조건적으로 제2 ANN 모델을 위한 제2 연산들을 제2 NPU 또는 상기 제1 NPU 내의 제2 그룹의 PE들에게 할당하는 단계를 포함할 수 있다. 상기 제1 ANN 모델을 위한 상기 제1 연산들은 상시 수행되고, 상기 제2 ANN 모델을 위한 제2 연산들은 상기 제1 연산들을 수행한 추론 결과에 기초하여 조건적으로 수행됨으로써, 상기 엣지 디바이스의 소비 전력이 저감될 수 있다. 상기 제1 ANN 모델을 위한 제1 연산들을 수행한 추론 결과 또는 상기 제2 ANN 모델을 위한 제2 연산들을 수행한 추론 결과에 따라, 상기 엣지 디바이스는 제1 모드 또는 제2 모드로 동작될 수 있다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 하나의 NPU를 통해 복수의 신경망 모델들을 동시에 병렬로 처리할 수 있는 효과가 있다. 본 개시에 따르면, 우선 순위가 높은 데이터가 NPU 내부 메모리에 유지되기 때문에, 저장된 데이터를 재사용하 여 메모리 재사용율을 높일 수 있는 효과가 있다. 본 개시에 따르면, 임의 신경망 모델을 특정 조건에서만 구동시켜, 엣지 디바이스의 소비 전력을 저감할 수 있 는 효과가 있다. 본 개시에 따르면, 엣지 디바이스는 독립적으로 동작할 수 있는 NPU를 포함하고 있기 때문에, 시간 지연을 단축 할 수 있고, 소비 전력을 저감할 수 있는 효과가 있다. 본 개시에 따르면, 엣지 디바이스는 사용자에게 편의를 제공하고, 소비 전력을 저감하면서 동시에 사생활 데이 터 누출 문제를 차단할 수 있는 효과가 있다."}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 또는 출원에 개시되어 있는 본 개시의 개념에 따른 예시들에 대해서 특정한 구조적 내지 단계적 설명 들은 단지 본 개시의 개념에 따른 예시를 설명하기 위한 목적으로 예시된 것으로, 본 개시의 개념에 따른 예시 들은 다양한 형태로 실시될 수 있으며 본 개시의 개념에 따른 예시들은 다양한 형태로 실시될 수 있으며 본 명 세서 또는 출원에 설명된 예시들에 한정되는 것으로 해석되어서는 아니 된다.본 개시의 개념에 따른 예시는 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있으므로 특정 예시들을 도면에 예시하고 본 명세서 또는 출원에 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따른 예시 를 특정한 개시 형태에 대해 한정하려는 것이 아니며, 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균 등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만, 예컨대 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소는 제2 구성요소로 명명 될 수 있고, 유사하게 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관 계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 명세서에서 사용한 용어는 단지 특정한 예시를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포 함하다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또 는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다. 예시를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기 술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 명세서에서 제시되는 개시들의 이해를 돕고자, 본 명세서에서 사용되는 용어들에 대하여 간략하게 정 리하기로 한다. NPU: 신경망 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인 공 신경망 모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. ANN: 인공 신경망(artificial neural network)의 약어로서, 인간의 지능을 모방하기 위하여, 인간 뇌 속의 뉴런 들(Neurons)이 시냅스(Synapse)를 통하여 연결되는 것을 모방하여, 노드들을 레이어(Layer: 계층) 구조로 연결 시킨, 네트워크를 의미할 수 있다. 인공 신경망의 구조에 대한 정보: 레이어의 개수에 대한 정보, 레이어 내의 노드의 개수, 각 노드의 값, 연산 처리 방법에 대한 정보, 각 노드에 적용되는 가중치 행렬에 대한 정보 등을 포함하는 정보이다. 인공 신경망의 데이터 지역성에 대한 정보: 신경망 프로세싱 유닛이 별개의 메모리에 요청하는 데이터 접근 요 청 순서에 기반하여 신경망 프로세싱 유닛이 처리하는 인공신경망모델의 연산 순서를 예측하게 하는 정보이다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공 신경망의 은닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다.커널: CNN에 적용되는 가중치 행렬을 의미할 수 있다. 이하, 첨부한 도면을 참조하여 본 개시의 바람직한 예시를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시의 예시를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 개시에 따른 신경망 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 1에 도시된 신경망 프로세싱 유닛(neural processing unit, NPU)은 인공 신경망을 위한 동작을 수행하 도록 특화된 프로세서이다. 인공 신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공 신경망은 입력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. 상기 NPU은 전기/전자 회로로 구현된 반도체일 수 있다. 상기 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. 상기 NPU은 프로세싱 엘리먼트 (processing element: PE) 어레이, NPU 내부 메모리, NPU 스케줄러, 및 NPU 인터페이스를 포함할 수 있다. 프로세싱 엘리먼트 어레이, NPU 내부 메모리, NPU 스케줄러, 및 NPU 인터페이 스 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식 별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예컨대, 임의 회로는 프로세싱 엘리먼트 어 레이으로 동작하기도 하고, 혹은 NPU 스케줄러로 동작될 수도 있다. 상기 NPU은 프로세싱 엘리먼트 어레이, 프로세싱 엘리먼트 어레이에서 추론될 수 있는 인공신경 망모델을 저장하도록 구성된 NPU 내부 메모리, 및 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리를 제어하도록 구성된 NPU 스케줄러 를 포함할 수 있다. 여기서, 인공신경망모델은 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정 보를 포함할 수 있다. 인공신경망모델은 특정 추론 기능을 수행하도록 학습된 AI 인식모델을 의미할 수 있다. 프로세싱 엘리먼트 어레이는 인공 신경망을 위한 동작을 수행할 수 있다. 예를 들어, 입력 데이터가 입력 되었을 때, 프로세싱 엘리먼트 어레이는 인공 신경망이 학습을 수행하도록 할 수 있다. 학습이 완료된 이 후, 입력 데이터가 입력되었을 때, 프로세싱 엘리먼트 어레이는 학습 완료된 인공 신경망을 통해 추론 결 과를 도출하는 동작을 수행할 수 있다. NPU 인터페이스는 시스템 버스를 통해서 엣지 디바이스 내의 다양한 구성요소들, 예컨대 메모리와 통신할 수 있다. 예를 들면, NPU은 NPU 인터페이스를 통해서 도 4a 또는 도 4b에 도시된 메모리에 저장된 인공신 경망모델의 데이터를 NPU 내부 메모리으로 불러올 수 있다. NPU 스케줄러는 NPU의 추론 연산을 위한 프로세싱 엘리먼트 어레이의 연산 및 NPU 내부 메모리 의 읽기 및 쓰기 순서를 제어하도록 구성된다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 분석하여 프로세싱 엘리먼 트 어레이 및 NPU 내부 메모리을 제어하도록 구성될 수 있다. NPU 스케줄러는 프로세싱 엘리먼트 어레이에서 작동할 인공신경망모델의 구조를 분석하거나 또는 제 공받을 수 있다. 인공신경망모델이 포함할 수 있는 인공 신경망의 데이터는 각각의 레이어의 노드 데이터, 레이 어들의 배치 데이터 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각각의 가중 치 데이터를 저장할 수 있다. 인공 신경망의 데이터는 NPU 스케줄러 내부에 제공되는 메모리 또는 NPU 내 부 메모리에 저장될 수 있다. NPU 스케줄러는 도 4a 또는 도 4b에 도시된 메모리에 액세스하여 필요한 데이터를 활용할 수 있다. 단, 이에 제한되지 않으며, 즉, 인공신경망모델의 노드 데이터 및 가중치 데 이터 등의 데이터에 기초하여 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 생성할 수 있다. 가중치 데이터는 가중치 커널로 지칭되는 것도 가능하다. 노드 데이터는 피처 맵으로 지칭되는 것도 가능하다. 예를 들면, 인공신경망모델의 구조가 정의된 데이터는 인공신경망모델을 설계하거나 또는 학습이 완료될 때 생 성될 수 있다. 단, 이에 제한되지 않는다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 인공신경망모델 의 연산 순서를 스케줄링 할 수 있다.NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 인공신경망모델 의 레이어의 노드 데이터 및 연결망의 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들 면, NPU 스케줄러는 메모리에 저장된 인공신경망모델의 레이어의 노드 데이터 및 연결망의 가중치 데이터 가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 스케줄러는 구동할 인공신경망모델의 레이어 의 노드 데이터 및 연결망의 가중치 데이터를 메모리에서 가져와서 NPU 내부 메모리에 저장할 수 있 다. 각각의 레이어의 노드 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 연결망의 가 중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델 의 인공 신경망의 레이어들의 배치 데이터 지역성 정보 또는 구조에 대한 정보에 기초해서 프로세싱 엘리먼트 어레이의 연산 순서를 스케줄링 할 수 있다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 스케줄링 하기 때문에, 일반적인 CPU의 스케줄링 개념과 다르게 동작할 수 있다. 일반적인 CPU의 스케줄링은 공평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동작한다. 즉, 우선 순위, 연산 시간 등을 고려 해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 한다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 프 로세싱 순서를 결정할 수 있다. 더 나아가면, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및/또는 사용 하려는 NPU의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 프로세싱 순서를 결정할 수 있다. 단, 본 개시는 NPU의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. 예를 들면, NPU(10 0)의 데이터 지역성 정보 또는 구조에 대한 정보는 NPU 내부 메모리의 메모리 크기, NPU 내부 메모리(12 0)의 계층(hierarchy) 구조, 프로세싱 엘리먼트들(PE1 to PE12)의 개수 데이터, 프로세싱 엘리먼트들(PE1 to PE12)의 연산기 구조 중 적어도 하나 이상의 데이터를 활용하여 프로세싱 순서를 결정할 수 있다. 즉, NPU(10 0)의 데이터 지역성 정보 또는 구조에 대한 정보는 NPU 내부 메모리의 메모리 크기, NPU 내부 메모리(12 0)의 계층(hierarchy) 구조, 프로세싱 엘리먼트들(PE1 to PE12)의 개수 데이터, 및 프로세싱 엘리먼트들(PE1 to PE12)의 연산기 구조 중 적어도 하나 이상의 데이터 포함할 수 있다. 단, 본 개시는 NPU의 데이터 지역성 정보 또는 구조에 대한 정보에 제한되지 않는다. NPU 내부 메모리의 메모리 크기는 메모리 용량에 대한 정 보를 포함한다. NPU 내부 메모리의 계층(hierarchy) 구조는 각각의 계층 구조에 대한 구체적인 계층 간의 연결 관계에 대한 정보를 포함한다. 프로세싱 엘리먼트들(PE1 to PE12)의 연산기 구조는 프로세싱 엘리먼트 내 부의 구성요소들에 대한 정보를 포함한다. 본 개시의 예시일 예시에 따른, NPU은 적어도 하나의 프로세싱 엘리먼트, 적어도 하나의 프로세싱 엘리먼 트에 의해서 추론될 수 있는 인공신경망모델을 저장할 수 있는 NPU 내부 메모리, 및 인공신경망모델의 데 이터 지역성 정보 또는 구조에 대한 정보에 기초하여 적어도 하나의 프로세싱 엘리먼트 및 NPU 내부 메모리 을 제어하도록 구성된 NPU 스케줄러를 포함할 수 있다. 그리고 NPU 스케줄러는 NPU의 데이 터 지역성 정보 또는 구조에 대한 정보를 더 제공받도록 구성될 수 있다. 또한 신경망 프로세싱 유닛의 데이터 지역성 정보 또는 구조에 대한 정보는 NPU 내부 메모리의 메모리 크기, NPU 내부 메모리의 계 층(hierarchy) 구조, 적어도 하나의 프로세싱 엘리먼트의 개수 데이터, 및 적어도 하나의 프로세싱 엘리먼트의 연산기 구조 중 적어도 하나의 데이터를 포함할 수 있다. 인공신경망모델의 구조에 의하면, 각 레이어 별 연산은 순차적으로 수행된다. 즉, 인공신경망모델의 구조가 확 정될 경우, 레이어 별 연산순서가 정해질 수 있다. 이러한 인공신경망모델의 구조에 따른 연산의 순서 또는 데 이터 흐름의 순서를 알고리즘 레벨에서의 인공신경망모델의 데이터 지역성으로 정의할 수 있다. 컴파일러가 인공신경망모델이 NPU에서 실행되도록 컴파일 할 경우, 신경망 프로세싱 유닛-메모리 레벨에서 의 인공신경망모델의 인공신경망 데이터 지역성이 재구성될 수 있다. 즉, 컴파일러, 인공신경망모델에 적용된 알고리즘들, 및 NPU의 동작 특성에 따라서 신경망 프로세싱 유닛- 메모리 레벨에서의 인공신경망모델의 데이터 지역성이 구성될 수 있다. 예를 들면, 동일한 인공신경망모델의 경우에도 NPU이 해당 인공신경망모델을 연산하는 방식, 예를 들면, 특징맵 타일링(feature map tiling), 프로세싱 엘리먼트의 스테이셔너리(Stationary) 기법 등, NPU의 프 로세싱 엘리먼트 개수, NPU내 특징맵 및 가중치 등의 캐쉬 메모리 용량, NPU내의 메모리 계층 구조, 해당 인공신경망모델을 연산 처리하기 위한 NPU의 연산 동작의 순서를 결정해 주는 컴파일러의 알고리즘 특성 등에 따라서 처리하고자 하는 인공신경망모델의 인공신경망 데이터 지역성이 다르게 구성될 수 있다. 왜냐 하면, 상술한 요인들에 의해서 동일한 인공신경망모델을 연산 처리하더라도 NPU이 클럭 단위로 매 순간 필 요한 데이터의 순서를 상이하게 결정할 수 있기 때문이다 컴파일러는 NPU의 워드 단위로 신경망 프로세싱 유닛-메모리 레벨에서 인공신경망모델의 인공신경망 데이 터 지역성이 구성하여 물리적인 연산 처리에 필요한 데이터의 순서를 결정할 수 있다. 다르게 설명하면, 신경망 프로세싱 유닛-메모리 레벨에서 존재하는 인공신경망모델의 인공신경망 데이터 지역성 이란 NPU이 메모리에 요청하는 데이터 접근 요청 순서에 기반하여 신경망 프로세싱 유닛 이 처 리하는 인공신경망모델의 연산 순서를 예측하게 하는 정보로 정의될 수 있다. NPU 스케줄러는 인공신경망의 데이터 지역성 정보 또는 구조에 대한 정보를 저장하도록 구성될 수 있다. 즉, NPU 스케줄러는 적어도 인공신경망모델의 인공 신경망의 데이터 지역성 정보 또는 구조에 대한 정보만 활용하더라도 프로세싱 순서를 결정할 수 있다. 즉, NPU 스케줄러는 인공 신경망의 입력 레이어부터 출력 레이어 까지의 데이터 지역성 정보 또는 구조에 대한 정보를 활용하여 연산 순서를 결정할 수 있다. 예를 들면, 입력 레이어 연산이 제1 순위로 스케줄링 되고 출력 레이어 연산이 마지막으로 스케줄링 될 수 있다. 따라서, NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 제공받는 경우, 인공신경 망모델의 모든 연산 순서를 알 수 있다. 따라서 모든 스케줄링 순서를 결정할 수 있는 효과가 있다. 더 나아가서, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 NPU의 데이터 지역성 정보 또는 구조에 대한 정보를 고려하여 프로세싱 순서를 결정할 수 있으며 결정된 순서 별 프로 세싱 최적화도 가능하다. 따라서, NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 NPU의 데이 터 지역성 정보 또는 구조에 대한 정보를 모두 제공받는 경우, 인공신경망모델의 데이터 지역성 정보 또는 구조 에 대한 정보에 의해서 결정된 스케줄링 순서 각각의 연산 효율을 보다 더 향상시킬 수 있는 효과가 있다. 예를 들면, NPU 스케줄러는 4개의 레이어의 인공 신경망 레이어들과 각각의 레이어들을 연결하는 3개의 레이어 의 가중치 데이터들을 가지는 연결망 데이터를 획득할 수 있다. 이러한 경우 NPU 스케줄러가 인공신경망모 델의 데이터 지역성 정보 또는 구조에 대한 정보를 기초로 프로세싱 순서를 스케줄링 하는 방법에 대하여 예를 들어 아래에서 설명한다. 예를 들면, NPU 스케줄러는 추론 연산을 위한 입력 데이터를 인공신경망모델의 입력 레이어인 제1 레이어 의 노드 데이터로 설정하고, 제1 레이어의 노드 데이터와 제1 레이어에 대응되는 제1 연결망의 가중치 데이터의 MAC(multiply and accumulate) 연산을 먼저 수행하도록 스케줄링 할 수 있다. 단, 본 개시의 예시들은 MAC 연 산에 제한되지 않으며, 인공 신경망 연산은 다양하게 변형실시 될 수 있는 곱셈기 및 덧셈기를 활용하여 인공신 경망 연산을 수행하는 것도 가능하다. 이하 단지 설명의 편의를 위해서 해당 연산을 제1 연산이라 지칭하고, 제 1 연산의 결과를 제1 연산 값이라 지칭하고, 해당 스케줄링을 제1 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제1 연산 값을 제1 연결망에 대응되는 제2 레이어의 노드 데이터로 설정하고, 제2 레이어의 노드 데이터와 제2 레이어에 대응되는 제2 연결망의 가중치 데이터의 MAC 연산을 제1 스케줄링 이 후에 수행하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 연산을 제2 연산이라 지칭하고, 제2 연산의 결과를 제2 연산 값이라 지칭하고, 해당 스케줄링을 제2 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제2 연산 값을 제2 연결망에 대응되는 제3 레이어의 노드 데이터로 설정하고, 제3 레이어의 노드 데이터와 제3 레이어에 대응되는 제3 연결망의 가중치 데이터의 MAC 연산을 제2 스케줄링에 수행하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 연산을 제3 연산이라 지칭하고, 제3 연 산의 결과를 제3 연산 값이라 지칭하고, 해당 스케줄링을 제3 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제3 연산 값을 제3 연결망에 대응되는 출력 레이어인 제4 레이어의 노드 데이 터로 설정하고, 제4 레이어의 노드 데이터에 저장된 추론 결과를 NPU 내부 메모리에 저장하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 스케줄링을 제4 스케줄링이라 지칭할 수 있다. 정리하면, NPU 스케줄러는 제1 스케줄링, 제2 스케줄링, 제3 스케줄링, 및 제4 스케줄링 순서대로 연산이 수행되도록 NPU 내부 메모리과 프로세싱 엘리먼트 어레이를 제어할 수 있다. 즉, NPU 스케줄러 는 설정된 스케줄링 순서대로 연산이 수행되도록 NPU 내부 메모리과 프로세싱 엘리먼트 어레이를 제 어하도록 구성될 수 있다. 정리하면, 본 개시의 일 예시에 따른 NPU은 인공 신경망의 레이어들의 구조와, 구조에 대응되는 연산 순서 데이터에 기초하여, 프로세싱 순서를 스케줄링 하도록 구성될 수 있다. 예를 들면, NPU 스케줄러는 인공신경망모델의 인공 신경망의 입력 레이어부터 출력 레이어 까지의 데이터 지역성 정보 또는 구조에 대한 정보를 기초로, 프로세싱 순서를 스케줄링 하도록 구성될 수 있다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초한 스케줄링 순서를 활용하여 NPU 내부 메모리을 제어하여 신경망 프로세싱 유닛의 연산 가동율을 향상시킬 수 있고, 메모리 재사용율을 향상시킬 수 있는 효과가 있다. 본 개시의 일 예시에 따른 NPU에서 구동되는 인공 신경망 연산의 특성상 하나의 레이어의 연산 값이 다음 레이어의 입력 데이터가 되는 특성을 가질 수 있다. 이에, NPU은 스케줄링 순서에 따라서 NPU 내부 메모리을 제어하여, NPU 내부 메모리의 메모리 재사용율을 향상시킬 수 있는 효과가 있다. 메모리 재사용은 메모리에 저장된 데이터를 몇 번 읽는지 횟수로 판 단할 수 있다. 예를 들면, 메모리에 특정 데이터를 저장한 다음, 특정 데이터를 1회만 읽은 후 해당 데이터를 삭제하거나 덮어쓰기 하면, 메모리 재사용율은 100%가 될 수 있다. 예를 들면, 메모리에 특정 데이터를 저장한 다음, 특정 데이터를 4회 읽은 후, 해당 데이터를 삭제하거나 덮어쓰기 하면, 메모리 재사용율은 400%가 될 수 있다. 즉, 메모리 재사용율은 한번 저장된 데이터의 재사용 횟수로 정의될 수 있다. 즉, 메모리 재사용은 메모 리에 저장된 데이터 또는 특정 데이터가 저장된 특정 메모리 어드레스를 재사용한다는 것을 의미할 수 있다. 구체적으로 설명하면, NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 제 공받도록 구성되고, 제공받은 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 의해서 인공 신경 망의 연산이 진행되는 순서 데이터를 파악할 수 있는 경우, NPU 스케줄러는 인공신경망모델의 특정 레이어 의 노드 데이터와 특정 연결망의 가중치 데이터의 연산 결과가 대응되는 다음 레이어의 노드 데이터가 된다는 사실을 인식하였다. 따라서 NPU 스케줄러는 특정 연산 결과가 저장된 메모리 어드레스의 값을 이어지는 다음 연산에서 재사용 할 수 있다. 따라서 메모리 재사용율이 향상될 수 있다. 예를 들면, 상술한 제1 스케줄링의 제1 연산 값은 제2 스케줄링의 제2 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제1 스케줄링의 제1 연산 값에 대응되는 메모리 어드레스 값을 제2 스케줄링의 제2 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제1 스케줄링의 메모리 어 드레스의 데이터를 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제2 스케줄링의 제 2 레이어 노드 데이터로 활용할 수 있는 효과가 있다. 예를 들면, 상술한 제2 스케줄링의 제2 연산 값은 제3 스케줄링의 제3 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제2 스케줄링의 제2 연산 값에 대응되는 메모리 어드레스 값을 제3 스케줄링의 제3 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제2 스케줄링의 메모리 어 드레스의 데이터를 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제3 스케줄링의 제 3 레이어 노드 데이터로 활용할 수 있는 효과가 있다. 예를 들면, 상술한 제3 스케줄링의 제3 연산 값은 제4 스케줄링의 제4 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제3 스케줄링의 제3 연산 값에 대응되는 메모리 어드레스 값을 제4 스케줄링의 제4 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제3 스케줄링의 메모리 어 드레스의 데이터를 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제4 스케줄링의 제 4 레이어 노드 데이터로 활용할 수 있는 효과가 있다. 더 나아가서, NPU 스케줄러는 스케줄링 순서와 메모리 재사용 여부를 판단해서 NPU 내부 메모리을 제 어하도록 구성되는 것도 가능하다. 이러한 경우 NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또 는 구조에 대한 정보를 분석해서 효율적인 스케줄링을 제공할 수 있는 효과가 있다. 또한 메모리 재사용이 가능 한 연산에 필요한 데이터를 중복해서 NPU 내부 메모리에 저장하지 않을 수 있기 때문에 메모리 사용량을 저감할 수 있는 효과가 있다. 또한 NPU 스케줄러는 메모리 재사용만큼 저감된 메모리 사용량을 계산해서 NPU 내부 메모리을 효율화 할 수 있는 효과가 있다. 더 나아가서, NPU 스케줄러는 NPU의 데이터 지역성 정보 또는 구조에 대한 정보에 기초해서, NPU 내 부 메모리의 리소스 사용량, 프로세싱 엘리먼트들(PE1 to PE12)의 리소스 사용량을 모니터링 하도록 구성 될 수 있다. 따라서 NPU의 하드웨어 리소스 활용 효율을 향상시킬 수 있는 효과가 있다. 본 개시의 일 예시에 따른 NPU의 NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 활용하여 메모리를 재사용할 수 있는 효과가 있다. 부연 설명하면, 인공신경망모델이 심층 신경망일 경우, 레이어의 개수 및 연결망의 개수가 상당히 증가할 수 있 으며, 이러한 경우 메모리 재사용의 효과가 더욱더 극대화될 수 있는 효과가 있다. *즉, 만약 NPU이 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보 및 연산 순서를 파악하지 않을 경우, NPU 스케줄러는 NPU 내부 메모리에 저장된 값들의 메모리 재사용 여부를 판단할 수 없다. 따라서, NPU 스케줄러는 각각의 프로세싱에 필요한 메모리 어드레스를 불필요하게 생성하고, 실질적으로 동일한 데이터를 하나의 메모리 어드레스에서 다른 메모리 어드레스로 복사해야 한다. 따라서 불필요한 메모리 읽기 쓰기 작업이 발생되며, NPU 내부 메모리에 중복되는 값들이 저장되게 되어, 불필요하게 메모리가 낭 비되는 문제가 발생할 수 있다. 프로세싱 엘리먼트 어레이는 인공 신경망의 노드 데이터와 연결망의 가중치 데이터를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1 to PE12)이 배치된 구성을 의미한다. 각각의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개 시에 따른 예시예시들은 이에 제한되지 않는다. 도 2에서는 예시적으로 복수의 프로세싱 엘리먼트들이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성 되는 것도 가능하다. 이러한 경우, 프로세싱 엘리먼트 어레이는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트들(PE1 to PE12)을 포함하도록 구성된다. 도 2에 도 시된 복수의 프로세싱 엘리먼트들(PE1 to PE12)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼 트들(PE1 to PE12)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수에 의해서 프로 세싱 엘리먼트 어레이의 크기 또는 개수가 결정될 수 있다. 프로세싱 엘리먼트 어레이의 크기는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 프로세싱 엘리먼트 어레이는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 프로세싱 엘리먼트 어레이의 크기는 NPU이 작동하는 인공신경망모델의 특성을 고려하여 설계할 수 있 다. 부연 설명하면, 프로세싱 엘리먼트의 개수는 작동할 인공신경망모델의 데이터 크기, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 인공신경망모델의 데이터 크기는 인공신경망모델의 레이어 수와 각각의 레이어의 가중치 데이터 크기에 대응되어 크기가 결정될 수 있다. 따라서, 본 개시의 일 예시에 따른 NPU의 프로세싱 엘리먼트 어레이의 크기는 제한되지 않는다. 프로 세싱 엘리먼트 어레이의 프로세싱 엘리먼트들의 개수가 증가할수록 작동하는 인공신경망모델의 병렬 연산 능력이 증가되나, NPU의 제조 비용 및 물리적인 크기가 증가될 수 있다. 예를 들면, NPU에서 작동되는 인공신경망모델은 30개의 특정 키워드를 감지하도록 학습된 인공 신경망, 즉 AI 키워드 인식모델일 수 있으며, 이러한 경우, NPU의 프로세싱 엘리먼트 어레이의 크기는 연산량 특 성을 고려하여 4 x 3로 설계될 수 있다. 다르게 설명하면, NPU은 12개의 프로세싱 엘리먼트들을 포함할 수 있다. 단, 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트들(PE1 to PE12)의 개수는 예를 들면, 8개 내지 16,384 범위 내에서 선택되는 것도 가능하다. 즉, 본 개시의 예시예시들은 프로세싱 엘리먼트의 개수에 제한되 지 않는다. 프로세싱 엘리먼트 어레이는 인공 신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 프로세싱 엘리먼트 어레이는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 이하 프로세싱 엘리먼트 어레이 중 제1 프로세싱 엘리먼트(PE1)를 예를 들어 설명한다. 도 2는 본 개시에 적용될 수 있는 프로세싱 엘리먼트 어레이 중 하나의 프로세싱 엘리먼트를 설명하는 개략적인 개념도이다. 본 개시의 일 예시에 따른 NPU은 프로세싱 엘리먼트 어레이, 프로세싱 엘리먼트 어레이에서 추 론될 수 있는 인공신경망모델을 저장하도록 구성된 NPU 내부 메모리 및 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리을 제어하도록 구성된 NPU 스케줄러를 포함하고, 프로세싱 엘리먼트 어레이는 MAC 연산을 수행하도록 구성되고, 프 로세싱 엘리먼트 어레이는 MAC 연산 결과를 양자화해서 출력하도록 구성될 수 있다. 단, 본 개시의 예시들 은 이에 제한되지 않는다. NPU 내부 메모리은 메모리 크기와 인공신경망모델의 데이터 크기에 따라 인공신경망모델의 전부 또는 일부 를 저장할 수 있다. 제1 프로세싱 엘리먼트(PE1)는 곱셈기, 가산기, 누산기, 및 비트 양자화 유닛을 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 프로세싱 엘리먼트 어레이는 인공 신경망의 연 산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 여기서 N과 M은 0보다 큰 정수이다. (N)bit 데이터를 입력 받는 제1 입력부는 변수같은 특성을 가 지는 값을 입력 받도록 구성될 수 있고 (M)bit 데이터를 입력 받는 제2 입력부는 상수 같은 특성을 가지는 값을 입력 받도록 구성될 수 있다. NPU 스케줄러가 변수 값과 상수 값 특성을 구분할 경우, NPU 스케줄러 는 NPU 내부 메모리의 메모리 재사용율을 증가시킬 수 있는 효과가 있다. 단, 곱셈기의 입력 데이터 는 상수 값과 변수 값에 제한되지 않는다. 즉, 본 개시의 예시들에 따르면, 프로세싱 엘리먼트의 입력 데이터는 상수 값과 변수 값의 특성을 이해하여 동작할 수 있기 때문에, NPU의 연산 효율을 향상시킬 수 있다. 하지 만 NPU은 입력 데이터의 상수 값 및 변수 값의 특징에 제한되지 않는다. 여기서 변수 같은 특성을 가지는 값의 의미 또는 변수의 의미는, 해당 값이 저장된 메모리 어드레스의 값의 경 우, 들어오는 입력 데이터가 갱신될 때마다 갱신된다는 것을 의미한다. 예를 들면, 각 레이어의 노드 데이터는 인공신경망모델의 가중치 데이터가 반영된 MAC 연산 값일 수 있으며, 해당 인공신경망모델로 동영상 데이터의 객체 인식 등을 추론할 경우, 매 프레임마다 입력 영상이 바뀌기 때문에, 각 레이어의 노드 데이터는 변하게 된 다. 여기서 상수 같은 특성을 가지는 값의 의미 또는 상수의 의미는, 해당 값이 저장된 메모리 어드레스의 값의 경 우, 들어오는 입력 데이터의 갱신과 상관없이 보존된다는 것을 의미한다. 예를 들면, 연결망의 가중치 데이터는 인공신경망모델의 고유한 추론 판단 기준으로, 해당 인공신경망모델로 동영상 데이터의 객체 인식 등을 추론하 더라도, 연결망의 가중치 데이터는 변하지 않을 수 있다. 즉, 곱셈기는 하나의 변수와 하나의 상수를 입력 받도록 구성될 수 있다. 부연 설명하면, 제1 입력부에 입 력되는 변수 값은 인공 신경망의 레이어의 노드 데이터일 수 있으며, 노드 데이터는 인공 신경망의 입력 레이어 의 입력 데이터, 은닉 레이어의 누산 값, 및 출력 레이어의 누산 값일 수 있다. 제2 입력부에 입력되는 상수 값 은 인공 신경망의 연결망의 가중치 데이터일 수 있다. NPU 스케줄러는 상수 값의 특성을 고려하여 메모리 재사용율을 향상시키도록 구성될 수 있다. 변수 값은 각 레이어의 연산 값이며, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 재사용 가능한 변수 값을 인식하고, 메모리를 재사용 하도록 NPU 내부 메모리을 제 어할 수 있다. 상수 값은 각 연결망의 가중치 데이터이며, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구 조에 대한 정보에 기초하여 반복 사용되는 연결망의 상수 값을 인식하고, 메모리를 재사용 하도록 NPU 내부 메 모리을 제어할 수 있다.즉, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 재사용 가능 한 변수 값 및 재사용 가능한 상수 값을 인식하고, NPU 스케줄러는 메모리를 재사용 하도록 NPU 내부 메모 리을 제어하도록 구성될 수 있다. 프로세싱 엘리먼트는 곱셈기의 제1 입력부 및 제2 입력부 중 하나의 입력부에 0이 입력될 때, 연산을 하지 않더라도 연산 결과가 0이 된다는 사실을 알기 때문에, 곱셈기가 연산을 하지 않도록 동작을 제한할 수 있 다. 예를 들면, 곱셈기의 제1 입력부 및 제2 입력부 중 하나의 입력부에 0이 입력될 때, 곱셈기는 제로 스키핑(zero skipping) 방식으로 동작하도록 구성될 수 있다. 제1 입력부 및 제2 입력부에 입력되는 데이터의 비트 폭(bit width)은 인공신경망모델의 각각의 레이어의 노드 데이터 및 가중치 데이터의 양자화에 따라서 결정될 수 있다. 예를 들면, 제1 레이어의 노드 데이터가 5bit로 양자화 되고 제1 레이어의 가중치 데이터가 7bit로 양자화될 수 있다. 이러한 경우 제1 입력부는 5bit 데이터를 입력 받도록 구성되고, 제2 입력부는 7bit 데이터를 입력 받도록 구성될 수 있다. NPU은 NPU 내부 메모리에 저장된 양자화된 데이터가 프로세싱 엘리먼트의 입력부들에 입력될 때 양자 화된 비트 폭이 실시간으로 변환되도록 제어할 수 있다. 즉, 레이어 마다 양자화 된 비트 폭이 다를 수 있으며, 프로세싱 엘리먼트는 입력되는 데이터의 비트 폭이 변환될 때 실시간으로 비트 폭 정보를 NPU에서 제공받 아서 실시간으로 비트 폭을 변환시켜서 입력 데이터를 생성하도록 구성될 수 있다. 누산기는 (L)loops 횟수만큼 가산기를 사용하여 곱셈기의 연산 값과 누산기의 연산 값을 누산 한다. 따라서 누산기의 출력부와 입력부의 데이터의 비트 폭은 (N+M+log2(L))bit로 출력될 수 있다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 인가 받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시예시들은 이에 제한되지 않는다. 비트 양자화 유닛은 누산기에서 출력되는 데이터의 비트 폭을 저감할 수 있다. 비트 양자화 유닛 은 NPU 스케줄러에 의해서 제어될 수 있다. 양자화된 데이터의 비트 폭은 (X)bit로 출력될 수 있다. 여기서 X는 0보다 큰 정수이다. 상술한 구성에 따르면, 프로세싱 엘리먼트 어레이는 MAC 연산을 수행하도 록 구성되고, 프로세싱 엘리먼트 어레이는 MAC 연산 결과를 양자화해서 출력할 수 있는 효과가 있다. 특히 이러한 양자화는 (L)loops가 증가할수록 소비 전력을 더 절감할 수 있는 효과가 있다. 또한 소비 전력이 저감되 면 발열도 저감할 수 있는 효과가 있다. 특히 발열을 저감하면 NPU의 고온에 의한 오동작 발생 가능성을 저감할 수 있는 효과가 있다. 비트 양자화 유닛의 출력 데이터(X)bit은 다음 레이어의 노드 데이터 또는 합성곱의 입력 데이터가 될 수 있다. 만약 인공신경망모델이 양자화되었다면, 비트 양자화 유닛은 양자화된 정보를 인공신경망모델에서 제공받도록 구성될 수 있다. 단, 이에 제한되지 않으며, NPU 스케줄러는 인공신경망모델을 분석하여 양자 화된 정보를 추출하도록 구성되는 것도 가능하다. 따라서 양자화된 데이터 크기에 대응되도록, 출력 데이터 (X)bit를 양자화 된 비트 폭으로 변환하여 출력될 수 있다. 비트 양자화 유닛의 출력 데이터(X)bit는 양자 화된 비트 폭으로 NPU 내부 메모리에 저장될 수 있다. 본 개시의 일 예시예시에 따른 NPU의 프로세싱 엘리먼트 어레이는 곱셈기, 가산기, 누산기 , 및 비트 양자화 유닛을 포함한다. 프로세싱 엘리먼트 어레이는 비트 양자화 유닛에 의해 서 누산기에서 출력되는 (N+M+log2(L))bit의 비트 폭의 데이터를 (X)bit의 비트 폭으로 저감할 수 있다. NPU 스케줄러는 비트 양자화 유닛을 제어하여 출력 데이터의 비트 폭을 LSB(least significant bi t)에서 MSB(most significant bit)까지 소정 비트 만큼 저감할 수 있다. 출력 데이터의 비트 폭이 저감되면 소 비 전력, 연산량, 메모리 사용량이 저감될 수 있는 효과가 있다. 하지만 비트 폭이 특정 길이 이하로 저감될 경 우, 인공신경망모델의 추론 정확도가 급격히 저하될 수 있는 문제가 발생될 수 있다. 따라서, 출력 데이터의 비 트 폭 저감, 즉, 양자화 수준은 인공신경망모델의 추론 정확도 저감 수준 대비 소비 전력, 연산량, 메모리 사용 량 저감 정도를 비교하여 결정할 수 있다. 양자화 수준의 결정은 인공신경망모델을 목표 추론 정확도를 결정하 고, 비트 폭을 점진적으로 저감하면서 테스트하는 방법으로 결정하는 것도 가능하다. 양자화 수준은 각각의 레 이어의 연산 값마다 각각 결정될 수 있다. 상술한 제1 프로세싱 엘리먼트(PE1)에 따르면 곱셈기의 (N)bit 데이터와 (M)bit 데이터의 비트 폭을 조절 하고, 비트 양자화 유닛에 의해서 연산 값(X)bit의 비트 폭을 저감함으로 써, 프로세싱 엘리먼트 어레이 는 MAC 연산 속도를 향상시키면서 소비 전력을 저감할 수 있는 효과가 있으며, 인공 신경망의 합성곱 (convolution)연산을 보다 더 효율적으로 할 수 있는 효과도 있다. NPU의 NPU 내부 메모리은 프로세싱 엘리먼트 어레이의 MAC 연산 특성 및 소비 전력 특성을 고려 하여 구성된 메모리 시스템일 수 있다. 예를 들면, NPU은, 프로세싱 엘리먼트 어레이의 MAC 연산 특성 및 소비 전력 특성을 고려하여 프로세 싱 엘리먼트 어레이의 연산 값의 비트 폭을 저감하도록 구성될 수 있다. NPU의 NPU 내부 메모리은 NPU의 소비 전력을 최소화하도록 구성될 수 있다. NPU의 NPU 내부 메모리은 작동되는 인공신경망모델의 데이터 크기 및 연산 단계를 고려하여 저전력으 로 메모리를 제어하도록 구성된 메모리 시스템일 수 있다. NPU의 NPU 내부 메모리은 인공신경망모델의 데이터 크기 및 연산 단계를 고려하여 가중치 데이터가 저장된 특정 메모리 어드레스를 재사용하도록 구성된 저전력 메모리 시스템일 수 있다. NPU은 비선형성을 부여하기 위한 여러 가지 활성화 함수를 제공할 수 있다. 예를 들면, 시그모이드 함수, 하이퍼볼릭 탄젠트 함수, 또는 ReLU함수를 제공할 수 있다. 활성화 함수는 MAC 연산 이후에 선택적으로 적용될 수 있다. 활성화 함수가 적용된 연산 값은, 활성화 맵으로 지칭될 수 있다. 도 3은 도 1에 도시된 NPU의 변형예를 나타낸 예시도이다. 도 3에 도시된 NPU은 도 1에 예시적으로 도시된 프로세싱 유닛과 비교하면, 프로세싱 엘리먼트 어레 이를 제외하곤 실질적으로 동일하기 때문에, 이하 단지 설명의 편의를 위해서 중복 설명은 생략할 수 있다. 도 3에 예시적으로 도시된 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트들(PE1 to PE12) 외에, 각각의 프로세싱 엘리먼트들(PE1 to PE12)에 대응되는 각각의 레지스터 파일들(RF1 to RF12)을 더 포함할 수 있 다. 도 3에 도시된 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)은 단지 설명 의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12) 의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1 to PE12) 및 복수의 레지스터 파일들(RF1 to RF12)의 개수에 의해서 프로세싱 엘리먼트 어레이의 크기 또는 개수가 결정될 수 있다. 프로세싱 엘리먼트 어레이 및 복수의 레지스터 파일들(RF1 to RF12)의 크기는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 프로세싱 엘리먼트 어레이의 어레이 크기는 NPU이 작동하는 인공신경망모델의 특성을 고려하여 설계 할 수 있다. 부연 설명하면, 레지스터 파일의 메모리 크기는 작동할 인공신경망모델의 데이터 크기, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. NPU의 레지스터 파일들(RF1 to RF12)은 프로세싱 엘리먼트들(PE1 to PE12)과 직접 연결된 정적 메모리 유 닛이다. 레지스터 파일들(RF1 to RF12)은 예를 들면, 플립플롭, 및/또는 래치 등으로 구성될 수 있다. 레지스터 파일들(RF1 to RF12)은 대응되는 프로세싱 엘리먼트들(RF1 to RF12)의 MAC 연산 값을 저장하도록 구성될 수 있 다. 레지스터 파일들(RF1 to RF12)은 NPU 시스템 메모리와 가중치 데이터 및/또는 노드 데이터를 제공하거 나 제공받도록 구성될 수 있다. 도 4a는 본 명세서의 개시에 따른 NPU을 포함하는 엣지 디바이스의 구성을 나타낸 블록도이다. 도 4a를 참조하면, 엣지 디바이스는 다양하게 변형 실시될 수 있는 다양한 전자 장치들 중 하나의 예시를 도시하고 있다. 엣지 디바이스는 도 1 또는 도 3에 도시된 NPU을 포함하고, NPU에 의해서 추론되는 인공신경망 모델을 활용하여 엣지 컴퓨팅에 활용될 수 있는 다양한 전자 장치들을 의미할 수 있다. 여기서 엣지 컴퓨팅은 컴퓨팅이 일어나는 가장자리, 주변부란 의미로, 데이터를 직접 생산하는 단말기나 단말기와 근접한 위치에 있는 다양한 전자 장치들을 의미할 수 있다. NPU은 neural processing unit(NPU)이라고 지칭되는 것도 가능하 다. 엣지 디바이스는 인공 신경망을 포함하는, 예를 들면, 휴대폰, 스마트 폰, 인공지능 스피커, 디지털 방송 단말기, 네비게이션, 웨어러블 디바이스, 스마트 시계, 스마트 냉장고, 스마트 TV, 디지털 사이니지, VR 장치, AR 장치, 인공지능 CCTV, 인공지능 로봇 청소기, 태블릿, 노트북 컴퓨터, 자율 주행 자동차, 자율 주행 드론, 자율 주행 2족 보행 로봇, 자율 주행 4족 보행 로봇, 자율 주행 모빌리티, 인공지능 로봇, 등을 포함할 수 있다. 다만, 본 개시의 예시들에 따른 엣지 디바이스는 상술한 전자 장치 들에 제한되지 않는다. 엣지 디바이스는 적어도 NPU을 포함하고, 무선 통신부, 입력부, 출력부, 인터페 이스, 시스템 버스, 메모리, 중앙 처리 유닛, 및 전원 제어부 중 적어도 일부를 선택적으로 더 포함하도록 구성될 수 있다. 또한 엣지 디바이스는 무선 통신부를 통해서 인터넷과 연결되어 클라우드 인공지능 서비스를 제공받을 수 있다. 시스템 버스는 엣지 디바이스의 각각의 구성요소들의 데이터 통신을 제어하도록 구성된다. 상기 시 스템 버스는 기판 상에 형성된 전기 전도성 패턴(electrically conductive pattern)에 의해서 구현될 수 있다. 이를 위해, 전술한 구성 요소들은 상기 기판 상의 전기 전도성 패턴과 전기 접속되도록, 상기 기판 상에 체결될 수 있다. 상기 시스템 버스는 엣지 디바이스의 교통 시스템이다. 시스템 버스는 컴퓨터 버스라고 지칭 될 수 있다. 엣지 디바이스의 모든 구성요소는 고유의 어드레스를 가질 수 있으며, 시스템 버스는 어드레스를 통해서 각각의 구성요소들을 연결시킬 수 있다. 시스템 버스는 예를 들면, 3가지 종류의 데이 터를 처리할 수 있다. 첫째, 시스템 버스는 데이터 전송을 할 때 데이터가 메모리에 저장된 어드레 스를 처리할 수 있다. 둘째, 시스템 버스는 해당 어드레스에 저장된 연산 결과 등 의미 있는 데이터를 처 리할 수 있다. 셋째, 시스템 버스는 어드레스 데이터와 데이터를 어떻게 처리하고, 언제 어디로 데이터가 이동해야 하는지 등의 데이터 흐름을 처리할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 중 앙 처리 유닛에서 생성되는 다양한 제어 신호는 시스템 버스를 통해서 대응되는 구성요소에 전달될 수 있다. 무선 통신부는 엣지 디바이스와 무선 통신 시스템 사이, 엣지 디바이스와 다른 엣지 디바이 스 사이, 또는 엣지 디바이스와 인터넷 사이의 무선 통신을 가능하게 하는 하나 이상의 통신 모듈을 포함 할 수 있다. 예를 들면, 무선 통신 부는 이동통신 송수신기, 근거리 통신 송수신기, 위치정보 수신기 중 적어도 하나를 포함할 수 있다. 무선 통신부의 이동통신 송수신기은 이동통신을 위한 기술표준들 또는 통신방식에 따라 구축된 이 동 통신망 상에서 기지국, 외부의 단말, 서버 중 적어도 하나와 무선 신호를 송수신을 위한 모듈을 의미한다. 이동통신 송수신기은 엣지 디바이스에 내장되거나 외장 될 수 있다. 기술표준들은 예를 들면, LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), LTE-Pro, 5G(Fifth Generation), 6G 등이 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 무선 통신부의 근거리 통신 송수신기는 근거리 통신(Short range communication)을 위한 송수신기 로서, 예를 들면 WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, 블루투 스(Bluetooth™RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra Wideband), ZigBee, NFC(Near Field Communication), Wireless USB(Wireless Universal Serial Bus) 등이 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 이러한, 근거리 통신 송수신기은, 근거리 무선 통신망(Wireless Area Networks)을 통해 엣지 디바이스 와 무선 통신 시스템 사이, 엣지 디바이스와 연동되는 다른 엣지 디바이스 사이, 또는 엣지 디바이 스와 별도의 네트워크 사이의 무선 통신을 지원할 수 있다. 예를 들면, 다른 엣지 디바이스는 본 개시에 따른 엣지 디바이스와 데이터를 상호 교환하는 것이 가능한 스마트 워치(smartwatch), 스마트 글래스 (smart glass), HMD(head mounted display) 등의 웨어러블 디바이스(wearable device)가 될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 무선 통신부의 위치정보 수신기은 엣지 디바이스의 위치를 획득하기 위한 모듈을 의미한다. 위치정보 기술들은, 예를 들면, 위성을 이용하는 GNSS(Global Navigation Satellite System), 블루투스를 이용하는 방식, 비컨을 이용하는 방식, Wi-Fi(Wireless Fidelity)를 이용하는 방식 등이 있다. GNSS에는 미국의 GPS(Global Positioning System), 러시아의 GLONASS(Global Navigation Satellite System) 또는 유럽의 GALILEO(European Satellite Navigation System) 등이 있다. 예를 들어, 엣지 디바이스는 위성에서 보내는 신호를 이용하여 엣지 디바이스의 위치를 획득할 수 있다. 다른 예로서, 엣지 디바이스는 Wi-Fi 모듈을 활용하면, Wi-Fi 모듈과 무선 신호를 송신 또는 수신 하는 무선AP(Wireless Access Point)의 데이터에 기반하여, 엣지 디바이스의 위치를 획득할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 무선 통신부를 통해서, 엣지 디바이스는 인터넷과 연결될 수 있으며, 엣지 디바이스는 다양 한 형태의 인공지능 서비스를 제공받을 수 있다. 예를 들면, 엣지 디바이스는 “오늘의 날씨는 어때?”라는 음성 신호를 무선 통신부를 통해서 인터 넷 상의 클라우드 인공지능 서비스에 전달하고, 클라우드 인공지능 서비스는 전달받은 음성 신호의 추론 결과를 무선 통신부를 통해서 엣지 디바이스로 전달할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되 지 않는다. 입력부는 엣지 디바이스에 입력되는 다양한 데이터 또는 신호를 제공하는 다양한 구성 요소들을 포 함할 수 있다, 입력부는 영상 신호 입력을 위한 카메라, 음향 신호 입력을 위한 마이크로폰, 사용자로부터 데이터를 입력 받기 위한 사용자로부터의 입력 수신기, 거리를 감지하기 위한 근접 센서 , 주변 광량을 감지하기 위한 조도 센서, 특정 주파수의 전파를 발사하여 물체를 감지하기 위한 레 이더, 레이저를 발사하여 물체를 감지하기 위한 라이다, 자이로스코프(gyroscope) 센서 그리 고 가속도 센서 등을 포함할 수 있다. 입력부는 영상 데이터, 음향 데이터, 사용자 입력 데이터, 거리 데이터 중 적어도 하나의 데이터 제공 기 능을 수행하도록 구성될 수 있다. 입력부의 카메라는 NPU에 의해서 추론되는 영상 처리, 제스처 인식, 객체 인식, 이벤트 인식 등을 위한 카메라일 수 있다. 입력부의 카메라는 정지영상 또는 동영상 데이터를 제공할 수 있다. 입력부의 카메라의 영상 신호는 중앙 처리 유닛에 전달될 수 있다. 영상 신호가 중앙 처리 유닛에 전달되면, 중앙 처리 유닛는 NPU에 영상 신호를 전달하도록 구성될 수 있다. 이때, 중 앙 처리 유닛는 영상 처리 프로세싱을 수행할 수 있으며, 영상 처리된 영상 신호가 NPU에 전달 될 수 있다. 단, 이에 제한되지 않으며 시스템 버스는 NPU에 영상 신호를 전달하는 것도 가능하다. 입력부의 카메라의 영상 신호는 NPU에 전달될 수 있다. 영상 신호가 NPU에 전달되면, NPU은 추론된 결과를 중앙 처리 유닛에 전달하도록 구성될 수 있다. 이때, NPU에서 작동되는 인공신경망모델에 따라서 영상 처리, 제스처 인식, 객체 인식, 이벤트 인식 등의 추론 연산을 수행할 수 있으며, 추론된 결과가 중앙 처리 유닛에 전달될 수 있다. 단, 이에 제한되지 않으며 NPU은 시스템 버스를 통해서 중앙 처리 유닛가 아닌 다른 구성요소에 추론된 결과를 전달하는 것도 가능하다. 입력부의 카메라는 적어도 하나 이상으로 구성될 수 있다. 예를 들면, 입력부의 카메라 는 자율 주행 자동차의 자율 주행용 전방, 후방, 좌측, 및 우측 방향의 영상 신호를 제공하는 복수의 카 메라일 수 있다. 또한 실내 운전자의 상태를 파악하기 위한 차량 실내 카메라가 더 포함될 수 있다. 예를 들면, 입력부의 카메라는 스마트 폰에서 서로 다른 화각을 가지는 복수의 카메라일 수 있다. 입력부의 카메라는 가시광선 카메라, 근적외선 카메라, 및 열화상 카메라 중 적어도 하나로 구성될 수 있다. 단, 이에 제한되지 않으며, 카메라는 가시광선, 근적외선을 동시에 감지하도록 구성된 복합 이 미지 센서로 구성되어 가시광선과 근적외선을 동시에 감지하도록 구성되는 것도 가능하다. 입력부의 카메라가 복수의 카메라일 경우, NPU의 추론 성능 향상을 위해서 엣지 디바이스 는 영상 신호를 배치(batch) 모드 형태로 NPU에 제공할 수 있다. 입력부의 마이크로폰은 외부의 음향 신호를 전기적인 음성 데이터로 변환하여 출력한다. 음성 데이 터는 아날로그 신호 또는 디지털 신호로 출력될 수 있다. 마이크로폰에는 외부의 음향 신호를 입력 받는 과정에서 발생되는 잡음(noise)을 제거하기 위한 다양한 잡음 제거 알고리즘이 구현될 수 있다. 입력부의 마이크로폰은 적어도 하나 이상으로 구성될 수 있다. 예를 들면, 복수의 마이크로폰들 은 양쪽 귀에 위치하는 2개의 이어폰 각각에 배치된 마이크로폰 일 수 있다. 입력부의 마이크로폰의 음향 신호는 중앙 처리 유닛에 전달될 수 있다. 음향 신호가 중앙 처 리 유닛에 전달되면, 시스템 버스를 통해서 NPU에 음향 신호가 전달될 수 있다. 이때, 중앙 처리 유닛는 푸리에 변환으로 음향 신호를 주파수 도메인으로 변환할 수 있으며, 변환된 음향 신호가 NPU에 전달될 수 있다. 단, 이에 제한되지 않으며 시스템 버스를 통해서 중앙 처리 유닛가 아 닌, 다른 구성요소를 통해서 NPU에 영상 신호를 전달하는 것도 가능하다. 입력부의 마이크로폰의 음향 신호는 NPU에 전달될 수 있다. 음향 신호가 NPU에 전달되면, NPU은 추론된 결과를 중앙 처리 유닛에 전달하도록 구성될 수 있다. 이때, NPU에서 작동되는 인공신경망모델에 따라서 음향 처리, 키워드 인식, 노이즈 제거, 문장 인식, 다른 언어로 변역 등의 추론 연산을 수행할 수 있으며, 추론된 결과가 중앙 처리 유닛에 전달될 수 있다. 단, 이에 제한되지 않 으며 NPU은 중앙 처리 유닛가 아닌, 전원 제어부, 무선 통신부, 인터페이스, 출 력부, 또는 메모리 등 다른 구성요소에 추론된 결과를 전달하는 것도 가능하다. 입력부의 사용자로부터의 입력 수신기은 예를 들면, 터치 버튼(touch button), 푸시 버튼(push button), 터치 패널(touch panel), 마우스(mouse), 키보드(keyboard), 터치 패드(touch pad), 리모트 컨트롤러 (remote controller), 사용자의 제스쳐(gesture) 인식기 중 적어도 하나를 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. NPU은 작동되는 인공신경망모델에 따라서 사용자로부터의 입력 수신기 의 신호를 입력 받아, 대응되는 추론 연산을 수행하도록 구성될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 입력부의 사용자로부터의 입력 수신기은 사용자로부터 데이터를 입력 받기 위한 것으로서, 사용자 로부터의 입력 수신기를 통해 데이터가 입력되면, 중앙 처리 유닛는 입력된 데이터에 대응되도록 엣지 디바이스의 동작을 제어할 수 있다. 이러한, 사용자로부터의 입력 수신기은 기계식 입력수단, 버튼, 스위치 및 터치식 입력수단을 포함할 수 있다. 터치식 입력수단은, 소프트웨어 적인 처리를 통해 터치스 크린에 표시되는 비주얼 키(visual key) 또는 터치스크린 이외의 부분에 배치되는 터치 키(touch key)로 이루어 질 수 있다. 터치 스크린은 저항막 방식, 정전용량 방식, 적외선 방식, 초음파 방식, 자기장 방식 등 여러 가지 터치방식 중 적어도 하나를 이용하여 디스플레이에 입력되는 터치를 감지할 수 있다. 터치 스크린은 터치 물체의 위치, 면적, 압력 등을 검출할 수 있도록 구성될 수 있다. 예를 들면, 정전용량 방식의 터치 스크린은 특정 부위에 가해진 압력 또는 특정 부위에 발생하는 정전 용량 등의 변화를 전기적인 입력신호로 변환하도록 구성될 수 있다. 예를 들면, 터치 물체는 손가락, 터치펜 또는 스타일러스 펜(Stylus pen), 포인터 등이 될 수 있다. 입력부의 근접센서는 엣지 디바이스에 접근하는 물체, 혹은 근방에 존재하는 물체의 유무를 전자계의 힘 또는 적외선 등을 이용하여 기계적 접촉이 없이 검출하는 센서를 의미한다. 예를 들면, 근접 센서 는 투과형 광전 센서, 직접 반사형 광전 센서, 미러 반사형 광전 센서, 고주파 발진형 근접 센서, 정전 용량형 근접 센서, 자기형 근접 센서, 적외선 근접 센서 등이 있다. 단, 본 개시에 따른 예시들은 이에 제한되 지 않는다. NPU은 작동되는 인공신경망모델에 따라서 근접센서의 신호를 입력 받아, 대응되는 추론 연산을 수행하도록 구성될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 입력부의 조도 센서는 포토 다이오드를 활용하여 엣지 디바이스의 주변 광량을 감지할 수 있 는 센서를 의미한다. NPU은 작동되는 인공신경망모델에 따라서 조도센서의 신호를 입력 받아, 대응 되는 추론 연산을 수행하도록 구성될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 입력부의 레이더는 전자기파를 송출하여 물체에 반사되는 신호를 감지하여, 물체의 거리와 각도 속 도 등의 데이터를 제공할 수 있다. 엣지 디바이스는 복수의 레이더를 포함하도록 구성될 수 있다. 레이더는 단거리 레이더(short range radar), 중거리 레이더(middle range radar), 장거리 레이더(long range radar) 중 적어도 하나를 포함하도록 구성될 수 있다. NPU은 작동되는 인공신경망모델에 따라서 레 이더의 신호를 입력 받아, 대응되는 추론 연산을 수행하도록 구성될 수 있다. 단, 본 개시에 따른 예시들 은 이에 제한되지 않는다. 입력부의 라이다는 광신호를 일정한 방식으로 조사하여 물체에 반사되는 광 에너지를 분석하여, 주 변의 3차원 공간 데이터를 제공할 수 있다. 엣지 디바이스는 복수의 라이다를 포함하도록 구성될수 있다. 상기 자이로 센서는 상기 엣지 디바이스의 회전 동작을 감지할 수 있다. 구체적으로, 상기 자이로 센서는 회전 각속도를 측정할 수 있다. 각속도는 회전 운동을 할 때 생기는 코리올리 힘(Coriolis Forc e)를 전기적 신호로 변환하여 계산될 수 있다. 코리올리 힘이란 운동하는 물체의 속도에 비례하여 운동방향에 수직인 힘을 의미한다. 상기 자이로 센서은 회전각과 기울기 등을 측정하여 출력할 수 있다. 상기 가속도 센서는 상기 엣지 디바이스가 이동될 때 이동 가속도를 측정할 수 있다. 상기 자이로 센서와 상기 가속도 센서의 조합을 통하여, 상기 엣지 디바이스의 여러 움직임 을 측정할 수 있다. NPU은 작동되는 인공신경망모델에 따라서 라이다의 신호를 입력 받아, 대응되는 추론 연산을 수행하 도록 구성될 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 단, 입력부는 상술한 예시들에 제한되지 않으며, 자기 센서(magnetic sensor), 중력 센서(G-sensor), 모 션 센서(motion sensor), 지문인식 센서(finger scan sensor), 초음파 센서(ultrasonic sensor), 배터리 게이 지(battery gauge), 기압계, 습도계, 온도계, 방사능 감지 센서, 열 감지 센서, 가스 감지 센서, 화학 물질 감 지 센서 중 적어도 하나를 더 포함하도록 구성될 수 있다. 본 개시의 예시들에 따른 엣지 디바이스의 NPU은 작동되는 인공신경망모델에 따라서 입력부의 신호를 입력 받아, 대응되는 추론 연산을 수행하도록 구성될 수 있다. 본 개시의 예시들에 따른 엣지 디바이스는 입력부에서 입력되는 다양한 입력 데이터들을 NPU 에 제공하여 다양한 추론 연산을 수행하도록 구성될 수 있다. 입력 데이터들은 중앙 처리 유닛에서 전 처 리된 후 NPU에 입력되는 것도 가능하다. 예를 들면, NPU은 카메라, 레이더 및 라이다 각각의 입력 데이터를 선택적으로 입력 받 아, 자율 주행 용 주변 환경 데이터를 추론하도록 구성될 수 있다. 예를 들면, NPU은 카메라 및 레이더의 입력 데이터를 입력 받아, 자율 주행에 필요한 주변 환 경 데이터를 추론하도록 구성될 수 있다. 출력부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시키기 위한 것으로, 디스플레이, 스피커 , 햅팁 출력기, 광 출력기 중 적어도 하나를 포함할 수 있다. 디스플레이은 복수의 픽 셀 어레이로 이루어진 액정 패널 또는 유기 발광 표시 패널 등 일 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 광 출력기은 엣지 디바이스(100 0)의 광원의 빛을 이용하여 이벤트 발생을 알리기 위한 광 신호를 출력할 수 있다. 발생 되는 이벤트의 예로는 메시지 수신, 부재중 전화, 알람, 일정 알림, 이메일 수신, 애플리케이션을 통한 데이터 수신 등이 될 수 있다. 인터페이스는 엣지 디바이스에 연결되는 모든 외부 기기와의 통로 역할을 한다. 인터페이스 는 외부 기기로부터 데이터를 전송 받거나, 전원을 공급받아 엣지 디바이스 내부의 각 구성요소에 전달하 거나, 엣지 디바이스 내부의 데이터가 외부 기기로 전송되도록 한다. 예를 들어, 유/무선 헤드셋 포트 (port), 외부 충전기 포트(port), 유/무선 데이터 포트(port), 메모리 카드(memory card) 포트(port), 식별 모 듈이 구비된 장치를 연결하는 포트(port), 오디오 I/O(Input/Output) 포트(port), 비디오 I/O(Input/Output) 포트(port), 이어폰 포트(port) 등이 인터페이스에 포함될 수 있다. 메모리은 엣지 디바이스의 제어에 따라 데이터를 저장하는 장치이다. 메모리는 휘발성 메모리 (Volatile Memory)와 비휘발성 메모리(Non-Volatile Memory)를 선택적으로 포함할 수 있다. 휘발성 메모리 장치는 전원이 공급된 경우에만 데이터를 저장하고, 전원 공급이 차단되면 저장된 데이터가 소멸 되는 메모리 장치일 수 있다. 비휘발성 메모리 장치는 전원 공급이 중단된 경우에도 데이터가 저장되는 장치일 수 있다. 메모리은 중앙 처리 유닛 또는 NPU의 동작을 위한 프로그램을 저장할 수 있고, 입/출력되는 데 이터들을 임시 저장할 수도 있다. 메모리은 플래시 메모리 타입(flash memory type), 하드디스크 타입 (hard disk type), SSD 타입(Solid State Disk type), SDD 타입(Silicon Disk Drive type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램 (random access memory; RAM), DRAM(dynamic random access memory), SRAM(static random access memory),MRAM(magnetic random access memory), STT-MRAM(spin-transfer torque magnetic random access memory), eMRAM(embedded magnetic random access memory), OST-MRAM(orthogonal spin transfer magnetic random access memory), PRAM(phase change RAM), FeRAM(ferroelectric RAM), 롬(read-only memory; ROM), EEPROM(electrically erasable programmable read-only memory), PROM(programmable read-only memory), 자기 메모리, 자기 디스크 및 광디스크 중 적어도 하나의 타입의 저장매체를 포함할 수 있다. 후술될 다양한 인공신경망모델들은 메모리의 비휘발성 메모리 장치에 저장될 수 있다. 인공신경망모델들 중 적어도 하나는 엣지 디바이스의 명령에 의해서 NPU의 휘발성 메모리에 저장되어 추론 연산 기능 을 제공할 수 있다. 중앙 처리 유닛는 엣지 디바이스의 전반적인 동작을 제어할 수 있다. 예를 들어, 중앙 처리 유닛 는 중앙 처리 장치(CPU), 어플리케이션 프로세서(AP), 또는 디지털 신호 처리 장치(DSP)일 수 있다. 중앙 처리 유닛는 엣지 디바이스를 제어하거나, 다양한 명령을 수행할 수 있다. 중앙 처리 유닛는 신경망 프로세스 유닛에 필요한 데이터를 제공 하거나 제공 받을 수 있다. 중앙 처리 유닛는 시스템 버스와 연결된 다양한 구성요소들을 제어할 수 있다. 전원 제어부는 각각의 구성 요소들의 전원을 제어하도록 구성된다. 중앙 처리 유닛는 전원 제어부 를 제어하도록 구성될 수 있다. 전원 제어부는 외부의 전원, 내부의 전원을 인가 받아 엣지 디바이 스에 포함된 각 구성요소들에 전원을 공급한다. 이러한 전원 제어부는 배터리를 포함할 수 있다. 전 원 제어부는 특정 시간 동안 중앙 처리 유닛로부터 제어 신호를 제공받지 않을 경우, 선택적으로 엣지 디바이스의 각각의 구성 요소의 전원의 공급을 차단할 수 있다. 또한 NPU은 상시 동작하며, 특 정 상황을 추론하여, 중앙 처리 유닛에 기상 신호를 제공하도록 구성될 수 있다. 중앙 처리 유닛는 NPU의 추론 결과에 의해서 전원 제어부가 엣지 디바이스의 특정 구성요소에 전원을 공급하도 록 제어할 수 있다. NPU은 다양한 인공 신경망 추론 연산을 수행하도록 구성된다. NPU은 중앙 처리 유닛가 연산하 는데 비효율 적인 인공 신경망 추론 연산을 효율적으로 연산하도록 구성된 것을 특징으로 한다. 도 4a에 도시된 엣지 디바이스의 하나의 예시일 뿐이며, 엣지 디바이스가 포함할 수 있는 다양한 구성요소들을 도시하고 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 각각의 구성요소는 예시의 목적 및 구성에 따라서 선택적으로 포함하거나 배제할 수 있다. 즉, 도 4a에 도시된 구성요소들 중 일부는 경우 에 따라서 필수적인 구성요소가 아닐 수 있으며, 각각의 예시는 최적화 관점에서 도 4a에 도시된 구성요소들 중 일부를 포함하거나 또는 포함하지 않는 것이 바람직할 수 있다. 도 4b은 도 4a에 도시된 엣지 디바이스의 변형예를 나타낸 블록도이다. 도 4b에 도시된 엣지 디바이스는 도 4a에 도시된 엣지 디바이스와 달리, 일부 구성 요소들만을 포함하고 있다. 이와 같이, 엣지 디바이스는 용도에 따라 일부의 구성 요소들만을 포함하여 구현될 수 있다. 도 4b에 도시된 엣지 디바이스가 예를 들어 AR(Augmented Reality) 기기 혹은 VR(Virtual Reality) 기기 라면, 엣지 디바이스는 하나의 NPU을 사용하여, 영상 인식, 키워드 인식 및 제스처 인식을 수행할 수 있다. 즉, 하나의 NPU은 복수의 추론 기능을 제공할 수 있다. 이와 같이 하나의 NPU으로 복수의 추론 연산을 수행함으로써, 엣지 디바이스의 부품 수 및 제조 비 용을 저감할 수 있다. 도 5는 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 이하 NPU에서 작동될 수 있는 예시적인 인공신경망모델(110-10)의 연산에 대하여 설명한다. 도 5의 예시적인 인공신경망모델(110-10)은 도 1 또는 도 3에 도시된 NPU에서 학습되거나 별도의 기계 학 습 장치에서 학습된 인공 신경망일 수 있다. 인공신경망 모델은 객체 인식, 음성 인식 등 다양한 추론 기능을 수행하도록 학습된 인공 신경망일 수 있다. 인공신경망모델(110-10)은 심층 신경망(DNN, Deep Neural Network)일 수 있다. 단, 본 개시의 예시들에 따른 인공신경망모델(110-10)은 심층 신경망에 제한되지 않는다. 예를 들어, 인공신경망모델(110-10)은 VGG, VGG16, DenseNet 및, encoder-decoder structure를 갖는 FCN (Fully Convolutional Network), SegNet, DeconvNet, DeepLAB V3+, U-net와 같은 DNN (deep neural network), SqueezeNet, Alexnet, ResNet18, MobileNet-v2, GoogLeNet, Resnet-v2, Resnet50, Resnet101, Inception-v3 등의 모델로 구현될 수 있다. 단, 본 개시는 상술한 모델들에 제한되지 않는다. 또한 인공신경망모델(110-10)은 적어도 두 개의 서로 다른 모델들에 기초한 앙상블 모델일 수도 있다. 인공신경망모델(110-10)은 NPU의 NPU 내부 메모리에 저장될 수 있다. 혹은 인공신경망모델(110-10)은 도 4a 또는 도 4b에 도시된 엣지 디바이스의 메모리에 저장된 다음, 인공신경망모델(110-10)의 구동 시 NPU의 NPU 내부 메모리에 로딩되는 방식으로 구현되는 것도 가능하다. 이하 도 5를 참조하여 예시적인 인공신경망모델(110-10)에 의환 추론 과정이 NPU에 의해서 수행되는 것에 관해 설명한다. 인공신경망모델(110-10)은 입력 레이어(110-11), 제1 연결망(110-12), 제1 은닉 레이어(110-13), 제2 연결망 (110-14), 제2 은닉 레이어(110-15), 제3 연결망(110-16), 및 출력 레이어(110-17)을 포함하는 예시적인 심층 신경망 모델이다. 단, 본 개시는 도 4에 도시된 인공신경망모델에만 제한되는 것은 아니다. 제1 은닉 레이어 (110-13) 및 제2 은닉 레이어(110-15)는 복수의 은닉 레이어로 지칭되는 것도 가능하다. 입력 레이어(110-11)는 예시적으로, x1 및 x2 입력 노드를 포함할 수 있다. 즉, 입력 레이어(110-11)는 2개의 입력 값에 대한 정보를 포함할 수 있다. 도 1 또는 도 3에 도시된 NPU 스케줄러는 입력 레이어(110-11)로 부터의 입력 값에 대한 정보가 저장되는 메모리 어드레스를 도 1 또는 도 3에 도시된 NPU 내부 메모리에 설정할 수 있다. 제1 연결망(110-12)은 예시적으로, 입력 레이어(110-11)의 각각의 노드를 제1 은닉 레이어(110-13)의 각각의 노 드로 연결시키기 위한 6개의 가중치 값에 대한 정보를 포함할 수 있다. 도 1 또는 도 3에 도시된 NPU 스케줄러 는 제1 연결망(110-12)의 가중치 값에 대한 정보가 저장되는 메모리 어드레스를 NPU 내부 메모리에 설정할 수 있다. 각각의 가중치 값은 입력 노드 값과 곱해지고, 곱해진 값들의 누산된 값이 제1 은닉 레이어 (110-13)에 저장된다. 제1 은닉 레이어(110-13)는 예시적으로 a1, a2, 및 a3 노드를 포함할 수 있다. 즉, 제1 은닉 레이어(110-13)는 3개의 노드 값에 대한 정보를 포함할 수 있다. 도 1 또는 도 3에 도시된 NPU 스케줄러는 제1 은닉 레이어 (110-13)의 노드 값에 대한 정보를 저장시키기 위한 메모리 어드레스를 NPU 내부 메모리에 설정할 수 있다. 제2 연결망(110-14)은 예시적으로, 제1 은닉 레이어(110-13)의 각각의 노드를 제2 은닉 레이어(110-15)의 각각 의 노드로 연결시키기 위한 9개의 가중치 값에 대한 정보를 포함할 수 있다. 도 1 또는 도 3에 도시된 NPU 스케 줄러는 제2 연결망(110-14)의 가중치 값에 대한 정보를 저장시키기 위한 메모리 어드레스를 NPU 내부 메모 리에 설정할 수 있다. 상기 제2 연결망(110-14)의 가중치 값은 제1 은닉 레이어(110-13)로부터 입력되는 노드 값과 각기 곱해지고, 곱해진 값들의 누산된 값이 제2 은닉 레이어(110-15)에 저장된다. 제2 은닉 레이어(110-15)는 예시적으로 b1, b2, 및 b3 노드를 포함할 수 있다. 즉, 제2 은닉 레이어(110-15)는 3개의 노드 값에 대한 정보를 포함할 수 있다. NPU 스케줄러는 제2 은닉 레이어(110-15)의 노드 값에 대한 정보를 저장시키기 위한 메모리 어드레스를 NPU 내부 메모리에 설정할 수 있다. 제3 연결망(110-16)은 예시적으로, 제2 은닉 레이어(110-15)의 각각의 노드와 출력 레이어(110-17)의 각각의 노 드를 연결하는 6개의 가중치 값에 대한 정보를 포함할 수 있다. NPU 스케줄러는 제3 연결망(110-16)의 가 중치 값에 대한 정보를 저장시키기 위한 메모리 어드레스를 NPU 내부 메모리에 설정할 수 있다. 제3 연결 망(110-16)의 가중치 값은 제2 은닉 레이어(110-15)로부터 입력되는 노드 값과 각기 곱해지고, 곱해진 값들의 누산된 값이 출력 레이어(110-17)에 저장된다. 출력 레이어(110-17)는 예시적으로 y1, 및 y2 노드를 포함할 수 있다. 즉, 출력 레이어(110-17)는 2개의 노드 값에 대한 정보를 포함할 수 있다. NPU 스케줄러는 출력 레이어(110-17)의 노드 값에 대한 정보를 저장시 키기 위해 메모리 어드레스를 NPU 내부 메모리에 설정할 수 있다. 즉, NPU 스케줄러는 프로세싱 엘리먼트 어레이에서 작동할 인공신경망모델의 구조를 분석하거나 또는 제공받을 수 있다. 인공신경망모델이 포함할 수 있는 인공 신경망의 정보는 각각의 레이어의 노드 값에 대한 정 보, 레이어들의 배치 데이터 지역성 정보 또는 구조에 대한 정보, 각각의 레이어의 노드를 연결하는 연결망 각 각의 가중치 값에 대한 정보를 포함할 수 있다.NPU 스케줄러는 예시적인 인공신경망모델(110-10)의 데이터 지역성 정보 또는 구조에 대한 정보를 제공받 았기 때문에, NPU 스케줄러는 인공신경망모델(110-10)의 입력부터 출력까지의 연산 순서를 파악할 수 있다. 따라서, NPU 스케줄러는 각각의 레이어의 MAC 연산 값들이 저장되는 메모리 어드레스를 스케줄링 순서를 고려해서 NPU 내부 메모리에 설정할 수 있다. 예를 들면, 특정 메모리 어드레스는 입력 레이어(110-11)와 제1 연결망(110-12)의 MAC 연산 값일 수 있으며, 동시에 제1 은닉 레이어(110-13)의 입력 데이터 일 수 있다. 단 본 개시는 MAC 연산 값에 제한되지 않으며, MAC 연산 값은 인공 신경망 연산 값으로 지칭되는 것도 가능하다. 이때, NPU 스케줄러는 입력 레이어(110-11)와 제1 연결망(110-12)의 MAC 연산 결과가 제1 은닉 레이어 (110-13)의 입력이 될 것을 알기 때문에, 동일한 메모리 어드레스를 사용하도록 제어할 수 있다. 즉, NPU 스케 줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 MAC 연산 값을 재사용할 수 있다. 따라서 NPU 시스템 메모리가 메모리 재사용 기능을 제공할 수 있는 효과가 있다. 즉, NPU 스케줄러는 스케줄링 순서에 따른 인공신경망모델(110-10)의 MAC 연산 값을 NPU 내부 메모리(12 0)의 임의 메모리 어드레스에 지정된 특정 영역에 저장하고, MAC 연산 값이 저장된 특정 영역에서 다음 스케줄 링 순서의 MAC 연산의 입력 데이터로 사용될 수 있다. 제1 프로세싱 엘리먼트(PE1) 관점에서의 MAC 연산 MAC 연산을 제1 프로세싱 엘리먼트(PE1) 관점에서 자세히 설명한다. 제1 프로세싱 엘리먼트(PE1)는 제1 은닉 레 이어(110-13)의 a1 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제1 프로세싱 엘리먼트(PE1)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x1 노드 데이터를 입 력하고, 제2 입력부에 x1 노드와 a1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0이 다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값은 1이 될 수 있다. 둘째, 제1 프로세싱 엘리먼트(PE1)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x2 노드 데이터를 입 력하고, 제2 입력부에 x2 노드와 a1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 x1 노드 데이터와 x1 노드와 a1 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 a1 노드에 대응되는 x1 노드와 x2 노드의 MAC 연산 값을 생성한다. 셋째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누산된 값에 따라 적절히 조절될 수 있다. 부연 설명하면, (L)loops가 증가할 수록, 출력되는 값의 비트 폭이 증가되게 된다. 이때 NPU 스케줄러는 제1 프로세싱 엘리먼트(PE1)의 연산 값의 비트 폭이 (x)bit이 되도록 소정의 하위 비트를 제거할 수 있다. 제2 프로세싱 엘리먼트(PE2) 관점에서의 MAC 연산 MAC 연산을 제2 프로세싱 엘리먼트(PE2) 관점에서 자세히 설명한다. 제2 프로세싱 엘리먼트(PE2)는 제1 은닉 레 이어(110-13)의 a2 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제2 프로세싱 엘리먼트(PE2)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x1 노드 데이터를 입 력하고, 제2 입력부에 x1 노드와 a2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0이 다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값은 1이 될 수 있다. 둘째, 제2 프로세싱 엘리먼트(PE2)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x2 노드 데이터를 입 력하고, 제2 입력부에 x2 노드와 a2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 x1 노드 데이터와 x1노드와 a2 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 a2 노드에 대응되는 x1 노드와 x2 노드의 MAC 연산 값을 생성한다. 셋째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 제3 프로세싱 엘리먼트(PE3) 관점에서의 MAC 연산 MAC 연산을 제3 프로세싱 엘리먼트(PE3) 관점에서 자세히 설명한다. 제3 프로세싱 엘리먼트(PE3)는 제1 은닉 레 이어(110-13)의 a3 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제3 프로세싱 엘리먼트(PE3)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x1 노드 데이터를 입 력하고, 제2 입력부에 x1 노드와 a3 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0이 다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값은 1이 될 수 있다. 둘째, 제3 프로세싱 엘리먼트(PE3)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 x2 노드 데이터를 입 력하고, 제2 입력부에 x2 노드와 a3 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 x1 노드 데이터와 x1 노드와 a3 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 a3 노드에 대응되는 x1 노드와 x2 노드의 MAC 연산 값을 생성한다. 셋째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 따라서, NPU의 NPU 스케줄러는 3개의 프로세싱 엘리먼트(PE1 내지 PE3)를 동시에 사용하여 제1 은닉 레이어(110-13)의 MAC 연산을 수행할 수 있다. 제4 프로세싱 엘리먼트(PE4) 관점에서의 MAC 연산 MAC 연산을 제4 프로세싱 엘리먼트(PE4) 관점에서 자세히 설명한다. 제4 프로세싱 엘리먼트(PE4)는 제2 은닉 레 이어(110-15)의 b1 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제4 프로세싱 엘리먼트(PE4)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a1 노드 데이터를 입력하고, 제2 입력부에 a1 노드와 b1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0 이다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값 은 1이 될 수 있다. 둘째, 제4 프로세싱 엘리먼트(PE4)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a2 노드 데이터를 입력하고, 제2 입력부에 a2 노드와 b1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 a1 노드 데이터와 a1 노드와 b1 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 b1 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값을 생성한다. 이때, (L)loops의 카운터 값은 2가 될 수 있다. 셋째, 제4 프로세싱 엘리먼트(PE4)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 a3 노드 데이터를 입 력하고, 제2 입력부에 a3 노드와 b1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 2일 경우, 이전 단계에서 연산 된 b1 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값이 저장되었다. 따라서 가산기는 b1 노드에 대응되는 a1 노드, a2 노드 및 a3 노드의 MAC 연산 값을 생성한다. 넷째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 제5 프로세싱 엘리먼트(PE5) 관점에서의 MAC 연산 MAC 연산을 제5 프로세싱 엘리먼트(PE5) 관점에서 자세히 설명한다. 제5 프로세싱 엘리먼트(PE5)는 제2 은닉 레 이어(110-15)의 b2 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제5 프로세싱 엘리먼트(PE5)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a1 노드 데이터를 입력하고, 제2 입력부에 a1 노드와 b2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0 이다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값 은 1이 될 수 있다. 둘째, 제5 프로세싱 엘리먼트(PE5)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a2 노드 데이터를 입력하고, 제2 입력부에 a2 노드와 b2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 a1 노드 데이터와 a1 노드와 b2 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 b2 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값을 생성한다. 이때, (L)loops의 카운터 값은 2가 될 수 있다. 셋째, 제5 프로세싱 엘리먼트(PE5)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 a3 노드 데이터를 입 력하고, 제2 입력부에 a3 노드와 b2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 2일 경우, 이전 단계에서 연산 된 b2 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값이 저장되었다. 따라서 가산기는 b2 노드에 대응되는 a1 노드, a2 노드 및 a3 노드의 MAC 연산 값을 생성한다. 넷째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 제6 프로세싱 엘리먼트(PE6) 관점에서의 MAC 연산 MAC 연산을 제6 프로세싱 엘리먼트(PE6) 관점에서 자세히 설명한다. 제6 프로세싱 엘리먼트(PE6)는 제2 은닉 레 이어(110-15)의 b3 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제6 프로세싱 엘리먼트(PE6)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a1 노드 데이터를 입력하고, 제2 입력부에 a1 노드와 b3 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0 이다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값 은 1이 될 수 있다. 둘째, 제6 프로세싱 엘리먼트(PE6)는 곱셈기의 제1 입력부에 제1 은닉 레이어(110-13)의 a2 노드 데이터를 입력하고, 제2 입력부에 a2 노드와 b3 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 a1 노드 데이터와 a1 노드와 b3 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 b3 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값을 생성한다. 이때, (L)loops의 카운터 값은 2가 될 수 있다. 셋째, 제6 프로세싱 엘리먼트(PE6)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 a3 노드 데이터를 입 력하고, 제2 입력부에 a3 노드와 b3 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 2일 경우, 이전 단계에서 연산 된 b3 노드에 대응되는 a1 노드와 a2 노드의 MAC 연산 값이 저장되었다. 따라서 가산기는 b3 노드에 대응되는 a1 노드, a2 노드 및 a3 노드의 MAC 연산 값을 생성한다. 넷째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누산된 값에 따라 적절히 조절될 수 있다. 따라서, NPU의 NPU 스케줄러는 3개의 프로세싱 엘리먼트(PE4 내지 PE6)를 동시에 사용하여 제2 은닉 레이어(110-15)의 MAC 연산을 수행할 수 있다. 제7 프로세싱 엘리먼트(PE7) 관점에서의 MAC 연산 MAC 연산을 제7 프로세싱 엘리먼트(PE7) 관점에서 자세히 설명한다. 제7 프로세싱 엘리먼트(PE7)는 출력 레이어 (110-17)의 y1 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제7 프로세싱 엘리먼트(PE7)는 곱셈기의 제1 입력부에 제2 은닉 레이어(110-15)의 b1 노드 데이터를 입력하고, 제2 입력부에 b1 노드와 y1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0 이다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값 은 1이 될 수 있다. 둘째, 제7 프로세싱 엘리먼트(PE7)는 곱셈기의 제1 입력부에 제2 은닉 레이어(110-15)의 b2 노드 데이터를 입력하고, 제2 입력부에 b2 노드와 y1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 b1 노드 데이터와 b1 노드와 y1 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 y1 노드에 대응되는 b1 노드와 b2 노드의 MAC 연산 값을 생성한다. 이때, (L)loops의 카운터 값은 2가 될 수 있다. 셋째, 제7 프로세싱 엘리먼트(PE7)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 b3 노드 데이터를 입 력하고, 제2 입력부에 b3 노드와 y1 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 2일 경우, 이전 단계에서 연산 된 y1 노드에 대응되는 b1 노드와 b2 노드의 MAC 연산 값이 저장되었다. 따라서 가산기는 y1 노드에 대응되는 b1 노드, b2 노드 및 b3 노드의 MAC 연산 값을 생성한다. 넷째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기 를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 제8 프로세싱 엘리먼트(PE8) 관점에서의 MAC 연산 MAC 연산을 제8 프로세싱 엘리먼트(PE8) 관점에서 자세히 설명한다. 제8 프로세싱 엘리먼트(PE8)는 출력 레이어 (110-17)의 y2 노드의 MAC 연산을 수행하도록 지정될 수 있다. 첫째, 제8 프로세싱 엘리먼트(PE8)는 곱셈기의 제1 입력부에 제2 은닉 레이어(110-15)의 b1 노드 데이터를 입력하고, 제2 입력부에 b1 노드와 y2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 0일 경우, 누산된 값이 없기 때문에, 누산된 값은 0 이다. 따라서 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. 이때, (L)loops의 카운터 값 은 1이 될 수 있다. 둘째, 제8 프로세싱 엘리먼트(PE8)는 곱셈기의 제1 입력부에 제2 은닉 레이어(110-15)의 b2 노드 데이터를 입력하고, 제2 입력부에 b2 노드와 y2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연 산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 1일 경우, 이전 단계에서 연산 된 b1 노드 데이터와 b1 노드와 y2 노드 사이의 가중치 곱셈 값이 저장되었다. 따라서 가산기는 y2 노드에 대응되는 b1 노드와 b2 노드의 MAC 연산 값을 생성한다. 이때, (L)loops의 카운터 값은 2가 될 수 있다. 셋째, 제8 프로세싱 엘리먼트(PE8)는 곱셈기의 제1 입력부에 입력 레이어(110-11)의 b3 노드 데이터를 입 력하고, 제2 입력부에 b3 노드와 y2 노드 사이의 가중치 데이터를 입력한다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 더한다. 이때 (L)loops가 2일 경우, 이전 단계에서 연산 된 y2 노드에 대응되는 b1 노드와 b2 노드의 MAC 연산 값이 저장되었다. 따라서 가산기는 y2 노드에 대응되는 b1 노드, b2 노드 및 b3 노드의 MAC 연산 값을 생성한다. 넷째, NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 제1 프로 세싱 엘리먼트(PE1)의 MAC 연산을 종료할 수 있다. 이때 초기화 신호(initialization Reset)를 입력해서 누산기를 초기화할 수 있다. 즉, (L)loops의 카운터 값을 0으로 초기화 할 수 있다. 비트 양자화 유닛은 누 산된 값에 따라 적절히 조절될 수 있다. 따라서, NPU의 NPU 스케줄러는 2개의 프로세싱 엘리먼트(PE7 내지 PE8)를 동시에 사용하여 출력 레이 어(110-17)의 MAC 연산을 수행할 수 있다. 제 8 프로세싱 엘리먼트(PE8)의 MAC 연산이 끝나면, 인공신경망모델(110-10)의 추론 연산이 마무리 될 수 있다. 즉, 인공신경망모델(110-10)은 하나의 프레임의 추론 연산을 완료했다고 판단될 수 있다. 만약 NPU이 실시 간으로 동영상 데이터를 추론한다면, 입력 레이어(110-11)의 x1 및 x2 입력 노드에 다음 프레임의 영상 데이터 가 입력될 수 있다. 이때, NPU 스케줄러는 입력 레이어(110-11)의 입력 데이터를 저장한 메모리 어드레스 에 다음 프레임의 영상 데이터를 저장할 수 있다. 이러한 과정을 프레임마다 반복하면 NPU은 실시간으로 추론 연산을 처리할 수 있다. 또한 한번 설정된 메모리 어드레스를 재사용할 수 있는 효과가 있다"}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 2, "content": "도 5의 인공신경망모델(110-10)의 경우를 요약하면, NPU은 인공신경망모델(110-10)의 추론 연산을 위해서 NPU 스케줄러가 인공신경망모델(110-10)의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여, 연산 스케줄링 순서를 결정할 수 있다. NPU 스케줄러는 연산 스케줄링 순서에 기초하여 NPU 내부 메모리에 필요한 메모리 어드레스를 설정할 수 있다. NPU 스케줄러는 인공신경망모델(110-10)의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여, 메모리를 재사용 하는 메모리 어드레스를 설정할 수 있다. NPU 스케줄러 는 추론 연산에 필요한 프로세싱 엘리먼트들(PE1 내지 PE8)을 지정하여 추론 연산을 수행할 수 있다. 부연 설명하면, 하나의 노드에 연결되는 가중치 데이터가 L개 만큼 증가하면, 프로세싱 엘리먼트의 누산기의 (L)loops의 횟수를 L-1로 설정할 수 있다. 즉, 누산기는 인공 신경망의 가중치 데이터가 증가하더라도, 누산기 의 누적 횟수를 증가시킴으로써 추론 연산을 용이하게 수행할 수 있는 효과가 있다. 즉, 본 개시의 일 예시에 따른 NPU의 NPU 스케줄러는 입력 레이어(110-11), 제1 연결망(110-12), 제 1 은닉 레이어(110-13), 제2 연결망(110-14), 제2 은닉 레이어(110-15), 제3 연결망(110-16), 및 출력 레이어 (110-17)의 데이터 지역성 정보 또는 구조에 대한 정보를 포함하는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리을 제어할 수 있다. 즉, NPU 스케줄러는 입력 레이어(110-11)의 노드 데이터, 제1 연결망(110-12)의 가중치 데이터, 제1 은닉 레이어(110-13)의 노드 데이터, 제2 연결망(110-14)의 가중치 데이터, 제2 은닉 레이어(110-15)의 노드 데이터, 제3 연결망(110-16)의 가중치 데이터, 및 출력 레이어(110-17)의 노드 데이터에 대응되는 메모리 어드레스 값들 을 NPU 메모리 시스템에 설정할 수 있다. 이하, NPU 스케줄러의 스케줄링에 대하여 자세히 설명한다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 인공신경망모델의 연산 순서를 스케줄링 할 수 있다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초하여 인공신경망모델 의 레이어의 노드 데이터 및 연결망의 가중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 예를 들면, NPU 스케줄러는 메인 메모리에 저장된 인공신경망모델의 레이어의 노드 데이터 및 연결망의 가 중치 데이터가 저장된 메모리 어드레스 값을 획득할 수 있다. 따라서 NPU 스케줄러는 구동할 인공신경망모 델의 레이어의 노드 데이터 및 연결망의 가중치 데이터를 메인 메모리에서 가져와서 NPU 내부 메모리에 저 장할 수 있다. 각각의 레이어의 노드 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. 각각의 연 결망의 가중치 데이터는 대응되는 각각의 메모리 어드레스 값을 가질 수 있다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보, 예를 들면, 인공신경망모델 의 인공 신경망의 레이어들의 배치 데이터 지역성 정보 또는 구조에 대한 정보에 기초해서 프로세싱 엘리먼트 어레이의 연산 순서를 스케줄링 할 수 있다. 예를 들면, NPU 스케줄러는 4개의 인공 신경망 레이어들과 각각의 레이어들을 연결하는 3개의 레이어의 가 중치 값들을 가지는 가중치 데이터, 즉, 연결망 데이터를 획득할 수 있다. 이러한 경우 NPU 스케줄러가 인 공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 기초로 프로세싱 순서를 스케줄링 하는 방법에 대 하여 예를 들어 아래에서 설명한다. 예를 들면, NPU 스케줄러는 추론 연산을 위한 입력 데이터를 인공신경망모델(110-10)의 입력 레이어(110- 11)인 제1 레이어의 노드 데이터로 설정하고, 제1 레이어의 노드 데이터와 제1 레이어에 대응되는 제1 연결망의 가중치 데이터의 MAC 연산을 먼저 수행하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 연산을 제1 연산이라 지칭하고, 제1 연산의 결과를 제1 연산 값이라 지칭하고, 해당 스케줄링을 제1 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제1 연산 값을 제1 연결망에 대응되는 제2 레이어의 노드 데이터로 설정하고, 제2 레이어의 노드 데이터와 제2 레이어에 대응되는 제2 연결망의 가중치 데이터의 MAC 연산을 제1 스케줄링 이 후에 수행하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 연산을 제2 연산이라 지칭하고, 제2 연산의 결과를 제2 연산 값이라 지칭하고, 해당 스케줄링을 제2 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제2 연산 값을 제2 연결망에 대응되는 제3 레이어의 노드 데이터로 설정하고, 제3 레이어의 노드 데이터와 제3 레이어에 대응되는 제3 연결망의 가중치 데이터의 MAC 연산을 제2 스케줄링에 수행하도록 스케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 연산을 제3 연산이라 지칭하고, 제3 연 산의 결과를 제3 연산 값이라 지칭하고, 해당 스케줄링을 제3 스케줄링이라 지칭할 수 있다. 예를 들면, NPU 스케줄러는 제3 연산 값을 제3 연결망에 대응되는 출력 레이어(110-17)인 제4 레이어의 노 드 데이터로 설정하고, 제4 레이어의 노드 데이터에 저장된 추론 결과를 NPU 내부 메모리에 저장하도록 스 케줄링 할 수 있다. 이하 단지 설명의 편의를 위해서 해당 스케줄링을 제4 스케줄링이라 지칭할 수 있다. 추론 결과 값은 엣지 디바이스의 다양한 구성요소들에 전달되어 활용될 수 있다. 예를 들면, 추론 결과 값이 특정 키워드를 감지한 결과 값이라면, NPU은 추론 결과를 중앙 처리 유닛 에 전달하여, 엣지 디바이스가 특정 키워드에 대응되는 동작을 수행할 수 있다. *예를 들면, NPU 스케줄러는 제1 스케줄링에서 제1 내지 제3 프로세싱 엘리먼트(PE1 내지 PE3)를 구동할 수 있다. 예를 들면, NPU 스케줄러는 제2 스케줄링에서 제4 내지 제6 프로세싱 엘리먼트(PE4 내지 PE6)를 구동할 수 있다. 예를 들면, NPU 스케줄러는 제3 스케줄링에서 제7 내지 제8 프로세싱 엘리먼트(PE7 내지 PE8)를 구동할 수 있다. 예를 들면, NPU 스케줄러는 제4 스케줄링에서 추론 결과를 출력할 수 있다. 정리하면, NPU 스케줄러는 제1 스케줄링, 제2 스케줄링, 제3 스케줄링, 및 제4 스케줄링 순서대로 연산이 수행되도록 NPU 내부 메모리과 프로세싱 엘리먼트 어레이를 제어할 수 있다. 즉, NPU 스케줄러 는 설정된 스케줄링 순서대로 연산이 수행되도록 NPU 내부 메모리과 프로세싱 엘리먼트 어레이를 제 어하도록 구성될 수 있다. 정리하면, 본 개시의 일 예시에 따른 NPU은 인공 신경망의 레이어들의 구조와, 구조에 대응되는 연산 순서 데이터에 기초하여, 프로세싱 순서를 스케줄링 하도록 구성될 수 있다. 스케줄링 되는 프로세싱 순서는 적어도 하나 이상일 수 있다, 예를 들면, NPU이 모든 연산 순서를 예측 할 수 있기 때문에, 다음 연산을 스케줄링 하는 것도 가능하고, 특정 순서의 연산을 스케줄링 하는 것도 가능하다. NPU 스케줄러는 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 기초한 스케줄링 순서를 활용하여 NPU 내부 메모리을 제어하여 메모리 재사용율을 향상시킬 수 있는 효과가 있다. 본 개시의 일 예시에 따른 NPU에서 구동되는 인공 신경망 연산의 특성상 하나의 레이어의 연산 값이 다음 레이어의 입력 데이터가 되는 특성을 가질 수 있다. 이에, NPU은 스케줄링 순서에 따라서 NPU 내부 메모리을 제어하면, NPU 내부 메모리의 메모리 재사용율을 향상시킬 수 있는 효과가 있다. 구체적으로 설명하면, NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 제 공받도록 구성되고, 제공받은 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보에 의해서 인공 신경 망의 연산이 진행되는 순서를 파악할 수 있는 경우, NPU 스케줄러는 인공신경망모델의 특정 레이어의 노드 데이터와 특정 연결망의 가중치 데이터의 연산 결과가 대응되는 레이어의 노드 데이터가 된다는 사실을 파악할 수 있다. 따라서 NPU 스케줄러는 해당 연산 결과가 저장된 메모리 어드레스의 값을 이어지는 다음 연산에 서 재사용할 수 있다. 예를 들면, 상술한 제1 스케줄링의 제1 연산 값은 제2 스케줄링의 제2 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제1 스케줄링의 제1 연산 값에 대응되는 메모리 어드레스 값을 제2 스케줄링의 제2 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제1 스케줄링의 메모리 어 드레스 값 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제2 스케줄링의 제2 레이어 노드 데이터로 활용할 수 있는 효과가 있다. 예를 들면, 상술한 제2 스케줄링의 제2 연산 값은 제3 스케줄링의 제3 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제2 스케줄링의 제2 연산 값에 대응되는 메모리 어드레스 값을 제3 스케줄링의 제3 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제2 스케줄링의 메모리 어 드레스 값을 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제3 스케줄링의 제3 레이 어 노드 데이터로 활용할 수 있는 효과가 있다. 예를 들면, 상술한 제3 스케줄링의 제3 연산 값은 제4 스케줄링의 제4 레이어의 노드 데이터로 설정된다. 구체 적으로 설명하면, NPU 스케줄러는 NPU 내부 메모리에 저장된 제3 스케줄링의 제3 연산 값에 대응되는 메모리 어드레스 값을 제4 스케줄링의 제4 레이어의 노드 데이터에 대응되는 메모리 어드레스 값으로 재설정할 수 있다. 즉, 메모리 어드레스 값을 재사용할 수 있다. 따라서 NPU 스케줄러가 제3 스케줄링의 메모리 어 드레스 값을 재사용함으로 써, NPU 내부 메모리은 별도의 메모리 쓰기 동작 없이 제4 스케줄링의 제4 레이 어 노드 데이터로 활용할 수 있는 효과가 있다. 더 나아가서, NPU 스케줄러는 스케줄링 순서와 메모리 재사용 여부를 판단해서 NPU 내부 메모리을 제 어하도록 구성되는 것도 가능하다. 이러한 경우 NPU 스케줄러가 인공신경망모델의 데이터 지역성 정보 또 는 구조에 대한 정보를 분석해서 최적화된 스케줄링을 제공할 수 있는 효과가 있다. 또한 메모리 재사용이 가능 한 연산에 필요한 데이터를 중복해서 NPU 내부 메모리에 저장하지 않을 수 있기 때문에 메모리 사용량을 저감할 수 있는 효과가 있다. 또한 NPU 스케줄러는 메모리 재사용만큼 저감된 메모리 사용량을 계산해서 NPU 내부 메모리을 최적화할 수 있는 효과가 있다. 본 개시의 일 예시에 따른 NPU은 제1 프로세싱 엘리먼트(PE1)의 제1 입력인 (N)bit 입력은 변수 값을 입력 받고, 제2 입력은 (M)bit 입력은 상수 값을 입력 받도록 구성될 수 있다. 또한 이러한 구성은 프로세싱 엘리먼 트 어레이의 다른 프로세싱 엘리먼트 들에게 동일하게 설정될 수 있다. 즉, 프로세싱 엘리먼트의 하나의 입력은 변수 값을 입력 받고, 다른 입력은 상수 값을 입력 받도록 구성될 수 있다. 따라서 상수 값의 데이터 갱 신 횟수를 저감할 수 있는 효과가 있다. 이때, NPU 스케줄러는 인공신경망모델(110-10)의 데이터 지역성 정보 또는 구조에 대한 정보를 활용하여 입력 레이어(110-11), 제1 은닉 레이어(110-13), 제2 은닉 레이어(110-15) 및 출력 레이어(110-17)의 노드 데 이터는 변수(variable)로 설정하고, 제1 연결망(110-12)의 가중치 데이터, 제2 연결망(110-14)의 가중치 데이터, 및 제3 연결망(110-16)의 가중치 데이터는 상수(constant)로 설정할 수 있다. 즉 NPU 스케줄러는 상수 값과 변수 값을 구분할 수 있다. 단, 본 개시는 상수와 변수 데이터 타입에 제한되지 않으며, 본질적으로, 자주 가변 되는 값과, 그러지 않은 값을 구분하여 NPU 내부 메모리의 재사용율을 향상시킬 수 있다. 즉, NPU 시스템 메모리는 NPU의 추론 연산이 지속되는 동안 NPU 시스템 메모리에 저장된 연결망 들의 가중치 데이터를 보존하도록 구성될 수 있다. 따라서 메모리 읽기 쓰기 동작을 저감할 수 있는 효과가 있 다. 즉, NPU 시스템 메모리는 추론 연산이 지속되는 동안 NPU 시스템 메모리에 저장된 MAC 연산 값을 재 사용 하도록 구성될 수 있다. 즉, 프로세싱 엘리먼트 어레이의 각각의 프로세싱 엘리먼트의 제1 입력부의 입력 데이터(N)bit가 저장된 메모리 어드레스의 데이터 갱신 횟수는 제2 입력부의 입력 데이터(M)bit가 저장된 메모리 어드레스의 데이터 갱 신 횟수보다 더 많을 수 있다. 즉, 제2 입력부의 데이터 갱신 횟수 제1 입력부의 데이터 갱신 횟수보다 적어질 수 있는 효과가 있다. 다른 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공 신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경 망(Deep Neural Network, DNN)이라고 한다.DNN에는 여러 종류가 있으나, 컨볼루션 신경망(Convolutional Neural Network, CNN)은 입력 데이터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 도 6a은 컨볼루션 신경망의 기본 구조를 설명하기 위한 도면이다. 컨볼루션 신경망(CNN)은 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼 루션 신경망은 영상처리에 적합한 것으로 알려져 있다. 도 6a을 참조하면, 입력 이미지는, 특정 개수의 행과 특정 개수의 열로 구성된 2차원적 행렬로 표현될 수 있다. 입력 이미지는 여러 채널로 나뉠 수 있는데, 여기서 채널은 입력 이미지의 컬러 성분의 수를 나타낼 수 있다. 컨볼루션 신경망은 컨볼루션(수학 용어로서, 한국어로는 합성곱이라 한다) 동작과 폴링(pooling) 동작이 반복되 는 형태이다. 컨볼루션 동작은 입력 이미지에 커널(kernel) 행렬을 합성곱한 후, 이미지의 특징을 나타내는 피처 맵(feature map)을 출력하는 과정이다. 커널 행렬은 가중치 값들을 포함할 수 있다. 커널 행렬에서 행은 미리 정해진 개수 일 수 있고, 열도 미리 정해진 개수일 수 있다. 예를 들어, 상기 커널 행렬은 N x M 크기일 수 있다. 열의 개수 와 행의 개수가 동일할 경우, N=M일 수 있다. 상기 커널은 채널 마다 존재할 수 있다. 일반적으로, 입력 이미지가 표현되는 행렬의 크기 보다 커널 행렬의 크기 보다 크기 때문에, 커널 행렬은 입력 이미지 상에서 슬라이딩하면서 합성곱이 수행된다. 폴링 동작은 행렬의 크기를 줄이거나 행렬 내의 특정 값을 강조하기 위한 동작이다. 패턴을 실제로 분류하는 신경망은 특징 추출 신경망의 후단에 위치하며, 완전 연결 레이어(Fully Connected Layer)라고 한다. 도 6b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 6b을 참조하면, 예시적으로 입력 이미지가 5 x 5 크기를 갖는 2차원적 행렬인 것으로 나타나 있다. 또한, 도 6b에는 예시적으로 3개의 노드, 즉 채널 1, 채널 2, 채널 3이 사용되는 것으로 나타내었다. 먼저, 레이어 1의 합성곱 동작에 대해서 설명하기로 한다. 입력 이미지는 레이어 1의 첫 번째 노드에서 채널 1을 위한 커널 1과 합성곱되고, 그 결과로서 피처 맵1이 출력 된다. 또한, 상기 입력 이미지는 레이어 1의 두 번째 노드에서 채널 2를 위한 커널 2와 합성곱되고 그 결과로서 피처 맵 2가 출력된다. 또한, 상기 입력 이미지는 세 번째 노드에서 채널 3을 위한 커널 3과 합성곱되고, 그 결 과로서 피처 맵3이 출력된다. 다음으로, 레이어 2의 폴링(pooling) 동작에 대해서 설명하기로 한다. 상기 레이어 1 로부터 출력되는 피처 맵1, 피처 맵2, 피처 맵3은 레이어 2의 3개의 노드로 입력된다. 레이어 2 는 레이어 1로부터 출력되는 피처 맵들을 입력으로 받아서 폴링(pooling)을 수행할 수 있다. 상기 폴링이라 함 은 크기를 줄이거나 행렬 내의 특정 값을 강조할 수 있다. 폴링 방식으로는 최대값 폴링과 평균 폴링, 최소값 폴링이 있다. 최대값 폴링은 행렬의 특정 영역 안에 값의 최댓값을 모으기 위해서 사용되고, 평균 폴링은 특정 영역내의 평균을 구하기 위해서 사용될 수 있다. 도 6b의 예시에서는 5 x 5 행렬의 피처맵이 폴링에 의하여 4x4 행렬로 크기가 줄어지는 것으로 나타내었다. 구체적으로, 레이어 2의 첫 번째 노드는 채널 1을 위한 피처 맵1을 입력으로 받아 폴링을 수행한 후, 예컨대 4x4 행렬로 출력한다. 레이어 2의 두 번째 노드는 채널 2을 위한 피처 맵2을 입력으로 받아 폴링을 수행한 후, 예컨대 4x4 행렬로 출력한다. 레이어 2의 세 번째 노드는 채널 3을 위한 피처 맵3을 입력으로 받아 폴링을 수행 한 후, 예컨대 4x4 행렬로 출력한다. 다음으로, 레이어 3의 합성곱 동작에 대해서 설명하기로 한다. 레이어 3의 첫 번째 노드는 레이어 2의 첫 번째 노드로부터의 출력을 입력으로 받아, 커널 4와 합성곱을 수행하 고, 그 결과를 출력한다. 레이어 3의 두 번째 노드는 레이어 2의 두 번째 노드로부터의 출력을 입력으로 받아, 채널 2를 위한 커널 5와 합성곱을 수행하고, 그 결과를 출력한다. 마찬가지로, 레이어 3의 세 번째 노드는 레이 어 2의 세 번째 노드로부터의 출력을 입력으로 받아, 채널 3을 위한 커널 6과 합성곱을 수행하고, 그 결과를 출 력한다.이와 같이 합성곱과 폴링이 반복되고 최종적으로는, 도 6a과 같이 fully connected로 출력될 수 있다. 해당 출 력은 다시 이미지 인식을 위한 인공 신경망으로 입력될 수 있다. 도 7a는 본 명세서의 개시에 따른 동작을 도 4a 또는 도 4b에 도시된 엣지 디바이스의 구성 요소들 중 일부를 이용하여 나타낸 예시도이다. 엣지 디바이스의 동작을 설명하기 위하여, 도 4a 또는 도 4b에 도시된 구성 요소들 중 일부만을 도 7에 나타내었다. 도 7a의 예시에서는 NPU, 메모리, 입력부, 출력부, 시스템 버스, 중 앙 처리 유닛(CPU)이 나타나 있다. 상기 메모리는 인공 신경망 모델 저장부와 인공 신경망 모델들의 조합 정보 저장부를 포함할 수 있다. 상기 메모리 내의 상기 인공 신경망 모델 저장부는 다수의 인공 신경망 모델에 대한 정보를 저장할 수 있다. 상기 인공 신경망 모델에 대한 정보는 인공 신경망 모델의 데이터 지역성 정보 또는 구조에 대한 정보 를 포함할 수 있다. 상기 데이터 지역성 정보 또는 구조에 대한 정보는 레이어의 개수에 대한 정보, 레이어들의 배치 데이터 지역성 정보 또는 구조에 대한 정보, 각 레이어 내의 채널들에 대한 정보, 각 레이어 내의 노드들 에 대한 정보, 그리고 연결망에 대한 정보 중 하나 이상을 포함할 수 있다. 상기 노드들에 대한 정보는 각 노드 의 값, 예컨대 가중치 값에 대한 정보를 포함할 수 있다. 상기 연결망에 대한 정보는 레이어들 간의 연결 관계 에 대한 정보, 또는 노드들 간의 연결 관계에 대한 정보를 포함할 수 있다. 상기 다수의 인공 신경망은 도 7a에 도시된 바와 같이, a) 영상 내 관심 영역을 추출하기 위한 인공 신경망, b) 영상 화질을 개선하기 위한 인공 신경망, c) 컨볼루션 신경망, d) 영상 내의 객체를 인식하기 위한 인공 신경망, e) 동작을 인식하기 위한 신경망 그리고 f) 음성을 인식하기 위한 신경망 중 하나 이상을 포함할 수 있 다. 상기 메모리 내의 상기 인공 신경망 모델들의 조합 정보 저장부는 전술한 하나 이상의 인공 신경망들 의 조합에 대한 정보 그리고 인공 신경망 모델들의 연산 순서에 대한 정보 중 하나 이상을 포함할 수 있다. 예 를 들어, 상기 조합에 대한 정보는 a) 영상 내 관심 영역 추출 신경망과, 그리고 c) 컨볼루션 신경망의 조합에 대한 정보를 포함할 수 있다. 상기 연산 순서에 대한 정보는 상기 조합 내에서 순차적인 순서 혹은 병렬적인 순 서에 대한 정보를 포함할 수 있다. 상기 조합에 대한 정보는 엣지 디바이스의 기능 또는 실행중인 애플리케이션 별로 다를 수 있다. 예를 들어, 엣지 디바이스에서 음성 인식을 포함하는 애플리케이션이 실행중인 경우, 입력부의 마 이크로폰에 의해 입력되는 사용자의 음성을 인식할 수 있다. 이 경우, e) 음성을 인식하기 위한 신경망이 단독으로 사용될 수 있다. 다른 예를 들어, 엣지 디바이스에서 VR(Virtual Reality) 게임과 관련된 애플리케이션이 실행중인 경우, 사용자의 제스쳐 또는 사용자의 음성을 게임의 입력으로 사용할 수 있다. 이 경우, a) 영상 내 관심 영역 추출 신경망, c) 컨볼루션 신경망, d) 영상 내의 객체를 인식하기 위한 신경망, e) 동작을 인식하기 위한 신경망 그 리고 f) 음성을 인식하기 위한 신경망의 조합이 필요할 수 있다. 구체적인 예를 들어 설명하면, 사용자의 행동 반경이 관심 영역으로 설정되면, a) 영상 내 관심 영역 추출 신경망은 상기 입력부 내의 카메라를 통해 촬영된 영상 내에서 상기 관심 영역 내의 영상만을 추출해낼 수 있다. 상기 d) 상기 객체 인식 인공 신경망은 상기 영상 내에서 객체, 즉 사물, 동물, 사람을 식별할 수 있다. 상기 e) 동작 인식 신경망은 사람의 동작, 즉 제스쳐를 인식할 수 있다. 그리고, 상기 f) 음성을 인식하기 위한 신경망은 입력부의 마이크로폰에 의해 입력되는 사용자의 음성을 인식할 수 있다. 상기 NPU은 상기 메모리 내의 인공 신경망 모델들의 조합 정보 저장부으로부터 정보를 읽어내어 상기 NPU 내부 메모리에 저장할 수 있다. 이는, 상기 NPU이 상기 CPU로부터 실행중인 애플리케 이션에 대한 정보를 획득하거나 상기 CPU로부터 특정 명령을 수신할 때, 수행될 수 있다. 또는, 상기 CPU은 실행중인 애플리케이션 정보(예컨대, 애플리케이션의 종류, 타입 혹은 식별 정보)에 기 초하여, 상기 메모리 내의 인공 신경망 모델들의 조합 정보 저장부으로부터 정보를 읽어낼 수 있다. 그리고, 상기 CPU은 상기 실행중인 애플리케이션과 관련된 인공 신경망 모델들의 조합에 대한 정보를 결 정한 후, 상기 결정된 조합 정보에 기초하여, 상기 NPU에게 하나 이상의 인공 신경망 모델을 위한 연산을 수행하도록 지시할 수 있다.상기 엣지 디바이스의 기능이 단순하거나, 하나의 애플리케이션만 실행가능한 경우, 상기 메모리에 대한 액세스 빈도를 줄이기 위하여, 상기 정보가 상기 NPU 내부 메모리에 상기 정보가 항시 저장되어 있을 수 있다. 상기 NPU 내의 NPU 스케줄러은 상기 NPU 내부 메모리에 저장된 정보에 기초하여, 프로세싱 엘리 먼트(PE) 어레이 내에서 각 신경망을 위한 동작을 수행할 수 있는 프로세싱 엘리먼트들을 설정한다. 예를 들어, 상기 NPU 스케줄러은 프로세싱 엘리먼트(PE) 어레이를 여러 그룹으로 나눈 후, 상기 a) 영상 내 관심 영역 추출 신경망을 위한 동작을 수행하도록 제1 그룹의 PE들을 설정하고, 상기 c) 컨볼루션 신경 망을 위한 동작을 수행하도록 제2 그룹의 PE들을 설정할 수 있다. 한편, 상기 엣지 디바이스의 종류 혹은 동작 모드에 따라서는, 상기 NPU은 일부 신경망, 예컨대 상 기 a) 영상 내 관심 영역을 추출하기 위한 인공 신경망 또는 b) 영상 화질을 개선하기 위한 인공 신경망을 구동 하지 않을 수도 있다. 대안으로, 도 7a에서는 상기 중앙 처리 유닛(CPU)은 영상 내 관심 영역 추출을 위 해 설정된 회로(예컨대, 트랜지스터들의 조합)와 영상 개선을 위해 설정된 회로(예컨대, 트랜지스터들의 조합) 를 포함하는 것으로 나타나 있다. 도시된 NPU 스케줄러는 복수의 신경망 모델들을 PE들에게 할당할 수 있다. 예를 들면, PE들의 개수가 100 개일 경우, 제1 신경망 모델의 추론 연산을 위해서 30개의 PE들이 할당될 수 있으며, 제2 신경망 모델의 추론 연산을 위해서 50개의 PE들이 할당될 수 있다. 그리고 이러한 경우 할당되지 않은 나머지 PE들은 동작하지 않을 수 있다. NPU 스케줄러는 복수의 신경망 모델들 각각의 레이어의 노드 값 및 각각의 연결망의 가중치 값의 크기 및 구조 데이터에 기초하여 스케줄링 순서를 결정하고, 스케줄링 순서에 따라서 PE들에게 할당할 수 있다. 이에 따르면 NPU 스케줄러에 의해서 특정 PE들이 특정 신경망 모델의 추론을 위해서 할당될 수 있기 때문 에, 하나의 NPU가 복수의 신경망 모델들을 동시에 병렬로 처리할 수 있는 효과가 있다. NPU 스케줄러는 신경망 모델의 구조 데이터를 활용하여 복수의 신경망 모델들 각각의 레이어의 노드 값 및 각각의 연결망의 가중치 값의 크기를 확인할 수 있기 때문에, 각각의 스케줄링 별 추론 연산에 필요한 메모리 크기를 계산할 수 있다. 따라서, NPU 스케줄러는 멀티태스킹을 수행할 수 있는 NPU 내부 메모리의 가용 한 도 내에서 스케줄링 순서별로 필요한 데이터를 저장할 수 있다. NPU 스케줄러는 NPU 내부 메모리에 저장되는 데이터의 우선 순위를 설정할 수 있다. 이에 의하면, 우선 순위가 높은 데이터가 NPU 내부 메모리에 유지되기 때문에, 저장된 데이터를 재사용하여 메 모리 재사용율을 높일 수 있는 효과가 있다. 따라서 추론 속도와 소비 전력을 저감할 수 있는 효과가 있다. NPU은 멀티태스킹 기능을 제공하기 위해서 최적화될 수 있다. NPU은 적어도 2개의 서로 다른 추론 연 산을 제공하기 위해서, 적어도 2개의 신경망 모델들을 구동하도록 구성될 수 있다. 또한 하나의 신경망 모델의 추론 결과에 의해서 다른 신경망 모델이 구동될 수 있다. 즉 하나의 신경망 모델은 상시 동작되고, 다른 신경망 모델은 특정 조건에서 구동될 수 있다. 이와 같이 임의 신경망 모델을 특정 조건에서만 구동시켜, 엣지 디바이스의 소비 전력을 저감할 수 있는 효과가 있다. 도 7b는 도 7a에 도시된 엣지 디바이스의 변형예를 나타낸 예시도이다. 변형예에 따르면, 엣지 디바이스는 복수의 NPU들을 포함할 수 있다. 도 7b에서는 예시적으로 상기 엣지 디바이스이 2개의 NPU(100a, 100b)를 포함하는 것으로 도시되었다. 도 7b에서는 예시적으로, 제1 NPU(100a) 내의 PE들(110a)은 영상 내 관심 영역 추출 신경망 모델을 위한 연산을 수행하는 제1 그룹의 PE와, 컨볼류션 신경망 모델을 위한 연산을 수행하는 제2 그룹의 PE와 그리고 영상 개선 신경망 모델을 위한 연산을 수행하는 제3 그룹의 PE를 포함하는 것으로 나타내었다. 그리고, 제2 NPU(100b) 내 의 내의 PE들(110b)은 객체 인식 신경망 모델을 위한 연산을 수행하는 제1 그룹의 PE와, 동작 인식 신경망 모델 을 위한 연산을 수행하는 제2 그룹의 PE와 그리고 음성 인식 신경망 모델을 위한 연산을 수행하는 제3 그룹의 PE를 포함하는 것으로 나타내었다. 그러나, 이는 예시에 불과하고, 제1 NPU(100a) 내의 PE들(110a)과 제2 NPU(100b) 내의 PE들(110b)에 의해서 수 행되는 신경망의 종류 혹은 개수는 자유롭게 변형될 수 있다. 또는 분산 처리를 통해 연산 속도를 높이기 위하여, 제1 NPU(100a) 내의 PE들(110a)와 제2 NPU(100b) 내의 PE들(110b)은 동일한 신경망 모델을 위한 연산을 수 행할 수도 있다. 도 7b에 도시된 예시에 따르면, 상기 CPU은 ANN 스케줄러를 포함할 수 있다. 실행중인 애플리케이션에 기초하여, 상기 CPU 내의 ANN 스케줄러는 제1 NPU(100a)내의 PE들(110a)에게 제 1 신경망 모델을 위한 연산을 할당하고, 제2 NPU(100b) 내의 PE들(100b)에게도 제2 신경망 모델을 위한 연산을 할당할 수 있다. 이를 위하여, 상기 CPU은 상기 실행중인 애플리케이션을 위해서 구동되어야 할 하나 또는 복수의 신경망 모델의 조합을 결정할 수 있다. 구체적으로, 상기 CPU의 ANN 스케줄러는 실행중인 애플리케이션을 위해서 구동되어야 할 신경망 모델들에 대한 조합 정보를 상기 메모리 내의 조합 정보 저장부로부터 읽어낸 후, 복수의 신경망 모델을 위한 연산들을 상기 제1 NPU(100a)내의 PE들(110a) 및 상기 제2 NPU(100b) 내의 PE 들(100b)에게 분배 할당할 수 있다. 그리고, 상기 CPU의 ANN 스케줄러는 상기 메모리 내의 인송 신 경망 모델 저장부 내에 저장된 신경망 모델에 대한 정보를 상기 제1 NPU(100a) 및 상기 제2 NPU(100b)에게 전달할 수 있다. 상기 조합에 대한 정보는 다수의 인공 신경망 모델들의 순서에 대한 정보를 포함할 수 있다. 상기 NPU 스케줄러는 상기 순서에 대한 정보에 기초하여, PE들에 대한 할당과 관련된 명령을 생성할 수 있 다. 상기 엣지 디바이스의 메모리는 상기 동작 순서에 대한 정보를 저장할 수 있다. 상기 CPU는 상기 애플리케이션을 위한 명령어들을 수행할 때에, 상기 명령을 생성할 수 있다. 도 8a는 도 7a 또는 도 7b에 도시된 엣지 디바이스의 동작을 나타낸 흐름도이다. 도 8a를 참조하면, 엣지 디바이스의 NPU은 인공 신경망 모델들의 조합 정보를 획득할 수 있다 (S101). 구체적으로, 상기 엣지 디바이스에서 실행중인 애플리케이션이 임의의 애플리케이션 일 경우, 상 기 임의의 애플리케이션을 구동하기 위해 필요한 인공 신공망 모델들의 조합에 대한 정보가 획득될 수 있다. 상 기 인공 신경망 모델들의 조합 정보는 상기 엣지 디바이스에서 실행중인 애플리케이션에 대한 정보(예컨 대, 애플리케이션의 종류, 타입 혹은 식별 정보)에 기초하여, 획득될 수 있다. 상기 엣지 디바이스의 NPU은 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있다(S103). 즉, 상기 엣지 디바이스에서 실행중인 애플리케이션이 임의의 애플리케이션일 경우, 상기 임의의 애플리케이 션을 구동하기 위해 필요한 인공 신공망 모델들에 대한 정보를 획득할 수 있다. 상기 정보는 전술한 조합 정보 에 기초하여 획득될 수 있다. 상기 정보는 다수의 인공 신경망 모델들의 순서에 대한 정보를 포함할 수 있다. 그러면, NPU 내의 NPU 스케줄러은 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할당할 수 있다 (S105). 또한, NPU 내의 NPU 스케줄러은 제2 인공 신경망 모델을 제2 그룹의 PE들에게 할당할 수 있다(S107). 도 7a에 도시된 바와 같이, 상기 엣지 디바이스가 하나의 NPU만을 포함하는 경우, 상기 제1 그룹의 PE들 과 상기 제2 그룹의 PE들은 서로 물리적으로 다를 수도 있다. 또는, 상기 제1 그룹의 PE들과 상기 제2 그룹의 PE들은 일부는 서로 중첩되지만 시분할(time division) 방식으로 구분될 수 있다. 도 7b에 도시된 바와 같이, 상기 엣지 디바이스가 복수의 NPU(100a, 100b)를 포함하는 경우, 상기 제1 그 룹의 PE들은 제1 NPU(100a) 내에 포함될 수 있고, 상기 제2 그룹의 PE들은 제2 NPU(100b)내에 포함될 수 있다. 도 8b는 도 8a의 변형예를 나타낸 흐름도이다. 도 8b에 도시된 과정은 도 8a에 도시된 과정과 달리, CPU에 의해서 수행될 수 있다. 이하, 도 8a에 도시 된 과정과 차이나는 부분만을 위주로 설명하기로 하고, 동일한 내용은 도 8a를 참조하여 설명한 내용을 원용하 기로 한다. 엣지 디바이스의 CPU는 인공 신경망 모델들의 조합 정보를 획득할 수 있다(S201). 상기 엣지 디바이스의 CPU은 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있다(S203). 그러면, 도 7b에 도시된 상기 CPU 내의 ANN 스케줄러는 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할 당할 수 있다(S205). 또한, 도 7b에 도시된 상기 CPU 내의 ANN 스케줄러는 제2 인공 신경망 모델을 제2 그룹의 PE들에게 할당 할 수 있다(S207). 도 9a 및 도 9b는 엣지 디바이스가 확장현실(eXtended Reality, XR) 기기인 예들을 나타낸다. 확장현실(XR)은 가상현실(Virtual Reality, XR), 증강현실(Augmented Reality, AR), 혼합현실(Mixed Reality, MR)을 총칭한다. VR 기술은 현실 세계의 객체나 배경 등을 CG 영상으로만 제공하고, AR 기술은 실제 사물 영상 위에 가상으로 만들어진 CG 영상을 함께 제공하며, MR 기술은 현실 세계에 가상 객체들을 섞고 결합시켜서 제공 하는 컴퓨터 그래픽 기술이다. MR 기술은 현실 객체와 가상 객체를 함께 보여준다는 점에서 AR 기술과 유사하다. 그러나, AR 기술에서는 가상 객체가 현실 객체를 보완하는 형태로 사용되는 반면, MR 기술에서는 가상 객체와 현실 객체가 동등한 성격으로 사용된다는 점에서 차이점이 있다. XR 기술은 HMD(Head-Mount Display), HUD(Head-Up Display), 휴대폰, 태블릿 PC, 랩탑, 데스크탑, TV, 디지털 사이니지 등에 적용될 수 있고, XR 기술이 적용된 장치를 XR 장치(XR Device)라 칭할 수 있다. 도 10a는 도 9a 또는 도 9b에 도시된 XR 기기의 구성을 예시적으로 나타낸 블록도이다. 도 10a를 참조하여 알 수 있는 바와 같이, 엣지 디바이스의 일 예인 XR 기기은 도 1 또는 도 3에 도시된 NPU, 메모리, 무선 통신부, 입력부, 디스플레이, 시스템 버스, CPU 을 포함할 수 있다. 상기 무선 통신부은 근거리 통신 송수신기를 포함할 수 있다. 상기 근거리 통신 송수신기는 예를 들면 WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, 블루투스 (Bluetooth™RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra Wideband), ZigBee, NFC(Near Field Communication), Wireless USB(Wireless Universal Serial Bus) 등을 지원할 수 있다. 그 외에 XR 기기은 스피커와 같은 음향 출력기 또는 오디오 신호 출력 단자를 포함할 수 있다. 상기 NPU는 XR을 위해 필요한 복수의 신경망을 위한 연산을 수행할 수 있다. 예를 들어, XR을 위해 필요한 복수의 신경망은 동작 인식 신경망, 영상 내 관심 영역 추출 신경망, 영상 개선 신경망, 음성 인식 신경망 중 하나 이상을 포함할 수 있다. 상기 NPU의 NPU 스케줄러는 상기 복수의 신경망을 위한 연산을 PE들에게 할당할 수 있다. 즉, 제1 신 경망을 위한 연산을 제1 그룹의 PE들에게 할당하고, 제2 신경망을 위한 연산을 제2 그룹의 PE들에게 할당할 수 있다. 구체적인 예를 들면, 상기 NPU 스케줄러은 동작 인식 신경망을 위한 연산을 제1 그룹의 PE들에게 할 당하고, 영상 내 관심 영역 추출 신경망을 위한 연산을 제2 그룹의 PE들에게 할당하고, 상기 영상 개선 신경망 을 위한 연산을 제3 그룹의 PE들에게 할당하고, 상기 음성 인식 신경망을 위한 연산을 제4 그룹의 PE들에게 할 당할 수 있다. 도 10b는 도 10a의 변형 예를 나타낸 블록도이다. 도 10b에 도시된 XR 기기는 도 10a와 달리 복수의 NPU, 예시적으로 2개의 NPU(100a, 100b)를 포함할 수 있다. CPU는 메모리로부터 인공 신경망 모델들의 조합 정보를 획득할 수 있다. 그리고, CPU는 상기 획득된 조합 정보에 기초하여, 상기 메모리로부터 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있 다. 그런 후, 상기 CPU는 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할당하고, 제2 인공 신경망 모델을 제 2 그룹의 PE들에게 할당할 수 있다. 상기 CPU에 의한 할당에 대해 예를 들어 설명하면, 제1 NPU(100a)는 동작 인식 신경망을 위한 연산을 수 행하는 제1 그룹의 PE와, 영상 내 관심 영역 추출 신경망 모델을 위한 연산을 수행하는 제2 그룹의 PE와, 영상 개선 신경망을 위한 연산을 수행하는 제3 그룹의 PE를 포함하는 것으로 도 10b 예시적으로 나타나 있다. 제2 NPU(100b)는 음성 인식 신경망을 위한 제1 그룹의 PE를 포함하는 것으로 도시되어 있다.그러나, 이는 예시에 불과하고, 제1 NPU(100a)와 제2 NPU(100b)에 의해서 수행되는 신경망의 종류 혹은 개수는 자유롭게 변형될 수 있다. 이하, 도 10a 및 도 10b를 함께 참조하여 설명하기로 한다. XR 기기의 근거리 통신 송수신기를 통해 서버 또는 다른 사용자 단말 등과 같은 외부 장치와 통신 할 수 있다. 또한, 상기 근거리 통신 송수신기는 통신 네트워크를 통해 XR을 위한 영상을 수신할 수 있다. 상기 수신 된 영상은 CPU으로 전달될 수 있다. 상기 카메라는 복수 개일 수 있다. 예를 들어, 복수 개수의 카메라들 중에서 제1 카메라는 사용자가 바라 보는 방향의 영상을 촬영하여 CPU에게 전달할 수 있다. 그리고, 제2 카메라는 사용자의 좌안을 촬영하여 CPU에게 전달하고, 제3 카메라는 사용자의 우안을 촬영하여 CPU에게 전달할 수 있다. 상기 근거리 통신 송수신기를 통해 수신된 영상 그리고 상기 카메라을 통해 촬영된 영상은 메모리 에 임시 저장된 후 상기 CPU에 전달될 수 있다. 즉, 메모리는 상기 영상을 임시로 저장할 수 있고, 상기 CPU는 메모리에 저장된 영상들을 읽어 내어 처리할 수 있다. 또한, XR 기기는 메모리 카드를 위한 접속 단자를 구비할 수 있다. 상기 메모리 카드는 예를 들어, 컴팩 트 플래시 카드, SD 메모리 카드, USB 메모리 등을 포함할 수 있다. 상기 CPU는 상기 메모리 카드로부터 적어도 하나의 영상을 읽어오거나 검색할 수 있으며, 해당 영상을 메모리에 저장할 수도 있다. CPU는 근거리 통신 송수신기, 메모리, 상기 카메라로부터 영상을 수신할 수 있으며, 이 러한 수신된 영상들을 조합하여, XR 영상으로 생성하여 디스플레이에 출력할 수 있다. 예를 들어, CPU는 상기 근거리 통신 송수신기 및 상기 메모리로부터 수신되는 영상과 상기 카메라 로부터 출력되는 영상을 합성시켜, XR 영상을 생성할 수 있다. 상기 디스플레이는 CPU의 제어에 따라 XR 영상을 출력할 수 있다. 상기 디스플레이는 투명 글래스를 포함할 수 있으며, 영상 개선 처리가 수행된 XR 영상을 투명 글래스 상의 관심 영역에 출력할 수 있다. 상기 CPU는 상기 입력부을 통해 사용자의 명령(예를 들어, XR 영상과 관련된 제어 명령)를 입력 받 을 수 있다. 예를 들어, 상기 CPU는 마이크로폰을 통해 음성 명령을 입력받을 수 있다. 다른 예를 들어, 상기 CPU는 카메라, 자이로센서 및/또는 가속도 센서 중 하나 이상을 통하여 사용자의 움직임 동작 기반 명령을 수신할 수 있다. 구체적으로, 상기 CPU은 상기 카메라, 상기 자이로 센서 및/또는 가속도 센서 중 하나 이상을 통하여 사용자의 움직임 동작을 감지할 수 있다. 상기 사용자의 동작 움직임은 사용자의 눈 시선 방향(예를 들어, 사용자의 동공의 위치), 사용자의 머리 방향 및 머리 기울기 중 적어도 하나를 포함할 수 있다. 상기 사용자의 눈 시선 방향을 감지하기 위해서, 상기 카메 라는 복수개가 구비될 수 있다. 즉, 제1 카메라는 사용자가 바라보는 방향의 영상을 촬영하고, 제2 카메 라는 사용자의 좌안을 촬영하고, 제3 카메라는 사용자의 우안을 촬영할 수 있다. 한편, 상기 사용자의 움직임 동작을 인식하기 위하여, 상기 CPU는 상기 NPU(도 10a의 100 또는 도 10b의 100a 및 100b)에게 동작 인식 신경망을 위한 연산을 수행하라고 지시할 수 있다. 또한, 상기 CPU은 상기 NPU(도 10a의 100 또는 도 10b의 100a 및 100b)에게 영상 내 관심 영역 추출 신 경망을 위한 연산을 수행하라고 지시할 수 있다. 그러면, NPU 스케줄러는 동작 인식 신경망을 위한 연산을 제1 그룹의 PE들에게 할당하고, 영상 내 관심 영역 추 출 신경망을 위한 연산을 제2 그룹의 PE들에게 할당할 수 있다. 상기 제1 그룹의 PE들은 상기 카메라, 자이로센서 및/또는 가속도 센서 중 하나 이상을 통하 여 감지된 사용자의 움직임 동작에 기반하여, 사용자의 움직임이 어떤 의도의 제어 명령인지를 추론해낼 수 있다. 상기 제2 그룹의 PE들은 상기 제1 그룹의 PE들이 추론해 낸 사용자의 제어 명령에 기초하여, 영상 내에서 관심 영역(Region of Interest, ROI)을 결정하기 위한 추론을 수행할 수 있다. 예를 들어, 상기 제1 그룹의 PE들은 상기 자이로센서 및/또는 가속도 센서 중 하나 이상에 의해서 감지된 사용자의 머리 방향 및 머리 기울기 중 적어도 하나에 기초하여, 동작 인식 신경망을 통해 사용자의 움 직임이 어떤 의도인지를 추론할 수 있다. 그러면, 상기 제2 그룹의 PE들은 상기 추론된 사용자의 의도에 기초하 여, ROI를 추론할 수 있다. 다른 예를 들어, 상기 제1 그룹의 PE들은 상기 카메라에 의해서 감지된 사용자의 동공의 위치를 기반으로, 사용자의 의도를 추론할 수 있다. 그러면, 제2 그룹의 PE들은 상기 추론된 사용자의 의도를 기반으로 상기 ROI를 추론할 수 있다. 즉, 상기 동공의 위치는 사용자가 바라보는 시선의 위치 및/또는 방향을 추론할 수 있게 하는데 사용될 수 있다. 상기 ROI는 상기 사용자의 움직임 정보에 따라 실시간으로 변경될 수 있다. 이러한 관심 영역은 영상 개선 신 경망을 위해서 활용될 수 있다. 제3 그룹의 PE들은 상기 ROI에 대한 정보를 수신하면, 영상 개선 신경망을 통하여 상기 ROI 내의 영상을 개선할 수 있다. *영상 개선이라 함은 도 18을 참조하여 후술하는 바와 같이, 압축 해제/디코딩 과정(S401), 영상 전처리 과정 (S403) 및 초해상화(S405) 과정을 포함할 수 있다. 이상에서 설명한 바와 같이, 도 10a 또는 도 10b에 도시된 XR 기기는, 사용자의 움직임에 기초하여 관심 영역을 결정하고, 결정된 관심 영역에 대한 영상을 개선함으로써, 사용자에게 몰입감 있는 실감형 콘텐츠를 제 공할 수 있다. 나아가, 관심 영역만을 고해상도로 연산 처리함으로써 필수 연산량이 최소화되어 디지털 렌더링 에 대한 부하가 줄일 수 있다. 도 11은 도 10a 또는 도 10b에 도시된 XR 기기의 동작을 나타낸 흐름도이다. 도 11을 참조하면, 엣지 디바이스의 일종인 XR 기기는 영상(video)을 수신할 수 있다(S301). 그리고, XR 기기는 사용자의 머리 움직임 및 시선 중 적어도 하나를 감지하는 할 수 있다(S303). 그 후, XR 기기는 상기 감지된 움직임 및 시선 중 적어도 하나에 기초하여 관심 영역을 결정할 수 있다 (S305). 그리고, XR 기기는 상기 관심 영역에 대해 영상 개선 처리를 수행할 수 있다(S307). 마지막으로, XR 기기는 상기 영상 개선 처리가 수행된 영상을 디스플레이에 출력할 수 있다(S309). 도 12는 도 10a 또는 도 10b에 도시된 XR 기기가 사용자의 머리에 착용된 예를 나타낸다. 도시된 바와 같이 XR 기기는 사용자의 머리에 착용될 수 있고, 전면에는 디스플레이가 구비되어 사용자의 눈에 영상을 표시할 수 있다. 상기 XR 기기는 카메라, 마이크로폰, 자이로 센서, 각속도 센서를 통해 사 용자로부터의 명령을 수신할 수 있으며, 수신된 명령 신호에 따라 동작될 수 있다. XR 기기는 실감형 콘 텐츠 제공 장치의 예시로써, 이에 한정되지 않으며, 안경형, 헬멧형, 모자형 등과 같이, 인체의 두부에 착용할 수 있는 다양한 형태로 구성될 수 있다. 도 12에 도시된 바와 같이, XR 기기의 디스플레이는 사용자의 우안 및 좌안 중 적어도 하나에 대응하도록 배치되어 사용자의 눈 앞에 영상을 직접 출력할 수 있다. 위에서 언급한 바와 같이, XR 기기는 자이로 센서 및/또는 각속도 센서를 포함하여, XR 기기를 착 용한 사용자의 머리 움직임을 감지할 수 있다. 일 예시에서, XR 기기의 자이로 센서 및/또는 각속도 센서는 사용자의 머리의 중심을 기준으로 x축, y 축 및 z축으로 움직이는 사용자의 머리 움직임을 감지할 수 있다. 여기서, 사용자의 머리 움직임은 머리 방향 및 머리 기울기 중 적어도 하나를 포함할 수 있다. 이렇게 측정된 머리 움직임을 기초로 사용자의 관심 영역이 결 정될 수 있다. 도 13은 본 개시의 일 실시예에 따른 XR 기기에 의해 제공되는 실감형 콘텐츠가 입체적 공간으로 표시되 었을 때의 예시도이다. XR 기기는 복수의 외부 장치로부터 제공하는 영상들 또는 메모리에 저장된 영상들을 디스플레이 상에 출 력할 수 있다. XR 기기에 의해 출력되는 영상은 VR 영상 또는 AR 영상일 수 있다. 상기 AR 영상 또는 VR 영상은, 사용자에게 극대화된 생동감과 몰입감을 제공하기 위해 파노라마 이미지 및/또는 영상일 수 있다. 상기 AR 영상 또는 VR 영상은 사용자를 중심축으로 하여 모든 방향(상하좌우 방향)의 시청을 지원하도록 하는 반구 형상의 영상일 수 있다. 예를 들어, 반구 형상의 영상은 360도 뷰어를 지원하는 360도 영상일 수 있다. 360도 뷰어를 지원하는 360도 영상은, XR 기기의 디스플레이를 통해 사용자에게 출력될 수 있으 며, 관심 영역에 대응되는 목적 영상을 포함할 수 있다. 여기서, 관심 영역은 디스플레이 상에 출력되는 영상의 부분 영상인 목적 영상에 대응될 수 있다. 예 를 들어, 360도 영상은 도시된 바와 같이, 사용자의 움직임을 통해 결정된 관심 영역에 대응되는 360도 영 상의 부분 영상인 목적 영상을 포함할 수 있다. 이러한 목적 영상은 영상 개선 신경망에 의해 영상 개선 처리된 영상을 포함할 수 있다. 또한, XR 기기를 착용한 사람의 머리 움직임 및/또는 시선 정보에 기초하여 목적 영상은 실시간으로 변경되어 표시될 수 있다. 이때, 영상 개선 신경망은 관심 영역에 대응하는 부분 영상에 대한 영상 개선 처리를 실시간으로 수행할 수 있다. 이상에서 설명한 AR 영상 또는 VR 영상은 장면 연출에 따라, 반구 형상, 구형상, 원통 형상 등과 같이 사용자가 가상현실 공간 안에 있는 느낌을 줄 수 있도록 구성될 수 있다. 도 14는 사용자가 바라보는 시야의 각도에 기초하여 정의될 수 있는 관심 영역(Region of Interest)의 범위를 나타낸 예시도이다. 관심 영역의 범위는 사용자가 바라보는 시야의 각도, 즉 시야각에 기초하여 정의될 수 있다. 여기서, 관 심 영역의 범위는 움직임 감지기에 의해 검출된 머리의 위치(예를 들어, 머리 방향 및/또는 머리 기울기) 및/또는 시선의 위치(예를 들어, 동공의 위치)에 의해 정의될 수 있다. 도 14에 도시된 바와 같이, 영상을 시 청하는 사용자는 일정 시야각을 가질 수 있다. 통상적으로, 시야각의 범위는 양안의 위치에 따라 다를 수 있어 개개인 마다 시야각이 다르다. 따라서, 시야각의 범위는 양안의 위치(예를 들어, 동공의 위치)에 기 초하여 정의되도록 구성될 수 있다. 시야각의 범위는 사용자의 머리의 위치(예를 들어, 머리 방향, 머리 기울기) 및/또는 양안의 위치에 기초 하여 정의될 수 있다. 일 예로서, 사람의 양안을 합친 시야각의 범위는 도시된 바와 같이 수평방향으로 180도, 수직방향으로 120도를 가질 수 있으나, 이에 한정되지 않으며, 다양한 각도로 정의될 수 있다. 관심 영역의 범위는 자이로 센서 및/또는 가속도 센서에 의해 검출된 머리의 위치 및/또는 양안의 위치를 통해 결정될 수 있으며, 시야각의 범위와 같거나 더 작게 정의될 수 있다. 예를 들어, 관심 영역의 범위 는 시야각의 범위(520, 여기서, 수평 방향으로 180도, 수직 방향으로 120도)보다 작게 정의될 수 있다. 자 이로 센서 및/또는 가속도 센서는 머리의 위치를 검출할 수 있고, 카메라는 동공의 위치를 검출할 수 있다. 상 기 검출된 머리의 위치 및/또는 동공의 위치를 통해 사용자의 시선의 위치 및 시선의 방향, 그리고 관심 영역의 범위가 결정될 수 있다. 일 예시에 따르면, 도 14에 도시된 바와 같이, 사용자의 머리(예를 들어, 두개골)의 위치를 검출하여 얼굴의 형 상에 외접하는 사각형을 생성할 수 있고, 사각형의 각 꼭지점의 위치(a, b, c, d)를 검출할 수 있다. 검 출한 4개의 꼭지점(a, b, c, d)과 사용자의 후두부의 중심점(e)을 이은 선의 연장선과 디스플레이 상에서 만나 는 위치(f, g, h, i)를 검출할 수 있다. 검출된 위치(f, g, h, i)에 기초하여 관심 영역의 범위를 결정할 수 있다. 도시된 바와 같이, 디스플레이 상의 4개의 점(f, g, h, i)을 연결한 영역을 관심 영역의 범위로 결정할 수 있다. 시야각의 범위 및 관심 영역의 범위를 정의하는 과정은 이상에서 설명한 예시에 한 정되지 않으며, 다양한 방법으로서 정의 가능하다. 도 15는 사용자의 시선에 기초하여 결정된 관심 영역을 영상 개선 처리하는 과정을 나타낸 예시도이다. 도 15에 도시된 예시도는 XR 기기가 사용자의 머리에 착용되어 있다고 가정하고 나타낸 것이다. NPU에 의해서 연산되는 영상 내의 관심 영역 추출 신경망은, 검출된 동공의 위치에 기초하여 디스플레이 장치 상에서 사용자의 시선의 위치를 결정할 수 있다. 일 예시에서, NPU에 의해서 연산되는 영상 내의 관심 영역 추출 신경망은 좌안의 동공의 위치에서 좌 안의 시선 방향 및 우안의 동공의 위치에서 우안의 시선 방향과 디스플레이 상 에서 만나는 지점(j)을 검출하여 사용자의 시선의 위치점(j)으로 결정할 수 있다. 여기서, 좌안의 시선 방 향은 좌안이 응시하는 방향을 나타내고, 우안의 시선 방향은 우안이 응시하는 방향을 나타낸다. 상기 관심 영역 추출 신경망은 미리 결정된 관심 영역의 범위의 중심점의 위치를 시선의 위치점(j)으 로 지정하여 관심 영역을 결정할될 수 있다. 도 15의 (a) 부분에 도시된 바와 같이, 사용자의 시선의 위치(j)에 기초하여 디스플레이에 투사될 관심 영역 이 결정될 수 있다. NPU에 의해서 연산되는 영상 개선 신경망은 결정된 관심 영역에 대응되는 목적 영상에 대해 영상 개선 처리를 수행할 수 있다. 영상 개선 신경망은 예를 들어, 관심 영역의 해상도를 높 일 수 있다. 도 15의 (b) 부분에 도시된 바와 같이, 도 15의 (a)에 도시된 관심 영역에 비해 관심 영역의 해상도 가 높아진 것을 확인할 수 있다. 한편, 도 15의 (c) 부분에 도시된 바와 같이, 사용자의 머리 및/또는 동공의 움직임으로부터 좌측 방향으로부터 우측 방향으로 이동되었다고 판단되는 경우(여기서, 사용자의 시선의 위치가 j지점에서 k지점으로 이동한 경 우), 사용자의 움직임에 기초하여 관심 영역이 새로이 결정될 수 있다. 이 경우에도, 도 15의 (a) 부분을 참조하여 설명했던 바와 같이, 좌안의 시선 방향 및 우안의 시선 방향에 기초하여 k지점이 사용자의 시선의 위치점으로 결정될 수 있고, 미리 결정된 관심 영역의 범위의 중심점의 위치(k)를 시선의 위치점(k)으로 지정하여 관심 영역이 결정될 수 있다. 대안적으로, 관심 영역의 범위는 미리 결정되어 있지 않고, 사용자의 시선에 따라 변경될 수 있다. 이상에서 설명한 바와 같이, 영상 개선 신경망은 새로 결정된 관심 영역에 대응되는 목적 영상에 대해 영 상 개선 처리를 수행할 수 있다. 예를 들어, 관심 영역에 해당되는 목적 영상에 대해 초해상화 연산 처리 할 수 있다. 도 15의 (d) 부분을 참조하면, 도 15의 (c) 부분에 도시된 관심 영역에 비해 관심 영역 내의 영상이 초해상화 과정을 통해 해상도가 높아져 선명해진 것을 확인할 수 있다. 이상에서 설명한 바와 같이, 결정된 관심 영역의 화질 만을 개선하여 영상 처리에 필요한 연산량을 최소화할 수 있고, 이에 따라 사용자에게 제공되는 영상(예를 들어, 실감형 콘텐츠)의 반응 속도를 높일 수 있다. 따라서, 자연스러우면서도 몰입감 높은 실감형 콘텐츠를 사용자에게 제공할 수 있다. 지금까지는, 영상 개선 신경망이 관심 영역에 대응하는 목적 영상에 대해 해상도를 높이는 영상 개선 처리가 수 행된다고 설명하였으나, 이에 한정되지 않으며, 앞서 설명한 압축 디코딩 연산, 전처리 연산 등 영상 개선과 관 련된 다양한 연산이 처리될 수 있다. 일 예시에 따르면, 영상 개선 신경망은 관심 영역에 대응되는 목적 영상에 대해서만 영상 개선 처리하지 않으며, 전체 영상에 대해 영상 개선 처리를 수행하는 것과 같이, 필요에 따라 영 상의 일부 또는 전부에 대해 영상 개선을 수행할 수 있다. 도 16은 감출된 사용자의 시선에 기초하여 결정된 관심 영역을 영상 개선 처리하는 과정을 나타낸 예시도이다. 도 16에 도시된 예시도는 XR 기기가 사용자의 머리에 착용되어 있다고 가정하고 나타낸 것이다. 자이로 센서 및/또는 가속도 센서로부터 감지된 움직임과 카메라를 통해 검출된 사용자의 시선 정보에 기초하여 관심 영역이 결정되며, 각 관심 영역에 대해 영상 개선 처리가 수행될 수 있다. NPU에 의해서 연산되는 관심 영역 추출 신경망은 상기 감지된 움직임에 기초하여 관심 영역을 결정할 수 있다. 예를 들어, NPU에 의해서 연산되는 관심 영역 추출 신경망은 머리 방향 및 머리 기울기 중 적어도 하나에 기초 하여 사용자의 머리(예를 들어, 두개골)의 위치를 검출해 디스플레이 상에서의 관심 영역의 범위를 결정할 수 있다. 관심 영역의 범위에 기초하여 도 16의 (a) 부분에 도시된 바와 같이, 머리 방향 및 머리 기울기에 기초한 관심 영역이 결정될 수 있다. 또한, NPU에 의해서 연산되는 관심 영역 추출 신경망은 좌안의 동공의 위치에서 좌안의 시선 방 향 및 우안의 동공의 위치에서 우안의 시선 방향 각각과 디스플레이 상에서 만나는 지점(l, m)을 검출하여 좌안 및 우안 각각의 관심 영역(760, 770)을 결정할 수 있다. NPU에 의해서 연산되는 영상 개선 신경망은 각각의 관심 영역의 순위를 결정하고, 결정된 순위에 기초하여, 각 각의 관심 영역에 대해 단계적으로 영상 개선 처리(예를 들어, 초해상화 연산, 압축 디코딩 연산, 전처리 연산 등)를 수행할 수 있다. 예를 들어, 영상 개선 신경망은 좌안 및 우안의 관심 영역이 겹치는 영역, 각각의 좌안 및 우안의 관심 영역(760, 770), 머리 방향 및 머리 기울기에 기초한 관심 영역의 순서로 사용자의 관심 영역의 순위를 결정할 수 있다. 도 16의 (b) 부분에 도시된 바와 같이, 결정된 순위에 기초하여, 영상 개선 신경망은 좌안 및 우안의 관심 영역 이 겹치는 영역의 해상도를 가장 고품질(예를 들어, 8K)로, 각각의 좌안 및 우안의 관심 영역(760, 770)은 관심 영역이 겹치는 영역의 해상도보다는 낮은 고품질(예를 들어, 4K)로, 머리 방향 및 머리 기울기에 기 초한 관심 영역의 해상도는 각각의 좌안 및 우안의 관심 영역(760, 770)의 해상도보다 더 낮은 고품질(예 를 들어, 4K)로 렌더링할 수 있다. 이상에서 설명한 바와 같이, 각각의 관심 영역에 따라 영상 개선 처리연산을 수행하여 사용자에게 생동감과 몰 입감이 극대화된 실감형 콘텐츠를 제공할 수 있다. 도 17은 감지된 사용자의 시선에 기초하여 결정된 관심 영역에 증강 현실 영상을 합성하여 제공하는 예를 나타 낸 예시도이다. 상기 NPU에 의해서 연산되는 관심 영역 추출 신경망은 상기 자이로 센서 및/또는 가속도 센서에 의해서 감지된 사용자의 머리 움직임 그리고 카메라를 통해 검출된 사용자의 시선 중 적어도 하나에 기초하여, 관심 영역 (Region of Interest)을 결정할 수 있다. 일 예시로서, 사용자의 동공의 위치(812, 822)를 검출하여 좌안 및 우안의 시선 방향과 디스플레이 상에서 만나는 지점(l)을 검출할 수 있다. 도 17의 (a) 부분에 도시된 바와 같이, 디스플레이 상에서 왼쪽 방향에 치우친 지점을 시선의 위치점(l)으로 결 정할 수 있고, 해당 위치점(l)에 기초하여 관심 영역이 결정될 수 있다. 도 17의 (b) 부분에 도시된 바와 같이, 사용자의 시선에 기초하여 관심 영역이 결정되는 것으로 설명되었으나, 이에 한정되지 않으며, 사용자의 머리 움직임에 기초하거나, 머리 움직임 및 시선에 기초하여 관심 영역이 결정 될 수도 있다. 다른 한편, 전술한 바와 같이 XR 기기는 복수의 카메라를 구비할 수 있다. 이때, 제1 카메라는 사용자가 바라보는 영상을 촬영할 수 있고, 제2 카메라와 제3 카메라는 사용자의 좌안과 우안을 각기 촬영할 수 있다. 상기 제1 카메라는 도 17의 (c) 부분에 도시된 바와 같이, 관심 영역에 대응하는 현실 영상을 촬영할 수 있다. 그러면, 상기 XR 기기는 관심 영역에 현실 영상을 합성시키고, 이렇게 합성된 영상을 출력할 수 있다. 상기 현실 영상을 상기 관심 영역에 합성시키기 전에, 상기 현실 영상은 영상 개선 신경망에 의해 영상 개선 처리될 수 있다. 한편, XR 기기의 디스플레이는 사람의 눈을 통해 실제 현실을 보도록 구성된 투명 글래스를 포함할 수 있 다. 여기서, XR 기기의 디스플레이는 투명 글래스로 구현될 수 있다. 즉, 사용자는 자신의 눈을 이용하 여 투명 글래스를 통해 실제 현실을 볼 수 있을 뿐만 아니라, 투명 글래스 상에서 출력되는 현실 영상 또한 볼 수 있다. 상기 NPU에 의해서 연산된 관심 영역 신경망에 의해 추론된 관심 영역은 투명 글래스 상에 표시 될 영역을 포함할 수 있다. 상기 XR 기기는 현실 영상을 생성하고, 관심 영역 상에 현실 영상 을 표시함으로써, 일반 시야에 현실 영상을 오버랩시킬 수 있다. 일 예로, 이러한 현실 영상은 디스 플레이에 표시되기 전에 영상 개선 신경망 의해 영상 개선 처리될 수 있다. 도 18은 영상 개선을 위한 과정을 나타낸 흐름도이다. 도 18을 참조하면, 영상 개선이라 함은 압축 해제/디코딩 과정(S401), 영상 전처리 과정(S403) 및 초해상화 (S405) 과정을 포함할 수 있다. 상기 압축 해제/디코딩 과정(S401)은 영상(예를 들어, AR/VR 영상)이 압축된 영상인 경우, 압축을 해제한 후 디 코딩하여 디스플레이에 출력되도록 할 수 있다. 여기서, 압축 영상이라 함은 예를 들어, HEVC, H.265, MPEG 등의 상용화된 영상 압축 기술로 압축된 영상일 수 있다. 상기 영상은 전체가 압축되어 있을 수 있다. 하지만, 상기 압축 해제는 상기 영상 내에서 관심 영역에 해당되는 영상의 부분만에 대해서 수행될 수 있다. 한편, 상기 수신된 영상이 압축되지 않았거나, 인코딩되지 않은 영상인 경우에는 압축 해제/디코딩 과정(S401) 은 건너띄어질 수 있다. 다음으로, 전체 영상 중에서 관심 영역에 해당하는 부분의 영상에 대해 화질이 개선될 수 있다. 화질 개선을 위 하여, 영상 전처리 과정(S403) 및/또는 초해상화 과정(S405)이 수행될 수 있다. 예를 들어, 영상 전처리 과정(S403)은 영상 신호 처리 과정 및/또는 영상의 파라미터를 조정하는 과정을 포함할 수 있다. 여기서, 파라미터 조정 과정은 Demosaicing 기법, WDR(Wide Dynamic Range) 또는 HDR(High Dynamic Range) 기법, Deblur 기법, Denoise 기법, Color Tone mapping 기법, White Balance 기법 및 Decompression 기 법 중 적어도 하나의 기법을 사용하는 것을 의미할 수 있다. 상기 파리미터 조정 과정에서는 영상에 대한 복수의 파라미터들이 순차적으로 혹은 병렬적으로 조정될 수 있다. 순차적 조정에 대해서 설명하면, 제1 파라미터가 조정된 영상에 대하여 제2 파라미터가 조정될 수 있다. 이를 위하여, 인공 신경망 모델은 제1 파라미터를 조정하는 제1 레이어, 제2 파라미터를 조정하는 제2 레이어 형태로 구현될 수 있다. 예를 들어, 인공 신경망 모델의 제1 레이어는 영상에 대해서 Demosaicing 기법을 적용하기 위 한 것이고, 제2 레이어는 Deblur 기법을 적용하기 위한 것일 수 있다. 병렬적 조정에 대해서 설명하면, 제1 파라미터와 제2 파라미터가 동시에 조정될 수 있다. 이런 경우, 인공 신경 망 모델의 제1 레이어는 제1 파라미터를 조정하기 위한 제1 노드 그리고 제2 파라미터를 조정하기 위한 제2 노 드를 포함할 수 있다. 예를 들어, Demosaicing 기법 및 Deblur 기법에 대해서 학습 완료된 신경망 모델을 이용 할 경우, 영상을 상기 학습 완료된 인공 신경망 모델의 제1 레이어에 입력하게 되면, 제1 레이어로부터 출력되 는 영상은 Demosaicing 기법 및 Deblur 기법이 적용된 영상일 수 있다. 상기 초해상화 과정(S405)은 영상의 해상도를 높이기 위해서 수행될 수 있다. 여기서, 초해상화는 기존에는 보 간법을 통해서 수행되었지만, 본 개시에 따르면, 인공 신경망을 통해서 수행될 수 있다. 상기 초해상화 과정은 영상 전체에 대해서 수행될 수도 있다. 또는 상기 초해상화 과정은 관심 영역 내의 영상에 대해서만 수행될 수 있다. 구체적으로, 사용자의 시선이 위 치한 관심 영역의 해상도를 고품질(예를 들어, 4K 또는 8K)로 렌더링할 수 있고, 사용자의 시선이 벗어나면 보 통 품질(예를 들어, Full HD)로 렌더링할 수 있다. 즉, 관심 영역에 대응되는 영상에 대해 전처리 과정(S403)이 수행되어 화질이 개선되고 나면, 상기 전처리 과정이 수행된 관심 영역에 대응하는 영상에 대해 초해상화 과정 (S405)이 수행될 수 있다. 애초 원본 영상이 고품질 영상이라면, 전처리 과정 및/또는 초해상화 과정이 적용되지 않을 수 있다. 그러나, 원본 영상이 고품질 영상이라면, 압축 해제 및/또는 디코딩 과정을 수행하는데 상당한 부하가 발생하고 전력 소 모가 증가하는 단점이 있다. 따라서, 사용자의 관심 영역 내의 영상에 대해서만 전처리 과정 및/또는 초해상화 과정을 수행함으로써, 전체 연산량을 줄여 부하를 낮출 수 있다. 이에 따르면. 원본 영상이 고품질이 아니더라 도(예를 들어, 저해상도의 영상), 사용자의 관심 영역 내의 영상을 전처리 과정 및/또는 초해상화 과정을 통해 개선함으로써, 사용자에게 몰입감을 극대화할 수 있을 만큼의 고품질 영상으로 출력할 수 있게 된다. 지금까지는, 관심 영역 내의 영상에 대해 전처리 과정 및 초해상화 과정이 함께 수행되는 것으로 설명하였으나, 관심 영역 내의 영상이 여러 구역으로 분할가능한 경우, 제1 구역에 대해서는 전처리 과정만이 수행되고, 제2 구역에 대해서는 전처리 과정 및 초해상화 과정이 함께 수행되는 것도 가능하다. 도 19a는 엣지 다비이스가 카메라 장치인 예를 나타내고, 도 19b는 엣지 디바이스가 드론(drone)인 예를 나타낸 다. 도 19a를 참조하여 알 수 있는 바와 같이, 엣지 디바이스는 CCTV(closed-circuit television), IP(Internet Protocol) 기반 혹은 웹 기반 카메라일 수 있다. 일반적인 카메라를 통해 원거리 상의 피사체를 촬 영하기 위해서는 반드시 고배율 광학 렌즈를 구비하여여만 하였다. 그러나, 고배율 광학 렌즈는 상당히 고가이 고, 렌즈를 줌인 줌아웃하기 위해서는 액추에이터(즉 모터)를 구동해야 하나, 빈번한 모터의 구동은 악조건 하에서 내구성을 보장할 수 없는 단점이 있었다. 또한, 원거리 상의 피사체를 줌 아웃을 하여 촬영하는 동안에는, 근거리 상의 피사체를 촬영할 수 없는 단점이 있었다. 따라서, 본 개시는 단일 렌즈를 통하여 촬영된 영상을 영상 개선 신경망, 컨볼류션 신경망, 객체 인식 신경망, 객체의 이동 경로 예측 신경망 중 하나 이상에 입력하는 것을 제안한다. 구체적으로, 본 개시는 단일 렌즈를 통 하여 촬영된 영상을 영상 개선 신경망을 통해 전처리 및 초해상화하고, 객체 인식 신경망을 통해 상기 초해상화 된 영상 내에서 특정 피사체(예컨대, 임의의 사람)를 객체로서 인식하는 것을 제안한다. 또한, 본 개시는 객체 의 이동 경로 예측 신경망을 통하여 상기 객체로서 인식된 피사체가 이동할 경로를 예측한 후, 상기 이동 경로 결정 신경망을 통하여 추론된 상하좌우의 방향으로 카메라를 회전시키는 것을 제안한다. 한편, 도 19b를 참조하여 알 수 있는 바와 같이, 엣지 디바이스는 카메라를 구비한 드론(drone)일 수 있 다. 이동하는 타겟 물체를 추적하면서 촬영하기 위해서는 사람이 컨트롤러를 이용하여 원격으로 드론을 조정하여야 만 한다. 그러나 이는 상당한 숙력도를 요구하기 때문에, 고비용이 수반되었다. 따라서, 본 개시는 드론에 장착된 카메라로 촬영된 영상을 영상 개선 신경망을 통해 전처리 및 초해상화하고, 객체 인식 신경망을 통해 상기 초해상화된 영상 내에서 특정 피사체(예컨대, 임의의 사람)를 객체로서 인식하는 것을 제안한다. 또한, 본 개시는 객체의 이동 경로 예측 신경망을 통하여 상기 객체로서 인식된 피사체가 이동 할 경로를 예측한 후, 이동 경로 결정 신경망을 통하여 추론된 방향으로 드론을 자동 조정하는 것을 제안한다. 도 20a는 도 19a에 도시된 카메라 장치 또는 도 19b에 도시된 드론의 구성을 예시적으로 나타낸 블록도이다. 도 20a를 참조하면, 엣지 디바이스의 일 예인 카메라 장치 또는 드론은 도 1 또는 도 3에 도시된 NPU, 메모리, 무선 통신부, 입력부, 시스템 버스, CPU을 포함할 수 있다. 상기 NPU는 상기 엣지 디바이스를 위해 필요한 복수의 신경망을 위한 연산을 수행할 수 있다. 예를 들어, 상기 엣지 디바이스 를 위해 필요한 복수의 신경망은 영상 개선 신경망, 컨볼류션 신경망, 객체 인식 신경망, 객체의 이동 경로 예측 신경망, 이동할 경로(방향) 결정 신경망 중 하나 이상을 포함할 수 있다. 상기 NPU의 NPU 스케줄러는 상기 복수의 신경망을 위한 연산을 PE들에게 할당할 수 있다. 즉, 제1 신 경망을 위한 연산을 제1 그룹의 PE들에게 할당하고, 제2 신경망을 위한 연산을 제2 그룹의 PE들에게 할당할 수 있다. 구체적인 예를 들면, 상기 NPU 스케줄러은 영상 개선 신경망을 위한 연산을 제1 그룹의 PE들에게 할 당하고, 컨볼류션 신경망을 위한 연산을 제2 그룹의 PE들에게 할당하고, 객체 인식 신경망을 제3 그룹의 PE들에 게 할당하고, 객체의 이동 경로 예측 신경망을 제4 그룹의 PE들에게 할당하고, 이동할 경로(방향) 결정 신경망 을 제5 그룹의 PE들에게 할당할 수 있다. 도 20b는 도 20a의 변형예를 나타낸 블록도이다. *도 20b에 도시된 엣지 디바이스는 도 20a와 달리 복수의 NPU, 예시적으로 2개의 NPU(100a, 100b)를 포 함할 수 있다. CPU는 메모리로부터 인공 신경망 모델들의 조합 정보를 획득할 수 있다. 그리고, CPU는 상기 획득된 조합 정보에 기초하여, 상기 메모리로부터 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있 다. 그런 후, 상기 CPU는 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할당하고, 제2 인공 신경망 모델을 제 2 그룹의 PE들에게 할당할 수 있다. 상기 CPU에 의한 할당에 대해 예를 들어 설명하면, 제1 NPU(100a)는 영상 개선 신경망을 위한 연산을 수 행하는 제1 그룹의 PE들, 컨볼류션 신경망을 위한 연산을 수행하는 제2 그룹의 PE들, 객체 인식 신경망을 위한 연산을 수행하는 제3 그룹의 PE들을 포함하는 것으로 도 20b 예시적으로 나타나 있다. 그리고, 제2 NPU(100b)는 객체의 이동 경로 예측 신경망을 위한 연산을 수행하는 제4 그룹의 PE들 그리고 이동할 경로(방향) 결정 신경망 을 위한 연산을 수행하는 제5 그룹의 PE들을 포함하는 것으로 도시되어 있다. 그러나, 이는 예시에 불과하고, 제1 NPU(100a)와 제2 NPU(100b)에 의해서 수행되는 신경망의 종류 혹은 개수는 자유롭게 변형될 수 있다. 이하에서는 객체 인식 신경망에 대해서 자세하게 설명하기로 한다. 객체 인식 신경망은 영상을 입력 받으면, 영상 내에 포함된 객체를 인식할 수 있다. 이미지 파라미터가 상이한 복수의 이미지에 포함된 객체를 인식하는 정확도, 즉 객체 인식률은 상이할 수 있다. 여기서, 이미지 파라미터 는 이미지의 특징을 나타내는 임의의 파라미터 또는 그 조합을 지칭할 수 있다. 또는, 이미지 파라미터는 이미 지의 세부 특징의 각각을 나타내는 임의의 서브 파라미터를 포함할 수 있다. 예를 들어, 이미지 파라미터는 이 미지의 Demosaicing, WDR(Wide Dynamic Range) 또는 HDR(High Dynamic Range), Deblur, Denoise, Color Tone mapping, White Balance, Decompression 중 적어도 하나와 연관된 서브 파라미터를 포함할 수 있으며, 이에 한 정되지 않으며, 이미지의 특징을 나타낼 수 있는 임의의 파라미터 또는 서브 파라미터를 포함할 수 있다. 도 21은 광량의 변화에 따른 영상 결과와 인식율을 나타낸 예시도이다. 구체적으로, 도 21의 (a) 부분에는 먼저 광량의 변화에 따른 영상 결과가 나타나 있고, 도 21의 (b) 부분에는 광량의 변화에 따른 인식률이 나타나 있다. 도 21의 (b) 부분에 도시된 그 래프는 ImageNet이라는 5만장의 영상 데이터에서 각 이미지에 광량을 조절한 상 태에서 GoogleNet이라는 딥러닝 인식 모델을 사용하여 인식률을 측정한 실험 결과이다. 수신된 이미지의 객체 인식률은 이미지를 나타내는 특징, 즉 이미지 파라미터 또는 서브 파라미터 중 하나인 광 량에 따라 달라질 수 있다. 도 21의 (a) 부분에 도시된 바와 같이, 수신된 이미지가 갖고 있는 광량의 평균값인 Δμ의 값을 변화시켜 영상 의 광량을 변화시킬 수 있으며, Δμ의 값이 클수록 광량이 밝아지는 것이 확인된다. 동일한 객체를 촬상한 이미지의 광량을 변화시킨 복수의 이미지 중 선호하는 이미지는 사람마다 상이할 수 있다. 즉, 사람마다 가지고 있는 망막 내 시세포(예를 들어, 원뿔세포(cone cell) 등)가 각각 상이하기 때문에, 그러한 이미지를 본 사람들의 각각은 다른 이미지를 선호할 수 있다. 이에 반하여, 객체 인식용 신경망을 이용하는 경우, 이와 같은 선호도가 전혀 기여되지 않는다. 예를 들어, 선 호도에 따라, 사람은 Δμ의 값이 50인 영상을 객체 인식을 위한 가장 적절한 영상으로 선택할 수 있으나, 도 22의 (b)에서 보이는 바와 같이, Δμ의 값이 0일 때 객체 인식 신경망에서의 객체 인식률이 가장 높았다. 즉, 광량이 적절한 값을 가질 때, 딥러닝 인식 모델의 인식률이 가장 높다는 것을 의미한다. 본 예시에서는, 객체 인식 신경망 모델로서 GoogleNet 모델이 이용되었으나, 이에 한정되지 않을 수 있다. 도 22는 선명도의 변화에 따른 영상 결과와 인식률을 나타낸 예시도이다. 구체적으로 도 22의 (a) 부분에는 선명도의 변화에 따른 영상 결과가 나타나 있고, 도 22의 (b) 부분에는 선명 도의 변화에 따른 인식률이 나타나 있다. 앞서 설명한 바와 같이, 객체 인식률은 광량뿐만 아니라, 이미지 파라미터 또는 서브 파라미터 중 하나인 선명 도에 따라 달라질 수 있다. 수신된 이미지가 갖고 있는 선명도와 관련된 σ의 값을 변화시켜 영상의 선명도를 변화시킬 수 있다. 도 22의 (a) 부분에 도시된 바와 같이, σ의 값이 0일 때(즉, 원본(Original)일 때) 예시가 가장 선명하며, σ 의 값이 클수록 영상이 점점 흐려지는 것을 확인할 수 있다. 도 22의 (b) 부분에 도시된 그래프는 ImageNet이라는 5만장의 영상 데이터에서 각 이미지에 선명도를 조절한 상 태에서 GoogleNet이라는 딥러닝 인식 모델을 사용하여 인식률을 측정한 실험 결과이다. 도 22의 (b) 부분에 도시된 바와 같이, σ의 값이 0일 때(즉, 원본(Original)일 때), 객체 인식 장치의 객체 인 식 모듈에서의 객체 인식률이 가장 높게 나타난다. 즉, 선명도와 관련된 σ의 값이 가장 작을 때 딥러닝 인식 모델의 인식률이 가장 높은 것을 의미한다. 이상에서 설명한 바와 같이, 객체 인식 신경망 모델로서 GoogleNet 모델이 이용되었으나, 이에 한정되지 않을 수 있다. 도 21 및 도 22를 참조하면, 영상의 광량이 적절한 값을 가지고, 선명도가 높을 때 객체 인식 신경망에 의한 인 식률이 높다는 것을 알 수 있다. 이상에서 설명한 바와 같이, 사람이 선호하는 고화질의 영상과 인공신경망 기반 객체 인식 장치의 인식률을 극 대화시킬 수 있는 영상은 차이가 있을 수 있다. 예를 들어, 개를 견종별로 분류하는 확률은 사람보다 인공 신 경망이 더 뛰어날 수 있다. 즉, 입력되는 이미지를 객체 인식 신경망의 입력층에 입력하기 이전에, 객체 인식률을 극대화하기 위해 영상에 대해 개선 처리가 수행될 수 있다. 이러한 영상 개선 처리에 대해 후술하기로 한다. 기존의 영상 전처리 기술은 사람이 선호하는 고화질 영상을 출력하기 위해 구현된 반면, 본 개시에서 목표하는 영상 처리 기술은 객체 인식 신경망의 인식률을 향상시키는 것을 목표로 한다. 도 23은 이미지에 포함된 객체를 인식하고 피드백 데이터를 제공하는 과정을 나타낸 블록도이다. 도시된 영상 개선 신경망은 입력된 영상에 대해서 개선 처리를 한 후 출력하여 객체 인식 신경망으로 전달할 수 있다. 영상 개선 신경망은 도 18을 참조하여 알 수 있는 바와 같이, 압축 해제/디코딩 과정(S401), 영상 전처리 과정 (S403), 그리고 초해상화 과정(S405)을 포함할 수 있다. 상기 영상 전처리 과정(S403)은 이미지의 신호 처리를 위하여 사용되는 임의의 함수 및 변수를 이용할 수 있다. 상기 영상 전처리 과정(S403)은, 입력된 이미지를 영상 전처리 신경망 모델을 통해 전처리한 후 출력할 수 있다. 여기서, 영상 전처리 신경망 모델은 객체 인식용 신경망에서 이미지 내의 객체 인식률을 극대화하기 위한 임의의 확률 모델을 포함할 수 있다. 또 다른 예로서, 영상 전처리 신경망 모델은 CNN(CNN: convolutional neural networks), Deblur Network, Denoise Network 등과 이미지 전처리 네트워크를 포함할 수 있다. 상기 영상 전처리 신경망 모델은 객체의 인식에 최적화된 이미지가 출력되도록 학습된 것일 수 있다. 구체적으 로, 상기 영상 전처리 신경망 모델은 복수의 참조 이미지와 복수의 참조 이미지 각각에 대한 객체 인식 결과를 피드백(feedback)받아, 객체의 인식에 최적화된 이미지가 출력되도록, 반복 학습될 수 있다. 여기서, 참조 이미 지는 열화(image degradation)된 이미지와 원본 이미지로 구성된 한 쌍의 학습 데이터일 수 있다. 이를 위하여, 상기 영상 전처리 과정(S403)는 학습 과정을 더 포함할 수 있다. 상기 학습 과정은 복수의 참조 이미지와 복수의 참조 이미지 각각에 대한 객체 인식 결과를 기초로 객체의 인식에 최적화된 이미지를 추론하는 영상 전처리 신경망 모델을 생성할 수 있다. 상기 영상 전처리 신경망 모델은 객체의 인식에 최적화된 이미지 를 출력할 수 있도록 기계학습 알고리즘을 통해 학습될 수 있다. 즉, 상기 영상 전처리 신경망 모델은 객체의 인식에 최적화된 이미지를 출력할 수 있도록 학습시킬 수 있다. 상기 영상 전처리 과정(S403)은 외부 장치로부터 수신한 이미지 또는 카메라로부터 촬상된 이미지를 입력받아, 객체의 인식에 최적화된 이미지를 출력하여 객체 인식 신경망으로 전달할 수 있다. 상기 객체 인식 신경망은, 상기 영상 개선 신경망에 의해서 출력된 이미지를 전달받아 이미지 내에 포함된 객체 를 인식할 수 있다. 그리고, 상기 객체 인식 신경망은 상기 영상 개선 신경망으로부터 출력된 이미지 내에 포함 된 객체의 인식 결과를 상기 영상 개선 신경망으로 피드백할 수 있다. 상기 객체 인식 신경망은 예시적으로 상기 미리 학습된 딥 뉴럴 네트워크(DNN: Deep Neural Network)일 수 있다. 혹은 상기 객체 인식 신경망은 판독기 네트워크(예, VGG, ResNet, YOLO, SSD 등)를 이용하여 입력된 이미 지 내의 객체를 탐지하거나 인식할 수 있다. 상기 피드백되는 인식 결과는 이미지 내에 포함된 객체가 인식되었는지 여부에 대한 정보를 포함할 수 있다. 예를 들어, 객체가 인식되었는지 여부는, 객체 인식률이 일정 이상의 임계 인식률을 초과했는지에 기초하여 판 단될 수 있다. 또 다른 예로서, 객체 인식에 대한 확률 뿐만 아니라 신뢰도(confidence level)을 산출하여 이미 지 내의 객체의 인식 여부가 결정될 수 있다. 상기 피드백되는 인식 결과는 객체가 인식되었는지 여부 뿐만 아 니라 객체의 인식 결과에 대한 임의의 가공 정보를 포함할 수 있다. 상기 피드백되는 인식 결과는 객체 인식 여부에 대한 정보만을 포함하는 것으로 한정되지 않으며, 객체 인식 속 도, 객체 인식의 정확도(또는, 객체 인식률), 객체를 인식한 이미지의 파라미터 등과 같이 객체 인식 중에 발생 하는 다양한 파라미터 또는 객체 인식에 관여되는 다양한 요소를 포함할 수 있다. 상기 영상 개선 신경망 내의 영상 전처리 과정(S403)은, 상기 피드백되는 인식 결과를 기초로, 이미지를 영상 개선 처리하는데 사용된 변수를 조정할 수 있다. 여기서, 변수는 영상 개선 처리 기술(예를 들어, 신호 처리 연 산)을 수행할 때, 변화되는 값일 수 있다. 예를 들어, 이러한 변수는 이미지 파라미터를 결정하는 요인을 포함 할 수 있다. 상기 영상 개선 신경망 내의 영상 전처리 과정(S403)은 이미지 파라미터를 조정하여 상기 이미지의 영상 개선 처리를 수행할 수 있다. 예를 들어, 영상 전처리 과정(S403)은 가우시안 필터의 아래 수학식을 이용하여 수신된 이미지의 블러(blur) 파라미터 또는 서브 파라미터를 조정해 영상 개선 처리할 수 있다. 수학식 1"}
{"patent_id": "10-2024-0053429", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, σ는 블러링의 정도를 결정하는 변수를 나타내며, 변수 σ의 값이 클수록 이미지를 더욱 더 블러시킬 수 있다. 예를 들어, 영상 전처리 과정(S403)은 객체 인식 신경망에 의해 피드백되는 인식 결과에 기초하여 변 수 σ의 값을 조정할 수 있고, 조정된 변수에 의해 수신된 이미지를 영상 개선 처리함으로써, 객체 인식률에 최 적화된 이미지를 출력할 수 있다. 영상 전처리 과정(S403)이 영상 전처리 신경망 모델을 통해 입력된 이미지를 영상 개선 처리하는 경우, 객체 인 식 신경망에 의해 피드백되는 인식 결과를 이용하여 영상 전처리 신경망 모델을 재학습 또는 업데이트할 수 있 다. 예를 들어, 영상 전처리 과정(S403)은 피드백되는 인식 결과를 분석하고, 분석된 결과에 기초하여 영상 전 처리 신경망 모델에 포함된 가중치 값들을 수정할 수 있다. 구체적으로, 영상 전처리 과정(S403)은 객체 인식 신경망의 객체 인식률을 극대화할 수 있는 전처리 이미지를 출력할 수 있도록, 사전 학습된 객체 인식 신경망을 통해 출력된 출력값인, 전처리된 이미지 내에 포함된 객체 의 인식 결과와 이에 대한 피드백 데이터에 기초하여 영상 전처리 신경망 모델의 파라미터(예: 가중치)를 학습 시킬 수 있다. 객체 인식 신경망은 영상 전처리 신경망으로부터 출력되는 이미지를 이용하여 객체를 인식할 수 있다. 객체 인 식 신경망으로부터 피드백되는 인식 결과를 이용하여, 상기 영상 개선 신경망(구체적으로, 영상 개선 신경망)의 가중치는 업데이트될 수 있다. 따라서, 객체 인식 신경망의 인식률이 향상될 수 있다. 영상 전처리 신경망 및 객체 인식 신경망은 딥러닝 모델과 같이 사전 학습된 네트워크 일 수 있으나, 이에 한정 되지 않는다. 이상에서 설명한 바와 같이, 학습이 반복되도록 함으로써, 영상 전처리 신경망 및 객체 인식 신 경망의 정확도 및/또는 신뢰도가 향상될 수 있다. 도 24는 영상 전처리 과정의 세부 과정을 나타낸 흐름도이다. 도 24에 도시된 영상 전처리 과정은 도 18에 도시된 영상 전처리 과정(S403)에 대응될 수 있다. 도시된 영상 전처리 과정은 입력되는 이미지의 파라미터를 조정하여 개선 처리를 수행할 수 있다. 여기서, 이미 지 파라미터는, 수신된 이미지의 디블러(Deblur), 디노이즈(Denoise), WDR(Wide Dynamic Range) 또는 HDR(High Dynamic Range), 색상 톤 매핑(color tone mapping), 디모자이킹(Demosaicing) 중 적어도 하나를 나타내는 이 미지 서브 파라미터를 포함할 수 있다. 상기 영상 전처리 과정은, 복수의 이미지 서브 파라미터의 각각을 순차적으로 조정할 수 있다. 예를 들어, 복 수의 이미지 서브 파라미터의 각각을 조정할 때, 제1 서브 파라미터의 조정 결과를 제2 서브 파라미터의 조정시 반영되도록 할 수 있다. 도시된 바와 같이, 영상 전처리 과정은 이미지를 디블러하는 과정(S501), 이미지를 디노이즈하는 과정(S503), 이미지에 대해 HDR 또는 WDR을 위한 처리를 수행하는 과정(S505), 이미지의 색상 톤 매핑을 수행하는 과정 (S507), 이미지를 디모자이킹하는 과정(S509) 중 하나 이상을 포함할 수 있다. 상기 영상 전처리 과정은 전술한 바와 같이 영상 전처리 신경망 모델을 이용하여 수행될 수 있다. 상기 영상 전 처리 신경망 모델은 디블러 과정(S501), 디노이즈 과정(S503), HDR 또는 WDR을 위한 처리를 수행하는 과정 (S505), 이미지의 색상 톤 매핑을 수행하는 과정(S507), 디모자이킹하는 과정(S509)을 순차적으로 수행하도록 학습될 수 있다. 또는, 상기 영상 전처리 신경망 모델은 상기 과정들(S501, S503, S505, S507, S509)을 동시에 수행하도록 학습될 수 있다. 상기 영상 전처리 과정은 각각의 서브 파라미터를 순차적으로 조정하는 것이 아닌, 수신된 이미지에 대해 다수 의 파라미터를 조정하여 객체 인식에 최적화된 전처리 이미지를 출력하도록 학습된 영상 전처리 신경망 모델을이용하여 수행될 수 있다. 도 24에서는 상기 영상 전처리 과정이 디블러 과정(S501), 디노이즈 과정(S503), HDR 또는 WDR을 위한 처리를 수행하는 과정(S505), 이미지의 색상 톤 매핑을 수행하는 과정(S507), 디모자이킹하는 과정(S509)을 포함하는 것으로 예시적으로 도시되었으나, 이에 한정되지 않을 수 있다. 또한, 상기 과정들의 순서는 도 24에 도시된 순 서에 한정되지 않을 수 있다. 도 25는 이미지에 포함된 객체를 인식하는 예를 나타낸 예시도이다. 객체 인식 과정은 R-CNN(Regions with Convolutional Neural Network)을 이용하여 수신된 이미지에 포함된 객 체를 인식할 수 있다. R-CNN은 도 25에 도시된 바와 같이, 입력된 이미지에서 선택적 탐색(Selective Search) 알고리즘을 이용하여 후보 영역들을 생성할 수 있다. 생성된 각 후보 영역들은 동일한 크기로 변환되고, CNN을 통해 이미지에 포함된 객체의 특징이 추출될 수 있다. 추출된 특징을 이용하여 후보 영역 내의 객체들이 서포트 벡터 머신(Support Vector Machine)을 이용하여 분류될 수 있다. 도 25에 도시된 바와 같이, 이미지에 포함된 객체들은 사람, 나무, 차량, 등 여러가지로 분류될 수 있다. 객체 인식 과정은 분류된 객체를 기초로 이미지 내의 객체를 탐지하거나 인식할 수 있다. 도 25에서는 객체 인식 과정이 R-CNN을 이용하는 것으로 예시적으로 도시되고 있으나, 이에 한정되지 않으며, 객체 인식 과정은 이미지 내의 객체를 인식할 수 있는 임의의 인공 신경망을 이용할 수 있다. 즉, AlexNet 또는 GoogleNet과 같이 사전 훈련된 네트워크를 이용하여 이미지에 포함된 객체가 인식될 수 있다. 상기 객체 인식 신경망은 학습에 의해 구축될 수 있다. 구체적으로, 수천에서 수만장의 학습 데이터(학습 이미 지)를 분석하여 각 객체를 구분하기 위한 특징들이 학습될 수 있고, 각 객체의 차이를 식별하는 방법이 학습되 어, 상기 객체 인식 신경망이 구축될 수 있다 도 26은 이미지에 포함된 객체를 인식한 결과를 나타낸 예시도이다. 도 26의 (a) 부분에 도시된 이미지와 같이, 촬상시 흔들린 이미지의 객체를 딥 뉴럴 네트워크를 통해 인식한 경 우, 객체 인식률이 61%로 나오며, 도 26의 (b)부분에 도시된 정상적으로 촬상된 (Ground truth) 이미지의 객체 를 딥 뉴럴 네트워크를 통해 인식하는 경우, 객체 인식률이 74%로 나오는 것을 확인할 수 있다. 따라서, 전술한 바와 같이 객체를 인식하기 전에, 디블러(Deblur) 처리하여 영상 개선 처리할 수 있다. 도 26의 (a) 부분에 도시된 이미지에 대해서 영상 개선 신경망을 이용하여 디블러 과정을 수행하면, 도 26의 (c) 부분에 도시된 이미지와 같이 될 수 있다. 즉, 도 26의 (a) 부분에 도시된 이미지가 도 26의 (c) 부분에 도시된 이미지와 같이 복원될 수 있다. 따라서, 객체 인식률은 82%로 향상될 수 있다. 도 27a는 엣지 다비이스가 로봇인 예를 나타내고, 도 29b는 엣지 디바이스가 자율 주행 차량인 예를 나타낸다. 도 27a에서는 엣지 디바이스가 2족 보행 로봇인 것으로 나타나 있다. 그러나 이와 달리 엣지 디바이스는 4족 보 행 로봇 혹은 바퀴(wheel)를 구비한 로봇일 수 있다. 또한, 도 27b에서는 엣지 디바이스가 자가용인 것으로 도 시되어 있다. 그러나 이와 달리 엣지 디바이스는 상업용 챠량, 예컨대 트럭 또는 버스일 수 있다. 도 28a는 도 27a 또는 도 27b에 도시된 엣지 디바이스의 구성을 나타낸 예시적인 블록도이다. 도 28a를 참조하여 알 수 있는 바와 같이, 엣지 디바이스의 일 예인 로봇 또는 자율 주행 차량은 도 1 또 는 도 3에 도시된 NPU, 메모리, 무선 통신부, 입력부, 시스템 버스, CPU을 포함할 수 있다. 상기 입력부는 카메라, 레이더, 라이더, 자이로 센서 및 가속도 센서 중 하나 이상을 포함할 수 있다. 상기 메모리는 인공 신경망 모델 저장부와 인공 신경망 모델들의 조합 정보 저장부를 포함할 수 있다. 상기 NPU는 상기 엣지 디바이스를 위해 필요한 복수의 신경망을 위한 연산을 수행할 수 있다. 예를 들어, 상기 엣지 디바이스를 위해 필요한 복수의 신경망은 컨볼류션 신경망, 객체 인식 신경망, 객체의 움직임 예측 신경망, 객체의 이동 경로 예측 신경망, 이동할 경로(방향) 결정 신경망 중 하나 이상을 포함할 수 있다. 상기 NPU의 NPU 스케줄러는 상기 복수의 신경망을 위한 연산을 PE들에게 할당할 수 있다. 즉, 제1 신 경망을 위한 연산을 제1 그룹의 PE들에게 할당하고, 제2 신경망을 위한 연산을 제2 그룹의 PE들에게 할당할 수 있다. 구체적인 예를 들면, 상기 NPU 스케줄러은 컨볼류션 신경망을 위한 연산을 제1 그룹의 PE들에게 할 당하고, 객체 인식 신경망을 위한 연산을 제2 그룹의 PE들에게 할당하고, 객체의 움직임 예측 신경망을 제3 그 룹의 PE들에게 할당하고, 객체의 이동 경로 예측 신경망을 위한 연산을 제4 그룹의 PE들에게 할당하고, 이동할 경로(방향) 결정 신경망을 제5 그룹의 PE들에게 할당할 수 있다. 도 28b는 도 28a의 변형예를 나타낸 블록도이다. 도 28b에 도시된 엣지 디바이스는 도 28a와 달리 복수의 NPU, 예시적으로 2개의 NPU(100a, 100b)를 포함 할 수 있다. 상기 입력부는 카메라, 레이더, 라이더, 자이로 센서 및 가속도 센서 중 하나 이상을 포함할 수 있다. 상기 메모리는 인공 신경망 모델 저장부와 인공 신경망 모델들의 조합 정보 저장부를 포함할 수 있다. CPU는 메모리로부터 인공 신경망 모델들의 조합 정보를 획득할 수 있다. 그리고, CPU는 상기 획득된 조합 정보에 기초하여, 상기 메모리로부터 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있 다. 그런 후, 상기 CPU는 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할당하고, 제2 인공 신경망 모델을 제 2 그룹의 PE들에게 할당할 수 있다. 상기 CPU에 의한 할당에 대해 예를 들어 설명하면, 제1 NPU(100a)는 컨볼류션 신경망을 위한 연산을 수행 하는 제1 그룹의 PE들, 객체 인식 신경망을 위한 연산을 수행하는 제2 그룹의 PE들, 객체의 움직임 예측 신경망 을 위한 연산을 수행하는 제3 그룹의 PE들을 포함하는 것으로 도 28b 예시적으로 나타나 있다. 그리고, 제2 NPU(100b)는 객체의 이동 경로 예측 신경망을 위한 연산을 수행하는 제4 그룹의 PE들 그리고 이동할 경로(방향) 결정 신경망을 위한 연산을 수행하는 제5 그룹의 PE들을 포함하는 것으로 도시되어 있다. 그러나, 이는 예시에 불과하고, 제1 NPU(100a)와 제2 NPU(100b)에 의해서 수행되는 신경망의 종류 혹은 개수는 자유롭게 변형될 수 있다. 이하, 도 28a 및 도 28b를 함께 참조하여 설명하기로 한다. 상기 엣지 디바이스의 일 예인 로봇 또는 자율 주행 차량은 상기 카메라를 통하여 촬영된 영상을 상기 CPU으로 전달할 수 있다. 상기 촬영된 영상은 메모리에 임시 저장된 후 상기 CPU으로 전 달될 수 있다. 상기 컨볼류션 신경망은 상기 카메라를 통해 촬영된 이미지에 대해서 컨볼류션 연산을 수행하여 특징을 추출한 후 객체 인식 신경망으로 전달할 수 있다. 상기 객체 인식 신경망은 상기 영상 내의 다수의 객체들을 인 식할 수 있다. 상기 영상 내에서 인식된 상기 다수의 객체들은 상기 레이더 및/또는 상기 라이다에 의해서 추적될 수 있다. 상기 객체의 움직임 예측 신경망 및 상기 객체의 이동 경로 예측 신경망은 상기 레이더 및/또는 상기 라 이다에 의해서 추적되는 상기 객체들의 움직임 및/또는 이동 경로를 예측할 수 있다. 상기 이동 경로(방향) 결정 신경망은 상기 예측된 객체들의 움직임 및/또는 이동 경로에 기초하여, 상기 로봇 또는 자율 주행 차량이 상기 객체를 회피할 수 있는 이동 경로(방향)을 추론할 수 있다. 상기 CPU는 상기 이동 경로(방향) 결정 신경망으로부터 출력되는 경로(혹은 방향)으로 상기 로봇 또는 자 율 주행 차량을 이동시킬 수 있다. 도 29a는 엣지 다비이스가 스마트 폰인 예를 나타내고, 도 29b는 엣지 디바이스가 웨어러블 기기인 예를 나타내 고, 도 29c는 엣지 디바이스가 스마트 스피커인 예를 나타내고, 도 29d는 엣지 디바이스가 TV(television)인 예 를 나타내고, 도 29e는 엣지 디바이스가 가전 제품인 냉장고인 예를 나타내고, 도 29f는 엣지 디바이스가 가전 제품인 세탁기인 예를 나타낸다. 도시된 바와 같이, 엣지 다비이스는 사용자에 의해서 활용되는 다양한 전자 제품일 수 있다. 예를 들어, 엣지 디바이스는 도시된 스마트폰 또는 웨어러블 기기 외에도 태블릿 (tablet), 노트북(notebook) 혹은 랩탑(laptop)컴퓨터 등과 같은 사용자 장비(User Equipment)일 수 있다. 다른 예를 들어, 예를 들어, 엣지 디바이스는 도시 된 TV, 냉장고, 세탁기와 같은 가전 제품 외에도, 전자 레인지(microwave oven), 보일러, 에어컨(air conditioner) 등일 수 있다. 도 30a는 도 29a 내지 도 29f에 도시된 엣지 디바이스의 구성을 나타낸 예시적인 블록도이다. 도 30a를 참조하여 알 수 있는 바와 같이, 도 29a 내지 도 29f에 도시된 엣지 디바이스는 도 1 또는 도 3 에 도시된 NPU, 메모리, 무선 통신부, 입력부, 시스템 버스, CPU을 포함할 수 있다. 상기 입력부는 카메라 및 마이크로 폰 중 하나 이상을 포함할 수 있다. 상기 메모리는 인공 신경망 모델 저장부와 인공 신경망 모델들의 조합 정보 저장부를 포함할 수 있다. 상기 NPU는 상기 엣지 디바이스를 위해 필요한 복수의 신경망을 위한 연산을 수행할 수 있다. 예를 들어, 상기 엣지 디바이스를 위해 필요한 복수의 신경망은 동작 인식 신경망, 사용 패턴 분석 신경망, 그리고 음성 인 식 신경망 중 하나 이상을 포함할 수 있다. 상기 NPU의 NPU 스케줄러는 상기 복수의 신경망을 위한 연산을 PE들에게 할당할 수 있다. 즉, 제1 신 경망을 위한 연산을 제1 그룹의 PE들에게 할당하고, 제2 신경망을 위한 연산을 제2 그룹의 PE들에게 할당할 수 있다. 구체적인 예를 들면, 상기 NPU 스케줄러은 동작 인식 신경망을 위한 연산을 제1 그룹의 PE들에게 할 당하고, 사용 패턴 분석 신경망을 위한 연산을 제2 그룹의 PE들에게 할당하고, 음성 인식 신경망을 제3 그룹의 PE들에게 할당할 수 있다. 도 30b는 도 30a의 변형예를 나타낸 블록도이다. 도 30b에 도시된 엣지 디바이스는 도 30a와 달리 복수의 NPU, 예시적으로 2개의 NPU(100a, 100b)를 포함 할 수 있다. 상기 입력부는 카메라 그리고 마이크로 폰 중 하나 이상을 포함할 수 있다. 상기 메모리는 인공 신경망 모델 저장부와 인공 신경망 모델들의 조합 정보 저장부를 포함할 수 있다. CPU는 메모리로부터 인공 신경망 모델들의 조합 정보를 획득할 수 있다. 그리고, CPU는 상기 획득된 조합 정보에 기초하여, 상기 메모리로부터 다수의 인공 신경망 모델들에 대한 정보를 획득할 수 있 다. 그런 후, 상기 CPU는 제1 인공 신경망 모델을 제1 그룹의 PE들에게 할당하고, 제2 인공 신경망 모델을 제 2 그룹의 PE들에게 할당할 수 있다. 상기 CPU에 의한 할당에 대해 예를 들어 설명하면, 제1 NPU(100a)는 동작 인신 신경망을 위한 연산을 수 행하는 제1 그룹의 PE들, 사용 패턴 분석 신경망을 위한 연산을 수행하는 제2 그룹의 PE들을 포함하는 것으로 도 28b 예시적으로 나타나 있다. 그리고, 제2 NPU(100b)는 음성 인식 신경망을 위한 연산을 수행하는 제3 그룹 의 PE들을 포함하는 것으로 도시되어 있다. 그러나, 이는 예시에 불과하고, 제1 NPU(100a)와 제2 NPU(100b)에 의해서 수행되는 신경망의 종류 혹은 개수는 자유롭게 변형될 수 있다. 이하, 도 30a 및 도 30b를 함께 참조하여 설명하기로 한다. 이하 설명의 편의를 위해 카메라와 마이크로폰을 통해 입력된 신호를 기반으로, 인공 신경망을 통 해 추론을 수행하는 예시에 대해서 설명하기로 한다. 상기 음성 인식 신경망은 마이크로폰으로부터 수신된 음향 신호를 기초로 키워드를 추론 하도록 학습되어 있을 수 있다. 그리고 동작 인식 신경망은 키워드 추론 결과에 응답하여 영상 신호를 기초로 사용자의 제스처를 추론 하도록 학습되어 있을 수 있다. 이때, 음성 인식 신경망은 특정 키워드들만 인식하도록 학습된 인공 신경망일 수 있다. 예를 들면, 특정 키워드 들은, “알렉사”, “시리야”, “볼륨업”, “볼륨다운”, “검색”, “켜줘”, “꺼줘”, ”인터넷”, “음악 ”, “영화” 등의 간단한 키워드 명령어들 일 수 있다. 예를 들면, 특정 키워드들은 1개 내지 100개의 자주 활 용되는 키워드 명령어들일 수 있다.상기 동작 인식 신경망은 특정 제스처만 인식하도록 학습된 인공 신경망일 수 있다. 예를 들면, 특정 제스처들 은, 특정 손동작, 몸동작, 얼굴 표정 등일 수 있다. 상기 사용 패턴 분석 신경망은 사용자의 사용 패턴, 즉 사용자의 음성 또는 제스쳐에 기반하여, 사용자가 엣지 디바이스를 사용하는 패턴을 분석할 수 있다. 상기 분석된 패턴에 따라, 상기 엣지 디바이스는 사 용자에게 여러 제안을 추천할 수 있다. 엣지 디바이스는 추론 결과에 의해서 제1 모드에서 제2 모드로 전환될 수 있다. 제1 모드는 저전력 모드, 즉 스탠바이(standby) 모드이고, 제2 모드는 동작 모드일 수 있다. CPU는 상기 음성 인식 신경망의 추론 결과를 전달받아 엣지 디바이스가 제2 모드가 되도록 제어할 수 있다. 상기 CPU은 제2 모드일 때 카메라에 전원을 공급하도록 구성될 수 있다. 상기 제2 모드에 서 상기 동작 인식 신경망이 추론 연산을 수행할 수 있다. 상기 엣지 디바이스의 NPU은 독립형 모드 혹은 스탠드 얼론(stand-alone) 모드로 동작할 수 있다. 즉, 엣지 디바이스은 인터넷을 통한 클라우드 인공지능 서비스를 제공받지 않고, NPU을 이용하여 인 공 신경망 기반 추론 연산을 자체적으로 수행할 수 있다. 만약, 엣지 디바이스가 무선 통신부를 통 해서 클라우드 컴퓨팅 기반의 서버에서 인공 신경망 추론 서비스를 제공 받을 경우, 추론을 위한 카메라 및 마이크로폰의 데이터를 메모리에 저장한 다음, 무선 통신부를 통해서 전송해야 한다. 이는, 시간 지연(latency)을 야기하고, 전력 모소량을 증가시키는 단점이 있다. 그러나, 본 명세서의 개시에 따르면, 엣지 디바이스는 독립적으로 동작할 수 있는 NPU을 포함하고 있기 때문에, 시간 지연을 단축할 수 있고, 소비 전력을 저감할 수 있는 효과가 있다. 또한, 음성 신호와 영상 신호는 사적인 데이터를 포함할 수 있다. 만약, 엣지 디바이스가 사용자의 대화 내용 또는 사생활을 촬영한 영상을 무선 통신부를 통해서 지속적으로 전송할 경우, 사생활 침해 문제가 발생될 수 있다. 따라서, 엣지 디바이스는 프라이버시 데이터가 포함될 수 있는 입력부의 신호들을 NPU을 이용 하여 인공 신경망 기반 추론 연산을 자체적으로 수행한 후, 상기 프라이버시 데이터를 다음 삭제할 수 있다. 즉, 프라이버시 데이터가 포함될 수 있는 영상 신호 및 음향 신호는 NPU에 의해서 추론 연산 후 삭제될 수 있다. 또한, 엣지 디바이스는 프라이버시 데이터가 포함될 수 있는 입력부의 신호들을 무선 통신부(101 0)를 통해서 전달하는 것을 차단할 수 있다. 또한, 엣지 디바이스는 프라이버시 데이터가 포함될 수 있는 입력부의 신호들을 메모리에 저 장하지 않을 수 있다. 또한, 엣지 디바이스는 프라이버시 데이터가 포함될 수 있는 입력부의 신호들을 프라이버시 데이터 가 포함된 데이터로 분류할 수 있다. 상술한 구성들에 따르면, 엣지 디바이스는 사용자에게 편의를 제공하고, 소비 전력을 저감하면서 동시에 사생활 데이터 누출 문제를 차단할 수 있는 효과가 있다. 도 31a는 복수의 신경망 모델들에 대한 연산이 수행되는 예를 나타낸다. 도 31a를 참조하여 알 수 있는 바와 같이, 제1 ANN 모델을 위한 제1 연산들이 수행된다. 상기 제1 ANN 모델의 i 번째 레이어에 대한 연산이 수행되면, 제2 ANN 모델을 위한 제2 연산이 시작될 수 있다. 도시된 바와 같이, 제1 ANN 모델을 위한 제1 연산과 제2 ANN 모델을 위한 제2 연산은 시분할로 수행될 수 있다. 도 31b는 복수의 신경망 모델들의 연산이 할당된 PE들을 나타낸 예시도이다. 도 31b에는 예시적으로 PE1부터 PE24까지 총 24개의 PE들이 존재하는 것으로 나타나 있다. 제1 ANN 모델을 위한 제1 연산을 위해 할당된 PE들은 PE1부터 PE16까지 총 16개일 수 있다. 제2 ANN 모델을 위한 제2 연산을 위해 할 당된 PE들은 PE10, PE11, PE14, PE15, PE16, PE18, PE19, PE20, PE22, PE23 그리고 PE24로서 총 12개일 수 있 다. 도 31a와 도 31b를 함께 참조하면, 제1 ANN 모델을 위한 연산을 위해 PE1부터 PE16까지 총 16개의 PE들이 할당 될 수 있다. 그러다가, 제1 ANN 모델의 i번째 레이어에 대한 연산이 수행되면, PE1 내지 PE16 중에서 상기PE10, PE11, PE14, PE15 및 PE16는 제2 ANN 모델을 위한 연산을 위해서 재할당될 수 있다. 즉, 상기 제1 ANN 모델의 후속 연산은 나머지 PE들, 즉 PE1, PE2, PE3, PE4, PE5, PE6, PE7, PE8, PE9 그리고 PE13에 의해서만 수행될 수 있다. 도시된 바와 같이 상기 제1 ANN 모델을 위한 제1 연산들과 상기 제2 ANN 모델을 위한 제2 연산들은 병렬적(in parallel) 또는 시분할(time division) 방식으로 수행될 수 있다. 또한, 도시된 바와 같이 상기 제1 ANN 모델의 연산을 위해 할당된 상기 제1 그룹의 PE들과 상기 제2 그룹의 ANN 모델의 연산을 위해 할당된 상기 제2 그룹의 PE들은 부분적으로 동일하거나, 서로 완전히 다를 수 있다. 도 31a에서는 상기 제1 ANN 모델의 i번째 레이어에 대한 연산이 수행되면, 제2 ANN 모델을 위한 제2 연산이 시 작되는 것으로 도시되어 있으나, 이와 달리 다른 변형예도 가능하다. 예를 들어, 제2 ANN 모델을 위한 제2 연산은 상기 복수의 ANN들의 연산 순서에 대한 정보에 기초하여 시작될 수 있다. 상기 연산 순서에 대한 정보는: 레이어에 대한 정보, 커널에 대한 정보, 프로세싱 시간에 대한 정보, 남은 시간 에 대한 정보, 그리고 클럭(clock)에 대한 정보 중 하나 이상을 포함할 수 있다. 상기 레이어에 대한 정보는: 상기 제1 ANN 모델의 모든 레이어들 중에서 i번째 레이어를 나타낼 수 있다. 상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 i번째 레이어를 위한 연산이 시작된 이후에 시작될 수 있다. 상기 커널에 대한 정보는: 상기 제1 ANN 모델의 모든 커널들 중에서 k번째 커널을 나타낼 수 있다. 상기 제2 ANN 모델을 위한 연산은 상기 제1 ANN 모델의 k번째 커널을 위한 연산이 시작된 이후에 시작될 수 있다. 상기 프로세싱 시간에 대한 정보는: 상기 제1 ANN 모델을 위한 연산을 수행한 이후에 경과된 시간을 나타낼 수 있다. 상기 제2 ANN 모델을 위한 연산은 상기 경과 시간 이후에 시작될 수 있다. 상기 잔여 시간에 대한 정보는: 상기 제1 ANN 모델의 연산들이 완료되기까지 남은 시간을 나타낼 수 있다. 상기 제2 ANN 모델을 위한 연산은 상기 남은 시간에 도달하기 전에 시작될 수 있다. 본 명세서와 도면에 나타난 본 개시의 예시들은 본 개시의 기술 내용을 쉽게 설명하고 본 개시의 이해를 돕기 위해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 지금까지 설명한 예시들 이외 에도 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 자명 한 것이다."}
{"patent_id": "10-2024-0053429", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시에 따른 신경망 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 2는 본 개시에 적용될 수 있는 프로세싱 엘리먼트 어레이 중 하나의 프로세싱 엘리먼트를 설명하는 개략적인 개념도이다. 도 3은 도 1에 도시된 NPU의 변형예를 나타낸 예시도이다. 도 4a는 본 명세서의 개시에 따른 NPU을 포함하는 엣지 디바이스의 구성을 나타낸 블록도이다. 도 4b은 도 4a에 도시된 엣지 디바이스의 변형예를 나타낸 블록도이다. 도 5는 예시적인 인공신경망모델을 설명하는 개략적인 개념도이다. 도 6a은 컨볼루션 신경망의 기본 구조를 설명하기 위한 도면이다. 도 6b는 컨볼루션 신경망의 동작을 이해하기 쉽게 나타낸 종합도이다. 도 7a는 본 명세서의 개시에 따른 동작을 도 4a 또는 도 4b에 도시된 엣지 디바이스의 구성 요소들 중 일부를 이용하여 나타낸 예시도이다. 도 7b는 도 7a에 도시된 엣지 디바이스의 변형예를 나타낸 예시도이다. 도 8a는 도 7a 또는 도 7b에 도시된 엣지 디바이스의 동작을 나타낸 흐름도이다. 도 8b는 도 8a의 변형예를 나타낸 흐름도이다. 도 9a 및 도 9b는 엣지 디바이스가 확장현실(eXtended Reality, XR) 기기인 예들을 나타낸다. 도 10a는 도 9a 또는 도 9b에 도시된 XR 기기의 구성을 예시적으로 나타낸 블록도이다.도 10b는 도 10a의 변형 예를 나타낸 블록도이다. 도 11은 도 10a 또는 도 10b에 도시된 XR 기기의 동작을 나타낸 흐름도이다. 도 12는 도 10a 또는 도 10b에 도시된 XR 기기가 사용자의 머리에 착용된 예를 나타낸다. 도 13은 본 개시의 일 실시예에 따른 XR 기기에 의해 제공되는 실감형 콘텐츠가 입체적 공간으로 표시되 었을 때의 예시도이다. 도 14는 사용자가 바라보는 시야의 각도에 기초하여 정의될 수 있는 관심 영역(Region of Interest)의 범위를 나타낸 예시도이다. 도 15는 사용자의 시선에 기초하여 결정된 관심 영역을 영상 개선 처리하는 과정을 나타낸 예시도이다. 도 16은 감출된 사용자의 시선에 기초하여 결정된 관심 영역을 영상 개선 처리하는 과정을 나타낸 예시도이다. 도 17은 감지된 사용자의 시선에 기초하여 결정된 관심 영역에 증강 현실 영상을 합성하여 제공하는 예를 나타 낸 예시도이다. 도 18은 영상 개선을 위한 과정을 나타낸 흐름도이다. 도 19a는 엣지 다비이스가 카메라 장치인 예를 나타내고, 도 19b는 엣지 디바이스가 드론(drone)인 예를 나타낸 다. 도 20a는 도 19a에 도시된 카메라 장치 또는 도 19b에 도시된 드론의 구성을 예시적으로 나타낸 블록도이다. 도 20b는 도 20a의 변형예를 나타낸 블록도이다. 도 21은 광량의 변화에 따른 영상 결과와 인식율을 나타낸 예시도이다. 도 22는 선명도의 변화에 따른 영상 결과와 인식률을 나타낸 예시도이다. 도 23은 이미지에 포함된 객체를 인식하고 피드백 데이터를 제공하는 과정을 나타낸 블록도이다. 도 24는 영상 전처리 과정의 세부 과정을 나타낸 흐름도이다. 도 25는 이미지에 포함된 객체를 인식하는 예를 나타낸 예시도이다. 도 26은 이미지에 포함된 객체를 인식한 결과를 나타낸 예시도이다. 도 27a는 엣지 다비이스가 로봇인 예를 나타내고, 도 29b는 엣지 디바이스가 자율 주행 차량인 예를 나타낸다. 도 28a는 도 27a 또는 도 27b에 도시된 엣지 디바이스의 구성을 나타낸 예시적인 블록도이다. 도 28b는 도 28a의 변형예를 나타낸 블록도이다. 도 29a는 엣지 다비이스가 스마트 폰인 예를 나타내고, 도 29b는 엣지 디바이스가 웨어러블 기기인 예를 나타내 고, 도 29c는 엣지 디바이스가 스마트 스피커인 예를 나타내고, 도 29d는 엣지 디바이스가 TV(television)인 예 를 나타내고, 도 29e는 엣지 디바이스가 가전 제품인 냉장고인 예를 나타내고, 도 29f는 엣지 디바이스가 가전 제품인 세탁기인 예를 나타낸다. 도 30a는 도 29a 내지 도 29f에 도시된 엣지 디바이스의 구성을 나타낸 예시적인 블록도이다. 도 30b는 도 30a의 변형예를 나타낸 블록도이다. 도 31a는 복수의 신경망 모델들에 대한 연산이 수행되는 예를 나타낸다. 도 31b는 복수의 신경망 모델들의 연산이 할당된 PE들을 나타낸 예시도이다."}
