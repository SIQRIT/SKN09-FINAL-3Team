{"patent_id": "10-2020-0065489", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0147771", "출원번호": "10-2020-0065489", "발명의 명칭": "신경망의 불확실성에 기반한 지식 증강 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "김현우"}}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 질문에 대한 지식을 생성하기 위한 지식 생성 신경망과 상기 지식을 기반으로 사용자 질문에 대한 답변을 생성하는 대화 생성 신경망을 포함하도록 컴퓨팅 장치에 의해 프로그래밍 되고 실행되는 인공지능 에이전트의 지식 증강 방법에서,복합 모달 표현부에서, 현재의 사용자 질문, 상기 지식 생성 신경망에 의해 생성된 과거의 지식과 상기 대화 생성 신경망에 의해 생성된 과거의 답변을 특정 차원의 임베딩 벡터값으로 각각 변환하는 단계;불확실성 계산부에서, 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한복수의 출력값을 산출하고, 상기 산출된 복수의 출력값을 기반으로 상기 지식 생성 신경망의 불확실성을 계산하는 단계; 및자율 샘플링부에서, 상기 계산된 불확실성을 기반으로 데이터베이스를 검색하여 학습용 데이터를 획득하고, 상기 획득된 학습용 데이터를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 훈련시켜서 지식 증강을 수행하는 단계를 포함하는 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 불확실성을 계산하는 단계는,상기 드롭아웃 기법을 기반으로 상기 지식 생성 신경망에 포함된 일부 뉴런들을 삭제하는 단계;상기 일부 뉴런들을 삭제하는 방법의 개수에 대응하는 상기 복수의 출력값을 상기 지식 생성 신경망으로부터 획득하는 단계; 및상기 복수의 출력값에 대한 평균과 분산을 기반으로 상기 불확실성을 계산하는 단계를 포함하는 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,상기 불확실성을 계산하는 단계는,데이터의 모델링 과정에서 발생하는 불확실성과 상기 데이터 내의 잡음으로 인해 발생하는 불확실성 계산하는단계인 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서, 상기 불확실성을 계산하는 단계는,상기 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 상기 복수의 출력값에 대한 평균을 계산하는 단계;상기 계산된 평균을 기반으로 데이터의 모델링 과정에서 발생하는 제1 불확실성을 계산하는 단계;상기 복수의 출력값에 대한 분산을 계산하는 단계;상기 계산된 분산을 기반으로 상기 데이터 내의 잡음으로 인해 발생하는 제2 불확실성을 계산하는 단계; 및공개특허 10-2021-0147771-3-상기 제1 불확실성과 상기 제2 불확실성을 합산하여, 상기 지식 생성 신경망의 불확실성을 계산하는 단계를 포함하는 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에서, 상기 지식 증강을 수행하는 단계는,상기 계산된 불확실성을 기반으로 상기 임베딩 벡터값에 대응하는 사용자 요구사항에 대한 상기 지식이 부족한지를 결정하는 단계; 및상기 지식이 부족한 것으로 결정된 경우, 상기 데이터베이스를 검색하여 상기 학습용 데이터를 획득하는 단계를 포함하는 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서,상기 지식이 부족한지를 결정하는 단계는,상기 계산된 불확실성과 사전에 설정한 임계값을 비교하여, 그 비교결과에 따라 상기 지식이 부족한지를 결정하는 단계인 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에서, 상기 학습용 데이터를 생성하는 단계는,상기 계산된 불확실성을 선형 변환하여, 보상값을 계산하는 단계; 및상기 임베딩 벡터를 쿼리값으로 이용하여 상기 계산된 보상값을 갖는 학습용 데이터를 상기 데이터베이스로부터획득하는 단계 를 포함하는 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에서, 상기 지식 증강을 수행하는 단계는,상기 지식 생성 신경망과 상기 대화 생성 신경망을 지도 학습으로 훈련시키는 단계인 것인 신경망의 불확실성에기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에서,상기 인공지능 에이전트는 상기 지식과 상기 답변의 정확도를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 강화 학습으로 훈련시키기 위한 파라미터를 생성하도록 프로그래밍된 가치 추정 신경망을 더 포함하고,상기 지식 증강을 수행하는 단계는,상기 생성된 학습용 데이터를 기반으로 상기 가치 추정 신경망을 더 훈련시키는 단계인 것인 신경망의 불확실성에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "사용자 질문에 대한 지식을 생성하기 위한 지식 생성 신경망과 상기 지식을 기반으로 사용자 질문에 대한 답변을 생성하는 대화 생성 신경망을 포함하도록 프로그래밍된 인공지능 에이전트를 저장하는 저장소와 상기 저장소에 저장된 상기 인공지능 에이전트를 실행하는 프로세서를 포함하는 지식 증강 장치에서, 공개특허 10-2021-0147771-4-상기 인공지능 에이전트는,현재의 사용자 질문, 상기 신경망에 의해 생성된 과거의 지식과 과거의 답변을 특정 차원의 임베딩 벡터값으로각각 변환하는 데이터 전처리부;드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 복수의 출력값을 생성하고, 상기 생성된 복수의 출력값을 기반으로 상기 지식 생성 신경망의 불확실성을 계산하는 불확실성 계산부; 및상기 계산된 불확실성을 선형 변환하여 보상값을 계산하고, 상기 계산된 보상값을 갖는 학습용 데이터를 비지도학습으로 구축된 데이터베이스로부터 획득하고, 상기 획득된 학습용 데이터를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 훈련시켜서 지식 증강을 수행하는 자율 샘플링부를 더 포함하는 지식 증강 장치."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에서,상기 불확실성 계산부는,상기 복수의 출력값에 대한 평균을 기반으로 데이터의 모델링 과정에서 발생하는 제1 불확실성을 계산하고, 상기 복수의 출력값에 대한 분산을 기반으로 상기 데이터 내의 잡음으로 인해 발생하는 제2 불확실성을 계산하고,상기 제1 불확실성과 상기 제2 불확실성을 합산하여, 상기 지식 생성 신경망의 출력에 대한 불확실성을 계산하는 것인 지식 증강 장치."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에서, 상기 자율 샘플링부는,상기 계산된 불확실성이 사전에 설정한 임계값을 비교하여, 그 비교결과에 따라 상기 지식이 부족한지를 결정하고, 상기 지식이 부족한 것으로 결정된 경우, 상기 데이터베이스를 검색하여 상기 학습용 데이터를 획득하는 것인 지식 증강 장치."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제10항에서, 상기 자율 샘플링부는,상기 임베딩 벡터값을 쿼리값으로 이용하여 상기 데이터베이스로부터 부족한 지식에 대응하는 상기 학습용 데이터를 획득하는 것인 지식 증강 장치."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에서,상기 자율 샘플링부는,상기 지식 생성 신경망과 상기 대화 생성 신경망을 지도 학습으로 훈련시켜서 지식 증강을 수행하는 것인 지식증강 장치."}
{"patent_id": "10-2020-0065489", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10항에서,상기 인공지능 에이전트는 상기 지식과 상기 답변의 정확도를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 강화 학습으로 훈련시키기 위한 파라미터를 생성하도록 프로그래밍된 가치 추정 신경망을 더 포함하고,상기 자율 샘플링부는,상기 생성된 학습용 데이터를 기반으로 상기 가치 추정 신경망을 더 훈련시키는 단계인 것인 신경망의 불확실성공개특허 10-2021-0147771-5-에 기반한 지식 증강 방법."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 지식 증강 방법은 명시적 메모리를 사용하는 신경망으로부터 획득한 지식의 불확실성을 계산하고, 계 산된 불확실성을 기반으로 상기 지식의 부족함을 판단하고, 부족한 지식을 증강시키기 위해 추가 데이터(학습용 데이터)를 획득하여 상기 추가 데이터를 이용하여 상기 신경망을 학습시켜서 지식을 자율 성장시킨다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 신경망의 불확실성에 기반한 지식 증강 기술에 관한 것이다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 에이전트(agent)는 타인과의 상호 작용을 통해 지식을 자율적으로 성장시킬 수 있는 소프트웨어 모듈 (또는 프로그램) 또는 이러한 소프트웨어 모듈이 탑재된 하드웨어 모듈을 지칭하는 용어로 사용될 수 있으며, 일반적인 신경망 또는 심층 학습(deep learning) 기반의 신경망을 그 예로 들 수 있다. 심층 학습 기반의 신경망은 뇌의 뉴런 (neuron)과 유사한 입출력 계층을 활용해 데이터로부터 높은 수준의 추상 화를 시도하는 기계학습 알고리즘이다. 한편, 종래의 심층 학습 기반의 신경망은 지식의 부족함을 스스로 판단하고, 그 부족한 지식의 학습을 통해 지 식을 성장시키는 기능을 가지고 있지 않다. 따라서, 신경망과 같은 인공지능 에이전트가 부족한 지식을 스스로 학습하기 위해서는, 사용자가 부족한 지식에 대한 학습을 위한 데이터(예, 훈련 데이터 등)를 직접 구축해야 하는 데, 이러한 데이터 구축에는 많은 시간과 비용이 발생한다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 인공지능 에이전트가 부족한 지식을 자율적으로 판단하고, 그 판단된 부족한 지식을 학습하기 위한 추가 데이터를 자율적으로 샘플링(생성 또는 획득) 하여 샘플링된(생성된 또는 획득된) 그 추가 데이터(학습용 데이터)를 기반으로 부족한 지식을 학습함으로써 지식을 자율적으로 성장시키는 지식 증강 방법 및 그 장치를 제공하는 데 목적이 있다. 본 발명의 전술한 목적 및 그 이외의 목적과 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부된 도면과 함 께 상세하게 후술되어 있는 실시예들을 참조하면 명확해질 것이다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일면에 따른 지식 증강 방법은 사용자 질문에 대한 지식을 생성하기 위한 지식 생성 신경망과 상기 지식을 기반으로 사용자 질문에 대한 답변을 생성하는 대화 생성 신경망을 포함하도록 컴퓨팅 장치에 의해 프로 그래밍 되고 실행되는 인공지능 에이전트의 지식 증강 방법에서, 복합 모달 표현부에서, 현재의 사용자 질문, 상기 지식 생성 신경망에 의해 생성된 과거의 지식과 상기 대화 생성 신경망에 의해 생성된 과거의 답변을 특정 차원의 임베딩 벡터값으로 각각 변환하는 단계; 불확실성 계산부에서, 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 복수의 출력값을 산출하고, 상기 산출된 복수의 출력값을 기반으 로 상기 지식 생성 신경망의 불확실성을 계산하는 단계; 및 자율 샘플링부에서, 상기 계산된 불확실성을 기반으 로 데이터베이스를 검색하여 학습용 데이터를 획득하고, 상기 획득된 학습용 데이터를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 훈련시켜서 지식 증강을 수행하는 단계를 포함한다. 본 발명의 다른 일면에 따른 지식 증강 장치는, 현재의 사용자 질문, 상기 신경망에 의해 생성된 과거의 지식과 과거의 답변을 특정 차원의 임베딩 벡터값으로 각각 변환하는 데이터 전처리부; 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 복수의 출력값을 생성하고, 상기 생성된 복수의 출력 값을 기반으로 상기 지식 생성 신경망의 불확실성을 계산하는 불확실성 계산부; 및 상기 계산된 불확실성을 선 형 변환하여 보상값을 계산하고, 상기 임베딩 벡터값을 쿼리값을 이용하여 비지도 학습으로 구축된 데이터베이스로부터 상기 계산된 보상값을 갖는 학습용 데이터를 획득하고, 상기 획득된 학습용 데이터를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 훈련시켜서 지식 증강을 수행하는 자율 샘플링부를 포함한다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 신경망의 불확실성을 기반으로 부족한 지식을 파악하고 추가 데이터를 학습하여 지식을 성장시키는 것으로, 새로운 지식의 필요성을 자가 결정하여 그 새로운 지식을 자율적으로 학습하는 인공지능의 원천기술을 확보할 수 있다. 이를 토대로 법률, 의료, 교육, 패션 등 다양한 응용 도메인에서 스스로 학습하여 자율적으로 성장하는 고품질 지능형 서비스 시장 응용에서 활용 가치가 매우 높다."}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명의 다양한 실시 예가 첨부된 도면과 연관되어 기재된다. 본 발명의 다양한 실시 예는 다양한 변경 을 가할 수 있고 여러 가지 실시 예를 가질 수 있는 바, 특정 실시 예들이 도면에 예시되고 관련된 상세한 설명 이 기재되어 있다. 그러나, 이는 본 발명의 다양한 실시 예를 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 다양한 실시예의 사상 및 기술 범위에 포함되는 모든 변경 및/또는 균등물 내지 대체물을 포 함하는 것으로 이해되어야 한다. 도면의 설명과 관련하여, 유사한 구성요소에 대해서는 유사한 참조 부호가 사 용되었다. 본 발명의 다양한 실시 예에서 사용될 수 있는 \"포함한다\" 또는 \"포함할 수 있다\" 등의 표현은 개시 (disclosure)된 해당 기능, 동작 또는 구성요소 등의 존재를 가리키며, 추가적인 하나 이상의 기능, 동작 또는 구성요소 등을 제한하지 않는다. 또한, 본 발명의 다양한 실시 예에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것이 존재함을 지정하려는 것이 지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 발명에 따른 인공지능 에이전트는, 명시적 메모리(explicit memory)를 사용하는 신경망을 통해, 질문, 그 질 문에 대한 과거의 답변 및 과거의 지식을 기반으로 사용자의 요구 사항을 판단하고, 그 요구 사항에 적합한 지 식과 대화를 생성한다.또한, 본 발명에 따른 인공지능 에이전트는 상기 생성된 지식의 불확실성을 계산하여, 부족한 지식을 자율적으 로 판단하고, 판단된 부족한 지식을 학습하기 위한 추가 데이터를 자율적으로 샘플링(생성) 하여 샘플링된(생성 된) 추가 데이터를 기반으로 부족한 지식(모르는 지식)에 대한 학습을 수행함으로써, 지식을 자율 성장시킨다. 여기서, 샘플링(또는 데이터 샘플링)은 사전에 비지도 학습을 기반으로 구축된 지식 데이터베이스에서 관련 데 이터를 가져오거나, 강화 학습과 같이 새로운 탐색 공간을 탐험하는 데이터 처리 프로세스이다. 이처럼 본 발명에 따른 인공지능 에이전트는 부족한 지식을 학습하기 위한 추가 데이터를 자율적으로 샘플링(생 성)함으로써, 종래의 인공지능 에이전트와 같이, 부족한 지식에 대한 학습을 위한 추가 데이터(예, 훈련 데이터 등)를 사용자가 직접 구축하는 과정에서 발생하는 비용 및 시간을 크게 절감할 수 있다. 이하, 도면을 참조하여 본 발명에 따른 지식 증강 방법을 구현하기 위한 장치(컴퓨팅 장치, 컴퓨팅 시스템)의 일 예에 대해 상세히 기술한다. 도 1은 본 발명의 실시 예에 따른 지식 증강 방법을 실현하기 위한 장치의 블록도이다. 도 1을 참조하면, 본 발명의 실시 예에 따른 지식 증강 방법은 컴퓨팅 장치에 의해 구현되거나, 또는 특정 알고리즘으로 프로그래밍되어 기록 매체에 기록될 수 있다. 컴퓨팅 장치, 특정 알고리즘 및 상기 특정 알고리즘이 기록된 기록매체는 '인공지능 에이전트'라는 용어로 사용될 수 있다. 즉, 인공지능 에이전트는 소프트웨어 모듈, 하드웨어 모듈 및 이들의 조합 중 어느 하나로 구 현될 수 있다. 컴퓨팅 장치는 적어도 하나의 프로세서, 데이터 통신 버스, 메모리, 사용자 입력 장치 , 사용자 출력 장치, 저장소를 포함할 수 있다. 전술한 구성 요소들은 데이터 통신 버스를 통해 데이터 통신을 한다. 추가로 컴퓨팅 장치는 네트워크에 커플링 된 네트워크 인터페이스를 더 포함할 수 있다. 프로세서는 중앙처리 장치(central processing unit (CPU))이거나, 혹은 메모리 및/또는 저장소 에 저장된 명령어를 처리하는 반도체 장치일 수 있다. 프로세서는 본 발명에 따른 지식 증강과 관련된 데이터 처리를 수행한다. 프로세서는, 예를 들면, 지식의 불확실성을 계산하기 위한 데이터 처리, 부족한 지식을 자율적으로 판단하 기 위한 데이터 처리, 상기 판단된 부족한 지식을 학습하기 위한 추가 데이터를 자율적으로 샘플링(생성) 하기 위한 데이터 처리, 상기 샘플링된(생성된) 추가 데이터를 기반으로 부족한 지식에 대한 학습을 수행하기 위한 데이터 처리 등을 수행한다. 그 밖에 프로세서는 중간 데이터 처리 과정에서 생성된 중간 데이터(intermediate data) 또는 최종 데이터 처리 과정에서 생성된 결과 데이터를 생성하고, 이를 데이터 통신 버스를 통해 다른 구성 요소들(123, 126, 127, 128 및 129) 중에서 적어도 하나로 전달하는 역할을 한다. 프로세서에 의해 생성된 중간 데이터는, 예를 들면, 기계 학습 파이프 라인을 구성하는 모든 스테이지들 각각의 출력값일 수 있다. 메모리 및 저장소는 다양한 형태의 휘발성 메모리(volatile memory) 혹은 비휘발성 메모리(non- volatile memory, non-volatile storage)를 포함할 수 있다. 메모리는, 예를 들면, ROM(124, Read Only Memory) 및 RAM(125, Random Access Memory)을 포함할 수 있다. 저장소는 프로세서에 의해 프로그래밍 되고 실행되는 본 발명에 따른 인공지능 에이전트를 저장 한다. 사용자 입력 장치는 지식 증강을 위한 사용자 입력값 또는 사용자 입력 데이터 등을 입력하기 위한 키보드, 마우스 등일 수 있으며, 사용자 입력값 또는 사용자 입력 데이터가 음성 데이터인 경우, 사용자 입력 장치는 마이크 등일 수 있다. 사용자 출력 장치는 프로세서에 의해 생성된 중간 데이터 및/또는 결과 데이터를 출력하는 것으로, 표시 장치일 수 있으며, 중간 데이터 및/또는 결과 데이터를 음성 데이터 형태로 출력하는 경우, 사용자 출력 장치는 스피커일 수 있다.네트워크 인터페이스는 컴퓨팅 장치를 네트워크에 연결하기 위한 통신 기능을 지원하며, 통신 기능은 유선 통신 기능과 무선 통신 기능을 포함한다. 무선 통신 기능은 3G 통신, 4G 통신, 5G 통신, 이동 통신 또는 무선 인터넷 통신 등을 지원하는 기능일 수 있다. 본 발명에 따른 신경망의 불확실성에 기반한 지식 증강 방법은 프로세서에서 실행 가능한 방법으로 구현될 수 있으며, 프로세서로 판독 가능한 명령어들이 본 발명에 따른 신경망의 불확실성에 기반한 지식 증강 방 법을 처리할 수 있다. 본 발명에 따른 신경망의 불확실성에 기반한 지식 증강 방법은 기록 매체에 컴퓨터가 읽을 수 있는 코드로서 구 현될 수 있다. 컴퓨터로 판독 가능한 기록 매체는 컴퓨터에 의하여 해독될 수 있는 데이터가 저장된 모든 종류의 기록 매체를 포함한다. 예를 들어, 컴퓨터로 판독 가능한 기록 매체는 ROM, RAM, 자기 테이프, 자기 디스크, 플래시 메모리, 광 데이터 저장장치 등이 있을 수 있다. 이러한 컴퓨터로 판독 가능한 기록 매체는 도 1에 도시한 저장소일 수 있으며, 이 경우, 저장소는 컴 퓨팅 장치로부터 자유롭게 분리되거나 장착될 수 있는 것일 수 있으며, 예를 들면, USB 메모리, 외장형 하 드 디스크일 수 있다. 또한, 컴퓨터로 판독 가능한 기록 매체는 컴퓨터 통신망으로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 읽을 수 있는 코드로서 저장되고 실행될 수 있다. 이하, 본 발명의 불확실성에 기반한 지식 증강 방법을 구현하기 위해, 도 1에 도시한 프로세서에 의해 실 행되는 소프트웨어 모듈들 또는 하드웨어 모듈들을 도 2를 참조하여 상세히 설명하기로 한다. 참고로, 지식 증강 방법을 실현하기 위한 소프트웨어 모듈들은 저장소에 저장되고, 프로세서의 호출 명령에 따라 호출되어, 메모리에서 로딩될 수 있다. 이 경우, 메모리는 프로세서에 호출된 소프 트웨어 모듈들의 실행 공간을 제공하는 역할을 한다. 도 2는 본 발명의 실시 예에 따른 지식 증강 방법을 구현하기 위해 프로그래밍된 인공지능 에이전트의 블록도이 다. 도 2를 참조하면, 컴퓨팅 장치 또는 프로세서에 의해 프로그래밍 되고 실행되는 인공지능 에이전트 는 정책망(Policy network, 210)과 가치망(Value Network, 220)으로 이루어진 2개의 인공 신경망들 (artificial neural networks)을 포함하도록 구성될 수 있다. 여기서, 정책망은 '정책 신경망'으로 불릴 수 있고, 가치망은 '가치 신경망'으로 불릴 수 있다. 정책망(Policy network, 210)과 가치망(Value Network, 220) 각각은 다수의 모듈을 포함하도록 구성될 수 있으 며, 각 모듈은 신경망으로 프로그래밍된 소프트웨어 모듈, 상기 소프트웨어 모듈을 실행하는 하드웨어 모듈 또 는 이들의 조합으로 이루어질 수 있다. 정책망(Policy network, 210)과 가치망(Value Network, 220)은 서로 협력하여, 부족한 지식을 자율적으로 판단 하고, 그 부족한 지식에 대한 추가 데이터(학습용 데이터)를 이용하여 지식을 성장시킨다. 본 명세서에서는, 패션 코디네이션(fashion coordination)과 관련된 지식(이하, '패션 코디네이션 지식'이라 함)을 성장시키는 지식 증강 방법에 대해 설명하기로 한다. 이러한 패션 코디네이션 지식은 시간(Time), 장소(Place) 또는 행사(Occasion)에 따라 사용자의 요구 사항이 반 영된 패션 아이템의 조합을 생성하는 지식으로 정의한다. 패션 아이템의 종류는 착용 위치에 따라 겉옷, 웃옷, 아래옷, 신발과 같은 항목으로 분류할 수 있다. 도 2에 도시된 바와 같이, 정책망은 복합 모달 표현부, 지식 생성부, 대화 생성부, 불확실 성 계산부 및 자율 샘플링부를 포함하도록 구성된다. 정책망에 포함된 상기 구성 요소들(211, 213, 215, 217, 219)은 신경망으로 프로그래밍된 소프트웨어 모듈, 상기 소프트웨어 모듈을 실행하는 하드웨어 모듈 또는 이들의 조합으로 구현될 수 있다. 가치망은 복합 모달 표현부 및 가치 추정부를 포함하도록 구성될 수 있다. 이때, 가치망에 포함된 구성요소들(221, 223)은 신경망으로 프로그래밍된 소프트웨어 모듈, 상기 소프트웨어 모듈을 실행하는 하드웨어 모듈 또는 이들의 조합으로 구현될 수 있다. 본 발명의 인공지능 에이전트는 역전파(back propagation) 알고리즘을 기반으로 단대단(end-to-end) 학습을 수 행한다. 역전파(back propagation) 알고리즘을 기반으로 단대단(end-to-end) 학습을 수행하기 위해, 복합 모달 표현부 (211, 221)를 제외한 구성 요소들(213, 215, 217, 219 및 223)은 미분 가능한 수학식으로 표현될 수 있다. 단일 계층의 신경망은 수학식 y=f(Ax+b)로 표현될 수 있다. 여기서 x는 입력, y는 출력, f는 비선형함수, A와 b 는 학습 대상을 나타내는 변수이다. 복수 계층의 신경망은 단일 계층의 신경망을 연속적으로 연결한 것이다. 예를 들면, 2 계층의 신경망을 수학식 으로 표현하면, y' = f'(A'y+b') = f'(A' f(Ax+b)+b')와 같다. 이때, 신경망으로 표현된 구성 요소를 역전파 알고리즘을 이용하여 학습시키기 위해서는, 신경망을 나타내는 수 학식의 미분값이 필요하다. 따라서, 구성 요소들(213, 215, 217, 219 및 223)은 미분 가능한 수학식으로 표현될 필요가 있다. 복합 모달 표현부(데이터 전처리부(data preprocessing unit)) 복합 모달 표현부는 사용자 질문 데이터(10, 이하, '질문'이라 함), 상기 질문과 관련하여 지식 생성 부가 과거에 생성한 지식 데이터(13, 이하, '과거에 생성한 지식') 및 상기 과거에 생성한 지식을 기반으로 대화 생성부가 과거에 생성한 답변 데이터(11, 이하, '과거에 생성된 답변'이라 함)를 특정 차원의 벡터 데이터로 각각 변환하는 신경망(또는 신경망 모델)일 수 있다. 여기서 벡터 데이터는 수치화된 값(또는 실수 형 태의 벡터값)일 수 있다. 질문은 언어, 이미지 또는 이들의 조합을 포함하는 데이터로서, 사용자 입력 장치(도 1의 126)로부터 입력 된다. 상기 과거에 생성된 답변은 대화 생성부의 출력으로부터 피드백된다. 상기 과거에 생성된 지식은 지식 생성부의 출력으로부터 입력된다(또는 피드백 된다). '질문', '과거에 생성된 답변' 및 '과거에 생성된 지식'을 특정 차원의 벡터로 변환하기 위해, 임 베딩 알고리즘 또는 단어 임베딩(word embedding) 알고리즘이 이용될 수 있다. 단어 임베딩(word embedding) 알고리즘은 텍스트를 구성하는 단어를 수치화하는 방법 중의 하나로, Word2Vec 또 는 페이스북의 fastText 등이 대표적인 예이다. '질문', '과거에 생성된 답변' 및 '과거에 생성된 지식' 각각에 포함된 단어는 단어 임베딩(word embedding) 알고리즘에 의한 처리 과정(또는 데이터 전처리 과정)에 따라 실수 형태의 벡터값으로 변환될 수 있 다. 실수 형태의 벡터값은 언어 임베딩 벡터(또는 언어 임베딩 벡터값)으로 지칭될 수 있다. 복합 모달 표현부는 각 단어에 대한 벡터값을 합치거나(concatenation), 평균화(averaging, normalized Bag of Words)하여, 언어 임베딩 벡터(또는 언어 임베딩 벡터값)를 산출(출력)한다. 복합 모달 표현부는 사전에 구축된 DeepFashion 데이터베이스에 대해 훈련된 신경망을 이용하여 패션 아이 템의 이미지 속성을 추출한다. 복합 모달 표현부는 추출된 이미지 속성에 대해 언어 임베딩 벡터를 산출하고, 산출된 언어 임베딩 벡터를 이미지 임베딩 벡터(또는 이미지 임베딩 벡터값)로서 획득할 수 있다. 패션 아이템의 이미지 속성은, 예를 들면, 형태(형상) 특징, 소재 특징, 색채 특징 및 감성 특징 중에서 적어도 하나의 특징을 포함할 수 있다. 이하에서 기술되는 임베딩 벡터는 언어 임베딩 벡터 및 이미지 임베딩 벡터 중 적어도 하나를 포함하는 용어로 사용될 수 있다.도 3에는 복합 모달 표현부로 입력되는 패션 코디네이션 지식과 관련된 질문, 과거에 생성된 답변 및 과거에 생성된 지식의 일 예가 도시된다. 도 3의 과거에 생성된 답변은 정보가 부족하다고 판단한 인공지능 에이전트(또는 대화 생성부)가 추가 적인 정보를 요구하는 질문형 답변이다. 도 4에는 도3의 질문, 과거에 생성된 답변 및 과거에 생성된 지식 각각에 대해 임베딩을 수행하여 획득한 임베딩 벡터들(40, 41 및 43)의 일 예가 도시된다. 지식 생성부(213: 지식 생성 신경망) 지식 생성부는 복합 모달 표현부로부터 입력된 임베딩 벡터(또는 임베딩 벡터값)을 입력으로 사용하 여 지식을 생성하는 신경망(또는 신경망 모델)이다. 지식 생성부는, 예를 들면, 명시적 메모리(explicit memory)를 사용하는 신경망으로 이루어지며, 사용자의 새로운 요구사항과 TPO(Time, Place, Occasion)에 적합한 패션 코디네이션 지식을 생성한다. 명시적 메모리는 장기 메모리(Long-Term Memory)와 단기 메모리(Short-term memory)를 포함한다. 장기 메모리 와 단기 메모리는 기계학습 분야에서 널리 사용되는 기술적 용어로 이에 대한 설명은 생략한다. 패션 코디네이션 지식의 예는, 도 5에 도시한 바와 같이, 밝은색 주름 치마가 포함된 이미지를 포함하도록 구성될 수 있다. 도 6은 본 발명의 실시 예에 따른 지식 생성부의 내부 구성을 나타내는 블록도이다. 도 6을 참조하면, 지식 생성부는, 패션 코디네이션 지식을 생성하기 위해, 사용자 요구 추정부(213_1), 지 식 탐색부(213_3), 지식 평가부(213_5) 및 지식 결정부(213_7)을 포함하도록 구성될 수 있다. 사용자 요구 추정부(213_1)는 명시적 메모리(external memory)를 사용하는 신경망 구조로 이루어지며, 이러한 신경망 구조를 기반으로 복합 모달 표현부로부터 입력되는 임베딩 벡터에 대해 사용자 요구사항을 나타내 는 벡터를 추정한다. 사용자 요구 추정부(213_1)는, 예를 들면, 과거의 사용자 요구 및 밝은 색의 주름 치마 스타일(도 5의 50)을 요 구하는 추가적인(새로운) 사용자 요구를 포함하도록 구성된 벡터를 추정한다. 지식 탐색부(213_3)는 사용자 요구 추정부(213_1)로부터 입력된 벡터에 적합한 지식을 탐색한다. 지탐색을 위해, 지식 탐색부(213_3)는, 예를 들면, 장기 메모리(Long-Term Memory: LTM)를 활용한 신경망 구조 를 가질 수 있다. 즉, 지식 탐색부(213_3)는, 장기 메모리(Long-Term Memory: LTM)를 이용하여 사용자 요구를 나타내는 벡터(이하, '사용자 요구 벡터'라 함)에 대응하는(적합한) 새로운 지식을 탐색한다. 장기 메모리(Long-Term Memory: LTM)에는 사전에 구축된 선언적 지식(declarative knowledge)의 임베딩 벡터가 저장된다. 예컨대, 장기 메모리(Long-Term Memory: LTM)에는 패션 아이템들의 메타 데이터가 저장된다. 그리고 지식 탐색부(213_3)는 매칭망(matching network)을 이용하여 장기 메모리에 저장된 패션 아이템들의 확 률값을 산출한다. 여기서, 상기 패션 아이템들의 확률 값은 특정 지식과 사용자 요구 사항이 매칭될 확률을 의미한다. 지식 탐색부(213_3)는, 매칭망을 이용하여 사용자 요구 벡터(사용자 요구 사항)과 지식들 간의 유사도를 구한 후, 소프트맥스(softmax) 함수에 상기 유사도를 적용하여 상기 패션 아이템들의 확률 값을 산출할 수 있다. 이처럼 매칭망을 사용하는 점에서 지식 탐색부(213_3)는 매칭망(matching network) 알고리즘 기반의 신경망 모 델일 수 있다. 지식 평가부(213_5)는 사용자 요구 추정부(213_1)로부터의 사용자 요구 벡터와 지식 탐색부(213_3)로부터의 상 기 탐색된 새로운 지식(과거에 생성된 지식을 포함)을 입력으로 하여, 상기 탐색된 새로운 지식을 평가하는 신 경망이다. 이러한 지식 평가부(213_5)는, 예를 들면, 사용자가 탐색하고자 하는 패션 아이템을 교체하여 새롭게 구성된 패 션 코디네이션 지식이 새로운 사용자 요구가 포함된 사용자 요구 벡터에 얼마나 부합하는 지를 평가한다. 이러한 평가를 위해, 지식 평가부(213_5)는 관계망(Relation Networks: RN) 알고리즘 기반의 신경망 구조로 이 루어질 수 있다. 관계망은 객체 간의 관계를 추론하는 신경망 모델로서, \"A simple neural network module for relational reasoning\"이란 제목의 논문에서 상세히 소개하고 있다. 이에, 새롭게 구성된 패션 코디네이션 지식이 사용자 요구 벡터에 얼마나 부합하는 지를 평가하는 방법에 대해서는 생략하기로 한다. 지식 결정부(213_7)는 상기 지식 탐색부(213_3)에서 탐색한 지식과 상기 지식 평가부(213_5)에서 평가한 평가 결과를 이용하여 지식을 결정하는 신경망 모델일 수 있다. 지식 결정부(213_7)는 상기 지식 탐색부(213_3)로부터 입력된 패션 아이템들의 확률값과 상기 지식 평가부 (213_5)로부터 입력된 패션 코디네이션의 평가 결과치를 곱하여 계산된 결과들 중에서 최대값에 대응하는 패션 아이템을 패션 코디네이션 지식으로 결정한다. 예를 들면, 패션 아이템들의 변수를 'x'라하고, 지식 탐색부(213_3)가 사용자의 요구 사항에 적합하게 탐색한 패션 아이템들의 확률 값을 'p1(x)'라 하고, 지식 평가부(213_5)가 계산한 패션 코디네이션의 평가 결과치를 'p2(x)'라 할 때, 패션 코디네이션 지식의 결정은 'p1(x) * p2(x)' 최대가 되는 x를 선택하는 과정일 수 있다. 대화 생성부(215: 대화 생성 신경망) 다시 도 2를 참조하면, 대화 생성부는 지식 생성부로부터 입력된 지식과 복합 모달 표현부로부 터 입력된 임베딩 벡터를 이용하여, 지식을 구성하기 위한 추가적인 정보를 요청하거나 새로운 지식을 설명하는 답변을 생성하는 신경망(신경망 모델)이다. 이러한 대화 생성부는 도 5와 같이 패션 코디네이션 지식을 설명하는 답변을 생성한다. 대화 생성부는, 예를 들면, LSTM(Long Short-Term Memory) 순환 신경망 또는 자기 집중(self-attention) 기반의 시퀀스-투-시퀀스(sequence-to-sequence) 신경망일 수 있다. 가치 추정부(223: 가치 추정 신경망) 가치망(220, value network)에 포함된 가치 추정부는 지식 생성부에서 생성한 지식, 대화 생성부 에서 생성한 답변(또는 대화), 복합 모달 표현부로부터 입력된 임베딩 벡터값을 사용하여 현재 상태 의 가치 파라미터를 추정한다. 여기서, 현재 상태는 정책망의 출력 상태, 즉, 대화 생성부로부터 출력되는 답변과 지식 생성부로부 터 출력되는 지식을 의미한다. 가치(value)는 지식과 답변에 대한 정확성과 사용자 만족도를 의미한다. 가치는 상기 지식 생성부와 상기 대화 생성부를 강화 학습으로 훈련시키기 위한 보상(파라미터)로 사 용된다. 대표적인 훈련 방법으로 액터-크리틱(actor-critic) 알고리즘이 이용될 수 있다. 가치망에 포함된 복합 모달 표현부는 정책망에 포함된 복합 모달 표현부와 동일한 구성 및 기능 을 갖는다. 불확실성 계산부 불확실성 계산부는 복합 모달 표현부로부터 입력된 임베딩 벡터(질문), 과거에 생성된 답변 및 과거 에 생성된 지식 중에서 적어도 하나를 입력으로 사용하여, 지식 생성 모델(지식 생성부)의 출력에 대한 불 확실성(uncertainty)을 계산하는 신경망(또는 신경망 모델)일 수 있다. 불확실성은 확률 변수의 무작위성(randomness)을 측정한 스칼라 값이다. 불확실성은 상기 지식 생성 모델(지식 생성부를 구성하는 신경망)의 불확실성과 관측(observation)에 내재 되어 있는 불확실성을 포함한다. 지식 생성 모델(지식 생성부)의 불확실성은 데이터의 모델링 과정에서 발생하는 오차와 관련된 불확실성을 의미한다. 즉, 지식 생성 모델(지식 생성부)의 불확실성은 완벽하지 않은 모델에 의해 발생하는 불확실성을 의미한다. 이러한 지식 생성 모델(지식 생성부)의 불확실성은 기계학습 분야에서 '인식론적 불확실성 (epistemic uncertainty)'이라 불리기도 한다. 관측에 내재되어 있는 불확실성은 센서 잡음과 모션 잡음 등과 같이 데이터 내의 잡음(noise)으로 인해 발생하 는 불확실성을 의미한다. 이러한 관측(observation)에 내재되어 있는 불확실성은 기계학습 분야에서 '물리적 불확실성(aleatoric uncertainty)'이라 불리기도 한다. 도 7은 도 1에 도시한 불확실성 계산부에 의한 불확실성을 계산하기 위해 사용되는 드롭 아웃 기법을 설명하기 위한 도면이다. 도 7을 참조하면, 불확실성 계산부은 드롭아웃(dropout)이 적용된 지식 생성 모델(지식 생성부)의 출 력들에 대한 평균과 분산을 기반으로 불확실성(값)을 계산한다. 드롭아웃 기법은 신경망 계층에 포함된 입력층(input layer, 81) 또는 중간층(hidden layer, 83)의 일부 뉴런들 을 임의로 생략(dropout)하고, 일부 뉴런들이 생략된 신경망을 이용하여 학습을 수행하는 기법이다. 도 7의 좌측에는 드롭아웃 기법의 적용전의 신경망 구조가 도시되고, 도 7의 우측에는 드롭아웃 기법을 적용 후 의 신경망 구조가 도시된다. 도 7의 우측에 도시된 바와 같이, 드롭아웃 기법이 적용된 신경망 계층에서는, 입력층 또는 중간층에 포함된 일부 뉴런들(일부 노드들)과, 그 일부 뉴런들(일부 노드들)의 출력을 다른 뉴런들로 전달하는 연결이 생 략(삭제)된다. 도 7의 우측에 도시된 신경망 계층에서 'X'가 표시된 원들은 생략된 뉴런들을 의미한다. 이처럼 입력층 또는 중간층의 일부 뉴런들을 다양한 조합으로 생략(삭제)하면, 출력층을 통해 조합의 수에 대응하는 복수의 최종 출력값을 획득할 수 있다. 예를 들면, 중간층의 출력이 10개라면, 1번째 뉴런(노드)과 5번째 뉴런을 생략(삭제)하여 하나의 최종 출력 값이 출력층으로부터 획득되고, 2번째 뉴런(노드)과 4번째 뉴런(노드)을 생략(삭제)하여 다른 하나의 최종 출력값이 출력층으로부터 획득되고, 8번과 9번에 대응하는 뉴런 연결을 생략(삭제)하여 또 다른 하나의 최 종 출력값이 출력층으로부터 획득될 수 있다. 이러한 방식으로 출력층을 통해 복수의 최종 출력값을 획 득할 수 있다. 불확실성 계산부은 드롭아웃이 적용된 신경망 계층으로부터 획득된 복수의 최종 출력값들에 대한 평균 ( ) 및 분산( )을 계산하고, 그 계산 결과치를 이용하여 상기 지식 생성 모델의 불확실성으로 계산 한다. 불확실성 계산부의 계산과정을 수학적으로 설명하면 다음과 같다. 입력층과 관련된 입력 에 대하여 t번째 드롭 아웃을 적용한 신경망은 평균( )과 분산( )을 출력으로 산출한다. 상기 지식 생성 모델(지식 생성부)의 불확실성(epistemic uncertainty: model_uncertainty)은 아래의 수학식 1 에 의해 계산될 수 있다.수학식 1"}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, T는 드롭 아웃이 적용된 신경망에서 일부 뉴런들을 삭제하는 방법의 총 개수이다. 예를 들면, T는 신경 망 계층에서 중간층에 포함된 일부 뉴런들의 연결을 삭제하는 모든 조합의 수이다. 한편, 불확실성 계산부에 의해 계산되는 불확실성들 중에서 관측에 내재되어 있는 불확실성(aleatoric uncertainty: observation_uncertainty)은 드롭 아웃 기법이 적용된 신경망에서 출력되는 분산을 사용하여 계 산될 수 있다. 입력층과 관련된 입력 에 대하여 번째 드롭 아웃을 적용한 신경망의 분산(또는 표준편차)을 라 하 면, 상기 관측에 내재되어 있는 불확실성(aleatoric uncertainty)은 아래의 수학식 2에 의해 계산될 수 있다. 수학식 2"}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "출력의 부정확함을 알려 주는 불확실성(total_uncertainty)은 상기 수학식 1에 따라 계산된 불확실성과 상기 수 학식 2에따라 계산된 불확실성의 합으로서, 아래의 수학식 3에 의해 계산될 수 있다. 수학식 3"}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이외에도 파티클 필터(particle filter)와 CRF(Conditional Random Fields) 기법을 사용하여 모델 불확실성을 계산할 수도 있다. 자율 샘플링부 다시 도 2를 참조하면, 자율 샘플링부는 전술한 불확실성 계산부에 의해 계산된 불확실성 값을 사용 하여 지식의 부족함을 자가 결정한 후, 새로운 데이터를 샘플링 하는 신경망(또는 신경망 모델)이다. 예를 들면, 자율 샘플링부는 불확실성값이 사전에 설정한 임계값보다 큰 경우, 지식의 부족함을 결정한다. 이것은 사용자가 인공지능 에이전트에게 특정 모임에 어울리는 옷을 요청했는데, 이러한 요청에 대한 패션 코디네이션 지식이 없음을 의미한다.자율 샘플링부는 부족한 지식(모르는 지식)을 파악하면, 사용자 요구 벡터와 관련 정보를 사전에 비지도 학습으로 구축된 데이터베이스에서 샘플링을 하여 학습용 데이터를 생성한다. 상기 생성된 학습용 데이터는 지식 생성부, 대화 생성부, 가치 추정부를 학습시키기 위한 데이 터로 이용되며, 이러한 학습에 의해 지식의 자율적인 성장이 가능해진다. 이때, 학습은 지도 학습일 수 있다. 또 다른 예로, 알파고(AlphaGo), 아타리(Atari) 게임처럼 강화 학습에서 새로운 탐색 공간을 탐험하는 방식으로 데이터 샘플링을 수행할 수 있다. 이처럼 강화 학습을 기반으로 데이터를 샘플링하는 경우, 자율 샘플링부는, 불확실성이 임계값보다 크면, 현재 상태에 대해 행위의 모름을 결정하고, 다른 상태에 대한 행위의 결과를 알기 위해 새로운 탐색 공간을 탐 험한다. 이처럼 자율 샘플링부는 사용자의 피드백에 의한 외부 보상값 외에 불확실성 계산부에 의해 계산된 불확실성값을 선형 변환하여, 내부 보상값을 계산한다. 상기 임베딩 벡터는 비지도 학습으로 구축된 데이터베이스의 쿼리값으로 사용될 수 있다. 상기 임베딩 벡 터를 쿼리값으로 사용하기 위해, 자율 샘플링부는 상기 데이터베이스의 스키마 구조에 따라 상기 내 부 임베딩 벡터를 상기 쿼리값으로 변환하는 처리 과정을 수행할 수 있다. 이처럼 자율 샘플링부는 상기 불확실성값을 기반으로 상기 임베딩 벡터를 쿼리값으로 이용하여 새로운 공 간(예를 들면, 비지도 학습으로 구축된 데이터베이스)에 대한 탐험(검색)을 수행할 수 있게 된다. 자율 샘플링부는, 예를 들면, 상태를 st라 하고, 행위를 at라 할 때, 수학식 3에 의해 계산된 불확실성값을 선형 변환하여 내부 보상값 을 계산한다. 내부 보상값 은 아래의 수학식 4에 의해 계산될 수 있다. 수학식 4"}
{"patent_id": "10-2020-0065489", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기서 와 는 학습으로 훈련되는 파라미터이다. 이러한 내부 보상값은 새로운 탐색 공간을 탐험하기 위한 값으로 이용된다. 예를 들면, 강화 학습에서는, 큰 내부 보상값에 대해 먼저 새로운 탐색 공간을 탐험한다. 비지도 학습에서는, 큰 내부 보상값을 갖는 데이터에 대해 먼저 샘플링을 수행하거나 많은 샘플링을 수행하게 된다. 즉, 비지도 학 습에서는 상기 임베딩 벡터값을 쿼리값으로 이용하여 상기 계산된 보상값을 갖는 학습용 데이터를 상기 데이터 베이스로부터 획득한다. 도 8은 본 발명의 컴퓨팅 장치에 의해 수행되는 인공지능 에이전트의 지식 증강 방법을 설명하기 위한 흐름도이 다. 도 8을 참조하면, 본 발명의 인공지능 에이전트의 지식 증강 방법은, 예를 들면, 도 1에 도시한 컴퓨팅 장치에 의해 수행된다. 인공지능 에이전트의 지식 증강을 수행하기 위해, 컴퓨팅 장치는 프로세서, 메모리 및 저 장소를 포함한다. 인공지능 에이전트는 사용자 질문에 대한 지식을 생성하기 위한 지식 생성 신경망과 상기 지식을 기반으로 사용자 질문에 대한 답변을 생성하는 대화 생성 신경망을 포함하도록 프로그래밍된 프로그램일 수 있으며, 이러 한 인공지능 에이전트는 저장소에 저장된다. 지식 생성 신경망은 도 2에 도시한 지식 생성부이고, 대화 생성 신경망은 도 2에 도시한 대화 생성부(21 5)를 지칭한다. 프로세서는 상기 저장소에 저장된 인공지능 에이전트를 호출하여 메모리에 로딩하고, 메모리에 로딩된 인공지능 에이전트를 실행한다. 지식 증강을 위해, 인공지능 에이전트는 복합 모달 표현부, 불확실성 계산부 및 자율 샘플링부 를 더 포함하도록 프로그래밍 된다. 복합 모달 표현부는 '데이터 전처리부'라는 용어로 대체될 수 있 다. 지식 증강 방법을 설명하기 위한 아래의 각 단계에서 도 1 내지 7을 참조하여 설명한 내용과 중복되는 내용은 간략히 기술하거나 생략하기로 한다. 먼저, 단계 810에서, 복합 모달 표현부에 의해, 현재의 사용자 질문, 상기 지식 생성 신경망에 의해 생성 된 과거의 지식과 상기 대화 생성 신경망에 의해 생성된 과거의 답변을 특정 차원의 임베딩 벡터값으로 각각 변 환하는 과정이 수행된다. 현재의 사용자 질문은 언어, 이미지 또는 이들의 조합을 포함하는 데이터일 수 있다. 현재의 사용자 질문, 과거 의 지식 및 과거의 답변을 특정 차원의 임베딩 벡터값으로 변환하기 위해, 임베딩 알고리즘 또는 단어 임베딩 알고리즘이 이용될 수 있다. 이어, 단계 820에서, 불확실성 계산부에 의해, 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 복수의 출력값을 산출(획득)하고, 상기 산출(획득)된 복수의 출력값을 기반으로 상 기 지식 생성 신경망의 불확실성을 계산하는 과정이 수행된다. 여기서, 상기 지식 생성 신경망의 불확실성은 데이터의 모델링 과정에서 발생하는 불확실성(epistemic uncertainty)과 상기 데이터 내의 잡음으로 인해 발생하는 불확실성(aleatoric uncertainty)을 포함한다. 이어, 단계 830에서, 자율 샘플링부에 의해, 상기 계산된 불확실성을 기반으로 데이터베이스를 검색하여 학습용 데이터를 획득하고, 상기 획득된 학습용 데이터를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신 경망을 훈련시켜서 지식 증강을 수행한다. 여기서, 상기 데이터베이스는 비지도 학습으로 구축된 데이터베이스 일 수 있다. 단계 830은, 상기 생성된 학습용 데이터를 기반으로 상기 가치 추정 신경망을 훈련시키는 과정을 더 포함 할 수 있다. 가치 추정 신경망은 상기 지식 생성 신경망에 의해 생성된 지식과 상기 대화 생성 신경망에 의해 생성된 답변의 정확도를 기반으로 상기 지식 생성 신경망과 상기 대화 생성 신경망을 강화 학습으로 훈련 시키기 위한 파라미터를 생성한다. 도 9는 도 8에 도시한 단계 820의 세부 과정의 일 예를 설명하는 흐름도이다. 도 9를 참조하면, 불확실성을 계산하기 위해, 먼저, 단계 821에서, 상기 드롭아웃 기법을 기반으로 상기 지식 생성 신경망에 포함된 일부 뉴런들을 삭제하는 과정이 수행된다. 여기서, 삭제되는 일부 뉴런들은 상기 지식 생 성 신경망의 입력층 또는 중간층에 포함된 일부 뉴런들일 수 있다. 이어, 단계 822에서, 상기 일부 뉴런들을 삭제하는 방법의 개수에 대응하는 상기 복수의 출력값을 상기 지식 생 성 신경망으로부터 획득하는 과정이 수행된다. 이어, 단계 823에서, 상기 복수의 출력값에 대한 평균과 분산을 기반으로 상기 불확실성을 계산하는 과정이 수 행된다. 이러한 불확실성의 계산은 전술한 수학식 1 내지 3에 의해 계산될 수 있다. 도 10은 도 8에 도시한 단계 820의 세부 과정의 다른 예를 설명하는 흐름도이다. 도 10을 참조하면, 먼저, 단계 824에서, 상기 드롭아웃 기법을 적용한 상기 지식 생성 신경망을 이용하여 상기 임베딩 벡터값에 대한 상기 복수의 출력값에 대한 평균을 계산하는 과정이 수행된다. 이어, 단계 825에서, 상기 계산된 평균을 기반으로 데이터의 모델링 과정에서 발생하는 제1 불확실성(epistemic uncertainty)을 계산하는 과정이 수행된다. 여기서, 제1 불확실성(epistemic uncertainty)은 전술한 수학식 1 에 의해 계산될 수 있다. 이어, 단계 826에서, 상기 복수의 출력값에 대한 분산을 계산하는 과정이 수행된다. 이어, 단계 827에서, 상기 계산된 분산을 기반으로 상기 데이터 내의 잡음으로 인해 발생하는 제2 불확실성 (aleatoric uncertainty)을 계산하는 과정이 수행된다. 여기서, 제2 불 확실성(aleatoric uncertainty)은 전술 한 수학식 2에 의해 계산될 수 있다. 이어, 단계 828에서, 상기 제1 불확실성과 상기 제2 불확실성을 합산하여, 상기 지식 생성 신경망의 불확실성 (total_uncertainty)을 계산하는 과정이 수행된다. 여기서, 상기 지식 생성 신경망의 불확실성 (total_uncertainty)은 전술한 수학식 3에 의해 계산될 수 있다. 도 11은 도 8에 도시한 단계 830의 세부 과정을 설명하는 흐름도이다. 도 11을 참조하면, 단계 831에서, 상기 계산된 불확실성을 기반으로 상기 임베딩 벡터값에 대응하는 사용자이 요구사항에 대한 상기 지식이 부족한지를 결정하는 과정이 수행된다. 단계 832에서, 상기 지식이 부족한 것으로 결정된 경우, 상기 계산된 불확실성을 선형 변환하여 보상값을 계산 하는 과정이 수행된다. 여기서, 보상값을 전술한 수학식 4에 의해 계산될 수 있다. 단계 833에서, 상기 임베딩 벡터를 쿼리값으로 이용하여 상기 보상값을 갖는 학습용 데이터를 상기 데이터베이 스로부터 획득한다. 여기서, 단계 833은 상기 임베딩 벡터를 데이터 베이스의 스키마 구조에 따라 쿼리값으로 변환하는 과정을 더 포함할 수 있다. 이처럼 획득된 학습용 데이터는 지식 생성 신경망, 대화 생성 신경망 및 가치 추정 신경망을 훈 련시키기 위한 데이터로 활용되고, 이로 인해 지식의 자율적 성장이 가능해진다. 이상 설명한 바와 같이, 본 발명은 신경망의 불확실성을 기반으로 모르는 지식을 파악하고 추가 데이터를 학습 함으로써 지식을 성장시키는 것으로, 다양한 목표를 요구하는 인간의 피드백을 받아 새로운 지식의 필요성을 자 가 결정하여 지식을 자율 성장하는 인공지능의 원천기술을 확보하고, 이를 토대로 법률, 의료, 교육, 패션 등 다양한 응용 도메인에서 스스로 학습하여 성장이 필요한 고품질 지능형 서비스 시장 응용에 기여할 것으로 보인 다. 이제까지 본 발명을 실시예들을 중심으로 살펴보았다. 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자 는 본 발명이 본 발명의 본질적인 특성에서 벗어나지 않는 범위에서 다양하게 변경 또는 변형된 형태로 구현될 수 있음을 이해할 수 있을 것이다. 그러므로 개시된 실시예들은 한정적인 관점이 아니라 설명을 위한 예시적인 관점에서 고려되어야 한다. 본 발명의 범위는 전술한 설명이 아니라 청구범위에 나타나 있으며, 그와 동등한 범 위 내에 있는 모든 차이점은 본 발명에 포함된 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11"}
{"patent_id": "10-2020-0065489", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 지식 증강 방법을 구현하기 위한 컴퓨팅 장치의 블록도. 도 2는 본 발명의 실시 예에 따른 지식 증강 방법을 구현하기 위해 프로그래밍된 인공지능 에이전트의 블록도. 도 3은 본 발명의 실시 예에 따른 패션 코디네이션 지식에 대한 질문, 과거에 생성된 답변 및 과거에 생성된 지 식의 일 예를 나타내는 도면. 도 4는 도3에 도시한 질문, 과거에 생성된 답변 및 과거에 생성된 지식 각각에 대해 임베딩을 수행하여 획득한 임베딩 벡터의 일 예를 나타내는 도면. 도 5는 도 2에 도시한 지식 생성부가 사용자의 새로운 요구 사항과 TPO에 따라 생성한 패션 코디네이션 지식과 대화 생성부가 생성한 답변(상기 패션 코디네이션을 설명하는 답변)의 일 예를 나타낸 도면. 도 6은 도 2에 도시한 지식 생성부의 내부 구성을 나타내는 블록도. 도 7은 본 발명의 실시 예에 따른 불확실성을 계산하기 위해 사용되는 드롭 아웃 기법을 설명하기 위한 도면. 도 8은 본 발명의 컴퓨팅 장치에 의해 수행되는 인공지능 에이전트의 지식 증강 방법을 설명하기 위한 흐름도. 도 9는 도 8에 도시한 단계 820의 세부 과정의 일 예를 설명하는 흐름도. 도 10은 도 8에 도시한 단계 820의 세부 과정의 다른 예를 설명하는 흐름도. 도 11은 도 8에 도시한 단계 830의 세부 과정을 설명하는 흐름도."}
