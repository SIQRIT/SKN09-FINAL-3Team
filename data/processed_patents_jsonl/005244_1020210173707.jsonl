{"patent_id": "10-2021-0173707", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2023-0085494", "출원번호": "10-2021-0173707", "발명의 명칭": "멀티 라벨 분류 방법 및 그 장치", "출원인": "주식회사 케이티", "발명자": "고준석"}}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "적어도 하나의 프로세서에 의해 동작하는 멀티 라벨 분류 장치의 멀티 라벨 분류 방법으로서,입력 문단을 복수의 문장 단위로 분할하는 단계,입력된 문장 특징 벡터로부터 문장 중요도 확률값을 출력하도록 학습된 문장 중요도 모델을 이용하여, 상기 분할한 문장들 각각의 문장 특징 벡터들에 대한 각각의 문장 중요도 확률값을 산출하는 단계, 그리고상기 분할한 문장들 각각의 문장 특징 벡터들과 상기 각각의 문장 중요도 확률값을 문단 분류 모델의 입력 데이터로 사용하여 상기 입력 문단에 대한 복수의 라벨 별 확률값을 산출하는 단계를 포함하는, 멀티 라벨 분류 방법."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에서,상기 분할하는 단계 이후,기 학습된 언어 표현 모델을 이용하여, 상기 분할한 각각의 문장으로부터 문장 대표 특징 벡터와 문장 워드 특징 벡터를 추출하는 단계를 더 포함하고,상기 문장 워드 특징 벡터는, 상기 문장 중요도 모델의 입력 데이터로 사용되고,상기 문장 대표 특징 벡터는,상기 각각의 문장 중요도 확률값과 함께 상기 문단 분류 모델의 입력 데이터로 사용되는, 멀티 라벨 분류 방법."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에서,상기 복수의 라벨 별 확률값을 산출하는 단계는,상기 입력 문단을 구성하는 문장들에 대한 각각의 문장 대표 특징 벡터와 각각의 문장 중요도 확률값을 행렬연산하여 생성한 특징 벡터 데이터를 입력 데이터로 사용하는, 멀티 라벨 분류 방법."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에서,상기 복수의 라벨 별 확률값을 산출하는 단계 이후,상기 입력 문단에 대한 복수의 라벨 별 확률값을 기 설정된 정답 데이터와 비교하여 문단 분류 모델의 손실을계산하고, 상기 문단 분류 모델의 손실을 통해 산출한 가중치를 이용하여 상기 문단 분류 모델을 역전파 학습시키는 단계, 그리고문장 별 중요도 확률값을 기 설정된 정답 데이터와 비교하여 문장 중요도 모델의 손실을 계산하고, 상기 문장중요도 모델의 손실과 상기 문단 분류 모델의 손실을 합산한 통합 손실을 통해 산출한 가중치를 이용하여 상기문장 중요도 모델을 역전파 학습시키는 단계를 더 포함하는, 멀티 라벨 분류 방법."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "멀티 라벨 분류 프로그램이 저장된 메모리, 그리고공개특허 10-2023-0085494-3-상기 멀티 라벨 분류 프로그램을 실행하는 적어도 하나의 프로세서를 포함하고,상기 멀티 라벨 분류 프로그램은,입력 문단에 대한 문장 특징 벡터를 입력 데이터로 사용하여 문장 중요도 확률값을 출력하도록 문장 중요도 모델을 학습시키고, 상기 입력 문단에 대한 문장 특징 벡터와 상기 문장 중요도 확률값을 입력 데이터로 사용하여상기 입력 문단에 대한 복수의 라벨 확률값을 산출하도록 문단 분류 모델을 학습시키는 명령어들(Instructions)을 포함하는, 멀티 라벨 분류 장치."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에서, 상기 멀티 라벨 분류 프로그램은, 상기 입력 문단을 복수의 문장 단위로 분할하고, 기 학습된 언어 표현 모델을 이용하여, 상기 분할한 각각의 문장으로부터 문장 대표 특징 벡터와 문장 워드 특징 벡터를 추출하고,상기 문장 워드 특징 벡터를 이용하여 상기 문장 중요도 모델을 학습시키고,상기 문장 대표 특징 벡터와 상기 문장 중요도 확률값을 이용하여 상기 문단 분류 모델을 학습시키는 명령어들을 포함하는, 멀티 라벨 분류 장치."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에서,상기 멀티 라벨 분류 프로그램은,제1 벡터 크기를 가지는 상기 분할한 각각의 문장에 대한 문장 워드 특징 벡터들을 신경망을 통해 상기 제1 벡터 크기보다 작은 제2 벡터 크기를 가지는 문장 워드 특징 벡터들로 축소시키고, 축소된 문장 워드 특징 벡터들을 통합하며, 통합된 문장 워드 특징 벡터를 학습 데이터로 사용하여 상기 문장 중요도 모델을 학습시키는 명령어들을 포함하는, 멀티 라벨 분류 장치."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에서,상기 멀티 라벨 분류 프로그램은,제1 벡터 크기를 가지는 상기 분할한 각각의 문장에 대한 문장 대표 특징 벡터들과 상기 분할한 각각의 문장에대해 산출된 문장 중요도 확률값들을 행렬 연산하여 문장 중요도가 반영된 문장 대표 특징 벡터들을 생성하고,상기 문장 중요도가 반영된 문장 대표 특징 벡터들을 학습 데이터로 사용하여 상기 문단 분류 모델을 학습시키는 명령어들을 포함하는, 멀티 라벨 분류 장치."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제5항에서,상기 멀티 라벨 분류 프로그램은,상기 입력 문단에 대한 복수의 라벨 별 확률값을 기 설정된 정답 데이터와 비교하여 계산한 문단 분류 모델의손실을 이용하여 상기 문단 분류 모델을 역전파 학습시키고, 상기 문장 중요도 확률값을 기 설정된 정답 데이터와 비교하여 계산한 문장 중요도 모델의 손실과 상기 문단 분류 모델의 손실을 합산한 손실을 이용하여 상기 문장 중요도 모델을 역전파 학습시키는 명령어들을 포함하는,멀티 라벨 분류 장치."}
{"patent_id": "10-2021-0173707", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제6항에서,공개특허 10-2023-0085494-4-상기 언어 표현 모델은,BERT(Bidirectional Encoder Representations from Transformers) 모델을 기반으로 학습되는, 멀티 라벨 분류장치."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "멀티 라벨 분류 방법 및 그 장치가 개시된다. 컴퓨팅 장치의 멀티 라벨 분류 방법으로서, 입력 문단을 복수의 문 장 단위로 분할하는 단계, 입력된 문장 특징 벡터로부터 문장 중요도 확률값을 출력하도록 학습된 문장 중요도 모델을 이용하여, 상기 분할한 문장들 각각의 문장 특징 벡터들에 대한 각각의 문장 중요도 확률값을 산출하는 단계, 그리고 상기 분할한 문장들 각각의 문장 특징 벡터들과 상기 각각의 문장 중요도 확률값을 문단 분류 모델 의 입력 데이터로 사용하여 상기 입력 문단에 대한 복수의 라벨 별 확률값을 산출하는 단계를 포함한다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 멀티 라벨 분류 방법 및 그 장치에 관한 것이다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "종래의 분류 학습은 분류 모델 내부에서 자체적으로 중요하다고 생각되는 문장에 가중치를 부여하여 학습을 진 행하므로, 학습 자체가 잘못 이루어질 수 있고 사용자가 학습 과정을 제어하지 못하는 문제가 있다. 또한, 문장은 단일 라벨로 분류할 수 있지만, 문단은 멀티 라벨(multi label)을 가지는 경우가 많다. 그러나, 종래의 멀티 라벨(multi label) 분류는 동일한 데이터에 다른 분류 라벨을 태깅하여 학습하므로, 사용 자의 의도대로 학습이 되지 않는 문제가 있다. 특히, 상담문(문단)은 구어체로 여러 의도의 문장들과 무의미한 문장들이 섞여 있는데, 이를 학습에 이용할 때 라벨에 따라 중요한 문장들을 선별하는 것이 필요하지만, 종래의 방법으로는 이를 해결할 수 없다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "해결하고자 하는 과제는 문단을 구성하는 문장들의 가중치를 고려하여 유의미한 신뢰도(confidence)를 가진 멀 티 라벨을 분류하는 방법 및 그 장치를 제공하는 것이다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "한 특징에 따르면, 적어도 하나의 프로세서에 의해 동작하는 멀티 라벨 분류 장치 멀티 라벨 분류 방법은 입력 문단을 복수의 문장 단위로 분할하는 단계, 입력된 문장 특징 벡터로부터 문장 중요도 확률값을 출력하도록 학 습된 문장 중요도 모델을 이용하여, 상기 분할한 문장들 각각의 문장 특징 벡터들에 대한 각각의 문장 중요도 확률값을 산출하는 단계, 그리고 상기 분할한 문장들 각각의 문장 특징 벡터들과 상기 각각의 문장 중요도 확률 값을 문단 분류 모델의 입력 데이터로 사용하여 상기 입력 문단에 대한 복수의 라벨 별 확률값을 산출하는 단계 를 포함한다. 상기 분할하는 단계 이후, 기 학습된 언어 표현 모델을 이용하여, 상기 분할한 각각의 문장으로부터 문장 대표 특징 벡터와 문장 워드 특징 벡터를 추출하는 단계를 더 포함하고, 상기 문장 워드 특징 벡터는, 상기 문장 중 요도 모델의 입력 데이터로 사용되고, 상기 문장 대표 특징 벡터는, 상기 각각의 문장 중요도 확률값과 함께 상 기 문단 분류 모델의 입력 데이터로 사용될 수 있다. 상기 복수의 라벨 별 확률값을 산출하는 단계는, 상기 입력 문단을 구성하는 문장들에 대한 각각의 문장 대표 특징 벡터와 각각의 문장 중요도 확률값을 행렬 연산하여 생성한 특징 벡터 데이터를 입력 데이터로 사용할 수 있다. 상기 복수의 라벨 별 확률값을 산출하는 단계 이후, 상기 입력 문단에 대한 복수의 라벨 별 확률값을 기 설정된 정답 데이터와 비교하여 문단 분류 모델의 손실을 계산하고, 상기 문단 분류 모델의 손실을 통해 산출한 가중치 를 이용하여 상기 문단 분류 모델을 역전파 학습시키는 단계, 그리고 문장 별 중요도 확률값을 기 설정된 정답 데이터와 비교하여 문장 중요도 모델의 손실을 계산하고, 상기 문장 중요도 모델의 손실과 상기 문단 분류 모델 의 손실을 합산한 통합 손실을 통해 산출한 가중치를 이용하여 상기 문장 중요도 모델을 역전파 학습시키는 단 계를 더 포함할 수 있다. 다른 특징에 따르면, 멀티 라벨 분류 장치는 멀티 라벨 분류 프로그램이 저장된 메모리, 그리고 상기 멀티 라벨 분류 프로그램을 실행하는 적어도 하나의 프로세서를 포함하고, 상기 멀티 라벨 분류 프로그램은, 입력 문단에대한 문장 특징 벡터를 입력 데이터로 사용하여 문장 중요도 확률값을 출력하도록 문장 중요도 모델을 학습시키 고, 상기 입력 문단에 대한 문장 특징 벡터와 상기 문장 중요도 확률값을 입력 데이터로 사용하여 상기 입력 문 단에 대한 복수의 라벨 확률값을 산출하도록 문단 분류 모델을 학습시키는 명령어들(Instructions)을 포함할 수 있다. 상기 멀티 라벨 분류 프로그램은, 상기 입력 문단을 복수의 문장 단위로 분할하고, 기 학습된 언어 표현 모델을 이용하여, 상기 분할한 각각의 문장으로부터 문장 대표 특징 벡터와 문장 워드 특징 벡터를 추출하고, 상기 문 장 워드 특징 벡터를 이용하여 상기 문장 중요도 모델을 학습시키고, 상기 문장 대표 특징 벡터와 상기 문장 중 요도 확률값을 이용하여 상기 문단 분류 모델을 학습시키는 명령어들을 포함할 수 있다. 상기 멀티 라벨 분류 프로그램은, 제1 벡터 크기를 가지는 상기 분할한 각각의 문장에 대한 문장 워드 특징 벡 터들을 신경망을 통해 상기 제1 벡터 크기보다 작은 제2 벡터 크기를 가지는 문장 워드 특징 벡터들로 축소시키 고, 축소된 문장 워드 특징 벡터들을 통합하며, 통합된 문장 워드 특징 벡터를 학습 데이터로 사용하여 상기 문 장 중요도 모델을 학습시키는 명령어들을 포함할 수 있다. 상기 멀티 라벨 분류 프로그램은, 제1 벡터 크기를 가지는 상기 분할한 각각의 문장에 대한 문장 대표 특징 벡 터들과 상기 분할한 각각의 문장에 대해 산출된 문장 중요도 확률값들을 행렬 연산하여 문장 중요도가 반영된 문장 대표 특징 벡터들을 생성하고, 상기 문장 중요도가 반영된 문장 대표 특징 벡터들을 학습 데이터로 사용하 여 상기 문단 분류 모델을 학습시키는 명령어들을 포함할 수 있다. 상기 멀티 라벨 분류 프로그램은, 상기 입력 문단에 대한 복수의 라벨 별 확률값을 기 설정된 정답 데이터와 비 교하여 계산한 문단 분류 모델의 손실을 이용하여 상기 문단 분류 모델을 역전파 학습시키고, 상기 문장 중요도 확률값을 기 설정된 정답 데이터와 비교하여 계산한 문장 중요도 모델의 손실과 상기 문단 분류 모델의 손실을 합산한 손실을 이용하여 상기 문장 중요도 모델을 역전파 학습시키는 명령어들을 포함할 수 있다. 상기 언어 표현 모델은, BERT(Bidirectional Encoder Representations from Transformers) 모델을 기반으로 학 습될 수 있다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "실시예에 따르면, 문단이 여러 내용을 포함하고 있기 때문에 더 작은 단위인 문장으로 나누어 문단에서 중요한 문장을 찾아 그 가중치를 최종 분류에 반영함으로써 문단의 분류 라벨을 확률값으로 잘 나타낼 수 있다."}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "아래에서는 첨부한 도면을 참고로 하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였 다. 명세서 전체에서, 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다 른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 또한, 명세서에 기재된 \"…부\", \"…기\", \"…모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어나 소프트웨어 또는 하드웨어 및 소프트웨어의 결합으로 구현될 수 있다. 본 발명에서 설명하는 장치들은 적어도 하나의 프로세서, 메모리 장치, 통신 장치 등을 포함하는 하드웨어로 구 성되고, 지정된 장소에 하드웨어와 결합되어 실행되는 프로그램이 저장된다. 하드웨어는 본 발명의 방법을 실행 할 수 있는 구성과 성능을 가진다. 프로그램은 도면들을 참고로 설명한 본 발명의 동작 방법을 구현한 명령어 (instructions)를 포함하고, 프로세서와 메모리 장치 등의 하드웨어와 결합하여 본 발명을 실행한다. 본 명세서에서 \"전송 또는 제공\"은 직접적인 전송 또는 제공하는 것 뿐만 아니라 다른 장치를 통해 또는 우회 경로를 이용하여 간접적으로 전송 또는 제공도 포함할 수 있다. 본 명세서에서 단수로 기재된 표현은 \"하나\" 또는 \"단일\" 등의 명시적인 표현을 사용하지 않은 이상, 단수 또는 복수로 해석될 수 있다. 본 명세서에서 도면에 관계없이 동일한 도면번호는 동일한 구성요소를 지칭하며, \"및/또는\" 은 언급된 구성 요 소들의 각각 및 하나 이상의 모든 조합을 포함한다. 본 명세서에서, 제1, 제2 등과 같이 서수를 포함하는 용어들은 다양한 구성요소들을 설명하는데 사용될 수 있지 만, 상기 구성요소들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요 소로부터 구별하는 목적으로만 사용된다. 예를들어, 본 개시의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 본 명세서에서 도면을 참고하여 설명한 흐름도에서, 동작 순서는 변경될 수 있고, 여러 동작들이 병합되거나, 어느 동작이 분할될 수 있고, 특정 동작은 수행되지 않을 수 있다. 본 개시의 신경망은 적어도 하나의 태스크를 학습하는 인공지능 모델로서, 컴퓨팅 장치에서 실행되는 소프트웨 어/프로그램으로 구현될 수 있다. 프로그램은 저장 매체(non-transitory storage media)에 저장되고, 프로세서 에 의해 본 개시의 동작을 실행하도록 기술된 명령들(instructions)을 포함한다. 프로그램은 네트워크를 통해 다운로드되거나, 제품 형태로 판매될 수 있다. 문단은 여러 문장들로 구성되는데, 각 문장 별로 서로 다른 의도를 가지고 있으므로, 문단의 분류는 어려운 경 우가 많고, 판단하는 사람에 따라 문단의 의도가 다양할 수 있다. 따라서, 문단은 단일 라벨 분류 보다는 top3 혹은 top5까지의 분류결과가 중요하며 각각의 신뢰도(confidence)값이 문단의 의도를 대표하는 유의미한 값이어 야 한다. 보이스봇(Voicebot)은 이러한 신뢰도값들을 이용하여 사용자의 대화 중 문단의 의도를 top3까지 파악 하여 대화에 참조할 수 있다. 이와 같이 문단으로부터 top3 혹은 top5까지의 분류, 즉, 멀티(Multi) 라벨(Label)을 분류하기 위해서는 싱글 (Single) 라벨(Label) 보다 더 많은 데이터를 필요로 한다. 본 발명의 실시예가 적용되기 전에 일반적인 멀티 라벨 분류 방식은 동일한 데이터에 라벨만 다양하게 적용하였다. 예를들어, 하나의 문단에 label #1과 label #2 를 적용하고자 한다면, 싱글 라벨과 마찬가지로 그 문단에 label #1을 태깅한 학습 데이터(label #1 ↔ 문단), 그 문단에 label #2를 태깅한 학습 데이터(label #2 ↔ 문단)를 이용하여 멀티 라벨 분류를 위한 학습이 이루어 진다. 하지만 이 방법은 동일한 데이터를 학습할 때 역전파(backpropagation)하는데 문제가 있다. 동일한 데이 터에 다른 손실(loss)을 적용하면 가중치(weight) 업데이트가 의도한 대로 안되는 경우가 많다. 이러한 문제를 해결하기 위하여, 실시예에 따르면, 동일한 문단 데이터를 이용하더라도 해당 라벨을 추론하는데 중요한 문장에 가중치를 부여하여 멀티 라벨 분류를 위한 학습을 수행한다. 실시예에 따른 멀티 라벨 분류 방식 에 대하여 이하 도면을 참고하여 설명하기로 한다. 도 1은 한 실시예에 따른 멀티 라벨 분류 장치의 구성도이다. 도 1을 참조하면, 멀티 라벨 분류 장치는 분류부, 언어 표현 모델, 문장 중요도 모델 및 문단 분류 모델을 포함할 수 있다. 멀티 라벨 분류 장치는 적어도 하나의 프로세서에 의해 동작하는 컴퓨팅 장치로서, 복수의 문장으로 구성 된 문단(예, 상담문)을 입력받아 문단에 대한 복수의 라벨을 분류한다. 멀티 라벨 분류 장치는 본 발명의 실시예에서 설명하는 동작을 위한 컴퓨터 프로그램을 탑재하고, 컴퓨터 프로그램은 프로세서에 의해 실행된다. 멀티 라벨 분류 장치의 학습 과정은 크게 선행 학습 과정과 전이 학습 과정으로 구분되는데, 선행 학습 과 정은 언어 표현 모델을 이용하고 전이 학습 과정은 문장 중요도 모델 및 문단 분류 모델을 이용 할 수 있다. 문단 분류 모델은 문장 중요도 모델의 출력을 입력 데이터로 사용하여 학습하고, 문장 중요도 모델 은 역전파시 사용하는 손실(loss)을 문단 분류 모델의 결과를 참고하여 학습할 수 있다. 분류부는 상담문을 입력받아 입력받은 상담문을 문장 단위로 분할한다. 분류부는 마침표와 특정 키워 드를 기준으로 상담문을 문장 단위로 분할할 수 있다. 여기서, 특정 키워드는 어미에 사용되는 용어들(예, 입니 다, 까요, 세요, 네, 예, 합니다 등)을 정의한 사전에 기초할 수 있다. 실시예에 따르면, 분류부는 문단 형태의 상담문을 오픈소스인 KSS(Korean sentence Splitter)를 사용하여 문장 단위로 1차 분류할 수 있다. 다음, 분류부는 사전에 정의된 규칙 기반 sentence segmentation을 통하 여 세부적인 문장 단위로 2차 분류할 수 있다. sentence segmentation을 통한 문장 단위 분할은 표 1에 나타낸 예시와 같을 수 있다. 표 1 문장 예시) 안녕하세요. kt 상담사 입니다. 지난달 요금이 너무 많이 나왔는데요. 개인정보 확인하겠 습니다. 성함이 어떻게 되시나요? 핸드폰 번호 적어주세요. 010888입니다. 네 확인하겠습니다. 감사합 니다. → sentence segmentation 이후 안녕하세요. / kt 상담사 입니다. / 지난달 요금이 너무 많이 나왔는데요. / 개인정보 확인하겠습니다. / 성함이 어떻게 되시나요? / 핸드폰 번호 적어주세요. / 010888입니다. / 네 / 확인 하겠습니다. / 감사합니다. 표 1에서 '/'는 문장 구분자이다. 분류부가 분할한 문장들은 언어 표현 모델의 인코더에 입력된다. 언어 표현 모델은 인코더와 디코더로 구성된다. 이때, 언어 표현 모델의 인코더는 분 류부가 출력하는 문장들을 입력받아 입력받은 문장들에 대한 [128×1024] 차원의 특징 벡터를 출력하고, 디코더는 [128×1024] 차원의 특징 벡터를 입력받아 원 문장들을 출력한다. 이처럼, 언어 표현 모델 은 인코더의 출력을 디코더로 입력하고, 디코더의 출력을 인코더에 입력하여 순환 학습된 다. 언어 표현 모델의 구조는 트랜스포머(transformer)로 구성된 인코더-디코더 모델로서, 한 실시예에 따르면, BERT(Bidirectional Encoder Representations from Transformers) 모델, RoBERTa-large 모델 또는 RoBERTa 모델을 응용하여 학습된 모델이 사용될 수 있다. 언어 표현 모델은 60G의 상담문 데이터를 학습 데이터로 사용하여 생성될 수 있다. 언어 표현 모델은 입력 문장에 대해 임베딩(embedding), 토큰화(tokenization), 패딩(padding), 멀티헤드 어텐션(multihead attention) 등의 과정을 통해 특징 벡터를 추출할 수 있다. 언어 표현 모델의 인코더는 입력받은 문장들에 대해 각각의 cls(Special Classification) 토큰 (token)과 워드 토큰(word token)을 출력한다. 인코더는 문장을 입력받아 [128×1024] 차원의 특징 벡터를 출력하는데, [128×1024] 차원의 특징 벡터는 [0,1024] 차원의 특징 벡터와 [1~128,1024] 차원의 특징 벡터를 포함한다. 여기서, [0,1024] 차원의 특징 벡터는 cls 토큰에 해당하고, [1~128,1024] 차원의 특징 벡터는 워드 토큰에 해 당한다. 즉, cls 토큰은 입력된 문장을 분류에 적합하도록 표현(presentation)한 [0,1024] 차원의 특징 벡터이다. cls 토큰은 문장의 시작 토큰이면서 분류 토큰으로서, 문장을 대표할 수 있도록 학습된 문장을 대표하는 특징 벡터, 즉, 문장 대표 특징 벡터라 할 수 있다. cls 토큰은 문단 분류 모델의 입력 데이터로 사용된다. 워드 토큰은 입력된 문장의 순서 정보와 워드(word)의 특성을 포함하여 표현(presentation)된 [1~128, 1024] 차 원의 특징 벡터, 즉, 문장 워드 특징 벡터이다. 워드 토큰은 문장 중요도 모델의 입력 데이터로 사용된다. 문장 중요도 모델은 CNN(Convolutional Neural Network) 기반의 모델로서, 인코더에서 출력된 워드 토큰을 입력받아 워드 토큰이 추출된 문장이 전체 상담문에서 차지하는 중요도를 나타내는 확률값(이하, '중요도 확률값'으로 통칭함)을 출력한다. 문단 분류 모델은 언어 표현 모델의 인코더에서 출력된 cls 토큰과 문장 중요도 모델에서 출력된 문장의 중요도 확률값을 입력받아 그 문장이 속하는 문단의 라벨 별 확률값을 출력한다. 문단 분류 모델은 문장을 입력받아 입력 문장이 속하는 문단의 라벨 별 확률값을 출력하도록 학습되어 있 다. 여기서, 라벨은 상담문의 분류 카테고리로서, 예를들어, 가입, 해지 등과 같이 사전에 지정되어 있을 수 있 다. 도 2는 한 실시예에 따른 문장 중요도 모델의 구조를 나타낸다. 도 2를 참조하면, 문장 중요도 모델의 구조는 CNN(Convolutional Neural Networks), 문자열 연결, dense layer, softmax layer, 손실 함수를 포함할 수 있다. 언어 표현 모델에 의한 인코딩을 통해 각 문장마다 [128×1024] 차원의 특징 벡터가 추출되며, [128× 1024] 차원의 특징 벡터 중에서도 [1~127, 1024] 차원의 특징 벡터들은 워드 토큰으로서 CNN에 입력된다. CNN은 워드 토큰(word token)을 입력받아 워드 토큰(word token)의 벡터 크기를 감소시킨다. CNN은 워드 토큰의 벡터 크기[1~127, 1024]를 [1×128] 벡터 크기로 줄일 수 있다. 문장 중요도 모델에서 CNN을 사용한 이유는 워드 토큰으로 출력된 결과는 입력된 문장을 리프리젠테이션 (representation)한 것으로 희소(sparse)한 특징을 가지고 있기 때문이다. 문단에 포함된 n개의 문장들로부터 추출된 [1~127, 1024] 벡터 크기를 가지는 n개의 워드 토큰들은 CNN을 통해 [1×128] 벡터 크기를 가지는 n개의 워드 토큰들로 생성된다. 다음, CONCAT 함수 등을 이용하여 [1×128] 벡터 크기를 가지는 n개의 워드 토큰들은 일렬로 나열되어 연결됨으 로써, n×128 차원의 워드 토큰 벡터로 생성되어 dense layer에 입력된다. dense layer는 fully connected layer로서, n×128 차원의 워드 토큰 벡터를 입력받아 워드 토큰 벡터에 대응 하는 문장이 문단, 즉, 전체 상담문에서 차지하는 중요도를 출력하도록 학습된다. dense layer는 [1×n] 크기의 중요도를 출력한다. softmax layer는 입력값을 확률값 형태로 표현해주는 함수로서, dense layer가 출력하는 문장 별 중요도를 확률 값(이하, '중요도 확률값'으로 통칭하여 기재함)으로 출력한다. softmax layer는 함수( , 여기서, x는 중요도)를 이용하여 문장 별 중요도를 확률값으로 출력할 수 있 다. 예를들어, 3개의 문장 별 중요도([1×3])가 [1,2,3]이라면, softmax layer를 통해 [0.09003057, 0.24472847, 0.66524096] 확률값으로 생성되고 이러한 확률값들은 더하면 1이 된다. 첫번째 중요도 확률값인 0.09003057에 대응하는 문장이 '안녕하세요'라면, 0.09003057는 '안녕하세요'가 전체 문장에서 차지하는 중요도 확률값을 의미한다. 손실함수는 softmax layer가 출력하는 중요도 확률값을 정답 데이터와 비교하여 손실(loss)을 계산한다. 이때, 손실 계산시 categorical cross entropy가 사용될 수 있다. 정답 데이터는 학습에 사용되는 상담문에 대하여 사 전에 구축된 멀티 라벨을 포함할 수 있다. 문장 중요도 모델의 손실은 softmax layer가 출력하는 중요도 확률값을 정답 데이터와 비교하여 계산한 손 실(loss)과 문단 분류 모델에서 계산된 손실(loss)을 합산하여 사용된다. 문장 중요도 모델은 이러한 합산 손실을 이용하여 가중치(weight)를 계산하고 이러한 가중치를 통해 역전 파(backpropagation) 학습된다. 역전파시 가중치는 문장 중요도 모델의 CNN, dense layer, softmax layer에 적용된다. 즉, 가중치 적용은 문장 중요도 모델의 결과를 이용하여 문장 중요도 모델을 학습함을 의미한다. 가중치는 gradient descent 방식등이 사용될 수 있으며, 역전파(backpropagation) 할 때, 문장 중요도 모델 에서 내부 모델의 가중치(weight)값들의 업데이트 수식은 다음 수학식 1과 같을 수 있다.[수학식 1]"}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서, w_new는 업데이트 가중치이고, w_old는 업데이트 이전 가중치를 의미한다. α는 learning rate로서, 상 수이다. E는 손실(L)로서, Error rate로 나타낼 수 있다. 수학식 1에서 E는 문단 분류 모델의 손실(Error rate) + 문장 중요도 모델의 손실(Error rate)과 같 다. w는 weight matrix로 입력된 벡터값을 확률로 계산할 때 사용하는 값이다. w_new는 w_old에서 일정한 값을 빼는데, 그 값은 W(w_old)값을 기준으로 E를 미분한 값에 α를 곱한 것이다. 이것이 의미하는 것은 w_old를 기 준으로 E가 변하는 정도만큼을 말한다. 도 3은 한 실시예에 따른 문단 분류 모델의 구조를 나타낸다. 도 3을 참조하면, 문단 분류 모델의 구조는 복수의 제1 dense layer, 문자열 연결, 제2 dense layer, softmax layer, 손실 함수를 포함할 수 있다. 문단 분류 모델은 cls token과 문장 중요도 모델이 출력한 문장별 중요도 확률값을 입력으로 사용한 다. 문단 분류 모델은 각 문장별 cls token을 해당 문장별 중요도 확률값과 행렬 연산(Element wise produc t)하여 문장 별로 [1×1024] 차원의 특징 벡터 데이터를 생성한다. 이러한 [1×1024] 차원의 특징 벡터 데이터 는 문장의 중요도가 반영된 문장을 대표하는 cls 토큰에 해당한다. 이와 같이, 문단 분류 모델은 입력 데 이터로 문장의 중요도를 반영하므로, 중요도가 상대적으로 낮은 문장의 비중을 줄여서 문단 라벨을 분류할 수 있다. 문장 별로 생성된 [1×1024] 차원의 특징 벡터 데이터들은 각각 문장 별로 복수의 제1 dense layer를 통해 [1× 256] 차원의 특징 벡터 데이터들로 벡터 크기가 축소된다. 이처럼, 축소된 특징 벡터들은 concat 함수 등과 같 은 문자열 연결 방식으로 합쳐져 [n×256] 크기를 가지는 하나의 특징 벡터가 된다. 여기서, n은 문장 개수를 의미한다. [n×256] 크기를 가지는 하나의 특징 벡터는 제2 dense layer를 통해 복수의 라벨로 분류된다. 제2 dense layer 를 통해 출력되는 값은 [1×분류 라벨 수]만큼의 라벨 별 정답값으로서, 라벨 별 정답값은 도 2에서 설명한 내 용과 마찬가지로 softmax layer를 통해 라벨 별 정답 확률값으로 생성된다. 이때, 라벨별 정답 확률값들 중에서 가장 큰 정답 확률값에 해당하는 라벨이 정답 라벨로 선정된다. 문단 분류 모델에서 손실함수는 제2 dense layer를 통해 출력되는 라벨 별 확률값을 정답 데이터와 비교해 서 손실을 산출하며, categorical cross entropy가 사용될 수 있다. 손실(L)은 수학식 2를 통해 산출될 수 있다. [수학식 2]"}
{"patent_id": "10-2021-0173707", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, N은 라벨의 개수이고, Ct는 정답 라벨 확률값이고, y는 추론을 통해서 출력된 라벨 확률값이다. 예를들어, 문단에 대한 정답 데이터인 라벨별 확률값이 [0,1,0,0]이고 제2 dense layer가 출력하는 라벨별 확률 값이 [0.1, 0.4, 0.01, 0.2]이라면, L = (-0log0.1 + 1log0.4 + 0log0.01 + 0log0.2)이 된다. 문단 분류 모델은 손실(L)을 이용하여 역전파(backpropagation) 학습시 가중치를 업데이트하며, 가중치 수 식은 수학식 1과 동일하다. 다만, 수학식 1을 통해 문단 분류 모델에서 가중치를 산출할때에는 E가 문단 분류 모델의 Error rate만 사용된다. 이러한 가중치는 문단 분류 모델의 내부 모델들, 즉, 복수의 제 1 dense layer, 제2 dense layer, softmax layer에 적용된다. 가중치 적용은 문단 분류 모델의 결과를 이 용하여 문단 분류 모델을 학습함을 의미한다.도 4는 한 실시예에 따른 멀티 라벨 분류 방법을 나타낸 순서도로서, 멀티 라벨 분류를 위한 문장 중요도 모델 및 문단 분류 모델의 학습 과정을 포함한다. 도 4를 참조하면, 멀티 라벨 분류 장치의 분류부는 입력받은 상담문을 문장 단위로 분할한다(S101). 멀티 라벨 분류 장치는 언어 표현 모델의 인코더를 이용하여 분할된 문장으로부터 cls 토큰 벡 터와 word 토큰 벡터를 추출한다(S102). 멀티 라벨 분류 장치는 word 토큰 벡터를 문장 중요도 모델에 입력하여 문장별 중요도 확률값을 산출 한다(S103). 멀티 라벨 분류 장치는 S103에서 산출한 문장별 중요도 확률값과 각 문장의 cls 토큰 벡터를 토대로, 문장 별로 그 문장의 중요도 확률값이 반영된 cls 토큰 벡터들을 생성하고, 생성한 cls 토큰 벡터들을 문단 분류 모 델에 입력하여 문단의 라벨별 확률값을 산출한다(S104). 멀티 라벨 분류를 위한 학습 과정이 아니라면, 멀티 라벨 분류 장치는 S101~S104를 수행하고, S104의 라벨 별 확률값 중에서 확률값이 가장 큰 라벨을 그 문단의 라벨로 추론한다. 멀티 라벨 분류를 위한 학습 과정에서는, S101~S104에 S105~S108이 추가된다. 즉, 멀티 라벨 분류 장치는 S104에서 산출한 문단 라벨별 확률값을 기 구축한 정답 데이터와 비교하여 손실(L)을 산출한다(S105). 멀티 라벨 분류 장치는 S105에서 산출한 손실을 이용하여 계산한 가중치를 이용하여 문단 분류 모델 을 역전파 학습한다. 멀티 라벨 분류 장치는 S103에서 산출한 문장 중요도 확률값을 기 구축한 정답 데이터와 비교하여 손실 (L)을 산출한다(S106). 멀티 라벨 분류 장치는 S106에서 산출한 손실을 이용하여 계산한 가중치를 이용하여 문장 중요도 모델 을 역전파 학습한다. 이와 같이, 멀티 라벨 분류를 위한 학습 과정에서는 S103을 통해 문장 중요도 모델의 순전파(forward propagation) 학습, S104를 통해 문단 분류 모델의 순전파 학습, S105~S106을 통해 문단 분류 모델 의 역전파 학습, S107~S108을 통해 문장 중요도 모델의 역전파 학습이 이루어진다. 순전파 학습은 입력된 데이터에 대해서 가중치 파라미터(weight parameter) 값들을 이용해 문단 분류 라벨을 예 측을 하는 단계이고, 역전파 학습은 순전파 학습에 의해 예측된 값과 정답값을 비교하여 산출한 손실을 이용하 여 가중치 파라미터를 업데이트하는 단계라 할 수 있다. 이상 기재한 바와 같이, 실시예에 따르면, 멀티 라벨 분류 장치는 문단 분류를 할 때에 문장들의 특징을 파악하여 그 문단에서 중요한 문단의 중요도를 반영하여 문단 라벨을 추론할 수 있다. 추론된 문단의 라벨은 확 률값(confidence)을 가지며, 이 확률값은 해당 문단에서 중요하다고 판단된 문장의 중요도에 따라 각 라벨들이 해당 문단을 대표하는 확률을 뜻한다. 따라서, 멀티 라벨 분류 장치는 동일한 데이터에 대해 라벨만 다르 게 사용하여 학습할때와 달리 멀티 라벨 학습이 용이하다. 또한, 멀티 라벨 분류 장치는 문장 중요도 모델을 역전파 학습할 때, 문단 분류 모델의 손실(L, Error rate)을 참고하여 학습하기 때문에 문단 분류 별로 문장 중요도에 가중치를 적용하여 학습할 수 있다. 또한, 종래에 멀티 라벨 분류시에는 문장 중요도 모델이 없으므로, 문단 라벨 확률값 중에서 top1으로 가중치가 너무 치우쳐 학습되므로 top3의 데이터를 사용할 수 없다. 그러나, 실시예에 따른 멀티 라벨 분류 장치는 문장의 중요도를 반영하여 학습하므로, top3의 가중치가 의 미있게 분포된 결과를 도출할 수 있다. 그리고 top3의 결과를 반영하여 상담문 기준으로 탐침, 즉, 사용자에게 재질문을 통해 최종 분류 성능이 개선될 수 있다. 따라서, 실시예에서는 상담문에서 의미있는 확률값, 예를들어, top3의 문단 라벨별 확률값을 이용하여 사용자에게 재질문하는 방법으로 응용하여 보이스봇을 제작할 수 있다. 한편, 도 5는 한 실시예에 따른 컴퓨팅 장치의 구성도이다. 도 5를 참고하면, 도 1 ~ 도 4에서 설명한 멀티 라벨 분류 장치는 적어도 하나의 프로세서에 의해 동작하 는 컴퓨팅 장치일 수 있다. 컴퓨팅 장치는 하나 이상의 프로세서, 프로세서에 의하여 수행되는 프로그램을 로드하는 메모리 , 프로그램 및 각종 데이터를 저장하는 저장 장치, 통신 인터페이스, 그리고 이들을 연결하는 버스를 포함할 수 있다. 이외에도, 컴퓨팅 장치는 다양한 구성 요소가 더 포함될 수 있다. 프로그램은 멀티 라벨 분류 프로그램으로서, 메모리에 로드될 때 프로세서로 하여금 본 개시의 다양 한 실시예에 따른 방법/동작을 수행하도록 하는 명령어들(instruction)을 포함할 수 있다. 즉, 프로세서는 명령어들을 실행함으로써, 본 개시의 다양한 실시예에 따른 방법/동작들을 수행할 수 있다. 명령어는 기능을 기 준으로 묶인 일련의 컴퓨터 판독가능 명령어들로서 컴퓨터 프로그램의 구성 요소이자 프로세서에 의해 실행되는 것을 가리킨다. 프로세서는 컴퓨팅 장치의 각 구성의 전반적인 동작을 제어한다. 프로세서는 CPU(Central Processing Unit), MPU(Micro Processor Unit), MCU(Micro Controller Unit), GPU(Graphic Processing Unit) 또는 본 개시의 기술 분야에 잘 알려진 임의의 형태의 프로세서 중 적어도 하나를 포함하여 구성될 수 있다. 또 한, 프로세서는 본 개시의 다양한 실시예들에 따른 방법/동작을 실행하기 위한 적어도 하나의 애플리케이 션 또는 프로그램에 대한 연산을 수행할 수 있다. 메모리는 각종 데이터, 명령 및/또는 정보를 저장한다. 메모리는 본 개시의 다양한 실시예들에 따른 방법/동작을 실행하기 위하여 저장 장치로부터 하나 이상의 프로그램을 로드할 수 있다. 메모리는 RAM과 같은 휘발성 메모리로 구현될 수 있을 것이나, 본 개시의 기술적 범위는 이에 한정되지 않는다. 저장 장치는 프로그램을 비임시적으로 저장할 수 있다. 저장 장치는 ROM(Read Only Memory), EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM), 플래시 메모리 등과 같은 비휘발성 메모리, 하드 디스크, 착탈형 디스크, 또는 본 개시가 속하는 기술 분야에서 잘 알려진 임의의 형태의 컴퓨터로 읽을 수 있는 기록 매체를 포함하여 구성될 수 있다. 통신 인터페이스는 컴퓨팅 장치의 유무선 통신을 지원한다. 이를 위해, 통신 인터페이스는 본 개시의 기술 분야에 잘 알려진 통신 모듈을 포함하여 구성될 수 있다. 버스는 컴퓨팅 장치의 구성 요소 간 통신 기능을 제공한 다. 버스는 주소 버스(Address Bus), 데이터 버스(Data Bus) 및 제어 버스(Control Bus) 등 다양한 형태의 버스로 구현될 수 있다. 이상에서 설명한 본 발명의 실시예는 장치 및 방법을 통해서만 구현이 되는 것은 아니며, 본 발명의 실시예의 구성에 대응하는 기능을 실현하는 프로그램 또는 그 프로그램이 기록된 기록 매체를 통해 구현될 수도 있다. 이상에서 본 발명의 실시예에 대하여 상세하게 설명하였지만 본 발명의 권리범위는 이에 한정되는 것은 아니고 다음의 청구범위에서 정의하고 있는 본 발명의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 본 발명의 권리범위에 속하는 것이다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2021-0173707", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 한 실시예에 따른 멀티 라벨 분류 장치의 구성도이다. 도 2는 한 실시예에 따른 문장 중요도 모델의 구조를 나타낸다. 도 3은 한 실시예에 따른 문단 분류 모델의 구조를 나타낸다. 도 4는 한 실시예에 따른 멀티 라벨 분류 방법을 나타낸 순서도이다. 도 5는 한 실시예에 따른 컴퓨팅 장치의 구성도이다."}
