{"patent_id": "10-2021-0082285", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0027746", "출원번호": "10-2021-0082285", "발명의 명칭": "로봇 주행에 따른 안전 가이드 라인을 제공하는 건물", "출원인": "네이버랩스 주식회사", "발명자": "김가현"}}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "안전 가이드 라인에 따라 로봇이 주행하는 건물에 있어서,상기 건물은,상기 로봇이 주행하는 실내 영역; 및상기 실내 영역에 배치되는 복수의 카메라를 포함하고,상기 로봇과 통신하는 클라우드 서버는,상기 카메라로부터 상기 로봇이 주행하는 공간에 대한 영상을 수신하고,상기 영상과 함께 상기 로봇이 상기 실내 영역에서 안전하게 주행하도록 하는 안전 정보와 관련된 그래픽 객체가 출력되도록 디스플레이부를 제어하고,상기 그래픽 객체의 시각적인 특성은 상기 로봇의 주행 정보에 근거하여 결정되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 그래픽 객체는 상기 영상에 오버랩되어 출력되고, 상기 그래픽 객체는, 일 방향을 따라 순차적으로 배열되는 복수의 영역을 포함하며,상기 그래픽 객체의 시각적인 특성은, 상기 그래픽 객체에서 상기 복수의 영역 각각이 차지하는 면적과 관련된것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 복수의 영역은 상기 로봇의 주행과 관련된 서로 다른 위험 등급에 각각에 매칭되며,상기 로봇으로부터 점차적으로 멀어지는 방향으로 배열되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서,상기 서로 다른 위험 등급 중 위험도가 가장 높은 제1 등급은, 상기 복수의 영역 중 상기 영상에서 상기 로봇이위치한 영역과 가장 가까운 제1 영역에 매칭되고,상기 서로 다른 위험 등급 중 위험도가 가장 낮은 제2 등급은, 상기 복수의 영역 중 상기 영상에서 상기 로봇과가장 먼 제2 영역에 매칭되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 그래픽 객체에서 상기 제1 영역 및 상기 제2 영역이 각각 차지하는 면적은, 상기 주행 정보에 근거하여 결정되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 주행 정보는 상기 로봇의 주행 속도를 포함하고,공개특허 10-2022-0027746-3-상기 로봇의 주행 속도가 커 질수록, 상기 그래픽 객체에서 상기 제1 영역이 차지하는 면적은 상기 제2 영역이차지하는 면적에 비하여 상대적으로 커지며, 상기 로봇의 주행 속도가 작아 질수록, 상기 그래픽 객체에서 상기 제1 영역의 차지하는 면적은 상기 제2 영역이 차지하는 면적에 비하여 상대적으로 작아지는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제5항에 있어서, 상기 제1 영역 및 상기 제2 영역은 서로 다른 색상으로 표시되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제4항에 있어서,상기 클라우드 서버는,상기 영상으로부터 상기 실내 영역에 위치한 동적인 객체를 센싱하고,상기 실내 영역에서의 상기 동적인 객체의 이동 방향 및 이동 속도 중 적어도 하나에 근거하여, 상기 제1 영역및 상기 제2 영역의 면적의 크기를 조정하는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제2항에 있어서,상기 그래픽 객체의 길이 및 폭 중 적어도 하나는, 상기 로봇의 주행 속도에 근거하여 변경되는 것을 특징으로하는 건물."}
{"patent_id": "10-2021-0082285", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제4항에 있어서, 상기 실내 영역에 배치된 상기 복수의 카메라 중, 상기 영상은 상기 로봇의 주행 방향을 화각으로 하는 특정 카메라로부터 수신되고,상기 제1 영역 및 상기 제2 영역은, 상기 디스플레이부의 하측으로부터 상측을 향하는 일방향을 따라 순차적으로 배치되는 것을 특징으로 하는 건물."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 로봇 관제에 관한 것으로서, 원격으로 로봇을 관리하고 제어할 수 있는 로봇 관제 방법 및 시스템이다. 위에서 살펴본 과제를 해결하기 위하여 본 발명에 따른 로봇 관제 방법은, 로봇이 주행하는 공간에 대한 영상을 수신하는 단계, 상기 로봇으로부터 상기 로봇의 주행과 관련된 주행 정보를 수신하는 단계 및 상기 영상과 함께 상기 주행 정보에 대응되는 그래픽 객체가 출력되도록 디스플레이부를 제어하는 단계를 포함하고, 상기 그래픽 객체의 시각적인 특성은 상기 주행 정보에 근거하여 결정되며, 상기 제어하는 단계에서는, 상기 영 상에 상기 그래픽 객체가 오버랩되어 출력되도록 상기 디스플레이부를 제어하는 것을 특징으로 한다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 로봇 관제에 관한 것으로서, 원격으로 로봇을 관리하고 제어할 수 있는 로봇 관제 방법 및 시스템이 다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "기술이 발전함에 따라, 다양한 서비스 디바이스들이 나타나고 있으며, 특히 최근에는 다양한 작업 또는 서비스 를 수행하는 로봇에 대한 기술 개발이 활발하게 이루어지고 있다. 나아가 최근에는, 인공 지능 기술, 클라우드 기술 등이 발전함에 따라, 로봇의 활용도가 점차적으로 높아지고 있다. 한편, 로봇으로 다양한 작업 또는 서비스를 제공하기 위해서는, 로봇을 정확하게 제어하는 것이 매우 중요하다. 그러나, 로봇의 경우, 사용자가 일일이 로봇의 주변에서 로봇에 대한 제어를 수행하는 데에는 현실적인 한계가 있으므로, 원격에서 로봇을 관리하고 제어하는 기술에 대한 필요성이 점차적으로 중요해지고 있다. 이에, 대한민국 등록특허 제10-1305944호(랩어라운드 영상을 이용한 로봇 원격 제어를 위한 방법 및 이를 위한 장치)에서는, 로봇의 사방에 카메라를 구비하여, 로봇 주변의 영상을 획득하고, 이를 이용하여 원격으로 로봇을 관리하는 기술에 대하여 개시하고 있다.한편, 로봇에 구비된 카메라 만으로 로봇의 주행을 원격으로 관리하는 경우, 로봇의 주변 상황에 즉각적으로 대 처할 수 없는 문제가 발생할 수 있다. 이에, 로봇의 주행과 관련된 정보를 보다 직관적으로 제공할 수 있는 관 제 시스템에 대한 니즈가 여전히 존재한다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 로봇에 대한 관제 방법 및 시스템을 제공하는 것이다. 보다 구체적으로 본 발명은, 보다 직관적으로 로봇을 원격으로 관리하고, 제어할 수 있는 로봇 관제 방법 및 시스템을 제공하는 것이다. 보다 구체적으로 본 발명은 로봇 주행시 발생할 수 있는 위험 상황을 유연하게 대처할 수 있는 사용자 인터페이 스를 제공할 수 있는 로봇 관제 방법 및 시스템을 제공하는 것이다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "위에서 살펴본 과제를 해결하기 위하여 본 발명에 따른 로봇 관제 방법은, 로봇이 주행하는 공간에 대한 영상을 수신하는 단계, 상기 로봇으로부터 상기 로봇의 주행과 관련된 주행 정보를 수신하는 단계 및 상기 영상과 함께 상기 주행 정보에 대응되는 그래픽 객체가 출력되도록 디스플레이부를 제어하는 단계를 포함하고, 상기 그래픽 객체의 시각적인 특성은 상기 주행 정보에 근거하여 결정되며, 상기 제어하는 단계에서는, 상기 영상에 상기 그 래픽 객체가 오버랩되어 출력되도록 상기 디스플레이부를 제어하는 것을 특징으로 한다. 나아가, 본 발명에 따른 로봇 관제 시스템은, 디스플레이부, 로봇이 주행하는 공간에 대한 영상 및 상기 로봇의 주행과 관련된 주행 정보를 수신하는 통신부 및 상기 영상과 함께 상기 주행 정보에 대응되는 그래픽 객체가 출 력되도록 상기 디스플레이부를 제어하는 제어부를 더 포함하고, 상기 제어부는, 상기 영상에서 상기 로봇의 주 행 방향과 대응되는 위치에 상기 그래픽 객체가 오버랩되어 출력되도록 상기 디스플레이부를 제어하는 것을 특 징으로 한다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "위에서 살펴본 것과 같이, 본 발명에 따른 로봇 관제 방법 및 시스템은, 로봇으로부터 영상을 수신하고, 수신된 영상에 로봇의 주행 정보와 관련된 그래픽 객체를 함께 표시할 수 있다. 보다 구체적으로, 본 발명은, 그래픽 객체를 활용하여 로봇의 주행으로 인하여 발생할 수 있는 위험 상황(예를 들어, 충돌 위험 상황 등)을 대처할 수 있는 정보를 제공할 수 있다. 따라서, 로봇을 원격으로 관리하는 사용자 는, 그래픽 객체를 참조하여, 원격에서도 로봇을 안정적으로 관리하고 운용할 수 있다."}
{"patent_id": "10-2021-0082285", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소에는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설 명에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실 시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 본 발명은 로봇에 대한 관제 방법 및 시스템을 제공하는 것으로서, 보다 구체적으로, 보다 직관적으로 로봇을 원격으로 관리하고 제어할 수 있는 방법 및 시스템을 제공하는 것이다. 이하에서는, 첨부된 도면과 함께, 로봇 이 주행하는 공간 및 이를 둘러싼 로봇(robot) 관제 시스템에 대하여 살펴보도록 한다. 도 1 및 도 2는 본 발명 에 따른 로봇 관제 방법 및 시스템을 설명하기 위한 개념도들이다. 도 1에 도시된 것과 같이, 기술이 발전함에 따라 로봇의 활용도는 점차적으로 높아지고 있다. 종래 로봇은 특수 한 산업분야(예를 들어, 산업 자동화 관련 분야)에서 활용되었으나, 점차적으로 인간이나 설비를 위해 유용한 작업을 수행할 수 있는 서비스 로봇으로 변모하고 있다. 이와 같이 다양한 서비스를 제공할 수 있는 로봇은, 부여된 임무를 수행하기 위하여 도 1에 도시된 것과 같은 공간을 주행하도록 이루어질 수 있다. 로봇이 주행하는 공간의 종류에는 제한이 없으며, 필요에 따라 실내 공간 및 실외 공간 중 적어도 하나를 주행하도록 이루어 질 수 있다. 예를 들어, 실내 공간은 백화점, 공항, 호 텔, 학교, 빌딩, 지하철역, 기차역, 서점 등과 같이 다양한 공간일 수 있다. 로봇은, 이와 같이, 다양한 공간에 배치되어 인간에게 유용한 서비스를 제공하도록 이루어질 수 있다. 한편, 로봇을 이용하여 다양한 서비스를 제공하기 위해서는, 로봇을 정확하게 제어하는 것이 매우 중요한 요소 이다. 이에, 본 발명은 공간에 배치된 카메라를 함께 이용하여 로봇을 원격으로 보다 정확하게 제어할 수 있는 방법에 대하여 제안한다. 도 1에 도시된 것과 같이, 로봇이 위치한 공간에는 카메라가 배치될 수 있다. 도시와 같이, 공간에 배치된 카메라의 수는 그 제한이 없다. 도시와 같이, 공간에는 복수개의 카메라들(20a, 20b, 20c)이 배 치될 수 있다. 공간에 배치된 카메라의 종류는 다양할 수 있으며, 본 발명에서는 특히 공간에 배치된 CCTV(closed circuit television)를 활용할 수 있다. 도 2에 도시된 것과 같이, 본 발명에 의하면 로봇 관제 시스템에서, 로봇(R)을 원격으로 관리하고, 제어할 수 있다. 본 발명에서 “관제”는 원격에서 로봇을 관리, 감시하거나, 제어할 수 있다는 의미를 포함하는 포괄적인 용어 로서 사용될 수 있다. 즉, 관제의 대상이 되는 로봇(R)은, 본 발명에 따른 로봇 관제 시스템으로부터 수신 되는 제어명령에 근거하여, 동작(예를 들어, 주행)이 제어될 수 있다. 본 발명에 따른 로봇 관제 시스템은 공간에 배치된 카메라(20, 예를 들어, CCTV)에서 수신되는 영상을 활용하여, 로봇을 원격에서 감시하거나, 적절한 제어를 수행할 수 있다. 이하에서는, 공간에 배치된 CCTV로부터 수신되는 영상을 로봇 관제에 활용하는 방법에 대하여 보다 구체적으로 살펴본다. 도 2에 도시된 것과 같이, 본 발명에 따른 로봇 관제 시스템은, 통신부, 저장부, 디스플레이부 , 입력부 및 제어부 중 적어도 하나를 포함할 수 있다. 통신부는, 공간에 배치된 다양한 디바이스와 유선 또는 무선으로 통신하도록 이루어질 수 있다. 통신 부는 도시와 같이 로봇(R)과 통신할 수 있다. 통신부는 로봇(R)과의 통신을 통해, 로봇(R)에 구비된 카메라로부터 촬영되는 영상을 수신하도록 이루어질 수 있다. 나아가, 통신부는 카메라와의 직접적인 통신을 수행할 수 있다. 나아가, 통신부는 카메라를 제어하는 영상 관제 시스템과 통신하도록 이루어질 수 있다. 영상 관제 시스템과 통신부 사이 에 통신이 이루어지는 경우, 로봇 관제 시스템은 통신부를 통해 영상 관제 시스템으로부터 카 메라에서 촬영되는(또는 수신되는) 영상을 수신할 수 있다. 나아가, 통신부는 적어도 하나의 외부 서버(또는 외부 저장소, 200)와 통신하도록 이루어질 수 있다. 여기 에서, 외부 서버는, 도시된 것과 같이, 클라우드 서버 또는 데이터베이스 중 적어도 하나를 포 함하도록 구성될 수 있다. 한편, 외부 서버에서는, 제어부의 적어도 일부의 역할을 수행하도록 구성 될 수 있다. 즉, 데이터 처리 또는 데이터 연산 등의 수행은 외부 서버에서 이루어지는 것이 가능하며, 본 발명에서는 이러한 방식에 대한 특별한 제한을 두지 않는다. 한편, 통신부는 통신하는 디바이스의 통신 규격에 따라 다양한 통신 방식을 지원할 수 있다. 예를 들어, 통신부는, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, DLNA(Digital Living Network Alliance), WiBro(Wireless Broadband), WiMAX(World Interoperability for Microwave Access), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G(5th Generation Mobile Telecommunication ), 블루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra-Wideband), ZigBee, NFC(Near Field Communication), Wi-Fi Direct, Wireless USB(Wireless Universal Serial Bus) 기술 중 적어도 하나를 이용하여, 공간 내외에 위치한 디바 이스(클라우드 서버 포함)와 통신하도록 이루어질 수 있다. 다음으로 저장부는, 본 발명과 관련된 다양한 정보를 저장하도록 이루어질 수 있다. 본 발명에서 저장부 는 로봇 관제 시스템 자체에 구비될 수 있다. 이와 다르게, 저장부의 적어도 일부는, 클라우드 서버 및 데이터베이스 중 적어도 하나를 의미할 수 있다. 즉, 저장부는 본 발명에 따른 로봇 관 제를 위하여 필요한 정보가 저장되는 공간이면 충분하며, 물리적인 공간에 대한 제약은 없는 것으로 이해될 수 있다. 이에, 이하에서는, 저장부, 클라우드 서버 및 데이터베이스를 별도로 구분하지 않고, 모 두 저장부라고 표현하도록 한다. 이때, 클라우드 서버는 “클라우드 저장소”를 의미할 수 있다. 나 아가, 저장부는 로봇 관제 시스템에 대한 정보 뿐만 아니라, 영상 관제 시스템과 관련된 다양 한 정보를 저장하도록 이루어질 수 있다. 먼저, 저장부에는, 로봇(R)에 대한 정보가 저장될 수 있다. 로봇(R)에 대한 정보는 매우 다양할 수 있으며, 로봇(R)에 대한 정보는 일 예로서, i)공간에 배치된 로봇 (R)을 식별하기 위한 식별 정보(예를 들어, 일련번호, TAG 정보, QR코드 정보 등), ii)로봇(R)에 부여된 임무 정보, iii)로봇(R)에 설정된 주행 경로 정보, iv)로봇(R)의 위치 정보, v)로봇(R)의 상태 정보(예를 들어, 전원 상태, 고장 유무, 배터리 상태 등), vi)로봇(R)에 구비된 카메라로부터 수신된 영상 정보 등이 존재할 수 있다. 다음으로, 저장부에는, 카메라에 대한 정보가 저장될 수 있다. 카메라에 대한 정보를 매우 다양할 수 있으며, 카메라에 대한 정보는, i) 각각의 카메라(20a, 20b, 20c, 20d…)의 식별 정보(예를 들어, 일련번호, TAG 정보, QR코드 정보 등), ii) 각각의 카메라(20a, 20b, 20c, 20d…)의 배치 위치 정보(예를 들어, 공간 내에서 각각의 카메라(20a, 20b, 20c, 20d…)가 어느 위치에 배 치되었는지에 대한 정보), iii) 각각의 카메라(20a, 20b, 20c, 20d…)의 화각 정보(anle of view, 예를 들어, 각각의 카메라(20a, 20b, 20c, 20d…)가 공간의 어느 뷰를 촬영하고 있는지에 대한 정보), iv) 각각의 카메라 (20a, 20b, 20c, 20d…)의 상태 정보(예를 들어, 전원 상태, 고장 유무, 배터리 상태 등), vi) 각각의 카메라 (20a, 20b, 20c, 20d…)로부터 수신된 영상 정보 등이 존재할 수 있다. 한편, 위에서 열거된 카메라에 대한 정보는 각각의 카메라(20a, 20b, 20c, 20d…)를 기준으로 서로 매칭되 어 존재할 수 있다. 예를 들어, 저장부에는, 특정 카메라(20a)의 식별정보, 위치 정보, 화각 정보, 상태 정보, 및 영상 정보 중 적어도 하나가 매칭되어 매칭 정보로서 존재할 수 있다. 이러한 매칭 정보는, 추후 영상을 보고자 하는 위치 가 특정되는 경우, 해당 위치의 카메라를 특정하는데 유용하게 활용될 수 있다. 다음으로 저장부에는, 공간에 대한 지도(map, 또는 지도 정보)가 저장될 수 있다. 여기에서, 지도는, 2차원 또는 3차원 지도 중 적어도 하나로 이루어 질 수 있다. 공간에 대한 지도는 로봇(R)위 현재 위치를 파악하거나, 로봇의 주행 경로를 설정하는데 활용될 수 있는 지도를 의미할 수 있다. 특히, 본 발명에 따른 로봇 관제 시스템에서는, 로봇(R)에서 수신되는 영상을 기반으로 로봇(R)의 위치를 파악할 수 있다. 이를 위하여, 저장부에 저장된 공간에 대한 지도는 영상에 기반하여 위치를 추정할 수 있도록 하는 데이터로 구성될 수 있다. 이때, 공간에 대한 지도는 사전에 공간을 이동하는 적어도 하나의 로봇에 의해, SLAM(Simultaneous Localization and Mapping)에 기반하여 작성된 지도일 수 있다. 한편, 위에서 열거한 정보의 종류 외에도 저장부에는 다양한 정보가 저장될 수 있다. 다음으로 디스플레이부는 로봇(R)에 구비된 카메라 및 공간에 배치된 카메라 중 적어도 하나로부 터 수신되는 영상을 출력하도록 이루어질 수 있다. 디스플레이부는 로봇(R)을 원격으로 관리하는 관리자의 디바이스에 구비된 것으로서, 도 2에 도시된 것과 같이, 원격 관제실(100a)에 구비될 수 있다. 나아가, 이와 다 르게, 디스플레이부는 모바일 디바이스에 구비된 디스플레이일 수 있다. 이와 같이, 본 발명에서는 디스플 레이부의 종류에 대해서는 제한을 두지 않는다. 다음으로, 입력부는 사용자(또는 관리자)로부터 입력되는 정보의 입력을 위한 것으로서, 입력부는 사 용자(또는 관리자)와 로봇 관제 시스템 사이의 매개체가 될 수 있다. 보다 구체적으로, 입력부는 사 용자로부터 로봇(R)을 제어하기 위한 제어 명령을 수신하는 입력 수단을 의미할 수 있다. 이때, 입력부의 종류에는 특별한 제한이 없으며, 입력부는 기계식 (mechanical) 입력수단(또는, 메커 니컬 키, 예를 들어, 마우스(mouse), 조이스틱(joy stic), 물리적인 버튼, 돔 스위치 (dome switch), 조그 휠, 조그 스위치 등) 및 터치식 입력수단 중 적어도 하나를 포함할 수 있다. 일 예로서, 터치식 입력수단은, 소프트 웨어적인 처리를 통해 터치스크린에 표시되는 가상 키(virtual key), 소프트 키(soft key) 또는 비주얼 키 (visual key)로 이루어지거나, 상기 터치스크린 이외의 부분에 배치되는 터치 키(touch key)로 이루어질 수 있 다. 한편, 상기 가상키 또는 비주얼 키는, 다양한 형태를 가지면서 터치스크린 상에 표시되는 것이 가능하며, 예를 들어, 그래픽(graphic), 텍스트(text), 아이콘(icon), 비디오(video) 또는 이들의 조합으로 이루어질 수 있다. 이때, 입력부가 터치 스크린을 포함하는 경우, 디스플레이부는 터치 스크린으로 이루어 질 수 있다. 이 경우, 디스플레이부는 정보를 출력하는 역할과, 정보를 입력받는 역할을 모두 수행할 수 있다. 다음으로 제어부는 본 발명과 관련된 로봇 관제 시스템의 전반적인 동작을 제어하도록 이루어질 수 있다. 제어부는 위에서 살펴본 구성요소들을 통해 입력 또는 출력되는 신호, 데이터, 정보 등을 처리하거 나 사용자에게 적절한 정보 또는 기능을 제공 또는 처리할 수 있다. 특히, 제어부는 로봇(R)에 구비된 카메라로부터 수신되는 영상과, 공간에 배치된 카메라로부터 수 신되는 영상 중 적어도 하나를 이용하여, 로봇(R)을 관제할 수 있는 디스플레이 환경을 제공할 수 있다. 제어부는 로봇(R)의 현재 위치 정보에 기반하여, 로봇(R)이 위치한 공간에 배치된 적어도 하나의 카메라를 특정할 수 있다. 특히, 로봇(R)에 구비된 카메라로부터 수신되는 영상으로부터 로봇(R)이 공간 상에서 위치한 위치 정보를 추출할 수 있다. 그리고, 제어부는 저장부로부터 상기 추출된 로봇(R)의 위치 정보에 대 응되는 카메라의 특정할 수 있다. 여기에서, 상기 특정된 카메라는, 로봇의 위치 정보에 대응되는 장소에 배치된 카메라일 수 있다. 더 나아가, 상기 특정된 카메라는 로봇(R)의 주행 방향에 대응되는 영역을 화각으로 하는 카메라일 수 있다. 한편, 제어부는 상기 로봇으로부터 수신된 영상과 상기 특정된 카메라로부터 수신된 영상을 디스플레이부 상에 함께 출력되도록 디스플레이부를 제어할 수 있다. 나아가, 제어부는 입력부를 통해 상기 디스플레이부에 대해 입력되는 사용자 입력에 근거하여, 로봇(R)에 대한 원격 제어 또한 수행할 수 있다. 한편, 위의 설명에서는, 제어부에서 로봇(R)의 위치를 추정하는 예에 대하여 설명하였으나, 본 발명은 이 에 한정되지 않는다. 즉, 로봇(R)의 위치 추정은 로봇(R) 자체에서 이루어질 수 있다. 즉, 로봇(R)은 로봇(R)자체에서 수신되는 영상에 근거하여, 앞서 살펴본 방식으로 현재 위치를 추정할 수 있다. 그리고, 로봇(R)은, 추정된 위치 정보를 제어부에 전송할 수 있다. 이 경우, 제어부는 로봇으로부터 수신되는 위치 정보 에 기반하여, 일련의 제어를 수행할 수 있다. 한편, 영상 관제 시스템은, 공간 내 배치된 적어도 하나의 카메라를 제어하도록 이루어 질 수 있 다. 도시와 같이, 공간에는 복수의 카메라(20a, 20b, 20c, 20d, …)가 배치될 수 있다. 이러한 복수의 카메 라(20a, 20b, 20c, 20d, …)는 공간 내에서 각각 서로 다른 위치에 배치될 수 있다. 이와 같이, 공간의 서로 다른 위치에 복수의 카메라(20a, 20b, 20c, 20d, …)가 배치되어 있으므로, 로봇 관제 시스템에서는 이러한 복수의 카메라(20a, 20b, 20c, 20d, …)를 이용하여, 로봇(R)을 원격으로 관리 할 수 있다. 영상 관제 시스템은 로봇 관제 시스템과의 상호 통신을 통하여, 로봇(R)의 관제에 필요한 정보를 로 봇 관제 시스템에 제공할 수 있다. 앞서 저장부의 구성에서 살펴본 것과 같이, 영상 관제 시스템 의 저장부에는, 카메라에 대한 다양한 정보를 저장하도록 이루어질 수 있다. 카메라에 대한 정보 를 매우 다양할 수 있으며, 카메라에 대한 정보는, i) 각각의 카메라(20a, 20b, 20c, 20d…)의 식별 정보 (예를 들어, 일련번호, TAG 정보, QR코드 정보 등), ii) 각각의 카메라(20a, 20b, 20c, 20d…)의 배치 위치 정 보(예를 들어, 공간 내에서 각각의 카메라(20a, 20b, 20c, 20d…)가 어느 위치에 배치되었는지에 대한 정보), iii) 각각의 카메라(20a, 20b, 20c, 20d…)의 화각 정보(anle of view, 예를 들어, 각각의 카메라(20a, 20b, 20c, 20d…) 가 공간의 어느 뷰를 촬영하고 있는지에 대한 정보), iv) 각각의 카메라(20a, 20b, 20c, 20d…)의 상태 정보(예를 들어, 전원 상태, 고장 유무, 배터리 상태 등), vi) 각각의 카메라(20a, 20b, 20c, 20d…)로부 터 수신된 영상 정보 등이 존재할 수 있다. 한편, 위에서 열거된 카메라에 대한 정보는 각각의 카메라(20a, 20b, 20c, 20d…)를 기준으로 서로 매칭되 어 존재할 수 있다. 예를 들어, 영상 관제 시스템의 저장부에는, 특정 카메라(20a)의 식별정보, 위치 정보, 화각 정보, 상태 정보, 및 영상 정보 중 적어도 하나가 매칭되어 매칭 정보로서 존재할 수 있다. 이러한 매칭 정보는, 추후 영상 을 보고자 하는 위치가 특정되는 경우, 해당 위치의 카메라를 특정하는데 유용하게 활용될 수 있다. 한편, 이하의 설명에서는 위에서 살펴본 카메라의 정보가, 어느 저장부(또는 저장소)에 저장되어 있는지에 대한 구분 없이, 설명의 편의를 위하여, 저장부의 구성에 저장되었음을 예를 들어 설명하도록 한다. 즉, 카메라 에 대한 정보는 상황에 따라, 다양한 저장부에 저장될 수 있으므로, 본 발명에서는 이에 대한 특별한 제한을 두 지 않는다. 한편, 위의 설명에 의하면, 본 발명에서, 영상 관제 시스템과 로봇 관제 시스템을 별개의 구성으로 설명하였다. 그러나, 본 발명은 이에 제한되지 않으며, 영상 관제 시스템과 로봇 관제 시스템은 하 나의 통합된 시스템으로 이루어질 수 있다. 이 경우, 영상 관제 시스템은 “카메라부”의 구성으로 명명 되는 것 또한 가능하다. 이하에서는, 로봇(R)에서 수신되는 영상에 기반하여 로봇(R)의 현재 위치를 추정하는 방법에 대하여 첨부된 도 면과 함께 보다 구체적으로 살펴본다. 도 3은 본 발명에 따른 로봇 관제 방법 및 시스템에서, 로봇에서 수집되 는 영상 및 로봇의 현재 위치를 추정하는 방법을 설명하기 위한 개념도이다. 앞서 살펴본 것과 같이, 본 발명에 따른 제어부는 로봇(R)에 구비된 카메라(미도시됨)를 이용하여 공간 에 대한 영상을 수신하고, 수신된 영상으로부터 로봇의 위치를 추정하는 Visual Localization수행하도록 이 루어진다. 이때, 로봇(R)에 구비된 카메라는 공간에 대한 영상, 즉, 로봇(R) 주변에 대한 영상을 촬영(또는 센싱)하도록 이루어진다. 이하에서는, 설명의 편의를 위하여, 로봇(R)에 구비된 카메라를 이용하여 획득된 영상 을 “로봇 영상”이라고 명명하기로 한다. 그리고, 공간에 배치된 카메라를 통하여 획득된 영상을 “공간 영상”이라고 명명하기로 한다. 제어부는 도 3의 (a)에 도시된 것과 같이, 로봇(R)에 구비된 카메라를 통하여 로봇 영상을 획득하도록 이 루어진다. 그리고, 제어부는 획득된 로봇 영상을 이용하여, 로봇(R)의 현재 위치를 추정할 수 있다. 제어부는 로봇 영상과 저장부에 저장된 지도 정보를 비교하여, 도 3의 (b)에 도시된 것과 같이, 로봇 (R)의 현재 위치에 대응하는 위치 정보(예를 들어, “3층 A구역 (3, 1, 1)”를 추출할 수 있다. 앞서 살펴본 것과 같이, 본 발명에서 공간에 대한 지도는 사전에 공간을 이동하는 적어도 하나의 로봇 에 의해, SLAM(Simultaneous Localization and Mapping)에 기반하여 작성된 지도일 수 있다. 특히, 공간 에 대한 지도는, 영상 정보를 기반으로 생성된 지도일 수 있다. 즉, 공간에 대한 지도는 vision(또는 visual)기반의 SLAM기술에 의하여 생성된 지도일 수 있다. 따라서, 제어부는 로봇(R)에서 획득된 로봇 영상에 대해 도 3의 (b)에 도시된 것과 같이 좌표 정보 (예를 들어, (3층, A구역(3, 1,1,))를 특정할 수 있다. 이와 같이, 특정된 좌표 정보는 곧, 로봇(R)의 현재 위 치 정보가 될 수 있다. 이때, 제어부는, 로봇(R)에서 획득된 로봇 영상과 vision(또는 visual)기반의 SLAM 기술에 의하여 생 성된 지도를 비교함으로써, 로봇(R)의 현재 위치를 추정할 수 있다. 이 경우, 제어부는 i)로봇 영상 과 기 생성된 지도를 구성하는 이미지들 간의 이미지 비교를 이용하여, 로봇 영상과 가장 비슷한 이미지를 특정하고, ii)특정된 이미지에 매칭된 위치 정보를 획득하는 방식으로 로봇(R)의 위치 정보를 특정할 수 있다. 이와 같이, 제어부는 도 3의 (a)에 도시된 것과 같이, 로봇(R)에서 로봇 영상이 획득되면, 획득된 로봇 영 상을 이용하여, 로봇의 현재 위치를 특정할 수 있다. 앞서 살펴본 것과 같이, 제어부는 저장부에 기 저장된 지도 정보(예를 들어, “참조 맵”으로도 명명 가능)로부터, 상기 로봇 영상에 대응되는 위치 정보(예를 들어, 좌표 정보)를 추출할 수 있다. 한편, 위의 설명에서는, 제어부에서 로봇(R)의 위치를 추정하는 예에 대하여 설명하였으나, 앞서 살펴본 것과 같이, 로봇(R)의 위치 추정은 로봇(R) 자체에서 이루어질 수 있다. 즉, 로봇(R)은 로봇(R) 자체에서 수신 되는 영상에 근거하여, 앞서 살펴본 방식으로 현재 위치를 추정할 수 있다. 그리고, 로봇(R)은, 추정된 위치 정 보를 제어부에 전송할 수 있다. 이 경우, 제어부는 로봇으로부터 수신되는 위치 정보에 기반하여, 일 련의 제어를 수행할 수 있다. 이와 같이, 로봇 영상으로부터 로봇(R)의 위치 정보가 추출되면, 제어부는 상기 위치 정보와 대응되는 장 소에 배치된 적어도 하나의 카메라를 특정할 수 있다. 제어부는 저장부에 저장된 카메라와 관련 된 매칭 정보로부터, 상기 위치 정보에 대응되는 장소에 배치된 카메라를 특정할 수 있다. 그리고, 제어부는, 로봇(R)의 관제를 위하여, 로봇(R) 자체에서 획득되는 로봇 영상 뿐만 아니라, 로봇 (R)이 위치한 공간에 배치된 카메라로부터 획득된 영상을 디스플레이부에 함께 출력시킬 수 있다. 한편, 로봇(R)의 식별 정보는 도 4에 도시된 것과 같이, 로봇(R)에 구비된 식별 표지(또는 식별 마크)에 근거하 여, 영상으로부터 추출될 수 있다. 도 4의 (a), (b) 및 (c)에 도시된 것과 같이, 로봇(R)의 식별 표지(301. 302. 303)는 로봇의 식별 정보를 포함할 수 있다. 도시와 같이, 식별 표지(301. 302. 303)는 바코드 (barcode, 301), 일련 정보(또는 시리얼 정보, 302), QR코드로 구성될 수 있으며, 바코드 (barcode, 301), 일련 정 보(또는 시리얼 정보, 302), QR코드는 각각 로봇의 식별 정보를 포함하도록 이루어질 수 있다. 로봇의 식별 정보는, 로봇 각각을 구분하기 위한 정보로서, 동일한 종류의 로봇이더라도, 서로 다른 식별 정보 를 가질 수 있다. 한편, 식별 표지를 구성하는 정보는, 위에서 살펴본 바코드, 일련 정보, QR코드 외에도 다양 하게 구성될 수 있다. 제어부는 카메라로부터 수신되는 영상으로부터 위에서 살펴본 식별 표지에 근거하여, 로봇(R)의 식별 정보 추출하여, 카메라에 의해 촬영된 로봇(R)을 특정하고, 공간에서의 특정된 로봇(R)의 위치를 파악 할 수 있다. 한편, 본 발명에서 카메라에 촬영된 로봇(R)을 특정하는 방법은 다양할 수 있다. 위에서 살펴본 것과 같이, 제어부는 영상으로부터 로봇(R)의 식별 정보를 추출하여, 로봇(R)을 특정하는 것이 가능하다. 이 외에도, 제어부는 영상이 촬영된 시간 및 영상을 촬영한 카메라에 매칭된 위치 정보 중 적어도 하나를 이용하 여, 카메라에 촬영된 로봇(R)의 현재 위치를 특정할 수 있다. 한편, 본 발명에서, 카메라에 의해 촬영된 로봇(R)을 특정하는 방법은 다양할 수 있다. 위에서 살펴본 것과 같이, 제어부는 영상으로부터 로봇(R)의 식별 정보를 추출하여, 로봇(R)을 특정하는 것이 가능하다. 이 외 에도, 제어부는 영상이 촬영된 시간, 영상을 촬영한 카메라에 매칭된 위치 정보 및 로봇(R)의 위치 정 보 중 적어도 하나를 이용하여, 카메라에 촬영된 로봇(R)을 특정할 수 있다. 제어부는 로봇(R)의 위치 정보를 이용하여, 영상이 촬영된 시간에, 영상을 촬영한 카메라의 화각에 대 응되는 영역에 위치했던 로봇(R)을 특정할 수 있다. 이때, 로봇(R)의 위치 정보는 로봇(R)으로부터 수신되거나,제어부에 의하여 획득될 수 있다. 여기에서, 로봇(R)의 위치 정보는, 로봇(R)의 절대 위치에 대한 정보를 포함할 수 있다. 로봇(R)의 위치정보는, 영상 기반으로 획득될 수 있으며, 보다 구체적으로, 로봇(R)은, 로봇(R)에 구비된 카메 라(미도시됨)를 이용하여 공간에 대한 영상을 수신하고, 수신된 영상으로부터 로봇의 위치를 추정하는 Visual Localization수행하도록 이루어질 수 있다. 이때, 로봇(R)에 구비된 카메라는 공간에 대한 영상, 즉, 로봇(R) 주변에 대한 영상을 촬영(또는 센싱)하도록 이루어진다. 로봇(R) 또는 제어부는, 로봇(R)에서 촬영된 영상과 저장부에 저장된 지도 정보를 비교하여, 로봇 (R)의 현재 위치에 대응하는 위치 정보를 추출할 수 있다. 앞서 살펴본 것과 같이, 본 발명에서 공간에 대한 지도는 사전에 공간을 이동하는 적어도 하나의 로봇 에 의해, SLAM(Simultaneous Localization and Mapping)에 기반하여 작성된 지도일 수 있다. 특히, 공간 에 대한 지도는, 영상 정보를 기반으로 생성된 지도일 수 있다. 이와 같이, 로봇(R) 또는 제어부에서 로봇의 위치 정보가 추출되면, 제어부는, 이러한 위치 정보로부 터, 카메라에 의해 촬영된 로봇을 특정할 수 있다. 저장부에는 로봇(R)의 식별 정보, 로봇(R)의 위치 정보 및 로봇(R)이 해당 위치에 머물렀던 시간 정보가 매칭되어 저장될 수 있다. 이러한 매칭된 정보는 로봇(R)마다 별개로 존재할 수 있으며, 이를 타임 스탬프(time stamp)라고도 명명할 수 있다. 제어부는 i)카메라에서 로봇(R)에 대한 영상이 촬영된 시점 및 카메라의 위치 정보와 ii)저장부 에 저장된 로봇(R)의 타임 스탬프에 근거하여, 카메라에 의해 촬영된 로봇을 특정할 수 있다. 이와 같이, 로봇의 식별 정보는, 로봇의 위치 정보 및 카메라의 위치 정보를 이용하여, 추출(또는 특정)될 수 있다. 한편, 본 발명에서는 디스플레이부 상에, 로봇이 주행하는 공간에 대한 영상 및 로봇의 주행에 대한 주행 정보를 반영한 그래픽 객체를 함께 출력함으로써, 관리자가 로봇의 주행과 관련하여 발생할 수 있는 위험 상황 을 미리 인지할 수 있도록 하는 로봇 관제 방법 및 시스템을 제공할 수 있다. 여기에서, 위험 상황은, 로봇과 관련하여 안전 사고가 발생할 수 있는 상황을 의미할 수 있다. 예를 들어, 위험 상황은 공간에 위치한 객체와 충돌할 수 있는 상황을 의미할 수 있다. 여기에서, 객체는, 정적인 객체 및 동적인 객체 중 적어도 하나를 의미할 수 있다. 보다 구체적으로, 정적인 객체는, 공간에 배치된 사물(예를 들어, 테이블, 소파, 간판, 마네킹 등) 및 공간을 형성하는 구조물(예를 들어, 계단, 벽(wall) 등) 중 적어도 하나를 의미할 수 있다. 나아가, 동적인 객체는 보행자(또는 사람), 동물(예를 들어, 강아지, 고양이 등) 및 다른 로봇 중 적어도 하나 를 의미할 수 있다. 본 발명에 따른 로봇 관제 시스템의 제어부는, 로봇 또는 공간에 배치된 카메라로부터 수신된 영상에, 로봇의 주행에 대한 주행 정보에 따른 그래픽 객체를 중첩하여 표시할 수 있다. 따라서, 로봇의 주행을 원격으로 관리하는 관리자는 이러한 정보에 기반하여, 위험 상황을 방지할 수 있도록 원 격으로 로봇을 관리할 수 있다. 이하에서는, 첨부된 도면과 함께, 본 발명에 따른 로봇 관제 방법 및 시스템에 대하여 보다 구체적으로 살펴본 다. 도 5는 본 발명에 따른 로봇 관제 방법을 설명하기 위한 흐름도이고, 도 6a, 도 6b 및 도 6c는 로봇의 주행 과 관련된 그래픽 객체를 출력하는 방법을 설명하기 위한 개념도들이다. 나아가, 도 7a, 7b, 7c, 7d, 도 7e 및 도 7f는 로봇의 주행 정보에 근거하여 그래픽 객체의 시각적인 특성을 제어하는 방법을 설명하기 위한 개념도들 이다. 또한, 도 8은 로봇의 주행과 관련된 안내 메시지를 제공하는 방법을 설명하기 위한 개념도이다. 먼저, 본 발명에 따른 로봇 관제 방법에서는, 로봇이 주행하는 공간에 대한 영상을 수신하는 과정이 진행된다 (S510). 이때, 영상을 제공하는 주체는 다양할 수 있다. 일 예로서, 상기 영상은 로봇에 구비된 카메라로부터 촬영되어, 로봇으로부터 제공될 수 있다. 제어부는 통신부를 통해, 로봇으로부터, 영상을 수신할 수 있다. 다른 예로서, 상기 영상은 공간 내 배치된 카메라(예를 들어, CCTV)로부터 촬영되어, 공간내 배치된 카메라로부 터 제공될 수 있다. 이 경우, 상기 영상은, 앞서 살펴본 영상 관제 시스템에서, 본 발명에 따른 로봇 관 제 시스템으로 영상을 제공할 수 있다. 제어부는 통신부를 통해, 영상 관제 시스템으로부 터 상기 영상을 수신할 수 있다. 한편, 로봇이 주행하는 공간에 대한 영상은 로봇의 주행 방향에 대응하는 공간을 화각(angle of view)으로 하는 카메라로부터 수신된 영상일 수 있다. 이 경우, 도 6a에 도시된 것과 같이, 영상은, 로봇을 1인칭 시점으 로 하는 영상일 수 있다. 이 경우, 본 발명에서는, 상기 영상을 “1인칭 시점의 영상”이라고도 명명할 수 있다. 예를 들어, 도 6a에 도시된 것과 같이, 로봇이 주행하는 공간에 대한 1인칭 시점의 영상은, 로봇을 1인칭 시점으로 하여, 로봇의 주행방향을 향해, 로봇이 바라보고 있는 공간에 해당하는 영상일 수 있다. 이 경우, 상기 1인칭 시점의 영상은 로봇으로부터 수신되는 영상일 수 있다. 로봇이 정면을 기준으로 주행 한다고 가정하였을 때, 상기 1인칭 시점의 영상은 로봇의 정면에 구비된 카메라로부터 수신될 수 있다. 한편, 로봇이 주행하는 공간에 해당하는 영상은, 도 6b에 도시된 것과 같이, 로봇을 3인칭 시점으로 하는 영상 일 수 있다. 이 경우, 본 발명에서는, 상기 영상을 “3인칭 시점의 영상”이라고도 명명할 수 있다. 이러한 3인칭 시점의 영상은, 공간에 구비된 카메라(예를 들어, CCTV)로부터 수신되는 영상일 수 있다. 상 기 3인칭 시점의 영상은, 공간을 주행하는 로봇을 피사체로서 촬영한 영상으로서, 도 6b에 도시된 것과 같 이, 촬영된 로봇에 대응되는 로봇 이미지(또는 피사체 이미지)(R)를 포함할 수 있다. 한편, 본 발명에 따른 로봇 관제 방법에서는 로봇의 주행과 관련된 주행 정보를 수신하는 과정이 진행된다 (S520). 보다 구체적으로, 로봇의 주행과 주행 정보는, 로봇의 주행 속도, 주행 방향, 회전 방향, 회전 속도, 주행 시작, 주행 정지, 주행 종료와 같이, 로봇의 주행과 관련된 적어도 하나의 정보를 포함할 수 있다. 이러한 주행 정보는, 상기 로봇으로부터 수신될 수 있다. 제어부는 공간을 주행 중인 로봇으로부터, 통신 부를 통해, 실시간 또는 기 설정된 시간 간격으로 로봇의 주행 정보를 수신할 수 있다. 한편, 로봇의 주행 정보를 획득하는 방법은, 로봇으로부터 수신하는 방법 외에도 다양할 수 있다. 일 예로서, 제어부는 기 설정된 알고리즘에 의한 연산에 의하여, 로봇의 주행 정보를 획득할 수 있다. 보 다 구체적으로 제어부는 로봇의 위치 변위를 이용하여, 로봇의 주행 속도를 산출할 수 있다. 이 경우, 제 어부는 공간에 배치된 카메라로부터 수신되는 영상을 이용하여 로봇을 센싱하고, 센싱된 로봇의 위치 변위 에 근거하여 로봇의 주행 속도를 산출할 수 있다. 이와 같이, 로봇이 주행하는 공간에 대한 영상 및 로봇의 주행과 관련된 주행 정보가 수신되면, 본 발명에서는 디스플레이부 상에 수신된 영상 및 로봇의 주행 정보에 대응되는 그래픽 객체를 함께 출력하는 과정이 진행된다 (S530). 여기에서, 상기 영상 및 그래픽 객체가 출력되는 디스플레이부의 종류에는 제한이 없다. 일 예로서, 디스플레이 부는 로봇을 원격으로 관리하는 관리자의 디바이스에 구비된 것으로서, 도 2에 도시된 것과 같이, 원격 관제실 (100a)에 구비될 수 있다. 나아가, 이와 다르게, 디스플레이부는 모바일 디바이스에 구비된 디스플레이일 수 있 다. 제어부는, 수신된 영상에 그래픽 객체가 중첩되어 표시되도록 디스플레이부를 제어할 수 있다. 도 6a 및 도 6b에 도시된 것과 같이, 수신된 영상(610, 630)에는 그래픽 객체(620, 640)가 오버랩(overlap)될 수 있다. 이때, 영상(610, 630)에서 그래픽 객체(620, 640)가 차지하는 면적은, 로봇과의 거리 및 영상의 배율을 고려하 여 결정될 수 있다. 본 발명에서 그래픽 객체는, 로봇이 주행함으로 인하여 발생할 수 있는 위험 상황을 관리자 또는 보행자에게 인지시키기 위한 역할을 수행한다. 따라서, 그래픽 객체는, 로봇으로 인하여 위험 상황이 발생 할 수 있는 실제 공간에 대한 정보를 반영해야 한다. 이에, 제어부는 영상에 그래픽 객체를 오버랩할 때, 로봇과의 실제 거리가 반영되도록 그래픽 객체의 크기를 결정할 수 있다. 저장부는 로봇의 주행 정보(예를 들어, 주행 속도)에 근거하여, 위험 영역의 면적(또는 크기)에 대한 정보 가 저장되어 존재할 수 있다. 예를 들어, 저장부에는 로봇의 주행 속도를 기준으로 구분되는 매칭된 위험 영역의 속성에 대한 속성 정보를 포함할 수 있다. 속성 정보는 위험 영역의 반경 정보, 위험 영역의 형상, 위험 영역의 가로 길이 및 세로 길이 중 적어도 하나를 포함할 수 있다. 제어부는 이러한 속성 정보를 참조하여, 위험 영역의 실제 크기에 대응되도록 그래픽 객체의 출력 크기를 결정할 수 있다. 한편, 영상은(610, 630)은 로봇이 포함된 공간을 촬영한 영상으로서, 이러한 영상은, 공간의 크기를 소정 비율 로 축소한 배율을 갖는다. 따라서, 제어부는, 그래픽 객체의 출력 크기 역시, 영상이 축소된 배율과 동일한 배 율로 위험 영역의 면적을 축소함으로써 결정할 수 있다. 이하에서는, 그래픽 객체가 출력되는 예에 대하여 살펴본다. 일 예로서, 도 6a에 도시된 것과 같이, 제어부는 1인칭 시점의 영상에 그래픽 객체를 오버랩하 여 출력시킬 수 있다. 다른 예로서, 도 6b에 도시된 것과 같이, 제어부는 3인칭 시점의 영상에 그래픽 객체를 오버랩 하여 출력시킬 수 있다. 이 밖에도 제어부는 로봇의 주행 정보에 따른 시각적인 특성을 갖는 그래픽 객체에 해당하는 데이터를 로 봇(R)으로 전송할 수 있다. 제어부는 주행 정보에 따른 시각적인 특성이 반영된 그래픽 객체에 해당하는 이미지 데이터를 생성할 수 있다. 그리고, 생성된 이미지 데이터를 통신부를 통해 로봇(R)으로 전송할 수 있다. 로봇(R)에서는 수신한 이미지 데이터 기반하여 그래픽 객체를 출력할 수 있다. 나아가, 로봇에서는 자체 적으로, 그래픽 객체를 생성하는 것이 가능하며, 이 경우, 그래픽 객체를 생성하는 방법은, 후술되는 방법이 동 일하게 적용될 수 있다. 한편, 로봇(R)에서는 도 6c의 (a) 및 (b)에 도시된 것과 같이, 출력부를 이용하여, 로봇 주변에 그래픽 객체 (650, 660)를 출력할 수 있다. 이때, 출력부는 공간을 향하여 시각적이 정보를 출력하는 구성요소이면 무방하며, 출력부가 배치된 위치 및 종류에 대해서는 특별한 한정을 하지 않기로 한다. 예를 들어, 출력부는 LED(Light-Emitting Diode) 또는 프로젝터(projector) 등이 될 수 있다. 한편, 도6a 도 6b 및 도 6c에 도시된 것과 같이, 그래픽 객체(620, 640, 650, 660)는, 일 방향을 따라 순차적으 로 배열되는 복수의 영역(621, 622, 623, 641, 642, 643, 651, 652, 653, 661, 662, 663)을 포함할 수 있다. 그래픽 객체가 디스플레이부 또는 로봇에 의해 출력되는 경우, 상기 일 방향은 로봇(R)이 주행하는 주행 방향과 대응될 수 있다. 예를 들어, 도 6a와 같이, 1인칭 시점의 영상은 로봇이 주행하는 방향을 바라보는 영상이므로, 이 경우, 그래픽 객체는, 로봇의 주행 방향에 대응되는 일 방향을 따라 순차적으로 배열된 복수의 영역(621, 622, 623)을 포함할 수 있다. 이때, 디스플레이부에 출력되는 영상은, 로봇이 주행 방향을 따라 주행하고 있는 공간을 바라보고 있는 영상에 해당한다. 따라서, 영상 중 디스플레이부의 하측에 출력되는 부분에 해당하는 공간 상에서의 위치는, 영상 중 디스플레이부의 상측에 출력되는 부분에 해당하는 공간 상에서의 위치보다 로봇으로부터 가까운 공간에 해당할 수 있다. 따라서, 도 6a에 도시된 것과 같이, 영상 중, 복수의 영역(621, 622, 623) 중 디스플레이부의 하측에 가장 가깝게 출력되는 제1 영역이 오버랩된 부분은, 상기 제1 영역을 제외한 다른 영역(622, 623)이 오버 랩된 부분 보다, 영상에 대응되는 실제 공간 중 로봇으로부터 상대적으로 가장 가까운 위치에 해당하는 공 간일 수 있다. 다른 예를, 도 6b와 같이, 3인칭 시점의 영상은 로봇에 해당하는 로봇 이미지(또는 피사체 이미지)(R) 및 로봇의 주행 방향을 향하여 배치된 그래픽 객체를 포함할 수 있다. 이 경우, 그래픽 객체 역시, 로봇 의 주행 방향에 대응되는 일 방향을 따라 순차적으로 배열된 복수의 영역(641, 642, 643)을 포함할 수 있다. 또 다른 예를, 도 6c와 같이, 로봇에서 직접 그래픽 객체를 출력하는 경우, 로봇은 도 6c의 (a)에 도시된 것과 같이, 주행 방향을 향하여, 그래픽 객체를 출력할 수 있다. 도시와 같이, 그래픽 객체는, 로봇 의 주행 방향에 대응되는 일 방향을 따라 순차적으로 배열된 복수의 영역(651, 652, 653)을 포함할 수 있다. 한 편, 로봇의 주행 방향이 변경되는 경우, 그래픽 객체의 출력 위치 또한 변경될 수 있다. 예를 들어, 로봇이, 정면 방향을 향하여 주행하는 경우, 그래픽 객체는 로봇의 정면 방향을 향하는 제1 공간에 출력되고, 로봇이 후면 방향(예를 들어, 후진 하는 경우)을 향하여 주행하는 경우, 그래픽 객체는, 상기 제1 공간과 반대되는 제2 공간 에 출력될 수 있다. 나아가, 도 6c의 (b)에 도시된 것과 같이, 로봇은 주행 방향 뿐만 아니라, 로봇을 둘러싼 주변 영역에 대하여 그래픽 객체를 출력할 수 있다. 도시와 같이, 로봇을 기준으로 360 반경으로 그래픽 객체가 출력될 수 있 다. 이 경우에도 마찬가지로, 그래픽 객체는, 로봇의 주행 방향에 대응되는 일 방향을 따라 순차적으로 배 열된 복수의 영역(661, 662, 663)을 포함할 수 있다. 한편, 본 발명에서 그래픽 객체는, 위험 상황 알리기 위한 것으로서, 로봇의 주행 정보에 따라 그래픽 객체의 시각적인 특성이 달라질 수 있다. 앞서 살펴본 것과 같이, 위험 상황은, 로봇과 관련하여 안전 사고가 발생할 수 있는 상황을 의미할 수 있다. 예 를 들어, 위험 상황은 공간에 위치한 객체(정적인 객체 및 동적인 객체 중 적어도 하나)와 충돌할 수 있는 상황 을 의미할 수 있다. 제어부는 로봇의 주행 정보에 근거하여, 로봇이 주행하는 공간에서, 로봇에 의해 위험이 발생할 가능성이 높은 영역을 위험 영역으로 설정하고, 위험 영역보다 상대적으로 위험이 발생할 가능성이 낮은 영역을 안전 영 역으로 설정할 수 있다. 나아가, 위험 영역과 안전 영역 사이에는, 적어도 하나의 중간 영역이 존재할 수 있다. 그리고, 본 발명은, 이러한 주행 정보가 시각적 특성으로 반영된 그래픽 객체를 위의 도 6a, 도 6b 및 도 6c와 같은 방식으로 출력할 수 있다. 이하에서는, 그래픽 객체가 어느 시점의 영상에 출력되는지 또는 어느 주체에 의해 출력되었는지를 구분하지 않 고, 로봇의 주행 정보에 따라 그래픽 객체의 시각적인 특성이 결정되는 방법에 대하여 도 7a, 7b, 도 7c, 도 7d 및 도 7e와 함께 구체적으로 살펴본다. 제어부는 디스플레이부 상에 이러한 위험 영역 및 안전 영역에 대한 정보를 그래픽 객체를 통하여 표시함 으로써, 로봇을 관제하는 관리자에게, 로봇의 주행에 의하여 발생할 수 있는 위험 상황에 대한 정보를 직관적으 로 제공할 수 있다. 나아가, 도 6c에 살펴본 것과 같이, 로봇이 대한 그래픽 객체를 직접 표시하는 경우, 로봇 주변에 위치한 보행자(또는 사용자)는 이러한 그래픽 객체를 인식하고, 로봇으로부터 발생할 수 있는 위험 상황 에 대한 정보를 직관적으로 파악할 수 있다. 본 발명에서, 제어부는 로봇의 주행 정보에 근거하여 그래픽 객체의 시각적인 특성을 제어하여, 위험 상황 에 대한 정보를 출력할 수 있다. 이때, 로봇으로부터 발생할 수 있는 위험 상황은 로봇의 주행 정보에 근거하여 결정될 수 있다. 예를 들어, 로 봇의 주행 속도가 빠른 경우, 로봇의 주행 속도가 느린 경우보다 보행자와의 충돌 위험이 더 클 수 있다. 따라 서, 본 발명에서, 제어부는 로봇의 주행 정보에 근거하여 그래픽 객체의 시각적인 특성을 결정할 수 있다. 그래픽 객체의 시각적인 특성은, 다양한 시각적인 요소(또는 특징)에 의하여 정의될 수 있다. 그래픽 객체의 시 각적인 특성을 결정하는 시각적인 요소는, 예를 들어, i)그래픽 객체 내에서 복수의 영역들 간의 크기 비율(또 는 복수의 영역들이 각각 차지하는 면적), ii)그래픽 객체를 구성하는 복수의 영역들 각각의 색상, iii)그래픽 객체의 세로 길이, iv)그래픽 객체의 가로 길이(너비 또는 폭) 등이 존재할 수 있다. 제어부는 위에서 열거된 시각적인 요소들 중 적어도 하나를 이용하여, 로봇에 대한 위험 상황을 반영할 수 있다. 일 예로서 제어부는 복수의 영역들 간의 비율을 달리하여, 위험 상황에 대한 정도를 다르게 표시할 수 있 다. 다른 예로서, 제어부는 복수의 영역들 간의 색상을 달리하여, 위험 상황에 대한 정보를 다르게 표시할 수 있다. 보다 구체적으로, 7a 및 도 7b에 도시된 것과 같이, 그래픽 객체(710a, 710b)는, 일 방향을 따라 순차적으로 배 열되는 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b)을 포함할 수 있다. 그래픽 객체가 디스플레이부 또 는 로봇에 의해 출력되는 경우, 상기 일 방향은 로봇(R)이 주행하는 주행 방향과 대응될 수 있다. 본 발명에서 그래픽 객체(710a, 710b)에 포함된 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b)은 상기 로 봇의 주행과 관련된 서로 다른 위험 등급에 각각에 매칭될 수 있다. 그리고, 그래픽 객체(710a, 710b) 내에서 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b)은 높은 위험 등급에서 낮은 위험 등급 순으로 배열 향하여 점차적으로 멀어지는 방향으로 배열 수 있다. 이때, “위험도가 높은 위험 등급에서 위험도가 낮은 위험 등급순으로 배열 향하여 점차적으로 멀어지는 방향”은 로봇의 주행 방향일 수 있다. 그리고, “위험도가 높은 위험 등급에서 위험도가 낮은 위험 등급 순으로 배열 향하여 점차적으로 멀어지는 방향”은 공간 상에서 로봇으로부 터 점차적으로 멀어지는 방향에 대응될 수 있다. 제어부는 서로 다른 위험 등급 중 위험도가 가장 높은 제1 등급은, 상기 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b) 중 로봇과 가장 가깝게 위치한 제1 영역(711a, 711b)에 매칭시킬 수 있다. 이때, 도 6a에서 로봇과 가장 가까운 영역은, 디스플레이부의 하측과 가장 가깝게 배치된 영역일 수 있다. 제어부는, 상기 서로 다른 위험 등급 중 위험도가 가장 낮은 제2 등급은, 상기 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b) 중 상기 로봇과 가장 멀게 위치한 제2 영역(713a, 713b)에 매칭시킬 수 있다. 이때, 도 6a에서 로봇과 가장 먼 영역은, 디스플레이부의 상측과 가장 가깝게 배치된 영역일 수 있다. 제어부는 로봇의 주행 정보에 근거하여, 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b)이 그래픽 객 체(710a, 710b)에서 각각 차지하는 면적(또는 각각의 비율)을 제어할 수 있다. 앞서 살펴본 것과 같이, 상기 주행 정보는 상기 로봇의 주행 속도를 포함하고, 제어부는 로봇의 주행 속도 에 따라, 복수의 영역(711a, 712a, 713a, 711b, 712b, 713b)이 그래픽 객체(710a, 710b)에서 각각 차지하는 면 적(또는 각각의 비율)을 제어할 수 있다. 제어부는 로봇의 주행 속도가 커질수록(또는 로봇이 빠르게 주행할 수록) 로봇으로부터 가깝게 위치하는 영역에 오버랩되는 제1 영역(711a, 71b)의 크기가 커지도록 그래픽 객체에 대응되는 데이터를 생성할 수 있다. 이때, 제1 영역(711a, 71b)은 가장 높은 위험도에 해당하는 제1 등급에 매칭된 영역에 해당할 수 있다. 따라서, 제어부는 로봇의 주행 속도가 커질수록(또는 로봇이 빠르게 주행할 수록) 제1 영역(711a, 71b)의 크기가 커지도록 디스플레이부를 제어하거나, 이에 대응되는 데이터를 생성할 수 있다. 앞서도 언급한 것과 같이, 영상 에서 그래픽 객체가 오버랩되는 영역은, 실제 공간과 대응되는 영역에 해당할 수 있다. 이와 같이, 상기 로봇의 주행 속도가 커 질수록, 상기 그래픽 객체에서 상기 제1 영역이 차지하는 면적은 상기 제2 영역이 차지하는 면적에 비하여 상대적으로 커질 수 있다. 도 7a의 (a)에서의 로봇의 주행 속도는, 도 7a의 (b)에서의 로봇의 주행 속도보다 클 수 있다. 이 경우, 제어부는 그래픽 객체에서 도 7a의 (a)의 제1 영역 (711a)이 차지하는 면적을, 도 7a의 (b)의 제1 영역(711b)차지하는 면적보다 크도록 디스플레이부 또는 관련된 데이터를 생성할 수 있다. 따라서, 로봇의 주행 속도가 작아 질수록, 도 7a에 도시된 것과 같이, 그래픽 객체(710a, 710b)에서 상기 제1 영역(711a, 711b)이 차지하는 면적은 상기 제2 영역(713a, 713b)이 차지하는 면적에 비하여 상대적으로 작을 수 있다. 한편, 로봇이 정지한 경우, 그래픽 객체이 출력 크기는 가장 작을 수 있다. 한편, 저장부에는 로봇의 주행 속도에 따라, 그래픽 객체에서 각각의 영역이 차지하는 면적의 비율에 대한 정보가 저장되어 존재할 수 있다. 제어부는 로봇의 현재 주행 속도 및 저장부에 저장된 정보에 근거하여, 그래픽 객체에서 각각의 영역 들이 차지하는 면적을 결정할 수 있다. 이와 같이, 제어부는 출력되는 그래픽 객체의 크기가 동일하더라도, 로봇의 주행 속도에 근거하여, 그래픽 객체 내에서의 복수의 영역들 간의 면적을 다르게 제어함으로써, 로봇의 주행에 따른 위험도에 대한 정보를 관 리자 또는 보행자에게 제공할 수 있다. 한편, 제어부는, 그래픽 객체를 구성하는 복수의 영역들 각각을 서로 다른 색상으로 표시할 수 있다. 제어 부는, 복수의 영역들 각각을 서로 다른 색상으로 표시되도록 디스플레이부를 제어함으로써, 관리자 또는 보행자가 어느 영역이 가장 위험한 영역인지에 대한 정보를 제공할 수 있다. 따라서, 도 7a에 도시된 것과 같이, 위험도 등급이 가장 높은 제1 영역(711a, 711b)과 위험도 등급이 가능 낮은 제2 영역(713a, 713b)는 서로 다른 색상으로 표시될 수 있다. 한편, 위에서 살펴본 그래픽 객체의 시각적인 특성은 로봇의 주행 정보가 변경되는 것에 연동하여 실시간 또는 기 설정된 간격으로 변경될 수 있다. 따라서, 로봇이 주행 중에 속도가 변경되는 경우, 앞서 살펴본 복수의 영 역들의 면적 또한 로봇의 주행 속도에 연동하여 변경될 수 있다. 한편, 앞서 살펴본 것과 같이, 영상에서 그래픽 객체가 차지하는 면적은, 로봇과의 거리 및 영상의 배율을 고려 하여 결정될 수 있다. 본 발명에서 그래픽 객체는, 로봇이 주행함으로 인하여 발생할 수 있는 위험 상황을 관리자 또는 보행자에게 인지시키기 위한 역할을 수행한다. 따라서, 그래픽 객체는, 로봇으로 인하여 위험 상황이 발생할 수 있는 실제 공간에 대한 정보를 반영해야 한다. 이에, 제어부는 영상에 그래픽 객체를 오버랩할 때, 로봇과의 실제 거리가 반영되도록 그래픽 객체의 크기를 결정할 수 있다. 저장부는 로봇의 주행 정보(예를 들어, 주행 속도)에 근거하여, 위험 영역의 면적(또는 크기)에 대한 정보 가 저장되어 존재할 수 있다. 예를 들어, 저장부에는 로봇의 주행 속도를 기준으로 구분되는 매칭된 위험 영역의 속성에 대한 속성 정보를 포함할 수 있다. 속성 정보는 위험 영역의 반경 정보, 위험 영역의 형상, 위험 영역의 가로 길이 및 세로 길이 중 적어도 하나를 포함할 수 있다. 제어부는 이러한 속성 정보를 참조하여, 위험 영역의 실제 크기에 대응되도록 그래픽 객체의 출력 크기를 결정할 수 있다. 한편, 영상은 로봇이 포함된 공간을 촬영한 영상으로서, 이러한 영상은, 공간의 크기를 소정 비율로 축소한 배 율을 갖는다. 따라서, 제어부는, 그래픽 객체의 출력 크기 역시, 영상이 축소된 배율과 동일한 배율로 위 험 영역의 면적을 축소함으로써 결정할 수 있다. 제어부는, 로봇의 주행 정보(예를 들어, 주행 속도)에 근거하여, 그래픽 객체의 출력 크기를 결정할 수 있 다. 앞서 살펴본 것과 같이, 제어부는 저장부에 저장된 속성 정보를 참조하여, 그래픽 객체의 출력 크기를 결정할 수 있다. 따라서, 그래픽 객체의 길이 및 폭 중 적어도 하나는, 상기 로봇의 주행 속도에 근거하 여 변경될 수 있다. 예를 들어, 도 7b에 도시된 것과 같이, 제어부는, 로봇의 주행 속도에 근거하여, 그래픽 객체의 길이(H1, H2)를 다르게 제어할 수 있다. 로봇의 주행 속도가 큰 경우(또는 로봇이 빠르게 주행하는 경우), 로봇의 주행 속도가 작은 경우(또는 로봇이 느리게 주행하는 경우)보다, 공간에서 로봇의 이동 변위가 크게 되므로, 위험 영 역이 더 크게 설정되어야 할 필요가 있다. 따라서, 제어부는, 도 7b의 (a) 및 (b)에 도시된 것과 같이, 로봇의 주행 속도에 따라, 그래픽 객체(720a, 720b)의 길이를 다르게 설정할 수 있다. 따라서, 디스플레이부 또는 로봇에서는 로봇의 주행 속도에 따라, 서로 다른 길이를 갖는 그래픽 객체(720a, 720b)가 출력될 수 있다. 이 경우, 그래픽 객체에 포함된 복수의 영역들이 차지하는 비율 역시, 앞서 도 7a와 함께 살펴본 방식으로 제어 될 수 있다. 또 다른 예를 들어, 도 7c에 도시된 것과 같이, 제어부는, 로봇의 주행 속도에 근거하여, 그래픽 객체의 폭(W1, W2)을 다르게 제어할 수 있다. 로봇의 주행 속도가 큰 경우(또는 로봇이 빠르게 주행하는 경우), 로봇의 주행 속도가 작은 경우(또는 로봇이 느리게 주행하는 경우)보다, 공간으로의 접근을 주의할 필요가 있다. 따라 서, 로봇의 속도가 빠를 수록 위험 영역이 더 크게 설정되어야 할 필요가 있다. 따라서, 제어부는, 도 7c의 (a) 및 (b)에 도시된 것과 같이, 로봇의 주행 속도에 따라, 그래픽 객체(730a, 730b)의 길이를 다르게 설정할 수 있다. 따라서, 디스플레이부 또는 로봇에서는 로봇의 주행 속도에 따라, 서로 다른 폭을 갖는 그래픽 객체(730a, 730b)가 출력될 수 있다. 이 경우, 그래픽 객체에 포함된 복수의 영역들이 차지하는 비율 역시, 앞서 도 7a와 함께 살펴본 방식으로 제어 될 수 있다. 한편, 제어부는 7d에 도시된 것과 같이, 로봇의 주행 속도에 근거하여, 그래픽 객체의 길이(H1, H2) 및 그 래픽 객체의 폭(W1, W2) 모두를 다르게 제어할 수 있다. 제어부는, 도 7d의 (a) 및 (b)에 도시된 것과 같 이, 로봇의 주행 속도에 따라, 그래픽 객체(740a, 740b)의 길이 및 폭을 모두 다르게 설정할 수 있다. 따라서, 디스플레이부 또는 로봇에서는 로봇의 주행 속도에 따라, 서로 다른 길이 및 폭을 갖는 그래픽 객체(740a, 740b)가 출력될 수 있다. 한편, 제어부는 로봇의 주행에 따른 주행 속성 뿐만 아니라, 로봇의 물리적인 크기를 고려하여 그래픽 객 체의 출력 크기를 결정할 수 있다. 도 7e의 (a) 및 (b)에 도시된 것과 같이, 제어부는 로봇의 폭(R1W, R2W)에 따라, 그래픽 객체(750a, 750b)의 폭(W1, W2)을 다르게 설정할 수 있다. 저장부는 로봇의 폭을 기준으로 하는 그래픽 객체의 폭에 대한 정보가 저장되어 존재할 수 있다. 이러한 폭에 대한 정보는, 로봇의 폭에 따라 그래픽 객체의 최소 폭에 대한 임계 정보를 의미할 수 있다. 제어부 는 이러한 폭에 대한 임계 정보를 근거로, 그래픽 객체의 폭을 상기 임계 정보에 대응되는 폭과 같게 또는 크게제어할 수 있다. 도시와 같이, 제1 로봇(R1)의 폭(R1W)이 제2 로봇(R2)의 폭(R2W)보다 작은 경우, 제1 로봇(R 1)에 대응되어 출력되는 제1 그래픽 객체(750a)의 폭이, 제2 로봇(R2)에 대응되어 출력되는 제2 그래픽 객체 (750b)의 폭보다 작을 수 있다. 이와 같이, 본 발명에서는 로봇의 물리적인 크기를 고려하여, 그래픽 객체의 출력 크기를 제어함으로써, 로봇으 로 인하여 발생할 수 있는 안전 사고에 대비할 수 있다. 한편, 일 방향을 따라 전진하는 로봇이, 주행방향을 변경하는 경우(예를 들어, 로봇이 회전하는 경우), 위험 영 역의 위치가 변경될 수 있다. 예를 들어, 도 7f의 (a)에서 도 7f의 (b)와 같이, 로봇의 주행방향이 변경되는 순 간에서는, 로봇의 위험 영역의 위치 또한 변경될 수 있다. 본 발명에서는, 로봇의 주행 방향이 변경됨으로 인하 여, 로봇의 위험 영역의 위치가 변경되는 경우, 로봇의 주행 방향이 변경됨에 연동하여, 위험 영역을 나타내는 그래픽 객체의 출력 위치 또한 변경시킬 수 있다. 이 경우, 제어부는 도 7f의 (a) 및 (b)에 도시된 것과 같이, 로봇의 회전 방향을 따라, 그래픽 객체(710 f)의 시각적인 외관 또한 변경되도록 할 수 있다. 이 경우, 그래픽 객체(710f)는 도 7f의 (b)에 도시된 것과 같 이, 로봇의 회전하는 방향을 따라 향하도록 기울어지거나, 곡선을 형성할 수 있다. 이때, 제어부는 로봇의 회전 속도에 따라, 그래픽 객체(710f)의 기울기 또는 곡률 정도를 달리하여, 관리자에게, 로봇의 회전으로 인하 여 발생하는 위험 영역의 범위를 직관적으로 알릴 수 있다. 한편, 제어부는 로봇이 회전을 하는 동안에만, 도 7f의 (b)에 도시된 것과 같이, 그래픽 객체(710f)의 기 울기 또는 곡률 정도를 다르게 함으로써, 로봇이 회전하는 상태임을 알릴 수 있다. 그리고, 제어부는 로봇 이 회전을 완료하여, 로봇이 일 방향(예를 들어, 로봇의 정면이 향하는 방향)으로 주행을 진행하는 경우, 다시, 도 7f의 (a)에 도시된 것과 같이, 로봇의 정면 방향을 향하도록 그래픽 객체(710f)의 출력을 제어할 수 있다. 나아가, 제어부는 로봇의 회전 속도가 기 설정된 기준 조건을 만족하는 속도 인 경우에 한하여, 그래픽 객 체의 기울기 또는 곡률 정도를 변형할 수 있다. 이 때, 기 설정된 기준 조건은, 로봇의 회전 속도가 매우 큼(매 우 급작스럽게 회전)으로 인하여, 주변에 위험이 발생할 수 있을 정도의 속도로, 로봇이 회전하는 경우를 의미 할 수 있다. 이상에서는, 로봇의 주행 정보 또는 로봇의 물리적인 크기에 근거하여 그래픽 객체의 시각적 특성을 결정하는 방법에 대하여 살펴보았다. 본 발명에 따른 로봇 관제 방법 및 시스템에서는, 로봇의 주행 정보 뿐만 아니라, 공간에 위치한 객체의 특성을 고려하여, 그래픽 객체의 시각적 특성을 결정할 수 있다. 예를 들어, 제어부는 영상으로부터 공간에 위치한 객체를 추출할 수 있다. 여기에서, 객체는 사람, 동물과 같이 공간 내에서 이동을 할 수 있는 동적인 객체를 의미할 수 있다. 제어부는 이러한 동적인 객체를 고려 하여, 그래픽 객체의 출력 크기를 결정할 수 있다. 제어부는 영상으로부터 동적인 객체의 이동 속도 및 이동 방향을 고려하여, 그래픽 객체의 출력 크기를 결 정할 수 있다. 이때, 제어부는 그래픽 객체의 출력 크기는 그래픽 객체의 길이 및 폭 중 적어도 하나에 의 하여 정의될 수 있다. 나아가, 제어부는 동적인 객체의 이동 속도 및 이동 방향을 고려하여, 그래픽 객체 내에서 복수의 영역들 의 면적을 결정할 수 있다. 제어부는 로봇의 주행 정보 및 동적인 객체의 이동 정보(예를 들어, 이동 속도 및 이동 방향) 중 적어도 하나를 이용하여, 그래픽 객체의 시각적인 특성을 결정할 수 있다. 예를 들어, 제어부는 동적인 객체의 이동 속도가 커질수록, 그래픽 객체가 크게 출력되도록 디스플레이부 를 제어할 수 있다. 나아가, 제어부는 동적인 객체의 이동 속도가 커질수록 그래픽 객체에서, 위험도 등급 이 가장 높은 제1 등급에 매칭되는 영역의 커지도록 디스플레이부를 제어할 수 있다. 이 경우, 위험도 등급이 가장 낮은 제2 등급에 매칭되는 영역의 크기는 상대적으로 작아질 수 있다. 이때, 제어부는 동적인 객체가 로봇을 향하여 이동하는 경우에 한하여, 그래픽 객체의 크기 및 제1 등급에 매칭된 영역의 크기가 커지도록 디스플레이부를 제어할 수 있다. 동적인 객체가 로봇으로부터 멀어지는 방향으 로 이동하는 경우, 로봇 과의 충돌 가능성이 낮기 때문이다. 한편, 제어부는 동적인 객체가 로봇으로부터 멀어지는 방향으로 이동하는 경우, 그래픽 객체의 크기 및 제 1 등급에 매칭된 영역의 크기가 작아지도록 디스플레이부를 제어할 수 있다. 이 경우, 위험도 등급이 가장 높은제1 등급에 매칭되는 영역의 크기는 상대적으로 커질 수 있다. 위에서 살펴본 것과 같이, 본 발명에서 제어부는, 상기 영상으로부터 상기 공간에 위치한 동적인 객체를 센싱할 수 있다, 이때, 제어부는 동적인 객체와 로봇과의 거리를 산출하고, 산출된 거리 및 로봇의 이동 속도를 고려하여, 동적인 객체와 로봇과의 충돌 가능성을 예측할 수 있다. 한편, 이러한 충돌 가능성은 동적인 객체의 특성에 따라서 다르게 예측될 수 있다. 동적인 객체의 특성은 매우 다양할 수 있다. 예를 들어, 제어부는 동적인 객체가 어린이인 경우, 동적인 객체가 어른인 경우보다 충돌 가능성이 높다고 예측할 수 있다. 다른 예를 들어, 제어부는 동적인 객체의 이동 속도가 빠른 경우, 동적인 객체의 이동 속도가 느린 경우보 다 충돌 가능성이 높다고 예측할 수 있다. 또 다른 예를 들어, 제어부는 동적인 객체가 로봇을 향하여 이동하는 경우, 동적인 객체가 로봇으로부터 멀어지는 방향으로 이동하는 경우보다 충돌 가능성이 높다고 예측할 수 있다. 또 다른 예를 들어, 제어부는 공간에 다수의 동적인 객체가 위치함으로 인하여 공간이 혼잡한 경우, 공간 이 한산한 경우보다 로봇과 동적인 객체와의 충돌 가능성이 높다고 예측할 수 있다. 또 다른 예를 들어, 제어부는 공간에 위치한 동적인 객체가 로봇을 바라보는 방향으로 자세를 취하고 있는 경우, 동적인 객체가 로봇을 바라보지 않는 방향으로 자세를 취하고 있는 경우보다 충돌 가능성이 낮다고 예측 할 수 있다. 이와 같이, 제어부는 기 설정된 알고리즘에 근거하여, 공간에 위치한 동적인 객체의 특성을 고려하여, 로 봇과의 충돌 가능성을 예측할 수 있다. 이때, 공간에 위치한 동적인 객체는 로봇 또는 공간에 배치된 카메라로 부터 수신한 영상으로부터 센싱될 수 있다. 나아가, 상기 기 설정된 알고리즘은 인공지능 또는 딥러닝에 기반한 알고리즘으로 구성될 수 있다. 그리고, 이러한 충돌 가능성에 근거하여, 도 8에 도시된 것과 같이, 로봇의 주행과 관련된 가이드 정보를 영상과 함께 출력할 수 있다. 한편, 이러한 동적인 객체와의 충돌 가능성에 대한 정보는, 그래픽 객체의 크기를 결정하는 데에도 활용 될 수 있다. 제어부는 로봇과 동적인 객체 간의 충돌 가능성이 높을수록 그래픽 객체의 출력 크기 및 위험 도 등급이 가장 높은 제1 등급에 매칭된 영역의 크기가 커지도록 디스플레이부를 제어할 수 있다. 한편, 제어부 는, 이러한 특성이 반영된 이미지 데이터를 로봇으로 전송할 수 있으며, 로봇에서 상기 이미지 데이터에 근거하 여, 로봇과 동적인 객체 간의 충돌 가능성이 고려된 그래픽 객체를 출력할 수 있다. 위에서 살펴본 것과 같이, 본 발명에 따른 로봇 관제 방법 및 시스템은, 로봇으로부터 영상을 수신하고, 수신된 영상에 로봇의 주행 정보와 관련된 그래픽 객체를 함께 표시할 수 있다. 보다 구체적으로, 본 발명은, 그래픽 객체를 활용하여 로봇의 주행으로 인하여 발생할 수 있는 위험 상황(예를 들어, 충돌 위험 상황 등)을 대처할 수 있는 정보를 제공할 수 있다. 따라서, 로봇을 원격으로 관리하는 사용자 는, 그래픽 객체를 참조하여, 원격에서도 로봇을 안정적으로 관리하고 운용할 수 있다. 한편, 위에서 살펴본 본 발명은, 컴퓨터에서 하나 이상의 프로세스에 의하여 실행되며, 이러한 컴퓨터로 판독될 수 있는 매체에 저장 가능한 프로그램으로서 구현될 수 있다. 나아가, 위에서 살펴본 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드 또는 명령어로서 구 현하는 것이 가능하다. 즉, 본 발명은 프로그램의 형태로 제공될 수 있다. 한편, 컴퓨터가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 나아가, 컴퓨터가 읽을 수 있는 매체는, 저장소를 포함하며 전자기기가 통신을 통하여 접근할 수 있는 서버 또 는 클라우드 저장소일 수 있다. 이 경우, 컴퓨터는 유선 또는 무선 통신을 통하여, 서버 또는 클라우드 저장소 로부터 본 발명에 따른 프로그램을 다운로드 받을 수 있다.나아가, 본 발명에서는 위에서 설명한 컴퓨터는 프로세서, 즉 CPU(Central Processing Unit, 중앙처리장치)가 탑재된 전자기기로서, 그 종류에 대하여 특별한 한정을 두지 않는다. 한편, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2021-0082285", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 및 도 2는 본 발명에 따른 로봇 관제 방법 및 시스템을 설명하기 위한 개념도들이다. 도 3은 본 발명에 따른 로봇 관제 방법 및 시스템에서, 로봇에서 수집되는 영상 및 로봇의 현재 위치를 추정하 는 방법을 설명하기 위한 개념도이다. 도 4는 본 발명에 따른 로봇 관제 방법 및 시스템에서 로봇을 식별하는 방법을 설명하기 위한 개념도이다. 도 5는 본 발명에 따른 로봇 관제 방법을 설명하기 위한 흐름도이다. 도 6a, 도 6b 및 도 6c는 로봇의 주행과 관련된 그래픽 객체를 출력하는 방법을 설명하기 위한 개념도들이다. 도 7a, 7b, 7c, 7d, 도 7e 및 도 7f는 로봇의 주행 정보에 근거하여 그래픽 객체의 시각적인 특성을 제어하는 방법을 설명하기 위한 개념도들이다. 도 8은 로봇의 주행과 관련된 안내 메시지를 제공하는 방법을 설명하기 위한 개념도이다."}
