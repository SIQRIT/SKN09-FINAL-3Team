{"patent_id": "10-2019-7035521", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2020-0012879", "출원번호": "10-2019-7035521", "발명의 명칭": "상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및", "출원인": "베리존 페이턴트 앤드 라이센싱 인크.", "발명자": "게르바시오 윌리엄 패트릭"}}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "실세계 장면의 복수의 상이한 유리한 지점들을 갖도록 상기 실세계 장면에 관련하여 배치된 복수의 3-차원(3D)캡처 장치들로부터 병합된 현실 장면 캡처 시스템에 의해, 제1의 복수의 표면 데이터 프레임들을 포함하는 제1프레임 세트를 수신하는 단계로서, 상기 제1의 복수의 표면 데이터 프레임들은,동일한 특정 시점에, 상기 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치에 의해 그리고 상기 복수의 상이한유리한 지점들 내의 각각의 유리한 지점에서 캡처되고,상기 실세계 장면 내에 포함된 실세계 오브젝트의 표면들이 상기 특정 시점에 상기 각각의 3D 캡처 장치의 상기각각의 유리한 지점에서 보일 때, 상기 표면들의 컬러 데이터 및 깊이 데이터를 나타내는, 상기 제1 프레임 세트를 수신하는 단계;상기 병합된 현실 장면 캡처 시스템에 의해, 상기 복수의 3D 캡처 장치들로부터 수신된 상기 제1 프레임 세트및 다른 시점들에 캡처된 복수의 다른 프레임 세트들에 기초하여, 상기 복수의 3D 캡처 장치들 내의 상기 3D 캡처 장치들 각각에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 운송 스트림을 생성하는 단계; 및상기 병합된 현실 장면 캡처 시스템에 의해, 상기 운송 스트림에 기초하여, 병합된 현실 장면의 3D 공간 내에포함된 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성하는 단계로서, 상기 복수의 엔트리들은,상기 병합된 현실 장면 캡처 시스템에 통신가능하게 연결된 자산 저장 시스템 내에 저장된 가상 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에 적어도 부분적으로 정의된 상기 가상 오브젝트,상기 복수의 3D 캡처 장치들로부터 수신된 상기 제1 프레임 세트 및 상기 복수의 다른 프레임 세트들에 기초하여 생성된 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림 내에 포함된 상기 실세계 오브젝트의 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에 적어도 부분적으로 정의된 상기 실세계 오브젝트, 및상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 가상 오브젝트 및 상기 실세계 오브젝트 모두의 표면들의 상기 컬러 데이터 및 상기 깊이 데이터는 나타내는 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트가 렌더링되는 상기 3D 공간 내의 복수의 가상 뷰포인트들(virtual viewpoints)을 포함하는, 상기 엔트리 설명 데이터를 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서,상기 병합된 현실 장면 캡처 시스템에 의해, 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터에 기초하여, 시간적 시퀀스(temporal sequence) 내의 특정 시점에 상기 병합된 현실 장면의 상기 3D 공간 내에 포함되는 상기 복수의 엔트리들 내의 엔트리의 상태를 나타내는 엔트리 설명 프레임을 생성하는 단계; 및상기 병합된 현실 장면 캡처 시스템에 의해, 콘텐츠 제공자 시스템에 연관된 복수의 3D 렌더링 엔진들에 상기엔트리 설명 프레임을 제공하는 단계로서, 상기 복수의 3D 렌더링 엔진들 내의 각각의 3D 렌더링 엔진은 상기3D 공간 내의 상기 복수의 가상 뷰포인트들과 상이한 가상 뷰포인트에 연관되고, 상기 엔트리 설명 프레임에 기초하여, 상기 제2의 복수의 표면 데이터 프레임들 내에 포함된 상이한 표면 데이터 프레임을 렌더링하도록 구성된, 상기 엔트리 설명 프레임을 제공하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서,공개특허 10-2020-0012879-3-상기 복수의 3D 렌더링 엔진들에 통신가능하게 연결된 비디오 데이터 패키징 시스템에 의해, 상기 제2의 복수의표면 데이터 프레임들을 포함하는 상기 제2 프레임 세트 및 각각의 추가적인 복수의 표면 데이터 프레임들을 포함하는 추가적인 프레임 세트들에 기초하여, 상기 복수의 가상 뷰포인트들 내의 각각의 상기 가상 뷰포인트들에대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 다른 운송 스트림을 생성하는 단계;및상기 비디오 데이터 패키징 시스템에 의해, 사용자에 연관된 클라이언트측 미디어 플레이어 장치에 스트리밍할상기 다른 운송 스트림을 제공하는 단계로서, 상기 클라이언트측 미디어 플레이어 장치는 상기 다른 운송 스트림 내에 포함된 각각의 상기 가상 뷰포인트들에 대한 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림에 기초하여, 상기 사용자에 의해 선택되고 상기 병합된 현실 장면의 상기 3D 공간 내의 임의 가상위치에 대응하는 동적으로 선택가능한 가상 뷰포인트에서 상기 사용자에 의해 경험되는 상기 병합된 현실 장면의 상기 3D 공간의 3D 표현을 생성하도록 구성된, 상기 다른 운송 스트림을 제공하는 단계를 더 포함하는,방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 1 항에 있어서,상기 실세계 오브젝트를 나타내는 상기 엔트리 설명 데이터를 생성하는 단계는:상기 제1 프레임 세트 및 상기 복수의 다른 프레임 세트들에 기초하여 생성된 상기 컬러 비디오 데이터 스트림및 상기 깊이 비디오 데이터 스트림에 기초하여, 상기 실세계 장면 내에 포함된 상기 실세계 오브젝트의 3D 표현을 생성하는 단계; 및상기 실세계 오브젝트의 상기 3D 표현에 대한 포인터들로서, 상기 엔트리 설명 데이터 내의 상기 실세계 오브젝트를 적어도 부분적으로 정의하는 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 상기 링크들을 생성하는 단계를 포함하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항에 있어서,상기 병합된 현실 장면 캡처 시스템에 의해, 장면 제어 시스템으로부터, 상기 복수의 엔트리들을 나타내는 상기엔트리 설명 데이터를 수정하기 위한 명령을 수신하는 단계; 및상기 병합된 현실 장면 캡처 시스템에 의해, 상기 명령을 수신하는 단계에 응답하여, 상기 명령에 따라 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터를 수정하는 단계를 더 포함하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1 항에 있어서,상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터를 생성하는 단계는,상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 가상 오브젝트와 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 실세계 오브젝트 사이의 가상 상호작용을 제작하는 단계를 포함하고, 상기 가상 상호작용은 물리-기반 오브젝트 거동 및 인공 지능-기반 (AI-기반) 오브젝트 거동 중 적어도 하나에기초하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 1 항에 있어서,상기 병합된 현실 장면 캡처 시스템에 의해, 상기 제1 프레임 세트에 연관된 메타데이터를 수신하는 단계로서,상기 메타데이터는 상기 특정 시점에 상기 실세계 장면 내에 포함된 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터를 추가적인 실세계 오브젝트의 다른 표면들을 나타내는 다른 컬러 데이터및 다른 깊이 데이터로부터 구별짓는, 상기 메타데이터를 수신하는 단계를 더 포함하고,상기 엔트리 설명 데이터가 생성된 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 복수의 엔트리들은공개특허 10-2020-0012879-4-상기 추가적인 실세계 오브젝트를 더 포함하고,상기 추가적인 실세계 오브젝트는 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림 내에 포함된 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에 적어도 부분적으로 정의되고,상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 상기 링크들 및 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이 데이터에의 상기링크들은 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터를 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이 데이터로부터 구별짓는상기 메타데이터에 기초하는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항에 있어서,상기 실세계 장면의 상기 복수의 상이한 유리한 지점들의 제1 공간 구성은 상기 병합된 현실 장면의 상기 3D 공간 내의 상기 복수의 가상 뷰포인트들의 제2 공간 구성과 별개인, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항에 있어서,상기 제1 프레임 세트를 수신하는 단계, 상기 운송 스트림을 생성하는 단계, 및 상기 엔트리 설명 데이터를 생성하는 단계는 각각, 이벤트들이 상기 실세계 장면 내에서 일어날 때 실시간으로 상기 병합된 현실 장면 캡처시스템에 의해 수행되는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항에 있어서,상기 방법은 적어도 하나의 비일시적(non-transitory) 컴퓨터-판독가능 매체 상의 컴퓨터-실행가능 명령어들로실시되는, 방법."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "병합된 현실 장면 캡처 시스템에 있어서,실세계 장면 내에 포함된 실세계 오브젝트를 나타내는 데이터를 캡처하기 위한, 상기 실세계 장면의 복수의 상이한 유리한 지점들을 갖도록 상기 실세계 장면에 관련하여 배치된 복수의 3-차원(3D) 캡처 장치들;가상 오브젝트의 표면들을 나타내는 컬러 데이터 및 깊이 데이터를 저장하는 자산 저장 시스템;상기 자산 저장 시스템에 통신가능하게 연결된 복수의 서버측 3D 렌더링 엔진들; 및상기 복수의 3D 캡처 장치들, 상기 자산 저장 시스템, 상기 복수의 서버측 3D 렌더링 엔진들에 통신가능하게 연결된 엔트리 상태 추적 시스템을 포함하고, 상기 엔트리 상태 추적 시스템은, 이벤트들이 상기 실세계 장면 내에서 일어날 때 실시간으로,상기 복수의 3D 캡처 장치들로부터, 상기 실세계 오브젝트의 표면들을 나타내는 제1의 복수의 표면 데이터 프레임들을 포함하는 제1 프레임 세트를 수신하고,상기 제1 프레임 세트 및 상기 복수의 3D 캡처 장치들로부터 수신되는 복수의 다른 프레임 세트들에 기초하여,상기 복수의 3D 캡처 장치들 내의 각각의 상기 3D 캡처 장치들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 운송 스트림을 생성하고,상기 운송 스트림 및 상기 자산 저장 시스템 내에 저장된 상기 컬러 데이터 및 깊이 데이터에 기초하여, 병합된현실 장면의 3D 공간 내에 포함된 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성하고, 상기 복수의 엔트리들은, 상기 자산 저장 시스템 내에 저장된 상기 컬러 데이터 및 상기 깊이 데이터에 의해 표현된 상기 가상공개특허 10-2020-0012879-5-오브젝트, 상기 제1 프레임 세트에 포함되고 상기 실세계 오브젝트의 상기 표면들을 나타내는 상기 제1의 복수의 표면 데이터 프레임들에 의해 표현되는 상기 실세계 오브젝트, 및 상기 병합된 현실 장면의 상기 3D 공간 내의 복수의 가상 뷰포인트들을 포함하는, 상기 엔트리 설명 데이터를생성하고, 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터에 기초하여, 시간적 시퀀스 내의 특정 시점에 상기 병합된 현실 공간의 상기 3D 공간 내에 포함된 상기 복수의 엔트리들 내의 엔트리의 상태를 나타내는 엔트리설명 프레임을 생성하고,상기 복수의 서버측 3D 렌더링 엔진들에 상기 엔트리 설명 프레임을 제공하도록 구성되고,상기 복수의 서버측 3D 렌더링 엔진들 내의 각각의 3D 렌더링 엔진은 상기 3D 공간 내의 상기 복수의 가상 뷰포인트들로부터의 상이한 가상 뷰포인트에 연관되고, 상기 엔트리 설명 프레임에 기초하여, 상기 시간적 시퀀스내의 상기 특정 시점에 상기 상이한 가상 뷰포인트로부터 가시가능한 상기 병합된 현실 장면의 상기 3D 공간의뷰를 나타내고 제2 프레임 세트의 제2의 복수의 표면 데이터 프레임들 내에 포함된 상이한 표면 데이터 프레임을 렌더링하도록 구성된, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "병합된 현실 장면 캡처 시스템에 있어서,적어도 하나의 물리적 컴퓨팅 장치를 포함하고,상기 적어도 하나의 물리적 컴퓨팅 장치는,실세계 장면의 복수의 상이한 유리한 지점들을 갖도록 상기 실세계 장면에 관련하여 배치된 복수의 3-차원(3D)캡처 장치들로부터, 제1의 복수의 표면 데이터 프레임들을 포함하는 제1 프레임 세트를 수신하는 것으로, 상기제1의 복수의 표면 데이터 프레임들은, 동일한 특정 시점에, 상기 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치에 의해 그리고 상기 복수의 상이한 유리한 지점들 내의 각각의 유리한 지점에서 캡처되고, 상기 실세계 장면 내에 포함된 실세계 오브젝트의 표면들이 상기 특정 시점에 상기 각각의 3D 캡처 장치의 상기 각각의 유리한 지점에서 보일 때, 상기 표면들의 컬러 데이터 및 깊이 데이터를 나타내는, 상기 제 1 프레임세트를 수신하고,상기 복수의 3D 캡처 장치들로부터 수신된 상기 제1 프레임 세트 및 다른 시점들에 캡처된 복수의 다른 프레임세트들에 기초하여, 상기 복수의 3D 캡처 장치들 내의 상기 3D 캡처 장치들 각각에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 운송 스트림을 생성하고,상기 운송 스트림에 기초하여, 병합된 현실 장면의 3D 공간 내에 포함된 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성하고, 상기 복수의 엔트리들은, 상기 병합된 현실 장면 캡처 시스템에 통신가능하게 연결된 자산 저장 시스템 내에 저장된 가상 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에 적어도 부분적으로 정의된 상기 가상 오브젝트, 상기 복수의 3D 캡처 장치들로부터 수신된 상기 제1 프레임 세트 및 상기 복수의 다른 프레임 세트들에 기초하여 생성된 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림 내에 포함된 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에적어도 부분적으로 정의된 상기 실세계 오브젝트, 및 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 가상 오브젝트 및 상기 실세계 오브젝트 모두의 상기표면들의 상기 컬러 데이터 및 상기 깊이 데이터는 나타내는 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트가 렌더링되는 상기 3D 공간 내의 복수의 가상 뷰포인트들을 포함하는, 병합된 현실 장면 캡처 시스템.공개특허 10-2020-0012879-6-청구항 13 제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는 또한;상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터에 기초하여, 시간적 시퀀스 내의 특정 시점에 상기병합된 현실 장면의 상기 3D 공간 내에 포함되는 상기 복수의 엔트리들 내의 엔트리의 상태를 나타내는 엔트리설명 프레임을 생성하고,콘텐츠 제공자 시스템에 연관된 복수의 3D 렌더링 엔진들에 상기 엔트리 설명 프레임을 제공하고, 상기 복수의3D 렌더링 엔진들 내의 각각의 3D 렌더링 엔진은 상기 3D 공간 내의 상기 복수의 가상 뷰포인트들과 상이한 가상 뷰포인트에 연관되고, 상기 엔트리 설명 프레임에 기초하여, 상기 제2의 복수의 표면 데이터 프레임들 내에포함된 상이한 표면 데이터 프레임을 렌더링하도록 구성된, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 13 항에 있어서,상기 복수의 3D 렌더링 엔진들은 비디오 데이터 패키징 시스템에 통신가능하게 연결되고, 상기 비디오 데이터패키징 시스템은:상기 제2의 복수의 표면 데이터 프레임들을 포함하는 상기 제2 프레임 세트 및 각각의 추가적인 복수의 표면 데이터 프레임들을 포함하는 추가적인 프레임 세트들에 기초하여, 상기 복수의 가상 뷰포인트들 내의 각각의 상기가상 뷰포인트들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 다른 운송 스트림을 생성하고,사용자에 연관된 클라이언트측 미디어 플레이어 장치에 스트리밍할 상기 다른 운송 스트림을 제공하고,상기 클라이언트측 미디어 플레이어 장치는 상기 다른 운송 스트림 내에 포함된 각각의 상기 가상 뷰포인트들에대한 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림에 기초하여, 상기 사용자에 의해 선택되고 상기 병합된 현실 장면의 상기 3D 공간 내의 임의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트에서 상기 사용자에 의해 경험되는 상기 병합된 현실 장면의 상기 3D 공간의 3D 표현을 생성하도록 구성된,병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는: 상기 제1 프레임 세트 및 상기 복수의 다른 프레임 세트들에 기초하여 생성된 상기 컬러 비디오 데이터 스트림및 상기 깊이 비디오 데이터 스트림에 기초하여, 상기 실세계 장면 내에 포함된 상기 실세계 오브젝트의 3D 표현을 생성하고,상기 실세계 오브젝트의 상기 3D 표현에 대한 포인터들로서, 상기 엔트리 설명 데이터 내의 상기 실세계 오브젝트를 적어도 부분적으로 정의하는 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 상기 링크들을 생성함으로써,상기 실세계 오브젝트를 나타내는 상기 엔트리 설명 데이터를 생성하는, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는 또한:장면 제어 시스템으로부터, 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터를 수정하기 위한 명령을수신하고;상기 명령을 수신한 것에 응답하여, 상기 명령에 따라 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이공개특허 10-2020-0012879-7-터를 수정하는, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 가상 오브젝트와 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 실세계 오브젝트 사이의 가상 상호작용을 제작함으로써, 상기 복수의 엔트리들을 나타내는 상기 엔트리 설명 데이터를 생성하고, 상기 가상 상호작용은 물리-기반 오브젝트 거동 및 인공 지능-기반 (AI-기반) 오브젝트 거동 중 적어도 하나에 기초하는, 병합된 현실 장면캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는 또한:상기 제1 프레임 세트에 연관된 메타데이터를 수신하고,상기 메타데이터는 상기 특정 시점에 상기 실세계 장면 내에 포함된 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터를 추가적인 실세계 오브젝트의 다른 표면들을 나타내는 다른 컬러 데이터및 다른 깊이 데이터로부터 구별짓고,상기 엔트리 설명 데이터가 생성된 상기 병합된 현실 장면의 상기 3D 공간 내에 포함된 상기 복수의 엔트리들은상기 추가적인 실세계 오브젝트를 더 포함하고,상기 추가적인 실세계 오브젝트는 상기 컬러 비디오 데이터 스트림 및 상기 깊이 비디오 데이터 스트림 내에 포함된 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이데이터에의 링크들에 의해 상기 엔트리 설명 데이터 내에 적어도 부분적으로 정의되고,상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터에의 상기 링크들 및 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이 데이터에의 상기링크들은 상기 실세계 오브젝트의 상기 표면들의 상기 컬러 데이터 및 상기 깊이 데이터를 상기 추가적인 실세계 오브젝트의 상기 다른 표면들을 나타내는 상기 다른 컬러 데이터 및 상기 다른 깊이 데이터로부터 구별짓는상기 메타데이터에 기초하는, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제 12 항에 있어서,상기 실세계 장면의 상기 복수의 상이한 유리한 지점들의 제1 공간 구성은 상기 병합된 현실 장면의 상기 3D 공간 내의 상기 복수의 가상 뷰포인트들의 제2 공간 구성과 별개인, 병합된 현실 장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제 12 항에 있어서,상기 적어도 하나의 물리적 컴퓨팅 장치는 상기 제1 프레임 세트를 수신하고, 상기 운송 스트림을 생성하고, 및이벤트들이 상기 실세계 장면 내에서 일어날 때 실시간으로 상기 엔트리 설명 데이터를 생성하는, 병합된 현실장면 캡처 시스템."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "예시적인 병합된 현실 장면 캡처 시스템(시스템)은 실세계 장면의 복수의 상이한 유리한 지점들을 갖기 위해 실 세계 장면에 관련하여 배치된 복수의 3-차원(3D) 캡처 장치들로부터 표면 데이터 프레임들의 제1 프레임 세트를 수신한다. 제1 프레임 세트에 기초하여, 시스템은 각각의 3D 캡처 장치들에 대한 컬러 및 깊이 비디오 데이터 스 트림들을 포함하는 운송 스트림을 생성한다. 운송 스트림에 기초하여, 시스템은 병합된 현실 장면의 3D 공간 내 에 포함되는 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성한다. 복수의 엔트리들은 가상 오브젝트, 실 시간 오브젝트, 및 가상 및 실세계 오브젝트들 모두에 대한 컬러 및 깊이 데이터는 나타내는 표면 데이터 프레임 들의 제2 프레임 세트가 렌더링되는 3D 공간 내의 가상 뷰포인트들을 포함한다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 출원은 발명의 명칭이 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들(vantage points)로부터 표 현된 가상 오브젝트(virtual object) 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 방법들 및 시스템들인, 2017년 5월 31일자 제출된 미국 특허 출원 제15/610,573호의 우선권을 주장하며, 그 전체가 참조에 의해 여기에 통합된다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사람들은 (예를 들어, 가상 오브젝트들을 포함하는 가상 장면들, 실세계 오브젝트들을 포함하는 실세계 장면들, 가상 및 실세계 오브젝트들을 포함하는 병합된 현실 장면들 등에 기초한) 가상 3-차원(\"3D\") 공간들을 다양한 이유들로 그리고 다양한 유형들의 애플리케이션들(applications)과 연계하여 경험할 수 있다. 예를 들어, 가상 3D 공간들의 표현들을 표시하도록 구성된 미디어 플레이어 장치들(media player devices)의 사용자 들을 오락 목적들, 교육 목적들, 장거리 통신 목적들, 대리 경험/여행 목적들을 위해, 또는 다양한 다른 목적들 및/또는 애플리케이션들과 관련하여 가상 3D 공간들을 경험할 수 있다. 가상 현실은 사용자들이 가상 3D 공간들을 경험하는 애플리케이션의 일례이다. 가상 현실 미디어 콘텐츠는 사용 자들(즉, 가상 현실 미디어 콘텐츠의 뷰어(viewer)들)이 동시에 몰입형 가상 현실 세계에 표시된 다양한 것들 중 임의의 것에 그들의 주의를 집중시킴으로써 경험할 수 있는 대화형 가상 현실 세계들에 사용자들을 몰입시키 는데 사용될 수 있다. 예를 들어, 가상 현실 미디어 콘텐츠를 표시하는 동안 아무때나, 가상 현실 미디어 콘텐 츠를 경험하고 있는 사용자는 어떤 방향으로든 몰입형 가상 현실 세계를 둘러볼 수 있으며, 사용자는 그 또는 그녀가 실제로 존재하고 있다는 느낌을 받게 되고, 몰입형 가상 현실 세계 내의 특정 위치 및 관점(예를 들어, 각도, 뷰포인트(viewpoint), 등)에서 몰입형 가상 현실 세계를 경험하게 된다. 몇몇 예들에서, 몰입형 가상 현실 세계들은 가상 요소들 및 실세계 요소들 모두를 포함할 수 있다. 이러한 가 상 3D 공간들은 병합된 현실 장면들로 언급될 수 있으며, 예를 들어, 사용자들이 실제 세계에는 존재하지 않는 가상 요소들에 의해 증강된 실세계 요소들(예를 들어, 라이브 이벤트(live event)와 연관된 요소들)을 경험하도 록 허용하는 것과 같은 다양한 이점들을 사용자들에게 제공할 수 있다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "병합된 현실 장면들을 경험하기 위한 최대 유연성을 사용자에게 제공하기 위해, 종래의 미디어 플레이어 장치들 은 전형적으로 사용자가 병합된 현실 장면을 경험하기 전에 병합된 현실 장면(예를 들어, 가상 및 실세계 오브 젝트들의 3D 모델들)을 나타내는 데이터를 수신했다. 불행히도, 데이터를 스트리밍(streaming)하기 보다 사전 로딩(preloading)해야 한다는 요구사항은 사용자에게 제공될 수 있는 어떤 유형들의 경험들을 하지 못하게 하거 나 그것에 큰 제한들을 둘 수 있다. 예를 들어, 사용자가 실시간으로 경험하기를 원할 수 있는 라이브 이벤트들 (예를 들어, 라이브 실세계 스포츠 이벤트들, 공유되는 가상 현실 이벤트들, 등)을 나타내는 데이터가 그 이벤 트들이 시작되기 전에 수신 및 사전 로딩될 수 없을 것이다. 게다가, 데이터를 스트리밍하는 미디어 플레이어 장치들 및/또는 다른 시스템 구성요소들은 더 크거나 더 상세 한 병합된 현실 장면들을 표시하도록 크기조정할 수 없다는 처리 부담들을 가질 수 있다. 예를 들어, 상당한 양 의 추가적인 데이터(예를 들어, 대략 2배의 데이터)가, 예를 들어, 5개의 엔트리들(entities)을 갖는 병합된 현 실 장면에 비해, 예를 들어, 10개의 엔트리들을 갖는 병합된 현실 장면에 대한 3D 모델들을 표현하기 위해 필요 할 수 있다. 따라서, 제공자 시스템이 5개의 엔트리들에 대한 3D 모델들을 실시간으로 미디어 플레이어 장치에 스트리밍할 수 있더라도, 제공자 시스템은 10개의 엔트리들, 100개의 엔트리들, 또는 그 이상에 대한 3D 모델들 을 스트리밍하기까지는 크기를 키울 수 없으며, 특히 병합된 현실 장면 내의 실세계 오브젝트들과 가상 오브젝 트들이 서로 상호작용할 때 그러하다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 방법들 및 시스템들이 여기에 설명된다. 예를 들어, 더 상세하게 후술될 바와 같이, 병합된 현실 장면 캡처 시스템은 실세계 장면의 복수의 상이한 유리한 지점들을 갖도록, 실 세계 장면에 관련하여 배치된 복수의 3-차원(\"3D\") 캡처 장치들로부터 제1의 복수의 표면 데이터 프레임들을 포 함하는 제1 프레임 세트를 수신할 수 있다. 제1의 복수의 표면 데이터 프레임들 내의 각각의 표면 데이터 프레 임들은 동일한 특정 시점에 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치에 의해 캡처될 수 있다. 게다가, 각각의 표면 데이터 프레임들은 복수의 상이한 유리한 지점들 내의 상이한 각각의 유리한 지점으로부터 캡처될 수 있다. 따라서, 각각의 표면 데이터 프레임들은, 실세계 장면 내에 포함된 하나 이상의 실세계 오브젝트들의표면들이 특정 시점에 각각의 3D 캡처 장치의 각각의 유리한 지점에서 보이는 것처럼, 그 표면들의 컬러 데이터 및 깊이 데이터를 표현할 수 있다. 복수의 3D 캡처 장치들로부터 수신된 제1 프레임 세트에 기초하고 또한 다른 시점들(예를 들어, 연속적인 시간 시퀀스(sequence) 내의 이전의 및/또는 이후의 시점들)에 캡처된 복수의 다른 프레임 세트들에 기초하여, 병합 된 현실 장면 캡처 시스템은 운송 스트림을 생성할 수 있다. 예를 들어,운송 스트림은 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치들에 대한 (예를 들어, 연속적인 시간 시퀀스에 걸처 3D 캡처 장치들의 각각의 유리한 지점들에서 각각 가시가능한 컬러 및 깊이 비디오 데이터는 나타내는) 컬러 비디오 데이터 시스템 및 깊이 비디 오 데이터 스트림을 포함할 수 있다. 운송 스트림에 기초하여, 병합된 현실 장면 캡처 시스템은 병합된 현실 장면의 3D 공간 내에 포함된 복수의 엔 트리들을 나타내는 엔트리 설명 데이터를 생성(예를 들어, 제작(creating), 갱신, 등)할 수 있다. 예를 들어, 복수의 엔트리들은 병합된 현실 장면 캡처 시스템에 통신가능하게 연결된 자산 저장 시스템(asset storage system) 내에 저장된 가상 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데 이터 내에 적어도 부분적으로 정의된 가상 오브젝트를 포함할 수 있다. 복수의 엔트리들은 결국 컬러 비디오 데 이터 스트림 및 깊이 비디오 데이터 스트림(즉, 복수의 3D 캡처 장치들로부터 수신된 제1 프레임 세트에 기초하 고 또한 복수의 다른 프레임 세트들에 기초하여 생성된 비디오 데이터 스트림들) 내에 포함된 실세계 오브젝트 의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내에 적어도 부분적으로 정의 될 수 있는 실세계 오브젝트를 더 포함할 수 있다. 추가적으로, 복수의 엔트리들은 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트가 렌더링되는 3D 공간 내의 복수의 가상 뷰포인트들을 포함할 수 있다. 예를 들어, 제2 프레임 세트 내에 포함된 제2의 복수의 표면 데이터 프레임들은 병합된 현실 장면의 3D 공간 내 에 포함된 가상 및 실세계 오브젝트들 모두의 표면들의 컬러 데이터 및 깊이 데이터를 나타내도록, 병합된 현실 장면 캡처 시스템에 통신가능하게 연결된 복수의 서버측 3D 렌더링 엔진들에 의해 렌더링될 수 있다. 여기에 설명된 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세 계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 방법들 및 시스템들은 다양한 장점들 및 이점들을 제공할 수 있다. 일례로서, 여기에 설명된 시스템들 및 방법들은, 미디어 플레이어 장치들을 사용하여 가상 3D 공간들을 경험할 때, 가상 3D 공간들의 표현들을 표시하도록 구성된 미디어 플레이어 장치들의 사용자들을 용이 하게 할 수 있다. 여기에 사용된 바와 같이, \"3D 공간\"은 사용자가 실세계를 경험하는 것과 유사한 방식으로 사 용자에 의해 경험될 수 있는 환경 또는 세계의 3D 표현(예를 들어, 완전히 가상화된 표현 또는 실세계 요소들의 재현에 적어도 부분적으로 기초한 표현)을 의미할 수 있다. 예를 들어, 가상 또는 병합된 현실 장면을 경험하는 사용자는 3D 공간 내에서 돌아다니거나 3D 공간 내에 포함된 오브젝트들을 보거나 및/또는 이와 달리 그들과 상 호작용할 수 있다. 몇몇 예들에서, 3D 공간은 완전히 가상화(예를 들어, 컴퓨터 생성됨)되고 실세계 장면이 표 현될 수 있는 것과 유사한 방식으로 표현될 수 있다. 다른 예들에서, 3D 공간은 실세계 장면으로부터 캡처된 하 나 이상의 실세계 오브젝트들에 적어도 부분적으로 기초할 수 있다. 어느 경우에서든, 미디어 플레이어 장치의 사용자가 3D 공간을 경험하기 전에, 3D 공간들 및 그것에 포함된 가 상 및 실세계 오브젝트들을 나타내는 데이터가 미디어 플레이어 장치 상에 사전 로딩되거나 저장될 필요 없도록 하기 위해, 여기에 설명된 시스템들 및 방법들은 제공자 시스템으로부터의 병합된 현실 장면들(즉, 실세계 오브 젝트들 및 가상 오브젝트들 모두를 포함하는 장면들의 가상 3D 공간들)의 3D 공간들 전체의 스트리밍을 용이하 게 할 수 있다. 사용자에게 병합된 현실 장면을 표시하기 전에, 병합된 현실 장면 콘텐츠를 표현하는 데이터가 (예를 들어, 국부적인 물리적 저장소를 통해) 다운로딩, 저장, 또는 이와달리 액세스될 필요 없게 하기 위해, 미디어 플레이어 장치가 병합된 현실 장면을 표시하는데 필요한 모든 데이터는 미디어 플레이어 장치에 스트리 밍될 수 있다. 몇몇 예들에서, 이 스트리밍 능력은 시간에-민감한 콘텐츠(예를 들어, 실시간으로 일어나는 실세 계 또는 가상 이벤트들)에 연관된 병합된 현실 장면들이, 병합된 현실 장면들 내에서 이벤트들이 일어날 때, 실 시간으로 사용자에 의해 경험되도록 허용할 수 있다. 게다가, 여기에 설명된 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 방법들 및 시스템들은, 가상 현실 미디어 콘 텐츠가 임의 가상 위치들에서 및 3D 공간 내의 동적으로 선택가능한 가상 뷰포인트들에서 렌더링될 수 있는 방 식으로, 병합된 현실 장면을 표현하는 가상 현실 미디어 콘텐츠를 미디어 플레이어 장치들에 제공하는 것을 용 이하게 할 수 있다. 특히, 더 상세하게 후술될 바와 같이, 상이한 가상 뷰포인트들에서 병합된 현실 장면의 프 레임들을 렌더링함으로써, 병합된 현실 장면 캡처 시스템 및/또는 병합된 현실 장면 캡처 시스템에 연관된 다른 서버측 시스템들은, 미디어 플레이어 디바이스가 복수의 2-차원(\"2D\") 비디오 스트림들에 기초하여 임의 뷰포인트들 및 동적으로 선택가능한 가상 뷰포인트들로부터 병합된 현실 장면들의 3D 공간을, 3차원으로, 렌더링하도 록 허용하기 위해 구성된 데이터 파이프라인 내에 프레임들을 포함시킬 수 있다. 예를 들어, 여기에 설명된 컬 러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트림들과 같은 2D 비디오 데이터 스트림들이 데이터 파이 프라인에 포함될 수 있다(예를 들어, 하나 이상의 운송 스트림들 내에 패키징(packaging)됨). 그러나, 2D 비디 오 데이터 스트림들은 비교적 고정된 뷰포인트들(예를 들어, 병합된 현실 장면 캡처 시스템에 의해 생성 및 유 지되는 엔트리 설명 데이터 내에 표현된 복수의 엔트리들에 포함된 복수의 가상 뷰포인트들)에 연관될 수 있지 만, 미디어 플레이어 장치는 미디어 플레이어 장치의 사용자가 3차원으로 및 임의 가상 뷰포인트들(예를 들어, 2D 비디오 데이터 스트림들이 연관된 비교적 고정된 뷰포인트들에 맞춰 정렬되지 않거나 그와 달리 관련되지 않 은 비-고정된 뷰포인트들)에서 병합된 현실 장면의 3D 공간을 경험하도록 허용할 수 있다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "그 결과, 미디어 플레이어 장치는 3D 공간에 연관된 가변적이고 잠재적으로 무제한의 개수의 3D 모델들을 나타 내는 3D 모델 데이터를 스트리밍할 필요 없이 임의 가상 뷰포인트들에서 3D 공간을 렌더링할 수 있다. 예를 들 어, 가상 3D 공간 내에 포함된 모든 엔트리의 3D 모델들을 표현하는 데이터를 제공하는 대신, 데이터 파이프라 인은 몇개의 가상 뷰포인트들에서 3D 공간 내의 모든 실세계 및 가상 오브젝트들을 나타내는 2D 비디오 데이터 (예를 들어, 컬러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트림들)를 제공할 수 있다. 이와 같이, 미 디어 플레이어 장치가, 예를 들어, 단지 1개 또는 2개의 오브젝트들만으로 병합된 현실 장면을 렌더링하기 위해 필요한 것보다 추가적인 데이터 또는 추가적인 양의 데이터를 수신하거나 추가적인 렌더링 작업을 수행하지 않 으면서, 병합된 현실 장면을 렌더링할 때 무제한 개수의 오브젝트들이 표현될 수 있다. 추가적으로, 미디어 플레이어 장치들에 이미 저장된 사전 로딩된 콘텐츠에 의존하지 않으면서 미디어 플레이어 장치들에 병합된 현실 장면을 표현하는 모든 데이터를 생성, 유지, 및 제공함으로써, 여기에 설명된 시스템 및 방법들은, 미디어 플레이어 장치 상에 저장되어 있는 사전 로딩된 데이터를 수정하지 않아도 되면서, 3D 공간들 이 공급자에 의해 (예를 들어, 이벤트들이 실세계 장면에서 일어나는 실시간으로 또는 거의 실시간으로) 생성 또는 수정되도록 허용할 수 있다. 그 결과, 병합된 현실 장면을 생성할 책임이 있는 콘텐츠 제작자들 또는 병합 된 현실 장면을 경험하는 하나 이상의 사용자들은 병합된 현실 장면의 양태들을 수정(예를 들어, 가상 또는 실 세계 오브젝트들 등과 같은 엔트리들을 수정, 대체, 또는 제거)하기 위한 명령들을 병합된 현실 장면 캡처 시스 템에 제공할 수 있고, 병합된 현실 장면이 실시간 또는 거의 실시간으로 수정되도록 이 수정들은 사용자들에게 스트리밍되고 있는 데이터에 즉각 반영될 수 있다. 마찬가지로, 계산적으로 비쌀 수 있는(예를 들어, 어떤 미디어 플레이어 장치들에 대해 매우 비싼) 다양한 동작 들이 병합된 현실 장면 캡처 시스템에 연관된 강력한 컴퓨팅 자원들에 의해 수행될 수 있는데, 이 병합된 현실 장면 캡처 시스템은 가상 현실 미디어 제공자에 의해 동작될 수 있고, 예를 들어, 사용자들에 연관된 미디어 플 레이어 장치들보다 훨씬 더 강력한 컴퓨팅 자원들(예를 들어, 대용량 서버들 등)에 연관될 수 있다. 예를 들어, 병합된 현실 장면 캡처 시스템은 병합된 현실 장면 내에서 실세계 오브젝트들과 가상 오브젝트들을 통합시키고, 병합된 현실 장면 내의 오브젝트들에 대하여 물리적 동작들을 수행하고, 오브젝트들에 대하여 인공 지능 동작들 을 수행하는 등을 위해, 계산적으로 비싼 동작들을 수행할 수 있다. 이들 동작들이 클라이언트측 보다는 서버측 에서 수행되기 때문에, 사용자에 의해 동작되는 미디어 플레이어 장치들은 특별히 강력한 컴퓨팅 자원들에 연관 될 필요가 없을 수 있어, 사용자들이 클라이언트측 미디어 플레이어를 갖는 한, 사용자 장치 자원들이 절약되고, 트랜스미션(transmission) 대역폭이 최소화되고, 사용자들에게 (예를 들어, 휴대성, 냉각성 등의 면 에서) 편리함이 제공되고, 다양한 유형들의 미디어 플레이어 장치들이 (예를 들어, 다양한 형태 인자들, 다양한 가격 점들 등으로) 사용자들에게 병합된 현실 장면의 경험을 제공할 수 있게 된다. 이제 다양한 실시예들이 도면들을 참조하여 더 상세하게 설명될 것이다. 게시된 방법들 및 시스템들은 전술된 하나 이상의 이점들, 및/또는 여기서 명확해질 다양한 추가적인 및/또는 대안적인 이점들을 제공할 것이다."}
{"patent_id": "10-2019-7035521", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 1, "content": "도 1은 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브 젝트에 적어도 기초하여 병합된 현실 장면을 생성하기 위한 예시적인 병합된 현실 장면 캡처 시스템(\"시스 템\")을 나타낸다. 도시된 바와 같이, 시스템은 서로 선택적으로 및 통신가능하게 연결된 실세계 장면 캡처 기능, 병합된 현실 엔트리 상태 추적 기능, 및 저장 기능을, 제한 없이 포함할 수 있다. 기능들(102 내지 106)이 도 1에서 개별적인 기능들로 도시되어 있지만, 기능들(102 내지 106)은 단일 기능과 같 은 더 적은 기능들로 조합되거나, 더 많은 기능들로 나뉠 수 있다는 것이 인식될 것이며, 이는 특정 구현에 사 용될 수 있다. 몇몇 예들에서, 각각의 기능들(102 내지 106)은 다수의 장치들 및/또는 다수의 위치들 간에 분산 될 수 있으며, 이는 특정 구현에 사용될 수 있다. 이제 각각의 기능들(102 내지 106)이 여기에 포함된 특정한 다른 도면들을 참조하여 더 상세하게 설명될 것이다. 실세계 장면 캡처 기능은 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위해 사용되는 데이터를 캡처 및 획득하 는 것에 연관된 다양한 동작들을 수행하는 하나 이상의 물리적 컴퓨팅 장치들 (예를 들어, 프로세서들, 메모리 들, 통신 인터페이스들과 같은 하드웨어 및/또는 소프트웨어 구성요소들, 프로세서들에 의해 실행되기 위해 메 모리 내에 저장된 명령어들, 등)을 포함할 수 있다. 구체적으로, 예를 들어, 실세계 장면의 복수의 상이한 유리 한 지점들을 갖기 위해, 실세계 장면 캡처 기능은 실세계 장면에 관련하여 배치된 복수의 3-차원(3D) 캡처 장치들로부터 제1의 복수의 표면 데이터 프레임들을 포함하는 제1 프레임 세트를 수신할 수 있다. 제1의 복수의 표면 데이터 프레임들 내의 각각의 표면 데이터 프레임들은 제1의 복수의 표면 데이터 프레임들 내의 다른 표면 데이터 프레임들과 동일한 특정 시점에 캡처될 수 있고, 복수의 상이한 유리한 지점들 내의 각 각의 유리한 지점에서 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치에 의해 캡처될 수 있다. 여기에 사용된 바와 같이, 표면 데이터 프레임들이 정확히 동일한 순간에 캡처되지 않았더라도, 표면 데이터 프레임들이 (즉, 시간 범위에 걸처 대상을 표현하는 것과 반대로) 한 순간에 대상(예를 들어, 실세계 장면 내의 실세계 오브젝트)을 효과적으로 표현하기에 충분히 인접한 시간에 캡처되면, 표면 데이터 프레임들은 \"동일한 특정 시 점에\" 캡처됐다고 언급될 수 있다. 예시로서, 특정 대상이 얼마나 동적인지(예를 들어, 하나 이상의 실세계 오 브젝트들이 실세계 장면을 얼마나 빨리 이동하는지, 등)에 따라, 표면 데이터 프레임들은, 예를 들어, 서로 수 십 또는 수 백 밀리초 내에 캡처될 때 또는 다른 적합한 타임프레임 내에서 (예를 들어, 마이크로초, 밀리초, 초, 등 내에서) 캡처될 때 동일한 특정 시점에 캡처되었다고 고려될 수 있으며, 이는 특정 구현에 사용될 수 있 다. 이와 같이, 각각의 표면 데이터 프레임들은, 실세계 장면 내에 포함된 실세계 오브젝트의 표면들이 특정 시 점에 각각의 3D 캡처 장치의 각각의 유리한 지점에서 보이는 것처럼, 그 표면들의 컬러 데이터 및 깊이 데이터 를 표현할 수 있다. 실례로서, 도 2는 시스템(예를 들어, 실세계 장면 캡처 기능)이 예시적인 실세계 오브젝트를 포함하 는 예시적인 실세계 장면을 표현하는 데이터를 캡처하기 위해 복수의 3D 캡처 장치들과 상호동작하는 예시적인 구성을 도시한다. 구체적으로, 도 2에 도시된 바와 같이, 구성은, 실세계 오브젝트를 포함하고 각각의 유리한 지점(예를 들어, 3D 캡처 장치(206-1)에 연관된 유리한 지점(208-1) 내지 3D 캡처 장치 (206-8)에 연관된 유리한 지점(208-8))에 각각 연관된 복수의 3D 캡처 장치들(예를 들어, 3D 캡처 장치들 (206-1 내지 206-8))에 의해 둘러쌓여 있는 실세계 장면을 포함한다. 3D 캡처 장치들은 시스템 과 (예를 들어, 전술된 바와 같은 시스템 내의 실세계 장면 캡처 기능과) 통신가능하게 연결될 수 있 는데, 이 시스템은 각각의 복수의 표면 데이터 프레임들을 각각 포함하는 각각의 프레임 세트들을 3D 캡처 장치들로부터 수신할 수 있다. 여기에 사용된 바와 같이, \"표면 데이터 프레임\"은 3D 공간에 연관된 특정 시점 또는 다른 시간적 시퀀스 (temporal sequence) 내의 시점에 특정한 유리한 지점 또는 가상 뷰포인트에서 3D 공간 내에서 가시가능한 오브 젝트(예를 들어, 실세계 오브젝트들, 가상 오브젝트들, 등)의 표면들에 연관된 다양한 유형들의 데이터는 나타 내는 데이터세트를 의미할 수 있다. 예를 들어, 표면 데이터 프레임은 3D 공간에 관련된 특정한 유리한 지점에 서 보이는 오브젝트들을 나타내는 컬러 데이터(즉, 이미지 데이터) 및 깊이 데이터를 포함할 수 있다. 이와 같 이, 복수의 관련 표면 데이터 프레임들은 장면(예를 들어, 가상 장면, 실세계 장면, 병합된 현실 장면, 등)의 (컬러 뿐만 아니라 깊이 데이터까지 표현하는) 비디오-유사 표현을 제작하기 위해 함께 순서화될 수 있는데, 장 면이 특정한 유리한 지점에서 뷰잉되거나 경험되기 때문이다. 어떤 예들에서, 표면 데이터 프레임은 오디오 데이터, 메타데이터(예를 들어, 표면 데이터 프레임 내에 표현된 구체적인 오브젝트들에 대한 정보 및/또는 장면 에 연관된 유리한 지점들에 대한 정보를 포함하는 메타데이터)와 같은 다른 유형들의 데이터 및/또는 다른 유형 들의 데이터에 더 연관될 수 있으며, 이는 특정 구현에 사용될 수 있다. 상이한 유리한 지점들에 연관된 표면 데이터 프레임들 및 관련된 표면 데이터 프레임들의 시퀀스들의 예들이 아래에 설명되고 나타내질 것이다. 여기에 사용된 바와 같이, \"컬러 데이터\"는 (컬러로 표현되는지 또는 그레이스케일(grayscale)(즉, \"흑백\")로 표현되는지) 대상(예를 들어, 가상, 실세계, 또는 병합된 현실 장면의 3D 공간 내에 포함된 실세계 또는 가상 오브젝트)이 특정한 유리한 지점의 관점에서 특정한 시점에 또는 특정한 시간 기간에 걸쳐 어떻게 보이는지를 표현하는 임의의 이미지 데이터, 비디오 데이터, 등을 광범위하게 포함할 수 있다. 컬러 데이터는 본 기술에서 이미지 데이터 및/또는 비디오 데이터를 정의하는 다양한 정의들 및/또는 표준들에 연관될 수 있는 임의의 특정 한 포맷, 파일 유형, 프레임 률, 해상도, 품질 수준, 또는 다른 특징에 제한되지 않는다. 마찬가지로, 여기에 사용된 바와 같이, \"깊이 데이터\"는 공간 내의 대상의 위치를 나타내는 임의의 데이터를 포함할 수 있다. 예를 들어, 실세계 또는 가상 오브젝트를 나타내는 깊이 데이터는 가상 오브젝트의 표면들 상의 상이한 점들에 대한 전체 좌표계(global coordinate system)(예를 들어, 3D 공간에 연관된 실세계, 가상, 또는 혼합된 현실 장면의 3D 공간에 연관된 전체 좌표계)에 관련된 좌표들을 포함할 수 있다. 구성의 각각의 요소들이 이제 상세히 설명될 것이다. 실세계 장면은 임의의 실세계 장면, 실세계 위치, 실세계 이벤트(예를 들어, 라이브 이벤트, 등), 또는 (예를 들어, 가상 세계 또는 공상 세계에서만 존재하는 것과 반대로) 실 세계에 존재하는 다른 대상을 표현할 수 있으며, 이는 특정 구현에 사용될 수 있다. 도 2 내의 실세계 장면을 표현한 원으로 나타낸 바와 같이, 실세계 장면은 무대, 경기장, 등과 같은 구체적으로 묘사된 영역일 수 있다. 반대로, 다른 예들에서, 실세 계 장면은 그렇게 잘 정의 또는 묘사되지 않을 수 있다. 예를 들어, 실세계 장면은 도시 거리, 박물 관, 경치, 등과 같은 임의의 실내 또는 실외 실세계 위치를 포함할 수 있다. 어떤 예들에서, 실세계 장면 은 스포츠 이벤트, 음악 이벤트, 연극 또는 극단 발표, 대규모 축하 행사(예를 들어, 타임 스퀘어의 새해 전야 제, 참회 화요일(Mardis Gras), 등), 정치 이벤트, 또는 임의의 다른 실세계 이벤트와 같은 실세계 이벤트에 연 관될 수 있다. 동일한 또는 다른 예들에서, 실세계 장면은 소설화된 장면(예를 들어, 실사 가상 현실 텔레 비전 쇼 또는 영화의 세트) 및/또는 임의의 다른 실내 또는 실외 실세계 위치에서의 임의의 다른 장면에 대한 세팅(setting)에 연관될 수 있으며, 이는 특정 구현에 사용될 수 있다. 따라서, 실세계 오브젝트는 (예를 들어, 실세계 장면 내에 또는 그 주변에 위치된) 실세계 장면(20 2)에 연관되고, 적어도 하나의 유리한 지점들에서 탐지가능한(예를 들어, 뷰잉가능한, 등) 임의의 실세계 오브젝트(생물이든 무생물이든)를 표현할 수 있다. 예를 들어, 실세계 오브젝트가 명료성을 위해 비교적 간단한 기하학적 모양으로 그려져있지만, 실세계 오브젝트는 다양한 복잡도 수준들을 갖는 다양한 유형들 의 오브젝트들을 표현할 수 있다는 것이 이해될 것이다. 기하학적 모양보다는, 예시로서, 실세계 오브젝트(20 4)는 사람이나 다른 생물, 불투명한 고체, 액체, 가스, 또는 벽, 천장, 바닥과 같은 거의 끊김이 없는 오브젝트, 또는 여기에 설명된 임의의 다른 유형의 오브젝트와 같은 임의의 생물 또는 무생물 오브젝트 또는 표 면을 표현할 수 있으며, 또는 이는 특정 구현에 사용될 수 있다. 도시된 바와 같이, 실세계 오브젝트는 3D 캡처 장치들에 의해 탐지되는 광(예를 들어, 실세계 장면 내의 환경 광, 깊이 캡처 장치에 의해 방출되는 구조화된 광 패턴의 적외선, 등)을 각각 반사할 수 있는 다양한 표면들을 포함할 수 있다. 실세계 오브젝트가 비교적 간단하게 묘사되어 있지만, 실세계 오브젝트 의 표면들은 어느 위치 및 유리한 지점에서 표면들이 탐지되는지에 기초하여 상이하게 보일 수 있는데, 이는 후술될 것이다. 즉, 실세계 오브젝트는 실세계 오브젝트가 뷰잉되는 관점(예를 들어, 위치, 유리한 지점, 등)에 기초하여 상이하게 보일 수 있다. 3D 캡처 장치들은 각각 실세계 장면에 관련하여 고정될 수 있다. 예를 들어, 실세계 장면 및 3D 캡처 장치들 모두는 정지되어 있을 수 있고, 또는 실세계 장면 및 3D 캡처 장치들은 함께 움직 일 수 있다. 몇몇 예들에서, 구성에 도시된 바와 같이, 3D 캡처 장치들은 실세계 장면에 연관된 적어도 2 차원을 따라(예를 들어, 땅과 같은 평면을 따라) 실세계 장면을 둘러쌀 수 있다. 어떤 예들에서, 3D 캡처 장치들은 (예를 들어, 실세계 장면 위 및 아래에 3D 캡처 장치들을 포함시킴으로써) 3 차원을 따라 실세계 장면을 둘러쌀 수 있다. 3D 캡처 장치들의 예들이 더 상세히 후술될 것이다. 유리한 지점들은 3D 캡처 장치에서 나오는 점선들에 의해 각각의 3D 캡처 장치에 관련하여 나타 내질 수 있다. 몇몇 예들에서, 구성에 도시된 바와 같이, 유리한 지점들은 각각 나중에 임의 가상 뷰포인트에서 실세계 장면을 렌더링할 수 있기에 충분한 관점들에서 실세계 장면을 캡처하도록 실세계 장면을 향해 안쪽으로 기울어질 수 있다. 추가적으로, 동일한 또는 다른 예들에서, 하나 이상의 유리한 지 점들은 실세계 장면 등을 둘러싸는 오브젝트들을 캡처하기 위해 바깥쪽으로 (예를 들어, 실세계 장면 에서 멀어져) 기울어질 수 있다. 예시로서, 추가적인 관점들에서 실세계 장면 내에 포함된 오브젝트 들을 캡처하고 및/또는 실세계 장면 외부의 장치들을 캡처하기 위해, 구 모양의, 바깥쪽을 향하고 있는 유 리한 지점을 갖는 360-도 캡처 장치가 실세계 장면의 중앙 위치에 위치될 수 있다(명시적으로 도시되지 않 음). 추가적으로 또는 대안적으로, 어떤 예들에서, 바깥쪽을 향하고 있는 복수의 유리한 지점들은 실세계 장면 의 파노라마, 광각, 또는 360-도 뷰의 캡처를 허용할 수 있다. 어떤 예들에서, 시스템(예를 들어, 실세계 장면 캡처 기능)은 하나 이상의 네트워크들 및/또는 임의 의 다른 적합한 통신 인터페이스들, 프로토콜들, 및 기술들에 의해 3D 캡처 장치들에 통신가능하게 연결될 수 있다. 따라서, 이 예들에서, 실세계 장면 캡처 기능은 하나 이상의 네트워크들 및/또는 다른 통신 인터 페이스들, 프로토콜들, 및 기술들을 통해 3D 캡처 장치들로부터 제1의 복수의 표면 데이터 프레임들을 포 함하는 제1 프레임 세트(및 후술될 다른 복수의 표면 데이터 프레임들을 포함하는 다른 프레임 세트들)를 수신 할 수 있다. 예를 들어, 도시된 바와 같이, 구성 내의 다양한 화살표들은 3D 캡처 장치들과 시스템 간의 통신들을 표현한다. 이들 통신들은 네트워크(예를 들어, 유선 또는 무선 근거리 네트워크, 광역 네 트워크, 제공자 네트워크, 인트라넷, 등)를 통해, 유선 통신 인터페이스(예를 들어, 범용 직렬 버스(Universal Serial Bus)(\"USB\"))를 통해, 무선 통신 인터페이스를 통해, 또는 임의의 다른 통신 인터페이스, 프로토콜, 및/ 또는 기술을 통해 구현될 수 있으며, 이는 특정 구현에 사용될 수 있다. 다른 예들에서, 복수의 3D 캡처 장치들은 시스템 내에 통합되거나, 이와 달리 시스템의 일부로서 (예 를 들어, 실세계 장면 캡처 기능의 일부로서) 포함될 수 있다. 이와 같이, 이 예들에서, 실세계 장면 캡처 기능은 통합된 3D 캡처 장치들을 사용하여 제1 프레임 세트를 캡처함으로써 제1 프레임 세트(및 다른 프레임 세트들)를 수신할 수 있다. 구성 내의 3D 캡처 장치들이 실세계 장면을 표현하는 표면 데이터 프레임들(예를 들어, 전술된 바와 같은 실세계 장면 캡처 기능에 의해 수신된 제1 프레임 세트 내의 제1의 복수의 표면 데이터 프레임 들)을 어떻게 캡처하는지를 나타내기 위해, 도 3a는 도 3b 및 도 3c에 그래픽적으로 묘사된 표면 데이터 프레임 을 캡처하는 3D 캡처 장치(206-1)를 도시한다. 도 3a에 도시된 바와 같이(그리고 도 2에 간단히 묘사된 바와 같이), 3D 캡처 장치(206-1)는 실세계 오브젝트 의 유리한 지점(208-1)을 갖기 위해, 실세계 장면 내의 실세계 오브젝트에 관련하여 배치될 수 있다. 게다가, 도 3a는, (도 2에 나타낸 다른 3D 캡처 장치들과 함께) 3D 캡처 장치(206-1)가 실세계 오브 젝트 및/또는 실세계 장면에 포함된 다른 오브젝트들을 나타내는 컬러 데이터(예를 들어, 풀 컬러 또 는 그레이스케일 이미지를 표현하는 2D 비디오 데이터)를 캡처하도록 구성된 2D 비디오 캡처 장치 및 실세 계 오브젝트 및/또는 실세계 장면에 포함된 다른 오브젝트들을 나타내는 깊이 데이터를 캡처하도록 구성된 깊이 캡처 장치를 포함할 수 있다는 것을 나타낸다. 2D 비디오 캡처 장치는 임의의 적합한 2D 비디오 캡처 장치(예를 들어, 비디오 카메라, 등)로 구현될 수 있고, 임의의 방식으로 2D 비디오 데이터를 캡처할 수 있으며, 이는 특정 구현에 사용될 수 있다. 몇몇 예들에 서, 2D 비디오 캡처 장치는 깊이 캡처 장치와 별개의 장치일 수 있다. 집합적으로, 이러한 별개의 장 치들은 (예를 들어, 장치들을 기능적으로 병합하는데 사용되는 임의의 통신 인터페이스들 및/또는 다른 하드웨 어 또는 소프트웨어 메커니즘들(mechanisms)도) 3D 캡처 장치(예를 들어, 3D 캡처 장치(206-1))로 언급될 수 있 다. 다른 예들에서, 도 3a에 도시된 바와 같이, 2D 비디오 캡처 장치 및 깊이 캡처 장치는 2D 비디오 데이터 및 깊이 데이터 모두를 캡처하는 단일 장치(즉, 3D 캡처 장치(206-1))로 통합될 수 있으며, 이는 후술될 것이다. 별개의 장치로 구현되었든 또는 2D 비디오 캡처 장치와 통합되었든, 깊이 데이터 캡처 장치는 임의의 방식으로 실세계 장면을 표현하는 깊이 데이터를 캡처할 수 있으며, 이는 특정 구현에 사용될 수 있다. 예 시로서, 깊이 데이터 캡처 장치는 구조화된 광 깊이 지도 캡처 기술, 입체적인 깊이 지도 캡처 기술, 비행 -시간(time-of flight) 깊이 지도 캡처 기술, 다른 적합한 깊이 지도 캡처 기술, 또는 깊이 지도 캡처 기술들의 임의의 조합과 같은 하나 이상의 깊이 지도 캡처 기술들을 사용할 수 있으며, 이는 특정 구현에 사용될 수 있다. 깊이 데이터를 캡처하기 위해 사용되는 깊이 지도 캡처 기술들의 유형 및 개수에 상관없이, 3D 캡처 장치(206- 1)에 의해 생성되는 각각의 표면 데이터 프레임은 유리한 지점(208-1)에서 실세계 오브젝트의 표면들을 나 타내는 컬러 데이터 및 깊이 데이터 모두를 포함할 수 있다. 마찬가지로, 다른 3D 캡처 장치들에 의해 캡 처되는 다른 표면 데이터 프레임들도 다른 3D 캡처 장치들에 연관된 각각의 유리한 지점들에서 실세 계 오브젝트의 표면들을 나타내는 컬러 데이터 및 깊이 데이터를 유사하게 포함할 수 있다. 도 3b 및 3c는 3D 캡처 장치(206-1)에 의해 캡처된 표면 데이터 프레임을 표현하는 데이터의 예시적인 그래픽 묘사들을 나타낸다. 구체적으로, 도시된 바와 같이, 표면 데이터 프레임은 적어도 2개의 구별되는 데이터세트들 을 포함할 수 있다: 컬러 데이터(도 3b에 도시됨) 및 깊이 데이터(도 3c에 도시됨). 도 3b에서, 컬러 데이터은 3D 캡처 장치(206-1) 내의 2D 비디오 캡처 장치에 의해 유리한 지점(208- 1)의 관점에서 뷰잉된 실세계 장면 내의 실세계 오브젝트를 묘사한다. 컬러 데이터가 비디오 프 레임들의 시퀀스 내의 단일 비디오 프레임을 표현할 수 있으므로, 컬러 데이터에 의해 표면된 실세계 오브 젝트의 묘사는 실세계 오브젝트가 (예를 들어, 실세계 장면에 연관된 다른 오브젝트들도) 특정 한 시점에 유리한 지점(208-1)에서 어떻게 보이는지를 표현할 수 있다. 도 3b에서는 이미지로 나타냈지만, 컬러 데이터가 임의의 적합한 형태로 캡처, 부호화(encoding), 포맷, 트랜스미션, 및 표현될 수 있다는 것이 이 해될 것이다. 예를 들어, 컬러 데이터는 표준 비디오 부호화 프로토콜, 표준 이미지 포맷, 등에 따라 포맷 된 디지털 데이터일 수 있다. 몇몇 예들에서, 컬러 데이터는 실세계 장면 내의 오브젝트들의 (예를 들어, 컬러 사진과 유사한) 컬러 이미지를 표현할 수 있다. 대안적으로, 다른 예들에서, 컬러 데이터는 (예를 들어, 흑백 사진과 유사한) 오브젝트들을 나타내는 그레이스케일 이미지일 수 있다. 도 3c에서, 깊이 데이터도 또한 (컬러 데이터와 같이) 유리한 지점(208-1)의 관점에서 실세계 장면 내의 실세계 오브젝트를 묘사한다. 그러나, 실세계 오브젝트의 가시적인 외형을 표현하기 (즉, 광이 실세계 오브젝트의 표면들과 어떻게 상호작용하는지를 컬러 또는 그레이스케일로 표현하기) 보다, 깊 이 데이터는, 예를 들어, 3D 캡처 장치(206-1) 내의 깊이 캡처 장치에 관련된 실세계 오브젝트 (및, 예를 들어, 실세계 장면 내의 다른 오브젝트들)의 표면 상의 각각의 지점의 깊이(즉, 거리 또는 위치)를 표현할 수 있다. 컬러 데이터와 같이, 깊이 데이터는 임의의 적합한 형태로 캡처, 부호화, 포맷, 트랜스미션, 및 표현될 수 있다. 예를 들어, 도시된 바와 같이, 깊이 데이터는 그레이스케일 이미지 데이터(예를 들어, 깊이 캡처 장치로 캡처되는 각각의 화소에 대한 6 또는 8 비트들)로 표현될 수 있다. 그러나, (즉, 컬러 데이터 내에 표현된 바와 같이) 실세계 오브젝트의 표면들로부터 광이 어떻게 반 사되는지를 표현하기 보다, 깊이 데이터의 그레이스케일 이미지는, 이미지 내의 각각의 화소에 대해, 그 화소에 의해 표현된 지점이 깊이 캡처 장치로부터 얼마나 멀리 있는지를 표현할 수 있다. 예를 들어, 깊이 캡처 장치에 인접한 지점들은 더 어두운 회색 음영들을 표현하는 값들(예를 들어, 0b111111이 검정색을 표 현하는 6-비트 구현의 경우, 0b111111에 인접한 이진 값들)로 표현될 수 있다. 반대로, 깊이 캡처 장치로 부터 더 멀리 떨어져있는 지점들은 더 밝은 회색 음영들을 표현하는 값들(예를 들어, 0b000000이 흰색을 표현하 는 6-비트 구현의 경우, 0b000000에 인접한 이진 값들)로 표현될 수 있다. 전술된 바와 같이, 실세계 장면 캡처 기능은 제1의 복수의 표면 데이터 프레임들을 포함하는 제1 프레임 세트 및 각각의 복수의 다른 표면 데이터 프레임들을 포함하는 하나 이상의 다른 프레임 세트들을 (예를 들어, 도 2 및 3a-3c에 관련하여 설명된 바와 같이, 3D 캡처 장치들로부터) 수신할 수 있다. 나타내기 위해, 도 4는 상이한 유리한 지점들에서 실세계 장면을 표현하기 위해 3D 캡처 장치들에 의해 캡처된 각 각의 복수의 예시적인 표면 데이터 프레임들을 각각 포함하는 예시적인 복수의 프레임 세트들(예를 들어, 프레임 세트들(402-1 내지 402-N))을 도시한다. 도 4에 도시된 표면 데이터 프레임들 상의 실세계 오브젝트 의 묘사들이 컬러 데이터 내의 실세계 오브젝트의 묘사와 유사하게 보일 수 있지만, 각각의 표 면 데이터 프레임은 실세계 오브젝트 및/또는 실세계 장면에 포함된 다른 오브젝트들의 표면들을 표 현하기 위해 사용될 수 있는 (예를 들어, 컬러 데이터와 유사한) 컬러 데이터, (예를 들어, 깊이 데이터 와 유사한) 깊이 데이터, 및/또는 임의의 다른 적합한 데이터를 포함할 수 있다는 것이 이해될 것이다. 도 4는, 전술된 바와 같이, 비디오 프레임들의 각각의 시퀀스들이 각각의 개별적인 3D 캡처 장치에 의해 어떻게 캡처될 수 있는지를 나타내기 위한 프레임 시퀀스들(예를 들어, 프레임 시퀀스들(404-1 내지 404- 8))을 더 도시한다. 구체적으로, 예시로서, 프레임 시퀀스(404-1)는 3D 캡처 장치(206-1)에 의해 순차적인 시점 들에서 캡처된 표면 데이터 프레임들의 시퀀스를 표현할 수 있고, 프레임 시퀀스(404-2)는 3D 캡처 장치(206- 2)에 의해 동일한 순차적인 시점들에서 캡처된 표면 데이터 프레임들의 시퀀스를 표현할 수 있는 등이다. 따라 서, 프레임 세트(402-1) 및 그것 내에 묘사된 실세계 오브젝트의 상이한 관점들에 의해 나타낸 바와 같이, 실세계 장면은 특정한 프레임 세트(예를 들어, 프레임 세트(402-1)) 내에 포함된 상이한 표면 데이터 프레임들 내의 상이한 유리한 지점들에서 뷰잉된 것으로 표현될 수 있다. 예를 들어, 프레임 세트(402-1) 내에 포함된 제1 표면 데이터 프레임(즉, 위에서 보이는 및 프레임 시퀀스(404-1) 내에 포함된 표면 데이터 프레임) 은 유리한 지점(208-1)에서 캡처된 컬러 데이터 및 깊이 데이터를 표현할 수 있고, 프레임 세트(402-1) 내에 포 함된 제2 표면 데이터 프레임(즉, 프레임 시퀀스(404-2) 내에 포함된 표면 데이터 프레임)은 유리한 지점(208- 2)에서 캡처된 컬러 데이터 및 깊이 데이터를 표현할 수 있는, 등이다. 시퀀스 내에서 프레임 세트(402-1) 다음 에 오는, 각각의 다른 프레임 세트들(즉, \"402-2 ... N\"으로 라벨링된 프레임 세트들(402-2 내지 402- N))의 경우에 대해서도 또한 동일할 수 있다. 도 1로 되돌아가서, 병합된 현실 엔트리 상태 추적 기능은 상이한 비디오 데이터 스트림들 내의 상이한 유 리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위해 사 용되도록 엔트리 설명 데이터를 준비, 생성, 및/또는 유지하는 것에 연관된 다양한 동작들을 수행하는 하나 이 상의 물리적 컴퓨팅 구성요소들(예를 들어, 실세계 장면 캡처 기능과 별개인 또는 실세계 장면 캡처 기능 과 공유되는 하드웨어 및/또는 소프트웨어 구성요소들)을 포함할 수 있다. 예를 들어, 병합된 현실 엔트리 상태 추적 기능은 실세계 장면 캡처 기능이 복수의 3D 캡처 장치들(예를 들어, 3D 캡처 장치들(20 6))로부터 수신한 제1 프레임 세트(예를 들어, 프레임 세트(402-1)) 및 실세계 장면 캡처 기능이 수신하고 다른 시점들(예를 들어, 시간 기간에 걸친 실세계 장면을 표현하기 위한 제1 프레임 세트의 캡처 직전 및/ 또는 직후)에 캡처된 복수의 다른 프레임 세트들(예를 들어, 프레임 세트들(402-1 내지 402-N))에 기초하여 운 송 스트림을 생성할 수 있다. 여기에 사용된 바와 같이, \"데이터 스트림들\" 및 \"운송 스트림들\"은 하나의 장치 또는 시스템에서 (데이터를 렌 더링하거나 이와 달리 처리하거나 분석하는) 다른 것으로의 데이터의 트랜스미션(즉, 운송)을 용이하게 할 목적 들 또는 다른 목적들을 위해 데이터를 패키징하는데 사용되는 데이터 구조들을 의미할 수 있으며, 이는 특정 구 현에 사용될 수 있다. 몇몇 예들에서, 여기에 사용된 바와 같이, \"운송 스트림\"은 하나 이상의 비디오 데이터 스트림들과 같은 하나 이상의 다른 데이터 스트림들을 포함하는 및/또는 메타데이터 등과 같은 다른 데이터를 포함할 수 있는 단일 운송 스트림을 의미할 수 있다. 예를 들어, 병합된 현실 엔트리 상태 추적 기능에 의 해 생성된 운송 스트림은 복수의 3D 캡처 장치들(예를 들어, 3D 캡처 장치들) 내의 각각의 3D 캡처 장치들 에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함할 수 있다. 즉, 단일 운송 스트림은 모든 비디오 데이터 스트림들(예를 들어, 각각의 3D 캡처 장치에 대한 하나의 컬러 비디오 데이터 스트림 및 각 각의 3D 캡처 장치에 대한 하나의 깊이 비디오 데이터 스트림) 및 특정 구현에서 시스템이 운송을 위해 포 함할 수 있는 임의의 메타데이터 또는 다른 적합한 데이터를 운송하는데 사용될 수 있다. 다른 예들에서, 여기 에 사용된 바와 같이, \"운송 스트림\"은 모든 비디오 데이터 스트림들을 집합적으로 운송하는 복수의 운송 스트 림들을 의미할 수 있다. 예시로서, \"운송 스트림\"은, 각각이 상이한 특정 3D 캡처 장치의 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하거나, 각각이 복수의 컬러 및/또는 깊이 비디오 데이터 스트림들 을 포함하는 개개의 운송 스트림들의 집합을 의미할 수 있고, 이는 특정 구현에 사용될 수 있다. 실례로서, 도 5는 예시적인 컬러 비디오 데이터 스트림(500-1-C) 및 예시적인 깊이 비디오 데이터 스트림(500- 1-D)을 도시하는데, 이 둘 모두는 특정한 유리한 지점에서 실세계 장면을 표현하기 위해 특정 3D 캡 처 장치에 의해 캡처된 표면 데이터 프레임들에 기초한다. 구체적으로, 도시된 바와 같이, 컬러 비디오 데 이터 스트림(500-1-C)은 (프레임 시퀀스의 깊이 부분과 반대로 프레임 시퀀스의 컬러('C') 부분을 나타내기 위 해 프레임 시퀀스(404-1-C)로 라벨링된) 프레임 시퀀스(404-1) 내에 포함된 표면 데이터 프레임들의 컬러 데이 터 부분들을 포함할 수 있고, 깊이 비디오 데이터 스트림(500-1-D)은 (프레임 시퀀스의 컬러 부분과 반대로 프 레임 시퀀스의 깊이('D') 부분을 나타내기 위해 프레임 시퀀스(404-1-D)로 라벨링된) 프레임 시퀀스(404-1) 내 에 포함된 표면 데이터 프레임들의 깊이 데이터 부분들을 포함할 수 있다. 컬러 비디오 데이터 스트림(500-1-C)은, 비디오 데이터 스트림이 실세계 장면 상의 제1('1') 관점에 연관 되고(즉, 3D 캡처 장치(206-1), 유리한 지점(208-1), 프레임 시퀀스(404-1), 등에 연관되고) 깊이 데이터가 아 니라 컬러('C') 데이터에 연관된다는 것을 나타내기 위해 그렇게 라벨링된다. 마찬가지로, 깊이 비디오 데이터 스트림(500-1-D)은, 비디오 데이터 스트림이 또한 실세계 장면 상의 제1('1') 관점에 연관되었지만 컬러 데이터가 아니라 깊이('D') 데이터에 연관된다는 것을 나타내기 위해 그렇게 라벨링된다. 추가적인 비디오 데이 터 스트림들(즉, 다른 도면들에 나타낸 비디오 데이터 스트림들, 그러나 여기 도 5에는 나타내지 않 았음)도 유사한 방식으로 라벨링되고 참조될 수 있다는 것이 이해될 것이다. 비디오 데이터 스트림들(예를 들어, 여기서 참조된 비디오 데이터 스트림들(500-1-C, 500-1-D) 및 다른 비디오 데이터 스트림들)은 임의 의 프로토콜들, 포맷들, 등을 사용하여 생성, 저장, 트랜스미션, 및/또는 이와 달리 구현될 수 있으며, 이는 특정 구현에 사용될 수 있다. 예를 들어, 어떤 구현들에서, 프레임 시퀀스들(404-1-C 및 404-1-D)로부터의 컬러 및 깊이 데이터(및, 예를 들어, 하나 이상의 추가적인 프레임 시퀀스들로부터의 컬러 및/또는 깊이 데이터)는 타일 매핑(tile mapping) 또는 텍스처 아틀라이싱(texture atlasing) 기술을 사용하여 단일 비디오 데이터 스트 림의 각각의 프레임의 개별적인 부분들(예를 들어, 개별적인 타일들, 줄무늬들, 등) 내에 표현될 수 있다. 도 6은 다른 컬러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트림들과 함께 컬러 비디오 데이터 스트림 (500-1-C) 및 깊이 비디오 데이터 스트림(500-1-D)을 포함하는 예시적인 운송 스트림을 나타낸다. 구체적 으로, 도시된 바와 같이, 도 6은 도 2에 나타낸 3D 캡처 장치들 및 유리한 지점들에 연관된 컬러 비 디오 데이터 스트림들(즉, 컬러 비디오 데이터 스트림들(500-1-C 내지 500-8-C)) 및 각각의 3D 캡처 장치들 및 유리한 지점들에 연관된 깊이 비디오 데이터 스트림들(즉, 깊이 비디오 데이터 스트림들(500-1-D 내지 500-8- D))을 포함하는 단일 운송 스트림을 나타낸다. 다른 예들에서, 운송 스트림은 다른 프로토콜들, 포맷 들, 등을 사용하여 생성, 저장, 트랜스미션, 및/또는 이와 달리 구현될 수 있으며, 이는 특정 구현에 사용될 수 있다. 예시로서, 전술된 바와 같이, 다양한 프레임 시퀀스들로부터의 데이터가 타일 매핑 기술들, 등을 사용하 여 하나의 비디오 데이터 스트림으로 (또는 도 6에 도시된 것보다 적은 비디오 데이터 스트림들을 갖는 복수의 비디오 데이터 스트림들로) 패키징될 수 있거나, 개별적인 운송 스트림들이 컬러 및 깊이 비디오 데이터 스트림 들의 각각의 세트를 포함하도록 사용될 수 있다(예를 들어, 하나의 운송 스트림은 비디오 데이터 스트림들(500- 1-C 및 500-1-D)을 포함하고, 다른 운송 스트림은 비디오 데이터 스트림들(500-2-C 및 500-2-D)을 포함하는 등). 도 1로 돌아가서, 병합된 현실 엔트리 상태 추적 기능에 의해 생성된 운송 스트림(예를 들어, 운송 스트림 내에 포함된 컬러, 깊이, 및 다른 데이터)에 기초하여, 병합된 현실 엔트리 상태 추적 기능은 병합 된 현실 장면의 3D 공간 내에 포함된 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성할 수 있다. 병합 된 현실 엔트리 상태 추적 기능는 임의의 적합한 방식으로 엔트리 설명 데이터를 생성할 수 있다. 예를 들 어, 병합된 현실 엔트리 상태 추적 기능은 병합된 현실 장면의 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 제작, 갱신, 수신, 추적, 유지, 분석, 조직화, 및/또는 이와 달리 처리할 수 있다. 더 상세하게 후술 될 바와 같이, 병합된 현실 엔트리 상태 추적 기능은 또한 엔트리 설명 데이터를 수정하기 위한 (예를 들 어, 엔트리들을 추가, 제거, 대체, 이동, 회전, 확대, 또는 이와 달리 수정함으로써와 같이 하나 이상의 엔트리 들을 수정하기 위한) 명령들을 수신하고, 엔트리 설명 데이터를 수정함으로써 그 명령들을 구현할 수 있다. 병 합된 현실 엔트리 상태 추적 기능은 각각의 엔트리에 대한 동적인 변화들을 표현하는 생성된 데이터에 대 한 갱신들을 저장 및 유지하기 위해 저장 기능과 상호동작함으로써 데이터를 더 생성할 수 있다. 여기에 사용된 바와 같이, 엔트리 설명 데이터가 생성된 \"엔트리\"는 가상 3D 공간(예를 들어, 병합된 현실 장면 의 3D 공간)에 연관될 수 있는 임의의 실세계 또는 가상 아이템을 의미할 수 있다. 예를 들어, 병합된 현실 엔 트리 상태 추적 기능이 데이터를 생성한 엔트리들 중에서, 병합된 현실 장면의 3D 공간은 하나 이상의 가 상 오브젝트들과 같은 가상 엔트리들 및/또는 3D 공간 내의 복수의 가상 뷰포인트들(예를 들어, 다양한 상이한 관점들에서 3D 공간을 캡처하기 위해 3D 공간에 관련하여 특정한 방식들로 위치되고 기울어진 가상 캡처 장치들 과 유사할 수 있음), 전술된 바와 같이 데이터가 3D 캡처 장치들에 의해 캡처된 실세계 엔트리들(예를 들어, 데 이터가 3D 캡처 장치들에 의해 캡처된 실세계 오브젝트) 및/또는 임의의 다른 실세계 또는 가상 엔트 리들을 포함할 수 있으며, 이는 특정 구현에 사용될 수 있다. 더 상세히 후술될 바로서, 엔트리 설명 데이터가 생성된, 3D 공간 내에 포함된 각각의 엔트리는 임의의 방식으 로 엔트리 설명 데이터에 정의될 수 있으며, 이는 특정 구현에 사용될 수 있다. 예를 들어, (예를 들어, 후술된 바와 같이 저장 기능 내에 저장될 수 있는) 엔트리 설명 데이터 자체는 3D 공간 내에서의 엔트리의 위치에 연관된 좌표 정보, 3D 공간 내에서의 엔트리의 방향에 연관된 방향 정보, 엔트리가 3D 공간 내에서 얼마나 크게 보이도록 만들어졌는지에 연관된 크기 정보, 등과 같은 3D 공간 내의 특정 엔트리의 상태를 정의하는 데이터를 포함할 수 있다. 그러나, 몇몇 예들에서, 특정 엔트리에 연관된 어떤 정보(예를 들어, 3D 모델, 텍스처, 등을 표현하는 이진 정보)는 직접적으로 엔트리 설명 데이터의 일부로서 유지되지 않고, 다른 곳에 유지되어 엔트리 설명 데이터로부터 링크될 수 있다. 예를 들어, 병합된 현실 엔트리 상태 추적 기능이 엔트리 설명 데이터를 생성한 복수의 엔트리들은 시스템 에 통신가능하게 연결되고 및/또는 시스템 내에 통합된 자산 저장 시스템 내에 저장된 가상 오브젝트 의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내에 적어도 부분적으로 정의 된 가상 오브젝트를 포함할 수 있다. 복수의 엔트리들은, 예를 들어, 전술된 3D 캡처 장치들에 의해 캡처 된 실세계 오브젝트를 더 포함할 수 있다. 이와 같이, 실세계 오브젝트는 운송 스트림 내에 포함된컬러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트림들(예를 들어, 운송 스트림 내의 컬러 비디오 데이터 스트림들(500-1-C 내지 500-8-C) 및 깊이 비디오 데이터 스트림들(500-1-D 내지 500-8-D)) 내에 포함된 실세계 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내에 적 어도 부분적으로 정의될 수 있다. 게다가, 복수의 엔트리들은, 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트가 렌더링되는 3D 공간 내의 복수의 가상 뷰포인트들을 포함할 수 있다. 예를 들어, 실세계 장 면 내의 실세계 오브젝트의 컬러 및 깊이 데이터는 나타내는, 제1 프레임 세트(즉, 프레임 세트(402- 1)) 및 전술된 다른 프레임 세트들(즉, 프레임 세트들(402-2 내지 402-N)) 내에 포함된 표면 데이터 프레임들과 대조적으로, 제2 프레임 세트 내에 포함된 제2의 복수의 표면 데이터 프레임들은 병합된 현실 장면의 3D 공간 내에 포함된 실세계 오브젝트 및 하나 이상의 가상 오브젝트들 모두의 표면들의 컬러 데이터 및 깊이 데이 터를 표현하도록 렌더링될 수 있다. 실세계 및 가상 오브젝트들 모두를 표현하는 제2 프레임 세트 및 추가적인 프레임 세트들이 아래에 더 상세하게 나타내지고 설명될 것이다. 몇몇 예들에서, 시스템(예를 들어, 시스템의 병합된 현실 엔트리 상태 추적 기능 또는 다른 기 능)은 적어도 하나의 엔트리의 상태를 나타내는 엔트리 설명 프레임을 생성할 수 있다. 예시로서, 시스템 은 병합된 현실 엔트리 상태 추적 기능에 의해 생성된 복수의 엔트리들을 나타내는 엔트리 설명 데이터에 기초하여 엔트리 설명 프레임을 생성할 수 있다. 엔트리 설명 프레임은 하나 또는 몇 개의 엔트리들을 표현할 수 있거나, 또는 몇몇 예들에서는, 시간적 시퀀스 내의 특정 시점(예를 들어, 실시간의 특정 순간, 실시간과 무 관한 가상의 시간선 상의 순간을 표현하는 특정 시점, 등)에 병합된 현실 장면의 3D 공간 내의 복수의 엔트리들 내의 모든 실세계 및 가상 엔트리들을 표현할 수 있다. 여기에 사용된 바와 같이, \"엔트리 설명 프레임\"은 병합된 현실 장면의 3D 공간 내에 포함된 하나 이상의 엔트 리들의 상태를 설명하는 (예를 들어, 자바 스크립트 오브젝트 표기법(Java Script Object Notation)(\"JSON\") 등과 같은 언어로 표현된 오브젝트 설명 데이터를 포함하는) 데이터세트를 의미할 수 있다. 예를 들어, 엔트리 설명 프레임은 시간적 시퀀스 내의 특정 시점에 3D 공간 내에 포함된 몇몇의 엔트리들 각각을 설명하는 데이터 를 포함할 수 있다. 예시로서, 엔트리 설명 프레임은 전술된 좌표 정보, 방향 정보, 크기 정보, 및 다른 유형들 의 상태 데이터와 같은 상태 데이터뿐만 아니라, 각각의 엔트리에 대한 하나 이상의 이동 벡터들, 각각의 엔트 리의 다양한 표면들에 대한 컬러들 및/또는 텍스처들, 및/또는 시간적 시퀀스 내의 특정 시점에 특정한 엔트리 들을 설명하기 위해 사용될 수 있는 임의의 다른 상태 데이터를 포함할 수 있으며, 이는 특정 구현에 사용될 수 있다. 몇몇 예들에서, 엔트리 설명 프레임은 전술된 바와 같은 엔트리 설명 데이터에 포함된 (예를 들어, 가상 오브젝트에 대한 자산 저장 시스템, 실세계 오브젝트에 대한 운송 스트림, 등에 대한) 링크들을 포함할 수 있다. 예시적인 엔트리 설명 프레임들이 아래에 더 상세하게 설명 및 나타내질 것이다. 시스템이 엔트리 설명 프레임을 생성하면, 시스템은 엔트리 설명 프레임을 콘텐츠 제공자 시스템에 연관된 복수의 서버측 3D 렌더링 엔진들(예를 들어, 시스템, 3D 렌더링 엔진들, 및 여기에 설명된 다른 서 버측 시스템들 및 구성요소들과 협동하는 가상 현실 미디어 콘텐츠 시스템)에 제공할 수 있다. 여기에 사용된 바와 같이, \"서버측\"은 콘텐츠 제공자 시스템이 콘텐츠(예를 들어, 가상 현실 미디어 콘텐츠)를 최종 사용자에 의해 사용되는 클라이언트 장치에 제공하는 트랜젝션(transaction)과 같은 서버-클라이언트 트랜젝션의 서버 측 (예를 들어, 제공자 측)을 의미할 수 있다. 예를 들어, 더 상세히 후술될 바로서, 가상 현실 미디어 콘텐츠 제 공자 시스템은 사용자에 연관된 미디어 플레이어 장치에 가상 현실 미디어 콘텐츠를 제공할 수 있다. 이와 같이, 서버측 시스템들 및 구성요소들은 데이터(예를 들어, 가상 현실 미디어 콘텐츠)를 미디어 플레이어 장치 에 (예를 들어, 네트워크를 통해) 제공하기 위해 콘텐츠 제공자 시스템에 연관된 (예를 들어, 그것에 포함된, 그것으로 구현된, 그것과 상호동작하는, 등) 그러한 시스템들 및 구성요소들을 의미할 수 있다. 반대로, \"클라 이언트 측\" 장치들은 네트워크의 다른 측 상에서 사용자에 의해 사용되는 클라이언트 장치(예를 들어, 미디어 플레이어 장치)에 연관될 수 있고, 클라이언트 장치(예를 들어, 네트워크의 사용자 측 상에서 사용자에 의해 동 작되는 미디어 플레이어 장치 및/또는 다른 컴퓨터 구성요소들)가 콘텐츠 제공자 시스템으로부터 데이터를 수신 하는 것을 용이하게 하는 장치들을 포함할 수 있다. 따라서, 아래에 나타내고 설명될 바와 같이, 3D 렌더링 엔진들은 시스템의 하드웨어 및/또는 소프트웨어 자원들과 통합되거나, 그들과 별개이면서 통신가능하게 연결될 수 있는 하드웨어 및/또는 소프트웨어 자원들에 의해 네트워크의 서버 측 상에 구현될 수 있다(즉, 시스템 및/또는 콘텐츠 제공자 시스템의 다른 요소들과 연관될 수 있음). 3D 렌더링 엔진들 각각은 3D 공간으로의 복수의 가상 뷰포인트들로부터의 상이한 가상 뷰포인 트에 연관될 수 있고, 제2 프레임 세트(즉, 병합된 현실 장면의 3D 공간의 실세계 및 가상 오브젝트들 모두를 표현하는 제2 프레임 세트) 내의 제2의 복수의 표면 데이터 프레임들 내에 포함된 상이한 각각의 표면 데이터프레임을 렌더링하도록 구성될 수 있다(예를 들어, 각각은 시스템에 의해 제공된 동일한 엔트리 설명 프레 임에 기초함). 저장 기능은 특정 구현에서 기능들(102 또는 104)에 의해 수신, 생성, 관리, 추적, 유지, 사용, 및/또는 트랜스미션된 임의의 적합한 데이터를 저장 및/또는 관리할 수 있다. 예를 들어, 도시된 바와 같이, 저장 기능 은 병합된 현실 장면의 3D 공간 내에 포함된 하나 이상의 실세계 오브젝트들(예를 들어, 실세계 오브젝트 )에 연관된 데이터(예를 들어, 캡처된 컬러 및/또는 깊이 데이터, 상태 데이터, 엔트리 설명 데이터, 등) 를 포함할 수 있는 실세계 오브젝트 데이터 및 3D 공간 내의 하나 이상의 가상 오브젝트들 또는 가상 뷰포 인트들에 연관된 데이터(예를 들어, 컬러 및/또는 깊이 데이터, 상태 데이터, 엔트리 설명 데이터, 등)를 포함 할 수 있는 가상 엔트리 데이터를 포함할 수 있다. 추가적으로, 저장 기능은 병합된 현실 장면의 3D 공간 내에 포함된 다른 유형들의 엔트리들에 연관된 데이터, 여기에 설명된 동작들을 수행하기 위한 명령어들 (예를 들어, 프로그래밍 명렁어들), 및/또는 여기에 설명된 동작들을 수행할 때 기능들(102 및 104)에 의해 사 용되기에 적합한 임의의 다른 데이터를 포함할 수 있다. 예를 들어, 저장 기능은 표면 데이터 프레임들, 엔트리 설명 프레임들, 등에 연관된 데이터(예를 들어, 오브젝트 설명 데이터, 컬러 데이터, 깊이 데이터, 오디 오 데이터, 메타데이터, 등)를 더 포함할 수 있다. 저장 기능은 또한 추가적인 또는 대안적인 데이터도 유 지할 수 있으며, 이는 특정 구현에 사용될 수 있다. 몇몇 예들에서, 시스템은 여기에 설명된 하나 이상의 동작들을, 이벤트들이 실세계 장면 내에서 일어나고 있을 때 실시간으로 또는 거의 실시간으로 수행될 수 있다. 따라서, 시스템이, 다른 시스템들이 또한 실시 간으로 동작하는 가상 현실 미디어 콘텐츠 제공자 파이프라인 내에서 사용되는 구현들에서, 가상 현실 미디어 콘텐츠(예를 들어, 시스템에 의해 실시간으로 생성되는 가상화된 표면 데이터 프레임 시퀀스들을 포함하는 가상 현실 미디어 콘텐츠)가 미디어 플레이어 장치들에 제공되어, 실세계 장면에 물리적으로 인접하게 위치해 있지 않지만 실세계 장면(예를 들어, 이벤트들이 실세계 장면 내에서 일어남)을 경험하길 원할 수 있는 미디어 플레이어 장치들의 각각의 사용자들은 그들의 각각의 미디어 플레이어 장치들을 사용하여 실세계 장면 및 그것 내에서 라이브로 일어나고 있는 이벤트들을 가상적으로 경험할 수 있다. 사용자가 정확히 실세계 장면 내에서 이벤트들이 일어날 때 실세계 장면을 경험할 수 없게, 데이터 처리 및 데이터 분배가 한정된 양의 시간을 취할 수 있지만, 여기에 사용된 바와 같이, 동작이 즉각적으로 및 지나치게 지연되지 않으면서 수행될 때 동작은 \"실 시간\" 또는 \"거의 실시간\"으로 수행된다고 고려된다. 따라서, 사용자가 실세계 장면 내의 특정 이벤트들을 지연 후(예를 들어, 실제로 일어난 몇 초 또는 몇 분 후)에 경험하더라도, 사용자는 실세계 장면을 실시간으로 경험 했다고 말해질 수 있다. 어떤 예들에서, 상이한 비디오 데이터 스트림들 내에 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실 세계 오브젝트에 기초하여 병합된 현실 장면을 생성하고, 사용자가 병합된 현실 장면을 경험하도록 허용하기 위 해 사용자에게 표시될 병합된 현실 장면을 표현하는 데이터를 (예를 들어, 가상 현실 미디어 콘텐츠의 일부로서) 제공하기 위해, 시스템은 콘텐츠 제공자 시스템(예를 들어, 가상 현실 미디어 콘텐트 제공자 시 스템) 내의 다양한 구성들과 함께 포함되는 다양한 다른 서버측 시스템들(예를 들어, 3D 캡처 장치들, 장면 제 어 시스템들, 자산 저장 시스템들, 비디오 데이터 패키징 시스템들, 3D 렌더링 엔진들, 등)과 연관될 수 있다. 몇몇 구현들에서, 이들 다른 서버측 시스템들의 하나 이상은 시스템과 통합(예를 들어, 그것 내에 포함)되 거나, 이와 달리 시스템에 밀접하게 연관(예를 들어, 시스템에 통신가능하게 연결, 동일한 또는 관련 된 가상 현실 미디어 제공자 엔트리들에 의해 동작, 등)될 수 있다는 것이 이해될 것이다. 예를 들어, 특정 구 현에서, 시스템은 실세계 장면 내에 포함된 실세계 오브젝트를 나타내는 데이터를 캡처하기 위한 실세계 장면의 복수의 상이한 유리한 지점들을 갖기 위해 실세계 장면에 관련하여 배치된 복수의 3D 캡처 장치들, 가상 오브젝트의 표면들을 나타내는 컬러 및 깊이 데이터를 저장하는 자산 저장 시스템, 자산 저장 시스템에 통신가 능하게 연결된 복수의 서버측 3D 렌더링 엔진들, 및 복수의 3D 캡처 장치들, 자산 저장 시스템, 및/또는 복수의 서버측 3D 렌더링 엔진들에 통신가능하게 연결된 엔트리 상태 추적 시스템을 포함할 수 있다. 엔트리 상태 추적 시스템은 기능들(102 내지 106)에 관련하여 전술된 동작들 중 하나 이상을 수행하도록 구성될 수 있다. 다른 구 현들에서, 시스템은 이들 다른 서버측 시스템들과 통합되지는 않았지만 다른 서버측 시스템들과 통신가능하게 연결되거나 및/또는 다른 서버측 시스템들과 이와 달리 상호동작하도록 구성된, 별개의 독립형 시스템으로 구현 될 수 있다. 예로서, 도 7은, 시스템이 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하는 것을 용이하게 하는 예시적인 구성(70 0)을 도시한다. 도 7에 도시된 바와 같이, 시스템의 구현은 복수의 장면 제어 시스템들(예를 들어,장면 제어 시스템들(702-1 내지 702-N)) 및 복수의 서버측 3D 렌더링 엔진들(예를 들어, 3D 렌더링 엔진들 (704-1 내지 704-N))에 통신가능하게 연결될 수 있다. 예를 들어, 시스템은 (예를 들어, 여기에 설명된 임 의의 네트워크들 또는 네트워크 기술들을 포함하는) 하나 이상의 네트워크들을 통해 또는 다른 통신 모드들을 통해 장면 제어 시스템들 및/또는 3D 렌더링 엔진들에 통신가능하게 연결될 수 있으며, 이는 특정 구 현에 사용될 수 있다. 구성에 도시된 바와 같이, 기능들(102 내지 106)에 관련하여 전술된 동작들을 수행 하는 엔트리 상태 추적 시스템은 시스템에 의해 구현될 수 있다. 전술된 바와 같이, 다른 구현들에서, 시 스템은 이들 동작들을 수행하도록 구성된 엔트리 추적 시스템 및 구성에 나타낸 하나 이상의 다른 시 스템들 및 장치들 모두를 실시할 수 있다. 복수의 엔트리들을 나타내는 엔트리 설명 데이터(예를 들어, 가상 오브젝트, 실세계 오브젝트, 복수의 가상 뷰 포인트들, 및 엔트리 설명 데이터가 전술된 바와 같이 시스템에 의해 생성된 다른 엔트리들을 나타내는 데 이터)에 기초하여, 시스템은 시간적 시퀀스 내의 특정 시점에 병합된 현실 공간의 3D 공간 내에 포함된 복 수의 엔트리들 내의 적어도 하나의 엔트리의 상태를 나타내는 엔트리 설명 프레임을 생성할 수 있다. 시스템 은 또한 생성된 엔트리 설명 프레임을 엔트리 설명 프레임으로서 3D 렌더링 엔진들에 (예를 들 어, 3D 렌더링 엔진들과의 통신 접속을 통해) 제공할 수 있다. 엔트리 설명 프레임 및 (3D 렌더링 엔 진들에 통신가능하게 연결될 수 있는) 자산 저장 시스템 및 (시스템 또는 다른 적합한 시스템의 저장 기능 내에 저장될 수 있는) 운송 스트림으로부터 요구 및 수신된 데이터에 기초하여, 3D 렌더링 엔진 들은 복수의 표면 데이터 프레임들(710-1 내지 710-N)을 렌더링할 수 있다. 구체적으로, 표면 데이터 프레 임들(710-1 내지 710-N)은 전술된 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트를 구현할 수 있고, 아래에서 집합적으로 프레임 세트로 언급될 수 있다. 이와 같이, 프레임 세트 내의 표면 데이 터 프레임들(710-1 내지 710-N)은 가상 오브젝트(예를 들어, 도 8에 관련하여 후술된 가상 오브젝트) 및 실세계 오브젝트(예를 들어, 실세계 오브젝트) 모두의 표면의 컬러 데이터 및 깊이 데이터를 표현할 수 있 고, 비디오 데이터 패키징 시스템에 제공될 수 있다. 시스템 및 운송 스트림은 상세하게 전술되 어 있다. 구성에 나타낸 다른 시스템들 및 아이템들 각각이 이제 더 상세하게 설명될 것이다. 장면 제어 시스템들은 병합된 현실 장면의 3D 공간 내에 포함된 하나 이상의 엔트리들(예를 들어, 시스템 에 의해 데이터가 생성된 엔트리들)에 대한 변화들을 요청 및/또는 이와 달리 구현하도록 구성된 임의의 컴퓨팅 시스템들을 표현할 수 있다. 예를 들어, 하나 이상의 장면 제어 시스템들(예를 들어, 장면 제어 시 스템(702-1))은 병합된 현실 장면의 3D 공간 내에 포함된 엔트리들을 나타내는 데이터를 원래 생성할 책임이 있 는 콘텐츠 제작자에 연관(예를 들어, 그에 의해 생성, 그에 의해 동작, 등)될 수 있다. 추가적으로, 어떤 구현 들에서, 하나 이상의 다른 장면 제어 시스템들(예를 들어, 장면 제어 시스템(702-2))은 병합된 현실 장면의 3D 공간을 경험하고 있는 최종 사용자에 연관될 수 있다. 예를 들어, 미디어 플레이어 장치의 사용자가 병합된 현 실 장면의 3D 공간 내의 엔트리들을 경험하고 그들과 상호작용하도록 허용하기 위해, 장면 제어 시스템(702- 2)은 현재 엔트리들을 렌더링하는 미디어 플레이어 장치에 의해 구현될 수 있다. 시스템이 3D 공간 내에 포함된 모든 실세계 및 가상 엔트리들을 나타내는 하나의 통일된 데이터 세트(예를 들어, 각각의 장면 제어 시스템에 대한 엔트리들을 나타내는 개별적인 데이터 세트들과 반대임)를 생성할 수 있기 때문에, 엔트리들에 대한 수정들은, 각각의 장면 제어 시스템들이 이러한 수정을 행할 때, 통일된 데이터 세트 내에 반영될 수 있다. 따라서, 다수의 사용자들(즉, 상이한 장면 제어 시스템들에 연관된 상 이한 사용자들)은 모두 동일한 병합된 현실 장면의 동일한 3D 공간을 수정할 수 있다. 그 결과, 모든 장면 제어 시스템들에 의해 만들어진 수정들은 시스템에 의해 출력된 엔트리 설명 프레임들(예를 들어, 엔트리 설명 프레임)에 반영될 수 있고, 결국 3D 렌더링 엔진들에 의해 렌더링되는 각각의 표면 데이터 프레 임들(예를 들어, 프레임 세트 내에 포함된 표면 데이터 프레임들(710-1 내지 710-N)) 내에 반영될 수 있다. 장면 제어 시스템들이 3D 공간 내의 엔트리들을 어떻게 수정할 수 있는지를 나타내기 위해, 도 8은 복수의 엔트리들을 포함하는 예시적인 병합된 현실 장면을 도시한다. 도시된 바와 같이, 병합된 현실 장면은 (전술된) 실세계 오브젝트 및 가상 오브젝트를 포함하는 3D 공간에 연관된다. 오브젝트들(204 및 804)은 복수의 가상 뷰포인트들(예를 들어, 가상 뷰포인트들(806-1 내지 806-N))에 의해 둘러싸여 있다. 전술된 실세계 장면과 같이, 병합된 현실 장면은 (즉, 적어도 하나의 가상 오브젝트 및 적어도 하나 의 실세계 오브젝트의 표현을 포함하는) 병합된 현실 요소들을 포함하는 임의의 적합한 크기, 모양, 또는 유형 의 장면을 표현할 수 있다. 예시로서, 도 8 내의 3D 공간을 원 형태로 나타낸 바와 같이, 3D 공간은무대, 경기장, 등과 같은 구체적으로 묘사된 영역을 표현할 수 있다. 반대로, 다른 예들에서, 3D 공간은 그렇게 잘 정의 또는 묘사되지 않을 수 있다. 예를 들어, 3D 공간은 실 세계(예를 들어, 실세계 장면, 이 벤트, 경치, 구조물, 등)에 기초한 또는 공상 세계나 다른 가상 세계, 이벤트, 경치, 구조물, 등에 기초한 임의 의 실내 또는 실외 위치를 표현할 수 있다. 전술된 바와 같이, 실세계 오브젝트는 실세계 장면(예를 들어, 실세계 장면)으로부터 캡처되어 병합 된 현실 장면의 3D 공간으로 투사된 임의의 오브젝트를 표현할 수 있다. 마찬가지로, 가상 오브젝트 는 병합된 현실 장면을 생성하기 위해 실세계 오브젝트와 함께 3D 공간에 투사된 가상 오 브젝트를 표현할 수 있다. 이와 같이, 가상 오브젝트는, 생물이던 무생물이던, 적어도 하나의 가상 뷰포인 트들로부터 탐지가능한(예를 들어, 뷰잉가능한) 임의의 가상 오브젝트일 수 있다. 실세계 오브젝트와 같이, 명료함을 위해 가상 오브젝트가 비교적 간단한 기하학적 모양으로 그려졌지만, 가상 오브젝트 는 다양한 수준들의 복잡도를 갖는 다양한 유형들의 오브젝트들을 표현할 수 있다는 것이 이해될 것이다. 기하 학적 모양 보다는, 예시로서, 가상 오브젝트는 사람이나 다른 생물, 불투명한 고체, 액체, 가스, 또는 벽, 천장, 바닥과 같은 거의 끊김이 없는 오브젝트, 또는 여기에 설명된 임의의 다른 유형의 오브젝트와 같은 임의 의 생물 또는 무생물 오브젝트 또는 표면을 표현할 수 있으며, 또는 이는 특정 구현에 사용될 수 있다. 또한 실 세계 오브젝트와 같이, 가상 오브젝트가 각각의 상이한 가상 뷰포인트에서 뷰잉될 때 상이하게 보일 수 있도록, 가상 오브젝트도 다양한 표면들을 포함할 수 있으며, 이는 후술될 것이다. 3D 공간 내의 가상 뷰포인트들도 또한 병합된 현실 장면 내에 나타내진다. 여기서 사용된 바와 같이, 3D 공간 \"내의\" 가상 뷰포인트는 임의의 적합한 방식으로 3D 공간에 관련하여 위치된, 기울어진, 향해있 는, 등의 가상 뷰포인트를 의미할 수 있다. 예를 들어, 3D 공간 내의 가상 뷰포인트는 3D 공간의 적어도 몇몇의 부분의 뷰를 제공하기 위해, 3D 공간 내에 포함된, 3D 공간을 들여다보는 관점으로 3D 공간 외부에 있는, 다른 가상 뷰포인트들과 함께 3D 공간을 둘러싸는, 및/또는 임의의 적합한 방식으로 3D 공간에 이와 달리 연관된 가 상 뷰포인트일 수 있다. 여기서 사용된 바와 같이, 가상 뷰포인트들과 같은 \"가상 뷰포인트들\"은 (예를 들어, 위에서 도 2와 관련 하여 실세계 장면이 캡처된다고 설명되었던 3D 캡처 장치들의 유리한 지점들과 같은) 실세계 장 면을 캡처하는 3D 캡처 장치들에 관련하여 설명된 \"유리한 지점들\"과 유사할 수 있다(그러나 상이하기도 함). 구체적으로, 가상 뷰포인트들은 유리한 지점들과 유사할 수 있는데, 그 둘 모두가 어떤 유형의 장면 또는 3D 공 간을 뷰잉하는 특정 위치들, 각도들, 및/또는 관점들에 연관된다는 점에서 그러하다. 이 방식으로, 가상 뷰포인 트들은 여기서 설명된 실제의 물리적 3D 캡처 장치들과 유사한 가상 3D 캡처 장치들로 생각될 수 있다. 그러나, 유리한 지점들이 실세계 위치들에 배치된 물리적 3D 캡처 장치들과 연관관계가 있을 수 있지만, 가상 뷰포인트 들은 단지 가상적인 개념으로만 존재할 수 있다. 예시로서, 병합된 현실 장면의 3D 공간이 실세계 장면에 기초하는 예에서, 가상 뷰포인트들은 실세계 장면에 연 관된 가상 위치들(예를 들어, 실세계 장면이 캡처되는 유리한 지점들의 위치들과 같은 가상 위치들)과 연관관계 가 있을 수 있다. 그러나, 3D 공간이 실세계 장면에 기초하든 또는 가상 장면에 기초하든, 가상 뷰포인트들은 단지 가상적인 것일 뿐이고, 따라서 실세계 위치들에 배치된 임의의 물리적 3D 캡처 장치들에 반드시 대응하지 않을 수 있다. 그 결과, 실세계 장면의 복수의 상이한 유리한 지점들(예를 들어, 실세계 장면의 유리한 지 점들)의 제1 공간 구성은 병합된 현실 장면의 3D 공간 내의 복수의 가상 뷰포인트들(예를 들어, 3D 공간 내의 가상 뷰포인트들)의 제2 공간 구성과 별개일 수 있다. 예를 들어, 3D 공간이 유리한 지점 들에 배치된 3D 캡처 장치들에 의해 캡처된 실세계 장면에 기초하더라도, 어떤 예들에서, 가상 뷰포인트들은 반드시 유리한 지점들에 맞춰 정렬될 필요는 없으며, 오히려 유리한 지점들과 별 개일(즉, 상이한 각각의 위치들에 위치하고 및/또는 장면 상에서 상이한 각각의 각도들 및 관점들을 가질) 수 있다. 다른 예들에서, 가상 뷰포인트들은 각각의 유리한 지점들에 맞춰 정렬되고 및/또는 그들과 연 관관계가 있을(예를 들어, 종속적일) 수 있다. 도시된 바와 같이, 각각의 가상 뷰포인트는 3D 공간에 관련하여 특정 위치에 배치된 라벨링된 원으로 도 8에 표현될 수 있다. 각각의 가상 뷰포인트는 가상 뷰포인트에 연관된 시계를 나타내기 위해 그것으로 부터 나오는 점선들로 묘사된다. 가상 뷰포인트들에 연관된 위치들은 3D 공간에 관련하여 고정될 수 있지만, 후술될 바와 같이, 그 고정된 위치들은 장면 제어 시스템들 중 하나에 의해 수정될 수 있다. 추가 적으로, 몇몇 예들에서, 3D 공간 및 가상 뷰포인트들은 (예를 들어, 우주선, 열기구, 등과 같은 탈 것 전용 3D 공간과 같은) 병합된 현실 장면을 함께 지나치며 이동할 수 있는 것이 이해될 것이다. 도시된 바와 같이, 몇몇 예들에서, 가상 뷰포인트들이 배치된 고정된 위치들은 3D 공간에 연관된 적어도 2차원을 따라 (예를 들어, 땅과 같은 평면을 따라) 3D 공간을 둘러쌀 수 있다. 다른 예들에서, 위치들(80 6)은 (예를 들어, 또한 802 위아래에 위치들을 포함시킴으로써) 3 차원을 따라 3D 공간을 더 둘러쌀 수 있다. 도 8에 나타낸 각각의 가상 뷰포인트들은, 3D 공간이 나중에 임의 가상 뷰포인트들에서 렌더링될 수 있도록 다양한 각도들에서 3D 공간을 캡처하기 위해 3D 공간을 향해 안쪽으로 기울어지지만, 어떤 예 들에서는, 하나 이상의 가상 뷰포인트들이 3D 공간을 둘러싸는 오브젝트들에 대한 관점을 획득하기 위해 바깥쪽으로 (예를 들어, 3D 공간으로부터 멀어져) 기울어질 수 있다는 것이 이해될 것이다. 예시로서, 360-도 가상 뷰포인트가 3D 공간의 중앙에 위치되어(명시적으로 도시되지 않음), 추가적인 관점 들에서 3D 공간 내에 포함된 실세계 및/또는 가상 오브젝트들을 나타내는 데이터 및/또는 3D 공간 외 부의 가상 오브젝트들을 나타내는 데이터를 제공할 수 있다. 도 8은 병합된 현실 장면에 만들어질 수 있는 예시적인 수정들을 더 나타낸다. 몇몇 예들에서, 시스템 은 복수의 엔트리들을 나타내는 엔트리 설명 데이터(즉, 3D 공간 내에 포함된 실세계 오브젝트, 가상 오브젝트, 가상 뷰포인트들, 및/또는 임의의 다른 엔트리들을 나타내는 데이터)를 수정하기 위 한 명령을 수신할 수 있고, 그 명령의 수신에 응답하여, 그 명령에 따라 복수의 엔트리들을 나타내는 엔트리 설 명 데이터를 수정할 수 있다. 예를 들어, 명령은 만들어질 수정을 설명하는 JSON 코드 또는 다른 적합한 오브젝 트 설명 코드를 사용하여 임의의 장면 제어 시스템들에 의해 (예를 들어, 웹 소켓(web socket) 또는 다른 적합한 유형의 통신을 통해) 송신될 수 있다. 병합된 현실 장면 내에 포함된 엔트리들은 부분적으로 수정되는 엔트리의 유형에 의해 결정될 수 있는 임 의의 적합한 방식으로 수정될 수 있다. 예를 들어, 수정되는 엔트리가 가상 오브젝트가면, 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 명령에 따라 수정하는 것은 복수의 엔트리들에 추가적인 가상 오브젝트를 추가 하는 것을 포함할 수 있다. 추가적으로 또는 대안적으로, 수정은 복수의 엔트리들 내에 포함된 가상 오브젝트를 추가적인 가상 오브젝트로 대체하는 것, 복수의 엔트리들에서 가상 오브젝트를 제거하는 것, 복수의 엔트리들 내에 포함된 가상 오브젝트의 적어도 하나의 속성을 수정하는 것, 및/또는 다른 엔트리들에 관련하여 및/또는 병합된 현실 장면의 3D 공간에 관련하여 가상 오브젝트를 이와 달리 수정하는 것을 포함할 수 있다. 수정되는 엔트리가 실세계 오브젝트(예를 들어, 실세계 오브젝트)이면, 엔트리 설명 데이터를 수정하는 것 은 가상 오브젝트들에 대하여 전술된 것과 동일한 또는 유사한 방식으로 수행될 수 있다. 그러나, 실세계 장면 에 연관된 데이터를 캡처하는 방식으로 인해, 하나의 실세계 오브젝트를 실세계 장면 내의 다른 것과 구별짓는 메타데이터가 실세계 장면을 표현하는 캡처된 데이터에 연관되지 않는 한, (예를 들어, 다양한 구별되는 실세계 오브젝트들을 포함하는) 전체 실세계 장면은 단일 실세계 오브젝트로 처리될 수 있다는 것이 이해될 것이다. 예 를 들어, 시스템은 동일한 특정 시점에 실세계 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터를, 실세계 장면 내에 포함된 다른 추가적인 실세계 오브젝트들(즉, 도 2 또는 도 8에 명시적으로 도시되지 않 은 실세계 장면 내에 포함된 실세계 오브젝트들)의 다른 표면들을 나타내는 다른 컬러 데이터 및 다른 깊 이 데이터와 구별짓는 제1 프레임 세트(예를 들어, 실세계 장면 내의 오브젝트들을 나타내는 프레임 세트 (402-1))에 연관된 메타데이터를 수신하도록 구성될 수 있다. 이와 같이, 엔트리 설명 데이터가 생성된 병합된 현실 장면의 3D 공간에 포함된 복수의 엔트리들은 하나 이상의 다른 추가적인 실세계 오브젝트들을 더 포함할 수 있다. 추가적인 실세계 오브젝트들은 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림 (즉, 운송 스트림 내의 컬러 비디오 데이터 스트림(500-1-C) 및 깊이 비디오 데이터 스트림(500-1-D)) 내 에 포함된 추가적인 실세계 오브젝트들의 다른 표면들을 나타내는 다른 컬러 데이터 및 다른 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내에 적어도 부분적으로 정의될 수 있다. 추가적으로, 실세계 오브젝트(20 4)의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들 및 추가적인 실세계 오브젝트들의 다른 표면들을 나타내 는 다른 컬러 데이터 및 다른 깊이 데이터에의 링크들은 실세계 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터를 추가적인 실세계 오브젝트들의 다른 표면들을 나타내는 다른 컬러 데이터 및 다른 깊이 데이터와 구별 짓는 메타데이터에 기초할 수 있다. 수정되는 엔트리가 가상 뷰포인트(예를 들어, 가상 뷰포인트들 중 하나)이면, 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 명령에 따라 수정하는 것은 복수의 엔트리들에 추가적인 가상 뷰포인트를 추가하는 것을 포함할 수 있다. 추가적으로 또는 대안적으로, 수정하는 것은 복수의 엔트리들 내에 포함된 복수의 가상 뷰포인 트들 중 적어도 하나를 수정하는 것, 복수의 엔트리들에서 복수의 가상 뷰포인트들 중 적어도 하나를 제거하는 것, 등을 포함할 수 있다. 예를 들어, 가상 뷰포인트들 중 하나(예를 들어, 가상 뷰포인트(806-1))에 연관 된 시계는 3D 공간의 상이한 각도 상에서의 관점을 획득하도록 변경되거나 방향이 돌려질(turn) 수 있다.다른 예들에서, 가상 뷰포인트들은 (예를 들어, 3D 공간 내의 특정한 가상 오브젝트에 관련하여 줌 인(zoom in) 또는 줌 아웃(zoom out) 효과를 제작하기 위해) 안쪽으로 또는 바깥쪽으로 이동되거나, 복수의 가 상 뷰포인트들에서 제거되거나, 또는 이와 달리 수정될 수 있다. 다른 예로서, 오브젝트들(204 및/또는 804) 상의 다른 관점(예를 들어, 가상 뷰포인트들(806-1 내지 806-8) 중 하나에 의해 잘 커버(cover)되지 않는 관점)을 획득하기 위해 추가적인 가상 뷰포인트가 복수의 가상 뷰포인트들에 추가될 수 있다. 전술된 바와 같이, 몇몇 예들에서, 가상 오브젝트와 같은 가상 오브젝트는 가상 오브젝트를 수정하기 위한, 장면 제어 시스템들 중 하나로부터의 직접적인 명령에 기초하여 수정(예를 들어, 3D 공간과 관 련하여 이동 및/또는 회전)될 수 있다. 그러나, 다른 예들에서, 가상 오브젝트는 3D 공간 내에 포함된 다 른 엔트리들과의 상호작용들에 기초하여 자동으로 수정(즉, 장면 제어 시스템으로부터의 명시적인 명령에 기초하지 않지만 동일한 또는 상이한 방식들로 수정)될 수 있다. 보다 구체적으로, 예를 들어, 복수의 엔트리들 을 나타내는 엔트리 설명 데이터의 시스템에 의한 생성은 오브젝트들 사이에 (예를 들어, 가상 오브젝트들 사이에, 구별되는 실세계 오브젝트들 사이에, 가상 오브젝트와 실세계 오브젝트 사이, 등에) 가상 상호작용을 제작하는 것을 포함할 수 있다. 예를 들어, 시스템은 가상 오브젝트와 실세계 오브젝트 사이의 가상 상호작용을 제작(예를 들어, 생성, 추적, 시뮬레이션, 등)할 수 있고, 가상 상호작용은 물리-기반 오브젝 트 거동 및 AI-기반 오브젝트 거동 중 적어도 하나에 기초할 수 있다. 예시로서, 물리-기반 오브젝트 거동이 도 8에 도시된다. 시스템은, 가상 오브젝트 및 실세계 오 브젝트 각각이 동일한 가상 공간 내에 존재할 수 없는 고체 가상 오브젝트들을 표현한다고 결정할 수 있다. 따라서, 물리-기반 오브젝트 거동에 의해 나타낸 바와 같이, 가상 오브젝트가 실세계 오브젝트 와 상호작용하게 (예를 들어, 부딪히고, \"충돌\"하는, 등) 보이도록, 가상 오브젝트의 위치 및 방향 속성들이 물리 법칙들에 따라 수정될 수 있다. 다른 물리-기반 오브젝트 거동들은, 오브젝트들이 다른 것과 그 리고 물리적인 힘들 및 원리들(예를 들어, 중력, 운동량, 마찰력, 부력, 광 반사, 등)과 어떻게 상호작용하는지 를 정의하는 다른 물리 (예를 들어, 실세계 물리 또는 가상 세계에서만 적용되는 공상 물리) 법칙들을 모방할 수 있다. 이 물리-기반 오브젝트 거동들은 또한 시스템에 의해, 3D 공간 내에 포함된 복수의 엔트리 들을 나타내는 엔트리 설명 데이터에 적용될 수 있다. 게다가, AI-기반 오브젝트 거동들은 또한, 실세계 및/또 는 가상 오브젝트들이 다른 것과 그리고 오브젝트들이 위치하는 환경과 어떻게 상호작용하는지 정의하는 것을 도울 수 있다. 예를 들어, AI-기반 오브젝트 거동들은, 3D 공간 내에서 어디를 걸을지, 누구에게 말할지, 무엇을 말할지, 언제 위험으로부터 도망칠지, 등과 같은 \"선택들\"을 하기 위해 인공 지능을 사용할 수 있는 생 물들(예를 들어, 아바타들(avatars), 사람들, 동물들, 등)을 표현하는 엔트리들에 특히 적용될 수 있다. 도 7로 되돌아가서, 시스템은 시간적 시퀀스 내의 특정 시점들(예를 들어, 실시간 시퀀스, 가상 세계 내의 시간에 연관된 가상 시간선, 등)에의 복수의 엔트리들 내의 엔트리들의 상태들을 표현하는 엔트리 설명 프레임 들을 생성한다. 예를 들어, 도시된 바와 같이, 시스템은 특정 엔트리 설명 프레임(즉, 엔트리 설명 프레임 )을 생성할 수 있고, 엔트리 설명 프레임을 각각의 3D 렌더링 엔진들에 제공할 수 있다. 3D 렌 더링 엔진들은 서버측 3D 렌더링 엔진들(예를 들어, 네트워크에 걸친 및/또는 이와 달리 사용자에 의해 사 용되는 미디어 플레이어 장치와 같은 클라이언트측 장치들과 별개인 3D 렌더링 엔진들)일 수 있다. 몇몇 예들에 서, 3D 렌더링 엔진들은 개별적인 장치들(예를 들어, 개별적인 서버들, 서버 내의 개별적인 프로세서들, 등) 또는 개별적인 소프트웨어 처리들(예를 들어, 개별적인 명령어 스레드들(threads), 등)에 의해 구현될 수 있는 반면, 다른 예들에서, 3D 렌더링 엔진들은 공동 하드웨어 및/또는 소프트웨어 장치들 또는 처리들로 함께 통합될 수 있으며, 이는 특정 구현에 사용될 수 있다. 몇몇 구현들에서, 3D 렌더링 엔진들은 시스템 과 같은 병합된 현실 장면 캡처 시스템과 합동하여 동작하거나 심지어는 그것에 완전히 통합될 수 있는 반면, 다른 구현들에서, 3D 렌더링 엔진들은 (예를 들어, 클라우드-기반 처리 서비스들, 등을 제공하는 상이한 엔트리 로서) 개별적으로 동작할 수 있다. 3D 렌더링 엔진들에 제공되는 어떤 엔트리 설명 프레임들은 시간적 시퀀스 내의 특정 시점에 병합된 현실 장면(즉, 병합된 현실 장면)에 연관된 모든 엔트리들을 나타내는 상태 데이터를 포함하는 핵심 설명 프레 임들일 수 있는 반면, 다른 엔트리 설명 프레임들은, 시간적 시퀀스 내의 이전 시점에 모든 엔트리들의 상태를 나타내는 핵심 설명 프레임이 생성된 이후 수정된, 병합된 현실 장면에 연관된 단지 그러한 엔트리들의 상태를 (예를 들어, 시간적 시퀀스 내의 특정 시점에) 표현하는 갱신 설명 프레임들일 수 있다. 실례로서, 도 9는 시스템에 의해 생성될 수 있는 복수의 예시적인 엔트리 설명 프레임들(예를 들어, 엔트리 설명 프레임들(900-1 내지 900-12))을 도시한다. 하나의 엔트리 설명 프레임에서 다른 것을 가리키 는 화살표들에 의해 나타낸 바와 같이, 엔트리 설명 프레임들은 엔트리 설명 프레임(900-1)에서 시작하여엔트리 설명 프레임(900-12)으로 진행하는 시간적 시퀀스로 순서화될 수 있고, 그 후 시간적 시퀀스는 도 9에 명시적으로 도시되지 않은 추가적인 엔트리 설명 프레임들로 진행할 수 있다. 각각의 엔트리 설명 프레임 의 하단을 따라, 엔트리 설명 프레임의 유형(예를 들어, 핵심 설명 프레임 또는 갱신 설명 프레임)이 표시 된다. 구체적으로, 엔트리 설명 프레임들(900-1, 900-5, 및 900-9)은 핵심 설명 프레임들로 표시되고, 엔트리 설명 프레임들(900-2 내지 900-4, 900-6 내지 900-8, 및 900-10 내지 900-12)은 갱신 설명 프레임들로 표시된 다. 따라서, 이 예에서, 각각의 핵심 설명 프레임은 시간적 시퀀스에서 몇개의 (예를 들어, 3개) 갱신 설명 프레임 들이 뒤따라지고, 이것은 결국 시간적 시퀀스에서 다른 핵심 설명 프레임이 뒤따라 진다. 그러나, 도 9에 도시 된 핵심 설명 프레임들 및 갱신 설명 프레임들의 배열은 단시 예시적인 것일 뿐, 핵심 및 갱신 설명 프레임들의 배열은 임의의 방식으로 구성될 수 있으며, 이는 특정 구현에 사용될 수 있다는 것이 이해될 것이다. 예를 들어, 특별히 동적이지 않은 (즉, 엔트리들에 대한 다수의 수정들에 의해 영향을 받지 않는) 병합된 현실 장면 은 비교적 다수의 갱신 설명 프레임들이 뒤따르는 비교적 적은 핵심 설명 프레임들에 의해 표현될 수 있다. 반 대로, 더 동적인 병합된 현실 장면은 더 큰 비율의 핵심 설명 프레임들(오로지 핵심 설명 프레임들만 포함할 때 까지 많아짐) 및 더 적은 비율의 갱신 설명 프레임들(갱신 설명 프레임들을 포함하지 않을 때까지 적어짐)로 표 현될 수 있다. 도 9에 더 도시된 바와 같이, 각각의 엔트리 설명 프레임은 엔트리 설명 코드(예를 들어, 시스템에 의해 엔트리 설명 데이터가 생성된 엔트리들에 연관된 상태 데이터를 설명하기에 적합한 JSON 코드, XML 코드, 또는 다른 유형의 코드)를 포함하거나 그것으로 구현될 수 있고, 시간적 시퀀스 내에서의 다른 엔트리 설명 프 레임들에 대한 각각의 엔트리 설명 프레임의 위치를 나타내는 시퀀스 번호(예를 들어, 식별 번호 또 는 \"ID\")에 연관될 수 있다. 예를 들어, 도시된 바와 같이, 엔트리 설명 프레임(900-1)은, 엔트리 설명 프레임 (900-1)이 핵심 설명 프레임임을 나타내고 다른 핵심 설명 프레임들에 대한 그 프레임의 상대적인 위치(예를 들 어, \"1.0\"는 \"2.0\" 전에 옴)를 나타내기 위해, 정수(즉, \"1.0\")인 시퀀스 번호를 가질 수 있다. 그 후 엔트리 설명 프레임들(900-2 내지 900-4) 각각은 1로 시작하고(즉, 이 프레임들이 핵심 설명 프레임(1.0)에 대한 갱신 들임을 나타내기 위함), 시간적 시퀀스 내에서의 다른 갱신 설명 프레임들에 대한 갱신 설명 프레임들의 상대적 인 위치들을 나타내는 (예를 들어, \"1.1\"은 \"1.2\" 전에 옴) 하위-식별자들(즉, \".1\", \".2\", 및 \".3\")을 포함하 는 시퀀스 번호들에 연관될 수 있다. 이 엔트리 설명 프레임에 번호를 매기는 방법은 단지 예시적인 것일 뿐이 며, 임의의 적합한 프레임에 번호를 매기는 방법이 사용될 수 있으며, 이는 특정 구현에 사용될 수 있다. 도 7로 되돌아 가서, 엔트리 설명 프레임이 (예를 들어, 엔트리 설명 프레임들(900-1, 900-5, 또는 900- 9)과 같은) 핵심 설명 프레임이든 또는 (예를 들어, 도 9 내의 다른 엔트리 설명 프레임들과 같은) 갱신 설명 프레임이든지에 관계 없이, 엔트리 설명 프레임을 포함하는 엔트리 설명 프레임들의 시퀀스는 각각의 3D 렌더링 엔진에 연관된 각각의 가상 뷰포인트들에서 병합된 현실 장면의 3D 공간을 묘사하는 표면 데이터 프레임들을 렌더링하기 위해 3D 렌더링 엔진들에 의해 필요로 되는 모든 정보를 제공할 수 있 다. 이와 같이, 3D 렌더링 엔진들은 엔트리 설명 프레임들을 순서대로 수신 또는 처리할 필요가 없을 수 있다. 오히려, 3D 렌더링 엔진들은 특정한 3D 렌더링 엔진에 편리하거나 효율적일 수 있는 임의의 순 서로 (예를 들어, 각각이 그 시퀀스 내의 단일 엔트리 설명 프레임으로부터 생성될 수 있는) 각각의 표면 데이 터 프레임들을 렌더링할 수 있고, 표면 데이터 프레임들은 (예를 들어, 비디오 데이터 패키징 시스템 에 의해) 나중에 재순서화 및 동기화될 수 있다. 전술된 바와 같이, 가상 오브젝트들을 나타내는 어떤 상세한 정보(예를 들어, 가상 오브젝트 기하학적 구조들, 텍스처들, 등을 표현하는 이진 데이터)는 (예를 들어, 엔트리 설명 프레임 내에 명시적으로 포함되기 보다 는) 자산 저장 시스템에 저장될 수 있고, 엔트리 설명 프레임 내에 포함된 데이터에의 링크들을 통해 액세스될 수 있다. 자산 저장 시스템은 시스템 및/또는 3D 렌더링 엔진들과 개별적인 장치(예를 들어, 개별적인 서버, 개별적인 프로세서, 및 서버 내의 저장 기능, 등)에 의해, 개별적인 소프트웨어 처리들 (예를 들어, 개별적인 명령어 스레드들, 등)에 의해 구현되거나, 시스템 및/또는 3D 렌더링 엔진들과 공동 하드웨어 및/또는 소프트웨어 장치들 또는 처리들 내에 통합될 수 있으며, 이는 특정 구현에 사용될 수 있 다. 몇몇 구현들에서, 자산 저장 시스템은 시스템과 같은 병합된 현실 장면 캡처 시스템 및/또는 3D 렌더링 엔진들을 또한 포함하는 시스템과 합동하여 동작하거나 그것에 완전히 통합될 수 있는 반면, 다른 구현들에서, 자산 저장 시스템은 (예를 들어, 클라우드-기반 처리 서비스, 등을 제공하는 상이한 엔트리로 서) 개별적으로 동작할 수 있다. 마찬가지로, 어떤 예들에서, 실세계 오브젝트를 나타내는 엔트리 설명 데이터를 생성하는 것은 실세계 장면 내에 포함된 실세계 오브젝트의 3D 표현을 생성하고, 실세계 오브젝트의 3D 표면에 대한 포인 터들로서 시스템에 의해 생성된 엔트리 설명 데이터 내에 실세계 오브젝트를 적어도 부분적으로 정의한 실 세계 오브젝트의 표면의 컬러 데이터 및 깊이 데이터에의 링크들을 생성하는 것을 포함할 수 있다. 예를 들어, 시스템은 제1 프레임 세트 및 복수의 다른 프레임 세트들에 기초하여 생성된 컬러 비디오 데이터 스 트림 및 깊이 비디오 데이터 스트림(즉, 프레임 세트들(402-1 내지 402-N)에 기초하고 운송 스트림 내에 저장된 비디오 데이터 스트림(500-1 내지 500-8))을 (예를 들어, 사용자에 의해 사용되는 미디어 플레이어 장치 가 렌더링하는 방식과 유사한 방식으로) 렌더링함으로서 실세계 오브젝트의 3D 표현을 생성할 수 있다. 그 후 시스템은 컬러 데이터 및 깊이 데이터에의 링크들(즉, 시스템 또는 다른 적합한 서버측 시스템 내 에 유지될 수 있는 운송 스트림 내의 데이터에의 포인터들)을 생성하고 그 링크들을 엔트리 설명 프레임 에 포함시킬 수 있다. 따라서, 엔트리 설명 프레임은 다른 곳에 저장된 세부 정보에의 링크들과 함께 (예를 들어, 가상 오브젝트 들의 경우 자산 저장 시스템 내에, 실세계 오브젝트들의 경우 운송 스트림 내에, 등) 엔트리들을 나 타내는 상태 정보를 포함할 수 있고, 엔트리 설명 프레임 내의 링크들에 기초하여, 필요에 따라 각각의 3D 렌더링 엔진들에 의해 액세스될 수 있다. 세부 정보가 개별적으로 (예를 들어, 실세계 오브젝트 데이터에 대해서는 운송 스트림 내에 및 가상 오브젝트 데이터에 대해서는 자산 저장 시스템 내에) 유지될 수 있지만, 구성 내에 도시된 시스템, 엔트리 설명 프레임, 3D 렌더링 엔진들, 및/또는 다른 시스템들은 실세계 오브젝트들과 가상 오브젝트들을 유사한 방식들로 처리(예를 들어, 렌더링, 표현, 등)할 수 있다는 것이 주의된다. 몇몇 예들에서, 예시로서, 구성의 어떤 요소들(예를 들어, 비디오 데이터 패키징 시스템)은 실세계 오브젝트와 같은 실세계 오브젝트들과 가상 오브젝트와 같은 가상 오브젝트들 간에 심지어 구별할 필요가 없거나 구별하지 못할 수 있다. 다른 예들에서는, 실세계 오브젝트들과 가상 오브젝 트들 사이에 고려되는 차이들이 있을 수 있다. 예시로서, 가상 오브젝트들에 대한 모델들은 비교적 고정적일 수 있지만, 실세계 오브젝트들을 나타내는 데이터는 실시간으로 동적으로 변할 수 있어, 3D 렌더링 엔진들이 특정 순간에 연관된 데이터에 액세스할 필요가 있다. 임의의 경우, 엔트리 설명 프레임 내에 포함된 데이터와 엔트리 설명 프레임 내에 제공된 링크들을 사용하여 자산 저장 시스템 및/또는 운송 스트림으로부터 액세스된 데이터 간에, 3D 렌더링 엔진들 에 의해 국부적으로 유지되는 정보에 의존해야하지 않으면서, 3D 렌더링 엔진들은 각각의 가상 뷰포 인트들에서 3D 공간을 표현하는 표면 데이터 프레임들을 렌더링하는데 필요한 모든 정보에 액세스할 수 있다. 각각의 3D 렌더링 엔진은, 시스템에 의해 엔트리 설명 데이터가 생성된 복수의 엔트리들에 표현된 가 상 뷰포인트들 중 하나에 연관될 수 있다. 예를 들어, 3D 렌더링 엔진들(704-1 내지 704-8)(도 7에는 단지 3D 렌더링 엔진들(704-1 및 704-2)만이 명시적으로 도시됨)은 각각 가상 뷰포인트들(806-1 내지 806-8)(도 8에 나 타냄)에 연관될 수 있다. 이와 같이, 각각의 3D 렌더링 엔진은, 특정 3D 렌더링 엔진이 연관된 가상 뷰포인트의 관점(즉, 위치, 각도, 시계, 등)에서 보이는 것처럼, 각각의 표면 데이터 프레임을 렌더 링할 수 있다. 게다가, 전술된 바와 같이, 각각의 표면 데이터 프레임은 각각의 가상 뷰포인트에서 가상 오브젝트들의 외형을 표현하는 컬러 데이터(즉, 이미지 데이터)뿐만 아니라 깊이 데이터도 포함할 수 있다. 실례로서, 도 7은 표면 데이터 프레임(710-1)을 표현하는 이미지들을 도시하는데, 여기서, 표면 데이터 프레임 (710-1)은 가상 뷰포인트(806-1)에 연관된 3D 렌더링 엔진(704-1)에 의해 렌더링된 표면 데이터 프레임일 수 있 다(도 8 참조). 도시된 바와 같이, 표면 데이터 프레임(710-1)은 컬러 데이터 및 깊이 데이터 모두를 포함할 수 있는데, 이들은 컬러 데이터 및 깊이 데이터에 관련하여 전술된 것과 유사한 유형들의 데 이터를 표현할 수 있다(도 3 참조). 그러나, 컬러 데이터 및 깊이 데이터에 연관된 표면 데이터 프레 임과 대조적으로, 표면 데이터 프레임(710-1)은 실세계 오브젝트 및 가상 오브젝트 모두의 표현들뿐 만 아니라 병합된 현실 장면의 3D 공간 내에 포함될 수 있는 임의의 다른 오브젝트들을 포함할 수 있 다. 구체적으로, 도시된 바와 같이, (예를 들어, 시스템에 의해 구현된 오브젝트들 간의 물리-기반 가상 상호작용을 통해) 가상 오브젝트는 실세계 오브젝트 위에 놓여 있는 것으로 보인다. 시스템에 의해 제공된 각각의 엔트리 설명 프레임(예를 들어, 엔트리 설명 프레임)이 3D 공간 내의 상이한 가상 뷰포인트들에서의 병합된 현실 장면의 3D 공간의 렌더링들을 표현하는 각각의 프레 임 세트에 연관되도록, 표면 데이터 프레임들의 각각의 프레임 세트들(예를 들어, 표면 데이터 프레임들(710-1 내지 710-N)을 포함하는 프레임 세트)이 3D 렌더링 엔진들에 의해 생성될 수 있다. 도 7에 도시된 바 와 같이, 각각의 프레임 세트들 내의 (예를 들어, 표면 데이터 프레임(710-1 내지 710-N)을 포함하는) 각각의표면 데이터 프레임은 그 후 비디오 데이터 패키징 시스템에 제공될 수 있는데, 데이터 패키징 시스템 은 각각의 가상 뷰포인트에 연관된 각각의 컬러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트 림들을 생성하기 위해 표면 데이터 프레임들을 조작화, 동기화, 부호화, 압축, 조합, 및/또는 이와 달리 처리할 수 있다. 실례로서, 도 10은 구성의 특정 구성요소들의 더 상세한 뷰를 도시한다. 구체적으로, 도 10은 표면 데이터 프레임들의 완전한 세트(즉, 표면 데이터 프레임들(710-1 내지 710-8)을 포함하는 프레임 세트)를 렌더링 하는 8개의 3D 렌더링 엔진들(즉, 3D 렌더링 엔진들(704-1 내지 704-8))을 나타낸다. 표면 데이터 프레임 들(710-1 내지 710-8)은 병합된 현실 장면의 3D 공간 내에 포함된 실세계 및 가상 오브젝트들의 표면들의 컬러 및 깊이 데이터를 표현할 수 있다. 도시된 바와 같이, 예시로서, 프레임 세트는, 3D 공간 내의 실세 계 오브젝트 및 가상 오브젝트의 표면들이 3D 공간에 관련하여 상이한 가상 뷰포인트들(예를 들 어, 가상 뷰포인트들(806-1 내지 806-8))에서 보이는 것처럼, 그표면들을 나타내는 데이터를 포함한다. 도 10에 서 표면 데이터 프레임들이 도 7에 나타낸 컬러 데이터 이미지(즉, 컬러 데이터를 나타내는 이미지)와 유 사한 이미지에 의해 표현되도록 도시되었지만, 프레임 세트 내의 각각의 표면 데이터 프레임은 도 7에 나 타낸 깊이 데이터 이미지(즉, 깊이 데이터를 나타내는 이미지)와 유사한 이미지에 의해 표현될 수 있는 깊 이 데이터를 나타내는 데이터도 또한 포함할 수 있다는 것이 이해될 것이다. 전술된 바와 같이, 3D 렌더링 엔진들은 엔트리 설명 프레임에 기초하여 그리고 자산 저장 시스템 및/또는 운송 스트림으로부터 액스된 데이터에 기초하여 연관된 가상 뷰포인트들에서 표면 데 이터 프레임들(710-1 내지 710-N)을 각각 생성할 수 있다. 예를 들어, 병합된 현실 장면에 연관된 복수의 엔트리들을 나타내는 엔트리 설명 데이터는 자산 저장 시스템에 저장된 가상 오브젝트를 표현하는 컬 러 및 깊이 데이터에의 링크 및/또는 운송 스트림 내에 포함된 실세계 오브젝트를 표현하는 컬러 및 깊이 데이터에의 링크를 포함할 수 있다. 이와 같이, (전술된 바와 같이 시스템에 의해 생성되어 3D 렌더 링 엔진들에 제공될 수 있는) 엔트리 설명 프레임은 운송 스트림 및 자산 저장 시스템 내 에 각각 유지된 오브젝트들(204 및 804)을 표현하는 컬러 및 깊이 데이터에의 각각의 링크들을 포함하도록 생성 될 수 있다. 각각의 3D 렌더링 엔진들은 시스템으로부터 엔트리 설명 프레임을 수신하고 엔트리 설명 프레임 내에 포함된 링크들을 사용하여 운송 스트림 및 자산 저장 시스템 내에 각각 유지 된 오브젝트들(204 및 804)를 표현하는 컬러 및 깊이 데이터에 액세스하는 것을 포함하는 동작들을 수행함으로 서 그들 각각의 가상 뷰포인트들로부터 그들 각각의 표면 데이터 프레임들을 렌더링하도록 구성될 수 있다. 도시된 바와 같이, 엔트리 설명 프레임 내에 포함된 데이터 및 링크들을 통해 액세스된 컬러 및 깊 이 데이터 모두로, 3D 렌더링 엔진들은 3D 공간을 둘러싸는 가상 뷰포인트들로부터의 (예를 들어, 실 세계 오브젝트 및 가상 오브젝트 모두를 포함하는) 3D 공간의 뷰들을 제공하기 위해 표면 데이 터 프레임들을 렌더링할 수 있다. 전술된 실세계 장면을 표현하는 프레임 시퀀스와 같이, 표면 데이터 프레임들(710-1 내지 710-N) 각 각은 시간적 시퀀스 동안의 각각의 가상 뷰포인트들로부터의 병합된 현실 장면의 컬러 및 깊이 데이 터는 나타내는 표면 데이터 프레임들의 개별적인 프레임 시퀀스 내에 포함될 수 있다. 예를 들어, 표면 데이터 프레임(710-1)은 시간적 시퀀스 동안 가상 뷰포인트(806-1)로부터 가시가능한 오브젝트들(204 및 804)의 표면들 의 컬러 및 깊이 데이터는 나타내는 표면 데이터 프레임들의 제1 프레임 시퀀스 내에 포함될 수 있고, 표면 데 이터 프레임(710-2)은 시간적 시퀀스 동안 가상 뷰포인트(806-2)로부터 가시가능한 오브젝트들(204 및 804)의 표면들의 컬러 및 깊이 데이터는 나타내는 표면 데이터 프레임들의 제2 프레임 시퀀스 내에 포함될 수 있는, 등 이다. 즉, 표면 데이터 프레임들이 렌더링되면, 각각의 3D 렌더링 엔진은 표면 데이터 프레임들의 상 이한 각각의 시퀀스들 내의 다른 표면 데이터 프레임들을 계속 렌더링할 수 있다. 예를 들어, 3D 렌더링 엔진들 은 (예를 들어, 도 9에 나타낸 엔트리 설명 프레임들의 시퀀스와 같은) 엔트리 설명 프레임 다 음에 추가적인 엔트리 설명 프레임들을 수신할 수 있고, 추가적인 엔트리 설명 프레임들에 기초하여 표면 데이 터 프레임들을 더 생성할 수 있다. 실례로서, 도 11은 각각 유리한 지점들(806-1 내지 806-8)에서 가시가능한 가상 오브젝트 및 실세계 오브 젝트 모두의 표면들의 컬러 및 깊이 데이터는 나타내는 표면 데이터 프레임들의 복수의 예시적인 프레임 시퀀스들(예를 들어, 프레임 시퀀스들(1102-1 내지 1102-8))을 도시한다. 예를 들어, 도시된 바와 같이, 각각의 프레임 시퀀스 내에 나타낸 제1 표면 데이터 프레임들(즉, 오브젝트들(204 및 804)의 상이한 뷰들 이 도 11에서 가시가능하도록 커버되지 않은 표면 데이터 프레임들)은 (프레임 시퀀스(1102-1) 내에 포함된) 표 면 데이터 프레임(710-1) 내지 (프레임 시퀀스(1102-8) 내에 포함된) 표면 데이터 프레임(710-8)에 대응할 수있다. 따라서, 프레임 시퀀스들(1102-1 내지 1102-8)은 각각 3D 렌더링 엔진들(704-1 내지 704-8)에, 그리고 따 라서, 가상 뷰포인트들(806-1 내지 806-8)에 연관될 수 있다. 예를 들어, 프레임 시퀀스(1102-1)는 특정한 시간 적 시퀀스 동안 (예를 들어, 특정 실시간 기간, 몰입형 가상 현실 세계에 연관된 특정한 가상 시간선, 등) 가상 뷰포인트(806-1)에서 뷰잉된 병합된 현실 장면의 3D 공간 내에 포함된 실세계 오브젝트들 및 가상 오브젝트들의 컬러 및 깊이 모두를 표현할 수 있다. 마찬가지로, 프레임 시퀀스(1102-2)는 시간적 시퀀 스 동안 가상 뷰포인트(806-2)에서 뷰잉된 3D 공간 내에 포함된 실세계 및 가상 오브젝트들의 컬러 및 깊이를 표현할 수 있고, 프레임 시퀀스들(1102-3 내지 1102-8)에 대해서도 마찬가지이다. 앞에서 설명 및 나타낸 바와 같이, 3D 렌더링 엔진들에 의해 생성되고 프레임 시퀀스들 내에 포함된 각각의 표면 데이터 프레임들은 3D 렌더링 엔진들에 통신가능하게 연결될 수 있는 비디오 데이터 패키징 시스템에 트랜스미션되거나 이와 달리 전달될 수 있다. 표면 데이터 프레임들의 각각의 상이한 프레임 시 퀀스들에 기초하여(예를 들어, 도시된 바와 같이 각각의 추가적인 복수의 표면 데이터 프레임들을 포함하 는 프레임 세트 및 프레임 세트를 뒤따르는 추가적인 프레임 세트들에 기초하여), 비디오 데이터 패 키징 시스템은 각각의 가상 뷰포인트들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스 트림을 포함하는 (즉, 이전에 생성된 운송 스트림 이외의) 적어도 하나의 추가적인 운송 스트림을 생성할 수 있다. 예를 들어, 비디오 데이터 패키징 시스템은 각각의 프레임 시퀀스에 연관된 (즉, 각각의 3D 렌더링 엔진 및 가상 뷰포인트에 연관된) 개개의 컬러 비디오 데이터 스트림들 및 깊이 비디오 데 이터 스트림들을 포함하는 단일 운송 스트림을 생성할 수 있거나, 또는 비디오 데이터 패키징 시스템은 각 각의 프레임 시퀀스에 연관된 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림에 대한 상이한 운송 스트림들을 생성할 수 있다. 적어도 하나의 추가적인 운송 스트림은 여기에 설명된 임의의 방식으로 (예를 들어, 운송 스트림과 유사한 방식으로) 또는 임의의 다른 적합한 방식으로 생성될 수 있는데, 이는 특정 구현에 사용될 수 있다. 적어도 하나의 추가적인 운송 스트림이 생성되면, 비디오 데이터 패키징 시스템은 사용자에 연관된 클라이 언트측 미디어 플레이어 장치에 스트리밍하기 위한 적어도 하나의 추가적인 운송 스트림을 제공할 수 있다. 예 를 들어, 비디오 데이터 패키징 시스템은 운송 스트림을 미디어 플레이어 장치 자체에 스트리밍(예를 들어, 네트워크를 통해 트랜스미션)할 수 있거나, 운송 스트림이 다른 시스템에 의해 더욱 처리되고, (예를 들 어, 파이프라인에 연관된 다른 장치들, 처리들, 및/또는 시스템들에 의해 처리 및/또는 다시 패키징된 후) 미디 어 플레이어 장치로 스트리밍될 데이터 파이프라인 내에 운송 스트림을 포함시킬 수 있다. 후술될 바와 같이, 클라이언트측 미디어 플레이어 장치는 사용자에 의해 선택되고 병합된 현실 장면의 3D 공간 내의 임 의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트에서, 사용자에 의해 경험될 병합된 현실 장면의 3D 공간의 3D 표현을 (예를 들어, 적어도 하나의 추가적인 운송 스트림 내에 포함된 각각의 가상 뷰포인트들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림에 기초하여) 생성하도록 구성될 수 있다. 전술된 바와 같이, 몇몇 예들에서, 여기에 설명된 시스템 및/또는 다른 시스템들(예를 들어, 다른 서버측 시스템들) 및 장치들은 사용자들에 의해 경험될 가상 현실 미디어 콘텐츠를 생성하는데 사용될 수 있다. 예를 들어, 전술된 동작들 이외에, (예를 들어, 여기에 설명된 시스템, 비디오 데이터 패키징 시스템, 및/ 또는 다른 장치들 및 시스템들이 포함될 수 있거나, 이들 시스템들이 이와 달이 연관될 수 있는) 가상 현실 미 디어 콘텐츠 제공자 시스템은, 비디오 데이터 패키징 시스템이 생성 및 제공하는 적어도 하나의 추가적인 운송 스트림에 기초하여 가상 현실 미디어 콘텐츠를 더 생성 및 제공할 수 있다. 가상 현실 미디어 콘텐츠는 병 합된 현실 장면(예를 들어, 병합된 현실 장면)을 표현할 수 있고, 병합된 현실 장면에 관련된 임의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트로부터 경험되도록 사용자에게 표시될 수 있다. 예를 들어, 동적으로 선택가능한 가상 뷰포인트는, 사용자가 미디어 플레이어 장치를 사용하여 병합된 현실 장면을 경험하 는 동안, 미디어 플레이어 장치의 사용자에 의해 선택될 수 있다. 게다가, 사용자가 병합된 현실 장면 내의 임 의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트에서 병합된 현실 장면을 경험하도록 허용하기 위 해, 가상 현실 미디어 콘텐츠가 미디어 플레이어 장치에 (예를 들어, 시스템을 포함하거나 이와 달리 그것 에 연관된 가상 현실 미디어 콘텐츠 제공자 시스템에 의해) 제공될 수 있다. 실례로서, 도 12는, 시스템 및 비디오 데이터 패키징 시스템을 포함하는 예시적인 가상 현실 미디어 콘텐츠 제공자 시스템(\"제공자 시스템\")이 병합된 현실 장면을 경험하도록 사용자에 의해 사 용되는 예시적인 클라이언트측 미디어 플레이어 장치(\"미디어 플레이어 장치\")에 네트워크를 통해 제공되는 가상 현실 미디어 콘텐츠를 생성하는 예시적인 구성을 도시한다. 전술된 바와 같이 적어도 하나의 추가적인 운송 스트림이 프레임 시퀀스들에 기초하여 생성된 후, 미디어 플레이어 장치가 렌더링하도록 구성될 수 있는 가상 현실 미디어 콘텐츠를 형성하기 위해, 제공자 시스템 은 하나 이상의 운송 스트림들을 더욱 부호화, 패키징, 암호화, 또는 이와 달리 처리할 수 있다. 후술된 바와 같이, 예를 들어, 병합된 현실 장면 내의 (예를 들어, 가상 뷰포인트들 이외의 사용자가 관심 있어 할 수 있는 가상 뷰포인트들을 포함하는) 임의의 임의 가상 뷰포인트에서의 병합된 현실 장면의 뷰를 제시하기 위해, 가상 현실 미디어 콘텐츠는 미디어 플레이어 장치에 의해 렌더링될 수 있는 복수의 2D 비디오 데이터 스트림들(예를 들어, 각각의 가상 뷰포인트에 대한 컬러 데이터 및 깊이 데이터에 연관 된 2D 비디오 데이터 스트림들)을 포함하거나 표현할 수 있다. 추가적으로 또는 대안적으로, 가상 현실 미디어 콘텐츠는 임의 가상 뷰포인트들로부터 뷰잉될 수 있도록 또한 렌더링될 수 있는 병합된 현실 장면 내에 포 함된 가상 오브젝트들의 하나 이상의 입체 모델들(예를 들어, 3D 또는 4D 모델들)을 표현하는 데이터를 포함할 수 있다. 가상 현실 미디어 콘텐츠는 그 후 네트워크를 통해 사용자에 연관된 미디어 플레이어 장 치와 같은 하나 이상의 미디어 플레이어 장치들에 분배될 수 있다. 예를 들어, 사용자가 미디어 플 레이어 장치를 사용하여 병합된 현실 장면을 가상적으로 경험할 수 있도록, 제공자 시스템는 미디어 플레이어 장치에 가상 현실 미디어 콘텐츠를 제공할 수 있다. 몇몇 예들에서, 사용자가 (예를 들어, 병합된 현실 장면을 표현하는) 가상 현실 미디어 콘텐츠에 의 해 표현되는 몰입형 가상 현실 세계 내의 하나 이상의 개별적인 위치들에 제한되는 것은 바람직하지 않을 수 있 다. 이와 같이, 병합된 현실 장면이 가상 뷰포인트들뿐만 아니라 병합된 현실 장면 내의 (예를 들어, 3D 공간 내의 또는 그 주변의) 임의 가상 위치에 대응하는 임의의 동적으로 선택가능한 가상 뷰포인 트에서 렌더링되도록 허용하기 위해, 제공자 시스템은 병합된 현실 장면을 표현하는 가상 현실 미디 어 콘텐츠 내에 충분한 데이터를 제공할 수 있다. 예를 들어, 동적으로 선택가능한 가상 뷰포인트는, 사용자 가 미디어 플레이어 장치를 사용하여 병합된 현실 장면을 경험하는 동안, 사용자에 의 해 선택될 수 있다. 여기에 사용된 바와 같이, \"임의 가상 위치\"는 병합된 현실 장면에 연관된 공간 내의 (예를 들어, 병합된 현실 장면의 3D 공간 내의 또는 그 주변의) 임의의 가상 지점을 의미할 수 있다. 예를 들어, 임의 가상 위치들은 병 합된 현실 장면 주변의 고정된 위치들(예를 들어, 가상 뷰 포인트들에 연관된 고정된 위치들)에 제한되지 않고, 가상 뷰포인트들에 연관된 위치들과 3D 공간 내의 위치들 사이의 모든 위치들을 또한 포함한다. 게다가, 임의 가상 위치들은 가상 뷰포인트들 중 임의의 것에 맞춰 정렬되도록 제한되지 않은 임의 가상 뷰포인트들에 연관될 수 있다. 몇몇 예들에서, 이러한 임의 가상 위치들은 병합된 현실 장면 내 의 가장 바람직한 가상 뷰포인트들에 대응할 수 있다. 예시로서, 병합된 현실 장면이 농구 경기를 포함하 면, 사용자는 농구 코트 상의 임의의 임의 가상 위치 내에 있는, 경기를 경험하기 위한 가상 뷰포인트들 을 동적으로 선택할 수 있다. 예를 들어, 사용자는 마치 경기 중간에 농구 코드 위에 서있는 것처럼 농구 코트 위아래로 농구 공을 따라가면서 농구 경기를 경험하도록 그의 또는 그녀의 가상 뷰포인트들을 동적으로 선택할 수 있다. 즉, 예를 들어, 가상 뷰포인트들이 농구 코트 주변의 고정된 위치들에 위치될 수 있지만, 사용자 는 농구 코드 위의 임의의 임의 위치에 연관된, 경기를 경험하기 위한 임의 가상 뷰포인트들을 동적으로 선택할 수 있다. 네트워크는 제공자-특정 유선 또는 무선 네트워크(예를 들어, 케이블 또는 위성 반송파 네트워크 또는 이 동 전화 네트워크), 인터넷, 광역 네트워크, 콘텐츠 전달 네트워크, 또는 임의의 다른 적합한 네트워크를 포함 할 수 있다. 데이터는 임의의 통신 기술들, 장치들, 미디어, 및 프로토콜들을 사용하여 제공자 시스템과 미디어 플레이어 장치(및 명시적으로 도시되지 않은 다른 미디어 플레이어 장치들) 사이에 흐를 수 있으 며, 이는 특정 구현에 사용될 수 있다. 미디어 플레이어 장치는 제공자 시스템으로부터 수신된 가상 현실 미디어 콘텐츠에 액세스하고 그 것을 경험하도록 사용자에 의해 사용될 수 있다. 예를 들어, 미디어 플레이어 장치는 임의 가상 뷰 포인트(예를 들어, 사용자에 의해 선택되고 3D 공간 내의 임의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트)에서 사용자에 의해 경험되게 병합된 현실 장면의 3D 공간의 3D 표현을 (예를 들어, 2D 비디오 데이터 스트림들일 수 있는 적어도 하나의 추가적인 운송 스트림 내에 포함된 각각의 가상 뷰 포인트들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림에 기초하여) 생성하도록 구성될 수 있다. 이를 위해, 미디어 플레이어 장치는 몰입형 가상 현실 세계(예를 들어, 병합된 현실 장면을 표현하는 몰입형 가상 현실 세계)의 시계를 표시하고, 사용자가 몰입형 가상 현실 세계를 경험할 때 시계 내에 표시된 몰입형 가상 현실 세계를 동적으로 갱신하기 위해 사용자로부터 사용자 입력을 탐지할 수 있 는 임의의 장치를 포함하거나 그것에 의해 구현될 수 있다.예를 들어, 도 13은 가상 현실 미디어 콘텐츠를 경험하기 위해 사용자에 의해 사용될 수 있는 다양한 예 시적인 유형들의 미디어 플레이어 장치들을 도시한다. 구체적으로, 도시된 바와 같이, 미디어 플레이어 장치는 머리-장착 디스플레이 화면을 포함하는 머리-장착 가상 현실 장치(예를 들어, 가상 현실 게 임 장치), 개인용 컴퓨터 장치(예를 들어, 데스트톱 컴퓨터, 랩톱 컴퓨터, 등), 이동 또는 무선 장치 (예를 들어, 머리 장착 기기들에 의해 사용자의 머리에 장착될 수 있는 스마트폰, 테블릿 장치, 등), 또는 가상 현실 미디어 콘텐츠를 수신 및/또는 제시하는 것을 용이하게 하기 위해 특정 구현에 사용될 수 있는 임의의 다른 장치 또는 장치들의 구성과 같은 몇몇의 상이한 형태 요소들 중 하나를 취할 수 있다. 상이한 유형들의 미디어 플레이어 장치들(예를 들어, 머리-장착 가상 현실 장치들, 개인용 컴퓨터 장치들, 이동 장치들, 등)은 사용자에게 상이한 수준의 몰입도를 갖는 상이한 유형들의 가상 현실 경험들을 제공할 수 있다. 도 14는 병합된 현실 장면에 관련된 예시적인 임의 가상 위치에 대응하는 동적으로 선택가능한 가상 뷰포인트로 부터 경험되는 병합된 현실 장면을 표현하는 예시적인 가상 현실 미디어 콘텐츠가 사용자에게 표시되는 예시적인 가상 현실 경험을 나타낸다. 구체적으로, 슛이 들어가고 있는 병합된 현실 장면의 3D 공간 내의 농구대 바로 아래의 임의 가상 위치에 대응하는 가상 뷰포인트에서 병합된 현실 장면을 도시하는 시계 내 에 가상 현실 미디어 콘텐츠가 표시된다. 몰입형 가상 현실 세계를 둘러보고 및/또는 주변을 이동 하기 위한(즉, 경험하기 위해 가상 뷰포인트를 동적으로 선택하기 위한) 사용자 입력(예를 들어, 머리 움직임, 키보드 입력, 등)을 제공함으로써, 뷰어는 병합된 현실 장면에 기초한 몰입형 가상 현실 세계를 경험하게 할 수 있게 된다. 예를 들어, 시계는 사용자가 몰입형 가상 현실 세계를 용이하고 자연스럽게 둘러볼 수 있는 창을 제공할 수 있다. 시계는 미디어 플레이어 장치에 의해 (예를 들어, 미디어 플레이어 장치 의 디스플레이 화면 상에) 제공되고, 몰입형 가상 현실 세계 내의 사용자 주변의 오브젝트들을 묘 사하는 비디오를 포함할 수 있다. 추가적으로, 시계는, 사용자가 몰입형 가상 현실 세계를 경험할 때 사용자에 의해 제공되는 사용자 입력에 응답하여 동적으로 변할 수 있다. 예를 들어, 미디어 플레이어 장치는 사용자 입력(예를 들어, 시계가 제시되는 디스플레이 화면을 움직이거나 방향을 돌림)을 탐지할 수 있다. 응답으로, 시계는 이전 가상 뷰포인트 또는 가상 위치에서 보이는 오브젝트들 대신에, 상이한 오브젝트들 및/또는 상이한 가상 뷰포인트 또는 가상 위치에서 보이는 오브젝트들을 디스플레이 할 수 있다. 도 14에서, 몰입형 가상 현실 세계는 반-구로 나타내지는데, 이것은 사용자가 현재 선택한 농구대 아래 위치의 가상 뷰포인트로부터 실질적으로 앞, 뒤, 좌, 우, 및/또는 위인 몰입형 가상 현실 세계 내의 임의의 방향으로 사용자가 볼 수 있다는 것을 나타낸다. 다른 예들에서, 몰입형 가상 현실 세계는 180°구에 의해 전체 360°를 포함할 수 있어, 사용자가 아래도 내려다 볼 수 있다. 추가적으로, 사용자 는 몰입형 가상 현실 세계 내의 다른 위치들로 돌아다닐 수 있다(즉, 3D 공간 내의 상이한 동적으 로 선택가능한 가상 뷰포인트들을 동적으로 선택함). 예를 들어, 사용자는 하프코트에의 가상 뷰포인트, 농구대를 향하고 있는 자유투라인으로부터의 뷰포인트, 농구대 위에 매달려있는 가상 뷰포인트, 등을 선택할 수 있다. 도 15는 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오 브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 예시적인 방법을 나타낸다. 도 15는 일 실시예에 따른 예시적인 동작들을 나타내며, 다른 실시예들은 도 15에 도시된 임의의 동작을 생략, 추가, 재순서화, 및/ 또는 수정할 수 있다. 도 15에 도시된 하나 이상의 동작들은 시스템, 그것의 구현, 및/또는 시스템에 연관(예를 들어, 통신가능하게 연결되고, 상호동작하도록 구성되는 등)된다고 전술된 다른 시스템에 의해 수행 될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템이 실세계 장면의 복수의 상이한 유리한 지점들을 갖도록 실세계 장면에 관련하여 배치된 복수의 3D 캡처 장치들로부터 제1의 복수의 표면 데이터 프레임들을 포함하는 제1 프레 임 세트를 수신할 수 있다. 몇몇 예들에서, 제1의 복수의 표면 데이터 프레임들 내의 각각의 표면 데이터 프레 임은 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치에 의해 동일한 특정 시점에 캡처될 수 있다. 이와 같이, 각각의 이들 표면 데이터 프레임들은 복수의 상이한 유리한 지점들 내의 각각의 유리한 지점으로부터 캡처될 수 있다. 제1의 복수의 표면 데이터 프레임들 내의 표면 데이터 프레임들은, 특정 시점에 각각의 3D 캡처 장치의 각각의 유리한 지점에서 표면들이 보이는 것처럼, 실세계 장면 내에 포함된 실세계 오브젝트의 표면들의 컬러데이터 및 깊이 데이터를 표현할 수 있다. 동작은 여기에 설명된 임의의 방식으로 수행될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템이 복수의 3D 캡처 장치들 내의 각각의 3D 캡처 장치들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 운송 스트림을 생성할 수 있다. 몇몇 예 들에서, 운송 스트림은 (예를 들어, 동작에서) 복수의 3D 캡처 장치들로부터 수신된 제1 프레임 세트에 기초하여 및/또는 다른 시점에 캡처된 복수의 다른 프레임 세트들에 기초하여 생성될 수 있다. 동작은 여 기에 설명된 임의의 방식으로 수행될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템은 병합된 현실 장면의 3D 공간 내에 포함된 복수의 엔트리들을 나타내는 엔트리 설명 데이터를 생성할 수 있다. 예를 들어, 엔트리 설명 데이터는 운송 스트림에 기초하여 생 성될 수 있다. 다양한 유형들의 엔트리들이 병합된 현실 장면의 3D 공간 내에 포함된 복수의 엔트리들 내에 포 함될 수 있다. 예시로서, 엔트리들은 병합된 현실 장면 캡처 시스템에 통신가능하게 연결된 자산 저장 시스템 내에 저장된 가상 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내 에 적어도 부분적으로 정의된 가상 오브젝트를 포함할 수 있다. 다른 예로서, 엔트리들은, 동작에서 데이 터가 캡처된 실세계 오브젝트를 포함할 수 있다. 예시로서, 실세계 오브젝트는 복수의 3D 캡처 장치들로부터 수 신된 제1 프레임 세트 및 복수의 다른 프레임 세트들에 기초하여 생성된 컬러 비디오 데이터 스트림 및 깊이 비 디오 데이터 스트림 (즉, 동작에서 생성된 운송 스트림 내에 포함된 비디오 데이터 스트림) 내에 포함된 실세계 오브젝트의 표면들의 컬러 데이터 및 깊이 데이터에의 링크들에 의해 엔트리 설명 데이터 내에 적어도 부분적으로 정의될 수 있다. 게다가, 엔트리 설명 데이터가 동작에서 생성된 엔트리들은 제2의 복수의 표 면 데이터 프레임들을 포함하는 제2 프레임 세트가 렌더링되는 3D 공간 내의 복수의 가상 뷰포인트들을 포함할 수 있다. 예를 들어, 제2의 복수의 표면 데이터 프레임들은 병합된 현실 장면의 3D 공간 내에 포함된 가상 및 실세계 오브젝트들 모두의 표면들의 컬러 데이터 및 깊이 데이터를 나타낼 수 있다. 동작은 여기에 설명 된 임의의 방식들로 수행될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템은 병합된 현실 장면의 3D 공간 내에 포함된 복수의 엔트리들 내 의 적어도 하나의 엔트리의 상태를 나타내는 엔트리 설명 프레임을 생성할 수 있다. 몇몇 예들에서, 엔트리 설 명 프레임은 시간적 시퀀스의 특정 시점에서의 적어도 하나의 엔트리의 상태를 표현할 수 있다. 동작은 동작에서 생성된 복수의 엔트리들을 나타내는 엔트리 설명 데이터에 기초하여 수행될 수 있다. 동작 은 여기에 설명된 임의의 방식들로 수행될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템은 콘텐츠 제공자 시스템에 연관된 복수의 서버측 3D 렌더링 엔 진들에 엔트리 설명 프레임을 제공할 수 있다. 예를 들어, 복수의 서버측 3D 렌더링 엔진들 내의 각각의 3D 렌 더링 엔진은, 동작에서 엔트리 설명 프레임 데이터가 생성된 3D 공간 내의 복수의 가상 뷰포인트들로부터 의 상이한 가상 뷰포인트에 연관될 수 있다. 몇몇 구현들에서, 복수의 서버측 3D 렌더링 엔진들 내의 각각의 3D 렌더링 엔진들은 엔트리 설명 프레임에 기초하여 제2의 복수의 표면 데이터 프레임들 내에 포함된 상이한 표면 데이터 프레임을 렌더링하도록 구성될 수 있다. 동작은 여기에 설명된 임의의 방식들로 수행될 수 있다. 동작에서, 복수의 서버측 3D 렌더링 엔진들에 통신가능하게 연결된 및/또는 병합된 현실 장면 캡처 시스 템에 이와 달리 연관된 비디오 데이터 패키징 시스템은 적어도 하나의 추가적인 운송 스트림을 생성할 수 있다. 예를 들어, 비디오 데이터 패키징 시스템은 제2의 복수의 표면 데이터 프레임들을 포함하는 제2 프레임 세트 및 /또는 각각의 추가적인 복수의 표면 데이터 프레임들을 포함하는 추가적인 프레임 세트들에 기초하여 적어도 하 나의 추가적인 운송 스트림을 생성할 수 있다. 몇몇 예들에서, 적어도 하나의 추가적인 운송 스트림은, 동작 에서 엔트리 설명 데이터가 생성된 복수의 가상 뷰포인트들 내의 각각의 가상 뷰포인트들에 대한 컬러 비 디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함할 수 있다. 동작은 여기에 설명된 임의의 방식 들로 수행될 수 있다. 동작에서, 병합된 현실 장면 캡처 시스템에 연관된 비디오 데이터 패키징 시스템은 사용자에 연관된 클라 이언트측 미디어 플레이어 장치에 스트리밍할 적어도 하나의 추가적인 운송 스트림을 제공할 수 있다. 예시로서, 클라이언트측 미디어 플레이어 장치는 적어도 하나의 추가적인 운송 스트림 내에 포함된 각각의 가상 뷰포인트들에 대한 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림에 기초하여, 사용자에 의해 경험 될 병합된 현실 장면의 3D 공간의 3D 표현을 생성하도록 구성될 수 있다. 몇몇 예들에서, 3D 공간의 3D 표현은 사용자에 의해 선택되고 병합된 현실 장면의 3D 공간 내의 임의 가상 위치에 대응하는 동적으로 선택가능한 가 상 뷰포인트로부터 사용자에 의해 경험될 수 있다. 동작은 여기에 설명된 임의의 방식들로 수행될 수 있 다.어떤 실시예들에서, 여기에 설명된 하나 이상의 시스템들, 구성요소들, 및/또는 처리들은 하나 이상의 적절하게 구성된 컴퓨팅 장치들에 의해 구현 및/또는 수행될 수 있다. 이를 위해, 전술된 하나 이상의 시스템들 및/또는 구성요소들은 임의의 컴퓨터 하드웨어 및/또는 여기에 설명된 처리들 중 하나 이상을 수행하도록 구성된 적어도 하나의 비일시적(non-transitory) 컴퓨터-판독가능 매체 상에 실시된 컴퓨터-구현 명령어들(예를 들어, 소프트 웨어)을 포함하거나 그것들에 의해 구현될 수 있다. 특히, 시스템 구성요소들은 하나의 물리적 컴퓨팅 장치 상 에 구현될 수 있거나 하나 이상의 물리적 컴퓨팅 장치 상에 구현될 수 있다. 따라서, 시스템 구성요소들은 임의 의 개수의 컴퓨팅 장치들을 포함할 수 있고, 다수의 컴퓨터 동작 시스템들 중 임의의 것을 사용할 수 있다. 어떤 실시예들에서, 여기에 설명된 하나 이상의 처리들은 비일시적 컴퓨터-판독가능 매체 내에 실시되고 하나 이상의 컴퓨팅 장치들에 의해 실행가능한 명령어들로 적어도 부분적으로 구현될 수 있다. 일반적으로, 프로세서 (예를 들어, 마이크로프로세서)는 비일시적 컴퓨터-판독가능 매체(예를 들어, 메모리, 등)로부터 명령어들을 수 신하고, 그 명령어들을 실행하여, 여기에 설명된 하나 이상의 처리들을 포함하는 하나 이상의 처리들이 수행된 다. 이러한 명령어들은 다양한 알려진 컴퓨터-판독가능 매체 중 임의의 것을 사용하여 저장 및/또는 트랜스미션 될 수 있다. (프로세서-판독가능 매체로도 또한 언급되는) 컴퓨터-판독가능 매체는 컴퓨터에 의해 (예를 들어, 컴퓨터의 프 로세서에 의해) 판독될 수 있는 데이터(예를 들어, 명령어들)를 제공하는데 참여하는 임의의 고정 매체를 포함 한다. 이러한 매체는 비-휘발성 매체, 및/또는 휘발성 매체를 포함하는 많은 형태들을 취할 수 있지만, 이에 제 한되는 것은 아니다. 비-휘발성 매체는, 예를 들어, 광학 또는 자기 디스크들 및 다른 영구 메모리를 포함할 수 있다. 휘발성 매체는, 예를 들어, 전형적으로 주 메모리를 이루는 동적 랜덤 액세스 메모리(dynamic random access memory(\"DRAM\"))를 포함할 수 있다. 흔한 형태들의 컴퓨터-판독가능 매체는, 예를 들어, 디스크, 하드 디스크, 자기 테이프, 임의의 다른 자기 매체, 콤팩트 디스크 판독-전용 메모리(compact disc read-only memory(\"CD- ROM\")), 디지털 비디오 디스크(\"DVD\"), 임의의 다른 광학 매체, 랜덤 액세스 메모리(\"RAM\"), 프로 그래밍가능 판독-전용 메모리(\"PROM\"), 전기로 삭제가능한 프로그래밍가능 판독-전용 메모리(\"EPROM\"), FLASH- EEPROM, 임의의 다른 메모리 칩 또는 카트리지, 또는 컴퓨터가 판독할 수 있는 임의의 다른 유형 매체를 포함한 다. 도 16은 여기에 설명된 처리들 중 하나 이상을 수행하도록 구체적으로 구성될 수 있는 예시적인 컴퓨팅 장치 를 나타낸다. 도 16에 도시된 바와 같이, 컴퓨팅 장치는 통신 인터페이스, 프로세서, 저장 장치, 및 통신 기반구조를 통해 통신가능하게 접속된 입/출력(\"I/O\") 모듈을 포함할 수 있다. 예시적인 컴퓨팅 장치가 도 16에 도시되어 있지만, 도 16에 나타낸 구성요소들은 제한하는 것으로 의도된 것은 아니다. 추가적인 또는 대안적인 구성요소들이 다른 실시예들에서 사용될 수 있다. 도 16에 도시된 컴퓨팅 장치의 구성요소들이 이제 추가적으로 상세히 후술될 것이다. 통신 인터페이스는 하나 이상의 컴퓨팅 장치들과 통신하도록 구성될 수 있다. 통신 인터페이스의 예들에는 (네트워크 인터페이스 카드와 같은) 유선 네트워크 인터페이스, (무선 네트워크 인터페이스 카드와 같 은) 무선 네트워크 인터페이스, 모뎀, 오디오/비디오 접속부, 및 임의의 다른 적합한 인터페이스가 포함되지만, 이에 제한된 것은 아니다. 프로세서는 일반적으로 데이터를 처리하거나 여기에 설명된 명령어들, 처리들, 및/또는 동작들 중 하나 이상을 해석, 실행, 및/또는 실행 지시할 수 있는 임의의 유형 또는 형태의 처리 유닛(예를 들어, 중앙 처리 유 닛 및/또는 그래픽 처리 유닛)을 표현한다. 프로세서는 저장 장치 또는 다른 컴퓨터-판독가능 매체 내에 저장될 수 있는 것과 같은 하나 이상의 애플리케이션들 또는 다른 컴퓨터-실행가능 명령어들에 따라 동작들의 실행을 지시할 수 있다. 저장 장치는 하나 이상의 데이터 저장 매체, 장치들, 또는 구성들을 포함할 수 있고, 데이터 저장 매체 및/또는 장치의 임의의 유형, 형태, 및 조합을 사용할 수 있다. 예를 들어, 저장 장치는 하드 디스크, 네 트워크 드라이브, 플래시 드라이브, 자기 디스크, 광학 디스크, RAM, 동적 RAM, 다른 비-휘발성 및/또는 휘발성 데이터 저장 유닛들, 또는 그들의 조합 또는 하위 조합을 포함할 수 있지만, 이에 제한되는 것은 아니다. 여기 에 설명된 데이터를 포함하는 전자 데이터는 저장 장치 내에 일시적으로 및/또는 영구적으로 저장될 수 있다. 예를 들어, 프로세서가 여기에 설명된 동작들 중 임의의 것을 수행하도록 지시하게 구성된 하나 이 상의 실행가능 애플리케이션들을 표현하는 데이터는 저장 장치 내에 저장될 수 있다. 몇몇 예들에 서, 데이터는 저장 장치 내에 존재하는 하나 이상의 데이터베이스들 내에 배치될 수 있다. I/O 모듈은 사용자 입력을 수신하고 사용자에게 출력을 제공하도록 구성된 하나 이상의 I/O 모듈들을 포 함할 수 있다. 하나 이상의 I/O 모듈들은 단일 가상 현실 경험에 대한 입력을 수신하기 위해 사용될 수 있다. I/O 모듈은 입력 및 출력 능력들을 지원하는 임의의 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들 의 조합을 포함할 수 있다. 예를 들어, I/O 모듈은 키보드 또는 키패드, 터치스크린 구성요소(예를 들어, 터치스크린 디스플레이), 수신기(예를 들어, RF 또는 적외선 수신기), 움직임 감지기들, 및/또는 하나 이상의 입력 버튼들을 포함하는(이에 제한되지는 않음), 사용자 입력을 캡처하기 위한 하드웨어 및/또는 소프트웨어를 포함할 수 있다. I/O 모듈은 그래픽 엔진, 디스플레이(예를 들어, 디스플레이 스크린), 하나 아상의 출력 드라이버들(예를 들어, 디스플레이 드라이버들), 하나 이상의 오디오 스피커들, 및 하나 이상의 오디오 드라이버들을 포함하는 (이에 제한되지 않음), 사용자에 출력을 제시하기 위한 하나 이상의 장치들을 포함할 수 있다. 어떤 실시예들에 서, I/O 모듈은 사용자에게 제시할 그래픽 데이터를 디스플레이에 제공하도록 구성된다. 그래픽 데이터는 하나 이상의 그래픽 사용자 인터페이스들 및/또는 임의의 다른 그래픽 콘텐츠를 표현할 수 있으며, 이는 특정 구현에 사용될 수 있다. 몇몇 예들에서, 여기에 설명된 기능들 중 임의의 것은 컴퓨팅 장치의 하나 이상의 구성요소들에 의해 또 는 그것들 내에 구현될 수 있다. 예를 들어, 저장 장치 내에 존재하는 하나 이상의 애플리케이션들(161 2)은 시스템의 실세계 장면 캡처 기능 또는 병합된 현실 엔트리 상태 추적 기능에 연관된 하나 이상의 동작들 또는 기능들을 수행하게 프로세서에 지시하도록 구성될 수 있다(도 1 참조). 마찬가지로, 시스템의 저장 기능은 저장 장치에 의해 또는 그것 내에 구현될 수 있다. 전술된 실시예들이 개인이 제공하는 개인 정보를 수집, 저장, 및/또는 이용하는 한, 그러한 정보는 개인정보 보 호에 관한 모든 해당 법률에 따라 사용되어야 한다는 것이 이해되어야 한다. 추가적으로, 그러한 정보의 수집, 저장, 및 사용은, 예를 들어, 상황 및 정보 유형에 적합할 수 있는 잘 알려진 \"사전동의(opt-in)\" 또는 \"사후철 회(opt-out)\" 처리들을 통해 그러한 활동에 대한 개인의 동의의 대상이 될 수 있다. 개인 정보의 저장과 사용은, 예를 들어, 특히 민감한 정보에 대한 다양한 암호화 및 익명화 기술들을 통해 정보 유형을 반영하는 적 절하게 안전한 방식으로 행해질 수 있다. 전술에서, 다양하고 예시적인 실시예들이 첨부된 도면들을 참조하여 설명되었다. 그러나, 다음의 청구항들에 언 급된 본 발명의 영역에서 벗어나지 않으면서, 다양한 수정들 및 변경들이 행해질 수 있고, 추가적인 실시예들이 구현될 수 있다는 것이 자명해질 것이다. 예를 들어, 여기에 설명된 일 실시예의 어떤 특징들은 여기에 설명된 다른 실시예의 특징들과 조합되거나 그들을 대신할 수 있다. 따라서 설명 및 도면들은 제한적인 의미보다는 예 시로 간주된다."}
{"patent_id": "10-2019-7035521", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면들은 다양한 실시예들을 나타내며, 명세서의 일부이다. 나타낸 실시예들은 단지 예들이며, 게시물의 범위를 제한하지 않는다. 도면들에 걸쳐, 동일하거나 유사한 도면 참조 부호들은 동일하거나 유사한 요소들을 지정한다. 도 1은 여기에 설명된 원리들에 따른, 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 예시적인 병합된 현실 장면캡처 시스템을 나타내는 도면. 도 2는 여기에 설명된 원리들에 따라, 도 1의 병합된 현실 장면 캡처 시스템이 예시적인 실세계 오브젝트를 포 함하는 예시적인 실세계 장면을 표현하는 데이터를 캡처하기 위해 복수의 3-차원(\"3D\") 캡처 장치들과 상호동작 하는 예시적인 구성을 나타내는 도면. 도 3a는 여기에 설명된 원리들에 따라, 도 2의 실세계 오브젝트를 나타내는 표면 데이터 프레임을 캡처하는 예 시적인 3D 캡처 장치를 도시하는 도면. 도 3b는 여기에 설명된 원리들에 따른, 도 3a의 3D 캡처 장치에 의해 캡처된 표면 데이터 프레임 내에 표현된 컬러 데이터(color data)의 예시적인 그래픽 묘사를 나타내는 도면. 도 3c는 여기에 설명된 원리들에 따른, 도 3a의 3D 캡처 장치에 의해 캡처된 표면 데이터 프레임 내에 표현된 깊이 데이터의 예시적인 그래픽 묘사를 나타내는 도면. 도 4는 여기에 설명된 원리들에 따른, 상이한 유리한 지점들로부터 도 2의 실세계 장면을 표현하기 위해 도 2의 3D 캡처 장치들에 의해 캡처된 각각의 복수의 예시적인 표면 데이터 프레임들을 포함하는 예시적인 복수의 프레 임 세트들을 나타내는 도면. 도 5는 예시적인 컬러 비디오 데이터 스트림 및 예시적인 깊이 비디오 데이터 스트림을 나타내는 도면으로서, 이 둘 모두는 여기에 설명된 원리들에 따라, 상이한 유리한 지점으로부터 도 2의 실세계 장면을 표현하기 위해 도 2의 특정한 3D 캡처 장치에 의해 캡처된 표면 데이터 프레임들에 기초하는 도면. 도 6은 여기에 설명된 원리들에 따른, 다른 컬러 비디오 데이터 스트림들 및 깊이 비디오 데이터 스트림들과 함 께 도 4의 컬러 비디오 데이터 스트림 및 깊이 비디오 데이터 스트림을 포함하는 예시적인 운송 스트림을 나타 내는 도면. 도 7은 여기에 설명된 원리들에 따라, 도 1의 병합된 현실 장면 캡처 시스템이 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생 성하는 것을 용이하게 하는 예시적인 구성을 나타내는 도면. 도 8은 여기에 설명된 원리들에 따른, 병합된 현실 장면의 예시적인 3D 공간 내에 가상 오브젝트, 실세계 오브 젝트, 및 복수의 가상 뷰포인트들과 같은 예시적인 복수의 엔트리들을 포함하는 예시적인 병합된 현실 장면을 나타내는 도면. 도 9는 여기에 설명된 원리들에 따라, 도 1의 병합된 현실 장면 캡처 시스템에 의해 생성될 수 있는 예시적인 엔트리 설명 프레임들을 나타내는 도면. 도 10은 여기에 설명된 원리들에 따라, 도 8의 병합된 현실 장면의 3D 공간 내에 포함되는 가상 및 실세계 오브 젝트들의 표면들의 컬러 데이터 및 깊이 데이터는 나타내는 표면 데이터 프레임들을 렌더링(rendering)하는 복 수의 예시적인 3-차원(\"3D\") 렌더링 엔진들을 나타내는 도면. 도 11은 여기에 설명된 원리들에 따른, 도 10의 3D 렌더링 엔진들에 의해 렌더링된 각각의 복수의 예시적인 표 면 데이터 프레임들을 각각 포함하는 예시적인 복수의 프레임 세트들을 나타내는 도면. 도 12는 여기에 설명된 원리들에 따라, 예시적인 가상 현실 미디어 콘텐츠 제공자 시스템이 병합된 현실 장면을 경험하기 위해 사용자에 의해 사용되는 예시적인 클라이언트측 미디어 플레이어 장치에 네트워크를 통해 제공되 는 가상 현실 미디어 콘텐츠를 생성하는 예시적인 구성을 나타내는 도면. 도 13은 여기에 설명된 원리들에 따른, 가상 현실 미디어 콘텐츠를 경험하기 위해 사용자에 의해 사용될 수 있 는 다양한 예시적인 유형들의 미디어 플레이어 장치들을 나타내는 도면. 도 14는 여기에 설명된 원리들에 따라, 병합된 현실 장면에 관련된 예시적인 임의 가상 위치에 대응하는 동적으 로 선택가능한 가상 뷰포인트로부터 경험되는 병합된 현실 장면을 표현하는 예시적인 가상 현실 미디어 콘텐츠 가 사용자에게 표시되는 예시적인 가상 현실 경험을 나타내는 도면. 도 15는 여기에 설명된 원리들에 따라, 상이한 비디오 데이터 스트림들 내의 상이한 유리한 지점들로부터 표현 된 가상 오브젝트 및 실세계 오브젝트에 기초하여 병합된 현실 장면을 생성하기 위한 예시적인 방법을 나타내는 도면.도 16은 여기에 설명된 원리들에 따른 예시적인 컴퓨팅 장치를 나타내는 도면."}
