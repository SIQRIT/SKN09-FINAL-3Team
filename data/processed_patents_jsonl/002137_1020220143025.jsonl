{"patent_id": "10-2022-0143025", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0009846", "출원번호": "10-2022-0143025", "발명의 명칭": "인공지능 엔진 및 인공지능 엔진의 학습 방법", "출원인": "삼성전자주식회사", "발명자": "이민하"}}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "복수의 데이터들을 학습하는 O-Ran(Open-Radio access network) 기반 인공지능(AI) 엔진(100)에 있어서, 로우 데이터들을 수신하는 버퍼(110);상기 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 파이프라인(120); 및상기 파이프 라인의 학습 결과를 저장하는 스토리지(130)를 포함하되,상기 파이프 라인(120)은,제1 경험 데이터를 처리하는 제1 시뮬레이션 엔진(122);상기 제1 시뮬레이션 엔진과 병렬적으로 작용하여 제2 경험 데이터를 처리하는 제2 시뮬레이션 엔진(123); 및 상기 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진으로부터 상기 처리된 제1 경험 데이터 및 제2 경험 데이터를 수신하고, 수신된 상기 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과에 기초하여 정책(Policy)을 생성하는 트레이너(121)를 포함하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 제1 시뮬레이션 엔진은,상기 제1 경험 데이터를 상호 학습하는 제1 에이전트 및 제1 시뮬레이터를 포함하고,상기 제2 시뮬레이션 엔진은,상기 제2 경험 데이터를 상호 학습하는 제2 에이전트 및 제2 시뮬레이터를 포함하는 제2 시뮬레이션 엔진을 포함하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1 항 또는 제2 항에 있어서상기 트레이너(121)는미리 정해진 코드에 기초하여 상기 학습 결과를 업데이트하여 상기 정책을 생성하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1 항 내지 제3 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,상기 정책에 기초하여 상기 제1 경험 데이터 및 제2 경험 데이터를 학습하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항 내지 제4 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,제1 파라미터에 의하여 상기 제1 시뮬레이션 엔진을 구동하고, 상기 제1 파라미터와는 상이한 제2 파라미터에의하여 상기 제2 시뮬레이션 엔진을 구동하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "공개특허 10-2024-0009846-3-제1 항 내지 제5 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,상기 경험 데이터를 모사하는 시뮬레이터 이미지에 의하여 상기 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진을구동하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1 항 내지 제6 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,상기 정책을 모사하는 정책 최적화 이미지에 의하여 트레이너를 구동하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1 항 내지 제7 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,상기 처리된 경험 데이터를 모사하는 에이전트(Agent) 이미지에 의하여 상기 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진을 구동하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1 항 내지 제8 항 중 어느 한 항에 있어서, 상기 파이프 라인(120)은,상기 정책에서 경험 데이터를 분리하고, 상기 분리된 경험 데이터에 기초하여 에이전트(Agent) 이미지를 생성하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1 항 내지 제9 항 중 어느 한 항에 있어서, 상기 스토리지(130)는,외부의 서버를 포함하는 인공지능 엔진."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "O-Ran(Open-Radio access network) 기반 인공지능(AI) 엔진이 복수의 데이터들을 학습하는 방법에 있어서,로우 데이터(Raw Data)들을 수신하는 단계(510);상기 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 단계(520); 및상기 학습 결과를 저장하는 단계(530)를 포함하되,상기 경험 데이터들을 추출하고 학습하는 단계(520)는,제1 경험 데이터를 처리하는 제1 단계(610);상기 제1 경험 데이터와 병렬적으로 제2 경험 데이터를 처리하는 제2 단계(620); 및 상기 제1 단계 및 제2 단계에서 처리된 상기 제1 경험 데이터 및 제2 경험 데이터를 수신하고, 수신된 상기 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고(630), 학습 결과에 기초하여 정책(Policy)을 생성하는 단계(640)를 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서,상기 제1 단계(610)는,공개특허 10-2024-0009846-4-상기 제1 경험 데이터를 상호 학습하는 것을 포함하고,상기 제2 단계(620)는,상기 제2 경험 데이터를 상호 학습하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11 항 또는 제12 항에 있어서상기 정책(Policy)을 생성하는 단계(640)는,미리 정해진 코드에 기초하여 상기 학습 결과를 업데이트하여 정책을 생성하는 것을 포함하는 인공지능 엔진의학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11 항 내지 제13 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,상기 정책에 기초하여 상기 제1 경험 데이터 및 제2 경험 데이터를 학습하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11 항 내지 제14 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,제1 파라미터에 의하여 상기 제1 단계를 수행하고, 상기 제1 파라미터와는 상이한 제2 파라미터에 의하여 상기제2 단계를 수행하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11 항 내지 제15 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,상기 경험 데이터를 모사하는 시뮬레이터 이미지에 의하여 상기 제1 단계 및 제2 단계를 수행하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11 항 내지 제16 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,상기 정책을 모사하는 정책 최적화 이미지에 의하여 상기 정책(Policy)을 생성하는 단계를 수행하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11 항 내지 제17 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,상기 처리된 경험 데이터를 모사하는 에이전트(Agent) 이미지에 의하여 상기 제1 단계 및 제2 단계를 수행하는것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11 항 내지 제18 항 중 어느 한 항에 있어서, 상기 경험 데이터들을 추출하고 학습하는 단계(520)는,공개특허 10-2024-0009846-5-상기 정책에서 경험 데이터 만을 분리하고, 상기 분리된 경험 데이터에 기초하여 에이전트(Agent) 이미지를 생성하는 것을 포함하는 인공지능 엔진의 학습 방법."}
{"patent_id": "10-2022-0143025", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항 내지 제19항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2022-0143025", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "개시된 실시예에 따른 복수의 데이터들을 학습하는 O-Ran(Open-Radio-network) 기반 인공지능(AI) 엔진은 로우 데이터들을 수신하는 버퍼, 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 파이프라인 및 파이프 라인 의 학습 결과를 저장하는 스토리지를 포함할 수 있다. 일 실시예에 따른 파이프 라인은, 제1 경험 데이터를 처리 하는 제1 시뮬레이션 엔진, 제1 시뮬레이션 엔진과 병렬적으로 작용하여 제2 경험 데이터를 처리하는 제2 시뮬레 이션 엔진 및 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진으로부터 처리된 제1 경험 데이터 및 제2 경험 데이터 를 수신하고, 수신된 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과에 기초하여 정책 (Policy)을 생성하는 트레이너를 포함할 수 있다."}
{"patent_id": "10-2022-0143025", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "개시된 실시예는 인공지능 엔진 및 인공지능 엔진의 학습 방법에 관한 것으로 더욱 상세하게는 복수의 데이터들 을 병렬적으로 학습하는 인공지능 엔진 및 인공지능 엔진의 학습 방법에 관한 것이다."}
{"patent_id": "10-2022-0143025", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 엔진의 학습 방법의 하나로 강화 학습이 활용되고있다. 강화 학습은 기본적으로 시행착오(Trial and Error)관점에서 데이터를 반복 학습 하는 방법으로 이루어지는데, 이는 데이터를 학습하는데 오랜 시간이 소요 되고, 비용도 계속 발생하는 문제점이 존재한다. 이에, 수신된 데이터를 빠르게 처리하고 학습하는 인공지능 엔 진이 필요하다."}
{"patent_id": "10-2022-0143025", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "개시된 실시예에 따른 복수의 데이터들을 학습하는 O-Ran(Open-Radio-network) 기반 인공지능(AI) 엔진은 로우 데이터들을 수신하는 버퍼, 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 파이프라인 및 파이프 라 인의 학습 결과를 저장하는 스토리지를 포함할 수 있다. 일 실시예에 따른 파이프 라인은, 제1 경험 데이터를 처리하는 제1 시뮬레이션 엔진, 제1 시뮬레이션 엔진과 병 렬적으로 작용하여 제2 경험 데이터를 처리하는 제2 시뮬레이션 엔진 및 제1 시뮬레이션 엔진 및 제2 시뮬레이 션 엔진으로부터 처리된 제1 경험 데이터 및 제2 경험 데이터를 수신하고, 수신된 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과에 기초하여 정책(Policy)을 생성하는 트레이너를 포함할 수 있다. 개시된 실시예에 따른 O-Ran(Open-Radio-network) 기반 인공지능(AI) 엔진이 복수의 데이터들을 학습하는 방법 은 로우 데이터(Raw Data)들을 수신하는 단계, 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 단계 및 학습 결과를 저장하는 단계를 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 제1 경험 데이터를 처리하는 제1 단계, 제1 경험 데이터와 병렬적으로 제2 경험 데이터를 처리하는 제2 단계 및 제1 단계 및 제2 단계에서 처리된 제1 경험 데이 터 및 제2 경험 데이터를 수신하고, 수신된 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과 에 기초하여 정책(Policy)을 생성하는 단계를 포함할 수 있다. 개시된 실시예에 따른 인공지능 엔진의 학습 방법은 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽 을 수 있는 기록매체로 구현될 수 있다."}
{"patent_id": "10-2022-0143025", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 발명에 대해 구체적으로 설명하기로 한다. 본 발명에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세 히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지 는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 한다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"...부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 실시예들에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자 가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 발명을 명확하게 설명하기 위해서 설명과 관 계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 본 명세서의 실시예에서 \"사용자\"라는 용어는 시스템, 기능 또는 동작을 제어하는 사람을 의미하며, 개발자, 관 리자 또는 설치 기사를 포함할 수 있다. 도 1은 개시된 실시예에 따른 인공지능(AI: Artificial Intelligence) 학습 시스템의 블록도이다. 도 1을 인공지능 학습 시스템은 인공지능 엔진 및 인공지능(AI) 모델 매니지먼트를 포함하고, 외 부의 RIC 플랫폼(Ran Intelligent Control Platform)으로 AI 모델(AL ML)을 배포할 수 있다. 일 실시예에 따른 인공지능 엔진은 데이터들을 학습하고 학습 결과에 기초하여 AI 모델(AL ML)을 생성한다. 예를 들면, 인공지능 엔진은 O-Ran(Open-Radio-network)상에서 복수의 로우 데이터들을 수신하 고, 수신된 로우 데이터(Raw Data)들로부터 경험 데이터를 추출하고, 추출된 경험 데이터들을 학습할 수 있다. 경험 데이터는 수신된 로우 데이터가 처리된 결과 생성된 데이터일 수 있다. 예를 들면, 인공지능 엔진은 복수의 로우 데이터들에 대하여 반복하여 처리 과정을 수행할 수 있는데, 경험 데이터는 로우 데이터가 적어도 한 번 이상 처리된 결과물일 수 있다. 일 실시예에 따른 인공지능 엔진은 경험 데이터를 학습하고, 해당 경험 데이터에 적합한 정책(Policy)를 포함한 AI 모델(AI ML)을 생성할 수 있다. 인공지능(AI) 모델 매니지먼트는 생성된 AI 모델(AI ML)을 RIC 플랫폼으로 배포할 수 있다. 예를 들 면, 인공지능(AI) 모델 매니지먼트는 경험 데이터로부터 생성된 AI 모델(AI ML)을 사용자 디바이스의 RIC 플랫폼으로 배포하고, 배포된 AI 모델(AI ML)에 기초하여 새로운 데이터들을 학습하게 할 수 있다. RIC 플랫폼은 임의의 사용자 디바이스에 존재하면서 새롭게 수신되는 데이터(Data)들을 학습하도록 AI 모 델(AI ML)을 수신하고, 새로운 데이터(Data)들을 인공지능 엔진으로 송신할 수 있다. 예를 들면, RIC 플랫 폼은 인공지능 학습 시스템에 적용되는 AI 모델(AI ML)에 의하여 데이터(Data)를 수신하고, 수신된 데 이터(Data)를 인공지능 엔진으로 전달할 수 있다. 도 2는 개시된 실시예에 따른 인공지능 엔진의 블록도이다. 도 2를 참조하면, 일 실시예에 따른 인공지능 엔진은 버퍼, 파이프 라인 또는 스토지리를 포함할 수 있다. 일 실시예에 따른 버퍼는 로우 데이터(Raw Data)를 수신할 수 있다. 일 실시예에 따른 로우 데이터(Raw Data)는 처리되지 않고 최초로 수신된 데이터일 수 있다. 예를 들면, 일 실시예에 따른 인공지능 엔진이 이미지 데이터를 학습하는 경우, 로우 데이터(Raw Data)는 임의의 사용자 디바이스가 센서로부터 획득한 최초의 이미지 데이터 일 수 있다. 다만, 로우 데이터(Raw Data)는 이에 한정되는 것은 아니고 아날로그 신호를 디지털 화 할 수 있는 다양한 종류의 데이터들을 포함할 수 있다. 또한, 일 실시예에 따른 버퍼는 복수의 경험 데 이터(Data_E)들을 저장할 수 있다. 예를 들면, 경험 데이터(Data_E)는 후술하는 파이프 라인에서 로우 데 이터(Raw Data)를 처리한 결과일 수 있다. 일 실시예에 따른 파이프 라인은 로우 데이터(Raw Data)들로부터 경험 데이터(Data_E)들을 추출하고, 추출 된 경험 데이터(Data_E)를 학습할 수 있다. 예를 들면, 파이프 라인은 복수의 로우 데이터(Raw Data)들을 1차 학습하고 복수의 경험 데이터(Data_E)들을 생성할 수 있다. 일 실시예에 따른 파이프 라인은 생성된 복수의 경험 데이터(Data_E)들을 다시 입력으로 하여 경험 데이터(Data_E)들에 대한 반복 학습을 수행할 수 있 다. 예를 들면, 파이프 라인은 경험 데이터(Data_E)들을 수신하고, 수신된 경험 데이터(Data_E)들을 병렬 적으로 처리할 수 있다. 복수의 경험 데이터(Data_E)들을 병렬적으로 처리할 수 있게 됨으로써, 일 실시예에 따 른 인공지능 엔진은 경험 데이터(Data_E)들을 빠르게 학습하고, 경험 데이터(Data_E)들에 적합한 AI 모델 (AI ML)을 생성할 수 있다. 예를 들면. 파이프 라인은 경험 데이터(Data_E)들을 병렬적으로 학습하고, 학 습 결과에 기초하여 정책(Policy)를 포함하는 AI 모델(AI ML)을 생성할 수 있다. 정책(Poilcy)가 생성되면, 일 실시예에 따른 파이프 라인은 생성된 정책(Policy)에 기초하여 제1 경험 데 이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습할 수 있다. 예를 들면, 파이프 라인은 복수의 경험 데 이터(Data_E)들의 학습 결과 생성된 AI 모델(AI ML)에 대한 피드백을 수행하고, 피드백 결과 생성된 정책 (Policy)를 업데이트 하여 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습할 수 있다. 파이프 라인에 대한 설명을 도 3에서 상세히 설명한다. 일 실시예에 따른 스토리지는 파이프 라인의 학습 결과 또는 학습 결과 처리된 데이터(Data_P)들을 저장하고, 처리된 데이터(Data_P)들을 파이프 라인으로 전달할 수 있다. 예를 들면, 스토리지는 경험 데이터(Data_E)들과 경험 데이터(Data_E)들의 학습 결과 또는 AI 모델(AI ML)들이 저장될 수 있으며, 학습 결과 처리된 데이터(Data_P)들은 다시 파이프 라인의 입력 데이터로 활용될 수 있다. 처리된 데이터(Data_P)들 이 다시 파이프 라인의 입력 데이터로 활용됨으로써, 일 실시예에 따른 인공지능 엔진은 AI 모델(AI ML)에 대한 피드백을 제공하고, 수신되는 데이터들에 적합한 AI 모델(AI ML)을 생성할 수 있다. 일 실시예에 따른 스토리지는 통상적인 저장매체를 포함할 수 있다. 예를 들어, 스토리지는 하드디스 크드라이브(Hard Disk Drive, HDD), ROM(Read Only Memory), RAM(Random Access Memory), 플래쉬메모리 (Flash Memory) 및 메모리카드(Memory Card)를 포함할 수 있으며, 외부에 마련된 클라우드 서버를 포함할 수 있다. 도 3은 개시된 실시예에 따른 인공지능 엔진의 파이프 라인을 설명하기 위한 블록도이다. 도 3을 참조하면, 일 실시예에 따른 파이프 라인은 트레이너, 제1 시뮬레이션 엔진 또는 제2 시 뮬레이션 엔진을 포함할 수 있다. 예를 들면, 파이프 라인의 제1 시뮬레이션 엔진은 제1 경험 데이터를 처리하고, 제2 시뮬레이션 엔진은 제1 시뮬레이션 엔진과 병렬적으로 작용하여 제2 경험 데 이터를 처리할 수 있다. 또한, 일 실시예에 따른 제1 경험 데이터 및 제2 경험 데이터는 버퍼에 저장될 수 있다. 여기서, 제1, 제2 의 용어는 복수의 경험 데이터(Data_E)를 구분하기 위한 용어에 불과하고, 제3 또는 제n 경험 데이터(Data_E)를 추가로 처리할 수 있음은 물론이다. 일 실시예에 따른 트레이너는 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진으로부터 각각 처리 된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 수신하고, 수신된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습할 수 있다. 또한, 일 실시예에 따른 트레이너는 제1 경험 데이터 (Data_P1) 및 제2 경험 데이터(Data_P2)를 학습 결과 정책(Policy)를 생성할 수 있다. 예를 들면, 트레이너 는 미리 정해진 코드에 기초하여 복수의 경험 데이터들(Data_E), 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)에 대한 학습을 수행하고, 학습 결과를 업데이트하여 정책(Policy)를 생성할 수 있다. 여기서 미리 정해진 코드는 사용자 설정에 의하여 정하여진 코드 또는 인공지능 엔진에 의한 학습 결과에 의하여 생성된 코드일 수 있다. 여기서, 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)는 각각 제1 시뮬레이션 엔진 또는 제2 시뮬레이션 엔진에 의하여 생성된 데이터일 수 있다. 일 실시예에 따른 정책(Policy)는 생성된 AI 모델(AI ML)에 활용되는 학습 방법을 의미할 수 있다. 또한, 일 실 시예에 따른 트레이너는 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습 결과 트레 이닝 데이터(Data_T)를 생성할 수 있다. 여기서 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P 2)는 복수의 처리된 경험 데이터들을 구분하기 위한 용어에 불과하며, 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)는 같거나 다를 수 있다. 일 실시예에 따른 트레이너는 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 강화 학습할 수 있다. 강화 학습은 경험 데이터(Data_E) 및 처리된 경험 데이터(Data_P1, Data_P2)들을 반복 학습 하는 것을 포 함할 수 있다. 예를 들면, 트레이너는 경험 데이터(Data_E) 및 처리된 경험 데이터(Data_P1, Data_P2)들을 입력으로 하고, 경험 데이터(Data_E) 및 처리된 경험 데이터(Data_P1, Data_P2)들에 기초하여 생성된 AI 모델 (AI)의 개선점을 판단하고 새로운 AI 모델을 생성할 수 있다. 일 실시예에 따른 트레이너는 시뮬레이션 엔진들(122, 123)으로부터 곧바로 경험 데이터들(Data_E) 또는 처리된 경험 데이터들(Data_P1, Data_P2)을 수신할 수 있다. 예를 들면, 버퍼에 경험 데이터들이 저장되어 있지 않은 경우, 파이프 라인은 제1 시뮬레이션 엔진에서 생성된 제1 경험 데이터(Data_P1) 및 제2 시뮬레이션 엔진에서 생성된 제2 경험 데이터(Date_P2)를 트레이너에 버퍼를 거치지 않고 전달 할 수 있다. 일 실시예에 따른 트레이너는 경험 데이터(Data_E) 및 처리된 경험 데이터(Data_P1, Data_P2)들에 대한 강 화 학습을 수행하기 위하여 회귀 분석을 활용한 최적화 동작을 수행할 수 있다. 예를 들면, 트레이너는 경 험 데이터(Data_E) 및 처리된 경험 데이터(Data_P1, Data_P2)들에 기초하여 생성된 AI 모델(AI ML)에서 오류를 찾아 제거하고 새로운 AI 모델을 생성할 수 있다. 일 실시예에 따른 새로운 AI 모델에는 트레이너가 생성 한 정책(Policy)가 적용될 수 있다. 제1 시뮬레이션 엔진는 제1 경험 데이터(Data_P1)을 상호 학습할 수 있다. 일 실시예에 따른 상호 학습은 상술한 강화 학습을 포함할 수 있다. 예를 들면, 제1 시뮬레이션 엔진은 트레이닝 데이터(Data_T)를 입력 으로 하고, 트레이닝 데이터(Data_T)를 처리한 결과 처리된 제1 경험 데이터(Data_P1)를 생성할 수 있다. 일 실 시예에 따른 트레이닝 데이터(Data_T)에는 복수의 경험 데이터(Data_E)들 및 처리된 제1 경험 데이터(Data_P1) 에 대한 상태 정보 및 보상 정보가 포함되어 있을 수 있다. 예를 들면, 트레이닝 데이터(Data_T)에는 복수의 경 험 데이터(Data_E)들 및 처리된 제1 경험 데이터(Data_P1)에서 오류가 제거된 보상 정보가 포함되어 있을 수 있 고, 복수의 경험 데이터(Data_E)들 및 처리된 제1 경험 데이터(Data_P1)에 대한 상태 정보가 포함되어 있을 수 있다. 제1 시뮬레이션 엔진이 상호 학습을 수행하는 과정은 도 4a에서 상세히 설명한다. 제2 시뮬레이션 엔진는 제1 시뮬레이션 엔진과 병렬적으로 작용하여 제2 경험 데이터(Data_P2)를 처 리할 수 있다. 예를 들면, 파이프 라인은 제1 파라미터에 의하여 제1 시뮬레이션 엔진을 구동하고, 제2 파라미터에 의하여 제2 시뮬레이션 엔진을 구동할 수 있다. 여기서 파라미터는 스토리지 주소, 버퍼 주소, 파이프 라인을 관리하기 위한 언어 또는 학습 반복 횟수 등이 포함될 수 있으나 이에 한 정되는 것은 아니고 인공지능 모델의 학습 과정에 영향을 미칠 수 있는 다양한 요소를 포함할 수 있다. 또 한, 제1 파라미터 및 제2 파라미터는 각각의 시뮬레이션 엔진에 적용되는 파라미터를 구분하기 위한 용어에 불 과하고, 제1 파라미터 및 제2 파라미터는 같거나 다를 수 있다. 또한, 제n 시뮬레이션 엔진을 포함하는 경우, n 개의 파라미터가 존재할 수 있다. 일 실시예에 따른 제2 시뮬레이션 엔진은 제2 경험 데이터(Data_P2)을 상호 학습할 수 있다. 예를 들면, 상호 학습은 상술한 강화 학습을 포함할 수 있다. 예를 들면, 제2 시뮬레이션 엔진은 트레이닝 데이터 (Data_T)를 입력으로 하고, 트레이닝 데이터(Data_T)를 처리한 결과 처리된 제2 경험 데이터(Data_P2)를 생성할 수 있다. 일 실시예에 따른 트레이닝 데이터(Data_T)에는 복수의 경험 데이터(Data_E)들 및 처리된 제2 경험 데 이터(Data_P2)에 대한 상태 정보 및 보상 정보가 포함되어 있을 수 있다. 예를 들면, 트레이닝 데이터(Data_T) 에는 복수의 경험 데이터(Data_E)들 및 처리된 제2 경험 데이터(Data_P2)에서 오류가 제거된 보상 정보가 포함 되어 있을 수 있고, 복수의 경험 데이터(Data_E)들 및 처리된 제2 경험 데이터(Data_P2)에 대한 상태 정보가 포 함되어 있을 수 있다. 제2 시뮬레이션 엔진이 상호 학습을 수행하는 과정은 도 4b에서 상세히 설명한다. 일 실시예에 따른 파이프 라인은 경험 데이터(Data_E)를 모사하는 시뮬레이터 이미지에 의하여 제1 시뮬레 이션 엔진 및 제2 시뮬레이션 엔진을 구동할 수 있다. 예를 들면, 실제 경험 데이터를 수신하는 대신에 파이프 라인은 생성된 AI 모델(AI ML)을 활용하여 경험 데이터를 모사하는 시뮬레이션 이미지를 생성하 고, 생성된 시뮬레이션 이미지를 가상의 경험 데이터로 활용하여 학습 동작을 수행할 수 있다. 일 실시예에 따른 파이프 라인은 정책(Policy)를 모사하는 최적화 이미지에 의하여 트레이너를 구동 할 수 있다. 예를 들면, 실제 정책(Policy)을 생성하는 대신에 파이프 라인은 생성된 AI 모델(AI ML)을 활 용하여 정책(Policy) 모사하는 최적화 이미지를 생성하고, 생성된 최적화 이미지를 가상의 경험 데이터로 활용 하여 학습 동작을 수행할 수 있다. 일 실시예에 따른 파이프 라인은 처리된 경험 데이터(Data_P1, Data_P2)를 모사하는 에이전트(Agent) 이미 지에 의하여 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진을 구동할 수 있다. 예를 들면, 파이프 라 인은 시뮬레이터 이미지 및 최적화 이미지에 의하여 생성된 정책(Policy)에서 경험 데이터만을 분리하고, 분리된 경험 데이터에 기초하여 에이전트(Agent) 이미지를 생성할 수 있다. 도 4a 및 도 4b는 개시된 실시예에 따른 인공지능 엔진의 시뮬레이션 엔진을 설명하기 위한 블록도이 다. 도 4a 및 도 4b를 참조하면, 각각의 시뮬레이션 엔진들(122, 123)은 에이전트(122a, 123a) 및 시뮬레이터(122b, 123b)를 포함할 수 있다. 일 실시예에 따른 시뮬레이션 엔진들(122, 123)은 경험 데이터(Data_E)에 대한 상호 학습을 수행함으로써 데이터에 대한 학습을 수행할 수 있다. 예를 들면, 시뮬레이션 엔진들(122, 123)은 이하 설명하는 액션(Action) 동작 및 보상 정보(Reward) 및 상태 정보(State)를 에이전트(Agent)로 전달하는 동작을 반복하면서 상호 학습을 수행할 수 있다. 일 실시예에 따른 에이전트(122a, 123a)는 생성된 정책(Policy)에 기초하여 액션(Action)을 수행한다. 일 실시 예에 따른 액션(Action)은 에이전트(122a, 123a)가 상태 정보(State) 및 경험 데이터들에 대한 보상 정보 (Reward)에 기초하여 학습에 활용되는 데이터를 시뮬레이터(122b, 123b)에 전달하는 동작을 의미할 수 있다. 일 실시예에 따른 시뮬레이터(122b, 123b)는 에이전트(122a, 123a)의 액션(Action) 동작에 의하여 경험 데이터 들(Data_E) 및 처리된 경험 데이터들(Data_P1, Data_P2)를 수신하고, 수신된 데이터들에 대한 피드백을 제공할 수 있다. 예를 들면, 시뮬레이터(122b, 123b)는 수신된 데이터들에 대한 피드백으로 보상 정보(Reward) 및 상태 정보(State)를 에이전트(Agent)로 전달할 수 있다. 일 실시예에 따른 보상 정보(Reward)는 미리 생성된 정책 (Policy)에 기초하여 처리된 경험 데이터(Data_P1, Data_P2)에서 오류(Error)를 제거한 데이터를 포함할 수 있 고, 상태 정보(State)는 미리 생성된 정책(Policy)에 기초하여 처리된 경험 데이터(Data_P1, Data_P2)를 포함할 수 있다. 일 실시예에 따른 시뮬레이션 엔진들(122, 123)은 상호 학습을 병렬적으로 수행하면서 데이터들의 종류에 관계 없이 빠른 학습을 도모할 수 있다. 도 5는 개시된 실시예에 따른 인공지능 엔진의 학습 방법의 흐름도이다. 도 5를 참조하면, 일 실시예에 따른 인공지능 엔진은 로우 데이터(Raw Data)들을 수신할 수 있다. 일 실시예에 따른 로우 데이터(Raw Data)는 임의의 사용자 디바이스가 센서로부터 획득한 최초의 데이터 일 수 있 다. 예를 들면, 로우 데이터(Raw Data)는 그 종류에 무관하게 인공지능 엔진이 학습할 수 있는 다양한 데 이터를 포함할 수 있다. 로우 데이터(Raw Data)들이 수신되면, 일 실시예에 따른 인공지능 엔진은 로우 데이터들로부터 경험 데이 터(Data_E)들을 추출하고 학습할 수 있다. 예를 들면, 인공지능 엔진은 복수의 로우 데이터(Raw Data)들을 1차 학습하고 복수의 경험 데이터(Data_E)들을 생성할 수 있다. 일 실시예에 따른 인공지능 엔진 은 생성된 복수의 경험 데이터(Data_E)들을 다시 입력으로 하여 경험 데이터(Data_E)들에 대한 반복 학습 을 수행할 수 있다. 예를 들면, 인공지능 엔진은 경험 데이터(Data_E)들을 수신하고, 수신된 경험 데이터 (Data_E)들을 병렬적으로 처리할 수 있다. 복수의 경험 데이터(Data_E)들을 병렬적으로 처리할 수 있게 됨으로 써, 일 실시예에 따른 인공지능 엔진은 경험 데이터(Data_E)들을 빠르게 학습하고, 경험 데이터(Data_E)들 에 적합한 AI 모델(AI ML)을 생성할 수 있다. 예를 들면. 인공지능 엔진은 경험 데이터(Data_E)들을 병렬 적으로 학습하고, 학습 결과에 기초하여 정책(Policy)를 포함하는 AI 모델(AI ML)을 생성할 수 있다. 정책 (Policy)가 생성되는 과정은 도 6에서 상세히 설명한다. 경험 데이터(Data_E)들에대한 학습이 이루어지면, 일 실시예예 따른 인공지능 엔진은 파이프 라인의 학습 결과를 저장할 수 있다. 일 실시예에 따른 인공지능 엔진은 경험 데이터(Data_E)들에 대한 학습결과 또는 학습 결과 처리된 데이터(Data_P)들을 저장하고, 처리된 데이터(Data_P)들을 다시 학습할 수 있다. 예를 들면, 인공지능 엔진은 경험 데이터(Data_E)들과 경험 데이터(Data_E)들의 학습 결과 또는 AI 모델 (AI ML)들을 저장할 수 있으며, 학습 결과 처리된 데이터(Data_P)들은 다시 입력 데이터로 활용될 수 있다. 처 리된 데이터(Data_P)들이 다시 인공지능 엔진의 입력 데이터로 활용됨으로써, 일 실시예에 따른 인공지능 엔진은 AI 모델(AI ML)에 대한 피드백을 제공하고, 수신되는 데이터들에 적합한 AI 모델(AI ML)을 생성할 수 있다. 일 실시예에 따른 인공지능 엔진은 내부에 존재하는 스토리지 또는 외부 클라우드 서버에 존재하는 스토리지에 경험 데이터(Data_E)들에 대한 학습 결과 또는 학습 결과 처리된 데이터(Data_P)들을 저장할 수 있다. 도 6은 개시된 실시예에 따른 인공지능 엔진이 정책을 생성하는 과정을 도시한 흐름도이다. 도 6을 참조하면, 일 실시예에 따른 인공지능 엔진은 제1 경험 데이터를 처리할 수 있다. 예를 들면, 인공지능 엔진은 제1 경험 데이터를 처리하고, 처리된 제1 경험 데이터(Data_P1)를 생성할 수 있다. 일 실시예에 따른 인공지능 엔진은 제2 경험데이터를 처리할 수 있다. 예를 들면, 인공지능 엔진 은 제2 경험 데이터를 제1 경험 데이터와 병렬적으로 처리하고, 처리된 제2 경험 데이터(Data_P2)를 생성 할 수 있다. 제1 경험 데이터 및 제2 경험 데이터가 처리되면, 일 실시예에 따른 인공지능 엔진은 제1 경험 데이터 및 제2 경험 데이터를 학습할 수 있다. 예를 들면, 인공지능 엔진은 각각 처리된 제1 경험 데이터 (Data_P1) 및 제2 경험 데이터(Data_P2)를 다시 입력으로 수신하고, 수신된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습할 수 있다. 일 실시예에 따른 인공지능 엔진은 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 강화 학습할 수 있다. 강화 학습은 경험 데이터(Data_E) 및 처리된 경험 데이터 (Data_P1, Data_P2)들을 반복 학습 하는 것을 포함할 수 있다. 제1 경험 데이터 및 제2 경험 데이터가 학습되면, 일 실시예에 따른 인공지능 엔진은 정책(Policy)를 생성 할 수 있다. 일 실시예에 따른 인공지능 엔진은 제1 경험 데이터(Data_P1) 및 제2 경험 데이터 (Data_P2)를 학습 결과 정책(Policy)를 생성할 수 있다. 예를 들면, 인공지능 엔진은 미리 정해진 코드에 기초하여 복수의 경험 데이터들(Data_E), 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)에 대한 학습 을 수행하고, 학습 결과를 업데이트하여 정책(Policy)를 생성할 수 있다. 여기서 미리 정해진 코드는 사용자 설 정에 의하여 정하여진 코드 또는 인공지능 엔진에 의한 학습 결과에 의하여 생성된 코드일 수 있다. 일 실시예에 따른 정책(Policy)는 생성된 AI 모델(AI ML)에 활용되는 학습 방법을 의미할 수 있다. 또한, 일 실 시예에 따른 인공지능 엔진은 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)를 학습 결과 트레이닝 데이터(Data_T)를 생성할 수 있다. 제1 여기서 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터 (Data_P2)는 복수의 처리된 경험 데이터들을 구분하기 위한 용어에 불과하며, 처리된 제1 경험 데이터(Data_P1) 및 제2 경험 데이터(Data_P2)는 같거나 다를 수 있다. 도 7은 개시된 실시예에 따른 인공지능 엔진을 구성할 수 있는 명령어들의 실시예이다. 일 실시예에 따른 인공지능 엔진의 파이프 라인은 파이프 라인 명령어들(120_1)로 구성될 수 있다. 예를 들면, 파이프 라인 명령어들(120_1)은 시뮬레이션 명령어(120_1a), 정책 최적화 명령어(120_1b) 또는 구성 명령어(120_1c)를 포함할 수 있다. 일 실시예에 따른 시뮬레이션 명령어(120_1a)는 시뮬레이터 이미지(Simulator image), 시뮬레이터 구성 (Simulator Configuration) 또는 에이전트 이미지(Agent image)를 포함할 수 있다. 일 실시예에 따른 시뮬레이 터 이미지(Simulator image)는 경험 데이터(Data_E)를 모사하도록 구성될 수 있다. 또한, 일 실시예에 따른 시 뮬레이터 구성(Simulator Configuration)은 정책(Policy) 정보를 포함하고, 시뮬레이션 엔진(122, 123)의 동작 을 제어하는 명령어를 포함할 수 있다. 또한, 일 실시예에 따른 에이전트 이미지(Agent image)는 처리된 경험 데이터(Data_P1, Data_P2)를 모사하도록 구성될 수 있다. 정책 최적화 명령어(120_1b)는 정책에 대한 최적화 이미지(Policy optimizer image)를 포함할 수 있다. 일 실시 예에 따른, 최적화 이미지(Policy optimizer image)는 정책(Policy)를 모사하도록 구성될 수 있다. 예를 들면, 실제 정책(Policy)을 생성하는 대신에 인공지능 엔진은 생성된 AI 모델(AI ML)을 활용하여 정책(Policy) 모사하는 최적화 이미지를 생성하고, 생성된 최적화 이미지(Policy optimizer image)를 가상의 경험 데이터로 활용하여 학습 동작을 수행할 수 있다. 일 실시예에 따른 구성 명령어(120_1c)는 학습 시간(number of epoch) 또는 학습 횟수(number of simulation) 정보를 포함할 수 있다. 예를 들면, 인공지능 엔진은 구성 명령어(120_1c)에서 설정된 학습 시간(number of epoch) 또는 학습 횟수(number of simulation)에 기초하여 시뮬레이션 엔진(122, 123)을 제어할 수 있다. 도 8은 개시된 실시예에 따른 인공지능 엔진의 학습 방법의 흐름도이다. 도 8을 참조하면, 배포 사용자가 인공지능 엔진의 학습 방법을 배포할 수 있다. 예를 들면, 인공지능 엔진 의 제조사는 배포 사용자일 수 있다. 인공지능 엔진의 제조사가 배포 사용자인 경우, 인공지능 엔진 의 학습 방법은 소프트웨어 형태로 사용자에 의해 설치될 수 있다. 인공지능 엔진의 학습 방법이 배포되면, 등록 사용자가 배포된 인공지능 엔진의 학습 방법에 워크 플로우(Work Flow)의 구성요소를 정의할 수 있다. 일 실시예에 따른 등록 사용자는 인공지능 엔진을 설치하고 사 용하는 주체로써, 사용자 디바이스의 사용자일 수 있으며, 워크 플로우(Work Flow)는 파이프 라인정보를 포함할 수 있다. 예를 들면, 등록 사용자는 인공지능 엔진에 사용되는 명령어들을 설정하고 인공지능 엔진 의 파이프 라인을 미리 설정할 수 있다. 파이프 라인이 사용자에 의해 설정되면, 일 실시예에 따른 인공지능 엔진의 학습 방법이 정의될 수 있다. 워크 플로우(Work Flow)의 구성 요소가 정의되면, 일 실시예에 따른 인공지능 엔진은 정의된 워크 플로우 (Work Flow)에 의하여 데이터를 학습할 수 있다. 예를 들면, 인공지능 엔진은 미리 정해진 시뮬레이 션 엔진(122, 123)의 구동 방법에 의하여 데이터들을 학습할 수 있다. 도 9는 개시된 실시예에 따른 인공지능 엔진의 데이터 흐름을 설명하기 위한 블록도이다. 도 9를 참조하면, 일 실시예에 따른 인공지능 엔진은 버퍼에 의하여 데이터들을 수신하고, 경험 데이 터를 추출할 수 있다. 추출된 경험 데이터들은 트레이너, 제1 시뮬레이션 엔진, 제2 시뮬레이션 엔진 , 제3 시뮬레이션 엔진 또는 제n 시뮬레이션 엔진(12n+1)에 의하여 학습될 수 있다. 예를 들면, 인공 지능 엔진은 n개의 시뮬레이션 엔진들을 포함할 수 있으며, n개의 시뮬레이션 엔진들은 각각 병렬로 구동 될 수 있다. 도 10은 개시된 실시예에 따른 인공지능 시스템의 블록도이다. 도 10을 참조하면, 일 실시예에 따른 인공지능 시스템은 오-픈랜(O-RAN) 방식의 통신 시스템에서 동작할 수 있다. 일 실시예에 따른 데이터는 사용자 장치(UE: User Equipment, 1010)에 의하여 획득될 수 있다. 또한, 일 실시예에 따른 데이터는 다양한 인터페이스들에 의하여 수신될 수 있다. 예를 들면, 데이터는 라디 오 유닛(O-RU, 1020), 분산 유닛(O-DU, 1030), 클라우드 유닛(O-CU, 1040)에 의하여 수신될 수 있다. 또한, 데 이터는 클라우드 네트워크(CN, 1050)에 의하여 통신될 수 있으며, 애플리캐이션 기능(AF, 1060)에 의해 송, 수신이 제어될 수 있다. 또한, 데이터는 반복 학습될 수 있는데, 동작 및 유지 유닛(OAM: Operation and Maintenance, 1070)에 의하여 데이터의 송, 수신이 제어될 수 있다. 또한, 데이터는 RIC 플랫폼들(1080, 1090)으로 배포될 수 있다. 수신된 최초의 데이터들은 로우 데이터(Raw Data)로 데이터 처리 준비 과정을 거칠 수 있다. 데이터 준비 과정은 데이터 수신/선택 과정 또는 데이터 병합/삭제 과정을 포함할 수 있다. 데이터 처리 준비과정이 종료되면, 데이터들은 학습 모델 정보(Model Training Info)에 기초하여 모델 학 습 호스트로 전달된다. 모델 학습 호스트는 학습 모델 선택 유닛, 특징 엔지니어링 유닛 , 학습/테스트 유닛, 모델 최적화 유닛 또는 모델 개선 유닛을 포함할 수 있다. 일 실 시예에 따른 모델 학습 호스트는 학습 모델을 인증하고 적용한다(Model Certification/Onboarding). 일 실시예에 따른 학습 모델의 인증 및 적용(Model Certification/Onboarding)은 모델 매니지먼트에 의 하여 구현될 수 있다. 모델 매니지먼트는 AI모델을 구동하는 인증 및 적용 유닛 및 AI 모델에 대한 피드백을 생성하는 개선/관리/종료 유닛을 포함할 수 있다. 일 실시예에 따른 데이터들은 분산 유닛(O- DU, 1030), 클라우드 유닛(O-CU, 1040), RIC 플랫폼(1080, 1090)에 의하여 제공될 수 있다. 모델 매니지먼트는 AI 모델에 대한 피드백을 수행하고, AI 모델을 개선 하여 모델 인터페이스 호스트 로 전달한다. 일 실시예에 따른 모델 인터페이스 호스트는 데이터 준비 과정에서 생성된 모 델 추론 정보(Model Inference Info)를 수신하고, 모델 추론 정보(Model Inference Info)와 생성된 AI 모델의정보를 비교할 수 있다. 일 실시예에 따른 모델 인터페이스 호스트는 AI 모델을 편집하는 특징 엔지니어 링 유닛 또는 모델 추론/학습 유닛을 포함할 수 있다. 일 실시예에 따른 모델 추론/학습 유닛 은 온라인 상에서 데이터의 학습 동작을 수행하게 할 수 있다. 모델 인터페이스 호스트는 액터으로 출력(Output)을 전달한다. 일 실시예에 따른 액터는 인 공지능 엔진의 에이전트(122a, 122b)일 수 있고, 출력(Output)은 데이터의 학습 결과 생성된 AI 모델을 포 함할 수 있다. 일 실시예에 따른 데이터들은 분산 유닛(O-DU, 1030), 클라우드 유닛(O-CU, 1040), RIC 플랫폼 (1080, 1090)에 의하여 제공될 수 있다. 일 실시예에 따른 액터은 배포된 AI 모델에 기초하여 학습 동작(Action)을 수행할 수 있다(1600, 1700). 일 실시예에 따른 시뮬레이터(122b, 123b)는 에이전트(122a, 123a)의 액션(Action) 동작에 의하여 경험 데이터 들(Data_E) 및 처리된 경험 데이터들(Data_P1, Data_P2)를 수신하고, 수신된 데이터들에 대한 피드백을 제공할 수 있다. 예를 들면, 시뮬레이터(122b, 123b)는 수신된 데이터들에 대한 피드백으로 보상 정보(Reward) 및 상태 정보(State)를 에이전트(Agent)로 전달할 수 있다. 일 실시예에 따른 보상 정보(Reward)는 미리 생성된 정책 (Policy)에 기초하여 처리된 경험 데이터(Data_P1, Data_P2)에서 오류(Error)를 제거한 데이터를 포함할 수 있 고, 상태 정보(State)는 미리 생성된 정책(Policy)에 기초하여 처리된 경험 데이터(Data_P1, Data_P2)를 포함할 수 있다. 한편, 인공지능 엔진은 지속 동작 제어 유닛에 의하여 학습 동작을 반복하여 수행할수 있다. 예를 들면, 지속 동작 제어 유닛은 인증부, 모니터링부, 분석부, 제안부 또는 지속 최적화부를 포함할 수 있다. 일 실시예에 따른 인증부은 생성된 AI 모델을 인증할 수 있다. 일 실시예에 따른 모니터링부은 AI 모델의 데이터 학습 활동을 모니터일 하고, 분석부 데이터의 학습 결과 및 생성된 AI 모델을 분석할 수 있다. 일 실시예에 따른 제안부는 분석결과 피드백이 포함된 AI 모델을 제한할 수 있다. 지속 최적화부 은 피드백 동작이 수행되어 업데이트된 AI 모델에 의하여 데이터의 학습을 계속하도록 인공지능 엔진 을 제어할 수 있다. 개시된 실시예에 따른 복수의 데이터들을 학습하는 O-Ran(Open-Radio-network) 기반 인공지능(AI) 엔진은 로우 데이터들을 수신하는 버퍼, 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 파이프라인 및 파이프 라인의 학습 결과를 저장하는 스토리지를 포함할 수 있다. 일 실시예에 따른 파이프 라인은, 제1 경험 데이터를 처리하는 제1 시뮬레이션 엔진, 제1 시뮬레이션 엔진과 병렬적으로 작용하여 제2 경험 데이터를 처리하는 제2 시뮬레이션 엔진 및 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진으로부터 처리된 제1 경험 데이터 및 제2 경험 데이터를 수신하고, 수신된 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과에 기초하여 정책(Policy)을 생성하는 트레이너를 포함 할 수 있다. 일 실시예에 따른 제1 시뮬레이션 엔진은, 제1 경험 데이터를 상호 학습하는 제1 에이전트 및 제1 시뮬레 이터를 포함할 수 있다. 일 실시예에 따른 제2 시뮬레이션 엔진은, 제2 경험 데이터를 상호 학습하는 제2 에이전트 및 제2 시뮬레 이터를 포함하는 제2 시뮬레이션 엔진을 포함할 수 있다. 일 실시예에 따른 트레이너는, 미리 정해진 코드에 기초하여 학습 결과를 업데이트하여 정책을 생성할 수 있다. 일 실시예에 따른 파이프 라인은, 정책에 기초하여 제1 경험 데이터 및 제2 경험 데이터를 학습할 수 있다. 일 실시예에 따른 파이프 라인은, 제1 파라미터에 의하여 제1 시뮬레이션 엔진을 구동하고, 제1 파라미터 와는 상이한 제2 파라미터에 의하여 제2 시뮬레이션 엔진을 구동할 수 있다. 일 실시예에 따른 파이프 라인은, 경험 데이터를 모사하는 시뮬레이터 이미지에 의하여 제1 시뮬레이션 엔 진 및 제2 시뮬레이션 엔진을 구동할 수 있다. 일 실시예에 따른 파이프 라인은, 정책을 모사하는 정책 최적화 이미지에 의하여 트레이너를 구동할 수 있다. 일 실시예에 따른 파이프 라인은, 처리된 경험 데이터를 모사하는 에이전트(Agent) 이미지에 의하여 제1 시뮬레이션 엔진 및 제2 시뮬레이션 엔진을 구동할 수 있다. 일 실시예에 따른 파이프 라인은, 정책에서 경험 데이터를 분리하고, 분리된 경험 데이터에 기초하여 에이 전트(Agent) 이미지를 생성할 수 있다. 일 실시예에 따른 스토리지는, 외부의 서버를 포함할 수 있다. 개시된 실시예에 따른 O-Ran(Open-Radio-network) 기반 인공지능(AI) 엔진이 복수의 데이터들을 학습하는 방법 은 로우 데이터(Raw Data)들을 수신하는 단계, 로우 데이터들로부터 경험 데이터들을 추출하고 학습하는 단계 및 학습 결과를 저장하는 단계를 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 제1 경험 데이터를 처리하는 제1 단계 , 제1 경험 데이터와 병렬적으로 제2 경험 데이터를 처리하는 제2 단계 및 제1 단계 및 제2 단계에서 처리된 제1 경험 데이터 및 제2 경험 데이터를 수신하고, 수신된 처리된 제1 경험 데이터 및 제2 경험 데이터를 학습하고, 학습 결과에 기초하여 정책(Policy)을 생성하는 단계를 포함할 수 있다. 일 실시예에 따른 제1 단계는, 제1 경험 데이터를 상호 학습하는 것을 포함할 수 있다. 일 실시예에 따른 제2 단계는, 제2 경험 데이터를 상호 학습하는 것을 포함할 수 있다. 일 실시예에 따른 정책(Policy)을 생성하는 단계는, 미리 정해진 코드에 기초하여 학습 결과를 업데이트하 여 정책을 생성하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 정책에 기초하여 제1 경험 데이터 및 제2 경험 데이터를 학습하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 제1 파라미터에 의하여 제1 단계를 수행하 고, 제1 파라미터와는 상이한 제2 파라미터에 의하여 제2 단계를 수행하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 경험 데이터를 모사하는 시뮬레이터 이미지 에 의하여 제1 단계 및 제2 단계를 수행하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 정책을 모사하는 정책 최적화 이미지에 의 하여 정책(Policy)을 생성하는 단계를 수행하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 처리된 경험 데이터를 모사하는 에이전트 (Agent) 이미지에 의하여 제1 단계 및 제2 단계를 수행하는 것을 포함할 수 있다. 일 실시예에 따른 경험 데이터들을 추출하고 학습하는 단계는, 정책에서 경험 데이터 만을 분리하고, 분리 된 경험 데이터에 기초하여 에이전트(Agent) 이미지를 생성하는 것을 포함할 수 있다. 개시된 실시예에 따른 인공지능 엔진의 학습 방법은 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽 을 수 있는 기록매체로 구현될 수 있다. 기기로 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, ‘비 일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다는 것을 의미 할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경우를 구분하 지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2022-0143025", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 개시된 실시예에 따른 인공지능 시스템의 블록도이다. 도 2는 개시된 실시예에 따른 인공지능 엔진의 블록도이다. 도 3은 개시된 실시예에 따른 인공지능 엔진의 파이프 라인을 설명하기 위한 블록도이다. 도 4a 및 도 4b는 개시된 실시예에 따른 인공지능 엔진의 시뮬레이션 엔진을 설명하기 위한 블록도이다. 도 5는 개시된 실시예에 따른 인공지능 엔진의 학습 방법의 흐름도이다. 도 6은 개시된 실시예에 따른 인공지능 엔진이 정책을 생성하는 과정을 도시한 흐름도이다. 도 7은 개시된 실시예에 따른 인공지능 엔진을 구성할 수 있는 명령어들의 실시예이다. 도 8은 개시된 실시예에 따른 인공지능 엔진의 학습 방법의 흐름도이다. 도 9는 개시된 실시예에 따른 인공지능 엔진의 데이터 흐름을 설명하기 위한 블록도이다. 도 10은 개시된 실시예에 따른 인공지능 시스템의 블록도이다."}
