{"patent_id": "10-2023-0027163", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0133348", "출원번호": "10-2023-0027163", "발명의 명칭": "이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기", "출원인": "한국과학기술원", "발명자": "유회준"}}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "스파이킹 신경망 처리 모듈과 합성곱 신경망 처리 모듈이 결합된, 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기에 있어서,스파이크 발생 시에 시냅스의 가중치를 누적하여 뉴런의 전압을 생성하는 누적기 배열의 스파이킹 신경망 처리모듈;신경망의 입력과 가중치의 곱을 누적하여 뉴런의 출력 값을 생성하는 곱셈 및 누적기 배열 합성곱 신경망 처리모듈;상기 스파이킹 신경망 처리 모듈과 합성곱 신경망 처리 모듈에 대한 컨트롤과 활성화함수 및 배치 정규화 처리를 담당하는 최상위 RISC 컨트롤러; 상기 입력에 대해 채널별 풀링을 수행한 후 사전 훈련된 가중치로 컨볼루션을 수행하여 어텐션 맵을 생성하는어텐션 모듈; 및상기 입력을 여러 타일로 나누고, 각 타일별로 발생하는 스파이크 빈도를 계산하여 에너지 소모가 적은 신경망처리 모듈을 예측하며, 해당 신경망 처리 모듈로 타일을 전달하여 연산이 수행될 수 있게 하는 신경망 연산 할당기;를 포함하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서, 신경망 연산에 필요한 가중치를 저장하며, 상기 스파이킹 신경망 처리 모듈의 합성곱 신경망 코어 또는 상기 합성곱 신경망 처리 모듈의 스파이킹 신경망 코어에 필요한 가중치를 전달하는 전역 L2 캐시; 및 상기 뉴런에 연결된 시냅스의 전방향 그래디언트 평균값을 구하고, 평균값이 임계값보다 작으면 상기 합성곱 신경망 처리 모듈의 합성곱 신경망 PE에 뉴런에 대한 오차 역전파를 생략하도록 하는 희소성 생성기;를 더 포함하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 스파이킹 신경망 처리 모듈은각각이 복수의 스파이킹 신경망 코어로 구성되는 복수의 스파이킹 신경망 클러스터로 구성되어 스파이킹 연산을할당받아 처리하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3항에 있어서, 상기 스파이킹 신경망 코어는 멀티플럭서와 카운터로 구성되어 있으며, 입력 메모리로부터 데이터를 전달받아 입력 데이터를 스파이크 패턴으공개특허 10-2024-0133348-3-로 변환하는 스파이크 인코더;레지스터와 XOR 로직들로 구성되어 있으며, 상기 스파이크 인코더 동작시에 스파이크 패턴의 시작점을 결정짓기위한 랜덤 값을 생성하는 LFSR;뺄셈기와 룩업테이블로 구성되어 있으며, 출력 스파이크와 입력 스파이크의 시간 차를 구하고, 시간 차를 그래디언트로 변환하는 로컬 그래디언트 유닛;상기 스파이크 인코더로부터 스파이크 입력시에 가중치를 누적하여 뉴런 전위를 계산하는 추론 로직과 상기 로컬 상기 그래디언트 유닛으로부터 그래디언트를 전달받아 누적하는 그래디언트 누적 로직으로 구성된 스파이킹신경망 PE;상기 스파이킹 신경망 PE들의 연산 결과를 세로 방향으로 누적하여 뉴런 전압을 생성하고 임계값을 초과하면 출력 스파이크를 발생시키는 가산기 트리&발화 로직; 및입력 스파이크와 출력 스파이크의 시간 차를 동시에 구하기 위해 사용되는 전역 카운터;를 포함하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 4항에 있어서,상기 스파이킹 신경망 PE은L1 캐시가 집적되어, 시냅스 전 뉴런에 대한 가중치를 상기 전역 L2 캐시에서 읽기 동작 전력 소모가 작은 L1캐시로 가져와 1개의 타임 스텝 후에, 상기 전역 L2 캐시에 대한 접근 없이, 상기 L1 캐시에 저장된 가중치를동일한 시냅스 전 뉴런들의 연산에 재사용하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 1항에 있어서,상기 합성곱 신경망 처리 모듈은각각이 복수의 합성곱 신경망 코어로 구성되는 복수의 합성곱 신경망 클러스터로 구성되어 합성곱 연산을 할당받아 처리하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 6항에 있어서,상기 합성곱 신경망 코어는곱셈 및 누적기와 희소성 처리기로 구성되어 있으며, 추론시에 상보적 심층 신경망에서 요구되는 합성곱 연산을수행하고, 학습시에 불필요한 가중치에 대한 역전파를 생략하고 학습이 필요한 가중치에 대해서만 그래디언트를계산하는 합성곱 신경망 PE;합성곱 신경망 연산시에 사용되는 입력데이터를 저장하는 입력 메모리;상기 합성곱 신경망 PE에서 사이클마다 요구하는 입력 데이터를 로드해 주는 입력 로더;상기 합성곱 신경망 연산시에 사용되는 가중치 데이터를 저장하는 가중치 메모리;상기 합성곱 신경망 PE에서 매 사이클마다 요구하는 가중치 데이터를 로드해주는 가중치 로더;전달받은 가중치와 입력의 곱을 구하고 이전에 연산된 결과와 누적함으로써 합성곱 연산을 수행하는 곱셈 및 누공개특허 10-2024-0133348-4-적기; 및학습 시에는 불필요한 가중치에 대한 역전파를 생략하고 학습이 필요한 가중치에 대해서만 그래디언트를 계산할수 있도록 상기 입력 로더와 상기 가중치 로더를 컨트롤하는 연산 생략 컨트롤러;를 포함하는 것을 특징으로 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1항에 있어서,상기 어텐션 모듈은 입력에 대해 각 픽셀 방향으로 존재하는 복수의 입력 채널 데이터 중에서 가장 큰 값을 찾아내고, 해당 값을 남기고 다른 값들을 입력 채널에서 제거함으로써 입력 채널의 크기를 줄이는 최대 풀링 유닛;입력에 대해 각 픽셀 방향으로 존재하는 복수의 입력 채널 데이터의 평균 값을 찾아내고, 해당 평균 값을 남기고 다른 값들을 입력 채널에서 제거함으로써 입력 채널의 크기를 줄이는 평균 풀링 유닛;곱셈기와 누적기를 이용해 가중치와 입력의 곱셈을 수행해 이전 결과 값과 누적함으로써 합성곱 연산을 수행하는 곱셈&누적기; 및가중치와 입력을 전달받아 곱셈을 수행하고, 결과 값을 누적기로 전달하는 곱셈기;를 포함하는 것을 특징으로하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기."}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기는 스파이크 발생 시에 시냅스의 가중치를 누적하여 뉴런의 전압을 생성하는 누적기 배열의 스파이킹 신경망 처리 모듈; 신경망의 입력과 가중치의 곱을 누적하여 뉴런의 출력 값을 생성하는 곱셈 및 누적기 배열 합성곱 신경망 (뒷면에 계속)"}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기에 관한 것으로써, 더욱 상세하게는 합성곱 신경망(Convolutional-Neural-Network)만을 처리하던 가속기 대신 합성곱 신 경망과 스파이킹 신경망(Spiking-Neural-Network)을 혼합하여 활용해 심층 신경망을 처리하는 가속기를 설계함 으로써 기존의 연산에 활용되던 곱셈기 및 누적기를 저전력 누적기만으로 대체하여 심층 신경망 추론과 학습 과 정에서의 에너지(Energy) 효율을 향상시킬 수 있는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기에 관한 것이다. 또한, 본 발명은 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기에 관한 것으로써, 더욱 상세하게는 신경망 계층 간 또는 신경망 계층 내의 스파이크(Spike) 빈도에 따라 사용하는 신경망의 종류를 다르게 사용하여 추론 시에 최적의 에너지 효율을 달성하며, 스파이킹 신경망을 통해 저전력으 로 학습되어야 할 가중치를 예측하고 합성곱 신경망을 통해 학습이 필요한 가중치에 대해서만 고정확 학습을 진 행함으로써 학습 시에 저전력과 높은 정확도를 달성하고, 새로운 스파이킹 신경망 알고리즘(Algorithm) 및 아키 텍처(Architecture)를 통해 스파이킹 신경망을 통해 학습이 필요한 가중치 예측에 사용되는 전력을 낮춰 높은 에너지 효율을 달성할 수 있는 하는 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심 층 신경망 가속기에 관한 것이다."}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 합성곱 신경망에서 스파이킹 신경망으로의 변환 알고리즘을(CNN-to-SNN) 이용하여 스파이킹 신경망이 합성 곱 신경망과 같은 정확도를 얻을 수 있게 되었다. 또한, 스파이킹 신경망은 합성곱 신경망의 프레임(Frame) 구동 연산과 달리 스파이크 기반 이벤트(Event) 구동 연산을 통해 높은 희소성을 가지기 때문에 초저전력 인공지능 어플리케이션(Application)의 유망한 선택지가 되 고 있다. 하지만, 스파이킹 신경망의 연산 횟수는 각 계층에 따라 달라지는 스파이크 희소성에 따라 변동하게 되며, 이에 따라 합성곱 연산과 다르게 계층별로 연산기의 에너지 소모량이 크게 차이가 나게 되고, 계층별로 효율적인 신 경망의 종류가 달라진다. 또한, 스파이킹 신경망은 생물학적 뉴런의 STDP 학습 알고리즘과 유사하게 시냅스(Synapse) 전 스파이크와 시냅 스 후 스파이크 사이의 시간 차이로 계산되는 전방향 그래디언트(Forward-Gradient)를 생성하여 저전력 학습을 달성할 수 있다. 그러나, 해당 학습 방식은 합성곱 신경망에 비해 정확도가 낮고, 반대로 합성곱 신경망은 역전파(Back- Propagation) 학습을 통해 높은 정확도를 얻을 수 있지만 반복적인 역전파와 그래디언트 생성(Gradient Generation)으로 인해 많은 계산이 필요하며, 이이 따라 저전력 학습이 어렵다는 문제점이 있다. 또한, 아래 선행기술문헌[1, 2]은 오직 한 종류의 신경망에 대해서만 효율적인 가속을 수행할 수 있는 구조로 되어 있으며, 이에 따라 다른 신경망을 처리하는 과정에서 높은 에너지 효율 달성이 매우 어려운 문제점이 있다. 또한, 제안된 동종의 아키텍처를 사용해 두 종류의 신경망을 모두 처리하는 선행기술문헌[3]은 두 신경망의 연 산 방식과 메모리(Memory) 접근 패턴(Pattern)이 다르므로 동종 코어(Core)에서는 높은 에너지 효율 달성이 매 우 어렵다는 문제점이 있다. 더불어, 해당 선행기술문헌[3]은 더 높은 에너지 효율을 달성하기 위해 두 종류의 신경망을 함께 활용하지 못하 며, 심층 신경망 학습도 불가능하다는 문제점이 있다. 따라서, 심층 신경망의 추론과 학습을 에너지 효율적으로 처리하기 위해 상보적 심층 신경망 처리가 가능한 이 기종 가속기가 필요한 실정이다. 선행기술문헌 비특허문헌 (비특허문헌 0001) [1] K. Hirose et al., \"Hiddenite: 4K-PE Hidden Network Inference 4D-Tensor Engine Exploiting On-Chip Model Construction Achieving 34.8-to-16.0TOPS/W for CIFAR-100 and ImageNet,\" 2022 IEEE International Solid- State Circuits Conference (ISSCC), 2022, pp. 1-3 (비특허문헌 0002) [2] Chen, Gregory K., et al. \"A 4096-neuron 1M-synapse 3.8-pJ/SOP spiking neural network with on-chip STDP learning and sparse weights in 10-nm FinFET CMOS.\" IEEE Journal of Solid- State Circuits 54.4 : 992-1002. (비특허문헌 0003) [3] L. Deng et al., \"Tianjic: A Unified and Scalable Chip Bridging Spike-Based and Continuous Neural Computation,\" in IEEE Journal of Solid-State Circuits, vol. 55, no. 8, pp. 2228- 2246, Aug. 2020"}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점을 해결하기 위해 본 발명은 합성곱 신경망과 스파이킹 신경망이 결합한 상보적 심층 신경망을 통 해 높은 정확도로 에너지 효율적인 추론과 훈련을 수행할 수 있는 이기종 합성곱 신경망 및 스파이킹 신경망 코 어 아키텍처를 갖춘 상보적 심층 신경망 가속기를 제공하는데 목적이 있다. 또한, 상술한 문제점을 해결하기 위해 본 발명은 두 신경망의 혼합 방식을 정하고 이를 최적화하는 방식을 제안 하여 상보적 심층 신경망 처리에 필요한 전력을 감소시킬 뿐만 아니라 각 신경망을 단일적으로 처리하는 과정에 서도 에너지 소모를 줄이기 위한 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기를 제공하는데 목적이 있다."}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기는 스파이크 발생 시에 시냅스의 가중치를 누적하여 뉴런의 전압을 생성하는 누적기 배열의 스파이킹 신경망 처리 모듈; 신경망의 입력과 가중치의 곱을 누적하여 뉴런의 출력 값을 생성하는 곱셈 및 누적기 배열 합성곱 신경망 처리 모듈; 상기 스파이킹 신경망 처리 모듈과 합성곱 신경망 처리 모듈에 대한 컨트롤과 활성화함수 및 배치 정규화 처리를 담당하는 최상위 RISC 컨트롤러; 상기 입력에 대해 채널별 풀링을 수행한 후 사전 훈련된 가중치로 컨볼루션을 수행하여 어텐션 맵을 생성하는 어텐션 모듈; 및 상기 입력을 여러 타일로 나누고, 각 타일별로 발생하는 스파이크 빈도를 계산하여 에너지 소모가 적은 신경망 처리 모듈을 예측 하며, 해당 신경망 처리 모듈로 타일을 전달하여 연산이 수행될 수 있게 하는 신경망 연산 할당기;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기는 신경망 연산에 필요한 가중치를 저장하며, 상기 스파이킹 신경망 처리 모듈 의 합성곱 신경망 코어 또는 상기 합성곱 신경망 처리 모듈의 스파이킹 신경망 코어에 필요한 가중치를 전달하 는 전역 L2 캐시; 및 상기 뉴런에 연결된 시냅스의 전방향 그래디언트 평균값을 구하고, 평균값이 임계값보다 작으면 상기 합성곱 신경망 처리 모듈의 합성곱 신경망 PE에 뉴런에 대한 오차 역전파를 생략하도록 하는 희소 성 생성기;를 더 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 스파이킹 신경망 처리 모듈은 각각이 복수의 스파이킹 신경망 코어로 구성되 는 복수의 스파이킹 신경망 클러스터로 구성되어 스파이킹 연산을 할당받아 처리하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 스파이킹 신경망 코어는 멀티플럭서와 카운터로 구성되어 있으며, 입력 메모 리로부터 데이터를 전달받아 입력 데이터를 스파이크 패턴으로 변환하는 스파이크 인코더; 레지스터와 XOR 로직 들로 구성되어 있으며, 상기 스파이크 인코더 동작시에 스파이크 패턴의 시작점을 결정짓기 위한 랜덤 값을 생 성하는 LFSR; 뺄셈기와 룩업테이블로 구성되어 있으며, 출력 스파이크와 입력 스파이크의 시간 차를 구하고, 시 간 차를 그래디언트로 변환하는 로컬 그래디언트 유닛; 상기 스파이크 인코더로부터 스파이크 입력시에 가중치 를 누적하여 뉴런 전위를 계산하는 추론 로직과 상기 로컬 상기 그래디언트 유닛으로부터 그래디언트를 전달받 아 누적하는 그래디언트 누적 로직으로 구성된 스파이킹 신경망 PE; 상기 스파이킹 신경망 PE들의 연산 결과를 세로 방향으로 누적하여 뉴런 전압을 생성하고 임계값을 초과하면 출력 스파이크를 발생시키는 가산기 트리&발 화 로직; 및 입력 스파이크와 출력 스파이크의 시간 차를 동시에 구하기 위해 사용되는 전역 카운터;를 포함하 는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 스파이킹 신경망 PE은 L1 캐시가 집적되어, 시냅스 전 뉴런에 대한 가중치를 상기 전역 L2 캐시에서 읽기 동작 전력 소모가 작은 L1 캐시로 가져와 1개의 타임 스텝 후에, 상기 전역 L2 캐 시에 대한 접근 없이, 상기 L1 캐시에 저장된 가중치를 동일한 시냅스 전 뉴런들의 연산에 재사용하는 것을 특 징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 합성곱 신경망 처리 모듈은 각각이 복수의 합성곱 신경망 코어로 구성되는 복수의 합성곱 신경망 클러스터로 구성되어 합성곱 연산을 할당받아 처리하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 합성곱 신경망 코어는 곱셈 및 누적기와 희소성 처리기로 구성되어 있으며, 추론시에 상보적 심층 신경망에서 요구되는 합성곱 연산을 수행하고, 학습시에 불필요한 가중치에 대한 역전파 를 생략하고 학습이 필요한 가중치에 대해서만 그래디언트를 계산하는 합성곱 신경망 PE; 합성곱 신경망 연산시 에 사용되는 입력데이터를 저장하는 입력 메모리; 상기 합성곱 신경망 PE에서 사이클마다 요구하는 입력 데이터 를 로드해 주는 입력 로더; 상기 합성곱 신경망 연산시에 사용되는 가중치 데이터를 저장하는 가중치 메모리; 상기 합성곱 신경망 PE에서 매 사이클마다 요구하는 가중치 데이터를 로드해주는 가중치 로더; 전달받은 가중치 와 입력의 곱을 구하고 이전에 연산된 결과와 누적함으로써 합성곱 연산을 수행하는 곱셈 및 누적기; 및 학습 시에는 불필요한 가중치에 대한 역전파를 생략하고 학습이 필요한 가중치에 대해서만 그래디언트를 계산할 수있도록 상기 입력 로더와 상기 가중치 로더를 컨트롤하는 연산 생략 컨트롤러;를 포함하는 것을 특징으로 한다. 또한, 상술한 목적을 달성하기 위한 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 어텐션 모듈은 입력에 대해 각 픽셀 방향으로 존재하는 복수의 입력 채널 데 이터 중에서 가장 큰 값을 찾아내고, 해당 값을 남기고 다른 값들을 입력 채널에서 제거함으로써 입력 채널의 크기를 줄이는 최대 풀링 유닛; 입력에 대해 각 픽셀 방향으로 존재하는 복수의 입력 채널 데이터의 평균 값을 찾아내고, 해당 평균 값을 남기고 다른 값들을 입력 채널에서 제거함으로써 입력 채널의 크기를 줄인이는 평균 풀링 유닛; 곱셈기와 누적기를 이용해 가중치와 입력의 곱셈을 수행해 이전 결과 값과 누적함으로써 합성곱 연 산을 수행하는 곱셈&누적기; 및 가중치와 입력을 전달받아 곱셈을 수행하고, 결과 값을 누적기로 전달하는 곱셈 기;를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기은 스파이킹 신경망과 합성곱 신경망의 상호적 보완을 통해 심층 신경망 가속기의 추론 및 학습의 정확도를 유지하 며 에너지 효율을 증가시킬 수 있는 효과가 있다. 또한, 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기은 이미지넷(ImageNet) 분류의 경우, 심층 신경망의 추론을 수행할 때 신경망 연산 할당기로 인해 합성곱 신경망 또는 스파이킹 신경망 하나만을 사용할 때 비해 에너지 효율이 각각 16.7%와 43.3%만큼 증가하였으며, 집적된 어텐션 유닛을 함께 사용하여 신경망 연산 할당 과정을 최적화했을 때 에너지 효율 증가량을 각각 85.8% 와 51.4%까지 상승시킬 수 있는 효과가 있다. 또한, 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기은 분산된 L1 캐시들이 집적된 스파이킹 신경망 코어가 반복되는 메모리 접근을 제거하여 가중치 재사용률 을 네트워크 종류에 따라 3.3~5.5배 증가시킬 수 있고, 스파이킹 신경망 처리에 소모되는 전력이 42.2~49.1%만 큼 감소시킬 수 있는 효과가 있다. 또한, 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기는 전방향 그래디언트 기반 희소성 생성기와 희소성 처리 합성곱 연산기가 사이파-10(CIFAR-10) 데이터 세 트(Dataset)에 대한 심층 신경망 학습 과정에서 역전파와 그래디언트 생성에 필요한 연산량을 각각 58%와 79%만 큼 감소시킬 수 있고, 이미지넷 데이터 세트에 대한 학습 과정에서 역전파와 그래디언트 생성에 필요한 연산량 을 각각 31%와 43%만큼 감소시킬 수 있는 효과가 있다. 또한, 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기는 전역 카운터 및 로컬 그래디언트 유닛이 스파이킹 신경망의 추론과 전방향 그래디언트 생성이 동시에 수 행될 때 소모되는 전력을 61%가량 감소시킬 수 있고, 전방향 그래디언트 기반 희소성 생성기 및 희소성 처리 합 성곱 코어와 함께 동작함으로써 사이파-10과 이미지넷 데이터 세트 대해 학습 에너지 효율을 각각 61.6%와 28.7%만큼 증가시킬 수 있는 효과가 있다. 또한, 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기는 상보적 심층 신경망 가속기가 사이파-10의 추론에 대해 94.1%의 정확도를 얻을 수 있으며, 이미지넷의 추론에 대해 77.1%의 정확도를 달성할 수 있는 효과가 있다."}
{"patent_id": "10-2023-0027163", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 및 청구범위에 사용된 용어나 단어는 통상적이거나 사전적인 의미로 한정하여 해석되어서는 아니 되 며, 발명자는 그 자신의 발명을 가장 최선의 방법으로 설명하기 위해 용어의 개념을 적절하게 정의할 수 있다는 원칙에 입각하여, 본 발명의 기술적 사상에 부합하는 의미와 개념으로 해석되어야만 한다. 따라서, 본 명세서에 기재된 실시예와 도면에 도시된 구성은 본 발명의 가 장 바람직한 일 실시예에 불과할 뿐 이고 본 발명의 기술적 사상을 모두 대변하는 것은 아니므로, 본 출원시점에 있어서 이들을 대체할 수 있는 다 양한 균등물과 변형예들이 있을 수 있음을 이해하여야 한다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기에 대해 상세히 설명한다. 도 1은 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기의 구조도 이다. 도 1에 도시된 바와 같이 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보 적 심층 신경망 가속기는 스파이킹 신경망 처리 모듈, 합성곱 신경망 처리 모듈, 최상위 RISC 컨트롤러, 어텐션 모듈, 신경망 연산 할당기, 전역 L2 캐시 및 전방향 그래디언트 기 반 희소성 생성기를 포함한다. 상기 스파이킹 신경망 처리 모듈은 복수의 스파이킹 신경망 클러스터들로 구성되고, 각각의 상기 스파이킹 신경망 클러스터는 복수의 스파이킹 신경망 코어로 구성된다. 보다 구체적으로, 상기 스파이킹 신경망 처리 모듈은 4개의 스파이킹 신경망 클러스터로 구성되어 있고, 각 스파이킹 신경망 클러스터는 8개의 스파이킹 신경망 코어로 구성되어 상보적 심층 신경망 에 필요한 스파이킹 연산을 할당받아 처리한다. 상기 스파이킹 신경망 코어는 상보적 심층 신경망에서 요구되는 스파이킹 신경망 연산을 수행한다. 또한, 상기 스파이킹 신경망 코어는 64×64의 스파이킹 신경망 PE들로 구성되어 있으며, 각 연산기 는 누적기로 구성되어 있고, 스파이크를 입력받게 되면 가중치를 누적하여 뉴런 연산을 수행한다. 세부적으로 상기 스파이킹 신경망 코어는 스파이크 인코더(Encoder:1111), LFSR(Linear-Feedback Shift Register:1112), 로컬 그래디언트 유닛, 스파이킹 신경망 PE, 가산기 트리(Tree)&발화 로직 , 전역 카운터를 포함하고, 상기 스파이킹 신경망 PE가 누산기 기반 추론부(1114a)와 로컬 그래디언트 누적기(1114b)를 포함하여 복수의 상기 스파이킹 신경망 PE를 그룹화하여 높은 복잡도의 신경 망이 필요한 작업을 위해 높은 비트 정밀도를 지원할 수 있다. 상기 스파이크 인코더(Encoder:1111)는 멀티플렉서와 카운터로 구성되어 있으며, 입력 메모리로부터 데이터를 전달받아 입력 데이터를 스파이크 패턴으로 변환하고, 변환된 스파이크는 순서대로 스파이킹 신경망 PE로 전달된다. 상기 LFSR(Linear-Feedback Shift Register:1112)는 레지스터와 XOR 로직들로 구성되어 있으며, 상기 스파이크 인코더(Encoder:1111) 동작시에 스파이크 패턴의 시작점을 결정짓기 위한 랜덤 값을 생성한다. 상기 로컬 그래디언트 유닛은 뺄셈기와 룩업테이블로 구성되어 있으며, 출력 스파이크와 입력 스파이크의 시간 차를 구하고, 시간 차를 그래디언트로 변환하여 각 스파이킹 신경망 PE로 전달한다. 상기 스파이킹 신경망 PE는 추론 로직과 그래디언트 누적 로직으로 구성되어 있고, 각 로직은 레지스터 파일, 멀티플럭서, 누적기로 구성되어 있으며, 추론 로직은 스파이크 입력시에 가중치를 누적하여 뉴런 전위를 계산하며, 그래디언트 누적 로직은 로컬 그래디언트 유닛으로부터 그래디언트를 전달받아 누적한다. 상기 가산기 트리(Tree)&발화 로직은 상기 스파이킹 신경망 PE들의 연산 결과를 세로 방향으로 누 적하여 뉴런 전압을 생성하고 임계값을 초과하면 출력 스파이크가 발생한다. 상기 전역 카운터는 카운터로 구성되어 있으며, 여러 입력 스파이크와 출력 스파이크의 시간 차를 동시에 구하기 위해 사용된다. 상기 합성곱 신경망 처리 모듈은 복수의 합성곱 신경망 클러스터를 포함하고, 해당 합성곱 신경망 클러스터는 복수의 합성곱 신경망 코어를 포함한다. 보다 구체적으로, 상기 합성곱 신경망 처리 모듈은 4개의 합성곱 신경망 클러스터로 구성되어 있고, 각 합성곱 신경망 클러스터는 8개의 합성곱 신경망 코어로 구성되어 상보적 심층 신경망에 필요한 합성곱 연산을 할당받아 처리한다. 상기 합성곱 신경망 코어는 각각 8×16개의 합성곱 신경망 PE, 입력 메모리, 입력 로더 (Loader:2113), 가중치 메모리 및 가중치 로더를 포함하고, 상기 합성곱 신경망 PE가 곱셈 및 누적기와 연산 생략 컨트롤러를 포함하여, 복수의 합성곱 신경망 PE의 결합을 통해 고정밀도를 지원할 수 있다. 상기 합성곱 신경망 PE은 곱셈 및 누적기와 희소성 처리기로 구성되어 있으며, 추론 시에는 상보적 심층 신경망에서 요구되는 합성곱 연산을 수행하며, 학습 시에는 불필요한 가중치에 대한 역전파를 생략하고 학습이 필요한 가중치에 대해서만 그래디언트를 계산한다. 상기 입력 메모리는 합성곱 신경망 연산시에 사용되는 입력데이터를 저장한다. 상기 입력 로더(Loader:2113)는 합성곱 신경망 PE에서 매 사이클마다 요구하는 입력 데이터를 로드해준다. 상기 가중치 메모리는 합성곱 신경망 연산시에 사용되는 가중치 데이터를 저장한다. 상기 가중치 로더는 합성곱 신경망 PE에서 매 사이클마다 요구하는 가중치 데이터를 로드해준다. 상기 곱셈 및 누적기는 전달받은 가중치와 입력의 곱을 구하고 이전에 연산된 결과와 누적함으로써 합성 곱 연산을 수행한다. 상기 연산 생략 컨트롤러는 학습 시에는 불필요한 가중치에 대한 역전파를 생략하고 학습이 필요한 가중 치에 대해서만 그래디언트를 계산할 수 있도록 입력 로더와 가중치 로더를 컨트롤한다. 상기 합성곱 신경망 코어의 곱셈 및 누적기의 곱셈기 부분은 공간적으로 배열된 가산기들로 구성되 어 입력의 크기와 관계없이 항상 동작한다. 상기 스파이킹 신경망 코어의 로컬 그래디언트 누적기(1114b)는 입력 스파이크 수만큼 시간 순서대로 출 력 스파이크와의 시간 차이를 계산하여 그래디언트를 구하고 이를 누적하는 작업을 반복하게 된다. 상기 스파이킹 신경망은 희소성-에너지 효율 그래프에 도시된 바와 같이 스파이크 희소성이 97.7% 이상일 때 합성곱 신경망보다 더 높은 에너지 효율을 보이는 것을 알 수 있다. 상기 최상위 RISC 컨트롤러은 인스트럭션 메모리, 데이터 메모리, SIMD 연산기를 가지고 있으며, 상기 스 파이킹 신경망 처리 모듈과 합성곱 신경망 처리 모듈(2000에 대한 컨트롤과 활성화함수 및 배치 정규화 처리를 담당한다. 상기 신경망 연산 할당기는 타일 또는 계층의 스파이크 희소성을 미리 계산하여 상기 합성곱 신경망 코어 와 스파이킹 신경망 코어의 에너지 소비량을 추정한 다음, 에너지가 낮은 코어에 해당 타일 또는 계층의 연산을 할당한다. 상술한 내용에서 상기 타일은 예를 들어 입력 데이터를 가로 길이 8 세로 길이 8에 해당하는 영역들로 분할 하 였을 때에 각 영역을 타일(Tile)이라고 정의할 수 있다. 또한, 상기 계층은 심층 신경망이 다수의 층으로 이루어져 있는데 그 중 나의 층을 의미한다. 상기에서 스파이크 희소성을 미리 계산하는 것은 상기 신경망 연산 할당기가 연산이 끝난 신경망 코어로 부터 결과를 전달받고, 스파이크 개수가 값의 크기에 비례하는 특성을 이용해 해당 값들을 스파이크 도메인으로 변환했을 때 몇 개의 스파이크(spike)를 발생시키는 지를 계산한다. 또한, 상기 합성곱 신경망 코어와 스파이킹 신경망 코어의 에너지 소비량을 추정은 스파이크 빈도 별 스파이킹 신경망과 합성곱 신경망이 소모하는 전력을 미리 측정해 놓음으로써 스파이크 빈도별 소모 전력에 대한 수식을 구할 수 있다. 신경망 연산 할당기가 스파이크 빈도를 구한 뒤, 구해놓은 수식을 활용해 해당하는 각 코어의 전력을 구할 수 있다. 상기에서 에너지가 낮은 코어는 스파이킹 신경망과 합성곱 신경망 중 에너지 소모가 더 낮은 코어를 의미한다. 도 2를 참조하여 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 신경망 연산 할당 방식과 어텐션 모듈 및 그 세부 동작에 대해 설명한다. 상기 어텐션 모듈은 최대 풀링 유닛과 평균 풀링 유닛, 8×2개의 곱셈&누적기, 곱셈기 를 포함하여 스파이크 발생 빈도를 줄여 결과적으로 스파이크 희소성을 더 높이기 위해 어텐션 맵(Map)을 생성한다. 상기 어텐션 모듈은 입력에 대해 채널별 풀링을 수행한 다음 사전 훈련된 가중치로 컨볼루션을 수행하여 상기 어텐션 맵을 얻게 되고, 그 다음 무의미한 영역의 값을 더 작게 만들기 위해 입력을 곱한다. 더욱이 이미지넷에서 객체는 이미지의 작은 부분에만 존재하므로 상기 어텐션 모듈이 입력 분포를 도 2의 입력 크기-데이터 수 그래프에 도시된 바와 같이 축소할 수 있게 되어 스파이크 희소성이 증가하고, 어텐 션으로 인한 신경망 코어 할당 변경 예시에 보이는 것과 같이 정확도 손실 없이 저전력 스파이킹 신경망 코어에 더 많은 연산이 할당된다. 상기 최대 풀링 유닛은 입력에 대해 각 픽셀 방향으로 존재하는 여러 입력 채널의 데이터 중에서 가장 큰 값을 찾아내고, 해당 값을 남기고 다른 값들은 입력 채널에서 제거함으로써 입력 채널의 크기를 줄인다. 상기 평균 풀링 유닛은 입력에 대해 각 픽셀 방향으로 존재하는 여러 입력 채널의 데이터의 평균 값을 찾 아내고, 해당 값을 남기고 다른 값들은 입력 채널에서 제거함으로써 입력 채널의 크기를 줄인다. 상기 곱셈&누적기는 곱셈기와 누적기로 구성되어 있으며, 이를 이용해 가중치와 입력의 곱셈을 수행해 이 전 결과 값과 누적함으로써 합성곱 연산을 수행한다. 상기 곱셈기는 가중치와 입력을 전달받아 곱셈을 수행하고, 결과 값을 누적기로 전달한다. 상기 신경망 연산 할당기는 입력을 여러 타일로 나누고, 각 타일별로 발생하는 스파이크 빈도를 계산하여 에너지 소모가 적은 연산기(모델)를 예측하며, 해당 연산기로 타일을 전달하여 연산이 수행될 수 있게 해준다. 상기 전역 L2 캐시는 신경망 연산에 필요한 가중치를 저장하며, 특정 가중치가 필요한 합성곱 신경망 코 어 또는 스파이킹 신경망 코어에 해당 가중치를 전달한다. 대표적으로 사용되는 신경망 중 하나인 ResNet-18은 이미지넷에 대해 평균 95.8%의 스파이크 희소성을 나타내며, 상기 어텐션 모듈의 동작 없는 상보적 심층 신경망 가속기는 신경망별 에너지 효율 그래프 에 보이는 것과 같이 에너지 효율이 스파이킹 신경망보다 43.3%, 합성곱 신경망보다 16.7%만큼 증가하는 것을 알 수 있다. 상기 어텐션 모듈의 동작을 통해 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍 처를 갖춘 상보적 심층 신경망 가속기는 동일 네트워크 및 동일 데이터 세트에 대해 97.9%의 스파이크 희소성을 달성하고 스파이킹 신경망보다 85.8%, 합성곱 신경망보다 51.4%만큼 높은 에너지 효율 달성이 가능하다. 상기 스파이킹 신경망 PE는 도 1에 도시된 바와 같이 복수개가 64×64로 배열되어 있는데, 도 3에 도시된 바와 같이 상기 스파이킹 신경망 PE는 시간 축으로의 가중치 재사용을 위해 누산기와 레지스터로 이루어 진 4b×9 L1 캐시(Cache:1114c)를 포함한 상태로 집적되어 배열되어 있다. 스파이킹 신경망 연산 시작 시에 가중치 재사용 방식에 도시된 것과 같이 상기 스파이킹 신경망 PE(111 4)은 9개의 시냅스 전 뉴런에 대한 가중치를 읽기 동작 전력 소모가 큰 SRAM 기반의 전역 L2 캐시에서 읽 기 동작 전력 소모가 작은 L1 캐시(1114c)로 가져온다. 상기 L1 캐시(1114c)의 가중치는 입력 스파이크에 의해 순차적으로 누적되며(사이클 0~8), 그런 다음 상기 가산 기 트리&발화 로직은 상기 스파이킹 신경망 PE들의 연산 결과를 세로 방향으로 누적하여 뉴런 전압 을 생성하고 임계값을 초과하면 출력 스파이크가 발생한다. 상기 스파이킹 신경망 PE은 다시 1개의 타임 스텝(사이클 9) 후에, 읽기 동작 전력 소모가 큰 SRAM 기반 의 상기 L2 캐시에 대한 접근 없이, 상기 L1 캐시(1114c)에 저장된 가중치가 동일한 시냅스 전 뉴런들의연산에 재사용되고, 가중치 재사용 방식 결과에 도시된 것과 같이 반복되는 SRAM 기반의 메모리에 대한 접 근을 제거하여 재사용률이 3.3~5.5배 증가하고 상기 스파이킹 신경망 코어 전력이 42.2~49.1% 감소한다. 또한, 비교기와 런 렝스 인코더(Run-length Encoder)가 있는 전방향 그래디언트 기반 희소성 생성기는 고 에너지 효율의 심층 신경망 학습을 위해 전방향 그래디언트 기반 연산 생략을 지원하며, 상기 스파이킹 신경망 PE의 로컬 그래디언트 누적기(1114b)로부터 전방향 그래디언트를 전달받고 희소 연산 플래그 길이를 합성 곱 신경망 PE의 생략 인덱스 레지스터(2111a)로 전달한다. 전방향 그래디언트 기반의 상기 희소성 생성기는 합성곱 신경망 역전파 과정에 도시된 것과 같이 뉴 런에 연결된 시냅스의 전방향 그래디언트 평균 값을 구하고, 평균이 임계값보다 작으면 상기 합성곱 신경망 PE에 뉴런에 대한 오차 역전파를 생략하도록 명령한다. 또한, 전방향 그래디언트 기반의 상기 희소성 생성기는 합성곱 신경망 그래디언트 생성 과정에 도시 된 것과 같이 뉴런의 각 시냅스에 대한 임계값과 전방향 그래디언트를 비교하여 합성곱 신경망 가중치 업데이트 를 위한 그래디언트 생성을 건너뛸 수 있는지 여부를 결정한다. 희소 연산 플래그 길이는 연산 생략을 통해 건너뛰어야 하는 상기 합성곱 신경망 PE의 출력 주소를 나타 내어 입력 및 가중치 가져오기를 건너뛰고, 출력 버퍼(Buffer:2111b)는 MAC 결과를 전달받지 않게 되며, 전방향 그래디언트 기반 연산 생략에 의해 사이파-10 및 이미지넷 데이터 세트에 대해 각각 58%/79%(역전파/그래디언트 생성) 및 31%/43% 계산을 건너뛸 수 있게 된다. 도 4는 상기 스파이크 인코더, 전역 카운터, 로컬 그래디언트 유닛, LFSR 및 로컬 그 래디언트 누적기(1114b)가 집적된 상기 스파이킹 신경망 코어의 구조와 상기 스파이크 인코더에 의 한 2단계 스파이크 인코딩 과정을 도시한다. 종래에는 레이트 인코딩(Rate Encoding) 방식을 사용하여 입력 크기에 따라 스파이크 수가 비례하도록 하였고, 이를 위해 포아송(Poisson) 함수를 사용했으며 스파이크 발생 시간의 기록을 유지하기 위해 카운터를 매우 많이 집적하여 전력을 크게 소모했으나, 본 발명에서는 카운터 수를 99.2% 줄이는 2단계 인코딩을 제안한 것이다. 본 발명에 따른 상기 스파이크 인코더에 의한 1단계 인코딩에서는 모든 시간대에서 스파이크가 발생할 수 있는 기존방식과 달리 특정 시간에만 스파이크가 생성될 수 있도록 하였으며, 해당 시점에서 스파이크가 발생할 수 있는 확률은 크기(심층 신경망 각 계층의 입력으로 들어오는 데이터의 값 크기를 의미)에 비례하게 되고, 해 당 시점에서 여러 뉴런이 동시에 스파이크를 생성하기 때문에 스파이크 발생 시간을 기록하기 위해 하나의 전역 카운터를 공유할 수 있다. 본 발명에 따른 상기 스파이크 인코더에 의한 2단계 인코딩에서는 상기 LFSR에 의해 생성된 랜덤 바이어스(random bias)를 1단계에서 생성된 각 뉴런의 스파이크 트레인(Spike Train)에 추가하여 이전 포아송 랜덤 함수를 사용한 레이트 인코딩이 가지고 있던 무작위성에 대해 보상한다. 시냅스 후 뉴런 중 하나가 발화하면, 본 발명에서 제안된 상기 스파이킹 신경망 코어에 집적된 각 로컬 그래디언트 유닛은 상기 전역 카운터에서 바이어스와 카운트를 수신하여 뉴런의 입력 스파이크와 출력 스파이크 사이의 시간 차이를 계산하고 룩업 테이블(Look-Up Table:1113a)을 사용하여 시간 차이를 전방향 그래디언트로 변환한다. 상기 로컬 그래디언트 유닛에 의해 생성된 전방향 그래디언트는 64×64로 배열된 상기 스파이킹 신경망 PE 중, 같은 행의 스파이킹 신경망 PE가 공유한다. 뉴련의 입력 스파이크와 출력 스파이크의 앤드(AND) 로직으로 활성화된 상기 로컬 그래디언트 누적기(1114b)만 이 각 시냅스의 전방향 그래디언트를 누적하며, 스파이킹 신경망의 첫 번째 계층에서 추론과 전방향 그래디언트 생성을 모두 수행한 후 전체 2단계 인코딩을 반복하여 스파이킹 신경망의 다음 계층을 처리한다. 결과적으로 기존의 입력 스파이크가 발화한 시점을 기록하기 위해 항시 동작하던 많은 수의 카운터를 전역 카운 터와 입/출력 스파이크가 모두 발화했을 때만 동작하도록 상기 로컬 그래디언트 유닛을 집적하여, 그래디 언트 생성에 소요되는 연산 비용을 줄였다. 구체적으로는 성능 그래프인 도 5에 도시된 것과 같이 추론 및 전방향 그래디언트 생성이 동시에 처리될 때 상 기 스파이킹 신경망 코어(1110 )의 전력을 61%까지 줄이며, 상기 희소성 생성기에 의한 전방향 그래디언 트 기반 연산 희소성 생성은 사이파-10 및 이미지넷에 대해 각각 61.6% 및 28.7%의 심층 신경망 학습 에너지 효율성을 증가시키며 정확도 손실은 0.5% 미만에 불과하다. 이상에서는 본 발명에 대한 기술사상을 첨부 도면과 함께 서술하였지만 이는 본 발명의 바람직한 실시 예를 예 시적으로 설명한 것이지 본 발명을 한정하는 것은 아니다. 또한 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자라면 누구나 본 발명의 기술적 사상의 범주를 이탈하지 않는 범위 내에서 다양한 변형 및 모방이 가능함 은 명백한 사실이다."}
{"patent_id": "10-2023-0027163", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기의 구성도 이다. 도 2는 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가 속기의 에너지 효율적인 상보적 심층 신경망 가속을 위한 연산 할당기의 예시적인 동작 및 어텐션 모듈의 예시 적인 구조 및 예시적인 동작을 설명하기 위한 도면이다. 도 3은 본 발명에 따른 이기종 합성곱 신경망 및 스파이킹 신경망 코어 아키텍처를 갖춘 상보적 심층 신경망 가속기의 파이킹 신경망 연산 처리를 위해 분산된 L1 캐시와 누적기 기반 추론 로직을 가진 스파이킹 신경망 PE 배열 아키텍처의 예시적인 구조와 이로 인한 가중치 재사용의 예시적인 동작, 에너지 효율적인 심층 신경망 학 습을 위한 전방향 그래디언트 기반 희소성 생성의 예시적인 동작, 및 연산 전방향 그래디언트 기반 희소성 생성 기 및 합성곱 신경망 PE 아키텍처의 예시적인 구조를 도시한 도면이"}
