{"patent_id": "10-2021-0020419", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0116940", "출원번호": "10-2021-0020419", "발명의 명칭": "학습 데이터 수집 시스템 및 방법", "출원인": "네이버랩스 주식회사", "발명자": "박순용"}}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "대상물에 대해 모델링을 수행한 3차원 모델링 객체를 생성하고, 제1 기준 좌표계를 갖는 제1 공간에서 서로 다른 자세를 가지는 상기 3차원 모델링 객체를 각각 포함하는 복수의 제1 영상을 수집하는 단계; 상기 제1 기준 좌표계와 다른 제2 기준 좌표계를 갖는 제2 공간에 배치된 카메라를 이용하여, 상기 제2 공간에배치된 상기 대상물을 촬영한 제2 영상을 수집하는 단계; 및상기 카메라의 자유도 정보와 상기 복수의 제1 영상에 포함된 상기 3차원 모델링 객체를 이용하여, 상기 대상물에 대한 학습 데이터를 생성하는 단계를 포함하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 학습 데이터는,상기 복수의 제1 영상에 포함된 상기 3차원 모델링 객체 각각에 대한 마스크(MASK) 및 상기 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보를 포함하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 마스크는, 상기 제2 영상에 상기 복수의 제1 영상에 포함된 상기 3차원 모델링 객체가 합성된 이미지인 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보는, 상기 제2 공간에 대한 자유도 정보인 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보는,상기 제2 기준 좌표계에서의 상기 마스크에 각각 매칭된 3차원 모델링 객체의 자유도 정보 및 상기 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 대응되는 상기 카메라의 자유도 정보 중 적어도 하나를포함하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제4항에 있어서,상기 학습 데이터를 생성하는 단계에서는,상기 제1 공간의 상기 제1 기준 좌표계에 대하여, 상기 제2 공간에 배치된 상기 카메라의 카메라 좌표계 간의상대적인 위치 관계를 이용하여,상기 제2 공간에 대한, 상기 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보를 추출하는것을 특징으로 하는 학습 데이터 수집 방법.공개특허 10-2022-0116940-3-청구항 7 제6항에 있어서,상기 상대적인 위치 관계는, 상기 카메라의 카메라 좌표계가 상기 제1 공간의 기준 좌표계에 대하여 회전(rotation) 및 변환(translation)된정도에 근거하여 특정되는 것을 포함하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 회전(rotation) 및 변환(translation)된 정도는,상기 제1 영상에 포함된 상기 3차원 모델링 객체와 상기 제2 영상에서 상기 대상물에 대응되는 그래픽 객체 간의 관계성에 근거하여 특정되는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 학습 데이터를 생성하는 단계에서는,상기 복수의 제1 영상으로부터, 상기 제2 영상에 포함된 상기 그래픽 객체에 대응되는 상기 대상물의 자세와 유사한 자세를 갖는 특정 3차원 모델링 객체가 포함된 제1 영상을 특정하고,상기 제2 영상과 상기 특정된 제1 영상을 이용하여 상기 관계성을 추출하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 카메라가 회전(rotation) 및 변환(translation)된 정도는,상기 제2 영상에서의 상기 그래픽 객체의 픽셀 좌표 및 상기 특정된 제1 영상에서의 상기 특정 3차원 모델링 객체의 3차원 좌표를 이용하여, 추출되는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1 공간에서의 대상물에 대해 모델링을 수행한 3차원 모델링 객체를 생성하는 모델링부;상기 제1 공간과 다른 제2 공간에 배치된 카메라로부터 상기 대상물에 대한 영상을 수신하는 통신부; 및제1 기준 좌표계를 갖는 상기 제1 공간에서 서로 다른 자세를 가지는 상기 3차원 모델링 객체를 각각 포함하는복수의 영상을 수집하는 제어부를 포함하고,상기 제어부는,상기 카메라의 자유도 정보와 상기 복수의 영상에 포함된 상기 3차원 모델링 객체를 이용하여, 상기 대상물에대한 학습 데이터를 생성하는 것을 특징으로 하는 학습 데이터 수집 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 1 항 내지 제 10 항 중 어느 한 항에 따른 방법을 실행하기 위해 컴퓨터 판독 가능한 기록 매체에 기록된 컴퓨터 프로그램."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "대상물에 대한 촬영 영상을 카메라로부터 수신하는 단계;기 설정된 데이터 세트에 포함된 복수의 기준 이미지로부터, 상기 촬영 영상에 대응되는 특정 기준 이미지를 검색하는 단계; 및공개특허 10-2022-0116940-4-상기 특정 기준 이미지에 매칭된 자유도 정보를 이용하여, 상기 촬영 영상에 대응되는 상기 대상물의 자유도 자세를 추출하는 단계를 포함하고,상기 복수의 기준 이미지는,상기 대상물에 대하여 서로 다른 자세를 갖는 3차원 모델링 객체를 각각 포함하는 것을 특징으로 하는 자유도자세 추출 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 특정 기준 이미지를 검색하는 단계에서는,상기 촬영 영상에 포함된 상기 대상물에 대응되는 그래픽 객체와 상기 복수의 기준 이미지에 각각 포함된 상기3차원 모델링 객체 간의 비교를 통하여, 상기 복수의 기준 이미지에 각각 포함된 상기 3차원 모델링 객체 중 상기 그래픽 객체와 가장 유사한 자세를 갖는 특정 3차원 모델링 객체를 포함하는 상기 특정 기준 이미지를 검색하는 것을 특징으로 하는 자유도 자세 추출 방법."}
{"patent_id": "10-2021-0020419", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 대상물의 자유도 자세를 추출하는 단계에서는,상기 그래픽 객체와 상기 특정 3차원 모델링 객체 간에 상호 대응되는 매칭 포인트들 간의 관계성에 기초하여,상기 촬영 영상에 대응되는 상기 대상물의 자유도 자세를 추출하는 것을 특징으로 하는 자유도 자세 추출 방법."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능에서 학습의 대상이 되는 학습 데이터 수집 시스템 및 수집 방법에 관한 것이다. 본 발명에 따른 학습 데이터 수집 시스템은, 대상물에 대해 모델링을 수행한 3차원 모델링 객체를 생성하고, 제1 기준 좌표 계를 갖는 제1 공간에서 서로 다른 자세를 가지는 상기 3차원 모델링 객체를 각각 포함하는 복수의 제1 영상을 수집하는 단계, 상기 제1 기준 좌표계와 다른 제2 기준 좌표계를 갖는 제2 공간에 배치된 카메라를 이용하여, 상 기 제2 공간에 배치된 상기 대상물을 촬영한 제2 영상을 수집하는 단계 및 상기 카메라의 자유도 정보와 상기 복 수의 제1 영상에 포함된 상기 3차원 모델링 객체를 이용하여, 상기 대상물에 대한 학습 데이터를 생성하는 단계 를 포함할 수 있다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능에서 학습의 대상이 되는 학습 데이터 수집 시스템 및 이를 이용한 학습 데이터 수집 방법에 관한 것이다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능의 사전적 의미는, 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해능력 등을 컴퓨터 프로그램 으로 실현한 기술이라 할 수 있다. 이러한 인공지능은 머신러닝에 인간의 뇌를 모방한 신경망 네트워크를 더한 딥러닝으로 인하여 비약적인 발전을 이루었다. 딥러닝(deep learning)이란, 컴퓨터가 인간처럼 판단하고 학습할 수 있도록 하고, 이를 통해 사물이나 데이터를 군집화하거나 분류하는 기술로서, 최근에는 텍스트 데이터 뿐만 아니라 영상 데이터에 대한 분석까지 가능해져, 매우 다양한 산업분야에 적극적으로 활용되고 있다. 예를 들어, 로봇 분야, 자율 주행 분야, 의료 분야 등 다양한 산업분야에서는 딥러닝 기반의 학습 네트워크(이 하, “딥러닝 네트워크”라 명명함)를 통하여, 학습 대상 데이터를 기반으로 학습을 수행하고, 의미 있는 학습 결과를 도출함으로써, 각 산업분야에 유용하게 활용되고 있다. 일 예로서, 로봇 분야에서는, 로봇이 수행하는 작업에 대한 이해를 위하여, 로봇 주변의 상황 또는 로봇 주변에 배치된 작업 대상물에 대한 정확한 판단이 가능해야 하며, 이를 위해, 딥러닝 기반의 영상인식 기술(예를 들어, 로봇 비전(vision)기술)이 적극 활용되고 있다. 한편, 딥러닝 뿐만 아니라 머신러닝과 같은 인공지능 분야에서는, 보다 많은 양에 대한 데이터에 대해 학습을 수행함에 따라, 정확도가 높아지고, 보다 양질의 결과물을 도출하는 것이 가능하다. 따라서, 인공지능 분야에서 는, 학습의 대상이 되는 데이터를 수집하는 것이 필수적이다. 특히, 영상 데이터를 기반으로 한 딥러닝 네트워크 또는 머신러닝 네트워크는, 영상 데이터에 대응되는 대상물 (또는 물체)의 위치 또는 자세를 추정할 수 있으며, 이러한 추정을 위해서는 영상 데이터와 함께, 대상물의 자 유도 정보(위치 정보 및 자세 정보)가 학습 데이터로서 확보되어야 한다. 종래, 영상 데이터 및 이에 대응되는 자유도 정보를 학습 데이터로서 수집하기 위해서는, 영상 데이터에 대해 라벨링을 수행하고(예를 들어, 영상 데이터에서 대상물에 대응되는 특정 이미지 객체를 식별시키기 위한 작업), 특정 이미지 객체와 자유도 정보를 일일이 매핑하는 수작업이 이루어져야 하므로, 학습 데이터를 확보하기 위한 엄청난 노동력이 필요했다. 예를 들어, 국내 등록특허 10-2010085호 에서는 수퍼픽셀을 이용한 미세조직의 라벨링 이미지 생성방법 및 생성 장치를 개시하고 있으며, 이는 대상물에 대응되는 특정 이미지 객체에 대한 라벨링을 간소화하기 위한 것에 불 과하여, 특정 이미지 객체와 자유도 정보의 매핑을 위해서는 여전히 수작업이 필요하다. 이에, 자유도 정보를 포함한 학습 데이터를 자동화 방식으로 수집하는 방법에 대한 개선이 매우 절실한 상황이 다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은, 인공지능 네트워크의 학습을 위한 학습 데이터를 수집하는 학습 데이터 수집 시스템 및 방법에 관한 것이다. 보다 구체적으로, 본 발명은, 자유도 정보를 포함하는 학습 데이터를 수집하는 학습 데이터 수집 시스템 및 방 법에 관한 것이다. 나아가, 본 발명은, 자유도 정보를 포함하는 학습 데이터를 자동으로 수집할 수 있는 학습 데이터 수집 시스템 및 방법에 관한 것이다. 더 나아가, 본 발명은 다양한 자세를 갖는 대상물에 대한 학습 데이터를 수집하는 학습 데이터 수집 시스템 및 방법에 관한 것이다. 나아가, 본 발명은 학습 데이터를 수집하는데 소요되는 시간 및 노동력을 최소화할 수 있는 학습 데이터 수집 시스템 및 방법에 관한 것이다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "위에서 살펴본 과제를 해결하기 위하여, 본 발명에 따른 학습 데이터 수집 방법은, 대상물에 대해 모델링을 수 행한 3차원 모델링 객체를 생성하고, 제1 기준 좌표계를 갖는 제1 공간에서 서로 다른 자세를 가지는 상기 3차 원 모델링 객체를 각각 포함하는 복수의 제1 영상을 수집하는 단계, 상기 제1 기준 좌표계와 다른 제2 기준 좌 표계를 갖는 제2 공간에 배치된 카메라를 이용하여, 상기 제2 공간에 배치된 상기 대상물을 촬영한 제2 영상을 수집하는 단계 및 상기 카메라의 자유도 정보와 상기 복수의 제1 영상에 포함된 상기 3차원 모델링 객체를 이용 하여, 상기 대상물에 대한 학습 데이터를 생성하는 단계를 포함할 수 있다. 나아가, 본 발명에 따른 학습 데이터 수집 시스템은, 제1 공간에서의 대상물에 대해 모델링을 수행한 3차원 모 델링 객체를 생성하는 모델링부, 상기 제1 공간과 다른 제2 공간에 배치된 카메라로부터 상기 대상물에 대한 영 상을 수신하는 통신부 및 제1 기준 좌표계를 갖는 상기 제1 공간에서 서로 다른 자세를 가지는 상기 3차원 모델 링 객체를 각각 포함하는 복수의 영상을 수집하는 제어부를 포함할 수 있다. 나아가, 제어부는, 상기 카메라의 자유도 정보와 상기 복수의 영상에 포함된 상기 3차원 모델링 객체를 이용하 여, 상기 대상물에 대한 학습 데이터를 생성할 수 있다. 이러한 제어부는 상기 제1 영상에서 상기 대상물에 대응되는 그래픽 객체와 상기 제2 영상에 포함된 3차원 모델 링 객체의 관계성에 근거하여, 상기 서로 다른 자세를 가지는 상기 3차원 모델링 객체의 각각에 대한 상기 제1 공간에 배치된 상기 카메라의 자유도 정보를 추출할 수 있다. 나아가, 본 발명에 따른 자유도 자세 추출 방법은, 대상물에 대한 촬영 영상을 카메라로부터 수신하는 단계, 기 설정된 데이터 세트에 포함된 복수의 기준 이미지로부터, 상기 촬영 영상에 대응되는 특정 기준 이미지를 검색 하는 단계 및 상기 특정 기준 이미지에 매칭된 자유도 정보를 이용하여, 상기 촬영 영상에 대응되는 상기 대상 물의 자유도 자세를 추출하는 단계를 포함하고, 상기 복수의 기준 이미지는, 상기 대상물에 대하여 서로 다른자세를 갖는 3차원 모델링 객체를 각각 포함할 수 있다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "위에서 살펴본 것과 같이, 본 발명에 따른 학습 데이터 수집 시스템 및 방법은, 대상물에 대한 3차원 모델링 객 체를 생성하고, 촬영된 영상에 포함된 대상물에 해당하는 그래픽 객체로부터 생성된 3차원 모델링 객체 간의 관 계성을 이용하여, 3차원 모델링 객체의 자유도 정보를 추출할 수 있다. 이를 통해, 본 발명은, 3차원 모델링 객 체를 실제 환경에서 촬영된 영상에 반영함으로써, 실제 환경에서의 조명, 그림자 등이 반영된 학습 데이터를 생 성할 수 있다. 결과적으로, 본 발명에 의하면, 보다 실제 환경에 가까운 학습 데이터를 수집하는 것이 가능하다."}
{"patent_id": "10-2021-0020419", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시 예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소에는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설 명에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼 용되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실 시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시 예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시 예를 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 본 발명은, 인공지능 네트워크의 학습을 위한 학습 데이터를 수집하는 학습 데이터 수집 시스템 및 방법에 관한 것으로서, 특히 자유도 정보(또는 자유도 자세)를 포함하는 학습 데이터를 자동으로 수집할 수 있는 학습 데이 터 수집 방법 및 시스템에 대한 것이다. 앞서 살펴본 것과 같이, 인공지능의 발전에 힘입어 영상인식 기술은 다양한 산업분야에 활용되고 있다. 특히, 로봇 분야에서는, 인공지능 기반의 영상 인식 기술(예를 들어, 딥러닝 기반의 영상인식 기술)에 기반하여, 로봇 이 속한 작업 환경을 분석 및 이해하고, 이를 기반으로 로봇이 목표로 하는 작업을 수행하고 있다.예를 들어, 도 1에 도시된 것과 같이, 로봇(R)에게 특정 작업(예를 들어, 설거지(dish-washing)이 주어진 경우, 로봇(R) 또는 로봇(R) 주변에 배치된 카메라(미도시됨)는 로봇(R)의 작업 환경에 해당하는 영상을 촬영할 수 있 다. 그리고, 로봇(R)의 제어부는, 촬영된 영상에 기반하여, 로봇(R)이 특정 작업을 수행하기 위하여, 어떻게 동 작해야 하는지에 대한 판단을 내리고, 판단에 따라 동작하도록 로봇(R)을 제어할 수 있다. 이 경우, 로봇(R)의 제어부는, 촬영된 영상에서 작업의 대상이 되는 대상물(A, 또는 객체(object), 예를 들어, 그릇(a1, a2))을 인식하고, 대상물(A)의 위치 및 자세(또는 포즈, pose)를 분석하여, 로봇(R)이 대상물에 대해 목표로 하는 작업을 수행할 수 있도록 로봇(R)을 제어해야 한다. 이를 위하여, 로봇(R)의 제어부는, 촬영된 영상으로부터 다양한 정보를 수집하여야 하며, 예를 들어, i) 작업의 대상이 되는 대상물의 종류, ii) 작업의 대상이 되는 대상물의 크기, iii) 작업의 대상이 되는 대상물의 형상, iv) 작업의 대상이 되는 대상물의 위치(예를 들어, 도 1에 도시된 것과 같이, 그릇(a1)이 싱크대(sink)의 어디 쯤에 놓여 있는지 등), v) 작업의 대상이 되는 대상물의 자세(예를 들어, 도 1에 도시된 것과 같이, 그릇(a1)이 싱크대에 놓여져 있는 자세(ex: 비스듬히 기울어져 있는지 등)), vi) 대상물을 촬영하는 카메라의 자세에 대한 정보 중 복수의 정보를 이용하여, 로봇(R)을 정확하게 제어할 수 있다. 여기에서, 작업의 대상이 되는 대상물 또는 대상물을 촬영하는 카메라의 위치 및 자세는 “자유도”, “자유도 자세” 또는 “자유도 정보”라고도 표현될 수 있으며, 본 명세서에서는 설명의 편의를 위하여, “자유도 정보 ”라고 통일하여 명명하도록 한다. 한편, 자유도 정보는 위치 정보 및 자세 정보를 포함한 개념으로 이해되어 질 수 있다. 이러한, 자유도 정보는, 3차원 위치(x, y, z)에 해당하는 위치 정보(또는 3차원 위치 정보) 및 3차원 자세(r(roll), θ(pitch), Φ (yaw))에 해당하는 자세 정보(또는 3차원 자세 정보)를 포함할 수 있다. 한편, 로봇(R)이 작업의 대상이 되는 대상물에 대하여 정확하게 작업을 수행하기 위해서는 자유도 정보를 파악 하는 것이 매우 중요하다. 예를 들어, 로봇(R)의 제어부는 작업의 대상이 되는 대상물(a1, a2)을 잡기 위하여, 로봇 팔(R1, R2)을 어떤 각 도로 제어하고, 어떤 자세로 파지를 해야 하는지를 결정해야 하며, 이는 작업의 대상이 되는 대상물(또는 대상 물을 촬영하는 카메라)의 자세 및 위치 중 적어도 하나에 근거하여 결정되기 때문이다. 이때, 촬영된 영상으로부터 작업의 대상이 되는 대상물(예를 들어, a1, a2)이 인식된 것만으로, 대상물(또는 대 상물을 촬영한 카메라)의 자유도 정보까지 인지할 수 있다면, 작업의 정확도 뿐만 아니라, 작업의 효율을 확보 할 수 있다. 이를 위하여, 촬영된 영상으로부터 획득되는 특정 형상(또는 특정 자세)를 갖는 대상물에 대한 이미지(또는 마 스크(mask)와 대상물에 대한 자세 정보가 상호 매칭되어, 학습 데이터로서 활용될 수 있다. 한편, 대상물에 대한 자세 정보는, i) 대상물이 특정 형상일때, 대상물의 기준 좌표계를 기준으로 어떤 위치 또 는 어떤 자세를 갖는지에 대한 대상물 기준의 자유도 정보 및 ii) 대상물이 특정 형상 일때, 대상물을 촬영한 카메라가, 카메라의 기준 좌표계를 기준으로, 어떤 위치 또는 어떤 자세를 갖는지에 대한 카메라 기준의 자유도 정보 중 적어도 하나를 포함할 수 있다. 본 발명에서 설명되는 자유도 정보는, 대상물 기준의 자유도 정보와 카메라 기준의 자유도 정보를 혼용하는 개 념으로 이해되어 질 수 있다. 즉, 대상물의 자유도 정보는 곧 대상물을 촬영한(또는 대상물을 바라보는) 카메라의 자유도 정보로 이해되어질 수 있다. 이와 반대로, 대상물을 촬영한 카메라의 자유도 정보는, 대상물의 자유도 정보로 이해되어 질 수 있음 은 물론이다. 이는, 대상물의 기준 좌표계와 카메라의 기준 좌표계는 서로 상대적인 위치 관계를 갖기 때문이다. 예를 들어, 대상물의 기준 좌표계에 대한 대상물의 자유도 정보에 역변환을 수행하는 경우, 대상물의 기준 좌표 계에 대한 카메라의 자유도 정보가 얻어질 수 있다. 이와 반대로, 카메라의 기준 좌표계에 대한 카메라의 자유도 정보에 역변환을 수행하는 경우, 카메라의 기준 좌 표계에 대한 대상물의 자유도 정보가 얻어질 수 있다. 나아가, 대상물의 기준 좌표계와 카메라의 기준 좌표계 간의 상대적인 위치 관계가 정의되는 경우, 대상물의 기 준 좌표계에 대한 대상물의 자유도 정보로부터, 카메라의 기준 좌표계에 대한 카메라의 자유도 정보가 얻어질 수 있다. 이와 반대로, 카메라의 기준 좌표계에 대한 카메라의 자유도 정보로부터, 대상물의 기준 좌표계에 대한 대상물 의 자유도 정보가 얻어질 수 있음은 물론이다. 예를 들어, 대상물의 자유도 정보에 대하여, 대상물의 기준 좌표계와 카메라의 기준 좌표계 간의 상대적인 위치 관계를 반영하는 경우, 카메라의 자유도 정보가 얻어질 수 있다. 이와 반대로, 카메라의 자유도 정보에 대하여, 카메라의 기준 좌표계와 대상물의 기준 좌표계 간의 상대적인 위치 관계를 반영하는 경우, 대상물의 자유도 정 보가 얻어질 수 있다. 여기에서, 상대적인 위치 관계는, 어느 하나의 기준 좌표계에 대하여 다른 하나의 기준 좌표계가 회전 (rotation) 및 변환(translation, 병진 이동)된 정도를 의미할 수 있다. 한편, 로봇(R)이 정확한 작업을 수행하기 위해서는, 방대한 학습 데이터를 기반으로 학습된 인공지능 알고리즘 (예를 들어, 딥러닝 알고리즘 또는 딥러닝 네트워크)이 필요하다. 따라서, 본 발명에서는, 학습 데이터를 수집 하는 방법에 대하여 첨부된 도면과 함께 보다 구체적으로 살펴본다. 도 2는 3차원 모델링 객체를 생성하는 방법 을 설명하기 위한 개념도이고, 도 3은 본 발명에 따른 학습 데이터 수집 시스템을 설명하기 위한 개념도이다. 나아가, 도 4는 본 발명에 따른 학습 데이터 수집 방법을 설명하기 위한 흐름도이며, 도 5, 도 6, 도 7, 도 8, 도 9 및 도 10은 학습 데이터를 수집하는 방법을 설명하기 위한 개념도들이다. 나아가, 도 11 및 도 12는 수집 된 학습 데이터를 활용하는 방법을 설명하기 위한 개념도들이다. 본 발명에 대한 설명에 앞서, 본 명세서에서 언급되는 “대상물”은, 그 종류에 제한이 없으며, 매우 다양한 물 체로 해석되어 질 수 있다. 대상물은 시각적 또는 물리적으로 구분이 가능한 구체적인 형태를 가지고 있는 것으 로서, 물건(또는 물체) 뿐만 아니라, 사람 또는 동물의 개념까지 포함하는 것으로 이해되어 질 수 있다. 앞서 살펴본 것과 같이, 로봇 또는 자율 주행 차량 등의 보다 높은 성능을 위해서는, 최대한 많은 양의 학습 데 이터를 기반으로, 학습을 수행하는 것이다. 이를 위하여, 학습 데이터를 확보하는 것은 매우 중요한 일이며, 본 발명에서는 3차원 모델링 객체를 활용하여 학습 데이터를 확보하는 방법에 대하여 제안한다. 도 2에 도시된 것과 같이, 본 발명에서는, 대상물(예를 들어, 도 2에 도시된 컵(cup))에 대해 모델링을 수행한 3차원 모델링 객체를 생성 생성할 수 있다. 이러한 3차원 모델링 객체는 실제 물체와 최대한 동일한 형상 및 크기(또는 크기 비율)을 갖도록 모델링 될 수 있다. 3차원 모델링 객체를 모델링 하는 방법은, 매 우 다양하며, 예를 들어, 3D CAD를 통하여 생성될 수 있다. 이러한 3차원 모델링 객체는 텍스쳐(texture) 가 입혀진 메쉬(mesh) 모델에 해당할 수 있다. 3차원 모델링 객체에 입혀지는 텍스쳐는, 실제 물체와 동일 또는 유사하게 이루어질 수 있다. 나아가, 본 발명에서는, 제1 기준 좌표계(W1)를 갖는 제1 공간에서 서로 다른 자세를 가지는 상기 3차원 모델링 객체를 각각 포함하는 복수의 제1 영상(611 내지 616 참고)을 수집할 수 있다. 여기에서, 제1 기준 좌표계(W1)는, 3차원 모델링 객체가 포함된 가상의 환경(모델링 환경)의 기준 좌표계 를 의미할 수 있다. 한편, 이러한 모델링 객체의 생성 및 영상의 수집은, 도 3에서 살펴볼 모델링부 또는 제어부에 의하 여 이루어질 수 있으며, 설명의 편의를 위하여, 제어부로 통일하여 설명하도록 한다. 복수의 제1 영상(611 내지 616 참고)에는, 도 2에 도시된 것과 같이, 서로 다른 자세를 갖는 3차원 모델링 객체 가 포함될 수 있다. 이러한, 복수의 제1 영상(611 내지 616 참고)에 포함된 3차원 모델링 객체는 제1 기준 좌표계(W1)를 기준 으로, 제1 기준 좌표계 내에서 소정의 위치에 소정의 자세로 위치할 수 있다. 이러한 소정의 위치 및 소정의 자 세는 각각의 3차원 모델링 객체의 자유도 정보가 될 수 있다. 즉, 제어부는 복수의 제1 영상(611 내지 616)에 서로 다른 자유도 정보를 갖는 3차원 모델링 객체가 포함되도록, 복수의 제1 영상(611 내지 616)을 생성할 수 있다. 나아가, 제어부는 복수의 제1 영상(611 내 지 616)에 포함된 3차원 모델링 객체에 대한 3D Point Cloud를 생성할 수 있으며, 이를 기반으로, 각각의 영상으로부터 3차원 모델링 객체의 깊이 정보를 확보할 수 있다. 한편, 이러한 복수의 제1 영상(611 내지 616)은, 제1 카메라 기준 좌표계(C1)를 갖는 가상의 카메라가 3차 원 모델링 객체를 촬영하였다는 가정을 전제로 생성된 영상일 수 있다. 즉, 제어부는 가상의 카메라가 3차원 모델링 객체를 촬영하였다는 가정하에, 복수의 제1 영상 (611 내지 616)을 생성할 수 있다. 이때, 복수의 제1 영상(611 내지 616)은, i)3차원 모델링 객체은 고정된 상태에서 가상의 카메라가 다양한 자세로 3차원 모델링 객체를 촬영하였다는 전제하에 생성된 영상이거나, ii) 가상의 카메라는 고정된 상태에서, 3차원 모델링 객체가 다양한 자세로 움직였졌다는 전제하에 촬영된 영상일 수 있다. 이와 같이, 본 발명에서는, 3차원 모델링 객체를 이용하여, 제어부 자체에서, 복수의 제1 영상(611 내지 616)을 생성하기에, 복수의 제1 영상(611 내지 616)에 대한 다양한 정보가 확보될 수 있다. 다양한 정보는, i) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체의 제1 기준 좌표계 (W1)를 기준으로 하는 자유도 정보, ii) 복수의 제1 영상(611 내지 616)을 촬영한 카메라의 제1 카메라 좌 표계(C1)에 대한 자유도 정보, iii) 제1 기준 좌표계(W1)과 제1 카메라 좌표계(C1) 간의 상대적인 위치관계 정 보, iv) 복수의 제1 영상(611 내지 616)을 촬영한 카메라의 제1 기준 좌표계(W1)에 대한 자유도 정보, v) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체의 제1 카메라 좌표계(C1)에 대한 자유도 정보, vii) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체의 뎁스(깊이) 정보 중 적어 도 하나를 포함할 수 있다. 한편, 위에서 열거된 다양한 정보는, 저장부(120, 도 3 참조)에 저장될 수 있다. 나아가, 저장부에는 위에 서 열거된 다양한 정보 중 적어도 하나와, 이에 대응되는 제1 영상(또는 제1 영상에 포함된 모델링 객체에 대응되는 이미지)이 매칭되어 저장될 수 있다. 즉, 복수의 제1 영상(611 내지 616) 각각은, 각각의 제1 영상에 포함된 3차원 모델링 객체의 자세와 관련 된 정보와 상호 매칭되어 저장될 수 있다. 다음으로, 본 발명에 따른 학습 데이터 수집 시스템에 대하여 도 3과 함께 보다 구체적으로 살펴본다. 본 발명에 따른 학습 데이터 수집 시스템은, 통신부, 저장부 및 제어부 중 적어도 하나를 포 함할 수 있다. 통신부는 카메라로부터 촬영된 영상을 수신하기 위한 수단으로서, 통신 방법에는 특별한 제한을 두지 않는다. 통신부는 유선 또는 무선 통신 중 적어도 하나를 수행하도록 이루어질 수 있다. 통신부는 통신이 가 능한 다양한 대상과 통신을 수행하도록 이루어질 수 있다. 한편, 통신부는 적어도 하나의 외부 서버와 통신하도록 이루어질 수 있다. 여기에서, 외부 서버는, 저장부 의 적어도 일부의 구성에 해당하는 클라우드 서버 또는 데이터베이스 중 적어도 하나를 포함할 수 있다. 한편, 외부 서버에서는, 제어부의 적어도 일부의 역할을 수행하도록 구성될 수 있다. 즉, 데이터 처리 또 는 데이터 연산 등의 수행은 외부 서버에서 이루어지는 것이 가능하며, 본 발명에서는 이러한 방식에 대한 특별 한 제한을 두지 않는다. 한편, 통신부는 통신하는 대상의 통신 규격에 따라 다양한 통신 방식을 지원할 수 있다. 예를 들어, 통신부는, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), Wi-Fi(Wireless Fidelity) Direct, DLNA(Digital Living Network Alliance), WiBro(Wireless Broadband), WiMAX(World Interoperability for Microwave Access), HSDPA(High Speed Downlink Packet Access), HSUPA(High Speed Uplink Packet Access), LTE(Long Term Evolution), LTE-A(Long Term Evolution-Advanced), 5G(5th Generation Mobile Telecommunication ), 블루투스(Bluetooth™), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), UWB(Ultra-Wideband), ZigBee, NFC(Near Field Communication), Wi-Fi Direct, Wireless USB(Wireless Universal Serial Bus) 기술 중 적어도 하나를 이용하여, 통신을 수행하도록 이루어질 수 있다. 한편, 카메라는 영상을 촬영하기 위한 수단으로서, 본 발명에 따른 시스템 내에 포함되거나, 또는 별 도로 구비될 수 있다. 본 발명에서 카메라는 “이미지 센서”라고도 명명될 수 있다. 카메라는 정적인 영상 및 동적인 영상 중 적어도 하나를 촬영하도록 이루어질 수 있으며, 단수 또는 복수 로 구비될 수 있다.카메라는 대상물(또는 피사체, 또는 물체, 도면부호 300 참조)의 깊이 정보를 획득할 수 있는 3차원 깊이 카메라(3D depth camera) 또는 RGB-깊이 카메라(RGB-depth camera) 등으로 이루어질 수 있다. 카메라가 3 차원 깊이 카메라로 이루어진 경우, 촬영된 영상을 이루는 각 픽셀(pixel)의 깊이 값을 알 수 있으며, 이를 통 하여 대상물의 깊이 정보가 획득될 수 있다 이러한 카메라는 도 3에 도시된 것과 같이, 제2 기준 좌표계를 갖는 제2 공간에 위치한 대상물 를 촬영하도록 이루어질 수 있다. 카메라는 실제 환경(현실 공간)에 존재하는 카메라를 의미할 수 있다. 나아가, 카메라는 제2 카메라 좌표계(C2)를 갖도록 이루어질 수 있다. 즉, 본 발명에서 3차원 모델링 객체(도 2 참조, 610)가 포함된 제1 공간은 제1 기준 좌표계(W1)를 가지고, 3차 원 모델링 객체(도 2 참조, 610)를 촬영하는 것으로 정의된 가상의 카메라는 제1 카메라 좌표계(C1)를 가 질 수 있다. 나아가, 실제 환경(현실 공간, 400)에 존재하는, 카메라는 제2 카메라 좌표계(C2)를 가지며, 이러한 카메 라는 제2 공간(실제 환경 또는 현실 공간, 400)에서의 제2 기준 좌표계(W2)에 놓여진 대상물을 촬영 하도록 이루어질 수 있다. 한편, 저장부는 본 발명에 따른 다양한 정보를 저장하도록 이루어질 수 있다. 저장부의 종류는 매우 다양할 수 있으며, 적어도 일부는, 외부 서버(클라우드 서버 및 데이터베이스(database: DB) 중 적어도 하나)를 의미할 수 있다. 즉, 저장부와 관련된 정보가 저장되는 공간이면 충분하며, 물리적인 공간에 대한 제약은 없는 것으로 이해될 수 있다. 저장부에는 i)앞서 도 2와 함께 살펴본 3차원 모델링 객체와 관련된 다양한 정보, ii)본 발명에 따른 데이 터 수집 시스템에 의해 수집된 학습 데이터, iii) 카메라를 통해 촬영된 영상, iv) 촬영된 영상과 관련된 대상물 및 카메라 중 적어도 하나와 관련된 자유도 정보, v) 제1 기준 좌표계(W1), 제1 카메라 좌표계(C1), 제2 기준 좌표계(W2) 및 제2 카메라 좌표계(C2)중 적어도 두개 간의 상대적인 위치관계에 대한 정보 중 적어도 하나 가 저장될 수 있다. 다음으로 제어부는 본 발명과 관련된 학습 데이터 수집 시스템의 전반적인 동작을 제어하도록 이루어 질 수 있다. 제어부는 인공지능 알고리즘을 처리 가능한 프로세서(processor, 또는 인공지능 프로세서)를 포함할 수 있다. 제어부는 도 2에서 함께 살펴본 3차원 모델링 객체를 생성 및 이와 관련된 다양한 제1 영상을 생성하 는 모델링부를 더 포함할 수 있다. 나아가, 제어부는 수집된 영상들 및 자유도 정보를 기반으로 학습을 수행하는 학습부를 더 포함할 수 있다. 이러한 학습부는 신경망 네트워크 구조를 가질 수 있다. 한편, 제어부는 딥러닝 알고리즘에 기반하여, 카메라를 통해 촬영되는 영상에서, 카메라에 의해 촬영된 대상물을 인식 및 추적할 수 있다. 이러한 작업은 트래킹(tracking)이라고도 명명될 수 있다. 나아가 제어부는 카메라로부터 촬영된 영상(이하, 제2 영상) 및 3차원 모델링 객체에 대한 제1 영상 을 이용하여, 다양한 학습 데이터를 수집(또는 생성)할 수 있다. 본 발명에서, 3차원 모델링 객체에 대한 영상은 “제1 영상”이라고 명명하고, 실제 환경에 존재하는 카메라 로부터 촬영된 영상은 “제2 영상”이라고 명명하도록 한다. 한편, 제어부는 제2 영상에 포함된 마커 보드(Marker Board, 410) 또는 제2 영상에서 대상물에 대응 되는 그래픽 객체를 제외한 배경의 텍스쳐에 기반하여, 대상물을 촬영한 카메라의 자유도 정보를 추 출할 수 있다. 제어부는 제2 영상에 포함된 시각적 특성(Visual Feature)에 기반하여, 카메라의 자유도 정보를 추출 할 수 있다. 상기 시각적 특성은, 제2 영상에 포함된 마커 보드(Marker Board, 410) 또는 제2 영상에서 대상물에 대응 되는 그래픽 객체를 제외한 배경의 텍스쳐에 근거하여 정의될 수 있다. 한편, 카메라의 자유도 정보는, 대상물을 촬영한 카메라의 3차원 위치(x, y, z)에 해당하는 위 치 정보(또는 3차원 위치 정보(병진 운동의 자유도에 해당함)) 및 3차원 자세(r(roll,롤), θ(pitch,피치), Φ (yaw,요우))에 해당하는 자세 정보(또는 3차원 자세 정보(회전 운동의 자유도에 해당함))를 포함할 수 있다. 카메라의 자유도 정보는, 대상물이 위치한 제2 공간 상의 제2 기준 좌표계(W2)에 대한 자유도 정보 및 카메라의 제2 카메라 좌표계(C2)에 대한 자유도 정보 중 적어도 하나를 포함할 수 있다. 나아가, 제어부는 제2 영상에 대응되는 대상물의 자유도 정보를 획득할 수 있음은 물론이다. 제어부 는 카메라의 자유도 정보에 기반하여, 대상물의 자유도 정보를 획득하거나, 제2 영상으로부터 대상물의 자유도 정보를 획득할 수 있다. 제어부는 제2 기준 좌표계(W2)와 카메라 제2 카메라 좌표계(C2)간의 상대적인 위치 관계에 근거하여, 카메 라의 자유도 정보로부터 대상물의 자유도 정보를 산출하거나, 이와 반대로 대상물의 자유도 정 보로부터 카메라의 자유도 정보를 산출할 수 있다. 이는, 대상물의 제2 기준 좌표계(W2)와 카메라의 기준 좌표계는 서로 상대적인 위치 관계를 갖기 때문이다. 예를 들어, 대상물의 제2 기준 좌표계(W2)에 대한 대상물의 자유도 정보에 역변환을 수행하는 경우, 대상물의 제2 기준 좌표계(W2)에 대한 카메라의 자유도 정보가 얻어질 수 있다. 이와 같이, 제어부는, 카메라와 대상물에 각각 포함된 제2 카메라 좌표계(C2) 와 제2 기준 좌표 계(W2) 간의 상대적인 위치 관계에 근거하여 다양한 정보를 연산 및 산출하고, 이러한 정보를 저장부에 저 장할 수 있다. 다양한 정보는, i) 대상물의 제2 기준 좌표계(W2)를 기준으로 하는 자유도 정보, ii) 대상물이 포함 된 제2 영상을 촬영한 카메라의 제2 카메라 좌표계(C2)에 대한 자유도 정보, iii) 제2 기준 좌표계(W2)과 제2 카메라 좌표계(C2) 간의 상대적인 위치관계 정보, iv) 제2 영상을 촬영한 카메라의 제2 기준 좌표계 (W2)에 대한 자유도 정보,2 v) 제2 영상에 대응되는 대상물의 제2 카메라 좌표계(C2)에 대한 자유도 정보, vii) 제2 영상에서의 대상물에 대응되는 그래픽 객체의 픽셀 좌표에 대한 정보 중 적어도 하나를 포함할 수 있다. 한편, 본 발명에서 제어부는 i)3차원 모델링 객체에 대한 복수의 제1 영상 및 대상물을 촬영한 제2 영상, ii) 제1 및 제2 영상에서의 대상물의 자유도 정보, iii) 제1 및 제2 영상에서의 카메라(또는 가상 카메라)의 자유도 정보, iv) 제1 기준 좌표계(W1), 제1 카메라 좌표계(C1), 제2 기준 좌표계(W2), 제2 카메라 좌표계(C2) 중 적어도 두개 간의 상대적인 위치 관계 중 적어도 일부를 이용하여, 방대한 양의 학습 데이터를 수집 또는 생성할 수 있다. 이러한 학습 데이터에는 복수의 제1 영상에 포함된 3차원 모델링 객체 각각에 대한 마스크(MASK) 및 상기 마스 크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보가 포함될 수 있다. 여기에서, 마스크는, 실제 공간에 대해 촬영된 제2 영상에 3차원 모델링 객체가 합성(또는 투영)된 이미지 이거나, 3차원 모델링 객체 자체에 대한 이미지일 수 있다. 한편, 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보는 제2 기준 좌표계(W2)에 대한 대 상물 또는 카메라의 자유도 정보이거나, 제2 카메라 좌표계(C2)에 대한 대상물 또는 카메라 의 자유도 정보일 수 있다. 여기에서, 대상물은 도 2에서 살펴본 3차원 모델링 객체에 대한 실 제 물체일 수 있다. 이와 같이, 제어부 3차원 모델링 객체를 포함하는 복수의 영상을 이용하여, 실제 공간에서 활용될 수 있는 대상물 또는 카메라의 자유도 정보를 획득할 수 있다. 한편, 3차원 모델링 객체를 이용하여 생 성할 수 있는 데이터의 양은 수만~ 수천만장 이상으로 매우 방대하므로, 본 발명에 의할 경우, 학습에 필요한 충분한 양의 학습 데이터를 생성할 수 있다. 이하에서는, 위에서 살펴본 본 발명에 따른 학습 데이터 수집 시스템의 구성에 기반하여, 학습 데이터를 수집하 는 방법에 대하여 보다 구체적으로 살펴본다. 먼저, 본 발명에 따른 학습 데이터 수집 방법에 의하면, 물체에 대해 모델리을 수행한 3차원 모델링 객체를 생 성하는 과정이 진행될 수 있다(S410).앞서 도 2와 함께 살펴본 것과 같이, 제어부는, 대상물(예를 들어, 도 2에 도시된 컵(cup), 도 3의 도면부 호 300의 물체 참조)에 대해 모델링을 수행한 3차원 모델링 객체를 생성 생성할 수 있다. 이러한 3차원 모델링 객체는 실제 대상물과 최대한 동일한 형상 및 크기(또는 크기 비율)을 갖도록 모델링 될 수 있다. 3차원 모델링 객체를 모델링 하는 방법은, 매우 다양하며, 예를 들어, 3D CAD를 통하여 생성될 수 있다. 이러한 3차원 모델링 객체는 텍스쳐(texture)가 입혀진 메쉬(mesh) 모델에 해당할 수 있다. 3차원 모델링 객체에 입혀지는 텍스쳐는, 실제 대상물과 동일 또는 유사하게 이루어질 수 있다. 이때, 3차원 모델링 객체는 가상의 모델링 공간에 해당하는 제1 공간(미도시됨)의 제1 기준 좌표계(W1)를 기준으로 제2 공간 상에 특정 자세(3차원 자세(r(roll,롤), θ(pitch,피치), Φ(yaw,요우))에 해당함) 및 특정 위치(3차원 위치(x, y, z)에 해당함)를 갖도록 놓여질 수 있다. 여기에서, 제1 기준 좌표계(W1)는, 3차원 모델링 객체가 포함된 가상의 환경(모델링 환경)의 기준 좌표계 를 의미할 수 있다. 다음으로, 본 발명에서는, 제1 기준 좌표계(W1)를 갖는 제1 공간에서 서로 다른 자세를 가지는 3차원 모델링 객 체를 각각 포함하는 복수의 제1 영상을 수집하는 과정이 진행될 수 있다(S420). 앞서 도 2에 도시된 것과 같이, 제어부는 제1 기준 좌표계(W1)를 갖는 제1 공간에서 서로 다른 자세를 가 지는 상기 3차원 모델링 객체를 각각 포함하는 복수의 제1 영상(611 내지 616 참고)을 수집할 수 있다. 여기에서, 제1 기준 좌표계(W1)는, 3차원 모델링 객체가 포함된 가상의 환경(모델링 환경)의 기준 좌표계 를 의미할 수 있다. 복수의 제1 영상(611 내지 616 참고)에는, 도 2에 도시된 것과 같이, 서로 다른 자세를 갖는 3차원 모델링 객체 가 포함될 수 있다. 이러한, 복수의 제1 영상(611 내지 616 참고)에 포함된 3차원 모델링 객체는 제1 기준 좌표계(W1)를 기준 으로, 제1 기준 좌표계 내에서 소정의 위치에 소정의 자세로 위치할 수 있다. 이러한 소정의 위치 및 소정의 자 세는 각각의 3차원 모델링 객체의 자유도 정보가 될 수 있다. 즉, 제어부는 복수의 제1 영상(611 내지 616)에 서로 다른 자유도 정보를 갖는 3차원 모델링 객체가 포함되도록, 복수의 제1 영상(611 내지 616)을 생성할 수 있다. 나아가, 제어부는 복수의 제1 영상(611 내 지 616)에 포함된 3차원 모델링 객체에 대한 3D Point Cloud를 생성할 수 있으며, 이를 기반으로, 각각의 영상으로부터 3차원 모델링 객체의 깊이 정보를 확보할 수 있다. 한편, 이러한 복수의 제1 영상(611 내지 616)은, 제1 카메라 기준 좌표계(C1)를 갖는 가상의 카메라(620, 도 2 참조)가 3차원 모델링 객체를 촬영하였다는 가정을 전제로 생성된 영상일 수 있다. 즉, 제어부는 가상의 카메라가 3차원 모델링 객체를 촬영하였다는 가정하에, 복수의 제1 영상 (611 내지 616)을 생성할 수 있다. 이때, 복수의 제1 영상(611 내지 616)은, i)3차원 모델링 객체는 고정된 상태에서 가상의 카메라가 다양한 자세로 3차원 모델링 객체를 촬영하였다는 전제하에 생성된 영상이거나, ii) 가상의 카메라는 고정된 상태에서, 3차원 모델링 객체가 다양한 자세로 움직여졌다는 전제하에 촬영된 영상일 수 있다. 이와 같이, 본 발명에서는, 3차원 모델링 객체를 이용하여, 제어부 자체에서, 복수의 제1 영상(611 내지 616)을 생성하기에, 복수의 제1 영상(611 내지 616)에 대한 다양한 정보가 확보될 수 있다. 앞서 살펴본 것과 같이, 다양한 정보는, i) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체 의 제1 기준 좌표계(W1)를 기준으로 하는 자유도 정보, ii) 복수의 제1 영상(611 내지 616)을 촬영한 카메 라의 제1 카메라 좌표계(C1)에 대한 자유도 정보, iii) 제1 기준 좌표계(W1)과 제1 카메라 좌표계(C1) 간 의 상대적인 위치관계 정보, iv) 복수의 제1 영상(611 내지 616)을 촬영한 카메라의 제1 기준 좌표계(W1) 에 대한 자유도 정보, v) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체의 제1 카메라 좌표계(C1)에 대한 자유도 정보, vii) 복수의 제1 영상(611 내지 616) 각각에 포함된 3차원 모델링 객체의 뎁스(깊이) 정보 중 적어도 하나를 포함할 수 있다. 한편, 위에서 열거된 다양한 정보는, 저장부(120, 도 3 참조)에 저장될 수 있다. 나아가, 저장부에는 위에 서 열거된 다양한 정보 중 적어도 하나와, 이에 대응되는 제1 영상(또는 제1 영상에 포함된 모델링 객체에 대응되는 이미지)이 매칭되어 저장될 수 있다. 즉, 복수의 제1 영상(611 내지 616) 각각은, 각각의 제1 영상에 포함된 3차원 모델링 객체의 자세와 관련 된 정보와 상호 매칭되어 저장될 수 있다. 다음으로 본 발명에서는, 제2 기준 좌표계를 갖는 제2 공간에 배치된 카메라를 이용하여, 제2 공간에 배치된 물 체(또는 대상물)를 촬영한 제2 영상을 수집하는 과정이 진행될 수 있다(S430). 한편, S430 과정은, 위에서 살펴본 S410 및 S420 과정보다 먼저 진행되거나, 동시에 진행될 수 있음은 물론이다. 보다 구체적으로 제어부는, 도 5의 (a) 내지 (c)에 도시된 것과 같이, 실제 환경(현실 공간, 400)에 존재 하는 대상물에 대하여, 카메라를 통하여 영상을 촬영하고, 촬영 결과로서 얻어진 제2 영상을 수집할 수 있다. 이때, 실제 환경에 해당하는 제2 공간은 제2 기준 좌표계(W2)를 가지며, 카메라는 제2 카메라 좌표계(C2) 를 가질 수 있다. 이러한 카메라는 제2 공간(실제 환경 또는 현실 공간, 400)에서의 제2 기준 좌표계(W2)에 놓여진 대상물 을 촬영하도록 이루어질 수 있다. 제어부는, 도5의 (a), (b) 및 (c)에 도시된 것과 같이, 대상물을 기준으로 카메라의 3차원 자세 및 3차원 위치를 변경할 수 있으며, 이를 통하여, 복수의 제2 영상에는 대상물에 대해 서로 다른 자세를 갖는 그래픽 객체가 포함될 수 있다. 한편, 제어부는 제2 영상에 포함된 마커 보드(Marker Board, 410) 또는 제2 영상에서 대상물에 대응 되는 그래픽 객체를 제외한 배경의 텍스쳐에 기반하여, 대상물을 촬영한 카메라의 자유도 정보를 추 출할 수 있다. 상기 시각적 특성은, 제2 영상에 포함된 마커 보드(Marker Board, 410) 또는 제2 영상에서 대상물에 대응 되는 그래픽 객체를 제외한 배경의 텍스쳐에 근거하여 정의될 수 있다. 예를 들어, 마커 보드를 통하여, 카메라의 자유도 정보가 추출되는 방법에 대하여 살펴보면, 제어부 는 도 5의 (a), (b) 및 (c)에 도시된 제2 공간을 촬영한 도 6의 (a), (b) 및 (c)에 도시된 제2 영상 (601, 602, 603)으로부터, 카메라의 자유도 정보를 추출할 수 있다. 제어부는 제2 영상(601, 602, 603) 상에서의 마커 보드에 해당하는 그래픽 객체(410a, 410b, 410c) 의 배열 위치 및 회전 정도를 기준으로, 대상물을 촬영한 카메라의 자유도 정보를 추출할 수 있다. 나아가, 제어부는 제2 영상(601, 602, 603) 상에서의 대상물에 대응되는 그래픽 객체(300a, 300b, 300c)의 배치 위치, 회전 정도를 고려하여, 상기 카메라의 자유도 정보를 추출할 수 있다. 한편, 카메라의 자유도 정보는, 대상물을 촬영한 카메라의 3차원 위치(x, y, z)에 해당하는 위 치 정보(또는 3차원 위치 정보(병진 운동의 자유도에 해당함)) 및 3차원 자세(r(roll,롤), θ(pitch,피치), Φ (yaw,요우))에 해당하는 자세 정보(또는 3차원 자세 정보(회전 운동의 자유도에 해당함))를 포함할 수 있다. 이때의 자유도 정보는, 제2 기준 좌표계(W2)를 기준으로 하는 카메라의 자유도 정보이거나, 제2 카메라 좌 표계(C2)를 기준으로 하는 카메라의 자유도 정보일 수 있다. 제어부는 실제 공간에 대한 제2 기준 좌표계(W2)와 제2 카메라 좌표계(C2) 간의 상대적인 위치 관계 를 알고 있으므로, 제2 영상(601, 602, 603)으로부터, 제2 기준 좌표계를 기준으로 하는 카메라의 자유도 정보를 추출한 경우라도, 제2 카메라 좌표계(C2)를 기준으로 하는 카메라의 자유도 정보를 추출할 수 있다. 나아가, 제어부는 제2 영상(601, 602, 603)에 대응되는 대상물의 자유도 정보를 획득할 수 있음은 물 론이다. 제어부는 카메라의 자유도 정보에 기반하여, 대상물의 자유도 정보를 획득하거나, 제2 영상으로부터 대상물의 자유도 정보를 획득할 수 있다. 제어부는 제2 기준 좌표계(W2)와 카메라 제2 카메라 좌표계(C2)간의 상대적인 위치 관계에 근거하여, 카메 라의 자유도 정보로부터 대상물의 자유도 정보를 산출하거나, 이와 반대로 대상물의 자유도 정보로부터 카메라의 자유도 정보를 산출할 수 있다. 이는, 대상물의 제2 기준 좌표계(W2)와 카메라의 기준 좌표계는 서로 상대적인 위치 관계를 갖기 때문이다. 예를 들어, 대상물의 제2 기준 좌표계(W2)에 대한 대상물의 자유도 정보에 역변환을 수행하는 경우, 대상물의 제2 기준 좌표계(W2)에 대한 카메라의 자유도 정보가 얻어질 수 있다. 이와 같이, 제어부는, 카메라와 대상물에 각각 포함된 제2 카메라 좌표계(C2) 와 제2 기준 좌표 계(W2) 간의 상대적인 위치 관계에 근거하여 다양한 정보를 연산 및 산출하고, 이러한 정보를 저장부에 저 장할 수 있다. 다양한 정보는, i) 대상물의 제2 기준 좌표계(W2)를 기준으로 하는 자유도 정보, ii) 대상물이 포함 된 제2 영상을 촬영한 카메라의 제2 카메라 좌표계(C2)에 대한 자유도 정보, iii) 제2 기준 좌표계(W2)과 제2 카메라 좌표계(C2) 간의 상대적인 위치관계 정보, iv) 제2 영상을 촬영한 카메라의 제2 기준 좌표계 (W2)에 대한 자유도 정보,2 v) 제2 영상에 대응되는 대상물의 제2 카메라 좌표계(C2)에 대한 자유도 정보, vii) 제2 영상에서의 대상물에 대응되는 그래픽 객체의 픽셀 좌표에 대한 정보 중 적어도 하나를 포함할 수 있다. 한편, S410 내지 S430 단계를 거쳐, 복수의 제1 영상 및 제2 영상이 획득되고, 대상물 및 카메라와 관련된 자유도 정보가 획득된 경우, 본 발명에서는 이를 활용하여 대상물(또는 물체)에 대한 학습 데이터를 생 성하는 과정이 진행될 수 있다(S440). 보다 구체적으로 제어부는 카메라의 자유도 정보와 복수의 제1 영상에 포함된 3차원 모델링 객체를 이용하여, 대상물(또는 물체)물체에 대한 학습 데이터를 생성할 수 있다. 이러한 학습 데이터에는 복수의 제1 영상에 포함된 3차원 모델링 객체 각각에 대한 마스크(MASK) 및 상기 마스 크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보가 포함될 수 있다. 이때, 3차원 모델링 객체 (600, 도 2 참조)의 자세와 관련된 자유도 정보는, 제2 공간에 대한 자유도 정보로서, 제2 기준 좌표계 (W2) 또는 제2 카메라 좌표계(C2)에 대한 정보일 수 있다. 본 발명에서 제어부는 제2 공간에 대한 3차원 모델링 객체의 자유도 정보를 획득하기 위하여, 제1 공간(또는 모델링 공간, 가상 공간)의 제1 기준 좌표계(W1)에 대하여, 제2 공간(400, 현실 환경 또는 실제 공간)에 배치된 카메라의 카메라 좌표계(C2) 간의 상대적인 위치 관계(800, 도 8 참조( ))를 이용할 수 있다. 도 8에 도시된 “T”는 homogeneous transformation matrix를 의미할 수 있다. 이를 위하여, 제어부는 도 7에 도시된 것과 같이, 복수의 제1 영상(3차원 모델링 객체에 대한 영상)으로부 터, 제2 영상(카메라를 통해 촬영된 영상, 720)에 포함된 그래픽 객체(대상물에 대응되는 이미지 객체, 300a)에 대응되는 대상물(300, 도 5의 (a) 참고)의 자세와 유사한 자세를 갖는 특정 3차원 모델링 객체가 포함된 제1 영상을 특정할 수 있다. 제어부는, 저장부에 저장된 복수의 제1 영상 및 제2 영상들 중 대상물에 대하여 상호 가장 유사한 자 세를 갖는 제1 영상 및 제2 영상을 각각 특정할 수 있다. 이때, 제어부는 도 7에 도시된 것과 같이, 로컬 피쳐 매칭(Local Feature Matching)을 수행하여, 제1 영상 및 제2 영상를 각각 특정할 수 있다. 이와 같이, 제1 및 제2 영상(710, 720)이 특정되면, 제어부는 제1 공간(또는 모델링 공간, 가상 공간)의 제1 기준 좌표계(W1)에 대하여, 제2 공간(400, 현실 환경 또는 실제 공간)에 배치된 카메라의 제2 카메라 좌표계(C2) 간의 상대적인 위치 관계(800, 도 8 참조( ))를 추출하기 위하여, 특정된 제1 및 제2 영상 (710, 720)을 이용할 수 있다. 제어부는 상기 상대적인 위치 관계(800, ( ))를 추출하기 위하여, 특정된 제1 영상에 포함된 상 기 3차원 모델링 객체와 특정된 제2 영상에서 대상물에 대응되는 그래픽 객체(300a) 간의 관계성을 이용할 수 있다. 한편, 상대적인 위치 관계(800, ( ))는, 제1 공간의 제1 기준 좌표계(W1)가 카메라의 제2 카메라 좌 표계(C2)에 대하여 회전(rotation) 및 변환(translation)된 정도를 의미할 수 있다. 즉, 상대적인 위치 관계 (800, ( ))는, 제2 카메라 좌표계(C2)에 대한 제1 공간의 제1 기준 좌표계(W1)의 상대적인 위치 관계를 의 미할 수 있다. 보다 구체적으로, 카메라의 제2 카메라 좌표계(C2)가 제1 공간의 제1 기준 좌표계(W1)에 대하여 회전 (rotation) 및 변환(translation)된 정도는, 상기 특정된 제1 영상에 포함된 3차원 모델링 객체와 제2 영상에서 대상물(또는 물체)에 대응되는 그래픽 객체(300a) 간의 관계성에 근거하여 특정될 수 있다. 제어부는 도 7 및 도 8에 도시된 것과 같이, 특정된 제2 영상에서의 그래픽 객체(300a)의 픽셀 좌표 (u, v(도 8의 도면부호 810참조)) 및 특정된 제1 영상에서의 3차원 모델링 객체의 3차원 좌표(x, y, z(도 8의 도면부호 820 참조))를 이용하여, 제2 카메라 좌표계(C2)가 제1 공간의 제1 기준 좌표계(W1)에 대하여 회전(rotation) 및 변환(translation)된 정도를 추출할 수 있다. 이러한 관계는, 제1 영상 및 제2 영상이 각각 대상물의 특정 자세에 대하여 상호 가장 유사한 3차원 모델링 객체 및 그래픽 객체(300a)로 이루어졌음을 이용한 것이다. 제어부는 도 8의 (a)에 도시된 것과 같이, i)3차원 모델링 객체에 해당하는 PnP방정식에 3차원 모델 링 객체의 3차원 좌표를 입력으로 넣고, ii) 제2 카메라 좌표계(C2)에 대하여 제1 공간의 제1 기준 좌표계(W1)가 회전 및 변환된 정도를 정의한 상대적인 위치 관계(800, ( ))에 대응되는 매트릭스(행렬, 830)을 적용할 수 있다. 그리고, 제어부는 iii)카메라에 의해 촬영된 대상물에 해당하는 그래픽 객체 (300a)의 픽셀 좌표가 결과값으로 도출됨을 이용하여, iv)제1 공간의 제1 기준 좌표계(W1)에 대하여 제2 카메라 좌표계(C2)가 회전 및 변환된 정도를 정의한 행렬(매트릭스, 830)를 추출할 수 있다. 이때, 특정된 제2 영상에서의 그래픽 객체(300a)의 픽셀 좌표(u, v(도 8의 도면부호 810참조)) 및 특정된 제1 영상에서의 3차원 모델링 객체의 3차원 좌표(x, y, z(도 8의 도면부호 820 참조))는 제어부(13 0)에서 이미 알고 있는 값에 해당하므로, 제어부는 제2 카메라 좌표계(C2)가 제1 공간의 제1 기준 좌표계 (W1)에 대하여 회전 및 변환된 정도를 정의한 상대적인 위치 관계(800, (( ))에 해당하는 매트릭스(행렬, 830)를 추출할 수 있다. 한편, 이러한 PnP방정식에는, intrinsic parameter가 적용되며, 이는 카메라의 특성을 나타내는 카메 라 고유 파라미터에 해당할 수 있다. 한편, 도 8의 (a)에 도시된 것과 같이, PnP 방정식에 대한 파라미터에 대하여 설명하면, “S”는 스케일 상수 (ex: 1), “r“은 skew parameter(일종의 왜곡 보정 상수에 해당), “t1”, “t2”, “t3”은 회전 및 변환에 대한 행렬 및 벡터를 의미할 수 있다. 한편, 위와 같이, PnP방정식을 통하여, 제2 카메라 좌표계(C2)에 대한 제1 기준 좌표계(W1)의 상대적인 위치 관 계(800, 도 8 참조( ))가 도출되면, 제어부는 이를 이용하여, 도 8의 (b)에 도시된 것과 같이, 제1 기준 좌표계(W1)와 제2 기준 좌표계(W2)간의 상대적인 위치 관계(801, ( ))를 추출할 수 있다. 제어부는 제1 기준 좌표계(W1)에 대하여 제2 기준 좌표계(W2)가 회전 및 변환된 정도 또는 제2 기준 좌표 계(W2)에 대하여 제1 기준 좌표계(W1)가 회전 및 변환된 정보를 추출할 수 있다.도 8의 (b)에서 제어부는 제1 기준 좌표계(W1)에 대하여 제2 기준 좌표계(W2)가 변환된 정도(801, ( ))추출할 수 있다. 제1 기준 좌표계(W1)에 대하여 제2 기준 좌표계(W2)가 변환된 정도(801, ( ))는, “제1 기준 좌표계(W1)에 대한 제2 기준 좌표계(W2)의 상대적인 위치관계”라고도 설명될 수 있다. 한편, 도 8의 (b)에 도시된 것과같이, 제1 기준 좌표계(W1)에 대한 제2 기준 좌표계(W2)의 상대적인 위치관계 (801, ( ))는, i)제1 기준 좌표계(W1)에 대한 제2 카메라 좌표계(C2) 간의 상대적인 위치 관계(802, ( ))와 ii)제2 카메라 좌표계(C2)와 제2 기준 좌표계 간의 상대적인 위치 관계(803, ( ))의 곱으로 구해질 수 있다. 제1 기준 좌표계(W1)에 대한 제2 카메라 좌표계(C2) 간의 상대적인 위치 관계(802, ( ))는 위에서 도 8의 (a)와 함께 살펴본. 제2 카메라 좌표계(C2)에 대한 제1 기준 좌표계(W1)의 상대적인 위치 관계(800, 도 8 참조 ( ))의 역행렬(또는 역변환, ( ), 804)을 통하여 얻어질 수 있다. 나아가, 제2 카메 라 좌표계(C2)와 제2 기준 좌표계(W2) 간의 상대적인 위치 관계(803, ( ))는, 저장부에 기 확보된 정보에 해당할 수 있다. 따라서, 제어부는 위의 관계를 이용하여, 도 8의 (b)에 도시된 것과 같이, 제1 기준 좌표계(W1)에 대한 제 2 기준 좌표계(W2)의 상대적인 위치관계(801, ( ))를 얻을 수 있다. 이와 같이, 제어부는 위에서 살펴본 상대적인 위치관계 추정을 통하여, i) 제1 기준 좌표계(W1)에 대한 제 2 기준 좌표계(W2)의 상대적인 위치관계(801,806, ( ) 를 얻었으면, 이러한 위치관계(801, 806)와 ii) 제 2 기준 좌표계(W2)에 대한 제2 카메라 좌표계(C2)의 상대적인 위치 관계(807, ( ))의 곱을 이용하여, 제1 기준 좌표계(W1)에 포함된 3차원 모델링 객체에 대한 제2 카메라 좌표계(C2)의 자유도 정보를 추출할 수 있다. 도 8의 (a) 및 (b)는 도 8의 (C)에 도시된, 제1 기준 좌표계(W1)에 포함된 3차원 모델링 객체에 대한 제2 카메 라 좌표계(C2)의 관계성(또는 상대적인 위치 관계, 805, ( ))를 도출하기 위한 과정으로서, 도면부호 805 에 따른 관계성은, 특정된 제1 및 제2 영상뿐만 아니라, 임의의 자세를 갖는 3차원 모델링 객체에 대한 임의의 제1 영상에도 적용될 수 있다. 도 8의 (C)에 도시된, 제1 기준 좌표계(W1)에 포함된 3차원 모델링 객체에 대한 제2 카메라 좌표계(C2)의 관계 성(805, ( ))은, 앞서 도 8의 (a) 및 (b)의 관계식을 통하여 도출된, i) 제1 기준 좌표계(W1)에 대한 제2 기준 좌표계(W2)의 상대적인 위치관계(801, ( ), 806)와 ii) 제2 기준 좌표계(W2)에 대한 제2 카메라 좌표 계(C2)의 상대적인 위치 관계의 곱을 통하여, 도출될 수 있다. 한편, 본 발명에서 설명되는 800, 801, 802, 803, 804, 805, 806, 807의 도면부호를 붙여서 설명하는 상대적인 위치관계는, homogeneous transformation matrix를 의미하는 것으로 이해되어 질 수 있다. 이러한 homogeneoustransformation matrix를 구성하는 요소들의 값은, 좌표계 간의 상대적인 위치 관계(회전 및 변환의 정도), 제1 영상에 포함된 3차원 모델링 객체의 자유도 정보, 제2 영상에 포함된 물체에 해당하는 그래픽 객체의 자유도 정 보, 제2 영상을 촬영한 카메라의 자유도 정보 중 적어도 하나가 활용될 수 있다. 한편, 제어부는 제1 기준 좌표계(W1)에 포함된 3차원 모델링 객체에 대한 제2 카메라 좌표계(C2)의 관계 성(또는 상대적인 위치 관계, 805, ( ))을 이용하여, 서로 다른 자세를 갖는 3차원 모델링 객체를 포함한 복수의 제1 영상에 적용하여, 복수의 제1 영상에 각각 포함된 3차원 모델링 객체에 대한 카메라의 제2 카 메라 좌표계(C2)의 자유도 정보를 추출할 수 있다. 즉, 제어부는 3차원 모델링 객체를 실제 공간(제2 공간)에 배치된 카메라를 통해 촬영하였을 경우의 카메라의 자유도 정보를 추출할 수 있다. 이때, 추출되는 자유도 정보의 기준 좌표계는 제2 카메라 좌표계(C2)에 대한 것이거나, 제2 기준 좌표계(W2)에 대한 것일 수 있다. 한편, 제어부는 저장부에 제2 기준 좌표계(W2) 및 제2 카메라(C2) 좌표계에 대한 상대적인 위치관계 에 대한 정보를 가지고 있으므로, 경우에 따라 필요한 형태의 좌표계에 대한 자유도 정보를 추출할 수 있다. 이를 통하여, 제어부는 3차원 모델링 객체가, 실제 공간에 놓여졌을 경우에, 카메라 또는 3차원 모델링 객 체에 대한 자유도 정보를 추출할 수 있다. 이와 같이 추출된 자유도 정보는, 학습 데이터에 포함될 수 있다. 이러한 학습 데이터에는 도 9에 도시된 것과 같이, 복수의 제1 영상(911, 921, 931)에 포함된 3차원 모델링 객체(911’, 921’, 931’) 각각에 대응되는 마 스크(MASK) 및 상기 마스크(910’, 920’, 930’)에 각각 매칭된 3차원 모델링 객체(911’, 921’, 931’)의 자세와 관련된 자유도 정보가 포함될 수 있다. 여기에서, 마스크는, 실제 공간에 대해 촬영된 제2 영상(910, 920, 930)에 3차원 모델링 객체((910’, 920’, 930’)가 합성(또는 투영)된 이미지이거나, 3차원 모델링 객체 자체에 대한 이미지일 수 있다. 나아가, 도 10에 도시된 것과 같이, 학습 데이터상에는, 마스크에 대한 정보 및 마스크에 각각 매칭된 3 차원 모델링 객체의 자세와 관련된 자유도 정보가 포함될 수 있다. 한편, 마스크에 각각 매칭된 3차원 모델링 객체의 자세와 관련된 자유도 정보는 제2 기준 좌표계(W2)에 대한 대상물 또는 카메라의 자유도 정보 이거나, 제2 카메라 좌표계(C2)에 대한 대상물 또는 카메라의 자유도 정보일 수 있다. 제어부 3차원 모델링 객체를 포함하는 복수의 영상을 이용하여, 실제 공간에서 활용될 수 있는 대상 물 또는 카메라의 자유도 정보를 획득할 수 있다. 한편, 3차원 모델링 객체를 이용하여 생성할 수 있 는 데이터의 양은 수만~ 수천만장 이상으로 매우 방대하므로, 본 발명에 의할 경우, 학습에 필요한 충분한 양의 학습 데이터를 생성할 수 있다. 한편, 도 11에 도시된 것과 같이, 위에서 살펴본 3차원 모델링 객체를 활용한 학습 데이터는, reference 이미지 데이터 셋(set)을 형성할 수 있다. 3차원 모델링 객체를 활용하여 학습 데이터를 활용하는 방법은 도 2의 설명으로 대체하도록 한다. 이러한 데이터 셋에는, i)3차원 모델링 객체에 대한 이미지(또는 모델링 이미지, 렌더링 이미지), ii)3차 원 모델링 객체에 대한 깊이 맵(depthmap), iii)3차원 모델링 객체를 렌더링한 카메라의 자유도 정보(제2 기준 좌표계(W2)에서의 카메라의 자유도 정보 또는 제2 카메라 좌표계(C2)에서의 카메라의 자유도 정보일 수 있음), iv)카메라의 Intrinsic parameters 중 적어도 두개가 상호 매칭되어 존재할 수 있다. 상기 데이터 셋은 앞서 살펴본 방식을 통하여 얻어질 수 있다. 제어부는 이러한 데이터 셋을 이용하여, 실제 환경에서 카메라(미도시됨)를 통해 센싱되는 대상물에 대한 자유도 정보를 추출할 수 있다. 도 12에 도시된 것과 같이, 제어부는 카메라(미도시됨)으로부터 대상물(예를 들어, 컵(CUP)을 촬영한 촬영 영상(또는 입력 영상)이 수신(S1210)되면, 입력 영상에 포함된 상기 대상물에 대응되는 그래픽 객체(1211 ’)와 데이터 셋 포함된 3차원 모델링 객체에 대한 이미지들 간의 비교를 수행할 수 있다. 제어부는 Global feature를 기반으로, 데이터 셋 포함된 3차원 모델링 객체에 대한 이미지들 중 입 력 영상에 포함된 그래픽 객체(1211’)와 가장 유사한 자세를 갖는 특정 이미지를 검색할 수 있다 (S1220). 제어부는 입력 영상에서의 그래픽 객체(1211’)에 해당하는 대상물을 바라보는 카메라의 방향(Orientation)의 차이에 근거하여, 상기 특정 이미지를 검색할 수 있다. 그리고, 제어부는 입력 영상과 특정 이미지 간의 local feature matching을 수행(S1230)하여, 입력 영상과 특정 이미지간의 매칭 포인트(매칭 점)를 추출할 수 있다. 입력 영 상과 특정 이미지 각각에 대응되는 매칭 포인트는 한 쌍을 이룰 수 있다. 제어부는 RGB 기반의 local feature matching을 수행할 수 있으며, 입력 영상과 특정 이미지 간의 key point들의 descriptor를 이용하여, 매칭 포인트를 추출할 수 있다. 한편, 의에서 살펴본 global feature matching 및 local feature matching은 Dual Feature Network에서 수행될 수 있다. Dual Feature Network는 이미지 검색(Image retrieval) 시 사용되는 global feature와 대상물의 자세 추정(pose estimation) 시 사용되는 local feature를 동시에 추출하는 deep neural network 일 수 있다. 제어부는 위의 매칭 포인트의 수가 임계치(기준 값)을 초과하는 경우(또는 만족하는 경우), 매칭 포인트들 을 이용하여, 입력 영상에 포함된 그래픽 객체(1211’)에 대응되는 대상물 또는 이를 촬영한 카메라의 자 세를 추정할 수 있다(S1240). 한편, 제어부는 위의 매칭 포인트의 수가 임계치(기준 값)을 초과하지 않는 경우, 입력 영상에 포함 된 그래픽 객체(1211’)에 대응되는 대상물 또는 이를 촬영한 카메라의 자세에 대한 추정은 이루어지지 않을 수 있다. 자제 추정 과정에 대하여 구체적으로 살펴보면, 제어부는 입력 영상에 포함된 그래픽 객체(1211’) 와 3차원 모델링 객체(1221’) 간의 관계성에 기초하여, 대상물의 자유도 자세를 추출할 수 있다. 보다 구체적으로, 제어부는 도 8의 (a)에서 살펴본, 3차원 좌표 정보에 데이터 셋에 포함되며, 상기 특정 이미지의 매칭 포인트에 해당하는 좌표 정보를 입력하고, 2차원 좌표 정보(810, 또는 픽셀 좌 표)에 입력 영상의 매칭 포인트에 해당하는 좌표 정보를 입력함으로써, PnP방정식(알고리즘)을 풀어, 대 상물 자체 또는 대상물을 촬영하는 카메라의 자유도 정보를 추정할 수 있다. PnP방정식(알고리즘)을 푸는 과정 에 대해서는, 도 8에 대한 설명으로 대체하도록 한다. 한편, 데이터 셋에는 특정 이미지에 포함된 3차원 모델링 객체의 자유도 정보가 존재하므로, 제어 부는 해당 자유도 정보에 대하여, 위에서 살펴본 매칭 포인트들 간의 상대적인 위치 관계를 반영함으로써, 입력 영상에 포함된 그래픽 객체(1211’)에 대응되는 대상물의 자유도 정보를 추출할 수 있다. 이때의 자유도 정보는, 대상물 자체 또는 대상물을 촬영하는 카메라의 자유도 정보일 수 있다. 위에서 살펴본 것과 같이, 본 발명에 따른 학습 데이터 수집 시스템 및 방법은, 대상물에 대한 3차원 모델링 객 체를 생성하고, 촬영된 영상에 포함된 대상물에 해당하는 그래픽 객체로부터 생성된 3차원 모델링 객체 간의 관 계성을 이용하여, 3차원 모델링 객체의 자유도 정보를 추출할 수 있다. 이를 통해, 본 발명은, 3차원 모델링 객체를 실제 환경에서 촬영된 영상에 반영함으로써, 실제 환경에서의 조명, 그림자 등이 반영된 학습 데이터를 생성할 수 있다. 결과적으로, 본 발명에 의하면, 보다 실제 환경에 가 까운 학습 데이터를 수집하는 것이 가능하다. 한편, 위에서 살펴본 본 발명은, 컴퓨터에서 하나 이상의 프로세스에 의하여 실행되며, 이러한 컴퓨터로 판독될 수 있는 매체(또는 기록 매체)에 저장 가능한 프로그램으로서 구현될 수 있다. 나아가, 위에서 살펴본 본 발명은, 프로그램이 기록된 매체에 컴퓨터가 읽을 수 있는 코드 또는 명령어로서 구 현하는 것이 가능하다. 즉, 본 발명은 프로그램의 형태로 제공될 수 있다. 한편, 컴퓨터가 읽을 수 있는 매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록장치를 포함한다. 컴퓨터가 읽을 수 있는 매체의 예로는, HDD(Hard Disk Drive), SSD(Solid State Disk), SDD(Silicon Disk Drive), ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 나아가, 컴퓨터가 읽을 수 있는 매체는, 저장소를 포함하며 전자기기가 통신을 통하여 접근할 수 있는 서버 또 는 클라우드 저장소일 수 있다. 이 경우, 컴퓨터는 유선 또는 무선 통신을 통하여, 서버 또는 클라우드 저장소 로부터 본 발명에 따른 프로그램을 다운로드 받을 수 있다.나아가, 본 발명에서는 위에서 설명한 컴퓨터는 프로세서, 즉 CPU(Central Processing Unit, 중앙처리장치)가 탑재된 전자기기로서, 그 종류에 대하여 특별한 한정을 두지 않는다. 한편, 상기의 상세한 설명은 모든 면에서 제한적으로 해석되어서는 아니되고 예시적인 것으로 고려되어야 한다. 본 발명의 범위는 첨부된 청구항의 합리적 해석에 의해 결정되어야 하고, 본 발명의 등가적 범위 내에서의 모든 변경은 본 발명의 범위에 포함된다."}
{"patent_id": "10-2021-0020419", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따라 수집된 학습 데이터가 활용되는 예를 설명하기 위한 개념도이다. 도 2는 3차원 모델링 객체를 생성하는 방법을 설명하기 위한 개념도이다. 도 3은 본 발명에 따른 학습 데이터 수집 시스템을 설명하기 위한 개념도이다. 도 4는 본 발명에 따른 학습 데이터 수집 방법을 설명하기 위한 흐름도이다. 도 5, 도 6, 도 7, 도 8, 도 9 및 도 10은 학습 데이터를 수집하는 방법을 설명하기 위한 개념도들이다. 도 11 및 도 12는 수집된 학습 데이터를 활용하는 방법을 설명하기 위한 개념도들이다."}
