{"patent_id": "10-2024-0130342", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0057634", "출원번호": "10-2024-0130342", "발명의 명칭": "AI 모델 배포 방법 및 그 시스템", "출원인": "리벨리온 주식회사", "발명자": "김기정"}}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "인공지능(Artificial Intelligence, AI) 모델 배포 시스템에 있어서,업데이트된 AI 모델 파일 전송을 위한 멀티캐스트 IP(internet protocol) 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성하여 복수의 서버들로 전송하고, 상기 복수의 서버들로부터 멀티캐스트 IP 세팅 완료 메시지 및전송속도 제한 완료 메시지를 수신하고, 상기 업데이트된 AI 모델 파일을 상기 복수의 서버들로 멀티캐스트 전송 기반으로 전송하는 배포 컨트롤러(deployment controller); 및 상기 배포 컨트롤러로부터 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보를 수신하고,상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 상기 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP주소의 세팅을 수행하고, 상기 전송속도 제한 요청 정보를 기반으로 상기 멀티캐스트 IP 주소에 전송속도를 할당하고, 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 생성하여 상기 배포 컨트롤러로 전송하고, 상기 멀티캐스트 IP 주소를 기반으로 상기 업데이트된 AI 모델 파일을 수신하고, 상기 업데이트된 AI 모델 파일을 기반으로 AI 모델 파일 교체를 수행하는 복수의 서버들을 포함하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정도 중 적어도 하나는 유니캐스트 기반으로 전송되고, 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지 중 적어도 하나는 상기 유니캐스트 기반으로 전송되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 기반으로 전송되는 것을 특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 전송속도 제한 정보는 AI 서비스를 위한 유니캐스트 IP 주소에 대한 전송속도보다 상기 업데이트된 모델배포를 위한 멀티캐스트 IP 주소에 대한 전송속도를 더 높게 설정함을 지시하는 것을 특징으로 하는, AI 모델배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,로드 밸런싱 장치에서의 전송속도에 대한 전송속도 정보를 생성하고, 상기 전송속도 정보를 배포 컨트롤러로 전송하는 상기 로드 밸런싱 장치를 더 포함하고,상기 로드 밸런싱 장치에서의 전송속도는 인바운드 또는 아웃바운드 중 적어도 하나에 대한 전송속도를 포함하고,상기 배포 컨트롤러는 상기 전송속도 정보를 기반으로 상기 전송속도 제한 요청 정보를 생성하는 것을 특징으로하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서로드 밸런싱 장치에서의 전송속도에 대한 전송속도 정보를 생성하고, 상기 전송속도 정보를 배포 컨트롤러로 전송하는 상기 로드 밸런싱 장치를 더 포함하고,상기 로드 밸런싱 장치에서의 전송속도는 인바운드 또는 아웃바운드 중 적어도 하나에 대한 전송속도를 포함하고,공개특허 10-2025-0057634-3-상기 배포 컨트롤러는 상기 수신한 전송속도 정보를 기반으로 상기 멀티캐스트 IP 주소에 할당된 전송속도의 조정을 지시하는 전송속도 제어 정보를 생성하고, 상기 전송속도 제어 정보를 상기 복수의 서버들로 전송하고,상기 복수의 서버들은 상기 전송속도 제어 정보를 기반으로 상기 멀티캐스트 IP 주소에 할당된 전송속도를 조정하고,상기 전송속도 제한 요청 정보가 지시하는 전송속도와 상기 전송속도 제어 정보가 지시하는 전송속도는 다른 것을 특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서상기 배포 컨트롤러는 상기 전송속도 정보를 기반으로 상기 로드 밸런싱 장치에서의 전송속도를 도출하고,제1 임계값 또는 제2 임계값 중 적어도 하나와 상기 로드 밸런싱 장치에서의 전송속도와의 비교를 기반으로 상기 전송속도 제어 정보를 생성하고,상기 제1 임계값 또는 상기 제2 임계값 중 적어도 하나는 AI 서비스에 대한 서비스 IP 주소에 할당된 전송속도를 기반으로 도출되는 것을 특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 복수의 서버들은 제1 AI 모델 배포 그룹에 속하는 제1 서버들을 포함하고,상기 제1 서버들에 할당되는 제1 멀티캐스트 IP 주소는 제2 AI 모델 배포 그룹에 속하는 제2 서버들에 할당되는제2 멀티캐스트 IP 주소와 다른 것을 특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 배포 컨트롤러는 복수의 AI 모델 배포 그룹을 구성하고,상기 복수의 AI 모델 배포 그룹은 AI 서비스, 모델 종류, AI 가속기 중 적어도 하나를 기반으로 결정되는 것을특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "인공지능(Artificial Intelligence, AI) 모델 배포를 위한 배포 컨트롤러에서 수행되는 방법에 있어서,업데이트된 AI 모델 파일 전송을 위한 멀티캐스트 IP(internet protocol) 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성하는 단계;상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보를 복수의 서버들로 전송하는 단계;상기 복수의 서버들로부터 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 수신하는 단계; 및상기 업데이트된 AI 모델 파일을 상기 복수의 서버들로 전송하는 단계를 포함하되,상기 업데이트된 AI 모델 파일은 멀티캐스트 전송 기반으로 전송되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 멀티캐스트 IP 주소및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할당되는 전송속도를 기반으로 전송되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서,상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정도 중 적어도 하나는 유니캐스트 기반으로 전송되고, 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지 중 적어도 하나는 상기 유니캐스공개특허 10-2025-0057634-4-트 기반으로 수신되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 기반으로 전송되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서,상기 전송속도 제한 정보는 AI 서비스를 위한 유니캐스트 IP 주소에 대한 전송속도보다 상기 업데이트된 모델배포를 위한 멀티캐스트 IP 주소에 대한 전송속도를 더 높게 설정함을 지시하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서,로드 밸런싱 장치에서의 전송속도에 대한 전송속도 정보를 수신하는 단계를 더 포함하고,상기 로드 밸런싱 장치에서의 전송속도는 인바운드 또는 아웃바운드 중 적어도 하나에 대한 전송속도를 포함하고,상기 전송속도 제한 요청 정보는 상기 전송속도 정보를 기반으로 생성되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서,로드 밸런싱 장치에서의 전송속도에 대한 전송속도 정보를 수신하는 단계;상기 수신한 전송속도 정보를 기반으로 상기 멀티캐스트 IP 주소에 할당된 전송속도의 조정을 지시하는 전송속도 제어 정보를 생성하는 단계;상기 전송속도 제어 정보를 상기 복수의 서버들로 전송하는 단계를 더 포함하고,상기 로드 밸런싱 장치에서의 전송속도는 인바운드 또는 아웃바운드 중 적어도 하나에 대한 전송속도를 포함하고,상기 전송속도 제한 요청 정보가 지시하는 전송속도와 상기 전송속도 제어 정보가 지시하는 전송속도는 다른 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 전송속도 정보를 기반으로 상기 로드 밸런싱 장치에서의 전송속도가 도출되고,제1 임계값 또는 제2 임계값 중 적어도 하나와 상기 로드 밸런싱 장치에서의 전송속도와의 비교를 기반으로 상기 전송속도 제어 정보가 생성되고,상기 제1 임계값 또는 상기 제2 임계값 중 적어도 하나는 AI 서비스에 대한 서비스 IP 주소에 할당된 전송속도를 기반으로 도출되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제9항에 있어서,복수의 AI 모델 배포 그룹을 구성하는 단계를 더 포함하되,상기 복수의 AI 모델 배포 그룹은 AI 서비스, 모델 종류, AI 가속기 중 적어도 하나를 기반으로 결정되는 것을특징으로 하는, AI 모델 배포 시스템."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "인공지능(Artificial Intelligence, AI) 모델 배포를 지원하는 서버에서 수행되는 방법에 있어서,멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 수신하는 단계;공개특허 10-2025-0057634-5-상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP 주소의 세팅을 수행하는 단계;상기 전송속도 제한 요청 정보를 기반으로 상기 멀티캐스트 IP 주소에 전송속도를 할당하는 단계;멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 생성하여 전송하는 단계;업데이트된 AI 모델 파일을 수신하는 단계를 포함하고,상기 업데이트된 AI 모델 파일은 멀티캐스트 전송 기반으로 수신되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP주소 및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할당되는 상기 전송속도를 기반으로 수신되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정도 중 적어도 하나는 유니캐스트 기반으로 수신되고, 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지 중 적어도 하나는 상기 유니캐스트 기반으로 전송되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 기반으로 수신되는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제16항에 있어서,상기 전송속도 제한 정보는 AI 서비스를 위한 유니캐스트 IP 주소에 대한 전송속도보다 상기 업데이트된 모델배포를 위한 멀티캐스트 IP 주소에 대한 전송속도를 더 높게 설정함을 지시하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제16항에 있어서,상기 배포 컨트롤러로부터 상기 멀티캐스트 IP 주소에 할당된 전송속도의 조정을 지시하는 전송속도 제어 정보를 수신하는 단계; 및 상기 전송속도 제어 정보를 기반으로 상기 멀티캐스트 IP 주소에 할당된 전송속도를 조정하는 단계를 더 포함하고,상기 전송속도 제한 요청 정보가 지시하는 전송속도와 상기 전송속도 제어 정보가 지시하는 전송속도는 다른 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제16항에 있어서,상기 서버는 상기 업데이트된 AI 모델 파일을 기반으로, 상기 서버의 AI 모델 파일 교체를 수행하거나, 또는 상기 서버에 구비된 하나 이상의 가상 머신 또는 하나 이상의 컨테이너에 대한 상기 AI 모델 파일 교체를 수행하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 실시예에 따른 인공지능(Artificial Intelligence, AI) 모델 배포 시스템은, 업데이트된 AI 모델 파 일 전송을 위한 멀티캐스트 IP(internet protocol) 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성하여 복수 의 서버들로 전송하고, 업데이트된 AI 모델 파일을 상기 복수의 서버들로 멀티캐스트 전송 기반으로 전송하는 배 포 컨트롤러(deployment controller), 및 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 상기 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP 주소의 세팅을 수행하고, 상기 전송속도 제한 요청 정보를 기반으로 상기 멀티캐스트 IP 주소에 전송속도를 할당하고, 상기 멀티캐스트 IP 주소를 기반으로 상기 업데이트된 AI 모델 파일을 수신하고, 상기 업데이트된 AI 모델 파일을 기반으로 AI 모델 파일 교체를 수행하는 복수의 서버들을 포 함할 수 있다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 AI(artificial intelligence) 서비스에 관한 것으로, AI 모델의 배포 방법 및 그 시스템에 관한 것이다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "딥러닝과 빅데이터에 대한 수요가 증가하면서 서버 및 클라이언트 간 트래픽보다는 서버들 간의 트래픽이 급증 하고 있다. 이와 같은 서버들 간의 트래픽을 수용하기 위하여 AI 데이터센터에서는 높은 대역폭의 네트워크에 대한 요구사항이 증가하고 있으며, 높은 대역폭의 네트워크 인프라가 확대/증설되고 있다. 한편, AI 서비스를 고객에게 제공하기 위하여 많은 수의 서버들과 AI 가속기들이 필요하고, 최적의 AI 서비스를 제공하기 위하여는 AI 모델의 업데이트가 지속적으로 수행되어야 한다. 현재 보통의 AI 모델의 데이터량은 수십 GB 수준이며, AI 모델의 발전과 함께 AI 모델의 데이터 크기가 증가하고 있기에, AI 모델의 크기는 조만간 TB 수준 혹은 그 이상에 도달할 것으로 예상된다. 이 경우, 네트워크 용량이 증설되는 속도보다 AI 모델의 크기가 커지는 속도가 더 빠를 수 있으며, 유니캐스트 기반으로 AI 모델을 업데이트하는 경우 원활한 AI 모델의 배포가 어려운 문제가 있다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 일 예에서는 AI 모델 배포 방법, 장치 및 시스템을 제공하는 것을 과제로 한다 본 개시의 일 예에서는 네트워크 부하를 줄이면서 AI 모델을 배포하는 것을 과제로 한다. 본 개시의 일 예에서는 안정적으로 AI 모델을 배포하는 것을 과제로 한다. 본 개시의 일 예에서는 다수의 서버에 동시에 AI 모델을 배포하는 것을 과제로 한다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따르면, 인공지능(Artificial Intelligence, AI) 모델 배포 시스템을 제공한다. 상기 시 스템은 업데이트된 AI 모델 파일 전송을 위한 멀티캐스트 IP(internet protocol) 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성하여 복수의 서버들로 전송하고, 상기 복수의 서버들로부터 멀티캐스트 IP 세팅 완료 메 시지 및 전송속도 제한 완료 메시지를 수신하고, 상기 업데이트된 AI 모델 파일을 상기 복수의 서버들로 멀티캐 스트 전송 기반으로 전송하는 배포 컨트롤러(deployment controller), 및 상기 배포 컨트롤러로부터 상기 멀티 캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보를 수신하고, 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 상기 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP주소의 세팅을 수행하고, 상기 전송속도 제한 요청 정보를 기반으로 상기 멀티캐스트 IP 주소에 전송속도를 할당하고, 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 생성하여 상기 배포 컨트롤러로 전송하고, 상기 멀티캐스트 IP 주소를 기반으 로 상기 업데이트된 AI 모델 파일을 수신하고, 상기 업데이트된 AI 모델 파일을 기반으로 AI 모델 파일 교체를 수행하는 복수의 서버들을 포함한다. 본 개시의 다른 일 실시예에 따르면, 인공지능(Artificial Intelligence, AI) 모델 배포를 위한 배포 컨트롤러 에서 수행되는 방법을 제공한다. 상기 방법은 업데이트된 AI 모델 파일 전송을 위한 멀티캐스트 IP(internet protocol) 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성하는 단계, 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보를 복수의 서버들로 전송하는 단계, 상기 복수의 서버들로부터 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 수신하는 단계, 및 상기 업데이트된 AI 모델 파일을 상기 복 수의 서버들로 전송하는 단계를 포함하되, 상기 업데이트된 AI 모델 파일은 멀티캐스트 전송 기반으로 전송되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 멀티캐스 트 IP 주소 및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할당되는 전송속 도를 기반으로 전송되는 것을 특징으로 한다. 본 개시의 또 다른 일 실시예에 따르면, 인공지능(Artificial Intelligence, AI) 모델 배포를 지원하는 서버에 서 수행되는 방법을 제공한다. 상기 방법은 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 수신 하는 단계, 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스 트 IP 주소의 세팅을 수행하는 단계, 상기 전송속도 제한 요청 정보를 기반으로 상기 멀티캐스트 IP 주소에 전 송속도를 할당하는 단계, 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 생성하여 전송하는 단계, 업데이트된 AI 모델 파일을 수신하는 단계를 포함하고, 상기 업데이트된 AI 모델 파일은 멀티캐스트 전송 기반으로 수신되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소 및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할 당되는 상기 전송속도를 기반으로 수신되는 것을 특징으로 한다. 본 개시의 또 다른 일 실시예에 따르면, 인공지능(Artificial Intelligence, AI) 모델 배포를 위한 배포 컨트롤 러를 제공한다. 본 개시의 또 다른 일 실시예에 따르면, 인공지능(Artificial Intelligence, AI) 모델 배포를 지원하는 서버를 제공한다. 본 개시의 또 다른 일 실시예에 따르면, 본 문서에서 개시된 실시예에 따른 방법을 수행하기 위한 프로그램이 저장된 컴퓨터 판독 가능한 저장매체를 제공할 수 있다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일 예에 따르면, 네트워크 부하를 줄이면서 AI 모델을 배포할 수 있다. 본 개시의 일 예에 따르면, 안정적으로 AI 모델을 배포할 수 있다. 본 개시의 일 예에 따르면, 다수의 서버에 동시에 AI 모델을 배포할 수 있다. 본 개시의 일 예에 따르면, 멀티캐스트 방식으로 AI 모델을 전송하여 전송 효율을 높일 수 있다. 본 개시의 일 예에 따르면, AI 서비스를 위한 유니캐스트 IP 주소에는 낮은 전송속도를 할당하고, AI 모델 배포 를 위한 멀티캐스트 IP 주소에는 높은 전송속도를 할당하여 지속적으로 AI 모델 배포를 원활하게 하면서도 AI 서비스가 가능한 상태를 유지할 수 있다. 또한, 본 개시의 일 예에 다르면 AI 서비스의 중단 없이 AI 모델의 배포를 완료할 수 있다. 또한, 본 개시의 일 예에 따르면, API 게이트웨이 또는 로드밸런서의 전송속도를 기반으로 AI 모델의 배포 속도 를 조절할 수 있다. 또한, 본 개시의 일 예에 따르면, 클라우드 내 가상 머신 또는 컨테이너는 호스트 서버가 멀티캐스트로 수신한 AI 모델 파일을 한번에 공유받아 AI 모델을 업데이트할 수 있다. 이를 통하여 복수의 가상 머신 혹은 복수의 컨 테이너들에 대한 AI 모델 교체를 위한 데이터 전송량을 상당히 줄일 수 있다. 또한, 본 개시의 일 예에 따르면, 서비스 IP 주소와 배포 IP 주소(멀티캐스트 IP)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, 복수의 가상 머신 혹은 복수의 컨테이너들에 AI 모델 배포를 위한 IP 접속 정보 관리의 복잡도를 낮출 수 있다."}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시(disclosure)는 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도 면에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 실시예들을 특정 실시예에 한정하려고 하는 것이 아니다. 본 개시에서 상용하는 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 개시의 기 술적 사상을 한정하려는 의도로 사용되는 것은 아니다. 본 개시에서 사용되는 단수형식은 문맥이 명확하게 다르 게 나타내지 않는 한 복수형식도 포함하도록 의도된다. 본 개시에서 사용되는 용어 \"및/또는\"은 관련 목록 항목 중 어느 하나 또는 둘 이상의 조합을 포함한다. 본 명세서에서 사용되는 용어 \"포함한다\", \"구성한다\" 및 \"보유 한다\"는 명시된 특징, 숫자, 동작, 요소, 구성요소 및/또는 이들의 조합의 존재를 명시하지만, 하나 이상의 다 른 특징, 숫자, 동작, 요소, 구성요소 및/또는 이들의 조합의 존재 또는 추가를 배제하지 않는다. 본 개시에서 예시 또는 실시예(예를 들어, 예시 또는 실시예가 무엇을 포함하거나 구현할 수 있는지에 대하여)와 관련하여 \"할 수 있다\"라는 용어를 사용하는 것은 이러한 특징이 포함되거나 구현되는 적어도 하나의 예시 또는 실시예 가 존재함을 의미하지만, 모든 예가 이에 한정되는 것은 아니며 해당 특징 또는 구성이 생략될 수도 있다. 한편, 본 개시에서 설명되는 도면 상의 각 구성들은 서로 다른 특징적인 기능들에 관한 설명의 편의를 위해 독 립적으로 도시된 것으로서, 각 구성들이 서로 별개의 하드웨어나 별개의 소프트웨어로 구현된다는 것을 의미하 지는 않는다. 예컨대, 각 구성 중 두 개 이상의 구성이 합쳐져 하나의 구성을 이룰 수도 있고, 하나의 구성이 복수의 구성으로 나뉘어질 수도 있다. 각 구성이 통합 및/또는 분리된 실시예도 본 개시의 본질에서 벗어나지 않는 한 본 개시의 권리범위에 포함된다. 본 개시에서 \"A 또는 B(A or B)\"는 \"오직 A\", \"오직 B\" 또는 \"A와 B 모두\"를 의미할 수 있다. 달리 표현하면, 본 개시에서 \"A 또는 B(A or B)\"는 \"A 및/또는 B(A and/or B)\"으로 해석될 수 있다. 예를 들어, 본 개시에서 \"A, B 또는 C(A, B or C)\"는 \"오직 A\", \"오직 B\", \"오직 C\", 또는 \"A, B 및 C의 임의의 모든 조합(any combination of A, B and C)\"를 의미할 수 있다. 본 개시에서 사용되는 슬래쉬(/)나 쉼표(comma)는 \"및/또는(and/or)\"을 의미할 수 있다. 예를 들어, \"A/B\"는 \"A 및/또는 B\"를 의미할 수 있다. 이에 따라 \"A/B\"는 \"오직 A\", \"오직 B\", 또는 \"A와 B 모두\"를 의미할 수 있다. 예를 들어, \"A, B, C\"는 \"A, B 또는 C\"를 의미할 수 있다. 본 개시에서 \"적어도 하나의 A 및 B(at least one of A and B)\"는, \"오직 A\", \"오직 B\" 또는 \"A와 B 모두\"를 의미할 수 있다. 또한, 본 개시에서 \"적어도 하나의 A 또는 B(at least one of A or B)\"나 \"적어도 하나의 A 및/또는 B(at least one of A and/or B)\"라는 표현은 \"적어도 하나의 A 및 B(at least one of A and B)\"와 동 일하게 해석될 수 있다. 또한, 본 개시에서 \"적어도 하나의 A, B 및 C(at least one of A, B and C)\"는, \"오직 A\", \"오직 B\", \"오직 C\", 또는 \"A, B 및 C의 임의의 모든 조합(any combination of A, B and C)\"를 의미할 수 있다. 또한, \"적어도 하나의 A, B 또는 C(at least one of A, B or C)\"나 \"적어도 하나의 A, B 및/또는 C(at least one of A, B and/or C)\"는 \"적어도 하나의 A, B 및 C(at least one of A, B and C)\"를 의미할 수 있다. 또한, 본 개시에서 사용되는 괄호는 \"예를 들어(for example)\"를 의미할 수 있다. 구체적으로, \"프로세싱 유닛 (NPU)\"로 표시된 경우, \"프로세싱 유닛\"의 일례로 \"NPU\"이 제안된 것일 수 있다. 달리 표현하면 본 개시의 \"프 로세싱 유닛\"은 \"NPU\"으로 제한(limit)되지 않고, \"NPU\"가 \"프로세싱 유닛\"의 일례로 제안될 것일 수 있다. 또 한, \"프로세싱 유닛(즉, NPU)\"으로 표시된 경우에도, \"프로세싱 유닛\"의 일례로 \"NPU\"가 제안된 것일 수 있다. 본 개시에서 하나의 도면 내에서 개별적으로 설명되는 기술적 특징은, 개별적으로 구현될 수도 있고, 동시에 구 현될 수도 있다. 이하, 첨부한 도면들을 참조하여, 본 개시의 실시예를 보다 상세하게 설명하고자 한다. 이하, 도면 상의 동일한 구성 요소에 대해서는 동일한 참조 부호를 사용할 수 있고, 동일한 구성 요소에 대해서 중복된 설명은 생략될 수 있다. 도 1 및 2는 서비스 배포(deployment) 방법의 예를 나타낸다. 여기서 도 1은 블루/그린 배포의 예를 나타내고, 도 2는 카나리 배포의 예를 나타낸다.도 1을 참조하면, 블루/그린 배포는 레드/블랙, A/B 배포라고도 한다. 이는 DB 서버의 다중화 리플리케이션(비 동기), 클러스트링(동기)과 비슷한 개념이라고 볼 수 있다. 서비스에 대하여 두개의 환경을 가지고 있고, 둘 중 하나에 새로운 버전을 순차적으로 반영할 수 있다. 즉, 하나의 환경에서 기존 버전을 통한 서비스를 제공하면서, 다른 환경에서 새로운 버전으로 업데이트하고, 기능 및 성능 테스트 후 새로운 버전의 환경으로 라 우팅할 수 있다. 즉, 구버전에서 신버전으로 한번에 변경할 수 있다. 도 2를 참조하면, 카나리 배포는 단계적으로 전환하는 방식을 사용한다. 카나리 배포에서는 일부분(ex. 일부 서 버)에 새로운 버전을 배포한 후에 특정 사용자들에게만 새로운 버전으로 접근되도록 라우팅할 수 있다. 테스트 후 새로운 버전을 모두 반영할 수 있다. 상기와 같은 서비스 배포 방법은 서비스 전환에 관한 것으로, 제한된 네트워크 환경에서의 AI 모델의 배포 속도 를 더 빠르게 및/또는 더 효율적으로 하는 방안을 제시하지는 못하고 있다. 또한, 과거에는 네트워크의 불안정성과 용량의 한계로 인하여, 데이터 전송과정에서 에러가 발생할 수 있기 때 문에 신뢰성을 보장하는 TCP(transmission control protocol)를 대중적인 통신 프로토콜로 활용하였다. 하지만, 최근에는 네트워크 기술과 설비의 수준이 높아지면서 안정성(신뢰성)보다는 레이턴시를 감소시키거나 대역폭을 높이는 방법에 대한 관심이 증가하고 있다. 예를 들어, HTTP/3는 아직 전세계적으로 8% 정도의 사용률을 보이지 만, UDP(user data protocol) 기반 전송 기술인 QUIC(quick UDP internet connections)를 채택하면서 기존의 SPDY 혹은 HTTP/2 프로토콜보다 높은 전송 효율을 보이고 있다. 더 나아가 데이터센터에서는 딥러닝과 빅데이터 를 중심으로 높은 수준의 네트워크 성능이 요구되고 있으며, 최근 100GB 네트워크의 사용이 점점 보편화되어 가 고 있지만, 이와 같은 대용량 네트워크를 효율적으로 활용하는 프로토콜은 제안되지 않는 상황이다. 한편, 최적의 AI 서비스를 제공하기 위하여는 AI 모델의 업데이트가 지속적으로 수행되어야 하며, 네트워크 성 능의 향상에도 불구하고, AI 모델의 업데이트를 위한 대용량의 데이터를 다수의 서버에 배포하는 것은 많은 시 간이 소요된다. 예를 들어, AI 모델의 업데이트를 위한 파일은 compiled_model 파일을 포함할 수 있으며, 이러 한 AI 모델의 업데이트를 위한 대용량의 파일의 전송을 효율적으로 지원할 수 있는 전송 프로토콜 또는 전송 방 법이 필요하다. 따라서, 본 개시의 일 예에서는, 상기와 같은 상황을 고려하여, 모델을 빠르고 효율적으로 배포하는 방안에 대 하여 제공한다. 본 개시는 예를 들어, 데이터센터에서 대용량 네트워크를 사용하는 경우에 대하여 적용될 수 있 으며, 그 외의 다양한 상황에도 적용될 수도 있다. 본 개시에서 AI 모델을 기반으로 설명하였으나, 이는 예시로 서 AI 모델 외의 다양한 서비스에 관련된 모델에도 본 개시가 적용될 수 있다. 본 개시에서 모델과 AI 모델은 혼용될 수 있고, AI 모델은 다른 서비스에 관련된 모델로 대체될 수 있다. 본 개시에서 AI 모델의 업데이트를 위한 파일로 compiled_model 파일을 기재하였으며, 다만 이는 예시로서 compiled_model 파일은 동일/유사한 기 능을 하는 다른 명칭의 파일들을 포함할 수 있다. 도 3은 본 개시에 따른 AI 서비스 시스템의 예를 나타낸다. 도 3을 참조하면 AI 서비스 시스템은 디바이스(device, 300), 로드 밸런싱 장치(load balancing apparatus, 310), 스위치(switch, 320), 빌드 시스템(build system, 330), 배포 컨트롤러(deployment controller, 340), 복수의 서버(server, 350)를 포함할 수 있다. 빌드 시스템 및 배포 컨트롤러은 개별 장치로 구성될 수 있고 또는 통합되어 구성될 수도 있다. 이하 마찬가지이다. 디바이스는 사용자로부터 인풋을 수신할 수 있다. 상기 인풋은 인풋 정보, 인풋 데이터, 인풋 메시지 등과 혼용될 수 있다. 디바이스는 유저 디바이스 또는 단말을 포함할 수 있다. 상기 인풋은 사용자, 센서, 혹은 다른 디바이스를 통하여 수신할 수 있다. 이는, 다른 실시예에서도 마찬가지이다. 디바이스는 인터넷 등의 네트워크를 통하여 로드 밸런싱 장치와 연결될 수 있다. 디바이스는 인 터넷을 통하여 상기 로드 밸런싱 장치로 AI 서비스에 대한 추론 작업을 요청할 수 있다. 즉, 예를 들어, 디바이스는 상기 로드 밸런싱 장치로 상기 AI 서비스에 대한 추론 작업 요청 메시지를 전달할 수 있 다. 예를 들어, 상기 추론 작업 요청 메시지는 URL 형식일 수 있다. 상기 URL 형식은 프로토콜(protocol) 식별 자, 호스트 주소(host address) 패스(Path) 및/또는 쿼리(query) 등으로 구성될 수 있다. 로드 밸런싱 장치는 복수의 서버들 중 적어도 하나로 상기 AI 서비스에 대한 추론 작업에 대한 로드 밸런싱을 수행할 수 있다. 즉, 로드 밸런싱 장치는 상기 복수의 서버로 상기 AI 서비스에 대한 추론 작업을 분배할 수 있다. 예를 들어, 상기 로드 밸런싱은 기설정된 로드 밸런싱 알고리즘을 기반으로 로드 밸런싱을 수행할 수 있다. 예를 들어, 상기 기설정된 로드 밸런싱 알고리즘은 라운드 로빈(round robin) 알고리즘, 스틱키 라운드 로빈(sticky round robin) 알고리즘, 가중 라운드 로빈(weighted round robin) 알고리즘, IP/URL 해쉬(IP/URL hash) 알고리즘, 최소 커넥션(least connection) 알고리즘 또는 최소 시간(least time) 알 고리즘을 포함할 수 있다. 상기 로드 밸런싱 장치는 로드 밸런서 또는 API 게이트웨이를 포함할 수 있다. API 게이트웨이는 URL 기반으로 라우팅 또는 포워딩을 수행할 수 있다. 상기 복수의 서버는 AI 서비스를 제공하기 위해 인공신경망을 이용하여 연산을 수행하는 뉴럴 프로세싱 장 치(Neural Processing Unit, NPU)를 포함한 AI 가속기를 포함할 수 있다. 상기 서버는 백엔드 서버(backend server)라고 불릴 수도 있다. 스위치는 경로 제어를 수행한다. 스위치는 AI 서비스 제공을 위하여 로드 밸런싱 장치와 복수의 서버들 중 적어도 하나를 연결할 수도 있고, AI 모델 배포를 위하여 빌드 시스템, 배포 컨트롤러 중 적어도 하나와 복수의 서버들 중 일부 또는 전부를 연결할 수도 있다. 또한, 스위치는 후술 하는 바와 같이 로드 밸런싱 장치에서의 전송속도 정보 수집 등을 위하여 로드 밸런싱 장치와 배포 컨트롤러를 연결할 수도 있다. 빌드 시스템은 AI 모델을 관리한다. 빌드 시스템은 AI 모델에 관한 학습, 소스코드 수정/빌드, AI 모 델 컴파일, 테스트 등을 수행할 수 있다. 빌드 시스템은 배포 컨트롤러를 통하여 (업데이트된) AI 모 델을 복수의 서버로 배포할 수 있다. 배포 컨트롤러는 AI 모델 배포 설정 및 배포를 수행할 수 있다. AI 모델 배포 설정을 위하여, AI 모델 배 포 그룹, AI 배포 서버 IP 주소, 그룹별 멀티캐스트 IP 주소, QoS 정보 등을 세팅할 수 있다. 배포 컨트롤러 는 빌드 시스템으로부터 (업데이트된) AI 모델을 수신하여 복수의 서버로 배포할 수 있다. 본 개시의 실시예에 따르면, 기존에 AI 모델을 유니캐스트 방식으로 전송하던 것과 다르게, 멀티캐스트 방식으 로 AI 모델을 전송하여 전송 효율을 높일 수 있다. 구체적으로 예를 들어, 서비스 IP(백엔드 서버의 IP 주소)에 는 낮은 전송속도를 할당하고, 배포 IP 주소(백엔드 서버의 멀티캐스트 IP 주소)에는 높은 전송속도를 할당하여 지속적으로 AI 서비스가 가능한 상태를 유지할 수 있다. 이 경우, 멀티캐스트를 이용하여 대용량의 AI 모델 파 일(ex. compiled_model 파일)을 고효율/초고속으로 배포하고, 배포 완료 후 전송속도 제한을 해제하여 서비스의 중단 없이 AI 모델 배포를 완료할 수 있다. 또한, AI 모델 파일의 전송 효율을 극대화하기 위하여 로드 밸런싱 장치의 인입 트래픽 정보를 기반으로 전송속도를 적응적(adaptive)으로 조정할 수도 있다. 이 경우, AI 모델 배포 전후의 전송속도를 제어함으로써 UDP 전송 등에 따른 에러 발생을 최소화할 수 있다. 또 한, 기존에는 AI 모델 배포 과정만 추적/관리할 수 있었던 것에 비하여, AI 모델 배포와 서비스에 관한 전과정 을 추적 및 관리할 수 있다. 또한, 본 개시의 실시예에 따르면, 서비스 IP 주소와 배포 IP 주소(멀티캐스트 I P)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, AI 모델 배포를 위한 IP 접속 정 보 관리의 복잡도를 낮출 수 있다. 또한, 로드 밸런싱 장치에 인입되는 서비스 트래픽의 수준을 고려하여 동적 으로 서비스/배포에 관한 전송속도를 조절함으로써 AI 서비스를 지속적으로 제공하면서도 AI 모델 배포를 효율 적으로 수행할 수도 있다. 도 4는 본 개시의 실시예에 따른 빌드 시스템 및 배포 컨트롤러 간 AI 모델 배포 준비 과정을 예시적으로 나타 낸다. 도 4에서 수행된 방법은 예를 들어, 후술하는 도 5, 6, 7, 12, 13, 14 등에 개시된 방법을 위하여 먼저 또는 병렬적으로 수행될 수 있다. 도 4를 참조하면, 배포 컨트롤러는 AI 모델 배포 구성(configuration)을 수행한다(S400). 배포 컨트롤러는 AI 모델 배포 구성을 위하여, AI 모델 배포 구성 테이블을 생성/관리할 수 있다. 상기 AI 모델 배포 구성 테이블을 통하여 AI 모델 배포 그룹, AI 배포 서버 IP 주소, 그룹별 멀티캐스트 IP 주소, 속도 제한 정보, QoS 정보 등을 세팅할 수 있다. 다음 표는 상기 AI 모델 배포 구성 테이블을 예시적으로 나타낸다. 표 1"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "표 1에서는 AI 모델 배포 그룹이 2개인 경우를 예로 나타내었으며, AI 모델 배포 그룹 별로 다른 멀티캐스트 IP 주소 및 다른 속도 제한 정보가 세팅될 수 있다. 빌드 시스템은 AI 모델 관리를 수행한다(S410). 빌드 시스템은 AI 모델 관리를 위하여, AI 모델에 관한 학습, 소스코드 수정/빌드, AI 모델 컴파일, 테스트 등을 수행할 수 있다. 빌드 시스템은 AI 모델 배포 요청 정보를 배포 컨트롤러로 전송한다(S420). 빌드 시스템은 AI 모델 관리를 통하 여 업데이트된 AI 모델의 배포가 필요하다고 판단된 경우, 상기 AI 모델 배포 요청 정보를 전송할 수 있다. 상 기 AI 모델 배포 요청 정보는 AI 모델에 대한 모델이름, 모델버전, 모델경로, 배포그룹 중 적어도 하나를 포함 할 수 있다. 한편, S400은 S420보다 먼저 수행되는 것으로 도식되었으나 이는 예시이며 S400은 S420 이후에 수행되거나 동시 에 수행될 수도 있다. 배포 컨트롤러는 AI 모델 파일 요청 정보를 빌드 시스템으로 전송한다(S430). 상기 AI 모델 파일 요청 정보는 AI 모델 배포 승낙 정보라고 불릴 수도 있다. 빌드 시스템은 AI 모델 파일 요청 정보를 기반으로 AI 모델 파일을 배포 컨트롤러로 전송한다(S440). 상기 AI 모델 파일은 업데이트된 AI 모델 파일로 불릴 수 있다. 배포 컨트롤러는 상기 수신한 AI 모델 파일을 본 개시에서 제안된 방법에 따라서 하나 이상의 서버로 배포할 수 있다. 배포 컨트롤러는 AI 모델 배포를 위하여 예를 들어, 다음과 같은 AI 모델 배포 관리 테이블을 생성/관리 할 수 있다. 표 2"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "상기 그룹 번호(Group No.) 및 그룹 이름(Group name)은 AI 모델 배포 그룹을 나타내며, AI 모델 배포 그룹은 서비스, 모델 종류, AI 가속기(하드웨어) 중 적어도 하나에 따라 다르게 결정될 수 있다. 상기 그룹 배포 상태 (Group deploy status)는 그룹 단위의 AI 모델 배포 상태를 나타내고, 서버 배포 상태(Server deploy status) 는 그룹 내 서버 단위의 AI 모델 배포 상태를 나타낸다. 즉, 그룹 내 모든 서버의 AI 모델 배포가 완료된 경우 에 그룹 배포 상태가 완료(Complete)로 표시될 수 있다.한편, 상기 표 2는 복수의 서버가 베어메탈 클라우드 서비스에 기반하는 경우에 관한 것일 수 있으며, 후술하는 가상머신(Virtual machine, VM) 및/또는 컨테이너(container) 방식의 클라우드 서비스가 적용되는 경우, 상기 AI 모델 배포 관리 테이블은 예를 들어 다음과 같이 구성될 수 있다. 표 3"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "표 4"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "표 5"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "표 6 표 3은 복수의 서버가 가상머신 방식에 기반하는 경우, 표 4는 복수의 서버가 컨테이너 방식에 기반하는 경우를 나타낸다. 표 5 및 6은 복수의 서버들 중 일부가 가상머신에 기반하고, 나머지 일부가 컨테이너 방식에 기반하 는 경우를 나타낸다. 표 5는 가상머신 방식에 기반하는 서버와 컨테이너 방식에 기반하는 서버를 다른 그룹으로 그룹핑한 경우를 나타내고, 표 6은 가상머신 방식에 기반하는 서버와 컨테이너 방식에 기반하는 서버가 하나의 그룹에서 혼용되는 경우를 나타낸다. 도 5는 본 개시의 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 5는 예를 들어 복수의 서버 가 베어메탈 구조에 기반하는 경우에 관한 것일 수 있다. 도 5를 참조하면, 배포 컨트롤러는 IP 주소 식별 절차 및 전송속도 제한 결정 절차를 수행한다(S510). IP 주소 식별 절차에서 배포 컨트롤러는 요청받은 배포 그룹의 서버 IP 주소와 배포그룹의 멀티캐스트 IP 주소를 식별할 수 있다. 또한, 전송속도 제한 결정 절차에서, 배포 컨트롤러는 서버의 서비스 IP와 멀티캐스트 IP에 할당할 전 송속도를 결정할 수 있다. 전송속도 제한은 AI 모델 배포를 위한 멀티캐스트 IP에는 높은 전송속도를 할당하고, 서비스 IP에는 낮은 전송속도를 할당하는 것을 포함할 수 있다. 즉, 전송속도 제한이 적용되는 경우, AI 모델 배포를 위한 멀티캐스트 IP에 할당되는 전송속도가 AI 서비스를 위한 유니캐스트 IP에 할당되는 전송속도보다 높게 설정될 수 있다. 배포 컨트롤러는 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 복수의 서버로 전송한다(S515). 상기 복수의 서버는 상술한 AI 모델 배포 그룹 내의 서버들일 수 있다. 즉, 배포 컨트롤러는 상술한 AI 모델 배 포 그룹에 대한 정보, AI 모델 배포 서버 IP 주소에 대한 정보, 그룹별 멀티캐스트 IP 주소에 대한 정보, 속도 제한 정보, QoS 정보 중 적어도 하나를 기반으로 상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정 보를 상기 복수의 서버로 전송할 수 있다. 예를 들어 이 과정은 유니캐스트 방식으로 수행될 수 있다. 복수의 서버들은 멀티캐스트 IP 세팅 및 전송속도 제한 설정 절차를 수행한다(S520). 복수의 서버들은 상기 멀 티캐스트 IP 세팅 요청 정보를 기반으로 멀티캐스트 IP 세팅을 수행할 수 있다. 복수의 서버들은 상기 전송속도 제한 요청 정보를 기반으로 상기 전송속도 제한 설정 절차를 수행할 수 있다. 복수의 서버들은 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 배포 컨트롤러로 전송한다 (S525). 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지는 별도의 메시지 형식으로 구성될 수도 있고, 하나의 메시지 형식으로 구성될 수도 있다. 비록 도면에서는 멀티캐스트 IP 세팅을 위한 일련의 절차와 전송속도 제한을 위한 일련의 절차가 동시에 수행되 는 것으로 개시되었으나(ex. S510 내지 S525), 이는 예시이며, 멀티캐스트 IP 세팅을 위한 일련의 절차와 전송 속도 제한을 위한 일련의 절차는 별도로 혹은 순차적으로 수행될 수도 있다. 이하 다른 실시예들에서도 마찬가 지이다. 예를 들어, 멀티캐스트 IP 세팅 중인 경우(예를 들어, 배포 컨트롤러가 멀티캐스트 IP 세팅 요청 정보 전송 후 멀티캐스트 IP 세팅 완료 메시지 수신 전), 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 7"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "또한, 예를 들어, 멀티캐스트 IP 세팅이 완료된 경우(예를 들어, 멀티캐스트 그룹 내 모든 서버에서 멀티캐스트 IP 세팅 완료 메시지 수신 후)에 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다.표 8"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "또한, 예를 들어, 전송속도 제한 설정 중인 경우(예를 들어, 배포 컨트롤러가 전송속도 제한 요청 정보 전송 후 전송속도 제한 완료 메시지 수신 전), 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 9"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "또한, 예를 들어, 전송속도 제한 설정이 완료된 경우(예를 들어, 멀티캐스트 그룹 내 모든 서버에서 전송속도 제한 완료 메시지 수신 후)에 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 10"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "배포 컨트롤러는 AI 모델 멀티캐스트 전송 설정을 수행하고(S530), AI 모델을 복수의 서버들로 멀티캐스트 전송 한다(S535). 상기 AI 모델은 상술한 AI 모델 파일을 포함한다. 예를 들어, AI 모델 멀티캐스트 전송 중인 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 11"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "복수의 서버들은 AI 모델 수신 완료 메시지를 생성하여 배포 컨트롤러로 전송한다(S545). 예를 들어 상기 AI 모 델 멀티캐스트 전송 외의 전송 과정은 유니캐스트 방식으로 수행될 수 있다. 예를 들어, AI 모델 멀티캐스트 전송이 완료된 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 12"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "배포 컨트롤러는 전송속도 제한 해제를 결정한다(S550). 배포 컨트롤러는 AI 모델 멀티캐스트 전송이 완료된 경 우 상기 전송속도 제한 해제를 결정할 수 있다. 배포 컨트롤러는 전송속도 제한 해제 요청 정보를 복수의 서버들로 전송한다(S555). 복수의 서버들은 전송속도 제한 해제 설정을 수행한다(S560). 복수의 서버들은 전송속도 제한 해제 요청 정보를 기반으로 상기 전송속도 제한 해제 설정을 할 수 있다. 전송속도 제한 해제라 함은 전송속도 제한을 없애는 것 뿐 아니라 전송속도 제한을 초기값 혹은 디폴트 값으로 전환하는 것을 포함할 수 있다. 예를 들어, 전송속도 제한 해제 중인 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 13"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 13, "content": "복수의 서버들은 전송속도 제한 해제 완료 메시지를 배포 컨트롤러로 전송한다(S565). 예를 들어, 전송속도 제한 해제가 완료된 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 14"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 14, "content": "복수의 서버들은 AI 모델 교체를 수행한다(S570). 복수의 서버들은 정해진 AI 모델 교체 알고리즘에 따라 AI 모 델 교체를 수행한다. 비록 도시되지는 않았으나 AI 모델 교체 시간을 지시하는 정보를 배포 컨트롤러로부터 수 신할 수도 있다. 예를 들어, AI 모델 교체 작업 중인 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 15"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 15, "content": "비록 S570은 S560 및 S565 후에 수행되는 것으로 도시되었으나 이는 예시로서, S570은 S560 혹은 S565 전에 수 행될 수도 있다. 복수의 서버들은 AI 모델 교체 완료 메시지를 배포 컨트롤러로 전송한다(S575). 배포 컨트롤러는 AI 모델 배포 상태 업데이트를 수행한다(S580). 배포 컨트롤러는 상기 AI 모델 교체 완료 메시 지를 기반으로 상기 AI 모델 배포 상태 업데이트를 수행할 수 있다. 상술한 바와 같이 그룹 내 모든 서버의 AI 모델 배포가 완료된 경우에 그룹 배포 상태가 완료(Complete)로 표시될 수 있다. 예를 들어, AI 모델 교체가 완료된 경우, 상기 AI 모델 배포 관리 테이블은 다음과 같이 구성될 수 있다. 표 16"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 16, "content": "표 17"}
{"patent_id": "10-2024-0130342", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 17, "content": "표 17은 그룹 내 모든 서버의 AI 모델 배포가 완료되어 그룹 배포 상태가 완료(Complete)로 표시된 경우를 나타 낸다.상술한 본 개시의 실시예에 따르면, AI 모델 교체 전까지는 기존 모델에 기반한 AI 서비스 이용이 가능하고, 이 후에는 신규 모델에 기반한 AI 서비스 이용이 가능하다. 또한, 상술한 본 개시의 실시예에 따르면, 기존에 AI 모델을 유니캐스트 방식으로 전송하던 것과 다르게, 멀티 캐스트 방식으로 AI 모델을 전송하여 전송 효율을 높일 수 있다. 구체적으로 예를 들어, AI 서비스를 위한 유니 캐스트 IP 주소에는 낮은 전송속도를 할당하고, AI 모델 배포를 위한 멀티캐스트 IP 주소에는 높은 전송속도를 할당하여 지속적으로 AI 서비스가 가능한 상태를 유지할 수 있다. 이 경우, 멀티캐스트를 이용하여 대용량의 AI 모델 파일을 고효율/초고속으로 배포하고, 배포 완료 후 전송속도 제한을 해제하여 서비스의 중단 없이 AI 모델 배포를 완료할 수 있다. 상술한 실시예에 따르면, AI 모델 배포 전후의 전송속도를 제어함으로써 UDP 전송에 따른 에러 발생을 최소화할 수 있다. 또한, 기존에는 AI 모델 배포 과정만 추적/관리할 수 있었던 것에 비하여, AI 모델 배포와 서비스에 관한 전과정을 추적 및 관리할 수 있다. 또한, 본 개시의 실시예에 따르면, 서비스 IP 주소와 배포 IP 주소(멀 티캐스트 IP)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, AI 모델 배포를 위한 IP 접속 정보 관리의 복잡도를 낮출 수 있다. 도 6은 본 개시의 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 6에서 개시된 실시예에 따르면 로드 밸런싱 장치에서의 전송속도를 기반으로 전송속도 제한 결정을 수행할 수 있다. 도 6을 참조하면, 일반적인 절차의 경우 도 5에서 설명한 절차와 동일하며, 차이가 있는 부분을 위주로 설명한 다. 로드 밸런싱 장치는 로드 밸런싱 장치에서의 전송속도를 측정하고, 전송속도 정보를 생성하여 배포 컨트롤러로 전송한다(S600). 상기 전송속도 정보는 로드 밸런싱 장치에서의 인바운드/아웃바운드 전송속도에 관한 정보를 포함하 수 있다. 배포 컨트롤러는 전송속도 제한 결정을 수행함에 있어서 상기 로드 밸런싱 장치에서부터 수신한 전송속도에 관 한 정보를 기반할 수 있다. 상기 실시예에 따르면, AI 모델 파일의 전송 효율을 극대화하기 위하여 로드 밸런싱 장치의 인입 트래픽 정보를 기반으로 전송속도를 조정할 수 있다. 이를 통하여 AI 서비스에 대한 영향을 최소화하면서 AI 모델 배포 속도를 효율적으로 높일 수 있다. 도 7은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 7에서 개시된 실시 예에 따르면 로드 밸런싱 장치에서의 전송속도를 기반으로 AI 모델 전송 과정에서 적응적으로 전송속도를 제어 할 수 있다. 도 7을 참조하면, 일반적인 절차의 경우 도 5에서 설명한 절차와 동일하며, 차이가 있는 부분을 위주로 설명한 다. 로드 밸런싱 장치는 로드 밸런싱 장치에서의 전송속도를 측정하고, 전송속도 정보를 생성하여 배포 컨트롤러로 전송한다(S700). 상기 전송속도 정보는 로드 밸런싱 장치에서의 인바운드/아웃바운드 전송속도에 관한 주기적/ 비주기적 정보를 포함하 수 있다. 예를 들어, 상기 전송속도 정보는 주기적(ex. 10ms)으로 전송될 수 있다. 또 는 상기 전송속도 정보는 특정 트리거링 조건을 기반으로 전송될 수 있다. 예를 들어, AI 서비스를 위한 패킷의 인바운드/아웃바운드 전송속도가 일정 값보다 큰 경우, 또는 일정 값보다 작은 경우 전송될 수 있다. 또는, AI 서비스를 위한 패킷의 인바운드/아웃바운드 전송속도 이전에 측정한 전송속도와 일정 이상 차이가 나는 경우 전 송될 수 있다. 배포 컨트롤러는 S730에서 전송속도 제어를 수행할 수 있다. 즉, 배포 컨트롤러는 S735에서 전송속도 제어 정보 를 전송하여 서버들에서 멀티캐스트 IP 주소로 AI 모델을 수신하는 속도를 제어할 수 있다. S700, S730, S735는 반복적으로 수행될 수 있다. 예를 들어, 이 경우 업데이트된 AI 모델 파일의 일부는 제1 전 송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되도록 제어될 수 있다. 상기 제1 전송속도는 상기 전송속 도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 상기 전송속도 제어 정보가 지시하는 전송속도일 수 있다. 다른 예로, 업데이트된 AI 모델 파일의 일부는 제1 전송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되고, 나머지 일부는 제3 전송속도로 전송되도록 제어될 수 있다. 예를 들어, 상기 제1 전송속 도는 상기 전송속도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 제1 전송속도 제어 정보가 지시하는 전송속도일 수 있고, 상기 제3 전송속도는 제2 전송속도 제어 정보가 지시하는 전송속도일 수있다. 상술한 전송속도 제한 및/또는 (적응적) 전송속도 제어는 예를 들어 다음과 같은 방법을 기반으로 수행될 수 있 다. 도 8은 본 개시의 상술한 실시예들에 적용될 수 있는 전송속도 제한 및/또는 전송속도 제어 방법을 예시적으로 나타낸다. 도 8을 참조하면, 배포 컨트롤러는 로드 밸런싱 장치의 인바운드/아웃바운드 전송속도 정보를 확인한다(S800). 상기 전송속도 정보는 상술한 바와 같이 주기적/비주기적으로 전송될 수도 있다. 배포 컨트롤러는 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 임계값보다 큰지 여부를 판단한다(S810). 즉, 상기 전송속도 정보에 포함된 인바운드 전송속도 및/또는 아웃바운드 전송속도가 임계값보다 큰지 여부를 판단한다. 상기 임계값은 미리 결정될 수도 있고, 사용자 인터페이스, 미리 정의된 알고리즘 또는 AI 추론을 기반으로 결 정될 수 있다. 예를 들어, 상기 임계값은 서비스 IP 주소에 대한 전송속도 제한 값일 수 있다. 또는 상기 임계 값은 서비스 IP 주소에 대한 전송속도 제한 값을 기반으로 도출될 수 있다. 예를 들어, 상기 임계값은 서비스 IP 주소에 대한 전송속도 제한 값에 소정의 오프셋을 더한 값을 기반으로 계산될 수 있다. S810에서 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 임계값보다 큰 경우에는, 배포 컨트롤러는 멀티 캐스트 IP 주소의 전송속도를 하향 조정되도록 전송속도 제어 정보를 생성한다. 이 경우 서비스 IP 주소소의 전 송속도는 상향 조정될 수 있다. S810에서 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 임계값보다 크지 않은 경우에는, 배포 컨트롤러 는 멀티캐스트 IP 주소의 전송속도를 유지하거나 상향 조정할 수 있다. 이 경우 서비스 IP 주소의 전송속도는 유지되거나 하향 조정될 수 있다. 또한, 예를 들어, S810에서 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도를 제1 임계값 및 제2 임계값과 비교할 수도 있다. 예를 들어, 상기 제1 임계값 및 상기 제2 임계값은 미리 결정될 수도 있고, 서비스 IP 주소 에 대한 전송속도 제한 값을 기반으로 도출될 수 있다. 예를 들어, 상기 제1 임계값은 서비스 IP 주소에 대한 전송속도 제한 값을 기반으로 도출되고, 상기 제2 임계값은 상기 전송속도 제한 값에 소정의 오프셋을 더한 값 을 기반으로 계산될 수 있다. 또는 예를 들어, 상기 제1 임계값은 서비스 IP 주소에 대한 전송속도 제한 값에 제1 오프셋을 뺀 값을 기반으로 도출되고, 상기 제2 임계값은 상기 전송속도 제한 값에 제2 오프셋을 더한 값을 기반으로 계산될 수 있다. 상기 제1 오프셋은 제2 오프셋과 동일할 수도 있고 다를 수도 있다. 또는, 상기 임계값(혹은 제1 임계값 및/또는 상기 제2 임계값)은 대역폭 용량(bandwidth capacity)을 기반으로 도출될 수도 있다. 만약, 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 제1 임계값보다 작은 경우, AI 모델 배포를 위한 멀 티캐스트 IP 주소의 전송속도를 상향 조정할 수 있다. 이 경우 서비스 IP 주소의 전송속도는 하향 조정될 수 있 다. 만약, 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 제1 임계값보다 크거나 같고 제2 임계값보다 작은 경우, AI 모델 배포를 위한 멀티캐스트 IP 주소의 전송속도를 유지할 수 있다. 이 경우 서비스 IP 주소의 전송 속도도 유지될 수 있다. 만약, 로드 밸런싱 장치에서 발생하는 트래픽의 전송속도가 제2 임계값보다 크거나 같은 경우, AI 모델 배포를 위한 멀티캐스트 IP 주소의 전송속도를 하향 조정할 수 있다. 이 경우 서비스 IP 주소소의 전송속도는 상향 조 정될 수 있다. 상기 실시예에 따르면, AI 모델 파일의 전송 효율을 극대화하기 위하여 로드 밸런싱 장치의 인입 트래픽 정보를 기반으로 전송속도를 적응적으로 조정할 수 있다. 이를 통하여 AI 서비스에 대한 영향을 최소화하면서 AI 모델 배포 속도를 효율적으로 높일 수 있다. 한편, 본 개시의 실시예들에 따른 방법은 상술한 바와 같이 베어메탈 구조뿐 아니라 가상머신 및/또는 컨테이너 구조에도 적용될 수 있다. 도 9는 본 개시의 실시예들이 적용될 수 있는 가상머신 및 컨테이너 구조를 설정하기 위한 도면이다. 도 9에서 (a)는 가상머신 구조를, (b)는 컨테이너 구조를 개략적으로 나타낸다. 가상머신은 말 그대로 컴퓨터 환경을 가상화 하여 소프트웨어(SW: software)로 구현한 것이다. 컴퓨팅 자원을 가상화 할 수 있는 하이퍼바이저에 기반할 수 있으며 하나의 서버에서 여러 대의 OS를 구성할 수 있는 장점이 있고 서버의 자원(resource)을 논리적으로 분할하는 개념이기 때문에 자원의 가용성을 더욱 높일 수 있다. 컨테이너는 가상머신(VM)과 비교하면 컴퓨터 환경과 달리 애플리케이션 중심으로 설계된 개념이다. 가상머신 (VM)처럼 별도의 OS나 드라이버를 요구하지 않고 호스트 OS의 커널을 공유하며 개별 애플리케이션을 위한 공간 (namespace)을 마련해준다. 따라서, 또 다른 OS 커널이 필요한 가상머신(VM)에 비해 가볍고 효율적으로 애플리 케이션을 실행시킬 수 있으며 배포에도 용이하다. 쿠버네티스(Kubernetes)와 같은 컨테이너 운영관리 도구는, 컨테이너화 된 애플리케이션의 관리를 위한 오픈 소 스 시스템(open source system)으로써 여러 클러스터(cluster)의 호스트 간 애플리케이션 컨테이너의 배치 운영 등을 자동화하기 위한 플랫폼(platform)을 제공하기 위한 목적으로 만들어졌다. 컨테이너는 별도의 컨테이너 엔진을 필요로 하고 대표적으로 도커(Docker)와 같은 컨테이너 엔진이 사용될 수 있다. 컨테이너 엔진은 컨테이너화 하고자 하는 어플리케이션(프로그램)을 그 어플리케이션이 실행하는 데 필요 한 기타 바이너리 파일(binary file)이나 라이브러리들과 함께 격리된 공간에 두어 컨테이너화 할 수 있다. 컨 테이너에는 자체적인 OS가 없고 컨테이너 엔진이 설치된 OS의 커널을 공유할 수 있다. 가상 머신 혹은 컨테이터가 구비된 서버는 호스트 서버라고 불릴 수 있다. 상술한 본 개시의 실시예들은 복수의 서버들이 상기 가상머신 및/또는 컨테이터 구조에 기반한 경우에도 적용될 수 있다. 도 10은 가상머신에 기반한 AI 서비스 시스템의 일 예를 나타낸다. 도 10을 참조하면 AI 서비스 시스템은 디바이스(device, 1000), 로드 밸런싱 장치(load balancing apparatus, 1010), 스위치(switch, 1020), 빌드 시스템(build system, 1030), 배포 컨트롤러(deployment controller, 1040), 복수의 서버(server, 1050)를 포함할 수 있다. 복수의 서버는 각각 하나 이상의 가상머신 (Virtual Machine, VM, 1060)을 포함할 수 있다. 디바이스는 사용자로부터 인풋을 수신할 수 있다. 상기 인풋은 인풋 정보, 인풋 데이터, 인풋 메시지 등 과 혼용될 수 있다. 디바이스는 유저 디바이스 또는 단말을 포함할 수 있다. 상기 인풋은 사용자, 센서, 혹은 다른 디바이스를 통하여 수신할 수 있다. 이는, 다른 실시예에서도 마찬가지이다. 디바이스는 인터넷 등의 네트워크를 통하여 로드 밸런싱 장치와 연결될 수 있다. 디바이스는 인터넷을 통하여 상기 로드 밸런싱 장치로 AI 서비스에 대한 추론 작업을 요청할 수 있다. 즉, 예를 들어, 디바이스는 상기 로드 밸런싱 장치로 상기 AI 서비스에 대한 추론 작업 요청 메시지를 전달 할 수 있다. 예를 들어, 상기 추론 작업 요청 메시지는 URL 형식일 수 있다. 상기 URL 형식은 프로토콜 (protocol) 식별자, 호스트 주소(host address) 패스(Path) 및/또는 쿼리(query) 등으로 구성될 수 있다. 로드 밸런싱 장치는 복수의 서버에 구비된 하나 이상의 가상 머신들 중 적어도 하나로 상기 AI 서비스에 대한 추론 작업에 대한 로드 밸런싱을 수행할 수 있다. 즉, 로드 밸런싱 장치는 상기 복수의 서버에 구비된 하나 이상의 가상 머신들로 상기 AI 서비스에 대한 추론 작업을 분배할 수 있다. 예 를 들어, 상기 로드 밸런싱은 기설정된 로드 밸런싱 알고리즘을 기반으로 로드 밸런싱을 수행할 수 있다. 예를 들어, 상기 기설정된 로드 밸런싱 알고리즘은 라운드 로빈(round robin) 알고리즘, 스틱키 라운드 로빈(sticky round robin) 알고리즘, 가중 라운드 로빈(weighted round robin) 알고리즘, IP/URL 해쉬(IP/URL hash) 알고리 즘, 최소 커넥션(least connection) 알고리즘 또는 최소 시간(least time) 알고리즘을 포함할 수 있다. 상기 로 드 밸런싱 장치는 로드 밸런서 또는 API 게이트웨이를 포함할 수 있다. API 게이트웨이는 URL 기반으로 라우팅 또는 포워딩을 수행할 수 있다. 상기 복수의 서버에 구비된 하나 이상의 가상 머신은 AI 서비스를 제공하기 위해 인공신경망을 이 용하여 연산을 수행하는 뉴럴 프로세싱 장치(NPU)를 포함한 AI 가속기를 포함할 수 있다. 스위치는 경로 제어를 수행한다. 스위치는 AI 서비스 제공을 위하여 로드 밸런싱 장치와 복 수의 서버들 중 적어도 하나를 연결할 수도 있고, AI 모델 배포를 위하여 빌드 시스템, 배포 컨트 롤러 중 적어도 하나와 복수의 서버들 중 일부 또는 전부를 연결할 수도 있다. 또한, 스위치는 후술하는 바와 같이 로드 밸런싱 장치에서의 전송속도 정보 수집 등을 위하여 로드 밸런싱 장치 와 배포 컨트롤러를 연결할 수도 있다. 빌드 시스템은 AI 모델을 관리한다. 빌드 시스템은 AI 모델에 관한 학습, 소스코드 수정/빌드, AI 모델 컴파일, 테스트 등을 수행할 수 있다. 빌드 시스템은 배포 컨트롤러를 통하여 (업데이트된) AI 모델을 복수의 서버에 구비된 하나 이상의 가상 머신으로 배포할 수 있다. 배포 컨트롤러는 AI 모델 배포 설정 및 배포를 수행할 수 있다. AI 모델 배포 설정을 위하여, AI 모델 배 포 그룹, AI 배포 서버 IP 주소, 그룹별 멀티캐스트 IP 주소, QoS 정보 등을 세팅할 수 있다. 배포 컨트롤러 는 빌드 시스템으로부터 (업데이트된) AI 모델을 수신하여 복수의 서버에 구비된 하나 이상 의 가상 머신으로 배포할 수 있다. 이 경우, 복수의 서버에서 (업데이트된) AI 모델을 수신하여 하 나 이상의 가상 머신으로 전달하는 과정이 수행된다. 도 11은 컨테이너에 기반한 AI 서비스 시스템의 일 예를 나타낸다. 도 11을 참조하면, AI 서비스 시스템은 디바이스(device, 1100), 로드 밸런싱 장치(load balancing apparatus, 1110), 스위치(switch, 1120), 빌드 시스템(build system, 1130), 배포 컨트롤러(deployment controller, 1140), 복수의 서버(server, 1150)를 포함할 수 있다. 복수의 서버는 각각 하나 이상의 가상머신 (Virtual Machine, VM, 1160)을 포함할 수 있다. 도 11의 AI 서비스 시스템의 복수의 서버들은 도 10의 AI 서비스 시스템과 비교하여 가상머신 대신 컨테이너 구 조에 기반하며, 하나 이상의 컨테이너를 통하여 AI 추론을 수행할 수 있다. 또한, AI 모델 배포를 위하여 복수 의 서버에서 (업데이트된) AI 모델을 수신하여 하나 이상의 가상 머신으로 전달하는 과정이 수행된 다. 도 12는 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 12는 예를 들어 복 수의 서버가 가상 머신 또는 컨테이너 구조에 기반하는 경우에 관한 것일 수 있다. 여기서, 배포 컨트롤러에서 생성/관리되는 AI 모델 배포 관리 테이블은 예를 들어 상술한 표 2 내지 표 17의 구조를 가질 수 있으며, 상기 AI 모델 배포 관리 테이블의 업데이트 방법은 상술한 바와 같다. 도 12를 참조하면, 배포 컨트롤러는 IP 주소 식별 절차 및 전송속도 제한 결정 절차를 수행한다(S1210). IP 주 소 식별 절차에서 배포 컨트롤러는 요청받은 배포 그룹의 서버 IP 주소와 배포그룹의 멀티캐스트 IP 주소를 식 별할 수 있다. 또한, 전송속도 제한 결정 절차에서, 배포 컨트롤러는 서버의 서비스 IP와 멀티캐스트 IP에 할당 할 전송속도를 결정할 수 있다. 전송속도 제한은 AI 모델 배포를 위한 멀티캐스트 IP에는 높은 전송속도를 할당 하고, 서비스 IP에는 낮은 전송속도를 할당하는 것을 포함할 수 있다. 즉, 전송속도 제한이 적용되는 경우, AI 모델 배포를 위한 멀티캐스트 IP에 할당되는 전송속도가 AI 서비스를 위한 유니캐스트 IP에 할당되는 전송속도 보다 높게 설정될 수 있다. 배포 컨트롤러는 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 복수의 서버로 전송한다(S1215). 상기 복수의 서버는 상술한 AI 모델 배포 그룹 내의 서버들일 수 있다. 즉, 배포 컨트롤러는 상술한 AI 모델 배 포 그룹에 대한 정보, AI 모델 배포 서버 IP 주소에 대한 정보, 그룹별 멀티캐스트 IP 주소에 대한 정보, 속도 제한 정보, QoS 정보 중 적어도 하나를 기반으로 상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정 보를 상기 복수의 서버로 전송할 수 있다. 예를 들어 이 과정은 유니캐스트 방식으로 수행될 수 있다. 복수의 서버들은 멀티캐스트 IP 세팅 및 전송속도 제한 설정 절차를 수행한다(S1220). 복수의 서버들은 상기 멀 티캐스트 IP 세팅 요청 정보를 기반으로 멀티캐스트 IP 세팅을 수행할 수 있다. 복수의 서버들은 상기 전송속도 제한 요청 정보를 기반으로 상기 전송속도 제한 설정 절차를 수행할 수 있다. 복수의 서버들은 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 배포 컨트롤러로 전송한다 (S1225). 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지는 별도의 메시지 형식으로 구성될 수도 있고, 하나의 메시지 형식으로 구성될 수도 있다. 비록 도면에서는 멀티캐스트 IP 세팅을 위한 일련의 절차와 전송속도 제한을 위한 일련의 절차가 동시에 수행되 는 것으로 개시되었으나(ex. S1210 내지 S1225), 이는 예시이며, 멀티캐스트 IP 세팅을 위한 일련의 절차와 전 송속도 제한을 위한 일련의 절차는 별도로 혹은 순차적으로 수행될 수도 있다. 이하 다른 실시예들에서도 마찬 가지이다. 배포 컨트롤러는 AI 모델 멀티캐스트 전송 설정을 수행하고(S1230), AI 모델을 복수의 서버들로 멀티캐스트 전 송한다(S1235). 상기 AI 모델은 상술한 AI 모델 파일을 포함한다. 복수의 서버들은 AI 모델 수신 완료 메시지를 생성하여 배포 컨트롤러로 전송한다(S1245). 예를 들어 상기 AI 모델 멀티캐스트 전송 외의 전송 과정은 유니캐스트 방식으로 수행될 수 있다. 배포 컨트롤러는 전송속도 제한 해제를 결정한다(S1250). 배포 컨트롤러는 AI 모델 멀티캐스트 전송이 완료된 경우 상기 전송속도 제한 해제를 결정할 수 있다. 배포 컨트롤러는 전송속도 제한 해제 요청 정보를 복수의 서버들로 전송한다(S1255). 복수의 서버들은 전송속도 제한 해제 설정을 수행한다(S1260). 복수의 서버들은 전송속도 제한 해제 요청 정보 를 기반으로 상기 전송속도 제한 해제 설정을 할 수 있다. 전송속도 제한 해제라 함은 전송속도 제한을 없애는 것뿐 아니라 전송속도 제한을 초기값 혹은 디폴트 값으로 전환하는 것을 포함할 수 있다. 복수의 서버들은 전송속도 제한 해제 완료 메시지를 배포 컨트롤러로 전송한다(S1265). 복수의 서버들은 가상 모델 및/또는 컨테이너로 AI 모델 파일을 전달한다(S1266). 복수의 서버들 각각에 구비된 하나 이상의 가상 모델 및/또는 컨테이너로의 AI 모델 파일 전달은 외부 네트워크를 거치지 않고 수행될 수 있 다. 복수의 서버들 각각에 구비된 하나 이상의 가상 모델 및/또는 컨테이너는 AI 모델 교체를 수행한다(S1270). 복 수의 서버들 각각에 구비된 하나 이상의 가상 모델 및/또는 컨테이너는 정해진 AI 모델 교체 알고리즘에 따라 AI 모델 교체를 수행한다. 비록 도시되지는 않았으나 AI 모델 교체 시간을 지시하는 정보를 배포 컨트롤러로부 터 서버를 통하여 수신할 수도 있다. 비록 S1266 및 S1270은 S1260 및 S1265 후에 수행되는 것으로 도시되었으나 이는 예시로서, S1266 및 S1270은 S1260 혹은 S1265 전에 수행될 수도 있다. 복수의 서버들 각각에 구비된 하나 이상의 가상 모델 및/또는 컨테이너는 AI 모델 교체 완료 메시지를 배포 컨 트롤러로 전송한다(S1275). 가상 모델 및/또는 컨테이너는 서버를 통하여 상기 AI 모델 교체 완료 메시지를 배 포 컨트롤러로 전송할 수 있다. 배포 컨트롤러는 AI 모델 배포 상태 업데이트를 수행한다(S1280). 배포 컨트롤러는 상기 AI 모델 교체 완료 메 시지를 기반으로 상기 AI 모델 배포 상태 업데이트를 수행할 수 있다. 상술한 바와 같이 그룹 내 모든 서버의 AI 모델 배포가 완료된 경우에 그룹 배포 상태가 완료(Complete)로 표시될 수 있다. 종래기술에 따르면 서버 내 복수의 가상 머신 혹은 복수의 컨테이너들에 대한 AI 모델 업데이트를 위하여 각 가 상 머신 혹은 각 컨테이너 단위로 반복하여 유니캐스트 기반으로 AI 모델을 전송하여야 했으나, 상술한 본 개시 의 실시예에 따르면, 클라우드 내 가상 머신 또는 컨테이너는 호스트 서버가 멀티캐스트로 수신한 AI 모델 파일 을 한번에 공유받아 AI 모델을 업데이트할 수 있다. 이를 통하여 복수의 가상 머신 혹은 복수의 컨테이너들에 대한 AI 모델 교체를 위한 데이터 전송량을 상당히 줄일 수 있다. 또한, 상술한 바와 같이, AI 서비스를 위한 유니캐스트 IP 주소에는 낮은 전송속도를 할당하고, AI 모델 배포를 위한 멀티캐스트 IP 주소에는 높은 전송속도를 할당하여 지속적으로 AI 서비스가 가능한 상태를 유지할 수 있다. 이 경우, 멀티캐스트를 이용하여 대용량의 AI 모델 파일을 고효율/초고속으로 배포하고, 배포 완료 후 전 송속도 제한을 해제하여 서비스의 중단 없이 AI 모델 배포를 완료할 수 있다. 상술한 실시예에 따르면, AI 모델 배포 전후의 전송속도를 제어함으로써 UDP 전송 등에 따른 에러 발생을 최소 화할 수 있다. 또한, 기존에는 AI 모델 배포 과정만 추적/관리할 수 있었던 것에 비하여, AI 모델 배포와 서비 스에 관한 전과정을 추적 및 관리할 수 있다. 또한, 서비스 IP 주소와 배포 IP 주소(멀티캐스트 IP)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, 복수의 가상 머신 혹은 복수의 컨테이너들에 AI 모델 배포를 위한 IP 접속 정보 관리의 복잡도를 낮출 수 있다. 도 13은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 13에서 개시된 실 시예에 따르면 로드 밸런싱 장치에서의 전송속도를 기반으로 전송속도 제한 결정을 수행할 수 있다. 도 13을 참조하면, 일반적인 절차의 경우 도 12에서 설명한 절차와 동일하며, 차이가 있는 부분을 위주로 설명 한다. 로드 밸런싱 장치는 로드 밸런싱 장치에서의 전송속도를 측정하고, 전송속도 정보를 생성하여 배포 컨트롤러로 전송한다(S1300). 상기 전송속도 정보는 로드 밸런싱 장치에서의 인바운드/아웃바운드 전송속도에 관한 정보를 포함하 수 있다. 배포 컨트롤러는 전송속도 제한 결정을 수행함에 있어서 상기 로드 밸런싱 장치에서부터 수신한 전송속도에 관 한 정보를 기반할 수 있다. 상기 실시예에 따르면, 가상머신 및/또는 컨테이너 구조에서 AI 모델 파일의 전송 효율을 극대화하기 위하여 로 드 밸런싱 장치의 인입 트래픽 정보를 기반으로 전송속도를 조정할 수 있다. 이를 통하여 AI 서비스에 대한 영 향을 최소화하면서 AI 모델 배포 속도를 효율적으로 높일 수 있다. 도 14은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 14에서 개시된 실 시예에 따르면 로드 밸런싱 장치에서의 전송속도를 기반으로 AI 모델 전송 과정에서 적응적으로 전송속도를 제 어할 수 있다. 도 14를 참조하면, 일반적인 절차의 경우 도 12에서 설명한 절차와 동일하며, 차이가 있는 부분을 위주로 설명 한다. 로드 밸런싱 장치는 로드 밸런싱 장치에서의 전송속도를 측정하고, 전송속도 정보를 생성하여 배포 컨트롤러로 전송한다(S1400). 상기 전송속도 정보는 로드 밸런싱 장치에서의 인바운드/아웃바운드 전송속도에 관한 주기적/ 비주기적 정보를 포함하 수 있다. 예를 들어, 상기 전송속도 정보는 주기적(ex. 10ms)으로 전송될 수 있다. 또 는 상기 전송속도 정보는 특정 트리거링 조건을 기반으로 전송될 수 있다. 예를 들어, AI 서비스를 위한 패킷의 인바운드/아웃바운드 전송속도가 일정 값보다 큰 경우, 또는 일정 값보다 작은 경우 전송될 수 있다. 또는, AI 서비스를 위한 패킷의 인바운드/아웃바운드 전송속도 이전에 측정한 전송속도와 일정 이상 차이가 나는 경우 전 송될 수 있다. 배포 컨트롤러는 S1430에서 전송속도 제어를 수행할 수 있다. 즉, 배포 컨트롤러는 S1435에서 전송속도 제어 정 보를 전송하여 서버들에서 멀티캐스트 IP로 AI 모델을 수신하는 속도를 제어할 수 있다. S1400, S1430, S1435는 반복적으로 수행될 수 있다. 예를 들어, 이 경우 업데이트된 AI 모델 파일의 일부는 제1 전송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되도록 제어될 수 있다. 상기 제1 전송속도는 상기 전송 속도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 상기 전송속도 제어 정보가 지시하 는 전송속도일 수 있다. 다른 예로, 업데이트된 AI 모델 파일의 일부는 제1 전송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되고, 나머지 일부는 제3 전송속도로 전송되도록 제어될 수 있다. 예를 들어, 상기 제1 전 송속도는 상기 전송속도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 제1 전송속도 제 어 정보가 지시하는 전송속도일 수 있고, 상기 제3 전송속도는 제2 전송속도 제어 정보가 지시하는 전송속도일 수 있다. 전송속도 제한 및/또는 전송속도 제어는 예를 들어 도 8 등에서 상술한 방법을 기반으로 수행될 수 있다. 상기 실시예에 따르면, 가상머신 및/또는 컨테이너 구조에서 AI 모델 파일의 전송 효율을 극대화하기 위하여 로 드 밸런싱 장치의 인입 트래픽 정보를 기반으로 전송속도를 적응적으로 조정할 수 있다. 이를 통하여 AI 서비스 에 대한 영향을 최소화하면서 AI 모델 배포 속도를 효율적으로 높일 수 있다. 도 15는 본 개시의 실시예(들)에 따른 AI 모델 배포 방법을 개략적으로 나타낸다. 본 도면에서 개시된 방법은 배포 컨트롤러에서 수행될 수 있다. 상기 배포 컨트롤러는 AI 모델 배포를 위한 장치일 수 있다. 본 도면에서 개시된 방법은 본 개시에서 상술한 실시예들을 포함할 수 있다. 도 15를 참조하면, 배포 컨트롤러는 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 생성한다 (S1500). 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보는 각각 생성될 수도 있고 통합 하여 생성될 수도 있다. 상기 멀티캐스트 IP 세팅 요청 정보는 업데이트된 AI 모델 파일 전송을 위한 멀티캐스 트 IP 주소를 지시할 수 있다. 상기 전송속도 제한 요청 정보는 상기 멀티캐스트 IP 주소에 할당되는 (제한된) 전송속도를 지시할 수 있다. 배포 컨트롤러는 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보를 복수의 서버들로 전송 한다(S1510). 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보는 각각 전송될 수도 있고, 통합하여 전송될 수도 있다. 배포 컨트롤러는 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 수신한다(S1520). 상기 멀티 캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지는 각각 수신될 수도 있고, 통합하여 수신될 수 도 있다. 배포 컨트롤러는 업데이트된 AI 모델 파일을 전송한다(S1530). 상기 업데이트된 AI 모델 파일은 멀티캐스트 전 송 기반으로 전송될 수 있다. 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소 및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할당되는 전송속도를 기반으로 전송될 수 있다. 이 경우, 상기 전송속도 제한 정보는 AI 서비스를 위한 유니캐스트 IP 주소에 대한 전송속도보다 상기 업데이트된 모델 배포를 위한 멀티캐스트 IP 주소에 대한 전송속 도를 더 높게 설정함을 지시할 수 있다. 상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정도 중 적어도 하나는 유니캐스트 기반으로 전송되 고, 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지 중 적어도 하나는 상기 유니캐스 트 기반으로 수신되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 기반으로 전송될 수 있다. 또한, 상기 배포 컨트롤러는 로드 밸런싱 장치에서의 전송속도에 대한 전송속도 정보를 주기적/비주기적으로 수 신할 수 있다. 상기 로드 밸런싱 장치에서의 전송속도는 인바운드 또는 아웃바운드 중 적어도 하나에 대한 전송 속도를 포함할 수 있다. 이 경우 일 예로, 상기 전송속도 제한 요청 정보는 상기 전송속도 정보를 기반으로 생 성될 수 있다. 다른 예로, 상기 배포 컨트롤러는 상기 수신한 전송속도 정보를 기반으로 상기 멀티캐스트 IP 주 소에 할당된 전송속도의 조정을 지시하는 전송속도 제어 정보를 생성하고, 상기 전송속도 제어 정보를 상기 복 수의 서버들로 전송할 수 있다. 이 경우 상기 전송속도 제한 요청 정보가 지시하는 전송속도와 상기 전송속도 제어 정보가 지시하는 전송속도는 다르게 설정될 수 있다. 상기 전송속도 정보를 기반으로 상기 로드 밸런싱 장 치에서의 전송속도가 도출되고, 제1 임계값 또는 제2 임계값 중 적어도 하나와 상기 로드 밸런싱 장치에서의 전 송속도와의 비교를 기반으로 상기 전송속도 제어 정보가 생성되고, 상기 제1 임계값 또는 상기 제2 임계값 중 적어도 하나는 AI 서비스에 대한 서비스 IP 주소에 할당된 전송속도를 기반으로 도출될 수 있다. 이 경우 예를 들어, 도 8 등과 관련하여 상술한 전송속도 제어 방법이 적용될 수 있다. 상기 전송속도 제어 정보의 생성 및 전송은 주기적 또는 조건 기반으로 반복하여 수행될 수도 있다. 예를 들어, 업데이트된 AI 모델 파일의 일부는 제1 전송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되도 록 제어될 수 있다. 상기 제1 전송속도는 상기 전송속도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 상기 전송속도 제어 정보가 지시하는 전송속도일 수 있다. 다른 예로, 업데이트된 AI 모델 파일 의 일부는 제1 전송속도로 전송되고, 다른 일부는 제2 전송속도로 전송되고, 나머지 일부는 제3 전송속도로 전 송되도록 제어될 수 있다. 예를 들어, 상기 제1 전송속도는 상기 전송속도 제한 요청 정보가 지시하는 전송속도 일 수 있고, 상기 제2 전송속도는 제1 전송속도 제어 정보가 지시하는 전송속도일 수 있고, 상기 제3 전송속도 는 제2 전송속도 제어 정보가 지시하는 전송속도일 수 있다. AI 서비스 제공을 위하여 상기 복수의 서버 각각에 할당된 유니캐스트 IP 주소는 상기 업데이트된 AI 모델 배포 를 위한 멀티캐스트 IP 주소와 다를 수 있다. 상기 전송속도 제한 요청 정보 및/또는 상기 전송속도 제어 정보 는 상기 유니캐스트 IP 주소에 대한 전송속도 또는 상기 멀티캐스트 IP 주소에 대한 전송속도 중 적어도 하나를 지시할 수 있다. 배포 컨트롤러는 AI 모델 배포 구성 테이블 및/또는 AI 모델 배포 관리 테이블을 생성 및 관리할 수 있다. 상기 AI 모델 배포 구성 테이블은 예를 들어 상술한 표 1을 포함할 수 있다. 상기 AI 모델 배포 관리 테이블은 예를 들어 상술한 표 2 내지 표 17을 포함할 수 있다. 배포 컨트롤러는 상기 AI 모델 배포 구성 테이블을 통하여 AI 모델 배포 그룹, AI 배포 서버 IP 주소, 그룹별 멀티캐스트 IP 주소, 속도 제한 정보, QoS 정보 등을 세팅할 수 있다. AI 서비스 제공을 위하여 상기 복수의 서 버 각각에 할당된 유니캐스트 IP 주소는 상기 업데이트된 AI 모델 배포를 위한 멀티캐스트 IP 주소와 다를 수 있다. 배포 컨트롤러는 복수의 AI 모델 배포 그룹을 구성할 수 있다. 상기 복수의 AI 모델 배포 그룹은 제1 AI 모델 배포 그룹 및 제2 AI 모델 배포 그룹을 포함할 수 있다. 이 경우 상기 제1 AI 모델 배포 그룹에 속하는 제1 서 버들에 할당되는 제1 멀티캐스트 IP 주소에 허용되는 전송속도와 상기 제2 AI 모델 배포 그룹에 속하는 제2 서 버들에 할당되는 제2 멀티캐스트 IP 주소에 허용되는 전송속도는 다르게 설정될 수도 있다. 상기 AI 모델 배포 관리 테이블은 그룹 배포 상태(group deploy status) 필드 및 서버 배포 상태 필드를 포함하고, 상기 제1 AI 모 델 배포 그룹에 포함된 모든 서버, 모든 가상 머신, 또는 모든 컨테이너에 대한 서버 배포 상태 필드가 완료 상태를 나타내는 경우에 상기 제1 AI 모델 배포 그룹에 대한 상기 그룹 배포 상태 필드는 완료 상태로 표시될 수 있다. 도 16은 본 개시의 실시예(들)에 따른 AI 모델 배포 지원 방법을 개략적으로 나타낸다. 본 도면에서 개시된 방 법은 서버에서 수행될 수 있다. 본 도면에서 개시된 방법은 본 개시에서 상술한 실시예들을 포함할 수 있다. 도 16을 참조하면, 서버는 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정보를 수신한다(S1600). 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보는 상기 배포 컨트롤러로부터 수신될 수 있다. 상기 멀티캐스트 IP 세팅 요청 정보 및 상기 전송속도 제한 요청 정보는 각각 수신될 수도 있고, 통합하여 수신 될 수도 있다. 상기 멀티캐스트 IP 세팅 요청 정보는 업데이트된 AI 모델 파일 전송을 위한 멀티캐스트 IP 주소 를 지시할 수 있다. 상기 전송속도 제한 요청 정보는 상기 멀티캐스트 IP 주소에 할당되는 (제한된) 전송속도를 지시할 수 있다. 서버는 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP 주소의 세팅을 수행한다(S1610). 서버는 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 업데이트된 AI 모델 파일을 수신하기 위한 멀티캐스트 IP 주소의 세 팅을 수행한다. 서버는 상기 멀티캐스트 IP 주소에 전송속도를 할당한다(S1620). 서버는 상기 전송속도 제한 요청 정보를 기반 으로 상기 멀티캐스트 IP 주소에 전송속도를 할당한다. 서버는 멀티캐스트 IP 세팅 완료 메시지 및 전송속도 제한 완료 메시지를 생성하여 전송한다(S1630). 상기 멀티 캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지는 상기 배포 컨트롤러로 전송될 수 있다. 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지는 각각 수신될 수도 있고, 통합하여 전송 될 수도 있다. 서버는 업데이트된 AI 모델 파일을 수신한다(S1640). 상기 업데이트된 AI 모델 파일은 멀티캐스트 전송 기반으 로 상기 배포 컨트롤러로부터 수신될 수 있다. 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 IP 세팅 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소 및 상기 전송속도 제한 요청 정보를 기반으로 지시되는 상기 멀티캐스트 IP 주소에 할당되는 상기 전송속 도를 기반으로 수신될 수 있다. 상기 멀티캐스트 IP 세팅 요청 정보 및 전송속도 제한 요청 정도 중 적어도 하나는 유니캐스트 기반으로 수신되 고, 상기 멀티캐스트 IP 세팅 완료 메시지 및 상기 전송속도 제한 완료 메시지 중 적어도 하나는 상기 유니캐스 트 기반으로 전송되고, 상기 업데이트된 AI 모델 파일은 상기 멀티캐스트 기반으로 수신될 수 있다. 상기 전송속도 제한 정보는 AI 서비스를 위한 유니캐스트 IP 주소에 대한 전송속도보다 상기 업데이트된 모델 배포를 위한 멀티캐스트 IP 주소에 대한 전송속도를 더 높게 설정될 수 있다. 서버는 상기 배포 컨트롤러로부터 상기 멀티캐스트 IP 주소에 할당된 전송속도의 조정을 지시하는 전송속도 제 어 정보를 더 수신할 수 있다. 서버는 상기 전송속도 제어 정보를 기반으로 상기 멀티캐스트 IP 주소에 할당된 전송속도를 조정할 수 있다. 이 경우 상기 전송속도 제한 요청 정보가 지시하는 전송속도와 상기 전송속도 제어 정보가 지시하는 전송속도는 다를 수 있다. 전송속도 제어 정보는 반복적으로 수신될 수 있다. 예를 들어, 업데이트된 AI 모델 파일의 일부는 제1 전송속도 로 수신되고, 다른 일부는 제2 전송속도로 수신되도록 제어될 수 있다. 상기 제1 전송속도는 상기 전송속도 제 한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 상기 전송속도 제어 정보가 지시하는 전송 속도일 수 있다. 다른 예로, 업데이트된 AI 모델 파일의 일부는 제1 전송속도로 수신되고, 다른 일부는 제2 전 송속도로 수신되고, 나머지 일부는 제3 전송속도로 수신되도록 제어될 수 있다. 예를 들어, 상기 제1 전송속도 는 상기 전송속도 제한 요청 정보가 지시하는 전송속도일 수 있고, 상기 제2 전송속도는 제1 전송속도 제어 정 보가 지시하는 전송속도일 수 있고, 상기 제3 전송속도는 제2 전송속도 제어 정보가 지시하는 전송속도일 수 있 다. AI 서비스 제공을 위하여 상기 복수의 서버 각각에 할당된 유니캐스트 IP 주소는 상기 업데이트된 AI 모델 배포 를 위한 멀티캐스트 IP 주소와 다를 수 있다. 상기 전송속도 제한 요청 정보 및/또는 상기 전송속도 제어 정보 는 상기 유니캐스트 IP 주소에 대한 전송속도 또는 상기 멀티캐스트 IP 주소에 대한 전송속도 중 적어도 하나를 지시할 수 있다.서버는 상기 업데이트된 AI 모델 파일을 기반으로, AI 모델 파일 교체 절차를 수행할 수 있다. 서버는 상기 업 데이트된 AI 모델 파일을 기반으로 상기 서버의 AI 모델 파일 교체를 수행하거나, 또는 상기 서버에 구비된 하 나 이상의 가상 머신 또는 하나 이상의 컨테이너에 대한 상기 AI 모델 파일 교체를 수행할 수 있다. 상술한 실시예에 따르면, 멀티캐스트 방식으로 AI 모델을 전송하여 전송 효율을 높일 수 있다. 구체적으로 예를 들어, AI 서비스를 위한 유니캐스트 IP 주소에는 낮은 전송속도를 할당하고, AI 모델 배포를 위한 멀티캐스트 IP 주소에는 높은 전송속도를 할당하여 지속적으로 AI 서비스가 가능한 상태를 유지할 수 있다. 이 경우, 멀티 캐스트를 이용하여 대용량의 AI 모델 파일을 고효율/초고속으로 배포하고, 배포 완료 후 전송속도 제한을 해제 하여 서비스의 중단 없이 AI 모델 배포를 완료할 수 있다. 또한, 상술한 실시예에 따르면, AI 모델 배포 전후의 전송속도를 제어함으로써 UDP 전송에 따른 에러 발생을 최 소화할 수 있다. 또한, 기존에는 AI 모델 배포 과정만 추적/관리할 수 있었던 것에 비하여, AI 모델 배포와 서 비스에 관한 전과정을 추적 및 관리할 수 있다. 또한, 본 개시의 실시예에 따르면, 서비스 IP 주소와 배포 IP 주소(멀티캐스트 IP)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, AI 모델 배포를 위한 IP 접속 정보 관리의 복잡도를 낮출 수 있다. 또한, 상술한 실시예에 따르면, AI 모델 파일의 전송 효율을 극대화하기 위하여 로드 밸런싱 장치의 인입 트래 픽 정보를 기반으로 전송속도를 적응적/비적응적으로 조정할 수 있다. 이를 통하여 AI 서비스에 대한 영향을 최 소화하면서 AI 모델 배포 속도를 효율적으로 높일 수 있다. 또한, 종래기술에 따르면 서버 내 복수의 가상 머신 혹은 복수의 컨테이너들에 대한 AI 모델 업데이트를 위하여 각 가상 머신 혹은 각 컨테이너 단위로 반복하여 유니캐스트 기반으로 AI 모델을 전송하여야 했으나, 상술한 본 개시의 실시예에 따르면, 클라우드 내 가상 머신 또는 컨테이너는 호스트 서버가 멀티캐스트로 수신한 AI 모델 파일을 한번에 공유받아 AI 모델을 업데이트할 수 있다. 이를 통하여 복수의 가상 머신 혹은 복수의 컨테이너들 에 대한 AI 모델 교체를 위한 데이터 전송량을 상당히 줄일 수 있다. 또한, 서비스 IP 주소와 배포 IP 주소(멀 티캐스트 IP)를 나누어서 관리할 수 있고, AI 모델 배포를 멀티캐스트로 진행함에 따라, 복수의 가상 머신 혹은 복수의 컨테이너들에 AI 모델 배포를 위한 IP 접속 정보 관리의 복잡도를 낮출 수 있다. 상술한 본 개시의 실시예는 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행가능한 명령어를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스될 수 있는 임 의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨터 판독가능 매체는 컴퓨터 저장 매체 및 통신 매체를 모두 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독가능 명령어, 데이터 구조, 프로그램 모듈, 또는 반송파와 같은 변조된 데이터 신호의 기타 데이터, 또는 기타 전송 메커니즘을 포함하며, 임의의 정보 전달 매체를 포함한다. 또한 상술한 본 개시의 실시예는 컴퓨터에 의해 실행 가능한 명령어를 포함하는 컴퓨터 프로그램(또는 컴퓨터 프로그램 제품)으로 구현될 수도 있다. 컴퓨터 프로그램은 프로세서에 의해 처리되는 프로그래밍 가능한 기계 명령어를 포함하고, 고레벨 프로그래밍 언어(High-level Programming Language), 객체지향 프로그래밍 언어 (Object-oriented Programming Language), 어셈블리 언어 또는 기계 언어 등으로 구현될 수 있다. 또한 컴퓨터 프로그램은 유형의 컴퓨터 판독가능 기록매체(예를 들어, 메모리, 하드디스크, 자기/광학 매체 또는 SSD(Solid- State Drive) 등)에 기록될 수 있다. 따라서 상술한 본 개시의 실시예는 상술한 바와 같은 컴퓨터 프로그램이 컴퓨팅 장치에 의해 실행됨으로써 구현 될 수 있다. 컴퓨팅 장치는 프로세서와, 메모리와, 저장 장치와, 메모리 및 고속 확장포트에 접속하고 있는 고 속 인터페이스와, 저속 버스와 저장 장치에 접속하고 있는 저속 인터페이스 중 적어도 일부를 포함할 수 있다. 이러한 성분들 각각은 다양한 버스를 이용하여 서로 접속되어 있으며, 공통 머더보드에 탑재되거나 다른 적절한 방식으로 장착될 수 있다. 여기서 프로세서는 컴퓨팅 장치 내에서 명령어를 처리할 수 있는데, 이런 명령어로는, 예컨대 고속 인터페이스 에 접속된 디스플레이처럼 외부 입력, 출력 장치상에 GUI(Graphic User Interface)를 제공하기 위한 그래픽 정 보를 표시하기 위해 메모리나 저장 장치에 저장된 명령어를 들 수 있다. 다른 실시예로서, 다수의 프로세서 및/ 또는 다수의 버스가 적절히 다수의 메모리 및 메모리 형태와 함께 이용될 수 있다. 또한 프로세서는 독립적인 다수의 아날로그 및(또는) 디지털 프로세서를 포함하는 칩들이 이루는 칩셋으로 구현될 수 있다.또한 메모리는 컴퓨팅 장치 내에서 정보를 저장한다. 일례로, 메모리는 휘발성 메모리 유닛 또는 그들의 집합으 로 구성될 수 있다. 다른 예로, 메모리는 비휘발성 메모리 유닛 또는 그들의 집합으로 구성될 수 있다. 또한 메 모리는 예컨대, 자기 혹은 광 디스크와 같이 다른 형태의 컴퓨터 판독 가능한 매체일 수도 있다. 그리고 저장장치는 컴퓨팅 장치에게 대용량의 저장공간을 제공할 수 있다. 저장 장치는 컴퓨터 판독 가능한 매 체이거나 이런 매체를 포함하는 구성일 수 있으며, 예를 들어 SAN(Storage Area Network) 내의 장치들이나 다른 구성도 포함할 수 있고, 플로피 디스크 장치, 하드 디스크 장치, 광 디스크 장치, 혹은 테이프 장치, 플래시 메 모리, 그와 유사한 다른 반도체 메모리 장치 혹은 장치 어레이일 수 있다. 또한, 네트워크는 근거리 통신망(Local Area Network; LAN), 광역 통신망(Wide Area Network; WAN) 또는 부가 가치 통신망(Value Added Network; VAN) 등과 같은 유선 네트워크나 이동 통신망(mobile radio communication network) 또는 위성 통신망 등과 같은 다양한 종류의 무선 네트워크로 구현될 수 있다. 이상에서 살펴본 본 개시는 도면에 도시된 실시예들을 참고로 하여 설명하였으나 이는 예시적인 것에 불과하며 당해 분야에서 통상의 지식을 가진 자라면 이로부터 다양한 변형 및 실시예의 변형이 가능하다는 점을 이해할 것이다. 즉, 본 개시의 권리범위는 상술된 구현예에 한정되지 않으며 다음의 청구범위에서 정의하고 있는 구현 예의 기본 개념을 이용한 당업자의 여러 변형 및 개량 형태 또한 구현예의 권리범위에 속하는 것이다. 따라서, 본 개시의 진정한 기술적 보호범위는 첨부된 청구범위의 기술적 사상에 의해서 정해져야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12 도면13 도면14 도면15 도면16"}
{"patent_id": "10-2024-0130342", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1 및 2는 서비스 배포(deployment) 방법의 예를 나타낸다. 도 3은 본 개시에 따른 AI 서비스 시스템의 예를 나타낸다. 도 4는 본 개시의 실시예에 따른 빌드 시스템 및 배포 컨트롤러 간 AI 모델 배포 준비 과정을 예시적으로 나타 낸다. 도 5는 본 개시의 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 6은 본 개시의 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 7은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 8은 본 개시의 상술한 실시예들에 적용될 수 있는 전송속도 제한 및/또는 전송속도 제어 방법을 예시적으로 나타낸다. 도 9는 본 개시의 실시예들이 적용될 수 있는 가상머신 및 컨테이너 구조를 설정하기 위한 도면이다. 도 10은 가상머신에 기반한 AI 서비스 시스템의 일 예를 나타낸다. 도 11은 컨테이너에 기반한 AI 서비스 시스템의 일 예를 나타낸다. 도 12는 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 13은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다. 도 14은 본 개시의 또 다른 일 실시예에 따른 AI 모델 배포 방법을 예시적으로 나타낸다.도 15는 본 개시의 실시예(들)에 따른 AI 모델 배포 방법을 개략적으로 나타낸다. 도 16은 본 개시의 실시예(들)에 따른 AI 모델 배포 지원 방법을 개략적으로 나타낸다."}
