{"patent_id": "10-2023-0064097", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0166633", "출원번호": "10-2023-0064097", "발명의 명칭": "연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법 및 장치", "출원인": "한양대학교 에리카산학협력단", "발명자": "조성현"}}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 사용자 장치(User Equipment, UE)동작 방법에 있어서,중앙 서버로부터 글로벌 모델 복사본을 수신하는 단계;상기 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 획득한 데이터를 통해 상기 로컬 모델에 대한 학습을 수행하여 제1 로컬 모델을 획득하는 단계;로컬 확산에 기초한 로컬 학습을 통해 상기 제1 로컬 모델을 제2 로컬 모델로 업데이트하는 단계; 및상기 제2 로컬 모델을 상기 중앙 서버로 전송하는 단계를 포함하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 UE는 상기 중앙 서버에 기초하여 상기 연합 학습에 참여하는 UE인, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 로컬 학습이 제1 확산 라운드(diffusion round)에서 수행되는 경우, 상기 UE는 DoL(Degree of Learning)을 상기 중앙 서버에 기초하여 상기 연합 학습에 참여하는 복수의 UE들로 전송하되,상기 DoL은 상기 로컬 모델에 기초하여 상기 획득한 데이터를 통해 학습된 누적 데이터 분포를 나타내는, 단말동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3 항에 있어서,상기 DoL은 브로드캐스팅 방식에 기초하여 상기 연합 학습에 참여하는 상기 복수의 UE들로 전송되는, 단말 동작방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3 항에 있어서,상기 DoL은 업링크 제어 채널을 통해 상기 중앙 서버로 전송되고,상기 중앙 서버가 상기 연합 학습에 참여하는 상기 복수의 UE들로 다운링크 제어 채널을 통해 상기 DoL을 전송하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3 항에 있어서,상기 UE가 상기 연합 학습에 참여하는 상기 복수의 UE들 각각으로부터 DoL을 수신하고,공개특허 10-2024-0166633-3-상기 DoL 각각에 상기 제1 로컬 모델의 데이터 셋 상태 정보(Data State Information, DSI)를 반영하여 예비DoL을 생성하고,상기 예비 DoL에 기초하여 제1 후보 IID(Independent and Identically Distributed) 거리를 도출하여 상기 중앙 서버로 전송하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 중앙 서버는 제2 확산 라운드에서 도출된 제2 후보 IID와 상기 제1 후보 IID 차이 값을 도출하여 입찰 가격 정보를 도출하되, 상기 제2 확산 라운드는 상기 제1 확산 라운드 이전의 확산 라운드이고,상기 중앙 서버는 상기 연합 학습에 참여한 상기 복수의 UE들 각각에 대한 입찰 가격 정보를 도출하여 UE 평가를 통해 다음 학습 UE를 결정하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7 항에 있어서,상기 중앙 서버는 상기 복수의 UE들 각각에 대한 채널 상태 정보(Channel State Information, CSI)를 더 고려하여 상기 UE 평가를 통해 상기 다음 학습 UE를 결정하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8 항에 있어서,상기 UE가 상기 다음 학습 UE로 결정된 경우, 상기 UE는 D2D(device to device) 방식에 기초하여 상기 연합 학습에 참여한 상기 복수의 UE들로 상기 제1 로컬 모델을 전송하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8 항에 있어서,상기 UE가 상기 다음 학습 UE로부터 다음 학습 UE의 로컬 모델을 D2D 방식에 기초하여 수신하는 경우, 상기 UE는 상기 다음 학습 UE의 로컬 모델에 기초하여 학습을 수행하여 상기 제2 로컬 모델을 획득하는, 단말 동작 방법."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 중앙 서버 동작 방법에 있어서,글로벌 모델 복사본을 상기 연합 학습에 참여하는 복수의 UE들로 전송하는 단계로서, 상기 복수의 UE들 각각은상기 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 상기 복수의 UE들 각각이 획득한 데이터를 통해 상기 로컬 모델에 대한 학습을 수행하고;로컬 학습에 기초하여 복수의 UE들 각각으로부터 후보 IID 거리를 수신하여 다음 학습 UE를 결정하는 단계;상기 결정된 다음 학습 UE에 기초한 스케줄링 정보를 상기 연합 학습에 참여하는 상기 복수의 UE들로 전송하는단계; 및상기 복수의 UE들로부터 상기 로컬 학습에 기초하여 학습된 로컬 모델을 획득하여 글로벌 모델을 업데이트하는단계를 포함하는, 중앙 서버 동작 방법.공개특허 10-2024-0166633-4-청구항 12 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 수행하는 사용자 장치(User Equipment,UE)에 있어서,메모리;송수신부; 및상기 메모리와 상기 송수신부를 제어하는 제어부를 포함하되,상기 제어부는,중앙 서버로부터 글로벌 모델 복사본을 수신하고,상기 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 획득한 데이터를 통해 상기 로컬 모델에 대한 학습을 수행하여 제1 로컬 모델을 획득하고,로컬 확산에 기초한 로컬 학습을 통해 상기 제1 로컬 모델을 제2 로컬 모델로 업데이트하고, 및상기 제2 로컬 모델을 상기 중앙 서버로 전송하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12 항에 있어서, 상기 UE는 상기 중앙 서버에 기초하여 상기 연합 학습에 참여하는 UE이고,상기 로컬 학습이 제1 확산 라운드(diffusion round)에서 수행되는 경우, 상기 UE는 DoL(Degree of Learning)을 상기 중앙 서버에 기초하여 상기 연합 학습에 참여하는 복수의 UE들로 전송하되,상기 DoL은 상기 로컬 모델에 기초하여 상기 획득한 데이터를 통해 학습된 누적 데이터 분포를 나타내는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서,상기 DoL은 브로드캐스팅 방식에 기초하여 상기 연합 학습에 참여하는 상기 복수의 UE들로 전송되는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제13 항에 있어서,상기 DoL은 업링크 제어 채널을 통해 상기 중앙 서버로 전송되고,상기 중앙 서버가 상기 연합 학습에 참여하는 상기 복수의 UE들로 다운링크 제어 채널을 통해 상기 DoL을 전송하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13 항에 있어서,상기 UE가 상기 연합 학습에 참여하는 상기 복수의 UE들 각각으로부터 DoL을 수신하고,상기 DoL 각각에 상기 제1 로컬 모델의 데이터 셋 상태 정보(Data State Information, DSI)를 반영하여 예비DoL을 생성하고,상기 예비 DoL에 기초하여 제1 후보 IID(Independent and Identically Distributed) 거리를 도출하여 상기 중공개특허 10-2024-0166633-5-앙 서버로 전송하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16 항에 있어서,상기 중앙 서버는 제2 확산 라운드에서 도출된 제2 후보 IID와 상기 제1 후보 IID 차이 값을 도출하여 입찰 가격 정보를 도출하되, 상기 제2 확산 라운드는 상기 제1 확산 라운드 이전의 확산 라운드이고,상기 중앙 서버는 상기 연합 학습에 참여한 상기 복수의 UE들 각각에 대한 입찰 가격 정보를 도출하고, 상기 입찰 가격 정보 및 상기 복수의 UE들 각각에 대한 채널 상태 정보(Channel State Information, CSI)를 통해 UE평가를 수행하여 다음 학습 UE를 결정하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17 항에 있어서,상기 UE가 상기 다음 학습 UE로 결정된 경우, 상기 UE는 D2D(device to device) 방식에 기초하여 상기 연합 학습에 참여한 상기 복수의 UE들로 상기 제1 로컬 모델을 전송하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제17 항에 있어서,상기 UE가 상기 다음 학습 UE로부터 다음 학습 UE의 로컬 모델을 D2D 방식에 기초하여 수신하는 경우, 상기 UE는 상기 다음 학습 UE의 로컬 모델에 기초하여 학습을 수행하여 상기 제2 로컬 모델을 획득하는, 단말."}
{"patent_id": "10-2023-0064097", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 수행하는 중앙 서버에 있어서,메모리;송수신부; 및상기 메모리와 상기 송수신부를 제어하는 제어부를 포함하되,상기 제어부는글로벌 모델 복사본을 상기 연합 학습에 참여하는 복수의 UE들로 전송하되, 상기 복수의 UE들 각각은 상기 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 상기 복수의 UE들 각각이 획득한 데이터를 통해 상기 로컬모델에 대한 학습을 수행하고,로컬 학습에 기초하여 복수의 UE들 각각으로부터 후보 IID 거리를 수신하여 다음 학습 UE를 결정하고,상기 결정된 다음 학습 UE에 기초한 스케줄링 정보를 상기 연합 학습에 참여하는 상기 복수의 UE들로 전송하고,및상기 복수의 UE들로부터 상기 로컬 학습에 기초하여 학습된 로컬 모델을 획득하여 글로벌 모델을 업데이트하는,중앙 서버."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 명세서는, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 사용자 장치(User Equipment, UE) 동작 방법에 있어서, 중앙 서버로부터 글로벌 모델 복사본을 수신하는 단계, 글로벌 모델 복사본 에 기초하여 로컬 모델을 구축하고, 획득한 데이터를 통해 로컬 모델에 대한 학습을 수행하여 제1 로컬 모델을 획득하는 단계, 로컬 확산에 기초한 로컬 학습을 통해 제1 로컬 모델을 제2 로컬 모델로 업데이트하는 단계, 및 제2 로컬 모델을 중앙 서버로 전송하는 단계를 포함할 수 있다."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 명세서는 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법 및 장치에 대한 것으 로서, 보다 구체적으로는 인공지능 모델 취합 이전에 사용자들 간에 인공지능 모델을 주고받는 새로운 확산 전략을 포함하는 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법 및 장치에 대한 것 이다. 본 연구는 삼성미래기술육성사업(과제번호: SRFC-TE2103-02)의 지원을 받아 수행되었다."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "차세대 이동통신 시스템에서는 많은 통신 기기들이 큰 통신 용량을 요구하게 됨에 따라 기존 RAT (radio access technology)에 비해 향상된 서비스로 초광대역 (enhanced mobile broadband, eMBB) 서비스, 매시브 MTC (Machine Type Communications, mMTC) 서비스 및 초고신뢰도 저지연 통신 URLLC(Ultra-Reliable and Low Latency Communications, URLLC) 서비스의 요구사항을 만족시키기 위한 다양한 기술들이 제안되고 있다. 또한, 차세대 이동통신 시스템에서는 인공지능 또는 머신러닝이 고려될 수 있으며, 이에 기초하여 연합 학습 (Federated learning, FL)이 적용될 수 있다. 연합 학습은 중앙 집중식 학습의 개인 정보 유출 문제를 해결하는 새로운 학습 패러다임으로 개인 프라이버시 보호 기능, 분산 처리를 통한 기지국의 로드 감소 기능 및 기지국과 단말의 트래픽 감소 기능을 가질 수 있다. 연합 학습은 분산 학습 기술로 다수의 사용자가 보유한 분산 데이터 를 통해 모델을 학습하는 방법일 수 있다. 즉, 연합 학습은 다수 사용자의 분산 데이터를 사용할 수 있다. 여기 서, 다수의 사용자 중 특정 사용자로서 비 독립 항등 분포(Non-independent and identically distributed, Non-IID)의 특성을 가진 사용자가 존재하는 경우에는 연합 학습 성능이 저하될 수 있으며, 하기에서는 이를 개 선하는 방법에 대해 서술한다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허 제 10-2022-0168128 A"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서는 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법 및 장치에 대한 것이다. 본 명세서는 연합 학습 시스템의 성능 향상을 위해 인공지능 모델의 학습도(Degree of Learning, DoL)와 DoL의 항등 분포 근접에 대한 IID(independent and identically distributed) 거리를 활용하여 최적화를 수행하는 방 법 및 장치에 대한 것이다. 본 명세서는 연합 학습 시스템에 로컬 확산(local diffusion) 과정을 추가하여 성능 향상을 수행하는 방법 및 장치에 대한 것이다. 본 명세서는 로컬 확산 과정의 구체적인 동작 방법 및 장치에 대한 것이다."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 명세서의 일 실시예에 따라, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 사 용자 장치(User Equipment, UE) 동작 방법에 있어서, 중앙 서버로부터 글로벌 모델 복사본을 수신하는 단계, 글 로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 획득한 데이터를 통해 로컬 모델에 대한 학습을 수행하여 제1 로컬 모델을 획득하는 단계, 로컬 확산에 기초한 로컬 학습을 통해 제1 로컬 모델을 제2 로컬 모델로 업데 이트하는 단계, 및 제2 로컬 모델을 중앙 서버로 전송하는 단계를 포함할 수 있다. 또한, 본 명세서의 일 실시예에 따라, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 수 행하는 사용자 장치(User Equipment, UE)에 있어서, 메모리, 송수신부 및 메모리와 송수신부를 제어하는 제어부 를 포함하되, 제어부는, 중앙 서버로부터 글로벌 모델 복사본을 수신하고, 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 획득한 데이터를 통해 로컬 모델에 대한 학습을 수행하여 제1 로컬 모델을 획득하고, 로컬 확 산에 기초한 로컬 학습을 통해 제1 로컬 모델을 제2 로컬 모델로 업데이트하고, 및 제2 로컬 모델을 중앙 서버 로 전송할 수 있다. 또한, 본 명세서의 일 실시예에 따라, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위 한 중앙 서버 동작 방법에 있어서, 글로벌 모델 복사본을 연합 학습에 참여하는 복수의 UE들로 전송하는 단계로 서, 복수의 UE들 각각은 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 복수의 UE들 각각이 획득한 데 이터를 통해 로컬 모델에 대한 학습을 수행하고, 로컬 학습에 기초하여 복수의 UE들 각각으로부터 후보 IID 거 리를 수신하여 다음 학습 UE를 결정하는 단계, 결정된 다음 학습 UE에 기초한 스케줄링 정보를 연합 학습에 참 여하는 복수의 UE들로 전송하는 단계 및 복수의 UE들로부터 로컬 학습에 기초하여 학습된 로컬 모델을 획득하여 글로벌 모델을 업데이트하는 단계를 포함할 수 있다. 또한, 본 명세서의 일 실시예에 따라, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 수 행하는 중앙 서버에 있어서, 메모리, 송수신부 및 메모리와 송수신부를 제어하는 제어부를 포함하되, 제어부는 글로벌 모델 복사본을 연합 학습에 참여하는 복수의 UE들로 전송하되, 복수의 UE들 각각은 글로벌 모델 복사본 에 기초하여 로컬 모델을 구축하고, 복수의 UE들 각각이 획득한 데이터를 통해 로컬 모델에 대한 학습을 수행하 고, 로컬 학습에 기초하여 복수의 UE들 각각으로부터 후보 IID 거리를 수신하여 다음 학습 UE를 결정하고, 결정 된 다음 학습 UE에 기초한 스케줄링 정보를 연합 학습에 참여하는 복수의 UE들로 전송하고, 및 복수의 UE들로부 터 로컬 학습에 기초하여 학습된 로컬 모델을 획득하여 글로벌 모델을 업데이트할 수 있다. 또한, 다음의 사항들은 공통으로 적용될 수 있다. 본 명세서의 일 실시예에 따라, UE는 중앙 서버에 기초하여 연합 학습에 참여하는 UE일 수 있다. 또한, 본 명세서의 일 실시예에 따라, 로컬 학습이 제1 확산 라운드(diffusion round)에서 수행되는 경우, UE는 DoL(Degree of Learning)을 중앙 서버에 기초하여 연합 학습에 참여하는 복수의 UE들로 전송하되, DoL은 로컬 모델에 기초하여 획득한 데이터를 통해 학습된 누적 데이터 분포를 나타낼 수 있다. 또한, 본 명세서의 일 실시예에 따라, DoL은 브로드캐스팅 방식에 기초하여 연합 학습에 참여하는 복수의 UE들 로 전송될 수 있다. 또한, 본 명세서의 일 실시예에 따라, DoL은 업링크 제어 채널을 통해 중앙 서버로 전송되고, 중앙 서버가 연합 학습에 참여하는 복수의 UE들로 다운링크 제어 채널을 통해 DoL을 전송할 수 있다. 또한, 본 명세서의 일 실시예에 따라, UE가 연합 학습에 참여하는 복수의 UE들 각각으로부터 DoL을 수신하고, DoL 각각에 제1 로컬 모델의 데이터 셋 상태 정보(Data State Information, DSI)를 반영하여 예비 DoL(preliminary DoL)을 생성하고, 예비 DoL에 기초하여 제1 후보 IID(Independent and Identically Distributed) 거리를 도출하여 중앙 서버로 전송할 수 있다. 또한, 본 명세서의 일 실시예에 따라, 중앙 서버는 제2 확산 라운드에서 도출된 제2 후보 IID와 제1 후보 IID 차이 값을 도출하여 입찰 가격 정보를 도출하되, 제2 확산 라운드는 제1 확산 라운드 이전의 확산 라운드이고, 중앙 서버는 연합 학습에 참여한 복수의 UE들 각각에 대한 입찰 가격 정보를 도출하여 UE 평가를 통해 다음 학 습 UE를 결정할 수 있다. 또한, 본 명세서의 일 실시예에 따라, 중앙 서버는 복수의 UE들 각각에 대한 채널 상태 정보(Channel State Information, CSI)를 더 고려하여 UE 평가를 통해 다음 학습 UE를 결정할 수 있다. 또한, 본 명세서의 일 실시예에 따라, UE가 다음 학습 UE로 결정된 경우, UE는 D2D(device to device) 방식에 기초하여 연합 학습에 참여한 복수의 UE들로 제1 로컬 모델을 전송할 수 있다. 또한, 본 명세서의 일 실시예에 따라, UE가 다음 학습 UE로부터 다음 학습 UE의 로컬 모델을 D2D 방식에 기초하 여 수신하는 경우, UE는 다음 학습 UE의 로컬 모델에 기초하여 학습을 수행하여 제2 로컬 모델을 획득할 수 있 다."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 명세서는 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법을 제공하는 효과가 있 다. 본 명세서는 연합 학습 시스템의 성능 향상을 위해 인공지능 모델의 학습도(Degree of Learning, DoL)와 DoL의 항등 분포 근접에 대한 IID(independent and identically distributed) 거리를 활용하여 최적화를 수행함으로 써 성능이 향상되도록 하는 효과가 있다. 본 명세서는 연합 학습 시스템에 로컬 확산(local diffusion) 과정을 추가하여 성능이 향상되도록 하는 효과가 있다. 본 명세서는 로컬 확산 과정의 구체적인 동작 방법을 제공하는 효과가 있다. 본 명세서에서 얻을 수 있는 효과는 이상에서 언급한 효과들로 제한되지 않으며, 언급하지 않은 또 다른 효과들"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 2, "content": "은 아래의 기재로부터 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것 이다."}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 발명에 따른 바람직한 실시 형태를 첨부된 도면을 참조하여 상세하게 설명한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 발명의 예시적인 실시형태를 설명하고자 하는 것이며, 본 발명이 실시될 수 있 는 유일한 실시형태를 나타내고자 하는 것이 아니다. 이하의 상세한 설명은 본 발명의 완전한 이해를 제공하기 위해서 구체적 세부사항을 포함한다. 그러나, 당업자는 본 발명이 이러한 구체적 세부사항 없이도 실시될 수 있 음을 안다. 이하의 실시예들은 본 발명의 구성요소들과 특징들을 소정 형태로 결합한 것들이다. 각 구성요소 또는 특징은 별도의 명시적 언급이 없는 한 선택적인 것으로 고려될 수 있다. 각 구성요소 또는 특징은 다른 구성요소나 특 징과 결합되지 않은 형태로 실시될 수 있다. 또한, 일부 구성요소들 및/또는 특징들을 결합하여 본 발명의 실시 예를 구성할 수도 있다. 본 발명의 실시예 들에서 설명되는 동작들의 순서는 변경될 수 있다. 어느 실시예의 일 부 구성이나 특징은 다른 실시예에 포함될 수 있고, 또는 다른 실시예의 대응하는 구성 또는 특징과 교체될 수있다. 이하의 설명에서 사용되는 특정 용어들은 본 발명의 이해를 돕기 위해서 제공된 것이며, 이러한 특정 용어의 사 용은 본 발명의 기술적 사상을 벗어나지 않는 범위에서 다른 형태로 변경될 수 있다. 몇몇 경우, 본 발명의 개념이 모호해지는 것을 피하기 위하여 공지의 구조 및 장치는 생략되거나, 각 구조 및 장치의 핵심기능을 중심으로 한 블록도 형식으로 도시된다. 또한, 본 명세서 전체에서 동일한 구성요소에 대해 서는 동일한 도면 부호를 사용하여 설명한다. 또한, 본 명세서에서 제1 및/또는 제2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로 부터 구별하는 목적으로만, 예컨대 본 명세서의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제1 구성요소 는 제2 구성요소로 명명될 수 있고, 유사하게, 제2 구성요소는 제1 구성요소로도 명명될 수 있다. 또한 명세서 전체에서, 어떤 부분이 어떤 구성 요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성 요소를 제외하는 것이 아니라 다른 구성 요소를 더 포함할 수 있는 것을 의미한다. 그리고 명세서 에 기재된 \"…유닛\", \"…부\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드 웨어 및/또는 소프트웨어의 결합으로 구현될 수 있다. 도 1은 본 개시에 적용되는 네트워크 환경을 나타낸 도면이다. 도 1을 참조하면, 장치들(110, 120, 130, 140)은 네트워크를 통해 연결될 수 있다. 일 예로, 장치들(110, 120, 130, 140)은 적어도 하나 이상의 장치 또는 서버로 무선 네트워크를 통해 연결될 수 있으며, 이를 통 해 데이터를 송수신할 수 있다. 또한, 장치들(110, 120, 130, 140, 또는 사용자 기기)은 이동성 장치이거나, 고 정형 장치일 수 있다. 구체적으로, 장치들(110, 120, 130, 140)은 스마트폰(smart phone)이나 태플릿, 웨어러블 디바이스와 같은 이동성 장치일 수 있다. 또한, 장치들(110, 120, 130, 140)은 컴퓨터, 노트북 및 PC와 같은 고 정형 장치일 수 있다. 또 다른 일 예로, 장치들(110, 120, 130, 140)은 IoT(internet of things) 디바이스, VR(virtual reality)/AR(augmented reality) 디바이스 및 그 밖의 장치일 수 있으며, 특정 실시예로 한정되지 않는다. 또한, 서버는 하나 이상의 장치와 네트워크를 통해 연결되어 콘텐츠나 서비스를 제공하는 기능을 구비한 장치일 수 있다. 구체적인 일 예로, 서버는 네트워크를 통해 접속하는 적어도 하나 이상의 장치로 서비스 를 제공하거나 요청에 따른 응답을 제공할 수 있다. 여기서, 서버는 적어도 하나 이상의 장치 각각에 설치된 소 프트웨어 또는 어플리케이션에 기초하여 연동될 수 있으며, 이를 통해 서비스를 제공할 수 있다. 도 2는 본 개시에 적용되는 장치 구성을 나타낸 도면이다. 도 2를 참조하면, 장치는 제어부, 송수신 부 및 메모리을 포함할 수 있다. 또한, 장치는 상술한 구성 이외의 다른 구성을 더 포함하는 것 도 가능할 수 있다. 도 2의 장치는 다른 장치와 통신을 통해 데이터를 송수신할 수 있으며, 사용자 기기(user equipment, UE)일 수 있다. 일 예로, 장치는 스마트폰, 스마트패드, 노트북, PC 및 그 밖의 통 신이 가능한 장비들을 지칭할 수 있으며, 특정 장치로 한정되는 것은 아닐 수 있다. 일 예로, 장치의 제어부는 장치의 구동 또는 동작과 관련된 명령어를 실행하고 처리하도록 구성 될 수 있다. 제어부는 송수신부 및 그 밖의 구성을 제어하는 논리적인 엔티티일 수 있으며, 특정 실 시예로 한정되지 않는다. 장치의 제어부는 산술, 로직 및 입출력 연산을 수행함으로써, 컴퓨터 프로 그램의 명령을 처리하도록 구성될 수 있다. 일 예로, 명령은 메모리에 저장되거나 송수신부를 통해 획득되는 신호에 기초하여 제어부에 제공될 수 있으며, 제어부는 이에 기초한 동작을 수행할 수 있다. 송수신부는 네트워크를 통해 다른 장치 및 서버 중 적어도 어느 하나와 통신을 위한 기능을 제공할 수 있다. 일 예로, 다른 장치도 제어부, 송수신부 및 메모리를 포함할 수 있다. 또한, 다 른 장치도 상술한 스마트 디바이스나 PC 및 그 밖의 통신이 가능한 장치일 수 있으며, 특정 실시예로 한정 되지 않는다. 또한, 일 예로, 장치는 입력부(미도시) 및 출력부(미도시)를 포함할 수 있다. 여기서, 입력부는 키보드, 마우스, 터치패드, 카메라 및 그 밖의 입력 신호를 제공하는 구성일 수 있으며, 출력부는 디스플레이나 스피커 및 그 밖의 출력 신호를 제공하는 구성일 수 있다. 다만, 이에 한정되는 것은 아닐 수 있다. 등의 장치를, 그리 고 외부 출력 장치는 디스플레이, 스피커, 햅틱 피드백 디바이스(haptic feedback device) 등과 같은 장치를 포함할 수 있다. 메모리는 비-일시적인 컴퓨터 판독 가능한 기록매체로서, RAM(random access memory), ROM(read only memory), 디스크 드라이브, SSD(solid state drive), 플래시 메모리(flash memory)와 같은 비소멸성 대용량 저 장 장치(permanent mass storage device)를 포함할 수 있으며, 특정 형태로 한정되지 않는다. 또한, 메모리 에는 장치의 구동 또는 동작과 관련된 명령어나 프로그램 코드를 저장할 수 있다. 또한, 메모리(21 3)에는 장치의 운영체제나 기타 소프트웨어들이 저장될 수 있다. 또한, 일 예로, 장치는 별도의 컴퓨 터에서 판독 가능한 기록매체로부터 로딩되는 소프트웨어를 이용할 수 있다. 여기서, 컴퓨터에서 판독 가능한 기록매체는 플로피 드라이브, 디스크, 테이프, DVD/CD-ROM 드라이브, 메모리 카드 및 그 밖의 기록매체를 포함 할 수 있으며, 특정 실시예로 한정되지 않는다. 하기에서는 도 2의 장치(210, 220)로서 사용자 기기(user equipment, UE)를 기준으로 서술한다. 하기에서 각각 의 UE는 기지국 또는 중앙 서버와 통신을 수행할 수 있으며, 인공지능 모델은 기지국 또는 중앙 서버에 구축되 어 각각의 UE들로 공유될 수 있다. 일 예로, 기지국 또는 중앙 서버는 5G 통신 시스템의 기지국 또는 차세대 통 신 시스템으로 6G 기지국이거나 그 밖의 다른 형태의 노드 또는 서버일 수 있다. 즉, 기지국 또는 중앙 서버는 UE와 연결되어 연합 학습에 기초하여 인공지능 모델을 학습하는 엔티티일 뿐 특정 형태로 한정되는 것은 아닐 수 있다. 하기에서는 설명의 편의를 위해 중앙 서버로 지칭한다. 다만, 이는 설명의 편의를 위한 명칭일 뿐 이 에 한정되는 것은 아니며, 다른 형태일 수 있다. 무선 통신망 또는 무선 근거리 통신망 (Wireless local area network, WLAN)에서 복수의 UE에 기초하여 하나의 인공지능 모델을 공동으로 학습시키는 연합 학습(Federated learning, FL) 시스템이 적용될 수 있다. 여기서, UE는 스마트폰뿐만 아니라 인공지능 학습을 수행할 수 있는 컴퓨팅 능력을 구비하고, 개인화된 데이터를 생산하 여 네트워크에 접속할 수 있는 모든 단말을 포함할 수 있으며, 상술한 도 2의 장치들 각각이 UE에 대응될 수 있 다. 여기서, 일 예로, 인공지능 모델은 개인화 데이터를 사용하는 인공지능 모델일 수 있다. 구체적인 일 예로, 인 공지능 모델은 데이터를 C개의 클래스로 분류하는 분류 문제(C-class classification problem)를 기반으로 학습 될 수 있다. 일 예로, C개의 클래스 분류 문제는 입력 데이터를 C개의 클래스 중 하나에 포함되는 것을 예측하 는 학습을 의미할 수 있으며, C는 클래스의 개수일 수 있다. 여기서, C개의 클래스 분류 문제를 기반으로 동작 하는 인공지능 모델은 입력 데이터를 분석하여 각 클래스에 속할 확률을 예측하는 함수를 학습할 수 있으며, 상 술한 FL에 의해 학습이 수행될 수 있다. 다만, C개의 클래스로 분류하는 분류 문제는 하나의 일 예일 뿐, 이에 한정되지 않고 다른 인공지능 모델에도 적용될 수 있다. FL은 중앙 서버의 인공지능 모델로서 글로벌 모델(global model)을 복사한 후 복수의 UE에게 전송하고, 각 UE는 자신의 데이터를 활용하여 각자 수신한 글로벌 모델에 대한 학습을 수행할 수 있다. 보다 구체적으로, 복수의 UE들은 글로벌 모델에 기초하여 각각에 구축된 로컬 모델(local model)에 대한 학습을 자신의 데이터를 통해 수행할 수 있다. 그 후, 복수의 UE 각각은 학습된 로컬 모델에 대한 정보를 중앙 서버로 전송할 수 있다. 중앙 서버는 복수의 UE 각각으로부터 수신한 로컬 모델에 기초하여 글로벌 모델을 업데이트할 수 있다. FL은 상술한 일련의 과정을 반복하여 UE(또는 사용자)로부터 데이터 수집을 수행하지 않고, 글로벌 모 델을 학습시킬 수 있다. 일 예로, 도 3은 본 개시에 적용 가능한 기존 인공지능 모델 학습 패러다임을 나타낸 도면이고, 도 4는 본 개시 에 적용 가능한 연합 학습 패러다임을 나타낸 도면이다. 도 3을 참조하면, 인공지능 모델은 중앙 서버(또 는 기지국, 330)에 구축될 수 있다. 중앙 서버는 복수의 UE들 각각으로부터 사용자 데이터를 수 집하고, 이에 기초하여 인공지능 모델을 학습할 수 있다. 여기서, 일 예로, 기존 인공지능 모델 학습 패러 다임은 중앙 서버에서 사용자 데이터를 수집하여 검수를 수행하는 과정에서 독립 항등 분포 (Independent and identically distributed, IID) 데이터 셋을 형성하고, 이에 기초하여 인공지능 모델을 학습 할 수 있다. 다만, 상술한 방식은 중앙 서버 관리 주체에 의해 사용자의 데이터 프라이버시가 침해될 수 있고 사용자 데이터의 수집 과정에서 막대한 통신 비용이 발생하는 문제점이 존재한다. 반면, 도 4를 참조하면, 연학 학습 패러다임에서 중앙 서버는 구축한 인공지능 모델을 복수의 UE들에게 공유하고, 복수의 UE 각각은 중앙 서버로부터 수신한 인공지능 모델에 기초하여 로컬 모델을 구축하여 각자 학습을 수행할 수 있다. 그 후, 복수의 UE들은 학습을 수행한 결과 정보를 중 앙 서버로 전송하고, 중앙 서버는 각각의 UE로부터 수신한 학습 결과 정보를 통해 인공지능 모델을 업데이트할 수 있다. 즉, 중앙 서버가 복수의 UE 각각에 대한 사용자 데이터를 직접 획득하는 것이 아니므로 사용자의 데이터 프라이버시가 보호되고, 중앙 서버와 복수의 UE 상호 간의 트래픽을 줄일 수 있다. 다만, 복수의 UE들 각각은 각자의 목적 및 성향에 기초하여 사용자 데이터를 획득할 수 있다. 즉, 복 수의 UE들에 대한 사용자 데이터를 비교해보면 특정 클래스에 편향되어 있거나 생산하는 데이터의 크 기 자체가 매우 적은 비 독립 항등 분포 (non-IID)의 특징을 갖는 UE가 존재할 수 있다. 여기서, non-IID 데이 터 셋에 기초하여 로컬 학습을 수행하는 UE가 중앙 서버로 전송하는 학습 결과 정보는 중앙 서버의 인공지능 모델로 글로벌 모델의 학습 성능을 크게 저하시키는 원인이 될 수 있다. 보다 구체적으로, 인공지능 모델의 학습은 학습 데이터 셋과 인공지능 모델의 예측 간 오차를 최소화하는 최적화를 통해 진행된다. 여기서, 각 UE의 학습 결과에 대한 차이가 큰 경우, 중앙 서버가 학습 결과 정보 를 통해 인공지능 모델을 학습하면 오차를 최소화하는 최적화가 불가능할 수 있으며, 이에 따라 성능이 저 하될 수 있다. 일 예로, 도 5는 본 개시에 적용 가능한 개인화 데이터가 학습 성능을 저하시키는 동작을 나타낸 도면이다. 도 5(a)를 참조하면, 중앙 서버는 복수의 UE 각각으로부터 로컬 모델 학습 정보를 획득할 수 있다. 중앙 서버 는 복수의 UE들의 로컬 모델 학습 정보에 기초하여 오차를 최소화하는 최적화 동작을 통해 인공지능 모델 을 업데이트할 수 있다. 즉, 로컬 모델 학습 정보에 기초하여 오차가 최소화 되는 인공지능 모델이 도출될 수 있다. 여기서, 로컬 모델 학습 정보가 독립 항등 분포(IID)를 이루고 있다면 상술한 최적화가 수행되는 데 성능 저하가 적을 수 있다. 반면, 도 5(b)를 참조하면, 로컬 모델 학습 정보가 non-IID를 이루고 있다 면 상술한 최적화가 수행되는데 성능 저하가 발생할 수 있다. 하기에서는 상술한 바를 고려하여 FL 시스템에서 UE의 사용자 데이터의 non-IID 특성으로 인한 학습 성능 저하 문제를 해결하는 방안에 대해 서술한다. 구체적으로, non-IID 특성으로 인한 학습 성능 저하 문제는 각 UE에서 사용자 데이터를 활용한 학습 과정에서 모델의 학습 경로를 최적점으로 갈 수 있도록 교정함으로써 수행될 수 있다. 즉, 중앙 서버가 인공지능 모델로서 각 UE들로부터 로컬 모델을 취합하기 이전에 로컬 모델들이 다른 UE 의 사용자 데이터를 학습하여 학습 경로를 보정하도록 할 수 있다. 즉, 다른 UE들의 사용자 데이터(또는 데이터 셋)을 학습할 수 있도록 인공지능 모델 취합 이전에 로컬 모델들이 확산(Federated diffusion, 이하 FedDif)될 수 있으며, 하기에서는 이에 대한 구체적인 방법에 대해 서술한다. 일 예로, 기존 인공지능 학습 패러다임에서 확률적 경사 하강법 (stochastic gradient descent, SGD)에 기초하 여 최적화가 수행될 수 있다. SGD는 최적화 알고리즘 중 하나로 손실 함수의 기울기를 이용하여 모델의 가중치 를 조정하는 방법일 수 있다. SGD는 무작위로 선택된 하나의 데이터 샘플에 대해서만 손실 함수의 기울기를 계 산하여 모델의 가중치를 업데이트하는 방식으로 전체 데이터 셋이 아닌 일부 샘플에 대해서만 계산을 수행하기 때문에 빠른 속도로 모델을 학습할 수 있다. SGD는 대규모 데이터 셋에서 미니 배치 크기(mini-batch size)에 기초하여 구성되는 데이터 샘플을 사용할 수 있으며, 미니 배치 크기를 조정하여 학습 속도와 정확도를 조절할 수 있다. 일 예로, 기존 인공지능 학습 패러다임에서는 하나의 IID 데이터 셋을 데이터 샘플들로 구성되는 미니 배치 (mini-batch)로 나누고 각 미니 배치 당 최적화를 한 번만 수행할 수 있다. SGD 기반 최적화는 최적화 성 능에 있어서 일부 손해가 발생할 수 있지만 학습 속도와 계산 복잡도가 계산될 수 있다. 하기에서는 상술한 SGD 방식을 고려하여 연합 학습의 모델을 업데이트하는 방안에 대해 서술한다. 일 예로, 연 합 학습에서 각각의 UE(또는 사용자)를 하나의 미니 배치로 취급할 수 있다. 여기서, UE는 인공지능 모델에 대 해 자신이 학습한 누적 데이터 분포가 항등 분포가 되도록 할 필요성이 있다. 일 예로, UE는 자신이 학습한 누 적 데이터 분포에 다른 특정 UE의 로컬 모델을 적용함으로써 학습한 누적 데이터 분포가 항등 분포가 되도록 할 수 있다. 이를 위해, 학습을 수행한 복수의 UE 중 특정 UE를 찾아 인공지능 모델을 효율적으로 확산시키는 FedDif 동작이 필요할 수 있으며, 이와 관련하여 서술한다. 일 예로, FedDif에 기초하여 로컬 모델들이 시스템 내에서 확산될수록 로컬 모델에 대한 전송 횟수는 증가할 수 있으며, 해당 동작에 의해 학습을 수행하는 과정에서 발생하는 통신 비용의 선형적으로 증가할 수 있다. 따라서, 단일 통신에서 사용되는 통신 비용과 모델의 전송 횟수를 최소화할 필요가 있다. 일 예로, 단일 통신에서 사용되는 통신 비용을 최소화하기 위해서 근거리 통신망(WLAN)에서 단말 간 통신 (Device-to-device communications, D2D communications)에 기초하여 로컬 모델 전송이 수행될 수 있다. 여기 서, 근거리 통신망은 로컬 영역 네트워크에 기초하여 수행되는 통신을 수 있으며, WiFi나 그 밖의 기술에 기초하여 통신이 수행될 수 있다. 일 예로, 로컬 모델 정보는 근거리 통신망에 기초하여 단말 상호 간에 교환될 수 있다. 또 다른 일 예로, 무선 통신망에 기초하여 D2D 통신이 수행될 수 있다. D2D 통신은 사이드링크, V2X 및 그 밖의 명칭으로 지칭될 수 있으나, 특정 형태로 한정되지 않는다. D2D 통신은 무선 통신망으로 5G 통신에 기 초하여 단말 상호 간의 통신을 지원할 수 있다. 또한, 일 예로, 차세대 통신 시스템에서도 D2D 통신 또는 사이 드링크 통신에 기초하여 단말 상호 간의 데이터 교환이 가능할 수 있으며, 특정 형태로 한정되지 않는다. 즉, UE가 학습한 누적 데이터 분포가 항등 분포가 되도록 다른 UE의 로컬 모델 정보를 수신하기 위해 로컬 모델 정보가 단말 상호 간에 교환될 수 있다. 여기서, 인공지능 모델의 전송 횟수 최소화를 위해 UE에서 인공지능 모 델에 기초하여 자신이 학습한 누적 데이터 분포를 나타내는 학습도 (Degree of learning, DoL) 및 DoL이 항등 분포에 얼마나 근접하였는지를 나타내는 IID 거리 (IID distance)가 활용될 수 있다. 일 예로, 확산 효율 (diffusion efficiency)은 FL에 참여하는 모든 UE들에서 발생하는 통신 비용 대비 IID 거리 의 감소율을 의미할 수 있다. 상술한 바에 기초하여 경매 이론을 통해 시스템 내에서 확산 효율을 최대화 할 수 있는 다음 학습자(next trainer) UE를 찾는 옥션을 고안할 수 있다. 경매 이론은 입찰에 참여하는 다수의 주체 가 존재하고, 다수의 주체 중 최상의 결과를 찾을 수 있도록 하는 옥션을 통해 특정 주체를 선정하도록 하는 방 식일 수 있다. 일 예로, 옥션에서 인공지능 모델에 대한 각 UE의 평가(valuation)는 학습율에 기초하여 IID 거 리의 감소율에 기반으로 결정될 수 있다. 여기서, 각 UE들은 자신의 데이터 셋이 각 인공지능 모델의 DoL에 대 한 IID 거리를 얼마나 감소시킬 수 있는지 여부를 수치화하여 중앙 서버로 입찰할 수 있다 중앙 서버는 UE 간 무선 채널 상태 정보 (channel state information, CSI)와 입찰 정보를 활용하여 확산 효율 을 계산하고, 인공지능 모델별로 확산 효율을 최대화하는 다음 학습 UE를 선정할 수 있다. 그 후, 선정된 다음 학습 UE 정보를 활용하여 다른 UE들에게 확산 설정 (diffusion configuration)을 수행하는 방식으로 확산 효율 을 최대화 할 수 있다. 상술한 바에 기초하여 FedDif는 FL에서 인공지능 모델의 학습 및 취합 과정에서 발생할 수 있는 학습의 최적 경 로 이탈 현상을 방지할 수 있다. 구체적으로, 학습의 최적 경로 이탈은 인공지능 모델의 가중치가 최적화가 진 행된 모델의 가중치와 비교하여 발산 가중치에 따라 발산 문제가 발생하는 경우일 수 있다. 여기서, FedDif는 non-IID 데이터 셋들의 조합을 통해 IID 데이터 셋을 생성할 수 있다는 점을 고려하여 IID 거 리를 활용하여 인공지능 모델들의 현재 DoL에서 IID 거리를 최소화하는 다음 학습 UE를 선출하는 일련의 과정일 수 있다. 상술한 바에 기초하여 FedDif는 non-IID 데이터 셋으로 인해 발생할 수 있는 가중치 발산 문제를 해결 할 수 있다. 일 예로, 도 6은 본 개시에 적용 가능한 non-IID 데이터 셋을 갖는 UE들을 대상으로 IID 거리를 최소화하여 확 산 효율을 최소화하는 방법을 나타낸 도면이다. 도 6(a) 및 도 6(b)를 참조하면, 확산 라운드가 진행되면 IID 거리(IID distance)와 확산 효율(diffusion efficiency)이 0으로 수렴하는지 여부에 대한 그래프이다. 여기서, 도 6(a) 및 도 6(b)의 시뮬레이션 환경에서 UE의 수는 10이고, 반지름 250m의 원형 시스템에 고르게 분포할 수 있다. 다만, 이는 설명의 편의를 위한 하나의 일 예일 뿐 이에 한정되는 것은 아닐 수 있다. 일 예로, UE의 데이터 셋은 CIFAR-10 데이터 셋을 디리슐레 분포 (Dirichlet distribution)를 기반으로 10조각 의 non-IID 데이터 셋으로 나누어 할당될 수 있다. 또한, 무선 채널 및 프레임 모델링은 5G 표준에 명시되어 있 는 채널 모델 및 numerology에 기초할 수 있으나, 이에 한정되는 것은 아닐 수 있다. 또한, 중앙 서버와의 통신 과 D2D 통신에서 사용되는 송신 전력은 각각 40dBm과 23dBm으로 설정할 수 있다. 여기서, UE 간 모델 송수신은 D2D 통신을 통해 이루어지므로 거리에 따른 신호 세기 감쇠가 더 심할 수 있다. 다만, UE 간 거리는 중앙 서버 와 UE 사이의 거리보다 상대적으로 짧기 때문에 적은 송신 전력으로도 고품질의 통신을 사용할 수 있다. 일 예로, FedDif는 인공지능 모델의 학습을 위한 하이퍼 파라미터 튜닝 및 통신 파라미터를 고려하여 확산의 정 도를 조절할 수 있는 두 가지 파라미터를 포함할 수 있다. 첫 번째 파라미터는 최소 허용 IID 거리(minimum tolerable IID distance, )로 시스템 내에 UE들의 데이터가 각 클래스별로 다양하게 분포되어 있는 경우에 이 론적으로 IID 거리를 0으로 수렴시킬 수 있으나, 실제로는 오차가 존재할 수 있다. 따라서, 는 IID 거리가 줄어들면 확산을 멈추도록 하는 최소 임계 값일 수 있으며, 관리자에 의해 설정될 수 있다. 도 6(a) 및 도 6 (b)에 의하면 확산 라운드가 진행될수록 IID 거리와 확산 효율이 0으로 수렴할 수 있으며, 이에 기초하여 연합 학습 시스템의 성능 향상을 위한 효율적인 인공지능 모델 확산의 구체적인 방법에 대해서는 도 10에서후술한다. 또한, 일 예로, 도 7은 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 7을 참조하면, 가 작을수록 인공지능 모델의 학습 성능은 증가하지만 더 많은 확산이 반복되어 수행될 수 있다. 또한, 두 번째 파라미터는 최소 허용 서비스 품질 (Minimum tolerable quality of service, )일 수 있다. 상술한 바에서 확산을 위한 옥션을 진행한 후 다음 학습 UE가 시스템의 정 반대에 위치하게 되면 현재 UE와 다 음 학습자 단말 사이의 거리가 매우 멀 수 있다. 따라서, 통신 비용이 기하급수적으로 증가할 수 있다. 상술한 점을 고려하여 일정 수준 이상의 통신 품질을 보장하면서 모델을 확산시키기 위한 최소 임계 값이 필요할 수 있 으며, 관리자에 의해 설정될 수 있다. 일 예로, 도 8은 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 8(a) 및 도 8(b)를 참 조하면, 학습 성능의 차이는 작지만 이 클수록 최적화를 위한 확산 반복 수가 증가할 수 있다. 일 예로, 도 8의 시뮬레이션 환경에서는 시스템의 규모가 다소 작기 때문에 변화가 크기 않지만 시스템 규모가 증가하면 해당 파라미터에 대한 성능의 차이가 커질 수 있다. 또한, 일 예로, 데이터 셋의 non-IID에 대한 정도는 디리슐레 분포의 농도 모수 (Concentration parameter, )에 의해 결정되며 크기가 작을수록 IID 분포와 멀어질 수 있다. 도 9는 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 9(a) 및 도 9(b)를 참조하면, 가 작아질수록 학습 성능 자체는 매우 낮아 지고 목표 학습 성능을 달성하기 위한 통신 비용은 크게 증가할 수 있다. 다만, FedDif는 기본적인 FL보다 항상 높은 성능을 나타낼 수 있다. 일 예로, 기존 UE 간 인공지능 모델의 교환은 중앙 서버가 무작위로 진행할 수 있다. 여기서, 상술한 바에 기초 하여 가중치 발산 문제가 해결되지 않을 수 있다. 일 예로, IID 거리를 늘리는 사용자들만 시스템에 남아있는 경우, 기본 FL보다 중앙 서버의 인공지능 모델의 학습 경로를 최적 경로로부터 멀어지게 할 수 있다. 반면, FedDif는 확산 효율 최대화 문제를 해결하는 과정이기 때문에 최적 경로로부터 멀어지게 하는 모델의 확산은 수 행하지 않을 수 있다. 또 다른 일 예로, 기존에는 통신 비용을 고려하지 않고 중앙 서버에 로컬 모델을 전송하면 중앙 서버가 글로벌 모델을 취합하지 않고, 다시 랜덤하게 섞어서 UE들에게 전송할 수 있다. 다만, 상술한 학습 과정에 참여하는 UE 들과 참여하지 않는 UE들이 함께 스케줄링 되는 경우, 학습 과정에 참여하지 않는 UE들의 통신 품질을 저하될 수 있다. 반면, FedDif는 확산을 수행하면 가중치 발산 문제의 해결을 이론적으로 보장해줄 뿐만 아니라 통신 효율적으로 확산에 대한 최적화를 수행하므로 종래 기술보다 효율적으로 문제를 해결할 수 있다. 도 10은 본 개시에 적용 가능한 FedDif 동작 과정을 나타낸 도면이다. 도 10을 참조하면, 중앙 서버는 인공지능 모델에 기초하여 글로벌 모델의 복사본을 복수의 UE들로 브로드캐스팅할 수 있다. 일 예로, 중앙 서버가 글로벌 모델의 복사본을 복수의 UE들로 브로드캐스팅하는 과정은 글로벌 초기화(global initialization, 1010) 과정일 수 있다. 그 후, 복수의 UE들은 각자의 데이터 셋에 기초하여 학습을 수행한 로컬 모델 정보를 중앙 서버로 전 달할 수 있다. 즉, 중앙 서버는 복수의 UE들 각각으로부터 학습된 로컬 모델 정보를 취합할 수 있으며, 해당 과 정은 글로벌 취합(global aggregation, 1030) 과정일 수 있다. 여기서, 일 예로, 도 10에서 글로벌 초기화 과정과 글로벌 취합 과정 사이에 로컬 확산(local diffusion, 1020) 과정이 더 수행될 수 있다. 구체적으로, 중앙 서버가 글로벌 초기화, 로컬 확산 및 글로벌 취합 과정을 통해 인공지능 모델을 학습하는 과정은 일정한 주기에 기초하여 수행될 수 있다. 일 예로, 연합학습을 위해 T번의 통신 라운드 (communication round)가 진행될 수 있다. 도 10은 T번의 통신 라운드 중 t번째 통신 라운드일 수 있다. 다만, 이는 설명의 편의를 위한 하나의 일 예일 뿐, 이에 한정되는 것은 아닐 수 있다. 여기서, 일 예로, 하나의 통신 라운드에서 글로벌 초기화 과정이 수행된 후 로컬 확산 과정이 수행 될 수 있다. 로컬 확산의 목표는 FL의 기본 원칙인 사용자 프라이버시를 보존함과 동시에 최소의 확산으 로 통신 비용을 최소화하고 학습 성능을 최대화하는 것일 수 있다. 따라서, 로컬 확산을 구성하는 세부 과정들은 UE의 데이터 샘플 자체나 분포를 유추할 수 없도록 해야 할 필요성이 있다. 상술한 점을 고려하여, 세 부 과정은 UE의 데이터 분포가 아닌 모델의 학습 데이터의 누적 분포로 상술한 DoL을 활용할 수 있다. DoL은 인공지능 모델이 자신이 학습한 누적 데이터 분포를 나타내기 위한 학습도일 수 있으며, 이는 상술한 바와 같다. 로컬 확산을 고려한 송수신 단계들은 셀룰러 이동 통신을 위한 채널을 함께 사용하는 D2D 오버레이 모드 (overlay mode)를 통해 수행될 수 있다. 즉, UE는 중앙 서버(또는 기지국)과 통신을 수행하는 채널을 사용하여 D2D 통신을 수행할 수 있다. 또 다른 일 예로, D2D 통신은 중앙 서버(또는 기지국)에 의해 스케줄링되어 제어되 는 모드 또는 UE가 직접 스케줄링하여 제어하는 모드에 기초하여 동작할 수 있으나, 특정 형태로 한정되는 것은 아닐 수 있다. 여기서, FedDif는 데이터 셋 상태, 모델의 학습 정도, 통신을 위한 채널 상태 및 그 밖의 요소를 고려하여 수행 될 필요성이 있다. 일 예로, 도 10을 참조하면, 시스템 내에 중앙 서버 및 FL에 참여하는 복수의 UE들이 존재할 수 있다. 구체적인 일 예로, 시스템에 N대의 단말이 존재하고, 대의 단말이 FL에 참여하는 경우를 고려할 수 있다. 다만, 이는 설명의 편의를 위한 일 예일 뿐, 이에 한정되는 것은 아닐 수 있다. FL에 참여하는 i번째 단말의 데이터 셋은 확률 변수 의 확률밀도함수 에 의해 생성되며 로 나타낼 수 있다. 여기서, 데이터 셋의 크기는 이다. UE들 상호 간의 데이터 셋은 non-IID 특성을 갖기 때문에 단말 i와 j의 분포는 ≠ 를 만족한다. 단말 i의 데이터 상태 정보(Data state information, DSI)는 각 클 래스별 데이터 샘플의 비율이며 로 나타낼 수 있다. 연합 학습은 총 T번의 통신 라운드를 수행하고, t번 째 통신 라운드에서 로컬 확산은 총 번의 확산 라운드가 진행될 수 있다. 여기서, k번째 확산 라운드에서 로 컬 모델 m이 거쳐간 UE들의 집합은 확산 체인(diffusion chain)으로 하기 수학식 1과 같이 나타낼 수 있다. 수학식 1"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에서 는 k번째 확산 라운드에서 로컬 모델 m을 학습시키는 UE일 수 있다. 또한, 확산 체인 를 학습 시킨 모든 데이터 셋의 크기는 하기 수학식 2와 같을 수 있다. 또한, k번째 확산 라운드에서 로컬 모델 m 의 DoL은 하기 수학식 3과 같을 수 있다. 수학식 2"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "수학식 3 여기서, DoL도 DSI와 유사하게 로컬 모델 m이 k 확산 라운드들에서 학습한 모든 데이터 샘플의 비율일 수 있다. 여기서, DoL 의 IID 거리는 항등 확률 분포와의 차이일 수 있으며, 하기 수학식 4와 같이 Wasserstein-1 distance (Earth-mover distance)로 정의될 수 있다. 일 예로, Wasserstein-1 distance는 두 확률 분포 사이의 거리(distance)를 계산하는 방법일 수 있다. 다만, 이에 한정되는 것은 아니고, Kullback-Leibler divergence (KLD)나 Jenssen-Shannon distance (JSD)와 같은 다양한 함수도 활용될 수 있다. 일 예로, 상술한 바에 기초하 여 확률 분포와 항등 분포의 유사성만 계산될 필요성이 있다. 따라서, 유사도를 표현할 수 있는 함수는 적용 가 능할 수 있으며, 특정 형태로 한정되는 것은 아닐 수 있다. 일 예로, 수학식 4에서 μ와 는 항등 확률분포와 그에 대한 DSI일 수 있다. 수학식 4"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "다음으로, 도 10을 참조하면, 로컬 확산 과정은 DoL 브로드캐스팅(DoL broadcasting, 1021), 후보 IID 거리 보 고(candidates of IID distance reporting, 1022), 확산 구성(diffusion configuration, 1023) 및 모델 전송 과 이에 기초한 로컬 트레이닝 과정을 포함할 수 있다. 일 예로, DoL 브로드캐스팅 과정은 모든 UE 들이 자신이 학습시키던 로컬 모델의 DoL을 주변 UE들로 브로드캐스팅 방식에 기초하여 전송하는 과정일 수 있 다. 일 예로, 각각의 UE는 브로드캐스팅 방식에 기초하여 수신한 로컬 모델의 DoL에 기초하여 로컬 모델을 학습 시킬 때 IID 거리를 줄일 수 있는 UE를 찾을 수 있다. 일 예로, DoL 브로드캐스팅 과정은 비콘 메시지 (beacon message)나 그 밖의 숏 메시지에 기초하여 주변 UE들에게 DoL을 브로드캐스팅함으로써 수행될 수 있다. 또 다른 일 예로, 각각의 UE는 업링크 제어 채널(physical uplink control channel, PUCCH)을 통해 DoL을 중앙 서버(또는 기지국)로 전송하고, 중앙 서버가 DoL을 수집한 후 전체 모델에 대한 DoL 정보를 다운링크 제어 채널 (physical downlink control channel, PDCCH)를 통해 다시 브로드캐스팅할 수도 있다. 또 다른 일 예로, 중앙 서버는 FL에 참여한 UE들을 인지하고 있으므로 지정된 UE들과 통신을 수행하여 상술한 정보를 전달할 수 있으며, 특정 실시예로 한정되는 것은 아닐 수 있다. 즉, 각각의 UE들에 대해서 로컬 모델의 DoL이 FL에 참여한 적어도 하나의 UE들로 전달될 수 있다. 그 후, 후보 IID 거리 리포팅 과정에서 각 UE들은 전송받은 DoL에 자신의 데이터 셋에 대한 데이터셋 상태 정보(Data state information, DSI)를 반영하여 예비 DoL을 생성할 수 있다. 그 후, 각각의 UE들은 생성한 예비 DoL에 대해서 후보 IID 거리(Candidate of IID distance)를 계산하여 중앙 서버(또는 기지국)으로 전송할 수 있다. 중앙 서버는 이전 확산 라운드에서 계산된 DoL에 대한 IID 거리와 수신한 후보 IID 거리 차이를 도출하고, UE들에 대한 평가(valuation)를 수행할 수 있다. 그 후, 중앙 서버는 UE들에 대한 평가에 기초하여 옥션을 구성할 수 있으며, 이는 상술한 바와 같다. 확산 구성 과정에서는 옥션에 기초하여 다음 학습자(next trainer) UE를 결정하고, 로컬 모델 교환을 위 한 스케줄링이 수행될 수 있다. 구체적으로, 중앙 서버는 UE들에 대한 평가를 기반으로 입찰 가격(bidding price)을 결정하고, 셀룰러 이동 통신 시스템에서 수집된 채널 상태 정보 (Channel state information, CSI)와 함께 승자 선택 알고리즘 (Winner selection algorithm)을 수행할 수 있다. 일 예로, 하기 표 1은 승자 선택 알고리즘일 수 있다. 다만, 승자 선택 알고리즘은 하기 표 1에 한정되는 것은 아닐 수 있다. 구체적으로, 중앙 서버는 입찰 가격과 CSI를 활용하여 하기 수학식 5에 기초하여 확산 효율 (Diffusion efficiency)을 산출할 수 있다.표 1"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "수학식 5"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기서 입찰 가격은 하기 수학식 6과 같을 수 있으며, 수학식 5에서 는 수학식 7과 같을 수 있다. 일 예로, 는 CSI를 통해 산출할 수 있는 모델 전송에 필요한 대역폭일 수 있다. 수학식 6"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "수학식 7"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "수학식 7에서 는 로컬 모델 m을 (k-1) 확산 라운드에서 학습시킨 UE 와 k 확산 라운드에서 학 습시킨 UE 사이의 신호 대 잡음비 (Signal-to-noise-ratio, SNR)이며, CSI 정보와 동일할 수 있다. 일 예로, 중앙 서버는 산출한 확산 효율을 활용하여 헝가리안 알고리즘 (Hungarian algorithm, Kuhn-Munkres algorithm)을 통해 현재 UE와 다음 학습 UE 사이의 최적의 매칭을 산출할 수 있다. 그 후, 중앙 서버는 산출된 매칭과 기본적인 무선 자원 할당 알고리즘을 통해 확산을 위한 스케줄링을 수행할 수 있다. 모델 전송 과 정과 로컬 훈련 과정은 중앙 서버의 스케줄링을 기반으로 UE 간 로컬 모델 송수신이 수행되고, 각각의 UE에서 수신한 로컬 모델에 기초하여 학습이 수행되는 과정일 수 있다. 일 예로, DoL 브로드캐스팅, 후보 IID 거리 보고, 확산 구성은 매우 작은 크기의 제어 메시 지 전송이므로 전송 자원을 많이 사용하지 않으며, 통신 비용도 매우 작을 수 있다. 다만, 모델 전송 과 정에서는 로컬 모델 송수신이 수행되므로 전송 자원이 많이 사용될 수 있다. 상술한 점을 고려하여 확산 라운드 수를 최소화하는 방식이 필요할 수 있다. 일 예로, 하기 표 2는 확산 라운드 수를 최소화하는 FedDif의 전체적 인 동작 과정에 대한 알고리즘일 수 있으나, 이에 한정되는 것은 아닐 수 있다. 표 2 도 11은 본 개시에 적용 가능한 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확신 방법을 나타낸 순서도이다. 도 11을 참조하면, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 사용자 장치(User Equipment, UE)는 중앙 서버로부터 글로벌 모델 복사본을 수신할 수 있다.(S1110) 그 후, UE는 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 자신이 획득한 데이터를 통해 로컬 모델에 대 한 학습을 수행하여 제1 로컬 모델을 획득할 수 있다.(S1120) 즉, 제1 로컬 모델은 UE가 자신이 획득한 데이터 를 통해 학습을 수행한 로컬 모델을 지칭할 수 있다. 그 후, UE는 로컬 확산에 기초한 로컬 학습을 통해 제1 로 컬 모델을 제2 로컬 모델로 업데이트할 수 있다.(S1130) 일 예로, UE가 로컬 확산 과정에 기초하여 다음 학습 UE로 선정된 경우, 제1 로컬 모델과 제2 로컬 모델은 동일할 수 있다. 반면, UE가 다음 학습 UE로 선정되지 않 고, 다른 UE로부터 로컬 모델을 획득한 경우, 제1 로컬 모델은 다른 UE의 로컬 모델을 반영하여 제2 로컬 모델 로 업데이트될 수 있다. 즉, 제2 로컬 모델은 다음 학습 UE의 로컬 모델을 반영하여 학습된 로컬 모델일 수 있 다. 그 후, UE는 제2 로컬 모델을 중앙 서버로 전송할 수 있다.(S1140) 여기서, 일 예로, UE는 메모리, 송수신 부 및 메모리와 송수신부를 제어하는 제어부를 포함할 수 있으며, 제어부는 상술한 동작을 수행할 수 있다.일 예로, UE는 중앙 서버에 기초하여 연합 학습에 참여하는 UE일 수 있다. 또한, 로컬 학습이 제1 확산 라운드 (diffusion round)에서 수행되는 경우, UE는 DoL(Degree of Learning)을 중앙 서버에 기초하여 연합 학습에 참 여하는 복수의 UE들로 전송할 수 있다. 여기서, DoL은 로컬 모델에 기초하여 획득한 데이터를 통해 학습된 누적 데이터 분포를 나타낼 수 있으며, 상술한 바와 같다. 구체적인 일 예로, DoL은 브로드캐스팅 방식에 기초하여 연합 학습에 참여하는 복수의 UE들로 전송되거나, 업링 크 제어 채널을 통해 중앙 서버로 전송된 후 중앙 서버에 의해 연합 학습에 참여하는 복수의 UE들로 전송될 수 있으며, 이는 상술한 바와 같다. 그 후, UE는 연합 학습에 참여하는 복수의 UE들 각각으로부터 DoL을 수신하고, DoL 각각에 제1 로컬 모델의 데 이터 셋 상태 정보(Data State Information, DSI)를 반영하여 예비 DoL을 생성할 수 있다. 또한, UE는 예비 DoL에 기초하여 제1 후보 IID 거리를 도출하여 중앙 서버로 전송할 수 있다. 중앙 서버는 제2 확산 라운드에서 도출된 제2 후보 IID와 제1 후보 IID 차이 값을 도출하여 입찰 가격 정보를 도출할 수 있다. 여기서, 제2 확산 라운드는 제1 확산 라운드 이전의 확산 라운드일 수 있다. 중앙 서버는 연합 학습에 참여한 복수의 UE들 각각에 대한 입찰 가격 정보를 도출하여 UE 평가를 통해 다음 학 습 UE를 결정할 수 있다. 일 예로, 중앙 서버는 복수의 UE들 각각에 대한 채널 상태 정보(Channel State Information, CSI)를 더 고려하여 UE 평가를 통해 다음 학습 UE를 결정할 수 있으며, 이는 상술한 바와 같다. 여기서, UE가 다음 학습 UE로 결정된 경우, UE는 D2D(device to device) 방식에 기초하여 연합 학습에 참여한 복수의 UE들로 제1 로컬 모델을 전송할 수 있다. 반면, UE가 다음 학습 UE로 결정되지 않은 경우, UE는 다음 학 습 UE로부터 다음 학습 UE의 로컬 모델을 D2D 방식에 기초하여 수신할 수 있다. 그 후, UE는 다음 학습 UE의 로 컬 모델에 기초하여 학습을 수행하여 제2 로컬 모델을 획득할 수 있다. 도 12는 본 개시에 적용 가능한 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법을 나타낸 순서도이다. 도 12를 참조하면, 연합 학습(Federated Learning, FL) 시스템에서 인공지능 모델 확산을 위한 중앙 서버는 글 로벌 모델 복사본을 연합 학습에 참여하는 복수의 UE들로 전송할 수 있다.(S1210) 여기서, 복수의 UE들 각각은 글로벌 모델 복사본에 기초하여 로컬 모델을 구축하고, 복수의 UE들 각각이 획득한 데이터를 통해 로컬 모델에 대한 학습을 수행할 수 있다. 그 후, 로컬 확산에 기초하여 로컬 학습이 수행되는 경우, 중앙 서버는 로컬 학습에 기초하여 복수의 UE들 각각 으로부터 후보 IID 거리를 수신하여 다음 학습 UE를 결정할 수 있다.(S1220) 그 후, 중앙 서버는 결정된 다음 학습 UE에 기초한 스케줄링 정보를 연합 학습에 참여하는 복수의 UE들로 전송할 수 있다.(S1230) 그 후, 중앙 서버는 복수의 UE들로부터 로컬 학습에 기초하여 학습된 로컬 모델을 획득하여 글로벌 모델을 업데이트할 수 있 다.(S1240) 여기서, 중앙 서버는 메모리, 송수신부 및 메모리와 송수신부를 제어하는 제어부를 포함할 수 있으 며, 제어부는 상술한 동작을 수행할 수 있다. 일 예로, 중앙 서버는 연합 학습에 참여하는 복수의 UE 각각으로부터 후보 IID 거리를 획득하여 입찰 가격 정보 를 도출하고, 입찰 가격 정보와 CSI를 고려하여 UE 평가를 수행하여 다음 학습 UE를 결정할 수 있으며, 이는 상술한 바와 같다. 여기서, 다음 학습 UE는 D2D 방식에 기초하여 연합 학습에 참여한 복수의 UE들로 로컬 모델을 전송할 수 있다. 그 후, 중앙 서버는 글로벌 취합 과정에서 연합 학습에 참여한 복수의 UE들 각각으로부터 업데이트된 로컬 모델 을 획득하여 글로벌 모델을 업데이트할 수 있다. 상술한 바와 같이 본 발명의 일 실시예에 따르면, 연합 학습 시스템에서 Non-IID 데이터로 인한 가중치 발산 문 제 및 개인 정보 유출 문제를 해결하기 위해, 기계 학습(Machine learning, ML) 모델의 새로운 확산 전략 (Federated diffusion, FedDif)이 제시된다. FedDif에서 사용자 단말은 기기 간 (Device-to-device, D2D) 통신 을 통해 이웃 사용자 단말에게 로컬 모델을 전파한다. 이러한 FedDif는 로컬 모델이 매개변수 집계 전에 다른 분포를 경험할 수 있도록 한다. 또한 이론적으로 FedDif가 가중치 분산 문제를 피할 수 있음을 입증한다. 이론 적으로 경매 이론을 기반으로 학습 성능과 통신 비용 간의 트레이드-오프(trade-off)를 결정할 수 있는 ML 모델 의 통신 효율적인 확산 전략이 제시된다. 성능 평가 결과는 FedDif가 IID가 아닌 설정이 있는 기준선 FL에 비해 글로벌 모델의 테스트 정확도를 10.37% 향상시키는 것으로 나타났다. 또한 FedDif는 모델 압축 방식을 제외한 최신 방식에 비해 소모되는 서브프레임의 수를 1.28~2.85배 향상시켰다. FedDif는 또한 최신 방법에 비해 전송 된 모델의 수를 1.43~2.67배 향상시켰다. 상술한 본 발명의 실시예들은 다양한 수단을 통해 구현될 수 있다. 예를 들어, 본 발명의 실시예들은 하드웨어, 펌웨어(firmware), 소프트웨어 또는 그것들의 결합 등에 의해 구현될 수 있다. 하드웨어에 의한 구현의 경우, 본 발명의 실시예들에 따른 방법은 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 프로세서, 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 펌웨어나 소프트웨어에 의한 구현의 경우, 본 발명의 실시예들에 따른 방법은 이상에서 설명된 기능 또는 동작 들을 수행하는 모듈, 절차 또는 함수 등의 형태로 구현될 수 있다. 소프트웨어 코드는 메모리 유닛에 저장되어 프로세서에 의해 구동될 수 있다. 상기 메모리 유닛은 상기 프로세서 내부 또는 외부에 위치하여, 이미 공지된 다양한 수단에 의해 상기 프로세서와 데이터를 주고 받을 수 있다. 상술한 바와 같이 개시된 본 발명의 바람직한 실시형태에 대한 상세한 설명은 당업자가 본 발명을 구현하고 실 시할 수 있도록 제공되었다. 상기에서는 본 발명의 바람직한 실시 형태를 참조하여 설명하였지만, 해당 기술 분 야의 숙련된 당업자는 하기의 특허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다. 따라서, 본 발명은 여기에 나 타난 실시형태들에 제한되려는 것이 아니라, 여기서 개시된 원리들 및 신규한 특징들과 일치하는 최광의 범위를 부여하려는 것이다. 또한, 이상에서는 본 명세서의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 명세 서는 상술한 특정의 실시예에 한정되지 아니하며, 청구범위에서 청구하는 본 명세서의 요지를 벗어남이 없이 당"}
{"patent_id": "10-2023-0064097", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "해 발명이 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러 한 변형 실시들은 본 명세서의 기술적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다. 그리고 당해 명세서에서는 물건 발명과 방법 발명이 모두 설명되고 있으며, 필요에 따라 양 발명의 설명은 보충 적으로 적용될 수 있다."}
{"patent_id": "10-2023-0064097", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시에 적용되는 네트워크 환경을 도시한 도면이다. 도 2는 본 개시에 적용되는 장치 구성을 나타낸 도면이다. 도 3은 본 개시에 적용 가능한 기존 인공지능 모델 학습 패러다임을 나타낸 도면이다. 도 4는 본 개시에 적용 가능한 연합 학습 패러다임을 나타낸 도면이다. 도 5는 본 개시에 적용 가능한 개인화 데이터가 학습 성능을 저하시키는 동작을 나타낸 도면이다. 도 6은 본 개시에 적용 가능한 non-IID 데이터 셋을 갖는 UE들을 대상으로 IID 거리를 최소화하여 확산 효율을 최소화하는 방법을 나타낸 도면이다. 도 7은 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 8은 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 9는 본 개시에 적용 가능한 학습 성능과 통신 비용을 비교한 그래프이다. 도 10은 본 개시에 적용 가능한 FedDif 동작 과정을 나타낸 도면이다. 도 11은 본 개시에 적용 가능한 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확신 방법을 나타낸 순서도이다. 도 12는 본 개시에 적용 가능한 연합 학습 시스템의 성능 향상을 위한 통신 효율적 인공지능 모델 확산 방법을 나타낸 순서도이다."}
