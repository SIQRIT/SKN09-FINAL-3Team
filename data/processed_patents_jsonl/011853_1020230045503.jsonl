{"patent_id": "10-2023-0045503", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0062080", "출원번호": "10-2023-0045503", "발명의 명칭": "비디오 인페인팅을 수행하는 전자 장치 및 그 동작 방법", "출원인": "삼성전자주식회사", "발명자": "공내진"}}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치가 비디오 인페인팅(video inpainting)을 수행하는 방법에 있어서,타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 상기타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 상기 타겟 동영상의 복수의 프레임들의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계;를 포함하고,상기 인페인팅 모델은,제1 훈련(300) 전의 상기 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련(300) 전의 상기 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에 대하여 상기 제1 훈련(300)이 수행되고,상기 제1 훈련(300)이 수행된 상기 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 상기 제1 훈련(300)이수행된 상기 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여 제2 훈련(350)이 수행된 인공 지능 모델이고,상기 제1 훈련(300)은,제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 상기 제1 복원 대상 영역을 포함하는단일 훈련용 이미지를 획득하기 위한 훈련이며,상기 제2 훈련(350)은,제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 상기 제2 복원 대상 영역이 복원된 훈련용동영상을 획득하기 위한 훈련인, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서상기 타겟 동영상을 획득하는 단계는,상기 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상에기초하여 복수의 프레임들의 제1 토큰들을 획득하는 단계;상기 복수의 프레임들의 제1 토큰들을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 시-공간적어텐션이 수행된 상기 복수의 프레임들의 제2 토큰들을 획득하는 단계;상기 복수의 프레임들 각각의 제2 토큰들을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 공간적 어텐션이 수행된 상기 복수의 프레임들의 각각의 제3 토큰들을 획득하는 단계;상기 복수의 프레임들의 제3 토큰들에 기초하여 상기 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항 내지 제2항 중 어느 한 항에 있어서, 상기 인페인팅 모델은,상기 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 복수의 제3공개특허 10-2024-0062080-3-모듈들을 더 포함하고,상기 방법은,상기 복수의 프레임들의 제2 토큰들을 상기 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 상기 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 상기 복수의 프레임들의 제1 피처맵들을 획득하는 단계; 를 더 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 복수의 프레임들 각각의 제3 토큰을 획득하는 단계;를 포함하는,방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항 내지 제3항 중 어느 한 항에 있어서,상기 복수의 프레임들의 제1 피처맵들을 획득하는 단계는,상기 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 상기 복수의 프레임들의 제1 피처맵들 중 상기 제1 프레임의 제1 피처맵을 획득하는 단계;를 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 제1 피처맵 중 적어도 하나에 기초하여 상기 복수의 프레임들의 제3 토큰들 중 상기 제1 프레임의 제3 토큰들을 획득하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항 내지 제4항 중 어느 한 항에 있어서,상기 제1 프레임의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 상기 제1 프레임의 제4 토큰들을 획득하는 단계; 및상기 제1 프레임의 제4 토큰들 및 상기 제1 프레임의 제2 토큰들에 기초하여 상기 제1 프레임의 제3 토큰들을획득하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항 내지 제5항 중 어느 한 항에 있어서,상기 제1 프레임의 제3 토큰들을 획득하는 단계는,상기 제1 프레임의 제2 토큰들을 디토큰화(de-tokenization)하여 상기 제1 프레임의 제2 피처맵을 획득하는 단계;상기 제1 프레임의 제1 피처맵 및 상기 제1 프레임의 제2 피처맵에 기초하여 상기 제1 프레임의 제3 피처맵을획득하는 단계; 및 상기 제1 프레임의 제3 피처맵을 토큰화하여 상기 제1 프레임의 제3 토큰들을 획득하는 단계;를 포함하는,방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항 내지 제6항 중 어느 한 항에 있어서,상기 인페인팅 모델은,상기 제1 훈련 전의 상기 복수의 제1 모듈들 중 하나 이상 및 상기 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련 전의 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 상기 제1 훈련이 수행되고,공개특허 10-2024-0062080-4-상기 제1 훈련이 수행된 상기 복수의 제1 모듈들 중 하나 이상 및 상기 제1 훈련이 수행된 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 상기 제2 훈련이 수행된 인공 지능 모델인, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항 내지 제7항 중 어느 한 항에 있어서,상기 인페인팅 모델은,제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 제2 훈련이 수행된 인공 지능 모델이고,상기 제1 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이의 옵티컬 플로우(opticalflow)에 기초하여 계산되고,상기 제2 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산되는, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항 내지 제8항 중 어느 한 항에 있어서,상기 옵티컬 플로우는,상기 제2 훈련 전의 상기 복수의 제3 모듈들로부터 획득되는 상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득되는, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항 내지 제9항 중 어느 한 항에 있어서,상기 방법은,상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라의 움직임의 크기, 상기 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별하는단계;를 더 포함하고,상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계는,상기 카메라의 움직임의 크기, 상기 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도하나에 기초하여 제1 추론 모드 및 제2 추론 모드 중 하나를 선택하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 단계;를 포함하고,상기 제1 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵에 기초하여 상기 복수의프레임들 각각의 제3 토큰들을 획득하는 모드이고,상기 제2 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰들에 기초하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드인, 방법."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "비디오 인페인팅(video inpainting)을 수행하는 전자 장치에 있어서,하나 이상의 명령어들(instructions)을 저장하는 메모리; 및상기 메모리에 저장된 하나 이상의 명령어들을 실행하는 적어도 하나의 프로세서를 포함하며,공개특허 10-2024-0062080-5-상기 적어도 하나의 프로세서는,타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 상기타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 상기 타겟 동영상의 복수의 프레임의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하고,상기 인페인팅 모델은,제1 훈련(300) 전의 상기 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및, 단일 훈련용 이미지에 대하여공간적 어텐션을 수행하는 상기 제1 훈련(300) 전의 상기 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에대하여 상기 제1 훈련(300)이 수행되고,상기 제1 훈련(300)이 수행된 상기 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 상기 제1 훈련(300)이수행된 상기 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여 제2 훈련(350)이 수행된 인공 지능 모델이고,상기 제1 훈련(300)은,제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 상기 제1 복원 대상 영역을 포함하는단일 훈련용 이미지를 획득하기 위한 훈련이며,상기 제2 훈련(350)은,제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 상기 제2 복원 대상 영역이 복원된 훈련용동영상을 획득하기 위한 훈련인, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서상기 적어도 하나의 프로세서는,상기 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 상기 타겟 복원 대상 영역을 표시하는 마스크 동영상에기초하여 복수의 프레임들의 제1 토큰들을 획득하고,상기 복수의 프레임들의 제1 토큰들을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 시-공간적어텐션이 수행된 상기 복수의 프레임들의 제2 토큰들을 획득하고,상기 복수의 프레임들 각각의 제2 토큰들을 상기 복수의 제1 모듈들 중 적어도 하나에 적용함으로써, 상기 공간적 어텐션이 수행된 상기 복수의 프레임들 각각의 제3 토큰들을 획득하고,상기 복수의 프레임들의 제3 토큰들에 기초하여 상기 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항 내지 제12항 중 어느 한 항에 있어서, 상기 인페인팅 모델은,상기 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 복수의 제3모듈들을 더 포함하고,상기 적어도 하나의 프로세서는,상기 복수의 프레임들의 상기 제2 토큰들을 상기 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 상기 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 상기 복수의 프레임들의 제1피처맵들을 획득하고,상기 복수의 프레임들 각각의 제2 토큰들 및 상기 복수의 프레임들 각각의 제1 피처맵을 상기 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는, 전자 장치.공개특허 10-2024-0062080-6-청구항 14 제11항 내지 제13항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 상기 복수의 프레임들의 제1 피처맵들 중 상기 제1 프레임의 제1 피처맵을 획득하고,상기 제1 프레임의 제2 토큰들 및 상기 제1 프레임의 제1 피처맵 중 적어도 하나에 기초하여 상기 복수의 프레임들의 제3 토큰들 중 상기 제1 프레임의 제3 토큰들을 획득하는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항 내지 제14항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 상기 제1 프레임의 제4 토큰들을 획득하고,상기 제1 프레임의 제4 토큰들 및 상기 제1 프레임의 제2 토큰들에 기초하여 상기 제1 프레임의 제3 토큰들을획득하는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항 내지 제15항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 제1 프레임의 제2 토큰들을 디토큰화(de-tokenization)하여 상기 제1 프레임의 제2 피처맵을 획득하고,상기 제1 프레임의 제1 피처맵 및 상기 제1 프레임의 제2 피처맵에 기초하여 상기 제1 프레임의 제3 피처맵을획득하고,상기 제1 프레임의 제3 피처맵을 토큰화하여 상기 제1 프레임의 제3 토큰들을 획득하는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항 내지 제16항 중 어느 한 항에 있어서,상기 인페인팅 모델은,상기 제1 훈련 전의 상기 복수의 제1 모듈들 중 하나 이상 및 상기 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 상기 제1 훈련 전의 상기 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 상기복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 상기 제1 훈련이 수행되고,상기 제1 훈련이 수행된 복수의 제1 모듈들 중 하나 이상 및 상기 제1 훈련이 수행된 상기 복수의 제2 모듈들중 하나 이상으로부터 각각 획득되는 둘 이상의 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에기초하여 상기 제2 훈련이 수행된 인공 지능 모델인, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제11항 내지 제17항 중 어느 한 항에 있어서,상기 인페인팅 모델은,제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 제2 훈련이 수행된 인공 지능 모델이고,상기 제1 손실 함수는,상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이의 옵티컬 플로우(opticalflow)에 기초하여 계산되고,상기 제2 손실 함수는,공개특허 10-2024-0062080-7-상기 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산되는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항 내지 제18항 중 어느 한 항에 있어서,상기 옵티컬 플로우는,상기 제2 훈련 전의 상기 복수의 제3 모듈들로부터 획득되는 상기 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득되는, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제11항 내지 제19항 중 어느 한 항에 있어서,상기 적어도 하나의 프로세서는,상기 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라의 움직임의 크기, 상기 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별하고,상기 카메라의 움직임의 크기, 상기 오브젝트의 움직임의 크기 및 상기 타겟 복원 대상 영역의 크기 중 적어도하나에 기초하여 제1 추론 모드 및 제2 추론 모드 중 하나를 선택하여 상기 복수의 프레임들 각각의 제3 토큰을획득하며, 상기 제1 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰 및 상기 복수의 프레임들 각각의 제1 피처맵에 기초하여 상기 복수의 프레임들 각각의 제3 토큰을 획득하는 모드이고,상기 제2 추론 모드는, 상기 복수의 프레임들 각각의 제2 토큰에 기초하여 상기 복수의 프레임들 각각의 제3 토큰들을 획득하는모드인, 전자 장치."}
{"patent_id": "10-2023-0045503", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_21", "content": "제1항 내지 제10항 중 어느 한 항의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 비디오 인페인팅(video inpainting)을 수행하는 방법이 개시된다. 본 비디오 인페인팅을 수행하는 방법은 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 인페인팅인 공간적 어텐션(spatial (뒷면에 계속)"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "비디오 내에 픽셀들을 채워 비디오를 복원하는, 비디오 인페인팅을 수행하는 전자 장치 및 그 동작 방법이 제공 된다."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이미지 인페인팅(Image inpainting) 또는 비디오 인페인팅(Video inpainting)은 이미지나 비디오에서 일부 영역 이 손상되거나 삭제된 경우, 해당 영역을 복원하는 기술을 의미힌다. 이미지 인페인팅 또는 비디오 인페인팅을 수행함에 있어서, 인페인팅된 부분이 주변의 이미지나 비디오와 일관 되고, 흐름이 자연스럽게 이어지고, 복원 결과가 뚜렷하고 선명하면서도 일관성이 유지되어야 하는 것이 중요하 다."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "개시된 실시예들은, 전술한 문제를 해결하기 위한 것으로서, 비디오 인페인팅을 수행하는 전자 장치 및 그 동작 방법을 제공하기 위한 것이다."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅(video inpainting)을 수행하는 방법은, 타겟 복원 대 상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 타겟 동영상의 단일 프 레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수 의 제1 모듈들 및 타겟 동영상의 복수의 프레임들의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐 션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득하는 단계를 포함할 수 있다. 일 실시예에서, 인페 이팅 모델은 제1 훈련 전의 복수의 제1 모듈들 및 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 제1 훈련 전의 복수의 제2 모듈들에 대하여 제1 훈련이 수행되고, 제1 훈련이 수행된 복수의 제1 모듈들 및 제1 훈 련이 수행된 복수의 제2 모듈들에 대하여 제2 훈련이 수행된 인공 지능 모델일 수 있다. 일 실시예에서, 제1 훈 련은 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련일 수 있다. 일 실시예에서, 제2 훈련은 제2 복원 대상 영역을 포함하 는 훈련용 동영상에 기초하여, 복원된 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련일 수 있다. 본 개시의 일 실시예에 따른 비디오 인페인팅(video inpainting)을 수행하는 전자 장치는 하나 이상의 명령어들 (instructions)을 저장하는 메모리 및 메모리에 저장된 하나 이상의 명령어들을 실행하는 적어도 하나의 프로세 서를 포함할 수 있다. 일 실시예에서, 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상을, 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 타겟 동영상의 복수의 프레임의 이미지들 사 이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모 듈들을 포함하는 인페인팅 모델에 적용함으로써, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득할 수 있다. 일 실시예에서, 인페이팅 모델은 제1 훈련 전의 복수의 제1 모듈들 및 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 제1 훈련 전의 복수의 제2 모듈들에 대하여 제1 훈련이 수행되고, 제1 훈련이 수행된 복수의 제1 모듈들 및 제1 훈련이 수행된 복수의 제2 모듈들에 대하여 제2 훈련이 수행된 인공 지능 모델일 수 있다. 일 실시예에서, 제1 훈련은 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련일 수 있다. 일 실시예에서, 제2 훈련은 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 제2 복원 대상 영역이 복원된 훈련용 동영상 을 획득하기 위한 훈련일 수 있다. 본 개시의 일 실시예에 따르면, 개시된 비디오 인페이팅을 수행하는 방법 중 적어도 하나의 방법을 컴퓨터에서 수행하기 위한 프로그램이 기록된 컴퓨터로 읽을 수 있는 기록매체가 제공될 수 있다."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서, \"a, b 또는 c 중 적어도 하나\" 표현은 \" a\", \" b\", \" c\", \"a 및 b\", \"a 및 c\", \"b 및 c\", \"a, b 및 c 모두\", 혹은 그 변형들을 지칭할 수 있다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있 다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의 미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 용어들은 본 명세서에 기재된 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가질 수 있다. 또한, 본 명세서에서 사용되는 '제1' 또는 '제2' 등과 같이 서수를 포함하는 용어는 다양한 구성 요소들을 설명하는데 사용할 수 있지만, 상기 구성 요소들은 상기 용 어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으 로만 사용된다. 명세서 전체에서 어떤 부분이 어떤 구성요소를 \"포함\"한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 또한, 명세서에 기재된 \"부\", \"모듈\" 등의 용어는 적어도 하나의 기능이나 동작을 처리하는 단위를 의미하며, 이는 하드웨어 또는 소 프트웨어로 구현되거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 명세서에서, '이미지 인페인팅', '비디오 인페인팅' 및 '인페인팅'은 타겟 이미지에 포함된 타겟 복원 대상 영역의 픽셀들을 복원 대상 영역의 주변 영역들과 시각적인 특징이 자연스럽게 연결되는 픽셀들로 변경하는 것 을 의미할 수 있다. 후술하는 실시예들에 따라 전자 장치는 인페인팅을 수행함으로써, 타겟 복원 대상 영역을 복원하고, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득할 수 있다. 본 명세서에서, '복원 대상 영역'은, 후술하는 실시예들에 따른 비디오 인페인팅 알고리즘이 적용됨으로써, 복 원이 수행되는 영역을 의미할 수 있다. 복원 대상 영역은 비디오 내 특정 오브젝트에 대응되는 영역을 의미하거 나, 비디오와 함께 표시되는 마스킹이 수행된 영역을 의미할 수 있으며, 반드시 상술한 예에 한정되는 것은 아 니다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해서 설 명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를붙였다. 또한, 각각의 도면에서 사용된 도면 부호는 각각의 도면을 설명하기 위한 것일 뿐, 상이한 도면들 각각 에서 사용된 상이한 도면 부호가 상이한 요소를 나타내기 위한 것은 아니다. 이하 첨부된 도면을 참고하여 본 개시를 상세히 설명하기로 한다. 도 1은 본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅을 수행하는 방법을 설명하기 위한 도면이다. 도 1을 참조하면, 비디오 인페이팅이 수행되는 타겟 동영상이 전자 장치에 입력될 수 있다. 타겟 동영상 은 비디오 인페이팅이 수행되는 타겟 복원 대상 영역을 포함하는 복수의 프레임들의 이미지로 구성될 수 있다. 예를 들어, 타겟 동영상의 복수의 프레임들의 이미지는 비디오 인페이팅이 수행되는 타겟 복원 대상 영역인 자동차를 포함할 수 있다. 일 실시예에서, 전자 장치는 입력된 타겟 동영상에 기초하여 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 복원 대상 영역을 표시하는 마스크 동영상을 생성할 수 있다. 타겟 복원 대상 영역이 마스킹된 타겟 동영상은 타겟 동영상에서 타겟 복원 대상 영역인 자동차의 픽셀 값이 0 또는 1로 채워진 동영 상을 의미할 수 있다. 복원 대상 영역을 표시하는 마스크 동영상은 타겟 동영상에서 타겟 복원 대상 영역인 자동차의 픽셀 값이 0으로 채워지고, 타겟 복원 대상 영역인 자동차를 제외한 나머지 영역의 픽셀 값이 1로 채워지거나, 반대로 타겟 복원 대상 영역의 픽셀 값이 1로 채워지고, 타겟 복원 대상 영역인 자동차를 제외 한 나머지 영역의 픽셀 값이 0으로 채워진 동영상을 의미할 수 있다. 일 실시예에서, 전자 장치는 마스킹된 복원 대상 영역을 포함하는 타겟 동영상과 복원 대상 영역을 표시하 는 마스크 동영상을, 인페인팅 모델에 적용함으로써, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 타겟 동영상의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초 한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈들 및 상기 타겟 동영상의 복수의 프레 임의 이미지들 사이에서 획득되는 특징에 기초한 특징 공간 인페인팅인 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함할 수 있다. 일 실시예에서, 복수의 제1 모듈들은 한 프 레임 이미지로부터 유래한 특징 영역에 한정된 공간적 어텐션을 수행한다는 점에서, 복수의 제1 모듈들을 공간 적 어텐션을 수행하는 공간적 어텐션 모듈로 지칭할 수 있다. 또한, 복수의 제2 모듈들은 동영상의 복수의 프레 임의 이미지들로부터 유래한 특징 영역들 전반에 걸쳐 시간적(프레임 간) 및 공간적(프레임 내) 어텐션을 혼합 하여 수행한다는 점에서, 복수의 제2 모듈들을 시-공간적 어텐션을 수행하는 시-공간적 어텐션 모듈로 지칭할 수 있으며, 이보다 표기를 단순화하고 시간적 어텐션 특성을 강조하기 위해, 간단히 시간적 어텐션을 수행하는 시간적 어텐션 모듈로 대체하여 지칭할 수도 있다.. 일 실시예에서, 인페인팅 모델은 복수의 제1 모듈들 중 적어도 하나를 이용하여 타겟 복원 대상 영역을 포함하 는 이미지의 공간적 일관성이 유지되도록 타겟 복원 대상 영역에 대한 인페인팅을 수행할 수 있다. 공간적 일관 성이 유지된다는 것은 복원이 수행된 타겟 복원 대상 영역이 이미지 내의 컨텍스트에 부합되도록 인페인팅이 수 행된 것을 의미할 수 있다. 다시 말해, 인페인팅 모델은 복수의 공간적 어텐션을 수행하여 타겟 복원 대상 영역 을 구성하는 픽셀들이 그 주변 픽셀들과 비슷한 값(예: 색상, 질감 등)을 가지도록 변환하여 복원된 타겟 복원 대상 영역이 부자연스러운 경계나 패치 등을 포함하지 않도록 할 수 있다. 일 실시예에서, 인페인팅 모델은 복수의 제2 모듈들 중 적어도 하나를 이용하여 공간적 일관성과 함께 시간적 일관성이 유지되도록 타겟 복원 대상 영역에 대한 인페인팅을 수행할 수 있다. 시간적 일관성이 유지된다는 것 은 복원이 수행된 타겟 복원 대상 영역이 인접한 프레임들의 이미지 사이에서 발생하는 움직임에 부합되도록 인 페인팅이 수행된 것을 의미할 수 있다. 다시 말해, 인페인팅 모델은 시간적 어텐션을 수행하여 타겟 복원 대상 영역을 구성하는 픽셀들이 인접한 프레임들의 이미지 사이에서 발생하는 움직임의 패턴과 일치하도록 변환하여 복원된 타겟 복원 대상 영역의 움직임이 부자연스럽게 보이지 않도록 할 수 있다. 일 실시예에서, 인페인팅 모델은 요소화된 훈련(Factorized Learning)이 수행된 인공지능 모델일 수 있다. 요소화된 훈련이란 모델을 모델에 포함된 복수의 모듈들 중 일부 또는 전체를 분리하고, 모델의 동작 범위를 단 일 훈련용 이미지에 한정하여 개별적으로 훈련시키고, 개별적으로 훈련된 분리된 일부 모듈들을 다른 모듈과 결 합하거나 또는 전체 그대로 복제하여 재구성된 모델을 구성하고, 재구성된 모델의 동작 범위를 훈련용 동영상으 로 확장하여 다시금 훈련시켜 최종 모델을 구성하는 훈련 방식을 의미할 수 있다. 일 실시예에서, 인페인팅 모 델은 요소화된 훈련을 통해 복수의 제1 모듈들과 제2 모듈들이 공간적 어텐션을 보다 잘 수행하도록 최적화하고, 다시금 공간적으로 최적화된 복수의 제2 모듈들이 시간적 어텐션을 보다 잘 수행하도록 최적화할수 있다. 이하 도 3을 통해 인페인팅 모델의 요소화된 훈련을 자세히 설명하도록 하겠다. 도 2는 본 개시의 일 실시예에 따른 인페인팅 모델의 알고리즘을 설명하는 흐름도이다. 이하에서 설명하는 전자 장치의 동작은 전자 장치에 포함된 인페인팅 모델에 의해 수행된 것으로 이해될 수 있다. 또한, 인페인팅 모델 의 동작은 전자 장치에 의해 수행되는 것으로 이해될 수 있다. S210 단계에서, 전자 장치는 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상에 기초하여 복수의 프레임들의 제1 토큰들을 획득한다. 일 실시예에서, 전자 장치는 전자 장치에 입력된 타겟 동영상에 기초하여 타겟 복원 대상 영역을 식별하고, 식 별된 타겟 복원 대상 영역 및 타겟 복원 대상 영역을 제외한 나머지 영역의 픽셀 값들을 변경하여 타겟 복원 대 상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상을 획득할 수 있다. 일 실시예에서, 전자 장치는 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대상 영역을 표시하는 마스크 동영상의 복수의 프레임들의 이미지 각각에 기초하여 복수의 프레임들 각각의 피처맵을 획득할 수 있다. 전자 장치는 획득한 복수의 프레임들 각각의 피처맵을 토큰화(Tokenization)하여 복수의 프레임들 각각의 제1 토큰들을 획득할 수 있으며, 전자 장치가 토큰화를 수행하는 구체적인 동작은 이하 도 5a를 통해 다시 설명하도 록 하겠다. S220 단계에서, 전자 장치는 복수의 프레임들의 제1 토큰들을 복수의 제2 모듈들 중 적어도 하나에 적용함으로 써, 시간적 어텐션이 수행된 복수의 프레임들의 제2 토큰들을 획득한다. 일 실시예에서, 전자 장치는 복수의 프 레임들의 제1 토큰들을 인페인팅 모델에서 제2 토큰들을 생성하는 제2 모듈 이전에 위치한 복수의 제1 모듈 및 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 복수의 프레임들의 제1 토큰들에 대하여 공간적 어텐션 및 시간적 어텐션 중 적어도 하나가 수행된 복수의 프레임들의 토큰들을 획득할 수 있다. 일 실시예에서, 전자 장 치는 획득된 복수의 프레임들의 토큰들을 제2 토큰들을 생성하는 제1 모듈에 적용함으로써, 공간적 어텐션이 수 행된 복수의 프레임들의 제2 토큰들을 획득할 수 있다. S230 단계에서, 전자 장치는 복수의 프레임들 각각의 제2 토큰들을 복수의 제1 모듈들 중 적어도 하나에 적용함 으로써, 공간적 어텐션이 수행된 복수의 프레임들 각각의 제3 토큰들을 획득한다. 일 실시예에서, 전자 장치는 복수의 프레임들의 제2 토큰들을 인페인팅 모델에서 제3 토큰들을 생성하는 제2 모듈 이전에 위치한 복수의 제1 모듈들 및 복수의 제2 모듈들 중 적어도 하나에 적용함으로써, 복수의 프레임들의 제2 토큰들에 대하여 공간적 어텐션 및 시간적 어텐션 중 적어도 하나가 수행된 복수의 프레임들의 토큰들을 획득할 수 있다. 일 실시예에서, 전자 장치는 획득된 복수의 프레임들 각각의 토큰들을 제3 토큰들을 생성하는 제2 모듈에 적용함으 로써, 시간적 어텐션이 수행된 복수의 프레임들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은, 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징 을 추출하기 위한 복수의 제3 모듈들을 포함할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들의 제2 토 큰들을 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 인접한 프레임들의 이미지 사이에서 존재하는 움직 임(motion)에 대한 정보를 포함하는 복수의 프레임들의 제1 피처맵들을 획득할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들 각각의 제2 토큰들 및 복수의 프레임들 각각의 제1 피처맵을 복수의 제1 모듈들 중 적 어도 하나에 적용함으로써, 복수의 프레임들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 상기 복수의 프레임들의 제1 피처맵 중 제1 프레임의 제1 피처맵 들을 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 프레임의 제1 토큰들 및 제1 프레임의 제1 피처맵 중 적 어도 하나에 기초하여 복수의 프레임들의 제3 토큰들 중 제1 프레임의 제3 토큰들을 획득할 수 있다. S240 단계에서, 전자 장치는 복수의 프레임들의 제3 토큰들에 기초하여 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득한다. 일 실시예에서, 전자 장치는 복수의 프레임들 각각의 제3 토큰들을 디토큰화(De- tokenization)하여 복수의 프레임들 각각의 피처맵을 획득할 수 있다. 전자 장치는 획득된 복수의 프레임들 각 각의 피처맵에 기초하여 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상의 복수의 프레임들 각각의 이미지 를 획득할 수 있다. 전자 장치가 디토큰화를 수행하는 구체적인 동작은 이하 도 5b를 통해 다시 설명하도록 하 겠다. 도 3은 본 개시의 일 실시예에 따른 요소화된 훈련이 수행된 인페인팅 모델을 설명하기 위한 도면이다. 일 실시예에서, 인페인팅 모델은 제1 훈련 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 제1 훈련 전의 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 복수의 제2 모듈들(321-1, 321-2, 321- 3, 321-4)에 대하여 제1 훈련이 수행될 수 있다. 이 경우, 인페이팅 모델은 모델의 동작을 단일 훈련용 이 미지에 한정하여 제1 훈련을 수행할 수 있다. 모델의 동작을 단일 훈련용 이미지에 한정하여 제1 훈련 을 수행한다는 것은 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 복수의 제2 모듈들(321-1, 321- 2, 321-3, 321-4)이 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션을 수행하도록 훈련되는 것을 의미할 수 있다. 일 실시예예서, 제1 훈련이 수행된 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 제1 훈련이 수행된 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여, 제2 훈련이 수행될 수 있다. 이 경우, 인페이팅 모델은 모델의 동작을 훈련용 동영상으로 확장하여 제2 훈련을 수행할 수 있다. 모델의 동작을 훈련용 동영상으로 확장하여 제2 훈련을 수행한다는 것은 복수의 제1 모듈들(351-1, 351-2, 351-3, 351- 4)은 훈련용 동영상에 포함된 복수의 프레임들의 이미지들 각각으로부터 획득되는 특징에 기초한 공간적 어텐션 을 수행하도록 훈련되고, 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)은 훈련용 동영상에 포함된 복수의 프 레임들의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션을 수행하도록 훈련되는 것을 의미할 수 있다. 다시 말해, 전자 장치는 제1 훈련 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에 대하여 먼저 제1 훈련을 수행하고, 제1 훈련이 수행된 복수 의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)에 대하여 제2 훈련을 수행할 수 있다. 이 경우, 제1 훈련을 수행하는 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)은, 제2 훈련을 수행하는 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)과 구조적으로 동일하나, 제1 훈련을 수행하는 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)은 단일 훈련용 이미지로부터 획득되 는 특징에 기초하여 공간적 어텐션을 수행하도록 동작이 한정되어, 사실상 복수의 제 1 모듈들(311-1, 311-2, 311-3, 311-4)과 동일하게 동작한다. 반면, 제2 훈련을 수행하는 복수의 제2 모듈들(361-1, 361-2, 361- 3, 361-4)은 훈련용 동영상에 포함된 복수의 프레임들의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션을 수행하기 때문에, 훈련용 동영상에 포함된 복수의 프레임들의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션을 수행하는 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4)과 다르게 동작할 수 있다. 이에 대한 자세한 내용은 이하 도 6 및 도 7을 통해 다시 설명하도록 하겠다. 일 실시예에서, 제1 훈련은 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 상기 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 획득하기 위한 훈련일 수 있다. 일 실시예에서, 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지는 제1 복원 대상 영역을 마스킹한 훈련용 이미지 및 제1 복원 대상 영역을 표시하는 훈련용 마스크 또는 훈련용 마스크 맵을 포함할 수 있다. 일 실시예에서, 제1 훈련 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 제1 훈련 전의 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)에 포함된 적어도 하나의 레이어들의 신경망 파라미터들은 제1 훈련을 통하여 제1 훈련 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 복수의 제2 모듈 들(321-1, 321-2, 321-3, 321-4)이 단일 훈련용 이미지로부터 획득되는 특징에 기초하여 단일 훈련용 이미 지에 포함된 복원 대상 영역의 복원을 수행하도록 최적화될 수 있다. 이에 따라, 제1 훈련 전의 복수 의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 및 제1 훈련 전의 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4)은 제1 훈련을 통하여 공간적 어텐션에 기초한 이미지 인페인팅을 수행하기에 적합하도록 최적화될 수 있다. 일 실시예에서, 제2 훈련은 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련일 수 있다. 일 실시예에서, 제2 복원 대상 영역을 포함하는 훈련용 동영상은 제2 복원 대상 영역을 마스킹한 훈련용 동영상 및 제2 복원 대상 영역을 표시하는 훈련용 마스크 동영상을 포함할 수 있다. 일 실시예에서, 제2 훈련 전의 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 복수의 제2 모듈들 (361-1, 361-2, 361-3, 361-4)에 포함된 적어도 하나의 레이어들의 신경망 파라미터들은 제2 훈련을 통하 여 제1 훈련이 수행된 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 제1 훈련이 수행된 복수 의 제2 모듈들(361-1, 361-2, 361-3, 361-4)이 훈련용 동영상에 포함된 복수의 프레임들의 이미지들 사이 에서 획득되는 특징에 기초하여 훈련용 동영상에 포함된 복원 대상 영역의 복원을 수행하도록 최적화될 수 있다. 이에 따라, 제1 훈련이 수행된 복수의 제1 모듈들(351-1, 351-2, 351-3, 351-4) 및 제1 훈련이 수행된 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4)은 제2 훈련을 통하여 시간적 어텐션에 기초한 비디오 인페인팅을 수행하기에 적합하도록 최적화될 수 있다. 이 경우, 제1 훈련이 수행된 복수의 제1 모 듈들(351-1, 351-2, 351-3, 351-4) 및 제1 훈련이 수행된 복수의 제2 모듈들(361-1, 361-2, 361-3, 361- 4)은 제1 훈련을 통해 이미 공간적 어텐션을 수행하도록 최적화되었기 때문에, 제2 훈련이 수행되어 도 공간적 어텐션을 수행하는 능력은 유지될 수 있다. 일 실시예에서, 인페인팅 모델이 복수의 제3 모듈들(미도시)을 포함하는 경우, 복수의 제3 모듈들(미도시)에 대 한 제2 훈련이 수행될 수 있다. 다시 말해, 전자 장치는 제1 훈련이 수행된 복수의 제1 모듈들(351- 1, 351-2, 351-3, 351-4) 및 제1 훈련이 수행된 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4) 및 제2 훈련 전 복수의 제3 모듈들(미도시)에 대하여 제2 훈련을 수행할 수 있다. 일 실시예에서, 전자 장치는 정답 이미지 및 인페인팅 모델이 생성한 이미지에 기초하여 손실 함수를 계산하고, 계산된 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행할 수 있다. 일 실시예에서, 손실 함수는 GAN 손실 함수(adversarial loss 및 feature matching loss), 및 재구성 손실 함수(L1 reconstruction loss 및 SSIM- based reconstruction loss)를 포함할 수 있다. 일 실시예에서, 전자 장치는 GAN 손실 함수 및 재구성 손실 함 수 중 적어도 하나에 기초하여 인페인팅 모델의 제1 훈련 및 제2 훈련 중 적어도 하나를 수행할 수 있다. 일 실시예에서, L1 reconstruction loss 및 SSIM-based reconstruction loss는 아래와 같은 수학식들에 기초하여 계산될 수 있다. 수학식 1"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 2"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기에서, 는 정답 이미지이며, 제1 복원 대상 영역을 포함하지 않는 훈련용 이미지 또는 제2 복원 대상 영역 을 포함하지 않는 훈련용 동영상의 특정 프레임의 이미지를 의미할 수 있다. 는 마스킹 이미지(마스크 맵)이 며, 제1 복원 대상 영역 또는 제2 복원 대상 영역에 대응하는 이미지 또는 동영상을 의미할 수 있다. 는 인페인팅 모델이 생성한 이미지로, 복원된 제1 복원 대상 영역을 포함하는 훈련용 이미지 또는 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상의 특정 프레임의 이미지를 의미할 수 있다. 일 실시예에서, 전자 장치는 제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 인페인팅 모델의 제2 훈련을 수행할 수 있다. 일 실시예에서, 전자 장치는 제1 손실 함수에 기초한 제2 훈련의 제1 서브 훈련을 수행하고, 제2 손실 함수에 기초한 제2 훈련의 제2 서브 훈련을 수행할 수 있다. 일 실시예에서, 전자 장치는 제1 서브 훈련과 제2 서브 훈련을 서로 번갈아가면서 반복하여 수행할 수 있다. 일 실시예에서, 전 자 장치는 제2 서브 훈련을 수행하지 않고, 제1 서브 훈련만 수행할 수 있다. 일 실시예에서, 제1 손실 함수는 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사 이의 옵티컬 플로우(optical flow)에 기초하여 계산될 수 있다. 옵티컬 플로우는 동영상 내에서 인접한 프레임 들의 이미지 사이에서 같은 오브젝트나 픽셀들이 이동하는 방향과 크기에 대한 정보를 포함할 수 있다. 일 실시 예에서, 옵티컬 플로우는 이미지 내의 복수의 픽셀들의 이동 크기와 방향을 나타내는 이동 벡터들을 포함할 수 있다. 일 실시예에서, 인페인팅 모델은 제1 손실 함수에 기초하여 제2 훈련이 수행될 수 있으며, 이에 대 한 구체적인 내용은 이하 도 9a, 9b 및 9c를 통해 자세히 설명하도록 하겠다. 일 실시예에서, 제2 손실 함수는 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 상기 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산될 수 있다. 아핀 변환은 오브젝트의 이동, 회전, 확대/축소, 전단(shear) 등을 포함하는 2차원 공간에서의 선형 변환 을 의미할 수 있으며, 아핀 변환을 통해 훈련용 동영상에 포함된 복수의 프레임들의 이미지의 모양이 변형될 수있다. 일 실시예에서, 인페인팅 모델은 제2 손실 함수에 기초하여 제2 훈련이 수행될 수 있으며, 이에 대 한 구체적인 내용은 이하 도 10을 통해 자세히 설명하도록 하겠다. 일 실시예에서, 전자 장치는 제1 훈련 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 중 하나 이 상 및 제1 훈련 전의 복수의 제2 모듈들(321-1, 321-2, 321-3, 321-4) 중 하나 이상으로부터 각각 획득되 는 둘 이상의 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 인페인팅 모델의 제1 훈련 을 수행할 수 있다. 일 실시예에서, 전자 장치는 인페인팅 모델은 제1 훈련이 수행된 복수의 제1 모 듈들(351-1, 351-2, 351-3, 351-4) 중 하나 이상 및 제1 훈련이 수행된 복수의 제2 모듈들(361-1, 361-2, 361-3, 361-4) 중 하나 이상으로부터 각각 획득되는 둘 이상의 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 인페인팅 모델의 제2 훈련을 수행할 수 있다. 이에 대한 구체적인 내용은 이하 도 11a, 11b 및 11c를 통해 자세히 설명하도록 하겠다. 이와 같이, 본 개시의 일 실시예에 따른 인페인팅 모델은 요소화된 훈련과 다양한 손실 함수에 기초하여 이미지 인페인팅을 수행할 수 있도록 최적화될 수 있다. 이에 따라, 전자 장치는 인페인팅 모델을 이용하여 타겟 복원 대상 영역을 보다 자연스럽고 명확하게 복원시킬 수 있다. 도 4a는 본 개시의 일 실시예에 따른 인페인팅 모델의 구조를 설명하기 위한 도면이다. 도 4a를 참조하면, 인페이팅 모델은 인코더, 디코더 및 어텐션 블록(400-1)을 포함할 수 있다. 일 실시예에서, 인코더는 입력된 복원 대상 영역이 마스킹 된 이미지 및 복원 대상 영역을 표시하는 마스 크맵으로부터 피처맵을 출력할 수 있다. 일 실시예에서, 전자 장치는 마스킹 된 타겟 복원 대상 영역을 포함하 는 입력 동영상의 복수의 프레임들의 이미지와 타겟 복원 대상 영역을 나타내는 입력 마스크 동영상의 복수의 프레임들의 마스크맵을 인코더에 적용함으로써, 복수의 프레임들의 피처맵을 획득할 수 있다. 일 실시예에 서, 인코더는 마스킹 된 이미지 및 마스크 맵을 입력으로 하여 피처맵을 출력하도록 훈련된 모델일 수 있 으며, 인코더의 신경망 파라미터들은 본 명세서에서 개시된 다양한 훈련들을 수행함으로써, 최적화될 수 있다. 일 실시예에서, 디코더는 입력된 피처맵으로부터 이미지를 출력할 수 있다. 일 실시예에서, 전자 장치는 인페인팅이 수행된 입력된 동영상의 복수의 프레임들의 피처맵을 디코더에 적용함으로써, 복원된 타겟 복 원 대상 영역을 포함하는 입력된 동영상의 복수의 프레임들의 이미지를 획득할 수 있다. 일 실시예에서, 디코더는 피처맵을 입력으로 하여 이미지를 출력하도록 훈련된 모델일 수 있으며, 디코더의 신경망 파라미터들은 본 명세서에서 개시된 다양한 훈련들을 수행함으로써, 최적화될 수 있다. 일 실시예에서, 어텐션 블록(400-1)은 토큰화 모듈, 디토큰화 모듈, 복수의 제1 모듈들(440-1, 440- 2, 440-3, 440-4) 및 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4)를 포함할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들의 피처맵들을 토큰화 모듈에 적용함으로써, 복수의 프레임들 의 토큰들을 획득할 수 있다. 토큰화 모듈은 복수의 프레임들의 피처맵들을 복수의 패치들로 분할하고, 분 할된 복수의 패치들에 기초하여 복수의 프레임들의 토큰들을 생성할 수 있다. 토큰화 모듈로부터 획득된 복수의 프레임들의 토큰들은 본 명세서에서 설명하는 다양한 이미지 및 비디오 인페인팅 알고리즘에 이용될 수 있다. 토큰화 모듈의 동작은 이하 도 5a를 통해 자세히 설명하도록 하겠다. 일 실시예에서, 전자 장치는 복수의 프레임들의 토큰들을 디토큰화 모듈에 적용함으로써, 복수의 프레임들 의 피처맵들을 획득할 수 있다. 디토큰화 모듈은 복수의 프레임들의 토큰들에 기초하여 복수의 패치들을 생성하고, 생성된 복수의 패치들을 결합하여 복수의 프레임들의 피처맵들을 생성할 수 있다. 디토큰화 모듈 로부터 획득된 복수의 프레임들의 피처맵들은 본 명세서에서 설명하는 다양한 이미지 및 비디오 인페인팅 알고리즘에 이용될 수 있다. 디토큰화 모듈의 동작은 이하 도 5b를 통해 자세히 설명하도록 하겠다. 일 실시예에서, 전자 장치는 복수의 프레임들 각각의 토큰들을 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 하나에 적용함으로써, 공간적 어텐션이 수행된 복수의 프레임들 각각의 토큰들을 획득할 수 있다. 일 실시예 에서, 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4)은 복수의 프레임들의 토큰들 중 특정 프레임의 토큰들에 기초하여 공간적 어텐션을 수행하고, 공간적 어텐션을 통해 복원된 복원 대상 영역을 포함하는 특정 프레임의 이미지에 대응하는 특정 프레임의 토큰들을 생성할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들의 토큰들을 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 하나에 적용함으로써, 시-공간적 어텐션이 수행된 복수의 프레임들의 토큰들을 획득할 수 있다. 일 실시예에서,복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4)은 복수의 프레임들의 토큰들에 기초하여 시-공간적 어텐션을 수행하고, 시-공간적 어텐션을 통해 복원된 복원 대상 영역을 포함하는 복수의 프레임들의 이미지에 대응하는 복수의 프레임들의 토큰들을 생성할 수 있다. 일 실시예에서, 어텐션 블록(400-1)은 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 및 복수의 제2 모듈들 (450-1, 450-2, 450-3, 450-4)이 순서대로 중첩되어 배열되는 구조일 수 있다. 일 실시예에서, 전자 장치는 어 텐션 블록(400-1)을 이용하여 복수의 프레임들의 토큰에 대한 공간적 어텐션과 시-공간적 어텐션을 반복하여 수 행할 수 있다. 예를 들어, 도 4a와 같이, 전자 장치는 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 하 나로부터 획득되는 공간적 어텐션이 수행된 복수의 프레임들의 토큰들을 공간적 어텐션을 수행한 제1 모듈 다음 에 위치한 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 하나에 적용함으로써, 복수의 프레임들의 토큰들 에 대한 시-공간적 어텐션을 수행할 수 있다. 또한, 전자 장치는 복수의 제2 모듈들(450-1, 450-2, 450-3, 450- 4) 중 하나로부터 획득되는 시-공간적 어텐션을 수행한 복수의 프레임들의 토큰들을 시-공간적 어텐션을 수행한 제2 모듈 다음에 위치한 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 하나에 적용함으로써, 복수의 프레 임들의 토큰들에 대한 공간적 어텐션을 수행할 수 있다. 일 실시예에서, 어텐션 블록(400-1)은 복수의 제1 모듈 들(440-1, 440-2, 440-3, 440-4) 중 하나를 처음 적용하고, 바로 다음으로 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 하나를 적용하는 순서로부터 시작하여 반복하는 순서로 구성될 수 있다. 일 실시예에서, 어텐 션 블록(400-1)은 도 4b에서 설명하는 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)이 없는 경우에 한정하 여, 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4)을 사용하지 않고 복수의 제2 모듈들(450-1, 450-2, 450- 3, 450-4)만을 적용하는 것으로 구성될 수 있다. 이와 같이, 본 개시의 일 실시예에 따른 전자 장치는 복수의 프레임들의 토큰에 대한 공간적 어텐션과 시간적 어텐션을 반복하여 수행하면서, 타겟 복원 대상 영역에 대한 이미지 인페인팅을 더욱 정확하고, 명확하게 수행 할 수 있다. 다만, 어텐션 블록(400-1)의 구조가 상술한 예에 반드시 한정되는 것은 아니며, 어텐션 블록(400- 1)에 포함된 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 및 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 일부가 생략되거나, 도 4a와 다른 다양한 방식으로 구성될 수 있다. 예를 들어, 전자 장치의 하드웨 어 자원(예: 컴퓨팅 시간, 전력 소모량, 대역폭, 스토리지 용량 등), 요구되는 인페인팅 성능 또는 이들을 설정 하는 사용자 입력 등에 기초하여 인페인팅 모델의 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 및 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 일부 모듈이 생략되거나 일부 모듈이 배치되는 위치가 변경될 수 있다. 도 4b는 본 개시의 일 실시예에 따른 인페인팅 모델의 구조를 설명하기 위한 도면이다. 도 4b를 참조하면, 어텐션 블록(400-2)는 도 4a 의 어텐션 블록(430-1)에서 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)을 더 포함할 수 있다. 일 실시예에서, 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)은 타겟 동영상의 복수의 프레임들 중 서로 인 접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 모듈일 수 있다. 일 실시예에서, 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)이 추출하는 특징을 모션 클루 또는 모션 클루 피처맵으로 지칭할 수 있으 며, 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)은 모션 클루를 생성한다는 점에서, 모션 클루 추정 모듈로 지칭할 수 있다. 일 실시예에서, 전자 장치는 복수의 프레임들의 토큰을 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 적 어도 하나에 적용함으로써, 복수의 프레임들 중 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 복수의 프레임들의 피처맵을 획득할 수 있다. 일 실시예에서, 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)은 인접한 프레임들의 토큰에 기초하여 인접한 프레임들의 이미지 사이에서 존재하는 움직 임에 대한 정보를 포함하는 복수의 프레임들의 피처맵을 생성할 수 있다. 일 실시예에서, 인접한 프레임들의 이 미지 사이에서 존재하는 움직임은 이미지에 포함된 오브젝트의 움직임 및/또는 이미지를 촬영하는 카메라의 움 직임에 의해 인접한 프레임의 이미지들 사이에서 발생하는 픽셀 값들의 변화 또는 픽셀 값들에 변화에 의해 획 득될 수 있는 특징을 의미할 수 있다. 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)의 구체적인 동작은 이하 도 9a, 도 9b 및 도 9c를 통해 자세히 설명하도록 하겠다. 일 실시예에서, 전자 장치는 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4)로부터 획득되는 시간적 어텐션이 수행된 복수의 프레임들 각각의 토큰들 및 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)로부터 획득되는 인 접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 복수의 프레임들 각각의 피처맵을 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 적어도 하나에 적용함으로써, 공간적 어텐션이 수행된 복수의 프레임들 각각의 토큰들을 획득할 수 있다. 이 경우, 복수의 프레임들 각각의 토큰들 만을 기초하여 공간적 어텐션을 수행하는 도 4a의 어텐션 블록(400-1)과 달리, 도 4b의 어텐션 블록(400-2)의 복수의 제1 모듈들(440- 1, 440-2, 440-3, 440-4)는 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 복수 의 프레임들 각각의 피처맵을 함께 고려한다는 점에서, 시간적 일관성이 유지되는 공간적 어텐션을 수행할 수 있게 된다. 일 실시예에서, 도 4b와 같이, 인페인팅 모델(400-2)의 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4)은 순 서대로 중첩되어 배열된 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 하나 및 복수의 제2 모듈들(450- 1, 450-2, 450-3, 450-4) 중 하나 각각에 연결되어 배열되는 구조일 수 있다. 일 실시예에서, 전자 장치는 어텐 션 블록(400-2)을 이용하여 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 복수 의 프레임들의 모션 클루 피처맵들을 반복하여 획득하고, 획득된 복수의 프레임들의 모션 클루 피처맵들 및 시 간적 어텐션이 수행된 복수의 프레임들의 토큰들에 기초하여 공간적 어텐션을 반복하여 수행할 수 있다. 예를 들어, 도 4b와 같이, 전자 장치는 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 하나로부터 획득한 시간 적 어텐션이 수행된 복수의 프레임들의 토큰들을 시간적 어텐션을 수행한 제2 모듈과 연결된 복수의 제3 모듈들 (460-1, 460-2, 460-3, 460-4) 중 하나에 적용함으로써, 복수의 프레임들의 모션 클루 피처맵들을 반복하여 획 득할 수 있다. 또한, 전자 장치는 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 하나로부터 획득한 복수 의 프레임들의 모션 클루 피처맵들 및 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 중 하나로부터 획득한 시간적 어텐션이 수행된 복수의 프레임들의 토큰들을 시간적 어텐션을 수행한 제2 모듈 다음에 위치한 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4) 중 하나에 적용함으로써, 공간적 어텐션을 반복하여 수행할 수 있다. 다만, 어텐션 블록(400-2)의 구조가 상술한 예에 반드시 한정되는 것은 아니며, 어텐션 블록(400-2)에 포함된 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 일부가 생략되거나, 도 4b와 다른 다양한 방식으로 구성될 수 있다. 예를 들어, 전자 장치의 하드웨어 자원, 요구되는 인페인팅 성능 또는 이들을 설정하는 사용자 입력 등에 기초하여 인페인팅 모델의 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4), 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 및 복수의 제3 모듈들 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 일부 모듈이 생략되거나 일부 모듈이 배치되는 위치를 변경할 수 있다. 이와 같이, 본 개시의 일 실시예에 따른 전자 장치는 인접한 프레임의 이미지들 사이에서 존재하는 움직임에 대 한 정보를 픽셀 차원이 아닌 피처맵 차원에서 반복하여 획득하고, 복수의 프레임들 각각의 토큰들에 반복하여 적용시켜 시간적 일관성이 유지되는 공간적 어텐션을 수행할 수 있다. 이에 따라, 전자 장치는 타겟 복원 대상 영역에 대한 비디오 인페인팅을 더욱 정확하고, 명확하게 수행할 수 있다. 도 5a는 본 개시의 일 실시예에 따른 토큰화 모듈의 동작을 설명하기 위한 도면이다. 도 5a에서 토큰화가 수행 되는 피처맵은 본 명세서에서 개시된 토큰화 모듈에 입력되는 복수의 프레임들의 피처맵들 중 하나일 수 있다. 도 5a를 참조하면, 토큰화 모듈은 토큰화가 수행되는 피처맵에 대하여 패딩을 수행하여 패딩된 피처 맵을 획득할 수 있다. 예를 들어, 토큰화 모듈은 H x W x C의 크기를 가지는 토큰화가 수행되는 피처 맵에 대하여 높이 및 너비 각각에 대하여 p 만큼 패딩을 수행하여 (H + 2p) x (W + 2p) x C 의 크기를 가 지는 패딩된 피처맵을 획득할 수 있다. 패딩이 수행되는 영역은 0 또는 다른 임의의 스칼라 값으로 채워질 수 있다. 일 실시예에서, 토큰화 모듈은 패딩된 피처맵을 복수의 패치들로 분할하고, 분할된 복수의 패치들을 복수의 펼쳐진 토큰들로 변환할 수 있다. 예를 들어, 토큰화 모듈은 (H + 2p) x (W + 2p) x C 의 크 기를 가지는 패딩된 피처맵을 k x k x C의 크기를 가지는 복수의 패치들로 분할하고, 분할된 복수의 패치 들을 펼쳐서 k x k x C의 길이를 가지는 복수의 펼쳐진 토큰들을 획득할 수 있다. 일 실시예에서, 토큰화 모듈은 패딩된 피처맵을 서로 인접한 복수의 패치들이 일정 영역만큼 겹치도록 분할하여 복수의 펼쳐 진 토큰들을 획득할 수 있다. 예를 들어, 토큰화 모듈은 패딩된 피처맵을 k x k x C의 크기를 가지는 서로 인접한 복수의 패치들을 s 간격으로 스트라이드하여 분할할 수 있다. 이 경우, 복수의 펼쳐진 토큰 들의 개수 n은 다음과 같이 계산될 수 있다. 수학식 3"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이와 같이, 토큰화 모듈은 패딩된 피처맵을 복수의 패치들이 서로 일정 영역만큼 겹쳐지도록 분할함 으로써, 피처맵의 토큰화 과정에서 발생될 수 있는 지역적 정보의 손실을 방지할 수 있다. 일 실시예에서, 토큰화 모듈은 복수의 펼쳐진 토큰들에 대한 제1 임베딩을 수행하여 복수의 토 큰들을 획득할 수 있다. 예를 들어, 토큰화 모듈은 제1 임베딩을 통하여 k x k x C의 길이를 가 지는 복수의 펼쳐진 토큰들을 길이가 d인 복수의 토큰들로 변환할 수 있다. 일 실시예에서, 토큰화 모듈은 복수의 펼쳐진 토큰들을 제1 임베딩 레이어(미도시)에 적용함으로써 복수의 토큰들을 획 득할 수 있다. 제1 임베딩 레이어(미도시)는 특정 길이의 토큰들을 입력으로 하여 특정 길이보다 길이가 줄어든 토큰을 출력하도록 훈련된 레이어일 수 있으며, 제1 임베딩 레이어(미도시)의 신경망 파리미터들은 본 명세서에 서 개시된 다양한 훈련들에 기초하여 최적화될 수 있다. 이와 같이, 토큰화 모듈은 복수의 프레임들 각각의 피처맵의 토큰화를 수행하여 복수의 프레임들 각각의 토큰들을 획득할 수 있다. 일 실시예에서, 토큰화 모듈을 통해 획득된 복수의 프레임들의 토큰들은 본 명 세서에서 개시된 비디오 인페인팅을 위한 다양한 알고리즘에 이용될 수 있다. 예를 들어, 복수의 프레임들의 토 큰들은 복수의 제1 모듈들에 적용됨으로써, 시간적 어텐션이 수행되거나, 복수의 제2 모듈에 적용됨으로써, 공 간적 어텐션이 수행될 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다. 도 5b는 본 개시의 일 실시예에 따른 디토큰화 모듈의 동작을 설명하기 위한 도면이다. 도 5b의 디토큰화가 수 행되는 복수의 토큰들은 본 명세서에 개시된 디토큰화 모듈에 입력되는 복수의 프레임들의 토큰들 중 하나의 프레임의 토큰들일 수 있다. 도 5b를 참조하면, 디토큰화 모듈은 디토큰화가 수행되는 복수의 토큰들에 대한 제2 임베딩을 수행하여 길이가 늘어난 복수의 토큰들을 획득할 수 있다. 예를 들어, 디토큰화 모듈은 제2 임베딩 을 통하여 d의 길이를 가지는 디토큰화가 수행되는 복수의 토큰들을 k x k x C의 길이를 가지는 길이 가 늘어난 복수의 토큰들로 변환할 수 있다. 일 실시예에서 디토큰화 모듈은 디토큰화가 수행되는 복 수의 토큰들을 제2 임베딩 레이어(미도시)에 적용함으로써, 길이가 늘어난 복수의 토큰들을 획득할 수 있다. 제2 임베딩 레이어(미도시)는 특정 길이의 토큰들을 입력으로 하여 특정 길이보다 길이가 늘어난 토큰 을 출력하도록 훈련된 레이어일 수 있으며, 제2 임베딩 레이어(미도시)의 신경망 파리미터들은 본 명세서에서 개시된 다양한 훈련들에 기초하여 최적화될 수 있다. 일 실시예에서, 디토큰화 모듈은 길이가 늘어난 복수의 토큰들을 복수의 패치들로 변환하고, 변환된 복수의 패치들을 합성하여 재구성된 피처맵을 획득할 수 있다. 예를 들어, 디토큰화 모듈은 k x k x C의 길이를 가지는 길이가 늘어난 복수의 토큰들을 k x k x C의 크기를 가지는 복수의 패치들로 변환하고, 변환된 복수의 패치들을 합성하여 (H + 2p) x (W + 2p) x C 의 크기를 가지는 재구성된 피처맵을 획득할 수 있 다. 일 실시예에서, 디토큰화 모듈은 변환된 복수의 패치들을 패딩된 피처맵이 토큰화 모듈에 의해 서로 인접한 복수의 패치들 사이에서 일정 영역만큼 겹치도록 분할될 때 복수의 패치들이 스트라이드된 간 격인 s 만큼 겹쳐지도록 합성할 수 있다. 이 경우, 서로 인접한 복수의 패치들 사이에서 겹쳐지는 영역의 값은 해당 영역에 대한 복수의 패치들의 값들을 합 또는 평균일 수 있으며, 반드시 이에 한정되는 것은 아니다. 일 실시예에서, 디토큰화 모듈은 재구성된 피처맵을 트리밍(trimming)하여 피처맵을 획득할 수 있다. 예를 들어, 디토큰화 모듈은 (H + 2p) x (W + 2p) x C 의 크기를 가지는 재구성된 피처맵을 높이 및 너비 각각에 대하여 p 만큼 트리밍을 수행하여 H x W x C의 크기를 가지는 피처맵을 획득할 수 있 다. 이와 같이, 디토큰화 모듈은 복수의 프레임들 각각의 토큰들의 디토큰화를 수행하여 복수의 프레임들 각각 의 피처맵을 획득할 수 있다. 일 실시예에서, 디토큰화 모듈을 통해 획득된 복수의 프레임들의 피처맵은 본 명세서에서 개시된 이미지 및 비디오 인페인팅을 위한 다양한 알고리즘에 이용될 수 있다. 예를 들어, 복수 의 프레임들의 피처맵은 디코더에 적용됨으로써, 복수의 프레임들의 이미지들이 획득될 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다.도 6은 본 개시의 일 실시예에 따른 시-공간적 어텐션을 수행하는 제2 모듈의 동작을 설명하기 위한 도면이다. 일 실시예에서, 도 6의 시-공간적 어텐션을 수행하는 제2 모듈은 본 명세서에서 설명하는 복수의 제2 모듈(450- 1, 450-2, 450-3, 450-4)들 중 하나일 수 있으며, 이하에서는 발명의 설명의 편의상 이를 시간적 어텐션 모듈 로 지칭하도록 하겠다. 일 실시예에서, 시간적 어텐션 모듈은 토큰화 모듈(미도시)와 디토큰화 모듈 (미도시)를 포함할 수 있으며, 토큰화 모듈(미도시)와 디토큰화 모듈(미도시)의 동작은 전술한 토큰화 모듈 및 디토큰화 모듈의 동작에 대응될 수 있다. 도 6을 참조하면, 전자 장치는 복수의 프레임들의 토큰들을 시간적 어텐션 모듈에 적용함으로써, 시- 공간적 어텐션이 수행된 복수의 프레임들의 토큰들을 획득할 수 있다. 일 실시예에서, 시간적 어텐션 모듈에 포함된 제1 정규화 레이어는 시간적 어텐션 모듈에 입력 된 복수의 프레임들의 토큰들에 대하여 레이어 정규화를 수행할 수 있다. 제1 정규화 레이어를 통하 여 레이어 정규화가 수행된 복수의 프레임들의 토큰들은 재차 쿼리(query) 토큰들, 키(key) 토큰들, 밸류 (value) 토큰들로 임베딩되어 시간적 어텐션 모듈에 포함된 시-공간 멀티-헤드 어텐션 모듈에 입력될 수 있다. 일 실시예에서, 시-공간적 멀티-헤드 어텐션 모듈은 시-공간적 멀티-헤드 어텐션 모듈에 입력된 복수 의 프레임들의 쿼리 토큰들, 키 토큰들, 밸류 토큰들에 대한 멀티-헤드 어텐션을 수행할 수 있다. 시-공간적 멀 티-헤드 어텐션 모듈은 입력된 복수의 프레임들의 쿼리 토큰들, 키 토큰들, 밸류 토큰들을 분할하여 복수 의 헤드들 각각에 입력하고, 복수의 헤드들 각각을 통해 입력된 복수의 프레임들의 쿼리 토큰들, 키 토큰들, 밸 류 토큰들로부터 의미 있는 특징을 추출할 수 있다. 복수의 헤드들 각각은 병렬적으로 신경망 연산을 수행하며, 서로 다른 관점에서 의미 있는 특징을 추출할 수 있도록 최적화된 신경망 파라미터들을 포함할 수 있다. 시-공 간적 멀티-헤드 어텐션 모듈은 복수의 헤드들 각각으로부터 추출된 특징들을 다시 합쳐서, 시-공간적 멀티 -헤드 어텐션이 수행된 복수의 프레임들의 토큰들을 생성할 수 있다. 시-공간적 멀티-헤드 어텐션이 수행된 복 수의 프레임들의 토큰들은 원래의 복수의 프레임들의 토큰들과 합쳐져서 시간적 어텐션 모듈에 포함 된 제2 정규화 레이어에 입력될 수 있다. 일 실시예에서, 제2 정규화 레이어는 제2 정규화 레이어에 입력된 복수의 프레임들의 토큰에 대하여 레이어 정규화를 수행할 수 있다. 제2 정규화 레이어를 통하여 레이어 정규화가 수행된 복수의 프레임들의 토큰 은 시간적 어텐션 모듈에 포함된 퓨전 피드 포워드(Fusion Feed Forward) 모듈에 입력될 수 있다. 일 실시예에서, 퓨전 피드 포워드 모듈은 아래와 같은 수학식에 따라서 퓨전 피드 포워드 모듈에 입 력된 복수의 프레임들의 토큰들의 신경망 연산을 수행할 수 있다. 수학식 4"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "여기에서, 는 퓨전 피드 포워드 모듈에 입력된 복수의 프레임들의 토큰들 중 하나를 의미하며, 는 퓨전 피드 포워드 모듈을 통하여 생성되는 복수의 프레임들의 토큰들 중 하나를 의미할 수 있다. 는 복수의 프 레임들의 총 개수를 나타내며, 은 각 프레임의 토큰들의 개수를 나타낼 수 있다. MLP 함수는 퓨전 피드 포워드 모듈에 포함된 다중 레이어 퍼셉트론(Mult-Layer Perceptron)에 의해 수행되는 연산을 의미할 수 있다. 또 한, SoftComp 함수는 퓨전 피드 포워드 모듈에 포함된 디토큰화 모듈에 의해 수행되는 연산을 의미하고, SoftSplit 함수는 퓨전 피드 포워드 모듈에 포함된 토큰화 모듈에 의해 수행되는 연산을 의미할 수 있다. 이와 같이, 시간적 어텐션 모듈에 포함된 퓨전 피드 포워드 모듈은 입력된 복수의 프레임들의 토큰들모두에 대한 신경망 연산을 수행하고, 신경망 연산이 수행된 복수의 프레임들의 토큰들 모두를 함께 생성한다는 점에서, 시간적 어텐션 모듈은 복수의 프레임들의 토큰들에 대한 시-공간적 어텐션을 수행할 수 있다. 일 실시예에서, 퓨전 피드 포워드 모듈로부터 획득된 복수의 프레임들의 토큰들은 제2 정규화 레이어(65 0)에 입력되기 전의 복수의 프레임들의 토큰들과 합쳐져서 시-공간적 어텐션이 수행된 복수의 프레임들의 토큰 들로 출력될 수 있다. 시-공간적 어텐션이 수행된 복수의 프레임들의 토큰들은 본 명세서에서 개시된 비디오 인페인팅을 위한 다양한 알고리즘에 이용될 수 있다. 예를 들어, 시-공간적 어텐션이 수행된 복수의 프 레임들의 토큰들은 복수의 제1 모듈 중 적어도 하나에 적용됨으로써, 공간적 어텐션이 수행되거나, 복수의 제3 모듈 중 적어도 하나에 적용됨으로써, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 복수의 프레임들의 모션 클루 피처맵들이 획득될 수 있으며, 반드시 상술한 예에 한정되는 것은 아니 다. 일 실시예에서, 시간적 어텐션 모듈은 후술하는 공간적 어텐션 모듈과 같이, 단일 프레임에 대한 동 작을 수행함으로써, 단일 훈련용 이미지로부터 획득되는 특징에 기초하여 공간적 어텐션을 수행할 수 있다. 다 시 말해, 시간적 어텐션 모듈이 전술한 제1 훈련을 수행할 때에는, 단일 훈련용 이미지로부터 획득되 는 특징에 기초하여 공간적 어텐션을 수행하기 때문에, 전자 장치는 단일 프레임의 토큰들을 시간적 어텐션 모 듈에 적용함으로써, 공간적 어텐션이 단일 프레임의 토큰들을 획득할 수 있다. 도 7은 본 개시의 일 실시예에 따른 공간적 어텐션을 수행하는 제1 모듈의 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른 공간적 어텐션을 수행하는 제1 모듈의 동작을 설명하기 위한 도면이다. 일 실시예에서, 도 7의 공간적 어텐션을 수행하는 제2 모듈은 본 명세서에서 설명하는 복수의 제1 모듈들(440- 1, 440-2, 440-3, 440-4) 중 하나일 수 있으며, 이하에서는 발명의 설명의 편의상 이를 공간적 어텐션 모듈 로 지칭하도록 하겠다. 일 실시예에서, 공간적 어텐션 모듈은 토큰화 모듈(미도시)와 디토큰화 모듈 (미도시)를 포함할 수 있으며, 토큰화 모듈(미도시)와 디토큰화 모듈(미도시)의 동작은 전술한 토큰화 모듈 및 디토큰화 모듈의 동작에 대응될 수 있다. 도 7을 참조하면, 전자 장치는 복수의 프레임들 각각의 토큰들을 공간적 어텐션 모듈에 적용함으로써, 공간적 어텐션이 수행된 복수의 프레임들 각각의 토큰들을 획득할 수 있다. 다시 말해, 제1 프레임의 토큰들인 에 기 초하여 공간적 어텐션이 수행된 제1 프레임의 토큰들인 가 획득되고, 제2 프레임의 토큰들인 에 기초하여 공간적 어텐션이 수행된 제2 프레임의 토큰들인 가 획득되고, T 시점의 프레임의 토큰들인 에 기초하여 공 간적 어텐션이 수행된 T 시점의 프레임의 토큰들인 가 획득될 수 있다. 즉, 각 프레임 내에서만 공간적 어텐 션이 수행되며, 각 프레임들의 공간적 어텐션은 다른 프레임들의 공간적 어텐션에 대해서 관여하지 않을 수 있 다. 도 7에서는 복수의 프레임들 중 현 프레임의 토큰들인 에 기초하여 공간적 어텐션이 수행된 현 프레 임의 토큰들인 를 획득하는 것만 도시하였으나, 실제로는, 복수의 프레임들 각각에 대하여 공간적 어텐 션이 동시 병렬적으로 수행될 수 있다. 따라서 이후의 서술부터는 공간적 어텐션 모듈의 동작을 각 프레임 하나에 대한 동작으로 서술한다. 일 실시예에서, 공간적 어텐션 모듈에 포함된 제1 정규화 레이어는 공간적 어텐션 모듈에 입력 된 현 프레임의 토큰들 에 대하여 레이어 정규화를 수행할 수 있다. 제1 정규화 레이어를 통하여 레이어 정규화가 수행된 해당 프레임의 토큰들은 공간적 어텐션 모듈에 포함된 모션 클루 퓨전 모듈 에 입력될 수 있다. 일 실시예에서, 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 하나로부터 획득된 인접한 프레임들의 이미 지 사이에서 존재하는 움직임에 대한 정보를 포함하는 복수의 프레임들의 모션 클루 피처맵들 중, 입력된 현 프 레임의 토큰들 에 상응하는 모션 플루 피쳐맵 가 모션 클루 퓨전 모듈에 입력될 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈는 제1 정규화 레이어를 통하여 레이어 정규화가 수행된 현 프레 임의 토큰들 및 현 프레임의 모션 클루 피처맵 에 기초하여 현 프레임과 인접한 프레임 사이에서 존재 하는 움직임에 대한 피쳐맵 레벨의 정보를 포함하는 현 프레임의 토큰들을 생성할 수 있다. 일 실시예에서, 모 션 클루 퓨전 모듈로부터 생성된 현 프레임의 토큰들은, 재차 쿼리(query) 토큰들, 키(key) 토큰들, 밸류(value) 토큰들로 임베딩되어 공간적 멀티-헤드 어텐션 모듈에 입력될 수 있다. 모션 클루 퓨전 모듈(73 5)의 구체적인 동작은 이하 도 8a 및 도 8b를 통하여 자세히 설명하도록 하겠다. 일 실시예에서, 공간적 어텐션 모듈은 모션 클루 퓨전 모듈을 포함하지 않거나, 공간적 어텐션 모듈 이 모션 클루 퓨전 모듈을 포함하더라도 모션 클루 퓨전 모듈의 동작을 비활성화 할 수 있다. 이 경우, 제1 정규화 레이어를 통하여 레이어 정규화가 수행된 토큰들은 모션 클루 퓨전 모듈에 입력 되지 않고, 공간적 어텐션 모듈에 포함된 공간적 멀티-헤드 어텐션 모듈에 입력될 수 있다. 일 실시예에서, 공간적 멀티-헤드 어텐션 모듈은 공간적 멀티-헤드 어텐션 모듈에 입력된 현 프레임 의 토큰들에 대한 멀티-헤드 어텐션을 수행할 수 있다. 공간적 멀티-헤드 어텐션 모듈은 입력된 현프레임 의 토큰들을 분할하여 복수의 헤드들 각각에 입력하고, 복수의 헤드들 각각을 통해 입력된 현 프레임의 토큰들 로부터 의미 있는 특징을 추출할 수 있다. 복수의 헤드들 각각은 병렬적으로 신경망 연산을 수행하며, 서로 다 른 관점에서 의미 있는 특징을 추출할 수 있도록 최적화된 신경망 파라미터들을 포함할 수 있다. 공간적 멀티- 헤드 어텐션 모듈은 복수의 헤드들 각각으로부터 추출된 특징들을 다시 합쳐서, 공간적 멀티-헤드 어텐션 이 수행된 현 프레임의 토큰들을 생성할 수 있다. 공간적 멀티-헤드 어텐션이 수행된 현 프레임의 토큰들은 원 래의 현 프레임의 토큰들 과 합쳐져서 공간적 어텐션 모듈에 포함된 제2 정규화 레이어에 입 력될 수 있다. 일 실시예에서, 제2 정규화 레이어는 제2 정규화 레이어에 입력된 현 프레임의 토큰들에 대하여 레이 어 정규화를 수행할 수 있다. 제2 정규화 레이어를 통하여 레이어 정규화가 수행된 현 프레임의 토큰들은 공간 적 어텐션 모듈에 포함된 퓨전 피드 포워드 모듈에 입력될 수 있다. 일 실시예에서, 퓨전 피드 포워드 모듈은 아래와 같은 수학식에 따라서 퓨전 피드 포워드 모듈에 입 력된 복수의 프레임들의 토큰들에 대한 신경망 연산을 수행할 수 있다. 수학식 5"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "여기에서, 는 퓨전 피드 포워드 모듈에 현 프레임의 토큰들 중 하나를 의미하며, 는 퓨전 피드 포워드 모듈을 통하여 생성되는 현 프레임의 토큰들 중 하나를 의미할 수 있다. 은 현 프레임의 토큰들의 개수를 나타낼 수 있다. MLP 함수는 퓨전 피드 포워드 모듈에 포함된 다중 레이어 퍼셉트론에 의해 수행되는 연산 을 의미할 수 있다. 또한, SoftComp 함수는 퓨전 피드 포워드 모듈에 포함된 디토큰화 모듈(미도시)에 의 해 수행되는 연산을 의미하고, SoftSplit 함수는 퓨전 피드 포워드 모듈에 포함된 토큰화 모듈(미도시)에 의해 수행되는 연산을 의미할 수 있다. 이와 같이, 공간적 어텐션 모듈에 포함된 퓨전 피드 포워드 모듈 은 입력된 복수의 프레임들 의 토큰들 중 특정 프레임의 토큰들에 대한 신경망 연산을 수행하고, 신경망 연산이 수행된 특정 프레임의 토큰들을 생성할 수 있다는 점에서, 공간적 어텐션 모듈은 복수의 프레임들 중 현 프레임의 토큰들 에 대한 공간적 어텐션을 동시 병렬적으로 모든 복수의 프레임들 각각에 걸쳐 수 행할 수 있다. 일 실시예에서, 퓨전 피드 포워드 모듈로부터 획득된 현 프레임의 토큰들은 제2 정규화 레이어에 입 력되는 현 프레임의 토큰들과 합쳐져서 공간적 어텐션이 수행된 현 프레임의 토큰들 로 출력될 수 있다. 공간적 어텐션이 수행된 현 프레임의 토큰들 은 본 명세서에서 개시된 이미지 또는 비디오 인페인팅을위한 다양한 알고리즘에 이용될 수 있다. 예를 들어, 공간적 어텐션이 수행된 복수의 프레임들의 토큰들은 복수 의 제2 모듈 중 적어도 하나에 적용됨으로써, 시-공간적 어텐션이 수행될 수 있으며, 반드시 상술한 예에 한정 되는 것은 아니다. 도 8a 및 도 8b는 본 개시의 일 실시예에 따른 모션 클루 퓨전 모듈의 동작을 설명하기 위한 도면이다. 도 8a의 모션 클루 퓨전 모듈(800-1, 800-2)은 도 7의 모션 클루 추정 모듈에 대응될 수 있다. 일 실시예에서, 도 8a 및 도 8b의 모션 클루 퓨전 모듈(800-1, 800-2)은 공간적 멀티-헤드 어텐션 모듈의 복수의 헤드들의 개수인 L 개의 동일한 구조를 가지며 동일한 동작을 수행하는 복수의 모듈들의 그룹으로 구성 될 수 있으며, 그룹에 포함된 복수의 모듈로부터 생성되는 토큰들(850-1)은, 재차 각 그룹마다 쿼리, 키, 밸류 토큰들로 임베딩되어, 각 모듈들에 대응하는 공간적 멀티-헤드 어텐션 모듈의 복수의 헤드들에 각각 입력 될 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-1, 800-2)에 입력되는 특정 프레임의 어텐션된 토큰들 은 제1 정규화 레이어를 통하여 레이어 정규화가 수행된 해당 특정 프레임의 토큰들이며, 모션 클루 퓨전 모듈(800-1, 800-2)의 복수의 모듈들 각각에 입력되기 위하여 L 개만큼 복제된 것일 수 있다. 일 실시예에 서, 모션 클루 퓨전 모듈(800-1, 800-2)에 입력되는 특정 프레임의 피처맵들은 복수의 제3 모듈들(460-1, 460-2, 460-3, 460-4) 중 하나로부터 획득된 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보 를 포함하는 복수의 프레임들의 모션 클루 피처맵들 중 하나의 현 프레임의 피쳐맵 일 수 있다. 일 실 시예에서, 모션 클루 퓨전 모듈(800-1, 800-2)로 입력되는 현 프레임의 모션 클루 피쳐맵 은 L 개의 3 차원 피쳐맵으로 구성되어 있어, 각 3차원 피쳐맵이 모션 클루 퓨전 모듈(800-1, 800-2) 내 각 그룹의 입력으로 각자 주어질 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-1, 800-2)은 토큰화 모듈 및 디토큰화 모 듈를 포함할 수 있으며, 토큰화 모듈 및 디토큰화 모듈의 동작은 전술한 토큰화 모듈 및 디토큰화 모듈의 동작에 대응될 수 있다. 도 8a를 참조하면, 전자 장치는 제1 프레임의 제2 토큰들 및 제1 프레임의 제1 피처맵에 기초하여 제1 프레임의 제3 토큰들을 획득하는 경우, 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 제1 프레임의 제4 토큰들을 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 프레임의 제4 토큰들 및 제1 프레임의 제2 토큰들에 기초하여 제1 프레임의 제3 토큰들을 획득할 수 있다. 모션 클루 퓨전 모듈(800-1)은 입력된 특정 프레임의 L 개의 3차원 피처맵들 중 하나의 피쳐맵을 토큰화 모듈에 적용함으로써, 모션 클루 토큰들을 획득할 수 있다. 토큰화 모듈에 입력된 특정 프레임 의 피처맵들은 토큰화가 수행되어 모션 클루 토큰들으로 변환될 수 있다. 여기에서, 특정 프레임의 L 개의 3차원 피처맵들 중 하나의 피쳐맵은 전술한 제1 프레임의 제1 피처맵에 대응되며, 모션 클루 토큰들 은 전술한 제1 프레임의 제4 토큰에 대응될 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-1)은 모션 클루 토큰들과 특정 프레임의 어텐션된 토큰들을 이어서 연결(Concatenate)하여 다중 레이어 퍼셉트론(미도시)에 적용함으로써, 혼합 토큰들(830-1)을 생성할 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-1)은 혼합 토큰들(830-1)과 모션 클루 토큰들의 성분-별 곱(element-wise product)을 수행하여 가중된 모션 토큰들(840-1)을 획득할 수 있다. 여기에서, 특정 프레임의 어텐션된 토큰들은 전술한 제1 프레임의 제2 토큰에 대응될 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-1)은 가중된 모션 토큰들(840-1)과 특정 프레임의 어텐션된 토큰들 을 이어서 연결하여, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 L 개 그룹들의 토큰들(850-1) 중 한 그룹의 토큰들을획득할 수 있다. 일 실시예에서, 인접한 프레임들 의 이미지들 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 토큰들(850-1)은 각 그릅의 토큰 들이 재차 쿼리(query) 토큰들, 키(key) 토큰들, 밸류(value) 토큰들로 임베딩되어 공간적 어텐션 모듈에 포함된 공간적 멀티-헤드 어텐션 모듈의 L개 헤드들 중 각각에 입력될 수 있다. 여기에서, 인접한 프레임 들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 L 개 그룹들의 토큰들(850-1) 중 한 그룹의 토큰들은 전술한 제1 프레임의 제3 토큰에 대응될 수 있다. 도 8b를 참조하면, 전자 장치는 제1 프레임의 제2 토큰들 및 제1 프레임의 제1 피처맵에 기초하여 제1 프레임의 제3 토큰들을 획득하는 경우, 제1 프레임의 제2 토큰들을 디토큰화 하여 제1 프레임의 제2 피처맵을 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 프레임의 제1 피처맵 및 제1 프레임의 제2 피처맵에 기초하여 제1 프레 임의 제3 피처맵을 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 프레임의 제3 피처맵을 토큰화하여 제1 프 레임의 제3 토큰들을 획득할 수 있다. 도 8b를 참조하면, 입력된 특정 프레임의 어텐션된 토큰들을 디토큰화 모듈에 적용함으로써, 어텐션 된 피처맵을 획득할 수 있다. 디토큰화 모듈에 입력된 특정 프레임의 어텐션된 토큰들은 디토큰 화를 통해 어텐션된 피처맵으로 변환될 수 있다. 여기에서, 특정 프레임의 어텐션된 토큰들은 전술한 제1 프레임의 제1 토큰에 대응되고, 어텐션된 피처맵은 전술한 제1 프레임의 제2 피처맵에 대응될 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-2)은 어텐션된 피처맵과 특정 프레임의 L 개의 3차원 피처맵들 중 하나의 피쳐맵을 이어서 연결하여 다중 레이어 퍼셉트론(미도시)에 적용함으로써, 혼합 피처맵(830- 2)을 생성할 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-2)는 혼합 피처맵(830-2)과 특정 프레임의 피처 맵들의 성분-별 곱을 수행하여 가중된 피처맵(840-2)를 획득할 수 있다. 일 실시예에서, 모션 클루 퓨전 모듈(800-2)은 어텐션된 피처맵과 가중된 피처맵(840-2)을 서로 연결하여 인접한 프레임들의 이미지 사이 에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 피처맵를 획득할 수 있다. 여기에서, 특정 프레임의 L 개의 3차원 피처맵들 중 하나의 피쳐맵은 전술한 제1 프레임의 제1 피처맵에 대응되며, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 피처맵은 전술한 제1 프레임의 제3 피처맵에 대응될 수 있다. 일 실시예에서, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 피 처맵을 토큰화 모듈에 적용함으로써, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정 보를 포함하는 특정 프레임의 L 개 그룹들의 토큰들(850-2) 중 한 그룹의 토큰들을 획득할 수 있다. 토큰화 모 듈에 입력된 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임의 피처맵은 토큰화를 통해 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특 정 프레임의 L 개 그룹들의 토큰들(850-2) 중 한 그룹의 토큰들로 변환될 수 있다. 일 실시예에서, 인접한 프레 임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임들의 L 개 그룹들의 토큰들(850- 2) 중 한 그룹의 토큰들은, 각 그룹마다 재차 쿼리 토큰들, 키 토큰들, 밸류 토큰들로 임베딩되어, 공간적 어텐 션 모듈에 포함된 공간적 멀티-헤드 어텐션 모듈의 L 개 헤드들 중 각각에 입력될 수 있다. 여기에서, 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 포함하는 특정 프레임들의 L 개 그룹들의 토큰들(850-2) 중 한 그룹의 토큰들은 전술한 제1 프레임의 제3 토큰들에 대응될 수 있다. 도 9a, 도 9b 및 도 9c는 본 개시의 일 실시예에 따른 전자 장치가 제1 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행하는 방법을 설명하기 위한 도면이다. 도 9a를 참조하면, 전자 장치는 제2 복원 대상 영역을 마스킹한 훈련용 동영상과 제2 복원 대상 영역을 표 시하는 동영상을 인페인팅 모델에 적용함으로써, 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상 을 생성할 수 있다. 일 실시예에서, 전자 장치는 본래의 제2 복원 대상 영역을 마스킹하지 않고 그대로 포 함하는 훈련용 동영상(미도시) 및 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 제1 손실 함수를 계산하고, 계산된 제1 손실 함수에 기초하여 인페인팅 모델의 제2 훈련을 수행할 수 있다. 도 9a의 제1 손실 함수에 기초한 인페인팅 모델의 제2 훈련은 도 3에서 설명한 인페인팅 모델의 요소화된 훈 련 중 하나인 제2 훈련에 포함될 수 있다. 일 실시예에서, 인페인팅 모델은 인코더, 디코더, 토큰화 모듈, 디토큰화 모듈, 복수의 제 1 모듈들(940-1, 940-2, 940-3, 940-4), 복수의 제2 모듈들(950-1, 950-2, 950-3, 950-4), 복수의 제3 모듈들 (960-1, 960-2, 960-3, 960-4) 및 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4)을 포함할 수 있다. 여기에 서, 인코더, 디코더, 토큰화 모듈, 디토큰화 모듈, 복수의 제1 모듈들(940-1, 940-2, 940-3, 940-4), 복수의 제2 모듈들(950-1, 950-2, 950-3, 950-4) 및 복수의 제3 모듈들(960-1, 960-2, 960-3, 960-4)의 동작들은 전술한 도 4b의 인코더, 디코더, 토큰화 모듈, 디토큰화 모듈, 복수의 제1 모듈들(440-1, 440-2, 440-3, 440-4), 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4) 및 복수의 제3 모 듈들(460-1, 460-2, 460-3, 460-4)의 동작들을 수행할 수 있다. 또한, 이하에서 설명하는 인페인팅 알고리즘은 도 4a 및 도 4b에서 설명한 인페인팅 모델이 타겟 복원 대상 영역을 포함하는 타겟 동영상에 대한 인페인팅을 수행하는 추론 단계에도 동일하게 적용될 수 있다. 일 실시예에서, 전자 장치는 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이의 옵 티컬 플로우를 획득할 수 있다. 일 실시예에서, 전자 장치는 생성된 옵티컬 플로우에 기초하여 제1 손실 함수를 계산하고, 제1 손실 함수에 기초한 인페인팅 모델의 제2 훈련을 수행할 수 있다. 일 실시예에서, 옵티컬 플로우는 복수의 제3 모듈들(960-1, 960-2, 960-3, 960-4) 중 하나로부터 획득되는 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득될 수 있다. 일 실시예에서, 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4)은 복수의 제2 모듈들(450-1, 450-2, 450-3, 450-4)로부터 획득한 시간적 어텐션이 수행된 복수의 프레임들의 토큰들 및 복수의 제3 모듈들(960-1, 960-2, 960-3, 960-4)로부터 획득한 피처맵에 기초하여 옵티컬 플로우를 생성할 수 있다. 일 실시예에서, 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4)은 인접한 프레임들의 이미지 사이의 옵티컬 플로우를 생성한다는 점에서, 옵티컬 플로우 생성 모듈로 지칭할 수 있다. 도 9b를 참조하면, 전자 장치는 특정 프레임의 토큰들에 기초하여 특정 프레임의 모션 피처맵과 컨텍스트 피처 맵을 획득할 수 있다. 일 실시예에서, 모션 피처맵은 프레임이 변화함에 따라 이미지 내의 오브젝트 또는 픽셀 의 움직임으로부터 획득 가능한 픽셀들의 특징들을 포함할 수 있다. 일 실시예에서, 컨텍스트 피처맵은 프레임 의 변화와 무관하게 이미지 자체로부터 획득 가능한 픽셀들의 특징들을 포함할 수 있다. 일 실시예에서, 전자 장치는 제2 시간적 어텐션 모듈(950-2)을 통해 제2 프레임의 어텐션된 토큰들(951-2b)을 획득할 수 있다. 제2 프레임의 어텐션된 토큰들(951-2b)은 제2 시간적 어텐션 모듈(950-2)의해 시간적 어텐션이 수행된 제2 프레임의 토큰을 의미할 수 있다. 일 실시예에서, 전자 장치는 제2 프레임의 어텐션된 토큰(951-2b)을 디토큰화 모듈에 적용함으로써, 제2 프레임의 어텐션된 피처맵(952-2b)를 획득할 수 있다. 디토큰화 모듈의 동작은 전술한 도 4a 및 4b의 디토 큰화 모듈의 동작들을 수행할 수 있다. 디토큰화 모듈에 입력된 제2 프레임의 어텐션된 토큰들(951- 2b)은 디토큰화를 통해 제2 프레임의 어텐션된 피처맵(952-2b)으로 변환될 수 있다. 일 실시예에서, 전자 장치는 제2 프레임의 어텐션된 피처맵(952-2b)을 선형 투영 레이어에 적용함으로써, 제2 프레임의 모션 피처맵(953-2b) 및 제2 프레임의 컨텍스트 피처맵(953-2b)를 획득할 수 있다. 일 실시예에서, 선형 투영 레이어는 특정 프레임의 피처맵을 입력으로 하여 특정 프레임의 모션 피처맵 및 특 정 프레임의 컨텍스트 피처맵을 출력하도록 훈련된 모델일 수 있다. 이와 같이, 본 개시의 일 실시예에 따른 전 자 장치는 하나의 선형 투영 레이어를 이용하여 모션 피처맵과 컨텍스트 피처맵을 함께 획득할 수 있다는 점에 서, 인페인팅 모델의 훈련 및 추론 과정에서 하드웨어 자원을 최적화하여 효율적으로 활용할 수 있다. 도 9c를 참조하면, 전자 장치는 제2 시간적 어텐션 모듈(950-2)로부터 획득된 제1 프레임의 모션 피처맵(953- 2a) 및 제2 프레임의 모션 피처맵(953-2b)을 제2 모션 클루 추정 모듈(960-2)에 적용함으로써, 제1 프레임의 모 션 클루 피처맵(961-2b)를 획득할 수 있다. 여기에서, 제2 프레임은 제1 프레임의 직후 프레임을 의미하며, 제1 프레임의 모션 클루 피처맵(961-2b)은 도 8a 및 8b에서 설명한 특정 프레임의 L 개의 3차원 피처맵들에 대 응될 수 있다. 일 실시예에서, 제1 프레임의 모션 클루 피처맵(961-2b)은 제2 공간적 어텐션 모듈(940-2) 및 제 2 옵티컬 플로우 생성 모듈(970-2)에 입력될 수 있다. 일 실시예에서, 제2 모션 클루 추정 모듈(960-2)는 제1 프레임의 모션 피처맵(953-2a) 및 제2 프레임의 모션 피 처맵(953-2b)에 기초하여 4D 코스트 볼륨(962-2)을 생성할 수 있다. 4D 코스트 볼륨(962-2)은 제1 프레임의 모 션 피처맵(953-2a)의 복수의 픽셀들과 제2 프레임의 모션 피처맵(953-2b)의 복수의 픽셀들 사이의 시각적 유사 성을 나타내는 복수의 픽셀들의 2D 코스트 맵(963-2)을 포함할 수 있다. 예를 들어, 제1 프레임의 모션 피처맵 (953-2a) 및 제2 프레임의 모션 피처맵(953-2b)의 크기가 H x W인 경우, 4D 코스트 볼륨(962-2)은 H x W 개의 복수의 픽셀들의 H x W 크기의 2D 코스트 맵(953-2a)을 포함할 수 있다. 일 실시예에서, 시각적 유사성은 비교 대상의 픽셀들 사이의 L1 거리, L2 거리 및 코사인 유사도 등에 기초하여 계산될 수 있으며, 반드시 상술한 예 에 한정되는 것은 아니다. 일 실시예에서, 제2 모션 클루 추정 모듈(960-2)는 복수의 픽셀들의 2D 코스트 맵(953-2a)의 토큰화를 수행하여 토큰화된 복수의 픽셀들의 각각의 2D 코스트 맵을 획득할 수 있다. 일 실시예에서, 제2 모션 클루 추정 모듈 (960-2)는 획득된 토큰화된 복수의 픽셀들의 2D 코스트 맵(963-2)에 기초하여 복수의 픽셀들 각각의 키 벡터 및 벨류 벡터를 획득할 수 있다. 일 실시예에서, 제2 모션 클루 추정 모듈(960-2)은 획득된 복수의 픽셀들 각각의 키 벡터, 벨류 벡터 및 그들의 쿼리 벡터에 대응하는 학습 가능한 코드(963-2)에 기초한 크로스 어텐션(Cross Attention)을 수행하여 제2 프레임의 모션 클루 피처맵(961-2b)을 생성할 수 있다. 일 실시예에서, 학습 가능한 코드(963-2)는 무작위로 초기화된 1차원 벡터들 L 개로 구성된 2차원 벡터일 수 있으며, 본 명세서에서 개시된 다양한 인페인팅 모델의 훈련을 통해 최적화될 수 있다. 일 실시예에서, 제2 모션 클루 추정 모듈(960-2)은 아 래와 같은 수학식에 기초하여 모션 클루 피처맵(961-2b)을 생성할 수 있다.수학식 6"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, , 및 C는, 순서대로, 복수의 픽셀들의 2D 코스트 맵(963-2), 제1 프레임의 모션 클루 피처맵 (961-2b) 및 학습 가능한 코드(963-2)을 의미하고, 는 2D 코스트 맵(963-2)에서의 복수의 픽셀들의 위치를 임베딩한 벡터를 의미할 수 있다. 일 실시예에서, 제1 프레임의 모션 클루 피처맵(961-2b)의 크기는 복수의 픽셀들의 각각의 2D 코스트 맵(963- 2)의 크기 및 학습 가능한 코드(964-2)의 크기에 기초하여 결정될 수 있다. 예를 들어, 복수의 픽셀들의 각각의 2D 코스트 맵(963-2)의 크기가 H x W이고, 학습 가능한 코드(964-2)의 크기가 D x L 이면, 제1 프레임의 모션 클루 피처맵(961-2b)의 크기는 H x W x D x L일 수 있다. 일 실시예에서, 학습 가능한 코드(964-2)의 크기를 결 정하는 차원 중 하나는 도 7의 공간적 멀티-헤드 어텐션 모듈의 복수의 헤드들의 개수에 대응되도록 결정 될 수 있다. 예를 들어, 학습 가능한 코드의 크기(964-2)를 결정하는 차원 중 하나인 L의 값은 멀티-헤드 어텐 션 모듈의 복수의 헤드들의 개수 L과 동일할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 4D 코스트 볼륨(962-2) 및 제1 옵티컬 플로우 생성 모듈 (970-1)로부터 획득되는 제1 옵티컬 플로우(971-1b)에 기초하여 복수의 픽셀들의 코스트 패치(965-2)를 획득할 수 있다. 여기에서, 제1 옵티컬 플로우(971-1b)는 제1 옵티컬 플로우 생성 모듈(970-1)이 생성한 제1 프레임의 이미지와 제2 프레임의 이미지 사이의 옵티컬 플로우를 의미할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 복수의 픽셀들 각각의 위치에 제1 옵티컬 플로우(971- 1b)를 더하여 복수의 픽셀들 각각의 기준 위치를 계산할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈 (970-2)은 4D 코스트 볼륨(962-2)에 포함된 복수의 픽셀들의 2D 코스트 맵 상에서, 복수의 픽셀들의 기준 위 치를 중심으로 기설정된 거리에 포함되는 영역을 크롭하여 복수의 픽셀들의 코스트 패치(965-2)를 획득할 수 있 다. 예를 들어, 코스트 패치는 특정 픽셀의 2D 코스트 맵 상에서, 특정 픽셀의 기준 위치인 (x, y)를 중심으 로 하는 3 x 3 크기를 가지도록 특정 픽셀의 2D 코스트 맵 의 (x - 1, y - 1), (x - 1, y + 1), (x + 1, y - 1) 및 (x + 1, y + 1)의 지점들을 꼭지점으로 하는 영역으로 크롭되어 생성될 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 획득된 코스트 패치(964-2)에 기초하여 코스트 쿼리 벡 터를 획득하고, 제1 프레임의 모션 클루 피처맵(961-2b)에 기초하여 코스트 키 벡터 및 코스트 벨류 벡터를 획 득할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 추출된 코스트 쿼리 벡터, 코스트 키 벡 터 및 코스트 큐 벡터들에 기초한 크로스 어텐션을 수행하여 코스트 피처맵을 생성할 수 있다, 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 제1 프레임의 컨텍스트 피처맵(954-2b), 제1 옵티컬 플 로우(971-1b), 코스트 패치(964-2) 및 코스트 피처맵을 GMA-Update 모듈(972-2)에 적용함으로써, 변화량 플로우 Flow를 생성할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)는 GMA-Update 모듈(972-2)에 의 해 획득된 변화량 플로우 Flow를 제1 옵티컬 플로우(971-1b)에 더하여 제1 옵티컬 플로우(971-1b)의 갱신을 수 행하고, 갱신된 제1 옵티컬 플로우(971-1b)를 업샘플링 모듈에 적용함으로써, 제2 옵티컬 플로우를(971-2b) 생 성할 수 있다. 제2 옵티컬 플로우를(971-2b)는 제2 옵티컬 플로우 생성 모듈(970-2)이 생성한 제1 프레임 이미 지와 제2 프레임 이미지 사이의 옵티컬 플로우를 의미할 수 있다. 일 실시예에서, GMA-Update 모듈(972-2)은 아 래와 같은 수학식에 기초하여 Flow를 생성할 수 있다.수학식 7"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "여기에서, 는 코스트 피처맵을 의미하고, 는 복수의 픽셀들의 코스트 패치(964-2)를 의미하고, 는 제1 옵티컬 플로우(971-1b)를 의미할 수 있다. 또한, net은 제1 프레임의 컨텍스트 피처맵(954-2b)을 채널 축으로 절반으로 나눠서 생성되는 제1 프레임의 제1 서브 컨텍스트 피처맵을 의미하고, inp는 나머지 절반인 제1 프레 임의 제2 서브 컨텍스트 피처맵을 의미하며, att는 제1 서브 컨텍스트 피처맵에 대한 셀프 어텐션이 수행되어 획득되는 피처맵을 의미할 수 있다. 일 실시예에서, GMA-Update 모듈(972-2)은 모션 인코더(미도시)를 포함하며, 모션 인코더(미도시)는 수학식 7에 서, MotionEncoder 함수에 대응되는 동작을 수행할 수 있다. 일 실시예에서, 모션 인코더(미도시)는 입력된 피 처맵의 크기를 줄이면서, 옵티컬 플로우의 생성에 필요한 중요한 정보를 추출하기 위한 모델일 수 있다. 일 실시예에서, GMA-Update 모듈(972-2)은 합성곱 게이트 순환 유닛(Convolutional Gated Recurrent Unit)(미 도시)를 포함할 수 있으며, 합성곱 게이트 순환 유닛(미도시)은 수학식 7에서, ConvGRU 함수에 대응되는 동작을 수행할 수 있다. 일 실시예에서, 합성곱 게이트 순환 유닛(미도시)은 이전 프레임에 대한 출력 데이터를 현재 프레임에 대한 입력 데이터와 함께 고려하여 현재 프레임의 출력 데이터를 생성하기 위한 모델일 수 있다. 일 실시예에서, GMA-Update 모듈(972-2)은 FlowHead 모듈(미도시)를 포함할 수 있으며, FlowHead 모듈(미도 시)는 수학식 7에서, FlowHead 함수에 대응되는 동작을 수행할 수 있다. 일 실시예에서, FlowHead 모듈(미도 시)은 복수의 컨볼루션 레이어들을 이용하여 합성곱 게이트 순환 유닛으로부터 출력된 데이터에 기초하여 복수 의 픽셀들에 대한 옵티컬 플로우를 예측하기 위한 모델일 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 제1 옵티컬 플로우(971-1b)의 갱신을 반복하여 수행할 수 있다. 일 실시예에서, 제2 옵티컬 플로우 생성 모듈(970-2)은 제1 옵티컬 플로우(971-1b)의 갱신이 반복하여 수행될 때 마다, 갱신된 제1 옵티컬 플로우(971-1b)를 업샘플링 모듈에 적용함으로써, 제2 옵티컬 플로우를 (971-2b) 반복하여 생성할 수 있다. 일 실시예에서, 반복하여 생성된 제2 옵티컬 플로우를(971-2b) 각각은 이하 에서 설명하는 제1 손실 함수를 계산하는데 이용될 수 있다. 일 실시예에서, 제3 모션 클루 추정 모듈(960-3) 및 제3 옵티컬 플로우 생성 모듈(970-3)은 전술한 제2 모션 클 루 추정 모듈(960-2) 및 제2 옵티컬 플로우 생성 모듈(970-2)의 동작들을 동일하게 수행할 수 있다. 예를 들어, 제3 모션 클루 추정 모듈(960-3)은 제3 시간적 어텐션 모듈(950-3)로부터 획득되는 제1 프레임의 모션 피처맵 (953-3a) 및 제2 프레임의 모션 피처맵(953-3b)에 기초하여 제1 프레임의 모션 클루 피처맵(961-3b)를 생성하고, 제3 옵티컬 플로우 생성 모듈(970-3)은 제2 옵티컬 플로우(971-2b), 제2 프레임의 모션 클루 피처맵 (961-3b) 및 제3 시간적 어텐션 모듈(950-3)로부터 획득되는 제1 프레임의 컨텍스트 피처맵(953-3b)에 기초하여 제3 옵티컬 플로우(971-3b)를 생성할 수 있다. 일 실시예에서, 전자 장치는 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4)로부터 획득되는 옵티컬 플로우에 기초하여 제1 손실 함수를 계산할 수 있다. 일 실시예에서, 인페인팅 모델은 계산된 제1 손실 함수에 기초하여 제2 훈련이 수행될 수 있다. 일 실시예에서, 제1 손실 함수는 옵티컬 플로우를 매개 변수로 하는, 및 중 적어도 하나를 포함할 수 있다. 일 실시예에 서, , 및 은 아래와 같은 수학식에 기초하여 계산될 수 있다. 수학식 8"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "여기에서, B는 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4)의 개수를 의미하고, R은 복수의 제4 모듈들 (970-1, 970-2, 970-3, 970-4)에 의해 옵티컬 플로우가 반복하여 생성된 횟수를 의미한다. 는 t 시점의 프레임 이미지와 s 시점의 프레임의 이미지 사이의 옵티컬 플로우에 기초하여 t 시점의 프레임의 이미지를 s 시 점의 프레임 이미지로 워핑을 수행하는 함수를 의미한다. 는 워핑에 의해 이미지 상의 특정 픽셀이 다른 픽셀 에 의해 가려져서 더 이상 볼 수 없는 영역을 나타내는 이진화된 마스크를 의미하며, 유효한 픽셀들에 대해서는 1의 값을 가지고, 유효하지 않은 즉, 가려진(Occluded) 픽셀들에 대해서는 0의 값을 가진다. 은 각 제4 모 듈에서 반복하여 생성된 옵티컬 플로우 각각에 대하여 적용되는 가중치를 의미하고, 반복하여 생성된 옵티컬 플 로우들 중 더 나중에 생성된 옵티컬 플로우가 더 최적화된 가중치를 가지며, 또한 더 나중의 제4 모듈에서 생성 된 옵티컬 플로우들이 이전의 제4 모듈들 에서 생성된 옵티컬 플로우들 보다 더 최적화된 가중치를 갖는다. 각 제4 모듈 안에서는 R번 반복하여 옵티컬 플로우를 생성할 수 있으며, 전체 인페인팅 모델은 B개의 제4 모듈들을 포함할 수 있다. 는 아래와 같은 수학식에 대응되는 로버스트(robust) 함수를 의미할 수 있다. . 수학식 9"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 10, "content": "일 실시예에서, 복수의 제4 모듈들(970-1, 970-2, 970-3, 970-4) 중 인페인팅 모델에서 앞쪽에 위치한 모듈로부 터 생성되거나 반복하여 갱신된 횟수가 적은 옵티컬 플로우들은 실제 정답에서 벗어난 데이터 즉, 아웃 라이어 에 해당될 가능성이 높다. 이에 따라, 본 개시의 일 실시예에 따른 전자 장치는 로버스트 함수를 이용하여 제1 손실 함수를 계산하여 인페인팅 모델의 훈련을 수행할 수 있다는 점에서, 훈련 과정에서 아웃 라이어에 해당하 는 옵티컬 플로우에 의해 발생되는 문제를 예방할 수 있다. 또한, 계산 중인 옵티컬 플로우는 불완전하여 만 으로는 가려진 픽셀이 완전히 걸러지지 않으므로, 로버스트 함수를 통해 다른 프레임으로 전파되는 아티팩트에 의한 아웃 라이어를 감소시켜 안정적인 계산을 돕는다. 도 10은 본 개시의 일 실시예에 따른 전자 장치가 제2 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행하는 방법을 설명하기 위한 도면이다. 도 10을 참조하면, 전자 장치는 제2 복원 대상 영역을 마스킹한 훈련용 동영상 및 제2 복원 대상 영역을 표시하는 마스크 동영상 에 아핀 변환을 수행하여 제1 아핀 변환된 훈련용 동영상 및 제1 아핀 변환된 마스크 동영상을 획득할 수 있다. 일 실시예에서, 전자 장치는 제1 아핀 변환된 훈련용 동영상 및 제1 아핀 변환된 마스크 동영상을 인페인팅 모델에 적용함으로써, 복원된 제1 아핀 변환된 훈련용 동 영상(1006-1)을 획득할 수 있다. 일 실시예에서, 전자 장치는 제2 복원 대상 영역을 마스킹한 훈련용 동영상 및 제2 복원 대상 영역을 표시 하는 마스크 동영상를 인페인팅 모델에 적용함으로써, 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영 상을 획득할 수 있다. 일 실시예에서, 전자 장치는 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상 에 아핀 변환을 수행하여 복원된 제2 아핀 변환된 훈련용 동영상(1006-2)을 획득할 수 있다. 일 실시예에서, 전자 장치는 복원된 제1 아핀 변환된 훈련용 동영상(1006-1) 및 복원된 제2 아핀 변환된 훈련용 동영상(1006-2)에 기초하여 제2 손실 함수를 계산하고, 계산된 제2 손실 함수에 기초하여 인페인팅 모델의 제2 훈련을 수행할 수 있다. 제2 손실 함수에 기초한 인페인팅 모델의 제2 훈련은 도 3에서 설명한 인 페인팅 모델의 요소화된 훈련 중 하나인 제2 훈련에 포함될 수 있다. 일 실시예에서, 제2 손실 함수는 아 래와 같은 수학식에 기초하여 계산될 수 있다. 수학식 10"}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 11, "content": "여기에서, 는 복수의 프레임들 중 t 번째 프레임의 이미지에 수행되는 아핀 변환을 의미한다. s는 전술한 수학식 6, 7 및 8에서 적용되는 가중치 과 호환성을 유지하기 위한 스케일 팩터를 의미한다. 는 아핀 변 환에 의해 이미지 상의 특정 픽셀이 다른 픽셀에 의해 가려져서 더 이상 볼 수 없는 영역을 나타내는 이진화된 마스크를 의미하며, 유효한 픽셀들에 대해서는 1의 값을 가지고, 유효하지 않은 즉, 가려진(Occluded) 픽셀들에 대해서는 0의 값을 가진다. 는 수학식 9에서 설명한 로버스트 함수이며, 전술한 로버스트 함수를 이용하여 기대할 수 있는 효과를 제2 손실 함수에 기초한 인페인팅 모델의 훈련에도 동일하게 기대할 수 있다. 일 실시예에서, 전자 장치는 다양한 종류의 아핀 변환을 수행할 수 있다. 예를 들어, 이동(Translation) 변환, 회전(Rotation) 변환, 스케일(Scale) 변환 및 선단(Shear) 변환 등을 수행할 수 있으며, 각 변환에 따라 수행될 수 있는 범위가 기설정되어 있을 수 있다. 예를 들면 이동 변환은 특정 방향에 대해서 최대 4 픽셀까지 가능하 고, 회전 변환은 최대 6 도까지 가능하고, 스케일 변환은 0.94 배부터 1.06 배까지 가능하고, 선단(Shear) 변환 은 최대 1도까지 가능할 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다. 일 실시예에서, 전자 장치는 특 정 프레임의 이미지에 대해서 전술한 다양한 종류의 아핀 변환들 중 적어도 하나의 아핀 변환을 기설정된 범위 내에서 선택하여 수행할 수 있으며, 전술된 아핀 변환 외의 다른 아핀 변환을 선택하여 수행할 수도 있다. 이와 같이, 본 개시의 일 실시예에 따른 전자 장치는 아핀 변환을 수행하여 획득될 수 있는 이미지들에 기초하 여 손실 함수를 계산하고, 계산된 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행할 수 있다. 따라서, 인페 인팅 모델은 타겟 동영상 이미지에 아핀 변환과 유사한 변화가 발생하더라도 더 자연스러운 인페인팅 결과물을 얻을 수 있도록 이에 대한 민감성이 줄어들 수 있다. 도 11a, 11b 및 11c는 본 개시의 일 실시예에 따른 복수의 디코더를 이용한 인페인팅 모델의 훈련을 설명하기 위한 도면이다. 도 11a, 도 11b 및 도 11c에서 인페이팅 모델은 토큰화 모듈(미도시) 및 디토큰화 모듈(미도 시)을 포함할 수 있으며, 토큰화 모듈(미도시) 및 디토큰화 모듈(미도시)의 동작 및 기능은 전술한 도 4a 및 도 4b의 토큰화 모듈 및 디토큰화 모듈에 대응될 수 있다. 도 11a를 참조하면, 인페인팅 모델은 제1 훈련(1100-1) 전의 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 중 하나 이상 및 복수의 제2 모듈(321-1, 321-2, 321-3, 321-4) 중 하나 이상으로부터 각각 획득되는 둘 이상의 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 제1 훈련(1100-1)이 수행될 수 있다. 여 기에서, 제1 훈련(1100-1)은 도 3의 인페인팅 모델의 요소화된 훈련 중 하나인 제1 훈련에 포함될 수 있다. 일 실시예에서, 전자 장치는 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지를 인페인팅 모델에 적용 함으로써, 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지와 제1 내지 제4 중간 출력 단일 훈 련용 이미지(1120-1, 1120-2, 1120-3, 1120-4)를 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 제1 내지 제4 단일 훈련용 이미지 입력에 대해 공간적 어텐션을 수행하는 시간 적 어텐션 모듈(321-1, 321-2, 321-3, 321-4) 각각으로부터 획득된 어텐션된 토큰들을 디토큰화 모듈(미도시)에 적용하여 어텐션된 피처맵을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 획득된 어텐션된 피처맵을 중간 디코더(intermediate decoder)(1110-1)에 적용함으로써, 제1 내지 제4 중간 출력 단일 훈련용 이미지(1120-1, 1120-2, 1120-3, 1120-4)를 획득할 수 있다. 일 실시예에서, 중간 디코더(1110-1)는 도 4a, 4b의 디코더와 동일한 동작을 수행하되, 인페인팅 모델의 최종 레이어가 아닌 중간 레이어에서 출력되는 데이터에 기초하여 중간 출력 이미지를 획득하기 위한 디코더를 의미할 수 있다. 일 실시예에서, 제1 내지 제4 중간 디코더(1110-1)는 어텐션된 피처맵을 입력으로 하여 제1 내 지 제4 중간 출력 단일 훈련용 이미지(1120-1, 1120-2, 1120-3, 1120-4)를 출력하도록 훈련된 모델일 수 있으며, 본 명세서에서 개시된 다양한 훈련들을 수행함으로써, 최적화 될 수 있다. 일 실시예에서, 중간 디코더(1110-1)의 신경망 파라미터들은 최종 디코더의 신경망 파라미터와 서로 공유 될 수 있으며, 중간 디코더(1110-1)는 공간적 어텐션과 동일하게 동작하는 제1 내지 제4 시간적 어텐션 모듈 (321-1, 321-2, 321-3, 321-4) 중 일부와 연결되지 않음으로써, 제1 내지 제4 중간 출력 단일 훈련용 이미지 (1120-1, 1120-2, 1120-3, 1120-4) 중 일부는 생성되지 않을 수 있다. 일 실시예에서, 중간 디코더(1110-1)는 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 중 일부 또는 전체와 연결되고, 복수의 제1 모듈들(311-1, 311-2, 311-3, 311-4) 중 일부 또는 전체로부터 획득되는 어텐션된 토큰들 에 기초하여 중간 출력 단일 훈련용 이미지를 출력할 수 있다. 일 실시예에서, 전자 장치는 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지와 제1 내지 제4 중간 출력 단일 훈련용 이미지(1120-1, 1120-2, 1120-3, 1120-4)에 기초하여 인페인팅 모델의 제1 훈련(1000- 1)을 수행할 수 있다. 일 실시예에서, 전자 장치는 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지 에 기초하여 계산되는 손실 함수와 제1 내지 제4 중간 출력 단일 훈련용 이미지(1120-1, 1120-2, 1120-3, 1120-4)에 기초하여 계산되는 손실 함수를 합하여 인페인팅 모델의 제1 훈련(1110-1)을 수행할 수 있다. 도 11b 및 도 11c를 참조하면, 인페인팅 모델은 복수의 제1 모듈들(940-1, 940-2, 940-3, 940-4)복수의 제2 모 듈들(950-1, 950-2, 950-3, 950-4) 중 둘 이상으로부터 각각 획득되는 둘 이상의 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 제2 훈련(1100-2, 1100-3)이 수행될 수 있다. 여기에서, 제2 훈련(1100-2, 1100-3)은 도 3의 인페인팅 모델의 요소화된 훈련 중 제2 훈련에 포함될 수 있다. 일 실시예에서, 전자 장치는 제1 손실 함수에 기초하여 인페인팅 모델의 제2 훈련(1100-2)을 수행하는 경우, 제 2 복원 대상 영역을 마스킹한 훈련용 동영상 및 제2 복원 대상 영역을 표시하는 마스크 동영상을 인 페인팅 모델에 적용함으로써, 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상과 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130-4)을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 제1 내지 제4 시간적 어텐션 모듈(950-1, 950-2, 950-3, 950-4) 각각으로부터 획득되는 복수의 프레임들의 어텐션된 토큰들을 디토큰화 모듈(미도시)에 적용하여 복수의 프레임들의 어텐션된 피처맵들을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 획득된 복수의 프레임들의 어텐션된 피처맵들을 중간 디코더(1110-2)에 적용함으로써, 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130- 4)을 획득할 수 있다. 일 실시예에서, 중간 디코더(1110-2)는 복수의 프레임들의 어텐션된 피처맵을 입력으로 하여 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130-4)를 출력하도록 훈련된 모델일 수 있으며, 본 명세서에서 개시된 다양한 훈련들을 수행함으로써, 최적화 될 수 있다. 일 실시예에서, 중간 디코더(1110-2)의 신경망 파라미터들은 최종 디코더의 신경망 파라미터와 서로 공유 될 수 있으며, 중간 디코더(1110-2) 중 일부는 제1 내지 제4 시간적 어텐션 모듈(950-1, 950-2, 950-3, 950-4) 중 일부와 연결되지 않음으로써, 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130-4) 중 일부는 생성되지 않을 수 있다. 일 실시예에서, 중간 디코더(1110-2)는 제1 내지 제4 공간적 어텐션 모듈(940-1, 940-2, 940-3, 940-4) 중 일 부 또는 전체와 연결되고, 제1 내지 제4 공간적 어텐션 모듈(940-1, 940-2, 940-3, 940-4) 중 일부 또는 전체로 부터 획득되는 어텐션된 토큰들에 기초하여 중간 출력 훈련용 동영상을 출력할 수 있다. 일 실시예에서, 전자 장치는 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상과 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130-4)에 기초하여 인페인팅 모델의 제2 훈련(1100-2)을 수행할 수 있다. 일 실시예에서, 전자 장치는 복원된 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여 계산되는 제1 손실 함수와 제1 내지 제4 중간 출력 훈련용 동영상(1130-1, 1130-2, 1130-3, 1130-4) 기초하여 계산되는 제1 손실 함수를 합하여 인페인팅 모델의 제2 훈련(1110-2)을 수행할 수 있다. 일 실시예에서, 전자 장치는 제2 손실 함수에 기초하여 인페인팅 모델의 제2 훈련(1100-3)을 수행하는 경우, 제 2 복원 대상 영역을 마스킹한 훈련용 동영상 및 제2 복원 대상 영역을 표시하는 마스크 동영상 , 제1 아핀 변환된 훈련용 동영상 및 제1 아핀 변환된 마스크 동영상을 인페인팅 모델에 적용함으로써, 복원된 제1 아핀 변환된 훈련용 동영상(1006-1), 복원된 제2 아핀 변환된 훈련용 동영상(1006-2) 및 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)를 획득할 수 있다. 여기에서, 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)은 각각 제1 아핀 변환된 훈련용 동영상 (1006-1) 및 제2 아핀 변환된 훈련용 동영상(1006-2)에 대응되는 동영상을 포함할 수 있다. 일 실시예에서, 인페인팅 모델은 제1 내지 제4 시간적 어텐션 모듈(950-1, 950-2, 950-3, 950-4) 각각으로부터 획득되는 복수의 프레임들의 어텐션된 토큰들을 디토큰화 모듈(미도시)에 적용하여 복수의 프레임들의 어텐션된 피처맵들을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 획득된 복수의 프레임들의 어텐션된 피처맵들을 중간 디코더(1110-3)에 적용함으로써, 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)을 획득할 수 있다. 일 실시예에서, 중간 디코더(1110-3)는 복수의 프레임들의 어텐션된 피처맵을 입력으로 하여 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)를 출력하도록 훈련된 모델일 수 있으며, 본 명세 서에서 개시된 다양한 훈련들을 수행함으로써, 최적화 될 수 있다. 일 실시예에서, 중간 디코더(1110-3)의 신경망 파라미터들은 최종 디코더의 신경망 파라미터와 서로 공유 될 수 있으며, 중간 디코더(1110-3) 중 일부는 제1 내지 제4 시간적 어텐션 모듈(950-1, 950-2, 950-3, 950-4) 중 일부와 연결되지 않음으로써, 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140- 4) 중 일부는 생성되지 않을 수 있다. 일 실시예에서, 중간 디코더(1110-3)는 제1 내지 제4 공간적 어텐션 모듈들(940-1, 940-2, 940-3, 940-4) 중 일부 또는 전체와 연결되고, 제1 내지 제4 공간적 어텐션 모듈들(940-1, 940-2, 940-3, 940-4) 중 일부 또는 전 체로부터 획득되는 어텐션된 토큰들에 기초하여 중간 아핀 변환된 훈련용 동영상을 출력할 수 있다.일 실시예에 서, 전자 장치는, 제1 아핀 변환된 훈련용 동영상(1006-1), 제2 아핀 변환된 훈련용 동영상(1006-2) 및 제1 내 지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)에 기초하여 인페인팅 모델의 제2 훈 련(1100-3)을 수행할 수 있다. 일 실시예에서, 전자 장치는 제1 아핀 변환된 훈련용 동영상(1006-1) 및 제2 아 핀 변환된 훈련용 동영상(1006-2)에 기초하여 계산되는 제2 손실 함수와 제1 내지 제4 중간 아핀 변환된 훈련용 동영상(1140-1, 1140-2, 1140-3, 1140-4)에 기초하여 계산되는 제2 손실 함수를 합하여 인페인팅 모델의 제2 훈 련(1100-3)을 수행할 수 있다. 일 실시예에서, 전자 장치는 중간 인코더(1110-1, 1110-2, 1110-3)에 기초하여 획득되는 이미지 또는 동영상에 대해서는 GAN 손실 함수를 계산하지 않고, 재구성 손실 함수, 제1 손실 함수 및 제2 손실 함수 중 적어도 하나 만을 계산할 수 있다. 이는 중간 인코더(1110-1, 1110-2, 1110-3)로부터 획득되는 이미지 또는 동영상에 기초하 여 생성적 적대적 신경망 훈련이 수행되지 않고 최종 디코더에서 획득되는 이미지 또는 동영상에 대해서만 GAN 손실 함수 계산에 집중하는 편이 최적화에 더 안정적이기 때문이다. 도 12는 본 개시의 일 실시예에 따른 인페인팅 모델의 훈련 방법을 설명하기 위한 흐름도이다. S1210 단계에서, 전자 장치는 테스트 훈련 동영상에 기초하여 예상 훈련 시간을 계산한다. 테스트 훈련 동영상 은 인페인팅 모델의 훈련을 수행하기 전에 전자 장치가 인페인팅 모델의 제1 훈련 및 제2 훈련을 수행할 때 소 요되는 시간을 계산하기 위한 훈련 동영상을 의미할 수 있다. 일 실시예에서, 테스트 훈련 동영상은 실제 인페 인팅 모델의 훈련에 이용되는 제2 복원 대상 영역을 포함하는 훈련용 동영상의 일부분일 수 있다. 일 실시예예 서, 제2 복원 대상 영역을 포함하는 훈련용 동영상은 제2 복원 대상 영역이 마스킹된 훈련용 동영상 및 제2 복 원 대상 영역을 표시하는 마스크 동영상을 포함할 수 있다.일 실시예에서, 전자 장치는 테스트 훈련 동영상에 기초하여 인페인팅 모델의 테스트 훈련을 수행하고, 테스트 훈련이 완료되는데 소요되는 시간을 계산할 수 있다. 일 실시예에서, 전자 장치는 테스트 훈련 동영상과 제2 복 원 대상 영역을 포함하는 훈련용 동영상 각각의 크기, 시간 및 개수 등에 대한 정보와 테스트 훈련이 완료되는 데 소요되는 시간에 기초하여 인페인팅 모델이 제1 훈련 및 제2 훈련을 수행하는데 필요한 예상 훈련 시간을 계 산할 수 있다. 일 실시예에서, 제2 훈련은 제1 훈련 모드와 제2 훈련 모드를 포함할 수 있다. 제1 훈련 모드는 제1 손실 함수 에 기초한 제2 훈련을 수행하는 모드이며, 제2 훈련 모드는 제1 손실 함수에 기초한 훈련과 제2 손실 함수에 기 초한 훈련을 번갈아가면서 반복하여 수행하는 훈련 모드를 의미할 수 있다. 일 실시예에서, 전자 장치는 예상 훈련 시간을 계산할 때, 제1 훈련 모드에 따라 인페인팅 모델의 훈련이 수행되었을 때 필요한 예상 훈련 시간과 제2 훈련 모드에 따라 인페인팅 모델의 훈련이 수행되었을 때 필요한 예상 훈련 시간을 각각 계산할 수 있다. S1220 단계에서, 전자 장치는 계산된 훈련 시간에 대한 정보를 출력할 수 있다. 일 실시예에서, 제1 훈련 모드 및 제2 훈련 모드 각각에 대한 계산된 훈련 시간에 대한 정보가 출력되는 경우, 제1 훈련 모드 및 제2 훈련 모 드 각각에 따른 예상 인페인팅 성능에 대한 정보도 함께 출력할 수 있다. 일 실시예에서, 전자 장치는 테스트 훈련을 제1 훈련 모드 및 제2 훈련 모드로 수행함으로써, 제1 훈련 모드 및 제2 훈련 모드 각각에 대한 예상 인 페인팅 성능을 계산할 수 있다. S1230 단계에서, 전자 장치는 훈련 모드를 선택하는 사용자 입력을 수신할 수 있다. 일 실시예에서, 사용자 입 력은 제1 훈련 모드 및 제2 훈련 모드 중 하나를 선택하는 사용자 입력일 수 있다. 일 실시예에서, 사용자 입력 은 훈련 시간 및 인페인팅 성능 중 적어도 하나를 설정하는 입력일 수 있다. 이 경우, 전자 장치는 예상 훈련 시간 및 예상 훈련 성능에 기초하여 제1 훈련 모드 및 제2 훈련 모드 중 사용자가 입력한 훈련 시간 및 인페인 팅 성능 중 적어도 하나를 만족하는 훈련 모드를 식별하고, 식별된 훈련 모드를 선택하는 사용자 입력을 수신한 것으로 판단할 수 있다. S1240 단계에서, 전자 장치는 사용자 입력에 대응하는 훈련 모드로 인페인팅 모델의 훈련을 수행한다. 일 실시 예에서, 사용자 입력에 대응하는 훈련 모드는 제1 훈련 모드 및 제2 훈련 모드 중 하나일 수 있다. 도 13a 및 도 13b는 본 개시의 일 실시예에 따른 인페인팅 모델을 이용한 추론 과정을 설명하기 위한 흐름도이 다. 일 실시예에서, 전자 장치는 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라의 움직임의 크기, 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별할 수 있다. 여기에서 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라는 타겟 복원 대상 영 역이 타겟 동영상의 촬영 시에는 포함되지 않았으나, 타겟 동영상 촬영 후에 촬영된 타겟 동영상에 타겟 복원 대상 영역이 설정된 경우에도, 해당 타겟 동영상을 촬영한 카메라도 포함될 수 있다. 일 실시예에서, 전자 장치는 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라 또는 카메라를 포함 하는 전자 장치에 포함된 카메라의 움직임을 감지하기 위한 센서로부터 획득한 카메라 움직임 정보에 기초하여 카메라의 움직임의 크기를 식별할 수 있다. 일 실시예에서, 카메라 움직임 정보는 실시간으로 타겟 동영상에 대 한 이미지 인페인팅을 수행하는 경우에, 타겟 동영상을 촬영하고 있는 카메라의 움직임을 감지하기 위한 센서로 부터 획득될 수 있다. 일 실시예에서, 카메라의 움직임을 감지하기 위한 센서는 IMU(Inertial Measurement Unit) 센서를 포함할 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다. 일 실시예에서, 카메라 모션 행렬 은 기설정된 시간 또는 기설정된 프레임 간격마다 생성될 수 있다. 일 실시예에서, 전자 장치는 생성된 카메라 모션 행렬의 매트릭스 놈(matrix norm)을 계산하고, 계산된 매트릭스 놈에 기초하여 카메라의 움직임의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 타겟 동영상의 이미지에 기초하여 타겟 동영상을 촬영한 카메라의 움직임의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 SFM(Structure from Motion) 알고리즘을 이용하여 타겟 동영상의 이미지에 기초하여 타겟 동영상을 촬영한 카메라의 모션 행렬을 생성할 수 있다. SFM 알고리즘은 이미지 내에서 대응되는 특징점(feature point) 쌍들을 검출하고, 검출된 특징점 쌍에 기초하여 카메라의 자세를 추정하여 동 영상을 촬영하는 카메라의 움직임을 추정하는 알고리즘을 의미할 수 있다. 일 실시예에서, 전자 장치는 생성된 카메라 모션 행렬의 매트릭스 놈을 계산하고, 계산된 매트릭스 놈에 기초하여 카메라의 움직임의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 타겟 동영상에 포함된 타겟 복원 대상 영역에 대응하는 오브젝트의 옵티컬 플로우 에 기초하여 움직임의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 타겟 복원 대상 영역에 대응하는 오 브젝트의 옵티컬 플로우를 예측하기 위하여 Lucas-Kanade 알고리즘 등 다양한 옵티컬 플로우 예측 알고리즘을 사용할 수 있다. 일 실시예에서, 전자 장치는 예측된 옵티컬 플로우의 크기의 평균이나 중간 값 등에 기초하여 타겟 복원 대상 영역에 대응하는 오브젝트의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 타겟 복원 대상 영역에 포함된 픽셀의 개수에 기초하여 타겟 복원 대상 영역의 크 기를 식별할 수 있다. 일 실시예에서, 전자 장치는 이미지 내의 전체 픽셀들의 개수에 대한 타겟 복원 대상 영 역에 포함된 픽셀들의 개수의 비율을 계산하고, 계산된 비율에 기초하여 타겟 복원 대상 영역의 크기를 식별할 수 있다. 일 실시예에서, 전자 장치는 카메라의 움직임의 크기, 오브젝트의 움직임의 크기 및 타겟 복원 대상 영역의 크 기 중 적어도 하나에 기초하여 제1 추론 모드 및 제2 추론 모드 중 하나를 선택하여 복수의 프레임 들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 제1 추론 모드는, 복수의 프레임들 각각의 제2 토큰들 및 복수의 프레임들 각각의 제1 피 처맵에 기초하여 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드일 수 있다. 즉, 제1 추론 모드는 도 4b의 인페인팅 모델 내 어텐션 블록(400-2)의 동작에 따라 복수의 프레임들의 각각의 제3 토큰들을 획득하는 것에 대응될 수 있으며, 이하에서 설명하는 제2 추론 모드에 비하여 추론 과정에 필요한 하드웨어 자원과 컴퓨팅 시간은 크지만, 복수의 제3 모듈들로부터 획득되는 인접한 프레임들의 이미지 사이에서 존재하는 움직임 에 대한 정보를 이용하여 인페인팅을 수행한다는 점에서, 인페인팅 성능은 높을 수 있다 일 실시예에서, 제2 추론 모드는, 복수의 프레임들 각각의 제3 토큰들에 기초하여 복수의 프레임들 각각 의 제3 토큰들을 획득하는 모드일 수 있다. 즉, 제2 추론 모드는 도 4a 인페인팅 모델내 어텐션 블록(400-1)의 동작에 따라 복수의 프레임들의 각각의 제3 토큰들을 획득하는 것에 대응될 수 있으며, 상술한 제1 추론 모드 에 비하여 추론 과정에 필요한 하드웨어 자원과 컴퓨팅 시간은 작지만, 복수의 제3 모듈들로부터 획득되 는 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보를 이용하지 않고 인페인팅을 수행한다는 점에서, 인페인팅 성능은 낮을 수 있다. 도 13a를 참조하면, S1310-1 단계에서, 전자 장치는 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카 메라의 움직임의 크기, 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기를 식별할 수 있다. S1320 단계에서, 전자 장치는 식별된 카메라의 움직임의 크기가 제1 임계 값 이상인지 식별한다. 일 실시예에서, 식별된 카메라의 움직임의 크기가 제1 임계 값 이상이면(S1320-Y), 전자 장치는 제1 추론 모드 를 선택하여 이미지 인페인팅을 수행할 수 있다. S1330 단계에서, 전자 장치는 식별된 카메라의 움직임의 크기가 제1 임계 값 미만이면(S1320-N), 식별된 오브젝 트의 움직임의 크기가 제2 임계 값 이상인지 식별한다. 일 실시예에서, 카메라의 움직임의 크기가 제2 임계 값 이상이면(S1330-Y), 전자 장치는 제1 추론 모드를 선택하여 이미지 인페인팅을 수행할 수 있다. 반면, 카 메라의 움직임의 크기가 제2 임계 값 미만이면(S1330-N), 전자 장치는 제2 추론 모드를 선택하여 이미지 인페인팅을 수행할 수 있다. 도 13b를 참조하면, S1310-2 단계에서, 전자 장치는 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카 메라의 움직임의 크기, 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 타겟 복원 대상 영역의 크기를 식별할 수 있다. 일 실시예에서, 카메라의 움직임의 크기가 제1 임계 값 이상으로 식별되거나(S1320-Y), 오브젝트의 움직임의 크 기가 제2 임계 값 이상으로 식별되면(S1330-Y), 전자 장치는 타겟 복원 대상 영역 대상 영역의 크기가 제3 임계 값 이상인지 식별할 수 있다(S1325). 일 실시예에서, 타겟 복원 대상 영역의 크기가 제3 임계 값 이상인 것으로 식별되면(S1325-Y), 전자 장치는 제1 추론 모드를 선택하여 이미지 인페인팅을 수행할 수 있다. 반면, 타겟 복원 대상 영역의 크기가 제3 임계 값 미만인 것으로 식별되면(S1325-N), 전자 장치는 제2 추론 모드를 선택하여 이미지 인페인팅을 수행할 수 있다. 이와 같이, 본 개시의 일 실시예에 따른 전자 장치는 타겟 복원 대상 영역 및 타겟 복원 대상 영역을 포함하는 타겟 동영상의 다양한 요소들을 고려하여 적절한 추론 모드를 선택하여 인페인팅을 수행할 수 있다는 점에서, 한정된 하드웨어 자원과 컴퓨팅 시간을 효율적으로 활용할 수 있다.도 14는 본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. S1410 단계에서, 전자 장치는 타겟 동영상의 특정 프레임의 이미지에 포함된 타겟 오브젝트를 지정하는 사용자 입력을 수신할 수 있다. 일 실시예에서, 특정 프레임의 이미지에 포함된 오브젝트는 타겟 동영상에서 움직이는 오브젝트(예: 보행자, 자동차, 동물 등)을 포함할 수 있다. 일 실시예에서, 오브젝트를 지정하는 사용자 입력은 특정 픽셀 또는 특정 영역을 지정하기 위한 입력 인터페이스 디바이스(예: 포인팅 디바이스, 터치 스크린 등)에 의해 입력되어 전자 장치가 수신하거나, 특정 좌표를 지정하는 사용자의 음성 등을 수신하여 입력될 수 있다. S1420 단계에서, 전자 장치는 타겟 오브젝트를 지정하는 사용자 입력에 기초하여 타겟 오브젝트에 대응하는 마 스크를 포함하는 특정 프레임의 이미지를 생성할 수 있다. 일 실시예에서, 타겟 오브젝트에 대응하는 마스크를 포함하는 특정 프레임의 이미지는 타겟 오브젝트가 마스킹된 이미지 및 타겟 오브젝트를 표시하는 마스크 이미 지 또는 마스크 맵을 포함할 수 있다. 일 실시예에서, 전자 장치는 타겟 오브젝트를 지정하는 사용자 입력에 기 초하여 타겟 동영상의 특정 프레임의 이미지에서 사용자가 지정한 타겟 오브젝트를 식별할 수 있다. 일 실시예 에서, 전자 장치는 세그멘테이션(Segmentation) 알고리즘, 인스턴스 세그멘테이션(Instance Segmentation) 알 고리즘 및 객체 탐지(Object Detection) 알고리즘 또는 해당 알고리즘들에 기초한 모델을 이용하여 사용자가 지 정한 타겟 오브젝트를 식별할 수 있다. 일 실시예에서, 전자 장치는 타겟 오브젝트에 포함되는 영역의 픽셀 값 들을 마스크에 대응되는 픽셀 값들로 변경하고, 타겟 오브젝트에 포함되지 않는 영역의 픽셀 값들은 기존 이미 지에 포함된 픽셀 값들을 유지하거나, 마스크 맵에서 타겟 복원 대상 영역을 제외한 나머지 영역에 대응되는 픽 셀 값들로 변경함으로써, 타겟 오브젝트에 대응하는 마스크를 포함하는 특정 프레임의 이미지를 획득할 수 있다. 일 실시예에서, 전자 장치는 생성된 타겟 오브젝트에 대응하는 마스크를 포함하는 특정 프레임의 이미지를 출력 할 수 있다. 일 실시예에서, 전자 장치는 타겟 오브젝트를 재지정하는 사용자 입력을 수신할 수 있다. 예를 들 어, S1420 단계에서, 생성된 특정 프레임의 이미지에 포함된 마스크가 사용자가 실제로 지정한 타겟 오브젝트의 일부에 대해서만 생성되거나, 지정하지 않은 타겟 오브젝트에 대해서 생성될 수 있다. 이 경우, 전자 장치는 타 겟 오브젝트를 재지정하는 사용자 입력에 기초하여 마스크가 생성되지 않았던 부분까지 포함된 타겟 오브젝트에 대응하는 마스크 또는 사용자가 실제로 지정한 타겟 오브젝트에 대응하는 마스크를 포함하는 특정 프레임의 이 미지를 획득할 수 있다. S1430 단계에서, 전자 장치는 특정 프레임의 이미지에 포함된 마스크를 타겟 동영상의 복수의 프레임들의 이미 지에 전파하여 타겟 오브젝트에 대응하는 마스크를 포함하는 타겟 동영상을 획득할 수 있다. 일 실시예에서, 전 자 장치는 마스크 전파(mask propagation)알고리즘을 이용하여 특정 프레임의 이미지에 포함된 마스크를 다른 프레임의 이미지에 전파하여 타겟 오브젝트에 복수의 프레임들의 이미지를 획득할 수 있다. 일 실시예에서, 마 스크 전파 알고리즘은 객체 기반 알고리즘과 인스턴스 세그멘테이션 알고리즘을 결합한 모델을 이용하여 객체 탐지, 마스크 추정 및 마스크 전파를 동시에 수행하는 방법 또는 마스크에 대한 옵티컬 플로우를 예측하고, 예 측된 옵티컬 플로우에 기초하여 마스크를 이동시키는 방법 등을 포함할 수 있으며, 반드시 상술한 예에 한정되 는 것은 아니다. S1440 단계에서, 전자 장치는 마스크를 포함하는 타겟 동영상을 인페인팅 모델에 적용함으로써, 마스크가 제거 된 타겟 동영상을 획득할 수 있다. 인페인팅 모델은 본 명세서에서 개시된 다양한 인페인팅 알고리즘에 기초하 여 인페인팅을 수행할 수 있으며, 중복되는 설명은 생략하도록 한다. 도 15는 본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. S1510 단계에서, 전자 장치는 타겟 동영상에 포함된 타겟 오브젝트를 설명하는 사용자 입력을 수신할 수 있다. 예를 들어, 사용자는 \"이 비디오에서 빨간 자전거를 지워줘\" 또는, \"이 비디오에서 소파에 앉아있는 사람을 지 워줘\"라는 음성 명령을 전자 장치에 입력하거나, 해당 음성 명령에 대응하는 텍스트를 전자 장치에 입력할 수 있다. S1520 단계에서, 전자 장치는 타겟 오브젝트를 설명하는 사용자 입력에 기초하여 타겟 오브젝트에 대응하는 마 스크 볼륨을 획득할 수 있다. 일 실시예에서, 전자 장치는 타겟 오브젝트를 설명하는 사용자 입력에 기초하여 타겟 오브젝트를 식별할 수 있다. 예를 들어, 전자 장치는 BERT, GPT-3, Transformer 등과 같은 다양한 자연어 처리(Natural Language Processing) 모델을 이용하여 사용자 입력에 기초하여 타겟 오브젝트를 식별할 수 있다.일 실시예에서, 전자 장치는 식별된 타겟 오브젝트에 대한 정보 및 타겟 동영상에 기초하여 타겟 오브젝트에 대 응하는 마스크 볼륨을 생성할 수 있다. 예를 들어, 전자 장치는 YOLO (You Only Look Once), SSD (Single Shot Detector), Faster R-CNN, RetinaNet 등과 같은 다양한 비전 기반 객체 탐지 모델들 및 Mask R-CNN, DeepLab, U-Net 등과 같은 다양한 세그멘테이션 모델을 이용하여 식별된 타겟 오브젝트에 대응하는 마스크 볼륨을 생성할 수 있다. S1530 단계에서, 전자 장치는 획득된 마스크 볼륨에 기초하여 타겟 오브젝트에 대응하는 마스크를 포함하는 타 겟 동영상을 생성할 수 있다. 일 실시예에서, 전자 장치는 생성된 마스크 볼륨을 타겟 동영상과 합성하여, 타겟 오브젝트에 대응하는 영역의 픽셀들이 마스크에 대응하는 픽셀들로 변환된 타겟 동영상을 생성할 수 있다. S1540 단계에서, 전자 장치는 마스크를 포함하는 타겟 동영상을 인페인팅 모델에 적용함으로써, 마스크가 제거 된 타겟 동영상을 획득할 수 있다. 인페인팅 모델은 본 명세서에서 개시된 다양한 인페인팅 알고리즘에 기초하 여 인페인팅을 수행할 수 있으며, 중복되는 설명은 생략하도록 한다. 도 16은 본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. S1610 단계에서, 전자 장치는 타겟 동영상에 특정 프레임에 포함된 타겟 오브젝트를 설명하는 사용자 입력을 수 신할 수 있다. 예를 들어, 사용자는 \"이 프레임에서 빨간 자전거를 지워줘\" 또는, \"이 프레임에서 자전거를 타 고 있는 사람을 지워줘\"라는 음성 명령을 전자 장치에 입력하거나, 해당 음성 명령에 대응하는 텍스트를 전자 장치에 입력할 수 있다. S1620 단계에서, 전자 장치는 타겟 오브젝트를 설명하는 사용자 입력에 기초하여 타겟 오브젝트에 대응하는 마 스크를 생성할 수 있다. S1620 단계의 전자 장치의 동작은 S1520 단계의 전자 장치의 동작에 대응될 수 있다는 점에서, 중복되는 설명은 생략하도록 한다. S1630 단계에서, 전자 장치는 생성된 마스크에 기초하여 타겟 오브젝트에 대응하는 마스크를 포함하는 특정 프 레임의 이미지를 생성할 수 있다. S1630 단계의 전자 장치의 동작은 S1530 단계의 전자 장치의 동작에 대응될 수 있다는 점에서, 중복되는 설명은 생략하도록 한다. S1640 단계에서, 전자 장치는 특정 프레임의 이미지에 포함된 마스크를 타겟 동영상의 복수의 프레임들의 이미 지에 전파하여 타겟 오브젝트에 대응하는 마스크를 포함하는 타겟 동영상을 생성할 수 있다. S1640 단계의 전자 장치의 동작은 S1430 단계의 전자 장치의 동작과 대응될 수 있다는 점에서, 중복되는 내용의 설명은 생략하도록 한다. S1650 단계에서, 전자 장치는 특정 프레임의 이미지에 포함된 마스크를 타겟 동영상의마스크를 포함하는 타겟 동영상을 인페인팅 모델에 적용함으로써, 마스크가 제거된 타겟 동영상을 획득할 수 있다. 인페인팅 모델은 본 명세서에서 개시된 다양한 인페인팅 알고리즘에 기초하여 인페인팅을 수행할 수 있으며, 중복되는 설명은 생략 하도록 한다. 도 17은 본 개시의 일 실시예에 따른 전자 장치가 메타 버스 상에서 비디오 인페인팅을 수행하는 방법을 설명하 기 위한 흐름도이다. S1710 단계에서, 전자 장치는 스트리밍 중인 가상 현실 상의 타겟 오브젝트를 지정하는 사용자 입력을 수신할 수 있다. 스트리밍 중인 가상 현실이란 메타버스 플랫폼 서비스를 제공하는 서버로부터 수신한 가상 현실을 구 현하기 위한 데이터에 기초하여 전자 장치에서 실시간으로 구현되어 표시되는 가상 현실을 의미할 수 있다. 가 상 현실에는 메타버스 플랫폼 서비스에 접속하는 다른 사용자들의 캐릭터들이 함께 표시될 수 있다. 일 실시예 에서, 전자 장치의 사용자의 캐릭터는 가상 현실 상의 다른 사용자의 캐릭터와 대화를 수행하거나, 가상 현실상 의 다양한 오브젝트를 조작 또는 활용하는 등의 상호 작용을 수행할 수 있다. 일 실시예에서, 전자 장치가 가상 현실 상의 타겟 오브젝트를 지정하는 사용자 입력은 도 14 내지 도 16에서 설명한 방법들을 포함한 다양한 방식 을 통해 입력될 수 있다. S1720 단계에서, 전자 장치는 수신된 사용자 입력에 기초하여 타겟 오브젝트에 대응하는 마스크를 포함하는 가 상 현실을 스트리밍할 수 있다. 일 실시예에서, 전자 장치는 스트리밍 중인 가상 현실을 나타내는 복수의 프레 임들의 이미지를 저장할 수 있다. 저장된 복수의 프레임들의 이미지는 이미 표시되었던 가상 현실을 나타내는 복수의 프레임들의 이미지이거나, 표시되지는 않았지만 표시될 예정인 가상 현실을 나타내는 복수의 프레임들의 이미지를 포함할 수 있다. 일 실시예에서, 전자 장치는 사용자가 지정한 타겟 오브젝트에 대한 정보 및 저장된복수의 프레임들의 이미지에 기초하여 타겟 오브젝트에 대응하는 마스크를 생성하고, 생성된 마스크를 표시될 예정인 가상 현실을 나타내는 복수의 프레임들의 이미지에 합성함으로써, 타겟 오브젝트에 대응하는 마스크를 포함하는 가상 현실을 스트리밍할 수 있다. 일 실시예에서, 전자 장치는 타겟 오브젝트에 대응하는 마스크를 포 함하는 가상 현실을 스트리밍 하기 위하여 전술한 다양한 비전 기반 객체 탐지 모델 및 다양한 세그멘테이션 모 델을 이용할 수 있으며, 반드시 상술한 예에 한정되는 것은 아니다. S1730 단계에서, 전자 장치는 가상 현실 인페인팅 시나리오를 선택하는 사용자 입력을 수신할 수 있다. 일 실시예에서, 가상 현실 인페인팅 시나리오는 가상 현실 상의 타겟 오브젝트의 제거를 포함할 수 있다. 이 경 우, 전자 장치는 타겟 오브젝트에 대하여 인페인팅을 수행하고, 타겟 오브젝트가 제거된 가상 현실을 표시할 수 있다. 일 실시예에서, 가상 현실 인페인팅 시나리오는 가상 현실 상의 타겟 오브젝트를 제거하고, 제거된 타겟 오브젝트가 존재하는 영역에 사용자의 캐릭터(또는, 사용자의 실제 모습) 또는 가상 현실 상의 오브젝트를 표시 하는 것을 포함할 수 있다. 이 경우, 전자 장치는 타겟 오브젝트에 대하여 인페인팅을 수행하고, 기저장된 사용 자의 캐릭터의 이미지 또는 가상 오브젝트의 이미지를 타겟 오브젝트가 존재하는 영역에 합성하여 표시할 수 있 다. 일 실시예에서, 가상 현실 인페인팅 시나리오는 타겟 오브젝트의 스케일 변경을 포함할 수 있다. 이 경우, 전자 장치는 타겟 오브젝트에 대하여 인페인팅을 수행하여 타겟 오브젝트가 제거된 가상 현실에서, 스케일 변환 된 타겟 오브젝트의 이미지를 타겟 오브젝트가 존재하는 영역에 합성하여 표시할 수 있다. S1740 단계에서, 전자 장치는 수신된 사용자 입력에 기초하여 선택된 가상 현실 인페인팅 시나리오에 대응하는 인페인팅을 수행할 수 있다. 일 실시예에서, 전자 장치는 전자 장치에 저장된 표시될 예정인 가상 현실을 나타 내는 복수의 프레임들의 이미지에 S1720 단계에서 생성된 가상 오브젝트에 대응하는 마스크를 전파하여 마스크 를 포함하는 표시될 예정인 가상 현실을 나타내는 복수의 프레임들의 이미지를 생성할 수 있다. 일 실시예에서, 전자 장치는 생성된 마스크를 포함하는 표시될 예정인 가상 현실을 나타내는 복수의 프레임들의 이미지를 인페 인팅 모델에 적용함으로써, 마스크가 제거된 표시될 예정인 가상 현실을 나타내는 복수의 프레임들의 이미지를 획득하고, 사용자 입력에 기초하여 선택된 가상 현실 인페인팅 시나리오에 대응하는 동작을 수행할 수 있다. 일 실시예에서, 인페인팅 모델에 적용되어 인페인팅이 수행되는 복수의 프레임들의 이미지는 기설정된 개수의 이미지들이 복수개의 그룹으로 묶여서 인페인팅 모델에 적용될 수 있으며, 복수개의 그룹들 중 서로 인접한 그 룹들 사이에서는 둘 이상의 특정 프레임의 이미지를 공유할 수 있다. 예를 들어, 제1 그룹에 제1 내지 제10 프 레임의 이미지가 포함되고, 제2 그룹에 제6 내지 제15 프레임의 이미지가 포함되어 제1 그룹 및 제2 그룹은 제5 내지 제10 프레임의 이미지를 공유할 수 있다. 이에 따라, 전자 장치는 타겟 오브젝트에 대한 인페인팅이 수행 된 가상 현실을 보다 자연스럽게 보이도록 구현하여 표시할 수 있다. S1750 단계에서, 전자 장치는 인페인팅이 수행된 가상 현실을 스트리밍할 수 있다. 일 실시예에서, 전자 장치는 인페인팅 모델로부터 획득된 인페인팅이 수행된 가상 현실을 나타내는 복수의 프레임들의 이미지를 미리 저장하 고, 저장된 복수의 프레임들의 이미지들 중 현재 스트리밍 시점에 대응되는 프레임의 이미지를 표시함으로써, 인페인팅이 수행된 가상 현실을 스트리밍할 수 있다. 도 18은 본 개시의 일 실시예에 따른 전자 장치의 구성을 설명하기 위한 흐름도이다. 도 18을 참조하면, 전자 장치는 통신 인터페이스, 메모리 및 적어도 하나의 프로세서 를 포함할 수 있다. 다만, 전자 장치에 상술한 구성 외의 디스플레이(미도시), 입력 인터페이스(미도시) 등과 같은 다른 구성이 포함될 수 있음은 물론이다. 통신 인터페이스는 적어도 하나의 프로세서의 제어에 의해 다른 전자 장치들과 데이터 통신을 수행 할 수 있다. 통신 인터페이스는 통신 회로를 포함할 수 있다. 통신 인터페이스는 예를 들어, 유선 랜, 무선 랜 (Wireless LAN), 와이파이(Wi-Fi), 블루투스(Bluetooth), 지그비(ZigBee), WFD(Wi-Fi Direct), 적외선 통신 (IrDA, infrared Data Association), BLE (Bluetooth Low Energy), NFC(Near Field Communication), 와이브로 (Wireless Broadband Internet, Wibro), 와이맥스(World Interoperability for Microwave Access, WiMAX), SWAP(Shared Wireless Access Protocol), 와이기그(Wireless Gigabit Alliances, WiGig) 및 RF 통신을 포함하 는 데이터 통신 방식 중 적어도 하나를 이용하여, 전자 장치와 다른 전자 장치들 간의 데이터 통신을 수 행할 수 있는, 통신 회로를 포함할 수 있다. 일 실시예에서, 통신 인터페이스는 본 개시의 다양한 인페이팅 알고리즘을 수행하기 위한 데이터를 외부 전자 장치와 송수신할 수 있다. 예를 들어, 전자 장치는 통신 인터페이스를 통해 외부 전자 장치로부터 타겟 복원 대상 영역을 포함하는 타겟 동영상을 수신하고, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 외부 전자 장치로 전송할 수 있다. 다른 예로, 전자 장치는 통신 인터페이스를 통해 가상 현실을 구현하여 표시하기 위한 데이터를 메타버스 플랫폼 서비스를 제공하는 서버로부터 수신할 수 있다. 메모리는 프로세서가 판독할 수 있는 명령어들, 데이터 구조, 및 프로그램 코드(program code)가 저장될 수 있다. 개시된 실시예들에서, 프로세서가 수행하는 동작들은 메모리에 저장된 프로그램의 명령어들 또는 코드들을 실행함으로써 구현될 수 있다. 메모리는 플래시 메모리 타입(flash memory type), 하드디스크 타입(hard disk type), 멀티미디어 카드 마이크로 타입(multimedia card micro type), 카드 타입의 메모리(예를 들어 SD 또는 XD 메모리 등), 램(RAM, Random Access Memory) SRAM(Static Random Access Memory), 롬(ROM, Read-Only Memory), EEPROM(Electrically Erasable Programmable Read-Only Memory), PROM(Programmable Read-Only Memory), 자기 메모리, 자기 디스크, 광디스크 중 적어도 하나를 포함하는 비휘발성 메모리 및 램(RAM, Random Access Memory) 또는 SRAM(Static Random Access Memory)과 같은 휘발성 메모리를 포함할 수 있다. 일 실시예에서, 메모리는 본 개시의 다양한 인페인팅 알고리즘을 수행하는데 이용되거나 필요한 데이터를 저장할 수 있다. 예를 들어, 메모리는 인페인팅 모델 및 인페이팅 모델을 훈련시키기 위한 훈련 데이터 생성 모듈을 저장할 수 있다. 일 실시예에서, 인페이팅 모델은 공간적 어텐션 모듈, 시간적 어텐션 모듈, 모션 클루 추정 모듈 및 옵티컬 플로우 생성 모듈를 포함할 수 있다. 여기에서, 공간적 어텐션 모듈은 본 개 시의 복수의 제1 모듈들에 대응되고, 시간적 어텐션 모듈는 본 개시의 복수의 제2 모듈들에 대응되고, 모 션 클루 추정 모듈은 본 개시의 복수의 제3 모듈들에 대응되고, 옵티컬 플로우 생성 모듈는 본 개 시의 복수의 제4 모듈들에 대응될 수 있다. 적어도 하나의 프로세서는 전자 장치의 전반적인 동작을 제어할 수 있다. 예를 들어, 적어도 하나 의 프로세서는 메모리에 저장된 프로그램의 하나 이상의 명령어들(instructions)을 실행함으로써, 전자 장치가 이미지를 복원하는 이미지 인페인팅을 수행하기 위한 전반적인 동작을 제어할 수 있다. 적어도 하나의 프로세서는 예를 들어, 중앙 처리 장치(Central Processing Unit), 마이크로 프로세서 (microprocessor), 그래픽 프로세서(Graphic Processing Unit), ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), AP(Application Processor), 뉴 럴 프로세서(Neural Processing Unit) 또는 인공지능 모델의 처리에 특화된 하드웨어 구조로 설계된 인공지능 전용 프로세서 중 적어도 하나로 구성될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 타겟 복원 대상 영역을 포함하는 타겟 동영상을, 타겟 동영상 의 단일 프레임의 이미지 각각으로부터 획득되는 특징에 기초한 공간적 어텐션(spatial attention)을 수행하기 위한 복수의 제1 모듈 및 타겟 동영상의 복수의 프레임의 이미지들 사이에서 획득되는 특징에 기초한 시-공간적 어텐션(spatio-temporal attention)을 수행하기 위한 복수의 제2 모듈들을 포함하는 인페인팅 모델에 적 용함으로써, 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은, 제1 훈련 전의 복수의 제1 모듈들 단일 훈련용 이미지에 대하여 공간적 어텐션을 수행하는 복수의 제2 모듈들에 대하여 제1 훈련이 수행되고, 제1 훈련이 수행된 복수의 제1 모듈 및 제1 훈련이 수행된 복수의 제2 모듈에 대하여 제2 훈련이 수행된 인공 지능 모델이고, 제1 훈련은, 제1 복원 대 상 영역을 포함하는 단일 훈련용 이미지에 기초하여, 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미 지를 획득하기 위한 훈련이며, 제2 훈련은, 제2 복원 대상 영역을 포함하는 훈련용 동영상에 기초하여, 복원된 제2 복원 대상 영역이 복원된 훈련용 동영상을 획득하기 위한 훈련일 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 타겟 복원 대상 영역이 마스킹된 타겟 동영상 및 타겟 복원 대 상 영역을 표시하는 마스크 동영상에 기초하여 복수의 프레임들의 제1 토큰들을 획득하할 수 있다. 일 실시예에 서, 적어도 하나의 프로세서는, 복수의 프레임들의 제1 토큰들을 복수의 제2 모듈들 중 적어도 하나에 적 용함으로써, 시-공간적 어텐션이 수행된 복수의 프레임들의 제2 토큰들을 획득할 수 있다. 일 실시예에서, 적어 도 하나의 프로세서는 복수의 프레임들 각각의 제2 토큰들을 복수의 제1 모듈들 중 적어도 하나에 적용함 으로써, 공간적 어텐션이 수행된 복수의 프레임들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 프레임들의 제3 토큰들에 기초하여 복원된 타겟 복원 대상 영역을 포함하는 타겟 동영상을 획득할 수 있다. 일 실시예에서, 인페인팅 모델은 복수의 프레임들 중 서로 인접한 프레임들의 이미지 사이에서 획득되는 특징을 추출하기 위한 복수의 제3 모듈들을 더 포함할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 프레임들의 제2 토큰들을 복수의 제3 모듈들 중 적어도 하나에 적용함으로써, 인접한 프레임들의 이미지 사이에서 존재하는 움직임(motion)에 대한 정보를 포함하는 복 수의 프레임들의 제1 피처맵들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 프레 임들 각각의 제2 토큰들 및 복수의 프레임들 각각의 제1 피처맵을 복수의 제2 모듈들 중 적어도 하나에 적용함 으로써, 복수의 프레임들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 복수의 프레임들의 제2 토큰들 중 제1 프레임의 제2 토큰들 및 제1 프레임의 다음 프레임인 제2 프레임의 제2 토큰들에 기초하여 복수의 프레임들의 제1 피처맵들 중 제1 프레 임의 제1 피처맵을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임의 제2 토큰들 및 제1 프레임의 제1 피처맵 중 적어도 하나에 기초하여 복수의 프레임들의 제3 토큰들 중 제1 프레임의 제3 토 큰들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임의 제1 피처맵을 토큰화(tokenization)하여 제1 프 레임의 제4 토큰들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임의 제4 토큰 들 및 제1 프레임의 제2 토큰들에 기초하여 제1 프레임의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임의 제2 토큰들을 디토큰화(de-tokenization)하여 제 1 프레임의 제2 피처맵을 획득할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 제1 프레임의 제1 피처맵 및 제1 프레임의 제2 피처맵에 기초하여 제1 프레임의 제3 피처맵을 획득할 수 있다. 일 실시예에서, 적 어도 하나의 프로세서는 제1 프레임의 제3 피처맵을 토큰화하여 제1 프레임의 제3 토큰들을 획득할 수 있 다. 일 실시예에서, 인페인팅 모델은 제1 훈련 전의 복수의 제1 모듈들 중 하나 이상 및 단일 훈련용 이미지 에 대하여 공간적 어텐션을 수행하는 제1 훈련 전의 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 복원된 제1 복원 대상 영역을 포함하는 단일 훈련용 이미지에 기초하여 제1 훈련이 수행될 수 있다. 일 실시예에서, 인페인팅 모델은 제1 훈련이 수행된 복수의 제1 모듈들 중 하나 이상 및 제1 훈련이 수행된 복수의 제2 모듈들 중 하나 이상으로부터 각각 획득되는 둘 이상의 복원된 제2 복원 대상 영역을 포함하는 훈련 용 동영상에 기초하여 제2 훈련이 수행된 인공 지능 모델일 수 있다. 일 실시예에서, 인페인팅 모델은 제1 손실 함수 및 제2 손실 함수 중 적어도 하나에 기초하여 제2 훈련이 수행된 인공 지능 모델일 수 있다. 일 실시예에서, 제1 손실 함수는 제2 복원 대상 영역을 포함하는 훈련용 동영상의 인접한 프레임들의 이미지 사 이의 옵티컬 플로우(optical flow)에 기초하여 계산될 수 있다. 일 실시예에서, 제2 손실 함수는 제2 복원 대상 영역을 포함하는 훈련용 동영상 및 복원된 제2 복원 대상 영역 을 포함하는 훈련용 동영상에 아핀 변환(affine transformation)을 수행하는 것에 기초하여 계산될 수 있다. 일 실시예에서, 옵티컬 플로우는 제2 훈련 전의 제3 모듈로부터 획득되는 제2 복원 대상 영역을 포함하는 훈련 용 동영상의 인접한 프레임들의 이미지 사이에서 존재하는 움직임에 대한 정보에 기초하여 획득될 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 타겟 복원 대상 영역을 포함하는 타겟 동영상을 촬영한 카메라 의 움직임의 크기, 타겟 복원 대상 영역에 대응하는 오브젝트의 움직임의 크기 및 타겟 복원 대상 영역의 크기 중 적어도 하나를 식별할 수 있다. 일 실시예에서, 적어도 하나의 프로세서는 카메라의 움직임의 크기, 오브젝트의 움직임의 크기 및 타겟 복원 대상 영역의 크기 중 적어도 하나에 기초하여 제1 추론 모드 및 제2 추 론 모드 중 하나를 선택하여 복수의 프레임들 각각의 제3 토큰들을 획득할 수 있다. 일 실시예에서, 제1 추론 모드는 복수의 프레임들 각각의 제2 토큰 및 복수의 프레임들 각각의 제1 피처맵에 기 초하여 복수의 프레임들 각각의 제3 토큰들을 획득하는 모드일 수 있다. 일 실시예에서, 제2 추론 모드는 복수의 프레임들 각각의 제2 토큰들에 기초하여 복수의 프레임들 각각의 제3 토큰들들을 획득하는 모드일 수 있다. 한편, 본 개시의 실시예들은 컴퓨터에 의해 실행되는 프로그램 모듈과 같은 컴퓨터에 의해 실행 가능한 명령어 를 포함하는 기록 매체의 형태로도 구현될 수 있다. 컴퓨터 판독 가능 매체는 컴퓨터에 의해 액세스 될 수 있는 임의의 가용 매체일 수 있고, 휘발성 및 비휘발성 매체, 분리형 및 비분리형 매체를 모두 포함한다. 또한, 컴퓨 터 판독 가능 매체는 컴퓨터 저장 매체 및 통신 매체를 포함할 수 있다. 컴퓨터 저장 매체는 컴퓨터 판독 가능 명령어, 데이터 구조, 프로그램 모듈 또는 기타 데이터와 같은 정보의 저장을 위한 임의의 방법 또는 기술로 구 현된 휘발성 및 비휘발성, 분리형 및 비분리형 매체를 모두 포함한다. 통신 매체는 전형적으로 컴퓨터 판독 가 능 명령어, 데이터 구조, 또는 프로그램 모듈과 같은 변조된 데이터 신호의 기타 데이터를 포함할 수 있다. 또한, 컴퓨터에 의해 읽을 수 있는 저장매체는, 비일시적(non-transitory) 저장매체의 형태로 제공될 수 있다. 여기서, '비일시적 저장매체'는 실재(tangible)하는 장치이고, 신호(signal)(예: 전자기파)를 포함하지 않는다 는 것을 의미할 뿐이며, 이 용어는 데이터가 저장매체에 반영구적으로 저장되는 경우와 임시적으로 저장되는 경 우를 구분하지 않는다. 예로, '비일시적 저장매체'는 데이터가 임시적으로 저장되는 버퍼를 포함할 수 있다. 일 실시예에 따르면, 본 문서에 개시된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있 다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로 배포되거나, 또는 어플리케이션 스토어를 통해 또는 두개의 사용자 장치들(예: 스마트폰들) 간에 직접, 온라인으로 배포(예: 다운로드 또는 업로드)될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품(예: 다운 로더블 앱(downloadable app))의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 기기로 읽을 수 있는 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다."}
{"patent_id": "10-2023-0045503", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 12, "content": "전술한 본 개시의 설명은 예시를 위한 것이며, 본 개시가 속하는 기술분야의 통상의 지식을 가진 자는 본 개시 의 기술적 사상이나 필수적인 특징을 변경하지 않고서 다른 구체적인 형태로 쉽게 변형이 가능하다는 것을 이해 할 수 있을 것이다. 그러므로 이상에서 기술한 실시예들은 모든 면에서 예시적인 것이며 한정적이 아닌 것으로 이해해야만 한다. 예를 들어, 단일형으로 설명되어 있는 각 구성 요소는 분산되어 실시될 수도 있으며, 마찬가 지로 분산된 것으로 설명되어 있는 구성 요소들도 결합된 형태로 실시될 수 있다. 본 개시의 범위는 상세한 설명보다는 후술하는 특허청구범위에 의하여 나타내어지며, 특허청구범위의 의미 및 범위 그리고 그 균등 개념으로부터 도출되는 모든 변경 또는 변형된 형태가 본 개시의 범위에 포함되는 것으로 해석되어야 한다.도면 도면1 도면2 도면3 도면4a 도면4b 도면5a 도면5b 도면6 도면7 도면8a 도면8b 도면9a 도면9b 도면9c 도면10 도면11a 도면11b 도면11c 도면12 도면13a 도면13b 도면14 도면15 도면16 도면17 도면18"}
{"patent_id": "10-2023-0045503", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 전자 장치가 비디오 인페인팅을 수행하는 방법을 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시예에 따른 인페인팅 모델의 알고리즘을 설명하는 흐름도이다. 도 3은 본 개시의 일 실시예에 따른 요소화된 훈련이 수행된 인페인팅 모델을 설명하기 위한 도면이다. 도 4a 및 도 4b는 본 개시의 일 실시예에 따른 인페인팅 모델의 구조를 설명하기 위한 도면들이다. 도 5a 및 도 5b는 본 개시의 일 실시예에 따른 토큰화 모듈 및 디토큰화 모듈의 동작을 설명하기 위한 도면들이 다. 도 6은 본 개시의 일 실시예에 따른 시간적 어텐션을 수행하는 제2 모듈의 동작을 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시예에 따른 공간적 어텐션을 수행하는 제1 모듈의 동작을 설명하기 위한 도면이다. 도 8a 및 도 8b는 본 개시의 일 실시예에 따른 모션 클루 퓨전 모듈의 동작을 설명하기 위한 도면들이다. 도 9a, 도 9b 및 도 9c는 본 개시의 일 실시예에 따른 전자 장치가 제1 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행하는 방법을 설명하기 위한 도면들이다. 도 10은 본 개시의 일 실시예에 따른 전자 장치가 제2 손실 함수에 기초하여 인페인팅 모델의 훈련을 수행하는 방법을 설명하기 위한 도면이다.도 11a, 11b 및 11c는 본 개시의 일 실시예에 따른 복수의 디코더를 이용한 인페인팅 모델의 훈련을 설명하기 위한 도면들이다. 도 12는 본 개시의 일 실시예에 따른 인페인팅 모델의 훈련 방법을 설명하기 위한 흐름도이다. 도 13a 및 도 13b는 본 개시의 일 실시예에 따른 인페인팅 모델을 이용한 추론 과정을 설명하기 위한 흐름도들 이다. 도 14는 본 개시의 일 실시예에 따른 전자 장치가 이미지 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. 도 15는 본 개시의 일 실시예에 따른 전자 장치가 이미지 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. 도 16은 본 개시의 일 실시예에 따른 전자 장치가 이미지 인페인팅을 수행하는 방법을 설명하기 위한 흐름도이 다. 도 17은 본 개시의 일 실시예에 따른 전자 장치가 메타 버스 상에서 이미지 인페인팅을 수행하는 방법을 설명하 기 위한 흐름도이다. 도 18은 본 개시의 일 실시예에 따른 전자 장치의 구성을 설명하기 위한 흐름도이다."}
