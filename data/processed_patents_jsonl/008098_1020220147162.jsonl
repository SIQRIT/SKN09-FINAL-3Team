{"patent_id": "10-2022-0147162", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0065892", "출원번호": "10-2022-0147162", "발명의 명칭": "영상분석 기반 피로도 측정엔진", "출원인": "(주)바이브컴퍼니", "발명자": "안창원"}}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "정보시스템에 탑재되어, 사람의 안면을 촬영한 동영상을 딥러닝 알고리즘으로 분석하여 피로도 수준을 판단할수 있는 영상분석 기반 피로도 측정엔진으로서, 상기 측정엔진은대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 제1단계;상기 분석대상 영상을 프레임 단위로 분해하는 제2단계;상기 프레임 중에서 분석대상 프레임을 선별하는 제3단계;상기 분석대상 프레임 각각에 대한 피로도 수준을 객체특징 분석모델로 분류하여 이를 제1피로도 수준으로 하는제4단계;상기 분석대상 프레임 각각에 대한 피로도 수준을 object-CNN 모델로 분류하여 이를 제2피로도 수준으로 하는제5단계;상기 분석대상 프레임 각각에 대한 피로도 수준을 CNN 모델로 분류하여 이를 제3피로도 수준으로 하는 제6단계;상기 제1피로도 수준, 상기 제2피로도 수준 및 상기 제3피로도 수준에 대하여 앙상블 알고리즘을 적용하여 상기분석대상 프레임 각각에 대한 최종피로도 수준을 산출하는 제7단계;상기 분석대상 프레임 모두에 대한 상기 최종피로도 수준의 평균 또는 상기 분석대상 프레임 각각에 대한 상기최종피로도 수준 중 가장 많이 나타나는 피로도 수준을 상기 대상자에 대한 피로도 수준으로 판단하는 제8단계;를 수행하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제4단계는- 객체인식 모델을 이용하여, 상기 분석대상 프레임에서 상기 대상자의 얼굴 영역을 검출하는 제4a단계;- 특징점 좌표검출 모델을 이용하여, 상기 얼굴 영역에서 정해진 개수의 특징점 좌표(x, y)들을 추출하는 제4b단계; 및- 분류 알고리즘 기반 피로도 측정모델을 이용하여, 상기 특징점 좌표들을 피로 참값과 비교하여 상기 분석대상프레임 각각에 대한 피로도 수준을 분류하여 이를 상기 제1피로도 수준으로 하는 제4c단계;를 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제5단계는- 객체인식 모델을 이용하여, 상기 분석대상 프레임에서 상기 대상자의 얼굴 영역을 검출하는 제5a단계;- CNN을 이용하여 상기 얼굴 영역에서 벡터 형식으로 특징요소를 추출하는 제5b단계; 및- 상기 CNN 내에서 상기 특징요소를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에대한 피로도 수준을 분류하여 이를 상기 제2피로도 수준으로 하는 제5c단계;를 포함하는 것을 특징으로 하는,영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,공개특허 10-2024-0065892-3-상기 제6단계는- CNN을 이용하여 상기 분석대상 프레임 각각에서 벡터 형식으로 특징요소를 추출하는 제6a단계; 및상기 CNN 내에서 상기 특징요소를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에대한 피로도 수준을 분류하여 이를 상기 제3피로도 수준으로 하는 제6b단계;를 포함하는 것을 특징으로 하는,영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서 상기 객체인식 모델은, YOLO에 대하여 WIDER FACE 데이터셋(data set)으로 얼굴을 학습시킨 것이며,상기 특징점 좌표검출 모델은 Dlib 라이브러리의 Ensemble of Regression Trees 모델이며,상기 특징점 좌표(x, y)는 상기 얼굴 영역 중에서 얼굴 윤곽, 눈, 코 및 입을 포함하는 68개의 특징점에 대한좌표이며,상기 피로도 측정모델은 3개의 완전연결층과 2개의 BN(Batch Normalization)층으로 이루어져서 상기 68개의 특징점을 구성하는 좌표 136개를 입력받으며, 상기 3개의 완전연결층의 각 뉴런은 16개, 8개 및 2개로 구성되어 마지막 층에서 softmax 함수를 적용하여 하나의 피로도 수준으로 분류하며,상기 피로도 측정모델의 학습 시에는 정답과 예측값 을 곱하여 손실함수를 최소화하는 방향으로 학습할 수 있도록, 상기 학습에 사용한 손실함수 cross entropy는 아래와 같은 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제3항 또는 제4항에 있어서 상기 완전연결층은 Global Average Pooling(GAP) 층에 연결하여 구현되며,상기 CNN은 하나의 프레임 크기(W*H*C)를 입력으로 사용하며, 1개의 Convolution 2D 층과 16개의 필터를가지며, 커널 크기는 (3, 3)으로 상기 분석대상 프레임에서 상기 특징요소를 추출하도록 하고, 상기 GAP층으로다차원 배열을 1차원으로 만든 후 상기 완전연결층 1개로 피로도를 분류하며,상기 CNN의 학습 시에는 정답과 예측값 을 곱하여 손실함수를 최소화하는 방향으로 학습할 수 있도록 상기 학습에 사용한 손실함수는 cross entropy로서 아래와 같은 것을 특징으로 하는, 영상분석 기반 피로도측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서상기 앙상블 알고리즘은, 각각의 모델별 학습결과에 따라 모델별 기여도(a)를 부여하되, 상기 모델별 기여도(a)의 합이 1이 되도록 부여하고,각각의 모델별 학습결과에 따라 각각의 모델별로 피로도 수준의 수준별 가중치(b)를 부여하되, 상기 수준별 가중치(b)는 0 ~ 2의 범위에서 부여하며,공개특허 10-2024-0065892-4-상기 분석대상 프레임 각각에 대한 상기 최종피로도 수준은, 각각의 모델이 분류한 피로도 수준에 상기 모델별기여도(a)와 상기 점수별 가중치(b)를 적용한 값의 평균으로 하는 것을 특징으로 하는, 영상분석 기반 피로도측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서 상기 제3단계에서 상기 분석대상 프레임 선별은, - 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지() 각각에 대하여 칼라 히스토그램을 생성하여시간 순서에 따른 M개의 칼라 히스토그램()을 만드는 제1과정;-m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정; 및-- 상기 M개의 칼라 히스토그램() 중 m번째 칼라 히스토그램()을 m+n번째(n은 1부터 시작) 칼라 히스토그램()과 비교한 유사도를 m번째 프레임 이미지()에 대한 유사도()로 하는 제2-1과정;-- 상기 유사도()가 임계값보다 큰 경우 m+n번째 프레임()을 유사프레임으로 정한 후, n+1을 n으로 하여 상기 제2-1과정부터 다시 반복하는 제2-2과정;-- 상기 유사도()가 상기 임계값보다 작은 경우 상기 m번째 프레임 이미지()를 키 프레임()으로 선별하고 n을 상기 유사프레임의 개수로 한 후, m + n을 m으로 하고 n을 다시 1로 하여 상기 제2-1과정부터 다시 반복하는 제2-3과정;- 상기 제2과정에서 선별된 상기 키 프레임()을 상기 분석대상 프레임으로 제공하는 제3과정;을 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서 상기 제3단계에서 상기 분석대상 프레임 선별은, - 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지() 각각에 대하여 Gaussian MixtureModel(GMM)을 이용하여 객체(object)를 제거하고 배경을 추출하는 제1a과정;- 상기 프레임 이미지() 각각의 원본 프레임에서 상기 배경의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정;- 상기 프레임 이미지() 각각의 객체중심 프레임에서 모든 픽셀을 합 연산하여 상기 프레임 이미지() 각각에 대한 행동정보()를 정의하는 제3a과정;- 상기 프레임 이미지() 각각에 대한 행동정보()들을 일정 개수 단위로 묶어 윈도우(windows)를 만드는제4a과정;- 상기 윈도우에 포함된 상기 행동정보()들의 묶음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보()에 대응되는 m번째 프레임 이미지를 키 프레임()으로 선별하는 제5a과정; - 상기 윈도우 내에서 상기 키 프레임()의 뒤프레임이 존재하는 경우 상기 뒤프레임을 상기 키 프레임()의 유사프레임으로 하는 제6a과정;- 상기 윈도우 내에서 상기 키 프레임()의 앞프레임이 존재하는 경우 상기 앞프레임을 상기 키 프레임()이전 키 프레임()의 유사프레임으로 하는 제7a과정; 및공개특허 10-2024-0065892-5-- 상기 제5a과정에서 선별된 상기 키 프레임()들을 상기 분석대상 프레임으로 제공하는 제8a과정;를 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항 또는 제9항에 있어서,상기 제8단계에서, 상기 피로도 수준의 평균 또는 상기 가장 많이 나타나는 피로도 수준을 산출하는 경우에는상기 키 프레임()별 상기 유사프레임의 개수(n)을 반영하여 산출하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진"}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 영상분석 기반 피로도 측정엔진에 관한 것이다. 보다 상세하게로는, 정보시스템에 탑재되어, 사람의 안면을 촬영한 동영상을 딥러닝 알고리즘으로 분석하여 피로도 수준을 판단할 수 있는 영상분석 기반 피로도 측 정엔진으로서, ‘일정 시간 동안 사람의 안면을 촬영한 동영상’을 분석하여 그 사람의 피로도 수준을 판단하는 방법에 관한 것이다. 본 발명은 대상자를 촬영한 동영상을 프레임 단위로 분해한 뒤 객체특징 분석모델, object- CNN 모델 및 CNN 모델 등 3가지의 각각 다른 모델을 사용하는 신경망으로 피로도 수준을 각각 분류하도록 한 뒤 각각의 분류결과에 대하여 앙상블 알고리즘을 적용하여 종합한 뒤 그 결과를 대상자에 대한 피로도 수준으로 판 단하게 한 것이다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 영상분석 기반 피로도 측정엔진에 관한 것이다. 보다 상세하게로는, 정보시스템에 탑재되어, 사람의 안면을 촬영한 동영상을 딥러닝 알고리즘으로 분석하여 피로도 수준을 판단할 수 있는 영상분석 기반 피로도 측 정엔진으로서, ‘일정 시간 동안 사람의 안면을 촬영한 동영상’을 분석하여 그 사람의 피로도 수준을 판단하는 방법에 관한 것이다. 본 발명은 대상자를 촬영한 동영상을 프레임 단위로 분해한 뒤 객체특징 분석모델, object-CNN 모델 및 CNN 모델 등 3가지의 각각 다른 모델을 사용하는 신경망으로 피로도 수준을 각각 분류하도 록 한 뒤 각각의 분류결과에 대하여 앙상블 알고리즘을 적용하여 종합한 뒤 그 결과를 대상자에 대한 피로도 수 준으로 판단하게 한 것이다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "현대 사회를 살아가는 사람들은 현실 공간은 물론 가상의 공간에서도 다양한 상호작용을 통하여 교류하게 되다 보니 충분한 쉼이 없이 바쁜 하루하루가 계속된다. 따라서 현대인에게 피로는 필연적으로 발생하게 되는데, 피 로는 사람들이 경험하는 환경과 상호작용만큼이나 원인과 형태도 다양하다. 특히 현대인의 생활패턴과 업무형태 가 매우 중대한 영향을 끼치게 되는데, 생리적 특성, 수면장애, 개인의 생활 특성, 스트레스, 생활환경, 건강 유지를 위한 다양한 활동 등이 그 원인으로 알려져 있다. 그중 수면장애는 피로의 가장 큰 원인으로 작용하고 있는데, 수면시간은 집중력, 의사결정 능력 반응시간 등의 수행능력에 영향을 주고 이러한 수행능력이 떨어지는 상태를 피로 상태로 나타낼 수 있다. 한편, 항공기나 전투기의 조종사 또는 정밀 무기체계의 운용자 등과 같이 업무수행 시 고도의 집중력이 필요한 전문 종사자들이나 함정의 승조원 또는 장거리 운전기사 등과 같이 장시간 동안 근무해야 하는 반면 돌발상황 발생 시 신속하고 정확한 판단력을 발휘해야 하는 고위험 직종에 근무하는 종사자들의 경우, 피로도가 높은 상 태에서 근무나 전투에 투입되면 인지기능이 저하되어 안전사고 발생의 가능성이 높아지게 될 뿐만 아니라, 전투 인력의 경우에는 전투력 저하에도 크게 영향을 끼치기 때문에 피로도가 높은 사람들을 사전에 가려내어 근무에 서 배제할 수 있도록 하는 제도를 엄격하게 운영할 필요성이 있다. 따라서, 근무자나 전투 인력 각 개인에 대한 피로도를 객관적으로 측정할 수 있는 방법이나 시스템이 필요하다. 그러나 사람에게 있어서 피로라는 개념은 명확하게 구분되지 않는 개념일 뿐만 아니라, 피로도가 높은지 여부는 본인조차도 쉽게 판단할 수 없기 때문에 객관적 기준으로 피로도가 높은 사람들을 걸러내는 것은 쉬운 일이 아 니다. 피로에 관한 가장 객관적인 개념 구분은 피로를 중추 피로 또는 정신적 피로라고 할 수 있는 주관적 피로와 어 떤 과업을 수행하기에 적절한 수준의 신체적 각성을 유지하고 있는가가 기준이 되는 임무 수행 피로 즉, 말초 피로로 구분하는 것이다. 주관적 피로를 측정하는 도구로서, 임상적인 척도로 사용되는 자기 보고식 평가도구는 여러 가지 버전으로 개발되어 있다. 또한, 피로도 수준을 측정하는 기존의 방법에는 설문조사, 생화학 및 생리 학적검사, 반응속도 테스트 등이 있다. 설문에 의한 방법은 다원적 피로 척도 (Multidimensional Fatigue Inventory, MFI)의 내용을 토대로 피로 측정 도구의 개발에 통계적으로 유의미한 결과를 보이고 있다. 그리고, Actigraphy를 활용한 일주기 리듬 교란 현상 에 대해 측정하여 평가하는 경우도 있으며, 심박수, 맥파 등의 지표를 활용하여 측정하는 ECG(ElectrocardioGraphy) 방법, 피부의 온도와 같은 지표를 이용하는 생태 순간 평가(Ecological MomentaryAssessment) 등의 방법도 있으며 혈액, 타액 등의 채취를 통해 측정하는 등의 생리학적 방법도 있다. 그러나 이러한 종래의 측정방법들은 질병의 관점에서 피로의 정도를 판단하는데 주된 목적이 있다 보니, 병적 피로의 원인과 정도를 특정하는 데는 용이하나, 다양한 원인과 형태로 나타나는 피로 정도를 측정하는 데는 제 한을 받는다. 또한, 피로도 측정방법에 있어서 주관적 판단을 사용하거나 개인의 상태와 상황에 따라 편차가 존 재하여 피로도 산정에 절대적 기준으로 사용하기는 어려울 뿐만 아니라 조사에 기반한 피로 측정방법은 시간과 비용이 소요되고 식사 또는 운동, 감염 등 피로 측정에 영향을 미치는 요소들이 많다. 한편, 머신러닝(Machine learning) 및 인공지능(AI) 기술의 발전으로 인해 수많은 데이터를 분석하여 다양한 분 야에서 활용하고자 하는 시도가 계속되고 있다. 특히 컴퓨터비전(Computer vision) 분야의 영상분석 기술의 발 전에 따라 특정 분야에서는 인간의 판단 영역까지 도달하는 분석결과를 나타내고 있는데, 행동분석 등 영상분석 기술은 인간의 상태 및 행동 등을 측정하여 차후 이어지는 행동이나 감정 등을 효과적으로 분석하고 있다. 최근에는 피로와 관련된 데이터를 수집하고 딥러닝 모델을 학습하여 피로를 측정하는 연구가 진행되고 있다. 대 표적으로 센서를 통해 신체의 움직임 데이터를 수집하여 피로를 측정하거나, 장시간 운전자 등을 지속적으로 촬 영하여 눈의 깜빡임 등 동공, 홍채의 형태변화를 학습하여 피로를 측정하는 연구 등이 있다. 그러나 이러한 방 법들은 장시간 동안 대상자를 촬영한 영상을 기반으로 하기 때문에 전투기 조종사 등에 대하여 근무 투입 전에 실시하는 피로도 검사 등에는 사용할 수가 없는 문제점이 있었다. 따라서 본 발명에서는 비교적 짧은 시간에 대상자의 얼굴 부분을 촬영한 동영상을 딥러닝 기반 영상분석 기법에 기반한 모델로 분석하여 대상자의 피로도 상태를 측정할 수 있는 기법을 제안하고, 동영상 분석에 따른 컴퓨터 자원의 효율적 활용과 분석시간의 단축 등을 위하여 영상을 구성하는 프레임 이미지 중에서 유의미한 이미지를 가지는 키 프레임만을 추출하여 이용하는 방법들을 제공한다. 선행기술문헌 비특허문헌 (비특허문헌 0001) Dinges, D. F. 1995. An overview of sleepiness and accidents. Journal of sleep research, 4: 4-14. (비특허문헌 0002) Smets, E.; Garssen, B; Bonke, B. d.; and De Haes, J. 1995. The Multidimensional Fatigue Inventory (MFI) Psychometric Qualifies of an Instrument to Assess Fatigue. Journal of Psychosomatic Research, 39:315-325. (비특허문헌 0003) Simonyan, K.; and Zisserman, A. 2014. Very Deep Convolutional Networks for Large- Scale Image Recognition. arXiv preprint arXiv:1409.1556. (비특허문헌 0004) Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; and Rabinovich, A. 2015. Going deeper with convolutions. In proceedings of the IEEE conference on computer vision and pattern recognition, 1-9. (비특허문헌 0005) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770- 778. (비특허문헌 0006) Chollet, F. 2017. Xception: Deep Learning with Depthwise Separable Convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1251-1258. (비특허문헌 0007) Huang, G.; Liu, Z.; van der Maaten, L.; and Weinberger, K. Q. 2017. Densely Connected Convolutional Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4700-4708. (비특허문헌 0008) Howard, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv preprint arXiv:1704.04861 (비특허문헌 0009) Tan, M.; and Le, Q. 2019. EfficientNet: Rethinking Model Scaling for ConvolutionalNeural Networks. International Conference on Machine Learning, 6106-6115. PMLR. (비특허문헌 0010) Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.,; Heigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929. (비특허문헌 0011) Dai, Z.; Liu, H.; Le, Q.; and Tan, M. 2021. CoAtNet: Marrying Convolution and Attention for All Data Sizes. Advances in Neural Information Processing Systems, 34. (비특허문헌 0012) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention Is All You Need. Advances in Neural Information Processing Systems, 30. (비특허문헌 0013) Hochreiter, S.; and Schmidhuber, J. 1997. LONG SHORT-TERM MEMORY. Neural Computation, 9:1735-1780. (비특허문헌 0014) Cho, K.; Van Merrinboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. arXiv preprint arXiv:1406.1078. (비특허문헌 0015) Dalal, N.; and Triggs, B. 2005. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR 05), volume 1, 886-893. IEEE. (비특허문헌 0016) Soomro, K.; Zamir, A. R.; and Shah, M. 2012. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. arXiv preprint arXiv:1212.0402. (비특허문헌 0017) Kay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev, P.; et al. 2017. The Kinetics Human Action Video Dataset. arXiv preprint arXiv:1705.06950. (비특허문헌 0018) Girshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 580-587. (비특허문헌 0019) Redmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 779-788. (비특허문헌 0020) Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.-Y.; and Berg, A. C. 2016. SSD: Single Shot MultiBox Detector. In European conference on computer vision, 21-37 Springer. (비특허문헌 0021) Sagonas, C.; Tzimiropoulos, G.; Zafeiriou, S.; and Pantic, M. 2013. 300 faces in- the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE international conference on computer vision work-shops, 397-403. (비특허문헌 0022) Redmon, J.; and Farhadi, A. 2018. YOLOv3: An Incremental. arXiv preprint arXiv:1804.02767. (비특허문헌 0023) Yang, S.; Luo, P.; Loy, C.-C.; and Tang, X. 2016. WIDER FACE: A Face Detection Benchmark. In Procedings of the IEEE conference on computer vision and pattern recognition, 5525-5533. (비특허문헌 0024) Kazemi, V.; and Sullivan, J. 2014. One Millisecond Face Alignment with an Ensemble of Regression Trees. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1867-1874. (비특허문헌 0025) Lin, M., Chen, Q., & Yan, S. . Network in network. arXiv preprint arXiv:1312.4400. (비특허문헌 0026) Lilleholt, L., Zettler, I., Betsch, C., & Bhm, R. . Pandemic fatigue:Measurement, correlates, and consequences. (비특허문헌 0027) Qiang, Y., Liu, J., & Du, E. . Dynamic fatigue measurement of human erythrocytes using dielectrophoresis. Acta biomaterialia, 57, 352-362. (비특허문헌 0028) Escobar-Linero, E., Domnguez-Morales, M., & Sevillano, J. L. . Worker’s physical fatigue classification using neural networks. Expert Systems with Applications, 198, 116784. (비특허문헌 0029) Zhuang, Q., Kehua, Z., Wang, J., & Chen, Q. . Driver fatigue detection method based on eye states with pupil and iris segmentation. Ieee Access, 8, 173440-173449. (비특허문헌 0030) Yan, P., Sun, Y., Li, Z., Zou, J., & Hong, D. . Driver fatigue detection system based on colored and infrared eye features fusion. Computers, Materials & Continua, 63, 1563-1574."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "상술한 문제점을 해결하기 위하여 창안된 본 발명은, 딥러닝 알고리즘을 사용하는 정보시스템이, 사람의 안면을 촬영한 동영상을 분석하여 사람의 피로도 수준을 신속하고 정확하게 판단하는 엔진을 제공하는 것을 그 목적으 로 한다. 본 발명의 또 다른 목적은, 장시간 동안 대상자를 촬영한 동영상을 이용하지 않고 비교적 짧은 시간 동안 대상 자의 안면을 촬영한 동영상을 이용하여 피로도 수준을 측정하는 엔진을 제공하는 것이다. 본 발명의 또 다른 목적은, 여러 가지 신경망 모델로 피로도 수준을 분류하도록 한 뒤, 분류결과에 대하여 앙상 블 알고리즘을 적용할 수 있도록 함으로써, 보다 정확한 피로도 수준을 측정하는 엔진을 제공하는 것이다. 본 발명의 또 다른 목적은, 동영상 분석에 따른 컴퓨터 자원의 효율적 활용과 분석시간 단축을 위하여 동영상 속에 포함되어 있는 많은 프레임 이미지 중에서 피로 측정에 사용할 수 있는 의미 있는 프레임 이미지만을 추출 하여 피로도를 측정하는 엔진을 제공하는 것이다. 본 발명이 이루고자 하는 기술적 과제들은 이상에서 언급한 기술적 과제들로 제한되지 않으며, 언급되지 않은 다른 기술적 과제들은 아래의 기재로부터 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이와 같은 목적을 달성하기 위하여 본 발명은, 정보시스템에 탑재되어, 사람의 안면을 촬영한 동영상을 딥러닝 알고리즘으로 분석하여 피로도 수준을 판단할 수 있는 영상분석 기반 피로도 측정엔진으로서, 상기 측정엔진은, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 제1단계; 상기 분석대상 영상을 프레임 단위로 분해하는 제2단계; 상기 프레임 중에서 분석대상 프레임을 선별하는 제3단계; 상기 분석대상 프레임 각각에 대 한 피로도 수준을 객체특징 분석모델로 분류하여 이를 제1피로도 수준으로 하는 제4단계; 상기 분석대상 프레임 각각에 대한 피로도 수준을 object-CNN 모델로 분류하여 이를 제2피로도 수준으로 하는 제5단계; 상기 분석대상 프레임 각각에 대한 피로도 수준을 CNN 모델로 분류하여 이를 제3피로도 수준으로 하는 제6단계; 상기 제1피로 도 수준, 상기 제2피로도 수준 및 상기 제3피로도 수준에 대하여 앙상블 알고리즘을 적용하여 상기 분석대상 프 레임 각각에 대한 최종피로도 수준을 산출하는 제7단계; 및 상기 분석대상 프레임 모두에 대한 상기 최종피로도 수준의 평균 또는 상기 분석대상 프레임 각각에 대한 상기 최종피로도 수준 중 가장 많이 나타나는 피로도 수준 을 상기 대상자에 대한 피로도 수준으로 판단하는 제8단계;를 수행하는 것을 특징으로 하는, 영상분석 기반 피 로도 측정엔진으로 하는 것이 바람직하다. 본 발명은 또한 상술한 특징들에 더하여 상기 제4단계는, - 객체인식 모델을 이용하여, 상기 분석대상 프레임에 서 상기 대상자의 얼굴 영역을 검출하는 제4a단계; - 특징점 좌표검출 모델을 이용하여, 상기 얼굴 영역에서 정 해진 개수의 특징점 좌표(x, y)들을 추출하는 제4b단계; 및 - 분류 알고리즘 기반 피로도 측정모델을 이용하여, 상기 특징점 좌표들을 피로 참값과 비교하여 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하여 이를상기 제1피로도 수준으로 하는 제4c단계;를 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여 상기 제5단계는, - 객체인식 모델을 이용하여, 상기 분석대상 프레임에 서 상기 대상자의 얼굴 영역을 검출하는 제5a단계; - CNN을 이용하여 상기 얼굴 영역에서 벡터 형식으로 특징요 소를 추출하는 제5b단계; 및 - 상기 CNN 내에서 상기 특징요소를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하여 이를 상기 제2피로도 수준으로 하는 제5c단계;를 포 함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여 상기 제6단계는, - CNN을 이용하여 상기 분석대상 프레임 각각에서 벡 터 형식으로 특징요소를 추출하는 제6a단계; 및 상기 CNN 내에서 상기 특징요소를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하여 이를 상기 제3피로도 수준으로 하 는 제6b단계;를 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여, 상기 객체인식 모델은, YOLO에 대하여 WIDER FACE 데이터셋(data se t)으로 얼굴을 학습시킨 것이며, 상기 특징점 좌표검출 모델은 Dlib 라이브러리의 Ensemble of Regression Trees 모델이며, 상기 특징점 좌표(x, y)는 상기 얼굴 영역 중에서 얼굴 윤곽, 눈, 코 및 입을 포함하는 68개의 특징점에 대한 좌표이며, 상기 피로도 측정모델은 3개의 완전연결층과 2개의 BN(Batch Normalization)층으로 이 루어져서 상기 68개의 특징점을 구성하는 좌표 136개를 입력받으며, 상기 3개의 완전연결층의 각 뉴런은 16개, 8개 및 2개로 구성되어 마지막 층에서 softmax 함수를 적용하여 하나의 피로도 수준으로 분류하며, 상기 피로도 측정모델의 학습 시에는 정답 과 예측값 을 곱하여 손실함수를 최소화하는 방향으로 학습할 수 있도 록, 상기 학습에 사용한 손실함수 cross entropy는 아래와 같은 것을 특징으로 하는, 영상분석 기반 피로도 측 정엔진으로 하는 것도 바람직하다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 2, "content": "본 발명은 또한 상술한 특징들에 더하여, 상기 완전연결층은 Global Average Pooling(GAP) 층에 연결하여 구현 되며, 상기 CNN은 하나의 프레임 크기(W*H*C)를 입력으로 사용하며, 1개의 Convolution 2D 층과 16개의 필터를 가지며, 커널 크기는 (3, 3)으로 상기 분석대상 프레임에서 상기 특징요소를 추출하도록 하고, 상기 GAP층으로 다차원 배열을 1차원으로 만든 후 상기 완전연결층 1개로 피로도를 분류하며, 상기 CNN의 학습 시에는 정답 과 예측값 을 곱하여 손실함수를 최소화하는 방향으로 학습할 수 있도록 상기 학습에 사용한 손실함수는 cross entropy로서 아래와 같은 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 3, "content": "본 발명은 또한 상술한 특징들에 더하여, 상기 앙상블 알고리즘은, 각각의 모델별 학습결과에 따라 모델별 기여 도(a)를 부여하되, 상기 모델별 기여도(a)의 합이 1이 되도록 부여하고, 각각의 모델별 학습결과에 따라 각각의 모델별로 피로도 수준의 수준별 가중치(b)를 부여하되, 상기 수준별 가중치(b)는 0 ~ 2의 범위에서 부여하며, 상기 분석대상 프레임 각각에 대한 상기 최종피로도 수준은, 각각의 모델이 분류한 피로도 수준에 상기 모델별 기여도(a)와 상기 점수별 가중치(b)를 적용한 값의 평균으로 하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여, 상기 제3단계에서 상기 분석대상 프레임 선별은, - 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지( ) 각각에 대하여 칼라 히스토그램을 생성하여 시간 순서에 따른 M 개의 칼라 히스토그램( )을 만드는 제1과정; -m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정; 및 -- 상기 M개의 칼라 히스토그램( ) 중 m번째 칼라 히스토그램( )을 m+n번째(n은 1부터 시작) 칼라 히스토 그램( )과 비교한 유사도를 m번째 프레임 이미지( )에 대한 유사도( )로 하는 제2-1과정; -- 상기 유사 도( )가 임계값보다 큰 경우 m+n번째 프레임( )을 유사프레임으로 정한 후, n+1을 n으로 하여 상기 제2-1과정부터 다시 반복하는 제2-2과정; -- 상기 유사도( )가 상기 임계값보다 작은 경우 상기 m번째 프레임 이미 지( )를 키 프레임( )으로 선별하고 n을 상기 유사프레임의 개수로 한 후, m + n을 m으로 하고 n을 다시 1 로 하여 상기 제2-1과정부터 다시 반복하는 제2-3과정; - 상기 제2과정에서 선별된 상기 키 프레임( )을 상 기 분석대상 프레임으로 제공하는 제3과정;을 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으 로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여, 상기 제3단계에서 상기 분석대상 프레임 선별은, - 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지( ) 각각에 대하여 Gaussian Mixture Model(GMM)을 이용하여 객체 (object)를 제거하고 배경을 추출하는 제1a과정; - 상기 프레임 이미지( ) 각각의 원본 프레임에서 상기 배경 의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정; - 상기 프레임 이미지( ) 각각의 객체중심 프 레임에서 모든 픽셀을 합 연산하여 상기 프레임 이미지( ) 각각에 대한 행동정보( )를 정의하는 제3a과정; - 상기 프레임 이미지( ) 각각에 대한 행동정보( )들을 일정 개수 단위로 묶어 윈도우(windows)를 만드는 제4a과정; - 상기 윈도우에 포함된 상기 행동정보( )들의 묶음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보( )에 대응되는 m번째 프레임 이미지를 키 프레임( )으로 선별하는 제5a과정; - 상기 윈도 우 내에서 상기 키 프레임( )의 뒤프레임이 존재하는 경우 상기 뒤프레임을 상기 키 프레임( )의 유사프레 임으로 하는 제6a과정; - 상기 윈도우 내에서 상기 키 프레임( )의 앞프레임이 존재하는 경우 상기 앞프레임 을 상기 키 프레임( ) 이전 키 프레임( )의 유사프레임으로 하는 제7a과정; 및 - 상기 제5a과정에서 선 별된 상기 키 프레임( )들을 상기 분석대상 프레임으로 제공하는 제8a과정;를 포함하는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다. 본 발명은 또한 상술한 특징들에 더하여, 상기 제8단계에서, 상기 피로도 수준의 평균 또는 상기 가장 많이 나 타나는 피로도 수준을 산출하는 경우에는 상기 키 프레임( )별 상기 유사프레임의 개수(n)을 반영하여 산출하 는 것을 특징으로 하는, 영상분석 기반 피로도 측정엔진으로 하는 것도 바람직하다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상에서 살펴본 바와 같이 본 발명은, 앙상블 알고리즘을 적용한 딥러닝 모델로, 사람의 안면을 비교적 짧은 시간 동안 촬영한 동영상을 분석하여 사람의 피로도 수준을 판단할 수 있는 엔진을 제공함으로써, 정보시스템을 통하여 대상자의 피로도 수준을 신속하고 정확하게 측정할 수 있는 효과가 있다. 본 발명은 또한, 정보시스템이 객체특징 분석모델, object-CNN 모델 및 CNN 모델이 포함된 앙상블 알고리즘을 이용하여 대상자에 대한 피로도 수준을 측정하기 때문에 객관적이고 정확한 피로도 측정결과를 제공할 수 있는 효과가 있다. 또한, 얼굴영역을 검출하여 분석한 후 피로도 수준을 분류하는 모델, 얼굴 영역에서 특징점 좌표들을 추출하여 비교, 분석함으로써 피로도 수준을 분류하는 모델 및 얼굴 외에 다른 부분까지도 포함하여 분석하여 피로도를 분류하는 모델 등 모두를 사용하며, 그 학습결과에 따라 결과값을 반영하는 앙상블 알고리즘을 적용하는 엔진이 기 때문에 피로도 측정결과를 객관적이고 정확하게 제공할 수 있는 효과가 있다. 본 발명은 또한, 분석대상 영상의 수많은 프레임 중에서 안면의 움직임이나 자세 등에 변화가 있는 시점의 프레 임 이미지를 키 프레임으로 하여 따로 추출하여 키 프레임만을 분석하고, 키 프레임이 아닌 이미지 즉, 변화가 없어서 키 프레임과 동일한 내용을 가지는 프레임 이미지에 대하여는 분석하지 않기 때문에, 컴퓨터 자원을 효 율적으로 활용할 수 있고 신속하게 피로도를 분석해 낼 수 있는 효과가 있다. 본 발명은 또한, 분석대상 영상 중 분석의 의미가 있는 이미지들만 골라내어 키 프레임으로 하여 분석하기 때문 에 일부 프레임에 대하여만 분석하더라도 모든 프레임 이미지에 대하여 분석하는 것과 동일한 효과를 거둘 수 있게 된다. 본 발명은 또한, 분석대상 영상 중 키 프레임만 추출하여 분석대상으로 사용하더라도 키 프레임 뒤에 오는 동일 또는 유사한 이미지에 대한 개수도 같이 파악하여 제공하고, 피로도 평균을 산출할 때 이를 반영하기 때문에 피로도 평균을 보다 정확하게 산출할 수 있는 효과가 있다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서 상술한 목적과 특징이 분명해지도록 본 발명을 상세하게 설명할 것이며, 이에 따라 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 본 발명의 기술적 사상을 용이하게 실시할 수 있을 것이다. 또한 본 발 명을 설명함에 있어서 본 발명과 관련한 공지기술 중 이미 그 기술 분야에 익히 알려져 있는 것으로서, 그 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에 그 상세한 설명을 생략하기로 한다. 아울러, 본 발명에서 사용되는 용어는 가능한 한 현재 널리 사용되는 일반적인 용어를 선택하였으나, 특정한 경 우는 출원인이 임의로 선정한 용어도 있으며 이 경우는 해당되는 발명의 설명부분에서 상세히 그 의미를 기재하 였으므로, 단순한 용어의 명칭이 아닌 용어가 가지는 의미로서 본 발명을 파악하여야 함을 밝혀두고자 한다. 실 시 예들에 대한 설명에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 실시 예들을 한 정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 실시 예들은 여러 가지 형태로 변경을 가할 수 있고 다양한 부가적 실시 예들을 가질 수 있는데, 여기에서는 특 정한 실시 예들이 도면에 표시되고 관련된 상세한 설명이 기재되어 있다. 그러나 이는 실시 예들을 특정한 형태 에 한정하려는 것이 아니며, 실시 예들의 사상 및 기술 범위에 포함되는 모든 변경이나 균등물 내지 대체물을 포함하는 것으로 이해되어야 할 것이다. 다양한 실시 예들에 대한 설명 가운데 “제1”, “제2”, “첫째” 또는“둘째”등의 표현들이 실시 예들의 다 양한 구성요소들을 수식할 수 있지만, 해당 구성요소들을 한정하지 않는다. 예를 들어, 상기 표현들은 해당 구 성요소들의 순서 및/또는 중요도 등을 한정하지 않는다. 상기 표현들은 한 구성요소를 다른 구성요소와 구분 짓기 위해 사용될 수 있다. 이하에서는 첨부된 도면 등을 참조하여 본 발명을 설명한다. 딥러닝을 기반으로 하여 동영상을 분석하는 기술들 은 다양한 분야에서 연구되어 왔다. 일반적으로 동영상은 여러 개의 프레임 이미지가 합쳐져 만들어진 데이터이 기 때문에 다양한 방법으로 구현되는 영상분석 기술이 개발되고 적용되어 분석되고 있다. 영상분석에 사용되는 대표적인 딥러닝 모델은 Convolutional Neural Network (CNN)이지만, 영상정보에는 시계열 정보를 포함하고 있 기 때문에 Recurrent Neural Network (RNN) 모델을 응용하기도 한다. 영상분석에서 활용할 수 있는 CNN 모델은 위에서 선행기술문헌으로 제시된 바와 같이, VGG, GoogleNet Inception, ResNet, Xception, MobileNet, DenseNet, EfficientNet, ViT, CoAtNet 등의 모델이 이상적인 모델 로 알려지고 사용되고 있다. 최근 영상 분류(Classification) 문제에서 가장 성능이 좋은 모델로 평가받고 있는 ViT, CoAtNet 등의 모델은 자연어처리 분야에서 사용되는 Transformer를 영상 분류 문제에 응용한 것이다. 한편, 동영상 데이터는 일반적인 데이터와 다르게 공간적인 요소가 그 특징 중 하나이므로, CNN 모델은 이를 포 함하여 데이터의 특징을 추출하기 때문에, 동영상 분석에 활용하기에 적합하다. 그리고 RNN 모델은 LSTM, GRU와 같은 은닉층을 활용하여 모델을 구성할 수 있다. RNN 모델은 시퀀스(Sequence)가 입력, 출력으로 사용되는 방법 으로 필기 인식, 음성인식 등과 같은 시계열 정보를 포함하는 데이터에 주로 사용한다. 본 발명에서는 알려진 영상분석 알고리즘을 피로도 측정에 적용할 수 있는 방법으로 조합하여 실험 및 검증을 통해 실제 활용할 수 있 는 모델들로 만들었다. 동영상에서 특정 의미에 맞는 프레임을 찾아내기 위한 기존의 연구들은 영상기반의 행동 인식을 주로 다루고 있 다. 초기에는 얼굴의 특징을 분류하는 연구가 주로 진행되었는데 얼굴의 정면 이미지를 활용하는 기존 HOG 방법 으로 시작하여 자연스러운 영상을 이용한 분석을 추구하고 있다. UCF101 데이터셋(data set)은 101개의 행동 클 래스와 13,000개의 클립과 27시간의 동영상 데이터가 포함되어 있다. 이를 통해 사람의 행동을 인식하는 기술이 연구되고 있다. 대표적으로 Convolution 3D를 활용한 기법이 있다. 그러나, 본 발명에서는 동영상에서 특정 의미에 해당하는 프레임을 찾아내는 것이 아니라, 영상 전체를 구성하 는 프레임에서 특징들을 찾아내고 이를 활용하여 피로도를 측정하는 것에 목표를 두었다. 다만, 동일 또는 유사 한 프레임들 모두까지 분석할 필요가 없기 때문에, 하나의 동영상에 포함된 모든 프레임 전체를 대상으로 하지 않고, 안면의 움직임이나 자세 등에 일정 크기 이상의 변화가 있는 시점의 프레임 등과 같이 분석의 의미가 있 는 키 프레임 이미지만을 따로 골라내어 피로도를 측정하는 방법도 제공함으로써 동일 또는 유사한 프레임까지 중복하여 분석함에 따른 자원 및 시간의 소비를 최소화하였다. 한편, 영상 데이터에 피로도와 관련된 정보가 주어져 있다고 가정할 경우, 영상분석을 통한 피로도 정보 적합 판단모델은 일종의 분류모델이 된다. 도 1에는 본 발명에 의한 영상분석 기반 피로도 측정엔진의 기본 구성도가 도시되어 있다. 도 1에 도시되어 있는 바와 같이 먼저, 동영상을 프레임 단위 화상으로 분해하는 과정을 거치게 된다. 동영상을 프레임 단위로 분해하는 과정에서 사용할 수 있는 방법 중에는 무작위 추출, 시간 단위 무작위 추출 및 Key frame 추출방법 등이 있다. 이렇게 분리된 프레임들을 영상분석 기법을 통해 분석하게 된다. 도 2에는 본 발명에 의한 영상분석 기반 피로도 측정엔진이 대상자에 대한 피로도 수준을 분류하는 절차에 대한 순서도가 도시되어 있다. 이하에서는 도 2를 참조하여 본 발명을 설명한다. 본 발명은 정보시스템에 탑재되어, 사람의 안면을 촬영한 동영상을 딥러닝 알고리즘으로 분석하여 피로도 수준을 판단할 수 있는 영상분석 기반 피 로도 측정엔진에 관한 것으로서, 본 발명에 의한 측정엔진은 먼저 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 제1단계(s10)를 수행하도록 하는 것이 바람직하다. 그 다음에는 상기 분석대상 영상을 프레 임 단위로 분해하는 제2단계(s20)를 수행한 후 상기 제2단계에서 분해된 상기 프레임 중에서 분석대상 프레임을 선별하는 제3단계(s30)를 수행하도록 하는 것이 바람직하다. 그리고 나서 상기 분석대상 프레임들 각각에 대하여 피로도 수준을 분류하도록 하는 것이 바람직한데(s41 ~ s43) 이 과정에서는 각각 다른 3가지 모델을 사용하여 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하 도록 하는 것이 바람직하다. 먼저, 상기 분석대상 프레임 각각에 대한 피로도 수준을 객체특징 분석모델로 분류 하여 이를 제1피로도 수준으로 하는 제4단계(s41)를 수행하면서, 이와 더불어 상기 분석대상 프레임 각각에 대 한 피로도 수준을 object-CNN 모델로 분류하여 이를 제2피로도 수준으로 하는 제5단계(s42) 및 상기 분석대상 프레임 각각에 대한 피로도 수준을 CNN 모델로 분류하여 이를 제3피로도 수준으로 하는 제6단계(s43)를 동시에 수행하도록 하는 것이 바람직하다. 그러나 정보시스템의 가용자원 등을 고려하여 상기 제4단계 내지 제6단계 (s41 ~ s43)를 순차적으로 수행하도록 하는 것도 바람직하다.그 다음에는 상기 제1피로도 수준, 상기 제2피로도 수준 및 상기 제3피로도 수준에 대하여 앙상블 알고리즘을 적용하여 상기 분석대상 프레임 각각에 대한 최종피로도 수준을 산출하는 제7단계(s50)를 수행하도록 하는 것이 바람직하다. 여기서 상기 앙상블 알고리즘은, 각각의 모델별 학습결과에 따라 모델별 기여도(a)를 부여하되, 상 기 모델별 기여도(a)의 합이 1이 되도록 부여하고, 각각의 모델별 학습결과에 따라 각각의 모델별로 피로도 수 준의 수준별 가중치(b)를 부여하되, 상기 수준별 가중치(b)는 0 ~ 2의 범위에서 부여하며, 상기 분석대상 프레 임 각각에 대한 상기 최종피로도 수준은, 각각의 모델이 분류한 피로도 수준에 상기 모델별 기여도(a)와 상기 점수별 가중치(b)를 적용한 후 이들을 평균한 값으로 하는 것이 바람직하다. 예를 들어 각각의 모델별 학습결과 객체특징 분석모델의 정확도가 52%, object-CNN 모델이 59% 및 CNN 모델의 정확도가 67%로 나왔다고 할 경우 객체특징 분석모델의 기여도(a1)는 52/(52+59+67) = 29%가 되며, object-CNN 모델의 기여도(a2)는 59/(52+59+67) = 33%가 되고, CNN 모델의 기여도(a3)는 67/(52+59+67) = 37%가 될 것이다. 그리고 각각의 모델별로 피로도 수준의 수준별 가중치(b)는, 각각의 모델이 평가한 피로도 수준이 어떤 점수대 는 정확하게 판단하나 어떤 점수대는 그렇지 않은 경우가 있는데, 이에 따라 각각의 모델내에서 피로도 수준의 분류결과에 따라 다르게 적용하도록 하기 위함이다. 예를 들어 피로도 수준을 1 ~ 5로 5단계로 분류했을 때, 객 체특징 분석모델에서 피로도 수준 4 ~ 5는 정확하게 분류하지만, 피로도 수준이 1 ~ 2일 경우 정확도가 높지 않 을 경우 그 정확도에 따라 4 ~ 5에 대하여는 수준별 가중치(b1)를 1.5를 부여하고, 3에 대하여는 수준별 가중치 (b1)를 1을, 1 ~ 2일 경우 0.5를 부여할 수 있을 것이다. 이와 같은 방법으로 object-CNN 모델과 CNN 모델에서 도 피로도 수준의 수준별 가중치(b)를 가지도록 하는 것이다. 따라서 상기 분석대상 프레임 각각에 대한 최종 피로도 수준은, (객체특징 분석모델이 분류한 피로도 수준 x 객 체특징 분석모델의 기여도(a1) x 객체특징 분석모델의 피로도 수준의 수준별 가중치(b1)) + (object-CNN 모델이 분류한 피로도 수준 x object-CNN 모델의 기여도(a2) x object-CNN 모델의 피로도 수준의 수준별 가중치(b2)) + (CNN 모델이 분류한 피로도 수준 x CNN 모델의 기여도(a2) x CNN 모델의 피로도 수준의 수준별 가중치(b2))을 3으로 나눈 값을 최종 피로도 수준이 되게 하는 것이다. 한편, 상기 앙상블 알고리즘으로 상기 최종피로도 수준이 산출된 후에는 상기 분석대상 프레임 모두에 대한 상 기 최종피로도 수준의 평균 또는 상기 분석대상 프레임 각각에 대한 상기 최종피로도 수준 중 가장 많이 나타나 는 피로도 수준을 상기 대상자에 대한 피로도 수준으로 판단하는 제8단계(s60)을 수행하도록 하는 것이 바람직 하다. 이하에서는 각각의 모델별로 피로도 수준을 분류하는 과정에 대하여 설명하기로 한다. 먼저, 도 3 내지 도 6에 는 본 발명에 포함되는 객체특징 분석모델을 설명하기 위한 도면이 도시되어 있다. 도 3에는 객체특징 분석모델 의 기본 개념을 설명하기 위한 개념도가 도시되어 있으며, 도 4에는 객체특징 분석모델이 수행되는 framework이 도시되어 있다. 그리고 도 5에는, 객체인식 모델에서 하나의 얼굴 화상에서 68개의 얼굴 주요 특징점 좌표를 추 출하는 개념이 도시되어 있으며, 도 6에는 객체특징 분석모델에서 사용하는 완전연결층 기반의 분류기가 도시되 어 있다. 본 발명에 포함된 피로도 수준 분류모델 중 객체특징 분석모델의 개요를 정리하면, 본 모델은 먼저 분석대상 영 상을 프레임 단위로 분해한 후, 분해해 놓은 프레임 중에서 분석대상 프레임을 선별하고, 이렇게 선별된 분석대 상 프레임에서 상기 대상자의 얼굴 영역을 검출한 뒤, 얼굴을 인식하여 얼굴을 구성하는 요소를 추출한 뒤 이를 이용하여 피로도를 산출하는 것이다. 이를 위하여 본 발명에 의한 분석엔진은, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계(s10), 상기 분석대상 영상을 프레임 단위로 분해하는 상기 제2단계(s20) 및 상기 프레임 중에서 분석대 상 프레임을 선별하는 상기 제3단계(s30)이후 수행되는 상기 제4단계(s41)에서, 객체인식 모델을 이용하여, 상 기 분석대상 프레임에서 상기 대상자의 얼굴 영역을 검출하는 제4a단계, 특징점 좌표검출 모델을 이용하여, 상 기 얼굴 영역에서 정해진 개수의 특징점 좌표(x, y)들을 추출하는 제4b단계 및 분류 알고리즘 기반 피로도 측정 모델을 이용하여, 상기 특징점 좌표들을 피로 참값과 비교하여 상기 분석대상 프레임 각각에 대한 피로도 수준 을 분류하여 이를 상기 제1피로도 수준으로 하는 제4c단계를 수행하는 것이 바람직하다. 즉, 동영상 분석을 위해 화상을 프레임 단위로 분해한 뒤, 분해된 프레임에서 배경, 고개 숙임 등의 노이즈를 제거하기 위하여 얼굴 검출을 실시하며, 검출된 얼굴에서 눈, 코, 입 등의 위치를 추출하기 위하여 정해진 개수 의 특징점 좌표(x,y)라고 명명된 랜드마크를 추출한 뒤, 추출된 좌표로 머신러닝/딥러닝 모델로 피로 참값에 적합한지 판단하도록 하는 것이다. 이하에서는 도 3 및 도 4를 참조하여 본 모델의 각 단계별 진행 과정을 상세하게 설명한다. 먼저 대상자의 안면 을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계를 수행한 뒤 상기 분석대상 영상을 프레임 단 위로 분해하는 상기 제2단계를 수행했을 때, 대상자를 촬영한 하나의 영상은 다음과 같은 화상의 집합으로 분해 된다. 표 1"}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": ": 영상이 분해되는 프레임의 수, ( : 영상의 길이(초), : 영상의 초당 프레임 수 : 수평 해상도(픽셀), : 수직 해상도(픽셀), : 컬러 처리를 위한 채널 설정(기본값 : 3) 여기서, 흑백 또는 컬러 중에서 어느 것을 선택하더라도 화상의 분석 효율 면에서는 큰 차이가 없으나, 일반적 으로 컬러가 성능을 더 높일 수 있다는 가정하에 channel은 3(컬러)으로 하는 것이 바람직하다. 그리고 프레임 의 크기는 동영상 원본 크기에서 바로 얼굴 검출 모델의 입력으로 사용되도록 하는 것이 바람직하다. 그러나 1280*720의 크기는 메모리에 저장하기에 너무 크므로, 화상 분해와 얼굴 영역 검출을 동시에 진행하여 프레임 크기를 줄여 자원의 효율성을 높이도록 하는 것도 바람직하다. 그리고 상기 프레임들 중에서 ‘분석대상 프레임’을 선별하는 제3단계를 수행하도록 하는 것이 바람직하다. 상 기 분석대상 프레임의 선별방법은, 일정 시간 간격마다 임의의 프레임을 추출하는 시간 단위 무작위 추출로 하 는 것도 바람직하다. 즉, 최대 프레임 수와 일정 간격을 설정하고, 일정 간격 사이에 추출할 임의의 프레임 수 를 설정 후 프레임을 추출하도록 하는 것이다. 한편, 이같이 상기 프레임 중에서 분석대상 프레임을 선별하는 상기 제3단계가 수행되고 나면, 객체인식 모델을 이용하여 상기 분석대상 프레임에서 상기 대상자의 얼굴 영역을 검출하는 상기 제4a단계를 수행하도록 하는 것 이 바람직하다. 이를 위하여 상기 분석대상 프레임 각각에서 얼굴 영역을 검출한다. 상기 얼굴 영역은 상기 분 석대상 프레임의 해상도 내에서 추출되며, 이에 사용되는 객체 인식 모델은 대표적으로는 R-CNN, YOLO, SSD 등 이 있다. 객체인식 모델을 이용하여 검출된 얼굴 영역은 다음과 같이 나타낼 수 있다. 표 2"}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": ": 검출된 얼굴의 수평 해상도(픽셀), : 검출된 얼굴의 수평 해상도(픽셀), 다음으로는 특징점 좌표검출 모델을 이용하여, 상기 얼굴 영역에서 정해진 개수(68개)의 특징점 좌표(x,y)들을 추출하는 상기 제4b단계를 수행하도록 하는 것이 바람직하다. 즉, 검출된 얼굴 영역에서 상기 특징점 좌표(x, y)로부터 분석에 사용될 주요 좌표 리스트를 추출한다. 상기 얼굴 영역에 포함된 상기 특징점 좌표 검출(Face landmark detection) 방법으로는 Dlib 라이브러리의 Ensemble of Regression Trees 모델을 사용하는 것도 바람 직하다. 얼굴의 주요 좌표들은 다음과 같이 나타낼 수 있다. 이때, 도 5에서 보는 바와 같이 |L| = 68로써, 하 나의 얼굴 화상에서 68개의 얼굴 주요 특징점 좌표를 추출하는 것이 바람직하다. 표 3"}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": ": 얼굴의 주요부분 좌표, 다음으로는 분류 알고리즘 기반 피로도 측정모델을 이용하여, 상기 특징점 좌표(x, y)들을 피로 참값과 비교하 여 상기 분석대상 프레임 각각에 대한 제1피로도 수준을 분류하는 제4c단계를 수행하도록 하는 것이 바람직하다. 이때 추출된 요소들을 딥러닝 알고리즘을 통해 주어진 피로 수준과 적합시키고, 이를 누적하여 최 종적으로 피로수준을 판단하는 분류기에 연결한다. 이 분석 엔진의 구축 방법은 다양한데, 얼굴의 주요 좌표 리 스트를 입력으로 사용하는 데이터의 특성상 완전연결층(Fully Connected Layer; FCL)을 이용한 분류기를 이용하 는 것이 바람직한데 그 기본 구조는 도 6과 같다. 학습 후 테스트 과정에서는 영상을 프레임으로 분해한 후 학 습된 모델을 통해 각 프레임의 피로 수준별 확률을 도출하고 도출된 피로 수준 중 가장 많은 값이 최종 피로수 준으로 반환 된다. 한편 도 7 내지 도 9에는 본 발명에 포함된 피로도 수준 분류모델 중 objct-CNN 모델을 설명하기 위한 도면이 도시되어 있는데, 도 7에는 objct-CNN 모델의 기본 개념을 설명하기 위한 개념도가 도시되어 있으며, 도 8에는 그 framework이 도시되어 있다. 그리고 도 9에는 object-CNN 모델에서 사용하는 CNN과 완전연결층 기반의 분류 기가 도시되어 있다. 본 발명에 포함된 object-CNN 모델의 개요를 정리하면, 본 발명에 포함된 object-CNN 모델은 먼저 분석대상 영 상을 프레임 단위로 분해한 후, 분해해 놓은 프레임 중에서 분석대상 프레임을 선별하고, 이렇게 선별된 분석대 상 프레임에서 상기 대상자의 얼굴 영역을 검출한 뒤, 얼굴을 인식하여 얼굴을 구성하는 요소를 추출한 뒤 이를 이용하여 피로도를 산출하는 것이다. 이를 위하여 본 발명에 의한 분석엔진은, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계(s10), 상기 분석대상 영상을 프레임 단위로 분해하는 상기 제2단계(s20) 및 상기 프레임 중에서 분석대 상 프레임을 선별하는 상기 제3단계(s30)이후 수행되는 상기 제5단계(s42)에서, 먼저 객체인식 모델을 이용하여, 상기 분석대상 프레임에서 상기 대상자의 얼굴 영역을 검출하는 제5a단계를 수행하고 난 후, CNN을 이용하여 상기 얼굴 영역에서 벡터 형식으로 특징요소를 추출하는 제5b단계 및 상기 CNN 내에서 상기 특징요소 를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하여 이 를 상기 제2피로도 수준으로 하는 제5c단계를 수행하는 것이 바람직하다. 여기서, 대상자를 촬영한 하나의 영상 은 상기 객체특징 분석모델에서 설명한 바와 같은 화상의 집합으로 분해되며, 상기 얼굴 영역 또한 상기 객체특 징 분석모델과 같은 방법으로 검출한다. 즉, 본 발명에 포함되는 objct-CNN 모델에서는 상기 객체특징 분석모델과 같은 방법으로 얼굴영역을 검출해 낸 후, CNN을 이용하여 분석대상 프레임별 각각의 얼굴 영역에 대한 특징요소를 추출하고 이를 완전연결층으로 보 내어 주어진 피로도 수준과 적합시켜 상기 제2피로도 수준을 분류하여 판단한다. 그리고 얼굴 영역의 특징요소 를 추출하기 위해서는 CNN모델이 사용되며, 모델의 구조에 따라 그 특징요소는 벡터 형식으로 추출된다. 이 벡 터는 완전연결층으로 이루어진 분류기로 바로 입력되어 주어진 피로 수준을 적합시키도록 하는 것이다. CNN에서 완전연결층을 구현하는 방식으로 Global Average Pooling (GAP) 층을 사용하도록 하는 것이 바람직한데, 이는 Object-Feature 모델과 같은 전통적인 완전연결층을 사용할 때 발생하는 폭발적인 파라미터 수의 증가로 인한 과적합을 예방한다. GAP 층은 추출된 특징맵의 크기에 관계없이 각 컬러 채널들을 포함되는 값들의 평균으로 대체하므로 과적합 방지와 공간정보의 반영을 통해 분류성능 향상을 기대할 수 있다. 한편 도 10 및 도 11에는 본 발명에 포함되는 CNN 모델을 설명하기 위한 도면이 도시되어 있는데, 도 10에는 본 발명에 포함되는 CNN 모델의 기본 개념을 설명하기 위한 개념도가 도시되어 있으며, 도 11에는 그 framework이 도시되어 있다. 그리고 본 발명에 포함되는 CNN 모델에서 사용하는 CNN과 완전연결층 기반의 분류기는 도 9와 동일하다. 본 발명에 포함된 CNN 모델의 개요를 정리하면, 본 발명에 포함된 CNN 모델은 동영상을 프레임 단위 화상으로 분해하기는 하지만, 분해된 프레임들에 대하여 특정 부분을 객체로 인식하는 전처리 과정이 없으며, 해당 화상 에 존재하는 주요 부분의 특징요소를 추출(Feature extraction)하는 과정을 따로 수행하지 않는다. 즉, 도 10 및 도 11에서 보는 바와 같이 객체 인식은 사용하지 않고 특징요소 추출은 별도의 방법이 아닌 CNN 모델 내에서 처리하도록 한 것이다. 이를 위하여 본 발명에 의한 분석엔진은, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계(s10), 상기 분석대상 영상을 프레임 단위로 분해하는 상기 제2단계(s20) 및 상기 프레임 중에서 분석대 상 프레임을 선별하는 상기 제3단계(s30)이후 수행되는 상기 제6단계(s43)에서, CNN을 이용하여 상기 분석대상 프레임 각각에서 벡터 형식으로 특징요소를 추출하는 제6a단계 및 상기 CNN 내에서 상기 특징요소를 완전연결층(Fully Connected Layer)으로 보내 상기 분석대상 프레임 각각에 대한 피로도 수준을 분류하여 이를 상기 제3피 로도 수준으로 하는 제6b단계를 수행하는 것이 바람직하다. 즉, CNN을 이용하여 프레임 별로 특징요소를 추출하고 이를 완전 연결층으로 보내 주어진 피로 수준과 적합시켜 상기 제3피로도 수준을 판단한다. 그리고 프레임의 특징요소를 추출하기 위해서는 CNN 계열 모델이 사용되며, 모델의 구조에 따라 그 특징요소는 벡터 형식으로 추출된다. 이 벡터는 완전연결층으로 이루어진 분류기(도 9) 로 바로 입력되어 주어진 피로 수준을 적합시키도록 하는 것이다. CNN 모델에서 완전연결층을 구현하는 방식으로 Global Average Pooling (GAP) 층을 사용하도록 하는 것이 바람 직한데, 이는 Object-Feature 모델과 같은 전통적인 완전연결층을 사용할 때 발생하는 폭발적인 파라미터 수의 증가로 인한 과적합을 예방한다. GAP 층은 추출된 특징 맵의 크기에 관계 없이 각 컬러 채널들을 포함되는 값들 의 평균으로 대체하므로 과적합 방지와 공간정보의 반영을 통해 분류성능 향상을 기대할 수 있다. 학습 후 테스 트 과정에서는 영상을 프레임으로 분해한 후 학습된 모델을 통해 각 프레임의 피로 수준별 확률을 도출하고 도 출된 피로 수준 중 가장 많은 값이 최종 피로수준으로 반환된다. 한편, 본 발명 중 상기 분석대상 프레임을 정하는 상기 제3단계에서는, ‘일정 간격마다 임의의 프레임을 추출 하는 방법’이 아닌, ‘키 프레임을 사용하는 방법’을 사용하는 것도 바람직하다. 상기 분석대상 프레임을 정 할 때 키 프레임을 사용하는 방법을 적용하게 되면 동영상 분석에 따른 컴퓨터 자원을 보다 효율적으로 활용할 수 있고, 분석시간을 단축할 수 있을 뿐만 아니라 전체 동영상에서 피로도 수준을 측정할 때 더욱 정확한 값으 로 측정할 수 있게 된다. 본 발명에서 키 프레임을 사용하는 방법은, 각각의 프레임 중에서 이전 프레임과 같거나 유사한 프레임은 분석 대상에서 제외하고 이전 프레임과 차이가 나기 때문에 분석의 의미가 있는 프레임만 선별하여 분석하되, 선별된 키 프레임 다음에 위치하는 동일 또는 유사한 프레임을 몇 개씩 가지는지도 파악하여 제공함으로써 전체 동영상 에서 피로도 평균 산출 시 보다 정확한 값이 산출될 수 있도록 한다. 즉, 대상자가 표정의 변화도 없고, 움직이지도 않는 동안에 촬영된 각각의 프레임들은 모두 동일하거나 유사한 프레임이기 때문에 얼굴 영역도 동일하게 검출되고, 특징점 좌표도 모두 동일하게 검출될 것이기 때문에 피로도 를 측정하면 모두 동일한 값으로 나올 것이다. 따라서 본 발명에서는 프레임 이미지 중에서 피로도 측정에 사용 하기에 의미 있는, 움직임이 있거나 표정의 변화가 있어서 이전 프레임과 다르게 나타나는 프레임을 키 프레임 으로 하여 키프레임에 대한 이미지만을 추출하여 피로도를 측정하되, 키 프레임 뒤에 반복되는 동일 또는 유사 한 프레임이 몇 개씩인지도 파악하여 피로도 평균 산출에 반영하도록 함으로써 피로도 평균의 산출의 정확도를 높이도록 한 것이다. 이를 위하여 본 발명에서는 칼라 히스토그램(color histogram)을 이용하는 방법과 함께 Gaussian Mixture Model (GMM)을 이용하는 방법도 제공한다. 먼저 칼라 히스토그램을 이용하는 키 프레임 생성방법은, 도 6에서 보는 바와 같이 칼라 히스토그램(Color Histogram) 색상분포도를 활용하여 프레임들을 검사하고, 이전 프레임과의 유사도 차이가 크게 나타나는 프레임 을 키 프레임으로 선정하고, 선정된 키 프레임들에 대하여는 동일 또는 유사한 프레임의 개수까지도 같이 제공 하는 것이다. 색상분포도는 화상 내부의 픽셀이 가질 수 있는 모든 픽셀 값들에 대해서, 각 값들의 출현 빈도를 나타내는데 사용한다. 이러한 색상분포도는 화상 내 픽셀들의 대략적인 분포를 확인할 수 있고 다른 화상과의 비교 시 활용될 수 있다. 칼라 히스토그램을 이용하여 키 프레임을 찾아내기 위해서는 먼저 영상의 프레임에 해당하는 색상분포도를 생성 하는데, 프레임의 흐름인 영상 데이터는 색상분포도의 흐름으로 확인할 수 있다. 이 흐름에서 큰 변화가 발생하 는 시점을 키 프레임으로 추출하게 되는데, cosine 유사도를 활용하여 전후 프레임의 색상분포도의 유사도를 계 산하고, 설정된 임계값(threshold)에 따라 프레임의 변화가 의미 있는 것인지 판단하도록 하는 것이 바람직하다. 이같이 본 발명은 프레임 단위의 색상분포도를 활용하므로 처리 속도가 빠르고, 사전에 특정 데이 터에 의존되는 함수나 전처리 방법을 사용하지 않으므로 여러 데이터셋의 활용을 위한 구현이 용이하다. 이를 위하여, 입력된 동영상을 각각의 프레임으로 분리한 후 각 프레임 이미지에 대한 색상분포도를 생성한다. 이후 생성된 색상분포도를 영상의 시간 순서에 따라 나열하게 되고, 각 색상분포도를 이전의 것과 비교하여 유 사도를 측정하게 된다. 여기서 측정된 유사도가 설정된 임계값보다 작은 경우, 이전 영상과 비교해 볼 때 유사 성이 낮은 경우 해당 프레임은 키 프레임으로 추출된다. 도 7에는 본 발명에 포함되는 칼라 히스토그램(Color Histogram)을 이용한 키 프레임 생성방법이 수행되는 구체 적인 절차 흐름이 도시되어 있다. 이하에서는 도 7을 참조하여 설명한다. 먼저, 대상자의 안면을 포함하는 동영 상을 분석대상 영상으로 입력받는 상기 제1단계(s110)를 수행한 뒤에, 상기 분석대상 영상을 프레임 단위로 분 해하는 상기 제2단계(s120)를 수행한 뒤에, 상기 제2단계에서 프레임 단위로 분해된 M개의 프레임 이미지( ) 각각에 대하여 칼라 히스토그램을 생성하여 시간 순서에 따른 M개의 칼라 히스토그램( )을 만드는 제1과정 (s130)을 수행하도록 하는 것이 바람직하다. 그리고 상기 정보시스템은, m = 1에서 m = M이 될 때까지 아래의 과정을 반복하는 제2과정(s140 ~ s148)을 수행 하도록 하는 것이 바람직하다. 즉, 상기 M개의 칼라 히스토그램( ) 중 m번째 칼라 히스토그램( )을 m+n번째(n은 1부터 시작) 칼라 히스토그 램( )과 비교한 유사도를 m번째 프레임 이미지( )에 대한 유사도( )로 하는 제2-1과정(s141)을 수행한 뒤, 상기 유사도( )가 임계값보다 큰 경우(s142) m+n번째 프레임( )을 유사프레임으로 정한 후, n+1을 n 으로 하여(s143) 상기 제2-1과정부터 다시 반복하는 제2-2과정을 수행하도록 하는 것이 바람직하다. 그러나 상기 유사도( )가 상기 임계값보다 작은 경우에는(s142) 상기 m번째 프레임 이미지( )를 키 프레임 ( )으로 선별하고 n을 상기 유사프레임의 개수로 한 후(s144), m + n을 m으로 하고(s146), n을 다시 1로 하 여(s147) 상기 제2-1과정부터 다시 반복하는 제2-3과정을 수행하되, m이 M이 되기까지 수행하도록 하는 것이 바 람직하다(s148). 그리고, 상기 제2과정이 완료된 후에는 상기 제2과정에서 선별된 상기 키 프레임( )을 상기 분석대상 프레임 으로 제공하는 제3과정(s150)을 수행하도록 하는 것이 바람직하다. 여기서 상기 유사도( )는 코사인 유사도를 이용하여 아래 식과 같이 측정되도록 하는 것이 바람직하다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "한편, 상기 제7단계에서, ‘피로도 수준( )의 평균’ 또는 ‘가장 많이 나타나는 피로도 수준’을 산출하는 경 우에는 상기 키 프레임( )별로 상기 유사프레임의 개수(n)을 반영하여 산출하는 것이 바람직하다. 예를 들어 상기 ‘피로도 수준( )의 평균’을 구할 때는, 각각의 키 프레임( )의 피로도 수준( )에 각각의 유사프레임 의 개수(n)까지 감안하여 각각의 키 프레임( )과 그에 따른 유사프레임에 대한 피로도 수준( )의 합계를 낸 후( ), 모든 키 프레임( )별로 상기 유사프레임의 개수(n)까지 감안한 상기 피로도 수준( )의 합계 를 다 더한 값에 동영상 내 모든 프레임 수( )로 나눈 값을 상기 ‘피로도 수준( )의 평균’으로 하는 것이다. 또한, 상기 ‘가장 많이 나타나는 피로도 수준’을 구할 때도 같은 방식으로 각각의 키 프레임( )의 피로도 수준( )에 각각의 유사프레임의 개수(n)까지 감안한 상기 피로도 수준( )의 개수를 산출한 후 가장 많이 나타나 는 피로도 수준( )을 구하는 것이다. 상술한 바와 같이, 상기 제3단계에서 상기 분석대상 프레임을 선별하는 또 다른 방법으로 가우시안 혼합모델 (Gaussian Mixture Model, GMM)을 활용하는 방법도 있다. 이에 대하여 도 8 및 도 9를 이용하여 설명한다. 이 방법 또한, 사람의 안면을 포함하여 촬영한 동영상 정보에서 안면의 움직임에 일정 크기 이상의 변화가 있는 시 점의 프레임 이미지만 추출하여 분석용 정보로 제공하는 방법으로서, GMM을 활용하여 프레임의 흐름을 분석한 후, 프레임 간의 전후 차이가 급격하게 나타나는 부분을 발견하여 해당 프레임을 키 프레임으로 추출하게 된다. 도 8에는 이에 대한 개념이 도시되어 있다. 우선 입력영상에 대해 프레임으로 분할한 후 GMM을 이용하여 각 프 레임에서 객체(object)를 제거하고 배경을 추출하게 된다. 여기서 추출된 배경의 픽셀 값만큼 원본 프레임에서 차감함으로써 배경이 제거된 객체 중심의 프레임을 추출할 수 있다. 그리고, 객체만 존재하는 각 프레임에서 모 든 픽셀을 합 연산하여 해당 프레임의 행동정보(Motion Information)를 정의한다. 여기서 프레임의 묶음 (Window)을 활용하여 순차적으로 정해진 수의 크기만큼 프레임들이 가진 행동정보를 확인하고 해당 묶음에서 가 장 큰 지역 최대값(local maxima)을 선정하게 된다. 이와 같은 방법으로 각 묶음당 최대값을 가지는 프레임이 키프레임으로 추출된다. 일반적인 기술에 의한 키 프레임 추출에 있어서는, 프레임의 흐름에서 큰 변화가 나타나는 것을 감지하기 힘든 경우가 있는데, 본 발명에서는 움직임의 정보를 활용하기 때문에 이를 보완하는 효과 를 확인할 수 있다. 도 9에는 가우시안 혼합 모델을 이용한 영상정보 분석용 키 프레임 생성방법이 수행되는 절차 흐름이 도시되어 있다. 먼저, 대상자의 안면을 포함하는 동영상을 분석대상 영상으로 입력받는 상기 제1단계(s210)와 상기 분석 대상 영상을 프레임 단위로 분해하는 상기 제2단계(s220)를 수행하고 난 뒤에, 상기 제2단계에서 프레임 단위로 분해된 상기 프레임 이미지( ) 각각에 대하여 Gaussian Mixture Model(GMM)을 이용하여 객체(object)를 제거 하고 배경을 추출하는 제1a과정을 수행하도록 하는 것이 바람직하다(s230). 그리고, 상기 프레임 이미지( ) 각각의 원본 프레임에서 상기 배경의 픽셀 값만큼 차감하여 객체중심 프레임을 추출하는 제2a과정(s240)을 수행하고, 그 다음에는 상기 프레임 이미지( ) 각각의 객체중심 프레임에서 모든 픽셀을 합 연산하여 상기 프레임 이미지( ) 각각에 대한 행동정보( )를 정의하는 제3a과정(s250)을 수행하 도록 하는 것이 바람직하다. 그리고, 상기 프레임 이미지( ) 각각에 대한 행동정보( )들을 일정 개수(N개) 단위로 묶어 윈도우 (windows)를 만드는 제4a과정을 수행한 뒤(s261), 상기 윈도우에 포함된 상기 행동정보( )들의 묶음 중에서 가장 큰 지역 최대값(local maxima)을 가지는 행동정보( )에 대응되는 m번째 프레임 이미지를 키 프레임 ( )으로 선별하는 제5a과정(s262)을 수행하도록 하는 것이 바람직하다. 상기 제5a과정(s262)을 수행하고 난 뒤에는 상기 윈도우 내에서 상기 키 프레임( )의 뒤프레임이 존재하는 경 우 상기 뒤프레임을 상기 키 프레임( )의 유사프레임으로 하는 제6a과정(s263) 및 상기 윈도우 내에서 상기 키 프레임( )의 앞프레임이 존재하는 경우 상기 앞프레임을 상기 키 프레임( ) 이전 키 프레임( )의 유사프레임으로 하는 제7a과정(s264)과정을 수행하도록 하는 것이 바람직하다. 그리고, 마지막으로 상기 제5a과정에서 선별된 상기 키 프레임( )들을 상기 분석대상 프레임으로 제공하는 제 8a과정(s265)을 수행한 뒤, 그 다음 N개의 프레임으로 상기 제4a과정 내지 상기 제7a과정 수행을 반복하는 것이 바람직하다. 그리고 상기 제7단계에서, 상기 피로도 수준의 평균 또는 상기 가장 많이 나타나는 피로도 수준을 산출하는 경우에는, 상기 히스토그램을 이용하는 방법에서 설명한 바와 같이, 상기 키 프레임( )별 상기 유사 프레임의 개수(n)을 반영하여 산출하도록 하는 것이 바람직하다. 이같이 칼라 히스토그램 또는 가우시안 혼합모델을 이용하여 선별된 키 프레임들을 상기 분석대상 프레임으로 제공하게 되면, 움직임이 없는 동일한 프레임에 대한 분석은 생략하고 의미있는 프레임 이미지에 대하여만 피로 도 측정이 가능하므로 컴퓨터 자원을 효율적으로 사용할 수 있고 빠른 시간 내에 분석이 가능해진다. 반면에 키 프레임 뒤에 반복되는 동일 또는 유사한 프레임이 몇 개씩인지도 파악하여 피로도 평균 산출에 반영할 수 있으 므로 피로도 평균의 산출 시 그 정확도를 높일 수 있게 된다. 이하에서는 실험예를 통하여 본 발명을 보다 상세하게 설명한다. 이하에서 설명되는 실험예는 본 발명의 이해를 돕기 위하여 제시된 것이며, 본 발명은 여기서 설명되는 실험예와 다르게 다양하게 변형되어 실시될 수 있음이 이해되어야 할 것이다. 이같이 본 발명의 범주 및 기술사상 범위 내에서 다양한 변경 및 수정이 가능함은 본 기 술분야에서 통상의 지식을 가진 자에게 있어서 명백한 것이며, 이러한 변형 및 수정이 첨부된 특허청구범위에 속하는 것도 당연한 것이다. <실험예> 1. Dataset 실험을 위해 사용한 데이터는 피로 관련 영상 및 생체데이터의 수집 시스템을 통하여 수집된 데이터로서, 피험 자의 영상 데이터와 피로도를 분석하여 도출된 피로수준이 상정되어 있다. 각 데이터는 특정 시나리오와 대본대 로 1분간 피실험자를 촬영한 영상으로서 대체로 피실험자의 정면에서 촬영되었으며, 피로도는 피실험자의 주관 적인 판단하에 가장 좋음, 좋음, 보통, 나쁨, 가장나쁨의 피로 수준으로 조사되었다. 본 발명을 위하여 전체 데이터셋 중 피로 수준 1과 5에 해당하는 영상을 사용하여 실험했다. 총 533개 영상을 80%, 10%, 10%로 나누어 학습, 검증, 테스트 데이터셋으로 사용하였으며, 검증 데이터셋으로 모델별 하이퍼 파라미터를 조정하여 가장 적합한 모델을 찾아내도록 하였고, 최종적으로 테스트 데이터셋을 이용하여 제안한 피 로도 측정 모델들의 성능을 검증했다. 영상에서 프레임을 추출하는 방법으로 구조적 임의추출방법(Structured random sampling)을 이용하였는데, 한 영상의 전체 시간을 n초의 단위로 분할하고 각 단위에서 임의의 프레임을 추출했다. 즉 실험대상의 모든 영상은 n개의 프레임 집합으로 전처리되었다. (S = 60) 2. Algorithms for Experiments 객체특징 분석모델은 얼굴 영역의 검출을 위한 모델에서는 객체 인식 모델인 YOLO v3를 사용하여 WIDER FACE 데 이터셋(비특허문헌 28)으로 얼굴을 학습시켜 얼굴 검출에 활용하였으며, 얼굴 특징점 추출에서는 Dlib 라이브러 리의 Ensemble of Regression Trees(비특허문헌 29) 모델을 활용했다. 프레임에서 총 68개의 얼굴, 눈, 코, 입 좌표(x,y)를 추출하여 분류기의 입력으로 사용했다. 학습에 사용된 분류기는 도 16과 같다. 68개의 좌표를 136개의 하나의 벡터를 입력으로 사용되고, 완전연결층 3개와 BN(Batch Normalization)층 2개로, 각 뉴런은 16, 8, 2개로 구성되어 마지막층에서 softmax 함수를 적용하여 주어진 하나의 피로수준으로 분류했다. CNN 모델은 CNN에서 완전연결층이 결합된 형태로 도 17과 같다. 하나의 프레임(H, W, 3)크기를 입력으로 사용되 며, Convolution 2D 층은 1개, 필터 개수는 16개, 커널크기는 (3, 3)으로 프레임의 특징요소를 추출하도록 하고, GAP(GlobalAveragePooling) 층으로 다차원 배열을 1차원으로 만든 후 완전연결층 1개로 피로도를 분류하 도록 했다. 학습 시 사용한 손실함수는 cross entropy를 사용하였으며, 방식은 아래와 같다. 정답( )과 예측값(log( )을 곱하여 손실함수를 최소화하는 방향으로 학습하도록 했다."}
{"patent_id": "10-2022-0147162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "모델의 학습 및 테스트는 리눅스 기반의 GPU 탑재 단일 워크스테이션에서 Python 3.7 기반의 TensorFlow-Keras 환경에서 진행하였으며, 성능 평가의 척도로 실험대상 피로 수준별 정밀도 (Precision), 재현율 (Recall)과 피 로 수준 전체에 대한 정확도 (Accuracy)를 사용했다. 3. Experimental Results and Discussions 객체특징 분석모델은 학습 진행에 따라 검증 정확도는 60%를 상회하고 있으나 검증 손실이 변동성이 크고 손실 값이 1~3.5로서, 학습된 모델이 검증용 데이터셋을 이용한 효과를 검증하는데 제한됨이 관찰되었다. CNN 모델은 학습 진행에 따라 검증 손실이 줄어들다가 빠르게 상승하는 과적합 양상이 관찰 되었다. 검증 손실 은 0.65~0.85로서 객체특징 분석모델보다 비교적 낮게 산출되었다. CNN 모델은 검증 정확도와 손실의 변동성이 크게 나타나 모델이 불안정하다고 볼 수도 있으나, 실험 데이터셋에서의 성능은 검증 정확도의 최댓값이 크고 검증 손실의 최솟값이 작은 CNN 모델이 높은 분류성능을 보일 것으로 예측되었다. 학습 및 검증 단계에서의 정확도 및 손실 (loss)의 변화는 도 18과 같다. 실험 결과 30 에포크(epoch) 이전에 검증정확도는 빠르게 수렴하고, 학습 손실은 지속적으로 하강하며 검증 손실은 상승하는 과적합의 양상을 보였 다. 학습 진행에 따라 검증 정확도는 60%를 상회하고 있으나 검증 손실이 변동성이 크고 손실값이 1 ~ 3.5로서, 학습된 모델이 검증용 데이터셋을 이용한 효과를 검증하는데 제한됨이 관찰되었다. 테스트 데이터를 통해 실험 한 결과 도출된 성능은 아래와 같다. 표 4 Model Class Precision Recall Accuracy 객체특징 분석모델 1 0.6154 0.5000 0.5185 객체특징 분석모델 5 0.4286 0.5455 0.5185 CNN 1 0.6591 0.9062 0.6667 CNN 5 0.7000 0.3182 0.6667 상술한 여러 가지 예로 본 발명을 설명하였으나, 본 발명은 반드시 이러한 예들에 국한되는 것이 아니고, 본 발 명의 기술사상을 벗어나지 않는 범위 내에서 다양하게 변형 실시될 수 있다. 따라서 본 발명에 개시된 예들은 본 발명의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위한 것이고, 이러한 예들에 의하여 본 발명의 기 술 사상의 범위가 한정되는 것은 아니다. 본 발명의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술사상은 본 발명의 권리범위에 포함되는 것으로 해석되어야 한다."}
{"patent_id": "10-2022-0147162", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 의한 영상분석 기반 피로도 측정엔진의 구성도이다. 도 2는 본 발명의 수행과정을 도시한 절차도이다. 도 3은 본 발명에 포함되는 객체특징 분석모델의 기본 개념을 설명하기 위한 개념도이다. 도 4는 본 발명에 포함되는 객체특징 분석모델이 수행되는 framework을 도시한 것이다. 도 5는 본 발명에 포함되는 객체인식 모델에서 하나의 얼굴 화상에서 68개의 얼굴 주요 특징점 좌표를 추출하는 개념을 도시한 것이다. 도 6은 본 발명에 포함되는 객체특징 분석모델에서 사용하는 완전연결층 기반의 분류기를 도시한 것이다. 도 7은 본 발명에 포함되는 object-CNN 모델의 기본 개념을 설명하기 위한 개념도이다. 도 8은 본 발명에 포함되는 object-CNN 모델이 수행되는 framework을 도시한 것이다. 도 9는 본 발명에 포함되는 object-CNN 모델 및 CNN 모델에서 사용하는 CNN과 완전연결층 기반의 분류기를 도시 한 것이다. 도 10은 본 발명에 포함되는 CNN 모델의 기본 개념을 설명하기 위한 개념도이다. 도 11은 본 발명에 포함되는 CNN 모델이 수행되는 framework을 도시한 것이다. 도 12은 본 발명에서, 칼라 히스토그램을 이용하여 키 프레임이 생성되는 프레임웍을 도시한 것이다. 도 13는 본 발명에서, 칼라 히스토그램을 이용하여 키 프레임 생성이 수행되는 절차 흐름도이다. 도 14는 본 발명에서, 가우시안 혼합모델을 이용하여 키 프레임이 생성되는 프레임웍을 도시한 것이다. 도 15은 본 발명에서, 가우시안 혼합모델을 이용하여 키 프레임 생성이 수행되는 절차 흐름도이다. 도 16은 본 발명의 객체특징 분석모델 실험예에서 학습에 사용된 분류기 모델을 도시한 것이다. 도 17는 본 발명의 CNN 모델 실험예에서 학습에 사용된 분류기 모델을 도시한 것이다. 도 18은 본 발명의 실험예에서 학습 및 검증단계의 정확도 및 손실의 변화를 도시한 것이다."}
