{"patent_id": "10-2010-7011168", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2010-0085120", "출원번호": "10-2010-7011168", "출원인": "한국과학기술원", "발명자": "피오릴로 크리스토퍼"}}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "하나 이상의 인공 뉴런을 이용하여 정보를 처리하는 방법으로서,상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하며, 상기 제1 및 제2 부류 중하나 이상은 복수의 입력을 포함하고, 하기 단계를 포함하는 방법:각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하는 단계;각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하는 단계;상기 뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서 상기 제1 부류에서 각 입력의 상기 가중치를 선택하는 단계;상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서 상기 제2 부류에서 각 입력의상기 가중치를 선택하는 단계; 및제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에 상응하는 상기 출력을 생성하는 단계;여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측한다."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 제1 부류의 입력의 가중치의 상기 선택은 Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로 하는방법."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 제2 부류의 입력의 가중치의 상기 선택은 anti-Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로하는 방법."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 제2 부류의 입력의 가중치의 상기 선택은 선형 평균 제곱 추정을 이용하여 수행되는 것을 특징으로 하는방법."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 하나 이상의 인공 뉴런은 막을 더 포함하고,각각의 상기 입력은 상기 하나 이상의 인공 뉴런의 막 내에 별개 세트의 복수의 이온 채널을 포함하고, 각각의 상기 가중치는 상기 입력과 연관된 기능성 이온 채널의 수에 상응하고,상기 차이는 상기 막의 전압차에 상응하는 것을 특징으로 하는 방법."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제1부류의 입력의 상기 가중치는 상기 차이의 함수 및 상기 제1부류의 입력의 가중 합계와 보상 목표 간의상관관계를 최대화하기 위해서 선택되는 것을 특징으로 하는 방법.공개특허 10-2010-0085120-2-청구항 7 하나 이상의 인공 뉴런을 이용하여 정보를 처리하는 방법으로서,상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 상기 하나 이상의 인공 뉴런은 출력을 생성하며, 하기 단계를 포함하는 방법:각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하는 단계;상기 제2 부류의 복수의 입력을 제공하는 단계로서, 각각의 상기 복수의 입력이 가중치 및 활성을 가지고, 각활성은 상기 하나 이상의 인공 뉴런의 과거 출력의 별개의 기억에 상응하는 단계; 및 Anti-Hebbian 타입 유연성 규칙에 따라 상기 제2부류의 각 입력에 상기 가중치를 수정하는 단계;여기서, 상기 제2부류의 입력은 상기 제1 부류의 입력의 활성의 가중 합계를 예측하여 상기 출력에 대응시키도록 학습한다."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "하나 이상의 인공 뉴런을 이용하여 정보를 처리하는 프로세서에 의해 실행가능한 프로그램 코드를 포함하는 컴퓨터 판독가능한 저장 매체로서, 상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하며, 상기 하나 이상의 상기제1 및 제2 부류는 복수의 입력을 포함하고, 하기 프로그램 코드를 포함하는 컴퓨터 판독가능한 저장 매체:각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하는 프로그램 코드;각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하는 프로그램 코드;상기 뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서 상기 제1 부류에서 각 입력의 상기 가중치를 선택하는 프로그램 코드;상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서 상기 제2 부류에서 각 입력의상기 가중치를 선택하는 프로그램 코드; 및제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에 상응하는 상기 출력을 생성하는 프로그램코드;여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측한다."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제1 부류의 입력의 가중치의 상기 선택은 Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로 하는컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 제2 부류의 입력의 가중치의 상기 선택은 anti-Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로하는 컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제8항에 있어서, 상기 제2 부류의 입력의 가중치의 상기 선택은 선형 평균 제곱 추정을 이용하여 수행되는 것을 특징으로 하는컴퓨터 판독가능한 저장 매체.공개특허 10-2010-0085120-3-청구항 12 제8항에 있어서,상기 하나 이상의 인공 뉴런은 막을 더 포함하고,각각의 상기 입력은 상기 하나 이상의 인공 뉴런의 막 내에 별개 세트의 복수의 이온 채널을 포함하고, 각각의 상기 가중치는 상기 입력과 연관된 기능성 이온 채널의 수에 상응하고,상기 차이는 상기 막의 전압차에 상응하는 것을 특징으로 하는 컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제8항에 있어서,상기 제1부류의 입력의 상기 가중치는 상기 차이의 함수 및 상기 제1부류의 입력의 가중 합계와 보상 목표 간의상관관계를 최대화하기 위해서 선택되는 것을 특징으로 하는 컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제8항에 있어서,상기 제2부류의 입력의 활성은 하나 이상의 인공 뉴런의 과거 출력의 복수의 기억을 포함하는 것을 특징으로 하는 컴퓨터 판독가능한 저장 매체."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "하나 이상의 인공 뉴런을 포함하는 정보 처리 장치로서,상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하도록 설정되고, 상기 하나 이상의 상기 제1 및 제2 부류는 복수의 입력을 포함하며;상기 하나 이상의 인공 뉴런은 각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하고;상기 하나 이상의 인공 뉴런은 각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하고;상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서상기 제1 부류에서 각 입력의 상기 가중치를 선택하고;상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서상기 제2 부류에서 각 입력의 상기 가중치를 선택하고;상기 하나 이상의 인공 뉴런은 제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에 상응하는상기 출력을 생성하고;여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측하는,정보 처리 장치."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "하나 이상의 인공 뉴런을 포함하는 정보 처리 장치로서,상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하도록 설정되고, 상기 하나 이상의 상기 제1 및 제2 부류는 복수의 입력을 포함하며;상기 하나 이상의 인공 뉴런은 각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하고;상기 하나 이상의 인공 뉴런은 각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하고;상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서상기 제1 부류에서 각 입력의 상기 가중치를 선택하고;공개특허 10-2010-0085120-4-상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서상기 제2 부류에서 각 입력의 상기 가중치를 선택하고;상기 하나 이상의 인공 뉴런은 제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에 상응하는상기 출력을 생성하고;여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측하는,정보 처리 장치로서, 상기 제1 부류의 입력의 가중치의 상기 선택은 Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로 하는정보 처리 장치."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제15항에 있어서, 상기 제2 부류의 입력의 가중치의 상기 선택은 anti-Hebbian 타입 유연성 규칙에 의해 수행되는 것을 특징으로하는 정보 처리 장치."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제15항에 있어서, 상기 제2 부류의 입력의 활성은 하나 이상의 인공 뉴런의 과거 출력의 복수의 기억을 포함하는 것을 특징으로하는 정보 처리 장치."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제15항에 있어서,상기 하나 이상의 인공 뉴런은 막을 더 포함하고,각각의 상기 입력은 상기 하나 이상의 인공 뉴런의 막 내에 별개 세트의 복수의 이온 채널을 포함하고, 각각의 상기 가중치는 상기 입력과 연관된 기능성 이온 채널의 수에 상응하고,상기 차이는 상기 막의 전압차에 상응하는 것을 특징으로 하는 정보 처리 장치."}
{"patent_id": "10-2010-7011168", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제15항에 있어서,상기 제1부류의 입력의 상기 가중치는 상기 차이의 함수 및 상기 제1부류의 입력의 가중 합계와 보상 목표 간의상관관계를 최대화하기 위해서 선택되는 것을 특징으로 하는 정보 처리 장치.명 세 서기 술 분 야본 발명은 일반적으로 신경 네트워크 모델 및 인공 지능에 관한 것이다. 보다 구체적으로 본 발명은 단일 인공 [0001]뉴런의 계산 기능 모델에 관한 것이다.배 경 기 술인간 또는 다른 동물의 지능을 나타내는 인공 시스템의 개발은 상당 기간 많은 관심을 받아 왔다. 이러한 시도 [0002]중 하나는 생물학적 신경 시스템 중 공지된 요소를 결합시키는 계산 시스템을 개발하는 것이다. 이러한 시도로부터 인공 신경 네트워크가 개발되었다. 인공 신경 네트워크는 다중의 상호 연결된 뉴런으로 구성되고, 각 뉴런의 출력 신호는 입력 신호에 의존한다. 상기 네트워크 전체로서 외부 입력을 받아 출력 신호를 생성시킨다. 상기 시스템의 목적은 뉴런 간의 연결 가중치를 조정함으로써 각 입력 패턴에 대하여 적절한 출력 신호를 생성시키도록 학습시키는 것이다. 연결 가중치의 조정은 출력 신호의 적절성에 대한 일종의 피드백 영향으로부터 발생공개특허 10-2010-0085120-5-할 수 있다. 신경 네트워크는 유용한 지능을 생성시키는 데 있어서 일정한 성과를 이루었다. 1950년대부터 연구되어 왔음에 [0003]도 불구하고, 생물학적 신경 시스템에 의하여 생성된 수준의 인공 지능을 이끌어내는 중요 시도들은 실패하여왔다. 유사하게, 인공 네트워크의 뉴런은 생물학적 뉴런과 제한된 유사성만을 가지고 있다.새물학적 신경 시스템은 그 자체들을 학습하는 능력(즉, 자기 조직화)을 갖는다. 신경 네트워크에 관한 연구의 [0004]주 목적은 동일한 능력을 갖는 인공 네트워크를 생성시키는 것이다. 이상적으로 네트워크는 외부 세상에 대한정보를 실질적으로 가지지 않고, 스스로 반복하여 지식을 갖게 되는 것이다. 이러한 목표가 달성될 수 있을지,없을지 여부와 관계없이, 실질적인 목적은 출발에서는 가능한 적은 정보를 가졌음에도 불구하고, 학습할 수 있는 대상이 보편적인 네트워크를 개발하는 것이다. 신경 네트워크는 일반적으로 특정 문제를 해결하는 방법에 대하여, 상당한 지식을 사용하도록 설계되었다. 네트 [0005]워크에게 수행 오류를 주로 전달하는 외부생성 감시 신호의 영향에서 훈련된 네트워크의 경우 이는 더욱 명백하다. 따라서, 네트워크에서는 전문가 정보가 공급된다. 반면, 생물학적 신경 시스템은 시도 및 오류(trial anderror) 방식으로 스스로 학습하여야 한다. 유사하게, 몇몇 인공 네트워크는 감시되지 않는 학습을 수행하며, 이것은 외부 감시 신호에 의존하지 않는 시스템이다. 하지만, 이들 네트워크는 일반적으로 특정 입출력 전환을 수행하도록 설계되었다. 따라서, 설계자는 어떻게 출력 신호가 입력에 의존하는 지에 대한 지식을 가지고 상기 네트워크를 설계하여야 한다. 적정한 입-출력 전환은 다른 임무 또는 환경에서 달라지기 때문에, 하나의 환경에서설계된 네트워크는 새로운 환경에 적응하기 어려울 수 있다. 반대로, 생물학 발견자는 그 자체로 어떻게 출력이입력에 의존하는지를 발견하고, 생물학적 신경 시스템은 새로운 환경에 상당 수준으로 적응할 수 있다. 본 발명의 사상은 생물학적 뉴런의 계산 모델에 기초한 것이다.네트워크의 목적은 적절한 출력값을 선택하는 것이고, 이 과정은 “의사결정”이라 지칭될 수 있다. 의사 결정 [0006]에 있어서의 문제는 외부의 현재 및 외부 상태에 대한 불확실성 때문이다. 예측에서의 불확실성은 역으로 예측과정이 기초한 정보의 양에 의존하며, 따라서, 불확실성을 최소화하는 것은 정보를 최대화시키는 것과동일하다. 만약, 상기 시스템이 정확하게 외부 세상의 상태를 예측한다면, 문제는 해결될 것이고, 시스템은 가장 유리한 것으로 알고 있는 출력값을 단순히 선택하면 된다(예를 들면, 미래의 보상 값을 최대화하는 것을 기대되는 출력 신호의 선택). 따라서 불확실성을 최소화시키는 과정은 의사결정 과정과 형식적으로 동일한 것으로판단되는데, 왜냐하면, 결정은 불확실성이 없는 상태에서는 사소한 문제일 뿐이기 때문이다.본 명세서에서 “정보”라는 용어는 네트워크 또는 뉴런이 생물리학적 구조 내에 저장 또는 담고 있는 정보를 [0007]의미한다. 뉴런 막에서의 이온 채널과 같은 단일 분자조차 정보를 담고 있다. 정의한 바와 같이, 정보는 무엇인가를 예측한다. 따라서, 뉴런에 의하여 수행된 예측은 전체적으로 뉴런에 의하여 처리된 정보에 의존한다. 본발명에서 관련 정보는 네트워크의 관찰자가 네트워크 또는 세상에 대하여 가지고 있는 것이 아니다. 본 발명은 단일 뉴런의 계산 기능에 대한 학습을 강화시키는 근본 원칙의 적용이라 볼 수 있다. 학습 강화는 어 [0008]떻게 생물학 또는 인공적인 구성요소(agent)가 스스로의 목적을 달성할 수 있는지에 연관된다. 구성요소의 모든출력은 미래의 목표를 달성하기 위하여 선택되어야 하므로, 구성요소는 “장래의 보상 총합”을 예측하도록 학습하여야 한다. 비록 장래 보상의 개념이 다소 추상적이고, 무형의 형태로 보이나, 그 자체가 장래 보상을 예측할 수 있는 구체적인 동기와 사건들을 예측함으로써, 구성요소는 장래 보상을 예측한다. 따라서, 구성요소는 감각 세계의 현재 상태를 예측하도록 노력하는데, 왜냐하면 감각 세계는 장래 보상에 있어서 유용한 정보를 기간때문이다. 예를 들면, 음식 모습은 음식 소화를 예측하게 하며, 따라서 구성요소는 음식 모습을 예측할 수 있기를 원하게 된다. 공개특허 10-2010-0085120-6-이러한 예측들을 학습하기 위하여, 구성요소는 예측 오류에 의존한다. 예측 오류는 기대되는 것과 실제 관찰되 [0009]는 것 사이의 차이이다. 본 명세서에서 기재되는 인공 뉴런은 일종의 예측 오류를 일으키며, 인공 뉴런은 이것을 이용하여, 장래 보상을 상당 수준으로 예측할 수 있는 세계의 일 측면을 예측하도록 학습한다. 반면, 학습강화에 대한 대부분의 과거 연구는 네트워크 및 시스템 수준에서의 학습 강화 원칙을 실행하였으나, 본 발명은 이들 원칙을 단일 뉴런의 기능에 적용하고, 이를 뉴런 네트워크까지 확장하였다. 인공 뉴런 네트워크는 다중 상호연결 뉴런으로 구성된다. 각 뉴런의 출력은 입력의 가중 합에 의하여 결정된다. [0010]각 입력은 정보원으로 기능하며, 원칙적으로 뉴런은 무한대의 입력수를 사실상 가질 수 있다. 하지만, 뉴런 출력은 충분히 높은 가중치를 갖는 입력에만 실질적으로 의존한다. 인공 네트워크를 창출하는 것은 따라서 적정한가중치를 선택하는 문제라 볼 수 있다. 이런 문제에 대한 50년간의 연구에도 불구하고, 일반적이고, 보편적으로적용가능하며, 뉴런 입력의 각각에 대하여 할당되어야 하는 가중치를 특정할 수 있는 일련의 가이드라인이 없었다. 본 명세서에서, 새로운 방식에 따라 뉴런 입력의 가중치를 선택할 수 있다. 몇몇 입력은 뉴런의 관점에서예측할 수 없다면 높은 가중치가 주어지며, 반면에 몇몇은 이들이 효과적인 예측자라면 높은 가중치가주어진다. 비록 이들 새로운 원리가 실행될 수 있는 복수의 방식이 있지만, 가장 효과적인 구체예는 유연성 알고리즘 [0011](plasticity algorithm)을 사용하는 것이다. 실제 뉴런들은 시냅틱 앞 부분(프리-시냅틱)으로부터 시냅틱 뒤 부분(포스트-시탭틱)으로의 일 방향에서 정보가 전달된다고 일반적으로 여겨지는 시냅스를 통하여 서로 통신한다.2개의 뉴런 사이의 연결 강도는 가중치로 설명되며, 많은 인공 신경 네트워크는 유연성 알고리즘을 사용하는데,이것은 시냅스 앞 부분의 입력 뉴런과 시냅스 뒤 부분의 출력 뉴런에서의 일치하는 활동(활성)에 따라 가중치를조정한다. 일반적으로 사용되는 유연성 원칙 등급은 Hebbian으로 알려져 있다. Hebbian 원칙에 따르면, 시냅스앞 부분의 활동이 시냅스 뒤 부분의 활동에 긍정적으로 연결되면, 연결 가중치는 커진다. 따라서, 만약 시냅스앞 부분 뉴런이 시냅스 뒤 부분 뉴런을 대략적으로 동시에 흥분시키면, 연결 가중치는 증가한다. 만약, 시냅스앞 부분 뉴런의 활동이 뒤 부분 뉴런 활동을 저해시키면, 연결 가중치는 떨어진다. 인공 신경 네트워크에서 유연성 보다는 덜 사용되는 것이, “anti-Hebbian” 원칙이다. [0012]Hebbian 규칙은 흥분 또는 저해와 각각 연관된 흥분 또는 저해 입력을 강조하며, 따라서 긍정적인 피드백을 포 [0013]함한다. Anti-Hebbian 규칙은 단지 그 반대를 하며, 부정적인 피드백을 발생시킨다. 따라서, 시냅스 뒤 부분 뉴런이 저해 입력이 활성일 때 흥분하게 된다면, 이들의 연결 가중치는 증가한다. Anti-Hebbian 규칙은 세포 활성을 비연관시키는데 효과적이며, 이로써 효과적인 코딩을 촉진시키고, 유하게는 예측 학습에 있어서 효과적이다. Hebbian 또는 Anti-Hebbian 원칙은 수십 년간 현재 문헌상으로 연구되지만, 이들 유연성 원칙을 신경 네트워크 [0014]에 적용하는 것이 인공 지능을 생성시키고자 하는 문제의 일반적인 해답이 되지 않고 있다. 비록, 유연성 원칙각각은 분명한 장점을 보였지만, 특정 뉴런 또는 이들의 입력 각각에 대하여 어떠한 원칙이 적용되어야 하는지가 명확하지 않다. 본 명세서에서 단일 뉴런 내에서 Hebbian 및 Anti-hebbian 유연성 원칙을 특정 방식으로 조합시키는 것이, 인공 출력을 생성시키도록 학습하는 인공 네트워크 생성시키는 데 있어서 효과적이다는 점을 제안한다.뉴런들 사이의 시냅스 연결성에 관하여 독점적으로 초점을 맞추는 데 있어, 인공 신경 네트워크에 관한 대부분 [0015]의 연구는 생물학적 뉴런의 중요 측면을 간과하였다. 생물학적 뉴런의 출력은 이들의 시냅스 입력에 의해서만결정되지 않는다. 대략적으로 동일한 수준으로 비-시냅스 이온 채널이 이 결정에 기여한다. 많은 이온 채널의생동은 막 전압에 의하여 조절된다. 막이 일단 비극성화되면, 뉴런을 개방 또는 저해시키는 전압 게이트 칼륨채널이 그 예가 된다. 이 외에도 다양한 형태의 전압-게이트 칼륨 체널이 있으며, 이들은 뉴런 각각에 의하여생성되나, 어쨌든 특정 뉴런은 선택적으로 몇몇 종류의 칼륨 채널을 발현시키나, 나머지는 발현시키지 않는다.생물학적 뉴런에서 이러한 선택을 결정하는 원칙은 알려져 있지 않으며, 대부분의 인공 네트워크는 이러한 비-시냅스 이온 채널과의 유사성을 포함하지 않는다. 본 발명은 비-시냅스 이온 채널에 관한 핵심 역할을공개특허 10-2010-0085120-7-제공하며, anti-Hebbian 유연성을 사용하여 다양한 형태의 비-시냅스 이온 채널 가운데서 선택하게 한다. 발명의 내용해결하려는 과제본 발명은 인공 뉴런을 이용한 정보 처리 방법 및 장치를 제공한다. [0016]과제의 해결 수단본 발명은 하나 이상의 인공 뉴런을 이용하여 정보를 처리하는 방법으로서, [0017]상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하며, 상기 제1 및 제2 부류 중 [0018]하나 이상은 복수의 입력을 포함하고, 하기 단계를 포함하는 방법: 각 입력이 가중치 및 활성을 가진 상기 제1부류의 입력을 제공하는 단계; 각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하는 단계; 상기뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서 상기 제1 부류에서 각 입력의 상기 가중치를 선택하는단계; 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서 상기 제2 부류에서 각 입력의 상기 가중치를 선택하는 단계; 및 제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에상응하는 상기 출력을 생성하는 단계; 여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측한다. 발명의 효과본 발명은 비-시냅스 이온 채널에 관한 핵심 역할을 제공하며, anti-Hebbian 유연성을 사용하여 다양한 형태의 [0019]비-시냅스 이온 채널 가운데서 선택하게 한다. 과거 정보에 기여하는 입력은 오류를 최소화하기 위해서 선택되며, 이는 anti-Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 현재의 정보 공급원은 오류를 최대화하기 위해 선택되며, 이는 Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 이것은 뉴런이 뉴런이 이미 보유하고 있는 과거의 정보와 중복되지 않은 외부 세계로부터 새로운 정보를 수신한다는 것을 확인시켜준다. 예측하기 위해스스로 학습함으로써, 뉴런 또는 이들 뉴런의 네트워크는 지적이고 유리한 출력을 생성하는데 필요한 정보를 획득한다. 도면의 간단한 설명도 1A-1D는 종래 기술에서 알려진 바와 같이 글루타메이트의 제곱 파 펄스(square wave pulse)에 대한 단순화된 [0020]뉴런의 멤브레인 전압 및 전도도 반응이며, 여기에서 뉴런은 글루타메이트-게이트된 양이온 채널 및 전압-게이트된 칼륨 채널을 갖는다;도 2는 단일 인공 뉴런에 대한 정보 공급원 또는 입력을 나타내며, 여기에서 각 화살표는 본 발명의 일 구체예에 따라 개별 입력에 대한 정보 입력 흐름의 방향을 나타낸다;도 3A-3H는 Hebbian-anti-Hebbian 규칙에 따라 상이한 이온 채널들 중에서 선택된 Hodgkin-Huxley 형 모델 뉴런 단일 구획, 구배 전위차, 단일 분획 시뮬레이션 결과이다; 그리고도 4는 본 발명의 일 구체예에 따른 컴퓨터 판독 개체를 나타내는 블록도이다.발명을 실시하기 위한 구체적인 내용발명의 요약 [0021]본 발명은 생물학적 뉴런에서 발생할 수 있는 과정에서 영감을 얻은 정보 처리 방법을 기재한다. 인공 뉴런은 [0022]복수의 입력을 통합하며, 이 중 몇몇은 흥분성이나, 나머지들은 저해성이다. 바람직한 구체예에서, 뉴런의 출력은 전압이고, 입력은 이온 전도도이다. 이들 입력은 두 부류(클래스)로 나뉜다. 제 1 부류(첫 번째 또는 제 1클래스)는 현재 정보를 제공하는 것으로 언급되며, 제 2 부류(두 번째 또는 제 2 클래스)는 (공간상 또는 시간상의) 선행 정보를 제공한다. 선행 정보를 기여하는 입력은 현재 정보를 기여하는 것들의 영향을 예측, 반작용하도록 작동하며, 따라서 뉴런의 출력은 예측 오류로 간주될 수 있다. 만약, 뉴런의 현재 정보가 흥분성이라면,흥분 정도가 선행 정보에 기초하여 기대되는 것보다 큰 경우, 뉴런은 흥분될 수 있으며, 흥분 정도가 선행 정보공개특허 10-2010-0085120-8-에 기초하여 기대되는 것보다 적은 경우, 뉴런은 저해될 수 있으며, 흥분 정도가 예측치를 만족하면, 중간 상태를 유지할 것이다.뉴런에 대한 각 입력은 이것의 영향 강도를 정량화하는 연관 가중치를 가질 수 있다. 본 발명은 가중치를 결정 [0023]해야 하는 원칙을 특정한다. 입력의 제 1 부류 및 제 2 부류 간의 주 차이점은 가중치를 할당하도록 사용되는원칙에 있다. 제 1 부류 입력은 예측 착오를 최대화하도록 선택되어야 한다(즉 가중되어야 한다). 이것은 뉴런에 대하여 정보를 제공하는 세계의 일 측면에 관한 현재 정보를 뉴런이 수신하는 것을 보증하는데, 따라서, 이것은 매우 흥미로운 것인데, 왜냐하면 예측은 어렵기 때문이다. 바람직한 구체예는 Hebbian 유연성(plasticity)을 통하여 예측 착오를 최대화시키는 원칙을 실행한다. Hebbian 유연성에 따르면, 뉴런이 흥분되는 때와 동시에 활성인 개별 흥분성 입력은 예측착오에 기여하며, 따라서 강화될 것이다. 뉴런이 저해되는 때에 활성인 흥분입력은 착오에 반작용이며, 약화될 것이다. 또 다른 구체예에서, Hebbian 규칙은 네트워크의 목적에 연관된 일종의 정보 피드백을 포함하며, 이로써 뉴런은 예측성은 낮으나 네트워크의 목적과 가장 가까이 연관된 제 1 부류 입력에 대하여 높은 가중치를 부여한다.입력의 제 2 부류에 연관된 가중치는 예측 오류를 최소화시키기 위하여 선택되며, 이로써 정확한 예측을 하게 [0024]된다. 바람직한 구체예의 뉴런에서, 이것은 오류가 0가 되는 영역 중간 값으로 멤브레인 전위차를 구동시키는입력에 높은 가중치를 주게 한다. 이를 구현하는 일 수단은 anti-Hebbian 규칙이다. 뉴런이 흥분함과 동시에 활성을 갖는 개별 저해성 입력(양성 예측 오류를 신호로 준다)은 오류에 반작용하며, 따라서, 이것은 강화될 것이지만, 반면 뉴런이 저해될 때 활성을 갖는 저해 입력은 오류를 부여하며, 따라서 약화될 것이다. 이 원칙은 제2 부류 입력(선행 정보를 부여하는 것들)이 뉴런 출력에서의 제 1 부류 입력(현재 정보 기여)의 효과를 예측하고, 제거하도록 학습시킨다.선행 정보가운데서 선택하게 하는 anti-Hebbian 유연성과 보상 피드백을 통합하는 Hebbian 유연성 둘 다를 활용 [0025]함을써, 인공 뉴런은 미래 보상을 예측하도록 학습된다. 강화 학습 분야에서 인정되는 바와 같이, 미래 보상의예측은 지능적 행동의 생성에 있어서 중요하다. 반면, 이들 뉴런 중 하나는 적은 양의 정보를 통합하여, 보상과연관된 세계의 측면을 예측할 수 있으나, 이들 뉴런의 네트워크는 가장 중요한 세계의 측면을 예측하여, 지능적행동을 생성시킬 수 있다.생물학적 뉴런은 일 실시예의 인공 뉴런이 예측을 학습하는지를 예시하기 위하여 기재된다. [0026]생물리학 및 예측 오류 [0027]뉴런은 막 사이의 전압차이로 정보를 축적한다. 뉴런은 에너지를 사용하여, 막 바깥에서 보다 높은 농도의 나트 [0028]륨 이온을 유지하며, 안쪽에서는 보다 높은 농도의 칼륨 농도를 유지한다. 뉴런은 이것의 막에서 다공 또는 채널 구조를 가지며, 이것은 관통하여 지나는 이온 종류가 상이하다. 예를 들면, 몇몇 이온 채널은 칼륨에 대하여선택적이며, 따라서 이온 채널이 개방되며, 칼륨이 농도 구배에 따라 흐르게 되며, 뉴런을 흥분시키고, 이에 따라 칼륨 평형 전위차(이것은 보통 -100mV) 로 막을 과분극시킨다. 뉴런은 일반적으로 다양한 종류의 이온 채널을 갖지만, 막 전압으로 정보를 축적하는 가장 간단한 뉴런은이온 선택성을 달리하는 두 종류만의 이온 채널을가질 것이다.이와 같이 단순화된 뉴런은 나트륨과 칼륨 투과성을 갖는 일단의 채널을 함유하며, 이들 채널은 따라서 제로 밀 [0029]리볼트(Ecat = 0)의 평형 전위차를 갖는다. 이들은 외부 화학물질의 충분한 농도에서 개방된다. 예를 들면, 화학물질인 글루타메이트이며, 따라서, 상기 채널들은 클루타메이트-게이트 이온 채널이 된다. 두 번째 종류의 채널은 개방될 때 칼륨 전도를 수행하며, 막 전압에 의하여 개방된다. 칼륨 평형 전위차는 -100mV(EK = -100)이다.뉴런 막 사이의 전압은 따라서 하기 식의 정상 상태 값(V)에 접근한다.공개특허 10-2010-0085120-9-(1) [0030]여기에서 Gcat 및 Gk는 각각 양이온 및 칼륨 전도도이다. [0031]글루타메이트의 파동 제곱 펄스에 대한 뉴런의 이러한 반응은 도 1(종래 기술)에 도시된다. 글루타메이트-게이 [0032]트 채널이 개방되면 뉴런은 탈분극되지만, 전위-게이트 칼륨 채널이 개방되기 시작함에 따라 서서히재분극된다. 글루타메이트 농도에 관한 정보는 따라서 글루타메이트-게이트 채널로 흐르며, 다시 막 전압, 그리고 그 다음으로 칼륨 채널로 흐르게 된다. 전압이 뉴런의 출력이므로, 칼륨 채널의 전도도(conductance)는 과거출력의 저장 값에 대응된다. 채널들의 두 집합은 글루타메이트 농도에 관한 정보를 함유하며, 따라서, 이들은 글루타메이트 농도를 예측하거 [0033]나, 측정할 수 있다. 각 짧은 순간에서, 칼륜 채널의 전도도는 과거의 글루타메이트 농도에 의존하는 반면, 글루타메이트 게이트 채널은 현재의 글루타메이트 농도에 적어도 일부 의존한다. 따라서, 글루타메이트-게이트 채널은 글루타메이트 농도에 관한 현재 정보를 보유하는 한편, 칼륨 채널은 글루타메이트 농도에 관한 선행 정보를 보유한다. 따라서 뉴런은 글루타메이트에 관한 두 개의 예측 값을 갖는 것으로 이해될 수 있는데, 하나는 현재 정보에 대하여 조건적이며 (즉, 제1 부류 입력), 다른 하나는 선행 정보에 조건적이다(즉, 제2 부류 입력).막 전압은 두 예측 값 또는 정보 집합의 비교에 의존한다(도 2). 글루타메이트-게이트 채널에 의한 글루타메이트 농도 추산치가 칼륨 채널에 의한 추산치를 초과하는 때, 뉴런은 [0034]탈분극된다. 그 역이 사실일 때, 뉴런은 과분극된다. 이온 채널의 두 집합이 거의 동일 정보를 가질 때, 막 전압은 중간대의 전압이다. 이것은 도 1에 도시된 바와 같으며, 도 1은 글루타메이트의 제곱파 펄스에 대하여 이들 두 종류의 이온채널만을 갖는 뉴런의 반응을 보여준다. 뉴런의 막 전압(도 1B)은 따라서 예측 오류로 여겨질수 있다. 글루타메이트 농도가 상승한 후, 즉시 뉴런은 놀랍게도 가장 크게 탈분극되며, 이것은 양성 예측 오류에 대응한다. 이것은 두 채널 전도도의 미스매치에 기인한다(도 1C와 1D를 비교). 글루타메이트 농도의 감소에후속하는 탈분극은 음성 예측 오류에 대응될 수 있다.양성 또는 음성 예측 오류는 청구항과 아래에서 “차이(discrepancy)”로 지칭된다. 이것의 물리적 상관성은 바 [0035]람직한 실시예에서 뉴런 막 사이의 전압이며, 중간 전압은 0 예측 오류에 대응된다. 막 전압과 예측 오류 사이의 정확한 정량적 상관관계는 단조함수형태로 추정되며, 반드시 선형은 아니다. 유사하게, 아래에서 예측 오류(prediction error)를 최대화 또는 최소화시키는 용어는 예측 오류 또는 막 전압의 임의 단조함수 형태에 대하여 수행되는 임의 과정을 적용하기 위한 것으로 해석된다. 예측 오류의 신호화는 상당한 장점이 있는 것으로 여겨진다. 단순히 예측에서의 오류에 대응하는 원칙은 뉴런이 [0036]이미 알고 있는 것으로 그 자체를 학습시킬 필요가 없으며, 이미 들은 바가 있는 것을 타겟 뉴런에게 말하는 것은 바람직하지 않다는 것이다. 예측 오류의 장점은 문헌상 학습 강화 및 효율적인 코딩에 있다고 인식되었다.도 1에서 뉴런의 출력은 예측 오류로 표현되는데, 왜냐하면 전압-게이트 칼륨 채널에 의하여 전달된 과거 출력의 기억은 글루타메이트 게이트 채널에 의한 흥분에 반대작용을 하기 때문이다. 하지만, 인공 네트워크의 전형적인 뉴런은 상기들과 같은 비-시냅스 이온 채널이 없었다.상술한 예에서, 글루타메이트 농도는 뉴런의 자극제이다. 뉴런 자극제는 뉴런이 추정하고, 예측하는 세계의 일 [0037]측면 또는 부분으로 본 명세서에서 정의된다. 뉴런은 자연적으로 이것은 생물물리학적 구조 내에서 정보를 담고있으며, 그 정보는 본 명세서에서 정의된 뉴런 자극제로 외부세계의 정보에 관한 것이어야 한다. 아래에서 보다공개특허 10-2010-0085120-10-상세히 설명되는 바와 같이, 뉴런 자극제는 뉴런의 제1 부류 입력의 가중 합에 의하여 정확하게 정의된다. 전형적으로 뉴런 자극제는 거의 연속적인 방법으로 강도가 변화되지만, 원칙적으로는 적은 수의 가능한 상태들 중어느 하나에서만 대신 존재할 수 있다. 전형적인 뉴런은 복수의 개개 글루타메이트 시냅스들로부터의 현재 정보를 받으며, 따라서 이것의 자극제는 이들 시냅스를 가로질러 총합된 국소적인 글루타메이트 농도이다.일반적으로, 뉴런은 복수 종류의 이온채널을 가지며, 이것은 선행 정보를 기여하게 된다. 상기 채널들은 현재 [0038]정보를 기여하는 채널들의 효과를 예측하고 없애는 기능을 수행한다. 선행 정보를 기여하는 채널은 제로 오류(0오류)에 대응하는 중간지점으로 막 전압을 유도하는 방식으로 전도도를 조절하는 경향이 있다. 따라서, 뉴런의선행 정보는 예측 오류를 최소화하는 기능을 수행한다. 이런 원칙은 망막 뉴런의 계산 모델로 예시된다. 공간적으로 국한된 광센서들을 단순히 발현시킴으로써 뉴런은 공간상 작은 영역에서의 현재 광 강도를 추정할 [0039]수 있다. 하지만, 자연적인 조건에서의 광 강도는 공간과 시간에서 인접한 지점들 사이의 강한 양성 상관관계를표시한다. 따라서, 공간 상의 작은 지점(즉, 중간)에서의 현재의 광 강도를 추정하는 대체 방식은 현재의 과거및 공간 상에 이웃하는 지점(즉, 주변)으로부터의 정보만을 이용하는 것이다. 본 발명은, 뉴런이 현재 정보를기여하는 몇몇 입력을 관심 공간으로부터 직접 수신하는 것과, 선행 정보를 기여하는 다른 입력을 공간 및 시간상으로 인접하는 지점들로부터 수신하는 방식이 제안된다. 뉴런은 이들 두 추정값 또는 정보집합들을 비교하며,이들 출력은 두 개 사이의 차이에 대응된다(식 1)(도 2). 차이는 보다 일반적으로 예측 오류로 지칭된다. 뉴런의 출력이 예측 오류에 대응한다는 제안은 순전히 서술적이다. 예측 오류의 개념은 정보 전달의 흐름과 모 [0040]델 뉴런의 기능을 이해하는 데 유용하다. 이 용어의 이유는 아래에서 설명되는 유연성 규칙 관점에서 보다 명확해진다. 하지만, 예측 오류 개념은 뉴런에 의하여 수행되는 과정-이것은 확립된 생물물리학을 통하여 결정되었다- 에 대하여 어떠한 제한을 부여하지 않으며, 이것은 식 1에서 예시되었다. 뉴런에 의하여 보유된 정보에 관해서도 예측 오류는 어떠한 제한을 부여하지 않는다. 뉴런의 예측은 단순히 뉴런이 가질 수 있는 어떠한 정보에대해서도 조건부이다. 원칙적으로, 뉴런 예측은 무한대의 불확실성을 가질 수 있으며, 이것은 완벽한 정보 부재에 해당한다. 하지만, 아래의 유연성 규칙은 보통 뉴런이 관계된 정보를 수집하여, 예측 오류 개념이 뉴런의 계산 기능을 설명하는데 유용한 개념이 되도록 한다. 일 실시예에서, 뉴런 출력은 이것의 막 전압에 대응된다. 반대로, 가장 현실적인 뉴런은 막 전압이 문턱전압을 [0041]넘을 때 시작되는 양자택일(all or nothing) 방식의 거동 전위차(action potentials)를 통하여 다른 뉴런과 소통한다. 이 경우, 짧은 시간 순간에서의 뉴런 출력은 이진수가 된다. 디지털화된 출력은 아날로그 전압 신호에의하여 보유된 정보에 비하여, 정보 잃음을 나타낸다. 하지만, 디지털화된 출력은 보다 먼 거리에서 신뢰성 있는 통신에 있어서는 유리하다. 장거리로 정보를 전달하지 않는 많은 뉴런은 거동 전위차를 방출하지 않고, 등급화된 막 전위차를 사용하는 신호를 방출한다. 바람직한 실시예는 이와 같이 등급화된 전위차 뉴런에 기반한다.하지만, 대신 이진수의 출력으로 뉴런을 자극하는 때에 본 발명의 원칙이 구현될 수 있음은 명백하다.선행 정보 공급원의 선택 [0042]뉴런은 복수의 개별 입력을 수신하며, 이것의 각각은 공간에서의 상이한 지점 또는 과거의 다른 시간기간으로부 [0043]터의 정보이다. 이들 정보들 각각은 정보 공급원으로 기능한다. 어쨌든 뉴런은 이들 정보 공급원 중 몇몇을 선택하며, 나머지는 버린다. 상술한 바와 같이, 뉴런은 2개 클래스의 입력을 수신하며, 이것들을 현재 정보 및 과거 정보를 기여하게 된다. 현재 정보 공급원(정보원)은 뉴런의 예측 오류를 최대화시키도록 선택된다(즉, 막 전압을 과분극과 탈분극 한계치까지 근접하게 한다). 반면, 과거 정보 공급원은 뉴런의 예측 오류를 최소화시키도록 선택된다(즉, 막 전압을 중간값으로 근접하게 한다). 비록 이 결과를 달성시키는 다른 많은 방식이 존재하지만, 본 발명의 바람직한 실시예에서는 뉴런이 Hebbian 또는 anti-Hebbina 유연성을 통하여 이것을 수행한다.선행 정보를 기여하는 입력은 공간 및 시간 상 스펙트럼의 폭을 가질 수 있으며, 뉴런은 자극 강도를 예측하는 [0044]공개특허 10-2010-0085120-11-데 있어서 오류를 가장 최소화시킬 수 있는 입력을 선택하도록 제안된다. 채널이 표현하는 과거(이것의 기억)중 어떤 때는 이것의 동역학적 특성에 의존한다. 가장 간단한 경우, 채널의 게이팅 동역학(gating kinetics)은단일의 기하급수 과정에 의하여 지배되지만, 전형적으로 실제 채널의 게이팅은 상대적으로 복잡한 복수의 기하급수 과정의 상호 작용에 의존한다. 전압 의존성뿐만 아니라, 동역학 특성을 달리하는 많은 종류의 비-시냅스이온 채널이 있으며, 칼륨 채널은 특히 이러한 다양성이 최고이다. 상이한 종류의 채널은 과거의 상이한 기간으로부터의 정보를 운반한다. 하지만, 뉴런은 이들 이온 채널의 하위 집합만을 표현한다. 이들 이온 채널의 하위부분만을 발현함으로써, 뉴런은 과거의 몇몇 기간 중의 기억은 유지하고, 나머지는 유지하지 않도록 선택한다.전압으로 매개된 오류 신호를 통하여 거동하는 뉴런 자극제의 패턴은 자극제 강도를 제일 잘 예측하는 과거의관련 기간, 비-시냅스 이온 채널의 종류를 선택할 수 있다. 유사한 과정이 공간 도메인에서 또한 일어날 수 있는데, 여기에서 비연속적인 시냅스는 비연속적인 공간 지점을 나타낸다. 예를 들면, 상술한 망막의 경우, 개별입력은 (시냅스 이전 뉴런 자극제 또는 수신 영역 중심에 의하여 결정된) 주변 특정 영역에서의 광 강도를 나타내는 저해 시냅스일 수 있다. 중심에서의 자극제 강도를 가장 잘 예측하는 주변으로부터의 이들 시냅스는 증가될 수 있다. 어떤 방식으로 이 과정이 벌어지는 지를 예시하기 위하여, 본 발명자는 선행 정보를 기여하는 입력을 칼륨 채널 [0045]로만 구성된 뉴런을 다시 고려하였다. 이때 동역학적 특징이 다른 구별된 종류의 칼륨 채널들이 있다. 시간상임의 순간에서의 전체 칼륨 전도도(Gk)는 각 성분의 활성 또는 칼륨 채널의 가중 합이다. (2) [0046]성분의 활성(Ui) 은 주어진 순간에서 그 종류의 채널이 개방될 시간- 및 전압-의존적 가능성이다(즉, 평균 채널 [0047]거동의 Hodgkin-Huxley 모델에서의 채널 개방 가능성이다). 성분의 가중치(wi)는 상기 종류의 기능성 채널의 수에 대응된다. 막으로부터 채널들을 삽입하거나 제거함으로써 또는 채널의 기능을 다른 기능으로 바꾸게 할 수있는 인산화반응과 같은 사건에 의하여 가중치는 조절될 수 있다. 오류의 최소화는 오류가 제로가 되는 영역의 중간값으로 막 전위차를 유도한다. 현재의 정보 공급원이 탈분극화 [0048]되는 전형적인 경우, 탈분극된 전위차는 양성 오류에 대응될 수 있으며, 과분급된 전위차는 음성 오류에 대응될수 있다. 만약 막이 탈분극화되는 때에 탈분극-활성화 칼륨 채널이 개방된다면(양성 오류), 비록 뉴런의 선행정보 공급원이 전체적으로 너무 낮다고 추측하였음에도 불구하고, 이것은 글루타메이트 농도가 높다라고 정확하게 추측한다. 따라서, 칼륨 채널의 이런 종류 가중치는 증가되어야 한다(하기 식 3에서 특정된 바와 같이 보다음성화 시킨다). 만약, 칼륨 채널이 개방될 때 막이 과분극된다면(음성 오류), 이런 종류의 채널은 제거되어야하는데, 왜냐하면 이것들은 너무 높게 추측하고, 음성 오류에 기여하였기 때문이다. 만약 칼륨 채널이 폐쇄된다면, 이것은 전압이 얼마인지에 대하여 어떠한 책임이 없으며, 이것의 해당 가중치는 실질적으로 변화되지 않아야 한다. 개방되어, 전도를 수행하는 때에 막을 탈분극하는 종류의 채널 가중치를 조정하는데, 동일 원칙이 적용할 수 있다.이들 원칙은 하기와 같은 학습 공식을 제안한다. [0049](3) [0050]여기에서 개별 성분의 가중치(w)는 시간의 각 순간에서의 활성도(U), 막 전압(V) 및 학습도(α 및 β)에 따라 [0051]업데이트된다. 이 경우, 가중치는 임의 음의 실수일 수 있다. 최종 항(βwt)은 막으로부터 낮은 속도로 제거되는공개특허 10-2010-0085120-12-채널에 대응되며, 이것은 활성도가 실질적으로 막 전위차와 관련되지 않은 채널 종류의 가중치를 0으로 근접시키는 데 유용하다. 항 θ는 범위의 중간에 근사한 전압을 지칭한다. 이것은 오류가 없는 막 전압의 널 포인트(null point)로 기능한다. θ를 넘어선 탈분극화는 가중치를 증가시킬 수 있는 반면, 과분극화는 가중치를 감소시킨다. 식 3은 막 전압에 과분극화 영향을 수행하는 칼륨 채널 등에 적적하다. 하지만, 선행 정보에 기여하는 몇몇 채 [0052]널 종류는 탈분극화 영향을 가질 수 있다(예를 들면 과분극화에 의하여 활성화되는 HCN 양이온 채널). 이런 종류 채널의 가중치는 이들 채널이 뉴런이 과분극화됨과 동시에 이들 채널이 개방되는 때에 증가하는데, 왜냐하면이때 채널이 음성 오류로 반작용할 수 있기 때문이다. 만약 탈분극화하는 입력에서 식 3에서 첫 번째 마이너스부호(왼쪽으로 가장 먼 항)가 플러스 부호로 바뀌게 되면, 이것은 달성될 수 있다.식 3과 같은 유연성 알고리즘은 종종 “anti-Hebbian”으로 지칭된다. Hebbian 규칙은 각각 탈분극 또는 과분극 [0053]과 쌍을 이루는 탈분극 또는 과분극 입력을 강화시키며, 따라서 이것은 양성(positive) 피드백을 포함한다.Anti-Hebbian 규칙은 단지 이것의 반대로서, 음성 피드백을 나타낸다. Anti-Hebbian 유연성은 생물학 및 인공신경 네트워크에서 몇몇 시냅스 연결의 강도를 통제하도록 예전에 제안되었다. 본 발명의 인공 뉴런은 비-시냅스 이온 채널 가운데에서의 선택에까지 이것의 적용을 확장한다. 본 발명의 체제에서 anti-Hebbian 규칙에 대하여 기능적으로 관련된 항은 오류 최소화로 지칭된다. anti-Hebbian 유연성을 활용, 선행 정보 공급원 가운데서 선택함으로써, 상술한 뉴런은 외부 세계의 일부에 대 [0054]한 상태를 예측하도록 학습할 수 있다. 외부 세계의 어떤 부분을 뉴런이 학습하여 예측할 수 있는지는 이것의현재 정보 공급원(이것은 뉴런 자극제를 정의한다)에 의존할 것이다. 뉴런이 이것의 선행 정보 공급원으로부터선택함에 따라, 이것들은 또한 이것의 현재 정보 공급원으로부터 선택한다. 이런 선택 과정은 뉴런의 정보가 중요하고 흥미로운 외부 세계의 일면에 관한 것이다라는 것을 보증하여야 한다. 현재 정보 공급원의 선택 [0055]상술한 뉴런은 어떠한 자극제가 현재 정보를 공급하는지를 학습하여 예측할 수 있다. 하지만, 실제 신경 시스템 [0056]은 미래 보상에 관련된 외부 시계의 측면들에 대해서만 관련된다. 본 명세서에서 사용되는 미래 보상은 주 목적이 미래 보상의 합을 예측하는 강화 학습 용어 사용과 매우 유사하다. 신경 시스템의 일반적 기능은 미래 보상을 예측하는 것이다(즉, 미래 보상에 대한 불확실성을 최소화는 것이다). 미래 보상은 결국 생물학적 적합성 관점 또는 모든 생물들의 목적인 유전 정보를 영속화시키는 동물들의 능력 관점에서 정의된다. 인공 신경 네트워크의 경우, 미래 보상은 (네트워크 설계자에 의하여 정의된 바에 따른) 목적을 달성하는 데 있어서의 네트워크의 미래 성취에 대응될 수 있다. 시스템의 모든 출력은 미래 보상을 높이도록 선택되어야 하기 때문에, 모든 시스템 정보는 미래 보상에 관한 것이어야 한다. 본 명세서에서 사용되는 “보상”이라는 용어는 좋은 결과값만을지칭하는 것이 아니라, 나쁨에서 좋음에 이르는 영역의 값의 측정 또한 지칭한다. 이러한 넓고 포괄적인 미래 보상의 개념은 다소 추상적이고, 무형의 형태이다. 하지만, 강화 학습에서 같이, 시 [0057]스템은 그 자체가 미래 보상을 예측할 수 있는 구체적인 물리학적 자극제를 예측함으로써 미래 보상을예측한다. 이러한 자극제들은 신경 시스템에 의하여 검출될 수 있는 외부 시계의 모든 측면을 포함할 수있으며, 여기에서 자극제는 시스템의 내부 세계 또는 외부 시계의 일부에 해당될 수 있다. 예를 들면, 이것은음식 모습이나 맛과 같은 강한 예측자 뿐만 아니라, 광강도와 같은 미래 보상의 약한 예측자를 일반적으로 포함할 수 있다. 모토 시스템에서의 뉴런에 대하여, 보상-예측적 자극제는 대략적으로 거동에 대한 계획에 대응할수 있다. 따라서, 세상의 현재 상태는 그 자체로 관심 대상이 되는 것이 아니라, 이것이 미래, 보다 구체적으로는 미래 보상에 관한 정보를 제공하기 때문에 관심의 대상이 된다. 공간 및 시간에서의 어떤 지점이 자극제 강도를 예측하는 데 있어서 가장 유용한지를 뉴런이 선택할 수 있는 것 [0058]공개특허 10-2010-0085120-13-과 같이, 이것은 미래 보상에 관하여 가장 유용한 자극제를 또한 선택할 수 있다. 뉴런의 근위 자극제는 시냅스하위 부분을 가로질러 합산된 글루타메이트 농도로 정의된다. 각 비연속적인 시냅스는 현재 정보의 개별 공급원에 해당한다. 뉴런은 이것의 자극제 또는 현재 정보 공급원의 집합을 선택하는데, 이는 몇몇 시냅스에 대하여높은 가중치를 주고, 나머지에 낮은 가중치를 줌으로써 달성된다. 현재 정보 공급원 중에서의 선택에 있어 중요한 두 개의 기준이 있다. 그 중 하나는 활성이 확립된 보상 예측자 [0059](예를 들면 음식)에 대하여 예측적인 개별 입력(예를 들면 시냅스)는 강해야 한다는 것이다. 만약, 뉴런이 스스로 어떤 입력이 보상에 대하여 가장 좋은 예측자인지를 학습하기 위해서, 보상 피드백에 관한 몇몇 형태가 요구된다. 보상 피드백의 일 예는 중뇌 도파민 뉴런에 의하여 신호화되는 보상 예측 오류일 수 있으며, 이것은 시스템의 광범위한 양의 정보를 통합하고, 세상의 현재 상태가 예측치보다 좋은지 또는 나쁜지에 관하여 신호를준다. 보상 피드백의 또 다른 형태는 신피질에서 선택적인 집중을 중재하는 탑-다운 피드백 예측으로부터 얻을수 있다. 보상 피드백 신호는 이 예들보다 다소 정교할 수 있는데, 가장 단순한 경우, 세대간 자연 선택에 의하여 제공될 수 있다. 인공 신경 네트워크에서 보상 피드백은 네트워크의 다른 뉴런 또는 외부 통제 신호에 의하여 제공될 수 있다. 본 발명 모델의 모든 측면에 대하여, 뉴런은 이것이 갖게 되는 어떠한 정보에 의하여 작동한다. 아래에서 설명되는 바와 같이, 보상에 대한 관련성은 뉴런이 이것의 제 1 부류 입력을 선택하는데 뉴런이사용하여야 하는 유일한 기준이 아니며, 모델 뉴런은 반드시 보상 피드백을 요구할 필요는 없다.두 번째 기준은 미래 보상을 가장 잘 예측하기 위하여, 뉴런은 주어진 뉴런의 선행 정보에서 최소 예측 가능성 [0060]의 자극제(즉, 제 1 부류 입력의 집합)를 선택하여야 한다. 이것은 일 매개변수(예를 들면 광 강도) 변동이 커질수록, 다른 매개변수(예를 들면, 물의 유연성)의 변동을 설명할 수 있는 가능성이 커진다는 통계학의 원칙과유사하다. 하지만, 비록 자극제 강도가 큰 변동값을 가지고, 이것이 보상과 연관된다고 하여도, 만약 이것이 상당히 예측 가능하다면 뉴런에 있어서 이것은 유용하지 않은데, 왜냐하면 이것은 단순히 뉴런에게 어떤 뉴런이알고 있는지 만을 알려줄 뿐이기 때문이다. 따라서, 가장 예측 가능하지 않은 입력이 가장 유용한 정보를 준다.따라서, 다른 사항이 동일하다면, 뉴런에게 미래 보상에 관하여 가장 많은 정보를 제공하는 것으로 기대되는 것이 가장 예측 가능하지 않은, 즉 비예측적인 자극제이다.미래 보상에 대하여 예측적이면서, 예측 가능하지 않은 자극제를 뉴런이 선택할 수 있는 일 방식은 상술한 식 3 [0061]과 유사한 학습 규칙을 적용하는 것이지만, 표시 변화에 의하여 가능한 임의의 보상 정보(R)을 포함한다: (4) [0062]이 식에서 가중치(w)는 임의의 양인 실수일 수 있다. 만약 보상에 관한 유일한 피드백이 세대간 자연선택에 의 [0063]하여 제공된다면, R은 유기체 일생 동안 상수일 수 있으며, 이 원칙은 단순히 최소로 예측 가능한 자극제를 선택하는 경향을 보인다. 비록 식 3에서는 나타나지 않았지만, 보상 정보는 또한 선행 정보를 기여하는 입력의 선택을 결정지을 수 있다. 하지만, 비록 식 3에서 보상의 직접적인 영향 없이도, 식 4에서의 보상 영향은 뉴런의선행 정보가 미래 보상에 대하여 예측적이다는 점을 보증한다. 또한, 가중치 변화가 일 순간의 시냅스전 활성과잠시 후의 시냅스후 활성 사이의 연결성에 의존한다면, 이 유연성 규칙(식 4)의 목표는 달성될 수 있으며, 이것은 스파이크-타이밍 의존적 유연성(spike-timing dependent plasticity) 경우와 같다. 식 4는 글루타메이트 시냅스와 같이 탈분극화시키는 전형적인 입력의 경우에 적용된다. 하지만, 만약 과분극화 [0064]시키는 영향을 입력이 갖게 된다면, 도 4의 플러스 부호는 마이너스 부호로 바뀔 수 있다. 이것은, 과분극화와일치하며, 따라서 음 오류에 기여하는 과분화 입력이 커지는 것을 보증한다. 식 4의 유연성 알고리즘은 Hebbian-타입 규칙이며, 이것은 뉴런의 예측 오류를 최대화하는 경향을 보인다. 상기 [0065]공개특허 10-2010-0085120-14-종류의 규칙은 활성이 동시 발생하는 경향의 시냅스를 강화시킨다. 동시 발생하는 활성화는 반복적인 공간 패턴또는 외부 세상의 객체에 의하여 구동되는 시냅스 하위부분에서 보다 종종 발생할 수 있다. 이들 시냅스는 강해질 수 있으며, 이로써 뉴런이 맞추어지는 자극제를 결정짓는다. 비록 Hebbian 유연성이 종종 인공 신경 네트워크에서 활용되지만, 본 발명의 특징적 제안은 오류를 최대화하는 [0066]데 Hebbian 규칙이 기능한다는 것이며, 신경 시스템의 궁극적 목표인 미래 보상을 예측하도록 학습하는 데 왜이것이 유용한지를 제안하는 것이다. 오류-최대화 규칙은 자극제가 뉴런이 기 보유하지 않은 정보를 기여한다는것이다. 예를 들면, 뉴런이 시간적으로 정형화된 순서의 흥분 시냅스 입력을 수신받는다면, Hebbian 규칙은 선택적으로 순서 중 첫 번째 입력을 강화시킬 것이다(왜냐하면 선행 정보는 마지막 자극 입력에 대한 반응을 억제하는 경향을 보이기 때문이다). 순서 중 첫 번째 입력은 가장 많은 정보를 제공하는데, 왜냐하면 이것은 마지막입력을 예측하기 때문이며, 이것은 예상된 마지막 사건에서 적절한 동작 출력을 준비하도록 네트워크를 준비시킬 수 있기 때문이다. 따라서, 오류-최대화 규칙은 외부 환경을 조사하여, 미래 보상에 관하여 가장 최적의 외부 정보 공급원을 식별하는 반면, 오류-최소화 규칙은 이 정보를 가장 잘 포착하고, 보유할 수 있는 내부 기재를 식별하는 것이다. 이것들은 모두 함께 미래 보상에 관한 뉴런의 정보를 최대화시키도록 기능한다.비록 상기 논의의 초점은 공간의 비연속적 지점(시냅스) 또는 과거의 비연속적인 기간(즉, 구분되는 동역학을 [0067]갖는 이온 채널 종류)으로부터의 정보 선택에 관한 것이지만, 이들 원칙은 뉴런의 이온 채널과 이들의 조절 단백질의 다른 측면들의 선택에 사용될 수 있다. 예를 들면, 이온 채널의 장벽이 되는 신경전달물질 수용자는 이들의 리간드 친화도가 상이하거나, 또는 탈감각화 속도가 상이할 수 있다. 하기 자극제에서, Hebbian 규칙을 통하여 뉴런이 노출되는 리간드 농도의 실제 범위에 적당한 리간드 친화도의 하위종류 이온 채널을 선택하였다.유사하게, 전압-게이트 칼륨 채널들은 (비록 유사한 동역학적 특성을 가져도) 전압 의존성을 달리하며, anti-Hebbian 규칙은 막 전압을 가장 잘 예측하고, 반작용하는 것을 선택할 수 있다. 원칙적으로, 입력이 상이할 수있는 크기에 상관 없이, 뉴런에 대한 모든 입력은 이들 두 종류의 유연성 규칙 중 어느 하나에 의하여 선택될수 있다.Hebbian 및 anti-Hebbian 유연성 규칙은 선행 기술 내에서 유연성 알고리즘의 클래스들로 식별되며, 상기 식 3 [0068]및 4에서 주어진 특정 알고리즘은 이들 클래스들의 예들로 제공된다. 뉴런이 학습할 수 있는 것은 명백히 가능한 정보 공급원 집합에 의존한다. 인공 네트워크를 설계할 때, 설계자 [0069]는 각 뉴런에게 어떤 정보 공급원을 사용할지를 결정할 필요가 있을 것이다. 뉴런은 (비록 몇몇 입력은 0의 가중치를 가지지만) 명백히 네트워크의 다른 뉴런 각각으로부터 또는 뉴런의 하위집합만으로부터 입력을 수신할수 있다. 유사하게, 설계자는 뉴런의 비-시냅스 이온 채널의 동역학을 결정하거나, 동등하게 선행 정보의 공급원으로 과거 출력 중 어떤 메모리 궤적을 뉴런이 사용할지를 결정할 필요가 있을 것이다. 본 발명은 뉴런에 사용가능한 정보 공급원으로 어떠한 제한을 두지 않으며, 이것은 네트워크 설계자에 의하여 선택될 수 있다. 예를들면, 실제 이온 채널의 동역학 특성은 인공 뉴런이 사용가능한 메모리 궤적의 종류들을 제한하지 않는다. 더나아가, 설계자는 시냅스 이전 뉴런을 배열하여 단일 시냅스 이후 뉴런에 대하여 두 개의 입력을 제공할 수 있다. 이들 중 하나의 가중치는 Hebbian rule에 의하여 제어될 수 있는 반면, 나머지는 anti-Hebbian 규칙에 의하여 제어될 수 있다. 유연성 규칙은 시냅스 이전 뉴런이 현재 정보 또는 선행 정보를 제공할지, 또는 아무런 정보도 제공하지 않을지를 결정할 수 있다. 기본 원칙은 사용 가능한 어떠한 종류의 정보 공급원을 뉴런이 사용,작동하며, 뉴런이 어떤 정보 공급원을 사용할지는 설계자에 의하여 결정된다. Hebbian 및 anti-Hebbian 유연성의 대안 [0070]본 발명의 주요 원리는 뉴런 입력 중 몇몇은 예측 오류를 최대화하도록 선택되는 반면, 다른 것은 예측 오류를 [0071]최소화하도록 선택되는 것이다. 유연성 규칙은 이러한 목적을 달성하는 유일한 수단이다. 유연성 규칙은 적어도두 개의 특징적인 장점을 갖는다. 첫째, 각 가중치는 국소적으로 사용가능한 정보만을 사용하여 변형된다는 것이다. 따라서, 이들 원칙은 생물논리 시스템을 포함하는 하드웨어에서 실현된다. 둘째, 유연성 규칙은 항상 가공개특허 10-2010-0085120-15-중치를 업데이트하며, 이로써 연속적으로 변화되는 환경에 뉴런이 적응되도록 한다. 이들 특징은 인공 네트워크가 스스로 학습하는 것을 가능하게 하며, 외부 공급원으로부터 이것에 정보를 공급하여야 할 필요가 없다.Hebbian 및 anti-Hebbian 유연성 규칙은 따라서 바람직한 실시예로 활용될 수 있다. 본 발명의 대체예로서, 통계적 추정 방식이 사용되어, 뉴런의 최적 입력 가중치를 결정한다. 선형 평균 제곱 추 [0072]정이 사용되어, 오류를 최소화하는 선행 정보의 최적 공급원을 결정할 수 있다. 예를 들면, 이 기술을 사용하여광 강도가 중심 영역에서 현재 빛에 대한 가장 효과적인 예측자(predictor)인 과거의 기간 및 주변 공간의 영역을 선택할 수 있다. 이 기술은 오류의 제곱을 최소화하도록 설계되지만, 유사한 통계학적 기술이 사용되어 오류의 제곱을 최대화하도록 제 1 부류 입력의 가중치(즉, 현재 정보 공급원)를 선택할 수 있다. 각 가중치는 공간및 시간상의 많은 지점에서의 모든 다른 가중치 및 강도에 관한 정보를 사용하여 각 가중치가 결정될 수 있기때문에, 이런 통계학적 방법 또는 다른 통계학적 방법은 잠재적으로 유연성 규칙에 의하여 얻어진 가중치 집합보다 더 최적에 가까운 가중치 집합을 선택할 수 있다. 하지만, 이러한 통계학적 방법은 국부적인 정보에만 의존적이지 않기 때문에, 이러한 접근은 네트워크로의 전문가 정보의 제공이 필요하다.또 다른 실시예에서, 네트워크의 설계자는 본 명세서에서 설명된 원칙에 따라 뉴런의 입력을 선택할 수 있으나, [0073]유연성 규칙 및 통계학적 기술의 조력 없이 할 수 있다. 예를 들면, 만약 뉴런 자극제가 물체의 위치인 경우,우리는 물체가 질량을 가지며 따라서 관성 또한 갖는다는 알 수 있다. 이러한 간단한 지식에만 기초하여, 만약선행 정보의 단일 공급원이 앞선 순간에서의 자극제 위치로부터 유도된다면 뉴런은 예측 오류를 최소화할 수 있는데, 왜냐하면 가장 최근 과거는 언제나 더 먼 과거보다는 좋은 예측자가 되기 때문이다. 마지막 순간의 자극제 위치와 위치가 상이할 때 이런 뉴런은 흥분되거나, 저해될 것이다. 유연성 규칙 또는 통계학적 방법은 동일결과를 가질 수 있지만, 이런 경우에는 불필요하게 간주될 수 있다. 따라서, 뉴런의 예측 오류를 최소화 또는최대화하는 원칙은 사용 가능한 지식에 기초하여 네트워크 설계자에 의하여 직접 실현될 수 있다. 도 2에 예시된 실시예의 요약 [0074]도 2는 바람직한 실시예의 단일 뉴런에 대한 입력을 나타낸다. 각 화살표는 개별 입력(즉, 이온 채널의 구별되 [0075]는 총수)을 나타내며, 대응되는 화살표는 정보 흐름의 방향을 나타낸다. 각 입력은 흥분성(즉, 탈분극)이거나저해성(즉, 과분극)일 수 있다. 개별 입력의 수는 본 발명에 의하여 제한되지 않으며, 본 명세서에서 도시된 수는 예시적인 목적으로 선택된 것이다. 각 개별 입력은 변형 가능한 가중치와 연관되며, 이는 뉴런의 막 전압에대한 기여도의 강도를 나타낸다. 막 전압 200은 예측 오류에 해당하며, 뉴런의 출력으로 기능한다. 각 개별 입력은 예측 오류를 최대화 또는 최소화시키기 위하여 가중치가 결정되었는지에 따라 꼬리표가 부가된다. 제 1 부류 입력(클래스 1, 제 1 클래스) 210와 연관된 가중치는 예측 오류를 최대화하기 위하여 선택된다. 본 [0076]명세서에서 이들 입력은 현재 정보를 기여하는 것으로 설명되며, 바람직한 실시예에서 이들의 가중치는 Hebbian유연성에 의하여 결정된다. 제 2 부류 입력(클래스 2, 제 2 클래스) 220 및 230은 예측 오류를 최소화하기 위하여 선택된다. 본 명세서에서 이들 입력은 선행 정보를 기여하는 것으로 설명되며, 바람직한 실시예에서 이들의가중치는 anti-Hebbian 규칙에 의하여 결정된다. 선행 공간 정보 220 및 현재 정보를 기여하는 입력 210은 네트워크의 다른 뉴런 또는 네트워크 외부의 공급원 [0077](즉, 시냅스 입력)으로부터 온다. 반대로, 선행 시간 정보를 기여하는 입력 230은 동일 뉴런의 막 전압에 의하여 조절되는 이온 채널로부터 온다. 이들 입력의 활성 또는 전도도는 과거 막 전압 및 현재 막 전압의 영향력에의존한다. 따라서 막 전압 200과 이들 입력 230의 활성 또는 전도도 사이에서 양 방향으로 정보가 흐른다. 선행시간 정보를 기여하는 각 개별 입력은 구별되는 동역학 특성을 갖는 채널 종류에 대응된다. 채널의 동역학 특성은 이것이 기억하는 과거의 어떤 기간을 결정하고, 따라서, 구별되는 채널들은 구별되는 선행 시간 정보(즉, 기억)를 기여한다. 일 실시예에서 뉴런은 생물학적 뉴런에 적용되는 기존의 생물리학적 원칙에 따라 입력을 통합한다. 공개특허 10-2010-0085120-16-네트워크 [0078]상술한 설명은 단일 뉴런의 입력에 관한 것이다. 단일 뉴런은 그 스스로가 몇몇 활용도를 가질 수 있으나, 뉴런 [0079]들의 네트워크에서 정보 처리 요소로 기능하도록 의도된다. 일 실시예에서 각 뉴런은 네트워크에서의 공간위치, 출력 표시(즉, 흥분성 또는 저해성), 및 임의 선택된 입력 가중치만이 초기에 상이하다. 가중치의 변경을통하여 네트워크는 이것의 목적을 달성하도록 학습할 수 있다.미래 보상의 예측은 신경 시스템의 중심 기능이며, 상술한 뉴런은 이러한 기능을 수행한다. 단일 뉴런은 세상의 [0080]작은 부분에 대한 상태만을 예측하며, 상기 부분은 미래 보상과 밀접하게 연관되지 않을 수 있다. 하지만, 각개별 뉴런이 이러한 중심 기능을 수행한다면, 뉴런 시스템은 통합되어 미래 보상을 보다 잘 예측하도록 작동한다. 각 뉴런은 적어도 생물학적 특성이 대략적으로 유사하므로, 각 뉴런은 자극제에 관하여 유사한 양의 정보를보유한다. 하지만, 몇몇 자극제는 미래 보상에 대하여 많은 정보를 주지만(예를 들면 음식 모습), 반면 다른 것들은 미래 보상과는 약하게 연관된다(예를 들면, 광 강도). 따라서, 뉴런들은 미래 보상에 대하여 얼마나 많은정보를 갖느냐에 있어서 상이하며, 이것은 네트워크 수준에서는 중요한 변수가 된다. 보상 피드백은 식 4를 통하여 각 뉴런 자극제의 선택에 기여하므로, 네트워크의 감각 입력으로부터 처리되는 일 [0081]련의 연속하는 각 뉴런의 자극제는 미래 보상과 보다 밀접하게 연관되고, 즉각적인 감각 세계와는 보다 덜 밀접하게 연관된다. 더 나아가 감각 주변부로부터의 뉴런은 더 많은 정보를 가지며, 미래 보상에 대한 불확실성을덜 갖는다. 이러한 현상은 막막으로부터 피질을 통하여 운동 뉴런에 이르는 긴 경로를 추적함으로써 예시될 수있다. 시각 시스템에서 연속하는 뉴런의 자극제는 광 강도의 작은 원으로부터 배향된 바로 변환되며, 마침내 얼굴로 변환된다. 양상에 상관없이 측두엽 및 전두엽 피질에서 보다 높은 뉴런은 관련된 정보에 반응한다. 이 경로에서 더 나아가, 뉴런은 미래 보상에 대하여 보다 선택적이지만, 특정 수족 및 근육에 특별히 관련된 자기수용성 전정(vestibular)정보를 통합시킴으로써 뉴론들은 보다 운동성을 가질 수 있다. 경로의 최종 뉴런으로, 운동 뉴런에 의하여 예측된 자극제는 대략적으로 행동 계획에 대응될 것이다. 모든 업스트림 시냅스에서 보상 피드백의 누적 효과는 운동 뉴런의 자극제를 결정하기 때문에, 운동 뉴런은 업스트림 뉴런의 어떤 것보다 미래 보상에 관하여 낮은 불확실성을 갖는다. 유사하게, 운동 뉴런은 시스템의 결정을 만들게 된다. 뉴런 및 이것의 자극제에 의하여 보유된 보상 정보의 양은 또한 감각 양상에 따라 상이할 수 있다. 예를 들면, [0082]맛은 광도 보다는 미래 보상과 더 강하게 관련되어 있기 때문에, 혀의 미각세포는 광수용체 보다 더 많은 정보와 더 적은 미래 보상에 대한 불확실성을 가진다. 마찬가지로, 감각세포는 광수용체 보다는 (즉, 더 적은 시냅스에 의해 분리된) 운동 뉴런에 더 가깝다. 감각 경로와 시각 경로가 모두 동일한 운동 뉴런에 모이면, 미래 보상에 대한 불확실성에 동일한 감소를 야기한다. 그러나, 광도는 미래 보상의 더 적은 예측자이기 때문에, 긴 경로의 시각계는 보상 불확실성에 동일한 감소를 야기시키기 위해서 짧은 경로의 감각계 보다 더 많은 일을 해야한다. 그러나, 광도로부터 보상 정보를 추출하기 위한 실제적인 일을 함에 있어서, 긴 경로의 시각계는 대체로세계에 대한 불확실성에 훨씬 더 큰 감소를 달성한다. 지능과 유용한 출력을 생성하도록 학습하는 이들 뉴런의 네트워크 능력은 대체로 하나 이상의 보상 피드백 신호 [0083]의 존재에 의존한다. 일 구체예에서, 보상 피드백의 적합한 형태는 한정되어 있지 않다. 오히려 인공뉴런은 네트워크의 정보가 그것의 목적과 관련되도록 보상 피드백을 이용한다. 보상 피드백이 없는 경우, 이들 뉴런의 네트워크는 예측하기 어려운 세계의 측면의 상태를 예측하도록 학습한다. 예를 들면, 이러한 네트워크는 시각 세계를 보고 이해하고 예측하도록 학습할 수 있다. 이것은 물체를 인식하도록 학습할 수 있지만, 생체계와 다르게다른 것들보다 더 중요한 어떤 물체를 보지는 못한다. 이러한 네트워크는 외부 관찰자에게 정보를 제공하므로유용하지만, 생체계를 닮은 목적-관련 운동 출력을 생성하기에는 적합하지 않다. 이들 뉴런의 네트워크를 설계하는데 중요한 이슈는 보상 피드백의 형태에 관한 것이다. '보상 피드백'은 네트워 [0084]크의 목적이 얼마나 잘 달성되는지에 대한 정보를 말한다. 식 4에서 나타낸 바와 같이, Hebbian 유연성에 대한공개특허 10-2010-0085120-17-그것의 영향을 통하여, 이 보상 피드백은 뉴런과 네트워크가 획득하는 정보를 결정할 것이다. 보상 피드백은 모든 네트워크의 정보가 네트워크의 목적과 관련된다는 것을 확인시키도록 돕는다. 상기에 몇 가지 예가 제공되었지만, 보상 피드백의 형태는 이들에 한정되지 않는다. 그것은 단순히 흥분성 또는 억제성 보다는 신경전달물질도파민과 유사한 조절 효과를 가진 네트워크 내에 특별한 뉴런으로부터 비롯될 수 있다. 그렇지 않으면, 그것은네트워크의 외부 공급원으로부터 제공될 수 있는데, 이 경우 네트워크는 지도 방식으로 학습한다고 한다. 따라서, 보상 피드백에는 상이한 많은 구체예가 있다. 이들 뉴런의 중요한 측면은 네트워크의 목적과 가장 관련된세계의 측면을 예측하기 위하여 무슨 보상 피드백이 제공되든지 간에 이를 이용할 수 있다는 것이다. 생물학적 신경 네트워크는 동물의 목적에 잠재적 관련성이 있는 세계의 측면들만을 고려한다. 그러나, 어떤 특 [0085]정 행동의 목적의 부재시 세계의 상태를 예측하는 것에 관심이 있는 인공 네트워크를 상상할 수 있다(즉, 이해만을 위한 이해). 따라서, 일 구체예에서, 아무런 보상 피드백이 제공되지 않는다. 이러한 네트워크가 어떻게작동할 수 있는지를 상상하기 위해서, 유용한 비유가 시각계에 적용될 수 있다. 시각계의 말기는 보상 관련 정보를 위하여 강하게 편향된다. 보상 피드백의 영향 하에서, 시각계는 대체로 바위와 같은 물체를 무시하면서얼굴의 상세한 공정을 수행하도록 학습한다. 그러나 초기 시각계는 보상과의 관련성과 상관없이 모든 종류의 물체를 인식 및 구별하는 것을 학습한다. 보상 피드백의 부재시에 학습하는 이들 뉴런의 네트워크는 비슷하게 물체를 인식 및 구별하도록 예상된다. 예를 들면, Hebbian 유연성은 뉴런이 명암의 연장된 부위에 선택적이 되도록 야기할 수 있다고 알려져 있다. 따라서, Hebbian 유연성은 일차 시각령(visual cortex)의 단순 세포의 자극선택성을 설명할 수 있다. 따라서, 보상 피드백이 결핍된 네트워크는 보는 것을 학습할 수 있다(즉, 시각 세계의 상태를 예측). 시뮬레이션 [0086]본 발명의 일 구체예에서, 하나 이상의 네트워크는 컴퓨터를 이용하여 시뮬레이션된다. 이러한 시뮬레이션의 결 [0087]과는 본 발명에서 제공된다. 종래 연구들은 Hebbian 또는 anti-Hebbian 시냅틱 유연성 규칙이 어떻게 네트워크를 형성할 수 있는지를 증명하 [0088]였다(비록 이들 법칙의 제시된 조합이 시뮬레이션되지는 않았지만). 본 발명의 구체예는 이와 동일한 종류의 유연성 규칙을 비-시냅스 이온 채널의 선택성에 적용하도록 확대시킨다. 이 시뮬레이션에서, 비-시냅스 이온 채널의 스펙트럼 중에서 선택된 단일 뉴런이 도 3A-H에 도시되어 있다.시뮬레이션은 단일 구획, 등급 전위(즉, 비 활동전위), Hodgkin-Huxley 타입 모델 뉴런에 기초한다. 뉴런은 [0089]Hebbian 법칙(식 4)을 통하여 글루타메이트-게이트 양이온 채널의 4가지 서브타입과, anti-Hebbian 법칙(식 3)을 통하여 전압-조절 칼륨 채널의 9개 서브타입 중에서 동시적으로 선택된다. 채널 활성(즉, 개발 가능성, 식2-4에서 “U”)은 도 3C, 3E, 및 3G에 도시된 바와 같이 0 과 1 사이에서 연속적으로 다양하다. 각 유형의 채널수(식 2-4에서 “W”)는 임의의 양의 실수를 가정할 수 있다. 각 채널의 단위 전도도는 1이었다. 식 3 및 4의 유연성 규칙을 위하여, θ가 칼륨과 비선택 양이온 역전위 사이에 중간인 -50 mV가 되도록 선택하 [0090]였다. 속도상수는 식 3의 anti-Hebbian 규칙과 식 4의 Hebbian 규칙에서 각각 0.1과 1.0이었다. 각 규칙에서,b는 0.0000001이었다. 글루타메이트 농도의 특정 패턴 외에, 유연성 속도는 시뮬레이션에서 유일한 자유 매개변수였다. 다른 매개변수는 채널 서브타입의 경우에, 알려진 물리적 수치의 근사치를 계산하거나, 관련 동역학 또는 글루타메이트 친화도의 스펙트럼을 포괄하도록 선택하였다. Hebbian 규칙에 대한 anti-Hebbian 규칙에서 a의더 큰 값은 Hebbian 규칙의 양성 피드백에서 기인한 글루타메이트-게이트 채널의 수의 증가를 제한하도록 선택하였다(anti-Hebbian 규칙의 음성 피드백을 더 빠르게 함으로써). “수동” 붕괴속도(b)는 “능동” 유연성 속도(a) 보다 더 작게 선택하였으며, 결과적으로 채널 활성의 변화 속도와 비교하여 느리도록 선택하였다. 공개특허 10-2010-0085120-18-각 단계(time step)에서, 글루타메이트 농도는 도 3A에 도시된 바와 같이 평균 20%의 표준편차로 가우스 분포 [0091](Gaussian distribution)에서 유도하였다. 평균 농도는 2000에서 시작한 10 단계와 2200에서 시작한 500 단계에서 50 μM 에서 1000 μM으로 증가하였다. 5000 단계 후, 패턴을 총 20,000 사이클 반복하였다. 초기에는, 4개의 서브타입 중에 균등하게 분리된 총 800 글루타메이트-게이트 비선택 양이온 채널 과, 9개의 서브타입 중에균등하게 분리된 800 전압-게이트 K+ 채널이 있었다. 상이한 수와 비율의 채널로 2가지 다른 시뮬레이션을 시작하였다(나타내지 않음). 각 서브타입의 채널의 최종 수는 시작 숫자와 상관없이 세가지 시뮬레이션 모두에서 동일하였다. 4가지 타입의 글루타메이트-게이트 채널은 글루타메이트에 대한 친화도(KD)가 상이했다. 이것은 단일 사이클에서 [0092]각 채널 타입의 활성(즉, 개방 가능성)을 나타내는 도 3C에서 확인할 수 있다. 중간 친화도의 서브타입은 뉴런이 노출된 실제 범위의 글루타메이트 농도에 가장 민감하였다. 이들 활성은 더 다양하고 덜 예측가능하며, 따라서 이들 중 하나(KD = 1000 μM)가 식 4의 Hebbian 유연성 규칙에 의해 선택된 우세한 서브타입이었다. 도 3D는각 타입의 글루타메이트-게이트 채널의 수(즉, 식 4에서 가중치 w)를 사이클 수의 함수로 나타내며, 각 사이클의 값은 사이클에서 모든 5000 시점에 걸친 평균 가중치이다. Hebbian 유연성 규칙은 가장 유리한 채널 타입,전압의 예측 오류 또는 편차를 최대화하고, 다른 것들을 없애는 것들을 선택하였다. 학습이 진행되고 더 높고더 낮은 친화성 수용체가 제거되면서(도 3D), 뉴런의 막 전위는 글루타메이트 농도에 더 민감하게 되었고, 따라서 보다 가변적이었다. 이는 도 3B에 도시되어 있는데, 마지막 사이클 동안 막 전압을 나타내는 하위 자취(trace)가 첫 번째 사이클 동안 막 전압을 나타내는 상위 자취에 비해서 매우 가변적이다. 뉴런의 과거 정보(즉, 제2부류 입력)는 9가지 서브타입의 칼륨 채널을 포함하였다. 이들 중 4가지는 타입 1 채 [0093]널이고, 5가지는 타입 2 채널이다. 타입 1 채널은 평형시 -40mV에서 최대의 반(half-maximal)의 활성화를 가진단일 2 상태 전압 센서에 의해 게이트되며, 따라서 각 순간에 이들의 활성은 단순히 과거 전압의 지수함수였다.마지막 사이클 동안 각 타입의 채널의 활성은 도 3E에 도시되어 있다. 4가지 서브타입은 10 내지 333 시간 단위의 -40 mV에서의 시간 상수 t를 비롯해서 동역학 특성이 상이했다. 첫번째 약 1000 사이클 동안, 각 타입의 칼륨 채널의 수는 식 3에서 막 전압이 θ (-50 mV) 이상으로 거의 항상 탈분극되었기 때문에 그것의 시작 값 89로부터 증가하였다. 이것은 총 칼륨 전도도가 막전압을 평균 오류가 0인 정지점(null point) θ가까이 이르게 하는데 충분할 때가지 계속되었다. 식 3의 anti-Hebbian 규칙은 도 3F에 도시된 바와 같이 가장 빠른 동역학을 가진(τ = 10 시점) 채널 타입을 선택하였다. 이 채널은 막 전압의 예측뿐만 아니라 글루타메이트 농도 예측시에가장 최근의 과거가 더 먼 과거 보다 더 낫기 때문에 적합하였다. 이것은막 시간 상수에 의해 도입된 막 전압에서 작은 범위의 상관관계와, 평균 글루타메이트 농도의 단계 변화 때문에 [0094]사실이다. 가장 빠른 동역학의 채널은 예측을 더 빠르게 적응할 수 있으며, 따라서 오류를 최소화할 수 있다.실제로, 많은 실제 전압-게이트 칼륨 채널은 빠른 동역학을 가진다. 그러나, 가장 최근의 과거가 항상 현재의가장 좋은 예측자인 것은 아니다. 마찬가지로, 글루타메이트 농도가 가우스 변이(Gaussian variation)를 나타냈지만 다른 일시적 패턴이 결핍되었을 때, 채널은 가장 긴 기간 동안의 과거 전압을 평균화함으로써 가장 좋은예측을 하기 때문에 anti-Hebbian 규칙은 가장 느린 동역학을 가진 채널 타입을 선택하였다(나타내지 않음).타입 1 칼륨 채널은 글루타메이트의 두 번째 펄스가 첫 번째 펄스에 발생했을 때 예측할 수 없었지만, 원칙적으 [0095]로 일부 타입의 채널은 예측할 수 있다. 타입 2 칼륨 채널은 이 목적을 위하여 설계되었다. 타입 2 칼륨 채널의각 5가지의 서브타입은 전술한 바와 같이 8개의 센서에 의해 게이트되었다. 상기 서브타입은 도 3G에 도신된바와 같이 동역학 특성이 상이했다. 복수 센서에 의한 각 타입 2 채널의 게이팅(gating)은 단일 센서 타입 1 채널에 비해서 훨씬 더 실제 채널과 같았다. 막 전압이 -25mV 이상으로 탈분극된 후 임의의 기간동안 채널이 열리도록 채널 게이팅 및 센서 동역학의 규칙을 선택하였다. 거의 첫번째 2000 사이클 동안, 이 역치에는 이르지 않았고, 따라서 타입 2 채널의 각 서브타입 수는 식 3의 수동 붕괴 구간 때문에 감소하였다(도 3H). 결국, 막 저압은 더 민감한 글루타메이트-게이트 이온 채널의 선택 때문에 글루타메이트 농도에 더 민감하게 됨에 따라, 글루타메이트의 첫 번째 펄스는 -25mV 이상으로 막을 탈분극시키기에 충분하게 되어 타입 2 채널을 활성화시켰다.그 이후, anti-Hebbian 규칙은 글루타메이트의 두 펄스 간의 실제 간격(t =100)을 거의 일치시킨 동역학을 가진채널 서브타입을 선택하였다(도 3H). 이 서브타입은 글루타메이트의 두 번째 펄스 전에 간단한 과분극(즉, 음성공개특허 10-2010-0085120-19-오류)을 야기함에도 불구하고, 글루타메이트-유래 탈분극(즉, 양성 오류)을 대응시키기 때문에 선택되었다(도3B, 하위 자취). 따라서 이 타입의 채널은 글루타메이트의 첫 번째 펄스를 이용하여 글루타메이트의 두 번째 펄스에 의해 야기된 과분극을 예측 및 대응시킬 수 잇다. 뉴런의 함수는 그것에 사용된 정보 공급원(즉, 채널 타입 또는 시냅스)에 의존한다. 이 실시예는 단지 설명하기 [0096]위해서 제공되는 것이며, 뉴런에 사용가능한 정보 공급원의 종류를 제한하는 것을 의미하지는 않는다. 컴퓨터 판독가능 매체 [0097]일 구체예에서, 인공뉴런은 의뢰인(400)에 저장된 소프트웨어 프로그램을 이용하여 생성된다. 도 4는 인공뉴런 [0098]의 네트워크를 생성하기 위한 예시적 구성을 설명하는 간단한 도표이다. 예시적 구성은 의뢰인(400)(예를 들면,컴퓨터와 같이 의뢰인 장치로서 역할을 하도록 설정된 컴퓨팅 플랫폼), 디지털 미디어 플레이어, 개인용 디지털보조자, 또는 RAM(random access memory) 또는 전자 프로세서(420)에 결합된 자기 또는 광학 매체와 같이 컴퓨터-판독가능 매체(410)을 포함하도록 설정된 휴대폰이다. 프로세서(420)는 컴퓨터-판독가능 매체(200)에 저장된프로그램 지시를 실행한다. 본 기술분야의 통상의 지식을 지닌 당업자에게 이해되는 바와 같이, 본 발명은 그것의 사상 또는 필수적 특징을 [0099]벗어나지 않고 다른 특정 형태로 구현될 수 있다. 마찬가지로, 구성, 특징, 속성 및 다른 양태의 특정 명명 및분류는 필수적이거나 중요하지 않으며, 본 발명을 수행하는 메커니즘 또는 그것의 특징은 다른 이름, 분류 및/또는 형식을 가질 수 있다. 따라서, 본 발명의 개시는 하기 청구범위에 나타난 본 발명의 범위를 한정하려는 것이 아니라 설명하기 위한 것이다.도면도면1a공개특허 10-2010-0085120-20-도면1b도면1c도면1d공개특허 10-2010-0085120-21-도면2도면3a공개특허 10-2010-0085120-22-도면3b도면3c공개특허 10-2010-0085120-23-도면3d도면3e공개특허 10-2010-0085120-24-도면3f도면3g공개특허 10-2010-0085120-25-도면3h도면4공개특허 10-2010-0085120-26-"}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "인공 지능은 현재 및 과거의 정보를 통합하고, 그 각각은 세계의 부분의 상태를 예측한다. 뉴런의 출력은 2가지 예측자 간의 차이 또는 예측 오류에 상응한다. 과거 정보에 기여하는 입력은 오류를 최소화하기 위해서 선택되며, 이는 anti-Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 현재의 정보 공급원은 오류를 최대화하 기 위해 선택되며, 이는 Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 이것은 뉴런이 뉴런이 이미 보유하고 있는 과거의 정보와 중복되지 않은 외부 세계로부터 새로운 정보를 수신한다는 것을 확인시켜준다. 예측하기 위 해 스스로 학습함으로써, 뉴런 또는 이들 뉴런의 네트워크는 지적이고 유리한 출력을 생성하는데 필요한 정보를 획득한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 일반적으로 신경 네트워크 모델 및 인공 지능에 관한 것이다. 보다 구체적으로 본 발명은 단일 인공 뉴런의 계산 기능 모델에 관한 것이다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간 또는 다른 동물의 지능을 나타내는 인공 시스템의 개발은 상당 기간 많은 관심을 받아 왔다. 이러한 시도 중 하나는 생물학적 신경 시스템 중 공지된 요소를 결합시키는 계산 시스템을 개발하는 것이다. 이러한 시도로 부터 인공 신경 네트워크가 개발되었다. 인공 신경 네트워크는 다중의 상호 연결된 뉴런으로 구성되고, 각 뉴런 의 출력 신호는 입력 신호에 의존한다. 상기 네트워크 전체로서 외부 입력을 받아 출력 신호를 생성시킨다. 상 기 시스템의 목적은 뉴런 간의 연결 가중치를 조정함으로써 각 입력 패턴에 대하여 적절한 출력 신호를 생성시 키도록 학습시키는 것이다. 연결 가중치의 조정은 출력 신호의 적절성에 대한 일종의 피드백 영향으로부터 발생할 수 있다. 신경 네트워크는 유용한 지능을 생성시키는 데 있어서 일정한 성과를 이루었다. 1950년대부터 연구되어 왔음에 도 불구하고, 생물학적 신경 시스템에 의하여 생성된 수준의 인공 지능을 이끌어내는 중요 시도들은 실패하여 왔다. 유사하게, 인공 네트워크의 뉴런은 생물학적 뉴런과 제한된 유사성만을 가지고 있다. 새물학적 신경 시스템은 그 자체들을 학습하는 능력(즉, 자기 조직화)을 갖는다. 신경 네트워크에 관한 연구의 주 목적은 동일한 능력을 갖는 인공 네트워크를 생성시키는 것이다. 이상적으로 네트워크는 외부 세상에 대한 정보를 실질적으로 가지지 않고, 스스로 반복하여 지식을 갖게 되는 것이다. 이러한 목표가 달성될 수 있을지, 없을지 여부와 관계없이, 실질적인 목적은 출발에서는 가능한 적은 정보를 가졌음에도 불구하고, 학습할 수 있 는 대상이 보편적인 네트워크를 개발하는 것이다. 신경 네트워크는 일반적으로 특정 문제를 해결하는 방법에 대하여, 상당한 지식을 사용하도록 설계되었다. 네트 워크에게 수행 오류를 주로 전달하는 외부생성 감시 신호의 영향에서 훈련된 네트워크의 경우 이는 더욱 명백하 다. 따라서, 네트워크에서는 전문가 정보가 공급된다. 반면, 생물학적 신경 시스템은 시도 및 오류(trial and error) 방식으로 스스로 학습하여야 한다. 유사하게, 몇몇 인공 네트워크는 감시되지 않는 학습을 수행하며, 이 것은 외부 감시 신호에 의존하지 않는 시스템이다. 하지만, 이들 네트워크는 일반적으로 특정 입출력 전환을 수 행하도록 설계되었다. 따라서, 설계자는 어떻게 출력 신호가 입력에 의존하는 지에 대한 지식을 가지고 상기 네 트워크를 설계하여야 한다. 적정한 입-출력 전환은 다른 임무 또는 환경에서 달라지기 때문에, 하나의 환경에서 설계된 네트워크는 새로운 환경에 적응하기 어려울 수 있다. 반대로, 생물학 발견자는 그 자체로 어떻게 출력이 입력에 의존하는지를 발견하고, 생물학적 신경 시스템은 새로운 환경에 상당 수준으로 적응할 수 있다. 본 발명 의 사상은 생물학적 뉴런의 계산 모델에 기초한 것이다. 네트워크의 목적은 적절한 출력값을 선택하는 것이고, 이 과정은 “의사결정”이라 지칭될 수 있다. 의사 결정 에 있어서의 문제는 외부의 현재 및 외부 상태에 대한 불확실성 때문이다. 예측에서의 불확실성은 역으로 예측 과정이 기초한 정보의 양에 의존하며, 따라서, 불확실성을 최소화하는 것은 정보를 최대화시키는 것과 동일하다. 만약, 상기 시스템이 정확하게 외부 세상의 상태를 예측한다면, 문제는 해결될 것이고, 시스템은 가 장 유리한 것으로 알고 있는 출력값을 단순히 선택하면 된다(예를 들면, 미래의 보상 값을 최대화하는 것을 기 대되는 출력 신호의 선택). 따라서 불확실성을 최소화시키는 과정은 의사결정 과정과 형식적으로 동일한 것으로 판단되는데, 왜냐하면, 결정은 불확실성이 없는 상태에서는 사소한 문제일 뿐이기 때문이다. 본 명세서에서 “정보”라는 용어는 네트워크 또는 뉴런이 생물리학적 구조 내에 저장 또는 담고 있는 정보를 의미한다. 뉴런 막에서의 이온 채널과 같은 단일 분자조차 정보를 담고 있다. 정의한 바와 같이, 정보는 무엇인 가를 예측한다. 따라서, 뉴런에 의하여 수행된 예측은 전체적으로 뉴런에 의하여 처리된 정보에 의존한다. 본 발명에서 관련 정보는 네트워크의 관찰자가 네트워크 또는 세상에 대하여 가지고 있는 것이 아니다. 본 발명은 단일 뉴런의 계산 기능에 대한 학습을 강화시키는 근본 원칙의 적용이라 볼 수 있다. 학습 강화는 어 떻게 생물학 또는 인공적인 구성요소(agent)가 스스로의 목적을 달성할 수 있는지에 연관된다. 구성요소의 모든 출력은 미래의 목표를 달성하기 위하여 선택되어야 하므로, 구성요소는 “장래의 보상 총합”을 예측하도록 학 습하여야 한다. 비록 장래 보상의 개념이 다소 추상적이고, 무형의 형태로 보이나, 그 자체가 장래 보상을 예측 할 수 있는 구체적인 동기와 사건들을 예측함으로써, 구성요소는 장래 보상을 예측한다. 따라서, 구성요소는 감 각 세계의 현재 상태를 예측하도록 노력하는데, 왜냐하면 감각 세계는 장래 보상에 있어서 유용한 정보를 기간 때문이다. 예를 들면, 음식 모습은 음식 소화를 예측하게 하며, 따라서 구성요소는 음식 모습을 예측할 수 있기 를 원하게 된다. 이러한 예측들을 학습하기 위하여, 구성요소는 예측 오류에 의존한다. 예측 오류는 기대되는 것과 실제 관찰되 는 것 사이의 차이이다. 본 명세서에서 기재되는 인공 뉴런은 일종의 예측 오류를 일으키며, 인공 뉴런은 이것 을 이용하여, 장래 보상을 상당 수준으로 예측할 수 있는 세계의 일 측면을 예측하도록 학습한다. 반면, 학습강 화에 대한 대부분의 과거 연구는 네트워크 및 시스템 수준에서의 학습 강화 원칙을 실행하였으나, 본 발명은 이 들 원칙을 단일 뉴런의 기능에 적용하고, 이를 뉴런 네트워크까지 확장하였다. 인공 뉴런 네트워크는 다중 상호연결 뉴런으로 구성된다. 각 뉴런의 출력은 입력의 가중 합에 의하여 결정된다. 각 입력은 정보원으로 기능하며, 원칙적으로 뉴런은 무한대의 입력수를 사실상 가질 수 있다. 하지만, 뉴런 출 력은 충분히 높은 가중치를 갖는 입력에만 실질적으로 의존한다. 인공 네트워크를 창출하는 것은 따라서 적정한 가중치를 선택하는 문제라 볼 수 있다. 이런 문제에 대한 50년간의 연구에도 불구하고, 일반적이고, 보편적으로 적용가능하며, 뉴런 입력의 각각에 대하여 할당되어야 하는 가중치를 특정할 수 있는 일련의 가이드라인이 없었 다. 본 명세서에서, 새로운 방식에 따라 뉴런 입력의 가중치를 선택할 수 있다. 몇몇 입력은 뉴런의 관점에서 예측할 수 없다면 높은 가중치가 주어지며, 반면에 몇몇은 이들이 효과적인 예측자라면 높은 가중치가 주어진다. 비록 이들 새로운 원리가 실행될 수 있는 복수의 방식이 있지만, 가장 효과적인 구체예는 유연성 알고리즘 (plasticity algorithm)을 사용하는 것이다. 실제 뉴런들은 시냅틱 앞 부분(프리-시냅틱)으로부터 시냅틱 뒤 부 분(포스트-시탭틱)으로의 일 방향에서 정보가 전달된다고 일반적으로 여겨지는 시냅스를 통하여 서로 통신한다. 2개의 뉴런 사이의 연결 강도는 가중치로 설명되며, 많은 인공 신경 네트워크는 유연성 알고리즘을 사용하는데, 이것은 시냅스 앞 부분의 입력 뉴런과 시냅스 뒤 부분의 출력 뉴런에서의 일치하는 활동(활성)에 따라 가중치를 조정한다. 일반적으로 사용되는 유연성 원칙 등급은 Hebbian으로 알려져 있다. Hebbian 원칙에 따르면, 시냅스 앞 부분의 활동이 시냅스 뒤 부분의 활동에 긍정적으로 연결되면, 연결 가중치는 커진다. 따라서, 만약 시냅스 앞 부분 뉴런이 시냅스 뒤 부분 뉴런을 대략적으로 동시에 흥분시키면, 연결 가중치는 증가한다. 만약, 시냅스 앞 부분 뉴런의 활동이 뒤 부분 뉴런 활동을 저해시키면, 연결 가중치는 떨어진다. 인공 신경 네트워크에서 유연성 보다는 덜 사용되는 것이, “anti-Hebbian” 원칙이다. Hebbian 규칙은 흥분 또는 저해와 각각 연관된 흥분 또는 저해 입력을 강조하며, 따라서 긍정적인 피드백을 포 함한다. Anti-Hebbian 규칙은 단지 그 반대를 하며, 부정적인 피드백을 발생시킨다. 따라서, 시냅스 뒤 부분 뉴 런이 저해 입력이 활성일 때 흥분하게 된다면, 이들의 연결 가중치는 증가한다. Anti-Hebbian 규칙은 세포 활성 을 비연관시키는데 효과적이며, 이로써 효과적인 코딩을 촉진시키고, 유하게는 예측 학습에 있어서 효과적이다. Hebbian 또는 Anti-Hebbian 원칙은 수십 년간 현재 문헌상으로 연구되지만, 이들 유연성 원칙을 신경 네트워크 에 적용하는 것이 인공 지능을 생성시키고자 하는 문제의 일반적인 해답이 되지 않고 있다. 비록, 유연성 원칙 각각은 분명한 장점을 보였지만, 특정 뉴런 또는 이들의 입력 각각에 대하여 어떠한 원칙이 적용되어야 하는지 가 명확하지 않다. 본 명세서에서 단일 뉴런 내에서 Hebbian 및 Anti-hebbian 유연성 원칙을 특정 방식으로 조 합시키는 것이, 인공 출력을 생성시키도록 학습하는 인공 네트워크 생성시키는 데 있어서 효과적이다는 점을 제 안한다. 뉴런들 사이의 시냅스 연결성에 관하여 독점적으로 초점을 맞추는 데 있어, 인공 신경 네트워크에 관한 대부분 의 연구는 생물학적 뉴런의 중요 측면을 간과하였다. 생물학적 뉴런의 출력은 이들의 시냅스 입력에 의해서만 결정되지 않는다. 대략적으로 동일한 수준으로 비-시냅스 이온 채널이 이 결정에 기여한다. 많은 이온 채널의 생동은 막 전압에 의하여 조절된다. 막이 일단 비극성화되면, 뉴런을 개방 또는 저해시키는 전압 게이트 칼륨 채널이 그 예가 된다. 이 외에도 다양한 형태의 전압-게이트 칼륨 체널이 있으며, 이들은 뉴런 각각에 의하여 생성되나, 어쨌든 특정 뉴런은 선택적으로 몇몇 종류의 칼륨 채널을 발현시키나, 나머지는 발현시키지 않는다. 생물학적 뉴런에서 이러한 선택을 결정하는 원칙은 알려져 있지 않으며, 대부분의 인공 네트워크는 이러한 비- 시냅스 이온 채널과의 유사성을 포함하지 않는다. 본 발명은 비-시냅스 이온 채널에 관한 핵심 역할을제공하며, anti-Hebbian 유연성을 사용하여 다양한 형태의 비-시냅스 이온 채널 가운데서 선택하게 한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 인공 뉴런을 이용한 정보 처리 방법 및 장치를 제공한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명은 하나 이상의 인공 뉴런을 이용하여 정보를 처리하는 방법으로서, 상기 하나 이상의 인공 뉴런은 제1 및 제2 부류의 입력을 수신하고, 출력을 생성하며, 상기 제1 및 제2 부류 중 하나 이상은 복수의 입력을 포함하고, 하기 단계를 포함하는 방법: 각 입력이 가중치 및 활성을 가진 상기 제1 부류의 입력을 제공하는 단계; 각 입력이 가중치 및 활성을 가진 상기 제2 부류의 입력을 제공하는 단계; 상기 뉴런의 상기 출력에 상응하는 차이를 최대화하기 위해서 상기 제1 부류에서 각 입력의 상기 가중치를 선택하는 단계; 상기 하나 이상의 인공 뉴런의 상기 출력에 상응하는 차이를 최소화하기 위해서 상기 제2 부류에서 각 입 력의 상기 가중치를 선택하는 단계; 및 제1과 제 2 부류의 입력의 활성의 가중 합계 사이에 있는 상기 차이에 상응하는 상기 출력을 생성하는 단계; 여기서, 상기 하나 이상의 인공 뉴런은 상기 하나 이상의 인공 뉴런이 예 측하기 어려운, 하나 이상의 인공 뉴런에게 유용한 세계의 측면을 예측한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명은 비-시냅스 이온 채널에 관한 핵심 역할을 제공하며, anti-Hebbian 유연성을 사용하여 다양한 형태의 비-시냅스 이온 채널 가운데서 선택하게 한다. 과거 정보에 기여하는 입력은 오류를 최소화하기 위해서 선택되 며, 이는 anti-Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 현재의 정보 공급원은 오류를 최대화하기 위 해 선택되며, 이는 Hebbian 타입 유연성 규칙을 통해 일어날 수 있다. 이것은 뉴런이 뉴런이 이미 보유하고 있 는 과거의 정보와 중복되지 않은 외부 세계로부터 새로운 정보를 수신한다는 것을 확인시켜준다. 예측하기 위해 스스로 학습함으로써, 뉴런 또는 이들 뉴런의 네트워크는 지적이고 유리한 출력을 생성하는데 필요한 정보를 획 득한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "발명의 요약 본 발명은 생물학적 뉴런에서 발생할 수 있는 과정에서 영감을 얻은 정보 처리 방법을 기재한다. 인공 뉴런은 복수의 입력을 통합하며, 이 중 몇몇은 흥분성이나, 나머지들은 저해성이다. 바람직한 구체예에서, 뉴런의 출력 은 전압이고, 입력은 이온 전도도이다. 이들 입력은 두 부류(클래스)로 나뉜다. 제 1 부류(첫 번째 또는 제 1 클래스)는 현재 정보를 제공하는 것으로 언급되며, 제 2 부류(두 번째 또는 제 2 클래스)는 (공간상 또는 시간 상의) 선행 정보를 제공한다. 선행 정보를 기여하는 입력은 현재 정보를 기여하는 것들의 영향을 예측, 반작용 하도록 작동하며, 따라서 뉴런의 출력은 예측 오류로 간주될 수 있다. 만약, 뉴런의 현재 정보가 흥분성이라면, 흥분 정도가 선행 정보에 기초하여 기대되는 것보다 큰 경우, 뉴런은 흥분될 수 있으며, 흥분 정도가 선행 정보에 기초하여 기대되는 것보다 적은 경우, 뉴런은 저해될 수 있으며, 흥분 정도가 예측치를 만족하면, 중간 상태 를 유지할 것이다. 뉴런에 대한 각 입력은 이것의 영향 강도를 정량화하는 연관 가중치를 가질 수 있다. 본 발명은 가중치를 결정 해야 하는 원칙을 특정한다. 입력의 제 1 부류 및 제 2 부류 간의 주 차이점은 가중치를 할당하도록 사용되는 원칙에 있다. 제 1 부류 입력은 예측 착오를 최대화하도록 선택되어야 한다(즉 가중되어야 한다). 이것은 뉴런 에 대하여 정보를 제공하는 세계의 일 측면에 관한 현재 정보를 뉴런이 수신하는 것을 보증하는데, 따라서, 이 것은 매우 흥미로운 것인데, 왜냐하면 예측은 어렵기 때문이다. 바람직한 구체예는 Hebbian 유연성(plasticit y)을 통하여 예측 착오를 최대화시키는 원칙을 실행한다. Hebbian 유연성에 따르면, 뉴런이 흥분되는 때와 동시 에 활성인 개별 흥분성 입력은 예측착오에 기여하며, 따라서 강화될 것이다. 뉴런이 저해되는 때에 활성인 흥분 입력은 착오에 반작용이며, 약화될 것이다. 또 다른 구체예에서, Hebbian 규칙은 네트워크의 목적에 연관된 일 종의 정보 피드백을 포함하며, 이로써 뉴런은 예측성은 낮으나 네트워크의 목적과 가장 가까이 연관된 제 1 부 류 입력에 대하여 높은 가중치를 부여한다. 입력의 제 2 부류에 연관된 가중치는 예측 오류를 최소화시키기 위하여 선택되며, 이로써 정확한 예측을 하게 된다. 바람직한 구체예의 뉴런에서, 이것은 오류가 0가 되는 영역 중간 값으로 멤브레인 전위차를 구동시키는 입력에 높은 가중치를 주게 한다. 이를 구현하는 일 수단은 anti-Hebbian 규칙이다. 뉴런이 흥분함과 동시에 활 성을 갖는 개별 저해성 입력(양성 예측 오류를 신호로 준다)은 오류에 반작용하며, 따라서, 이것은 강화될 것이 지만, 반면 뉴런이 저해될 때 활성을 갖는 저해 입력은 오류를 부여하며, 따라서 약화될 것이다. 이 원칙은 제 2 부류 입력(선행 정보를 부여하는 것들)이 뉴런 출력에서의 제 1 부류 입력(현재 정보 기여)의 효과를 예측하 고, 제거하도록 학습시킨다. 선행 정보가운데서 선택하게 하는 anti-Hebbian 유연성과 보상 피드백을 통합하는 Hebbian 유연성 둘 다를 활용 함을써, 인공 뉴런은 미래 보상을 예측하도록 학습된다. 강화 학습 분야에서 인정되는 바와 같이, 미래 보상의 예측은 지능적 행동의 생성에 있어서 중요하다. 반면, 이들 뉴런 중 하나는 적은 양의 정보를 통합하여, 보상과 연관된 세계의 측면을 예측할 수 있으나, 이들 뉴런의 네트워크는 가장 중요한 세계의 측면을 예측하여, 지능적 행동을 생성시킬 수 있다. 생물학적 뉴런은 일 실시예의 인공 뉴런이 예측을 학습하는지를 예시하기 위하여 기재된다. 생물리학 및 예측 오류 뉴런은 막 사이의 전압차이로 정보를 축적한다. 뉴런은 에너지를 사용하여, 막 바깥에서 보다 높은 농도의 나트 륨 이온을 유지하며, 안쪽에서는 보다 높은 농도의 칼륨 농도를 유지한다. 뉴런은 이것의 막에서 다공 또는 채 널 구조를 가지며, 이것은 관통하여 지나는 이온 종류가 상이하다. 예를 들면, 몇몇 이온 채널은 칼륨에 대하여 선택적이며, 따라서 이온 채널이 개방되며, 칼륨이 농도 구배에 따라 흐르게 되며, 뉴런을 흥분시키고, 이에 따 라 칼륨 평형 전위차(이것은 보통 -100mV) 로 막을 과분극시킨다. 뉴런은 일반적으로 다양한 종류의 이온 채널 을 갖지만, 막 전압으로 정보를 축적하는 가장 간단한 뉴런은이온 선택성을 달리하는 두 종류만의 이온 채널을 가질 것이다. 이와 같이 단순화된 뉴런은 나트륨과 칼륨 투과성을 갖는 일단의 채널을 함유하며, 이들 채널은 따라서 제로 밀 리볼트(Ecat = 0)의 평형 전위차를 갖는다. 이들은 외부 화학물질의 충분한 농도에서 개방된다. 예를 들면, 화학 물질인 글루타메이트이며, 따라서, 상기 채널들은 클루타메이트-게이트 이온 채널이 된다. 두 번째 종류의 채널 은 개방될 때 칼륨 전도를 수행하며, 막 전압에 의하여 개방된다. 칼륨 평형 전위차는 -100mV(EK = -100)이다. 뉴런 막 사이의 전압은 따라서 하기 식의 정상 상태 값(V )에 접근한다. 여기에서 Gcat 및 Gk는 각각 양이온 및 칼륨 전도도이다. 글루타메이트의 파동 제곱 펄스에 대한 뉴런의 이러한 반응은 도 1(종래 기술)에 도시된다. 글루타메이트-게이 트 채널이 개방되면 뉴런은 탈분극되지만, 전위-게이트 칼륨 채널이 개방되기 시작함에 따라 서서히 재분극된다. 글루타메이트 농도에 관한 정보는 따라서 글루타메이트-게이트 채널로 흐르며, 다시 막 전압, 그리 고 그 다음으로 칼륨 채널로 흐르게 된다. 전압이 뉴런의 출력이므로, 칼륨 채널의 전도도(conductance)는 과거 출력의 저장 값에 대응된다. 채널들의 두 집합은 글루타메이트 농도에 관한 정보를 함유하며, 따라서, 이들은 글루타메이트 농도를 예측하거 나, 측정할 수 있다. 각 짧은 순간에서, 칼륜 채널의 전도도는 과거의 글루타메이트 농도에 의존하는 반면, 글 루타메이트 게이트 채널은 현재의 글루타메이트 농도에 적어도 일부 의존한다. 따라서, 글루타메이트-게이트 채 널은 글루타메이트 농도에 관한 현재 정보를 보유하는 한편, 칼륨 채널은 글루타메이트 농도에 관한 선행 정보 를 보유한다. 따라서 뉴런은 글루타메이트에 관한 두 개의 예측 값을 갖는 것으로 이해될 수 있는데, 하나는 현 재 정보에 대하여 조건적이며 (즉, 제1 부류 입력), 다른 하나는 선행 정보에 조건적이다(즉, 제2 부류 입력). 막 전압은 두 예측 값 또는 정보 집합의 비교에 의존한다(도 2). 글루타메이트-게이트 채널에 의한 글루타메이트 농도 추산치가 칼륨 채널에 의한 추산치를 초과하는 때, 뉴런은 탈분극된다. 그 역이 사실일 때, 뉴런은 과분극된다. 이온 채널의 두 집합이 거의 동일 정보를 가질 때, 막 전 압은 중간대의 전압이다. 이것은 도 1에 도시된 바와 같으며, 도 1은 글루타메이트의 제곱파 펄스에 대하여 이 들 두 종류의 이온채널만을 갖는 뉴런의 반응을 보여준다. 뉴런의 막 전압(도 1B)은 따라서 예측 오류로 여겨질 수 있다. 글루타메이트 농도가 상승한 후, 즉시 뉴런은 놀랍게도 가장 크게 탈분극되며, 이것은 양성 예측 오류 에 대응한다. 이것은 두 채널 전도도의 미스매치에 기인한다(도 1C와 1D를 비교). 글루타메이트 농도의 감소에 후속하는 탈분극은 음성 예측 오류에 대응될 수 있다. 양성 또는 음성 예측 오류는 청구항과 아래에서 “차이(discrepancy)”로 지칭된다. 이것의 물리적 상관성은 바 람직한 실시예에서 뉴런 막 사이의 전압이며, 중간 전압은 0 예측 오류에 대응된다. 막 전압과 예측 오류 사이 의 정확한 정량적 상관관계는 단조함수형태로 추정되며, 반드시 선형은 아니다. 유사하게, 아래에서 예측 오류 (prediction error)를 최대화 또는 최소화시키는 용어는 예측 오류 또는 막 전압의 임의 단조함수 형태에 대하 여 수행되는 임의 과정을 적용하기 위한 것으로 해석된다. 예측 오류의 신호화는 상당한 장점이 있는 것으로 여겨진다. 단순히 예측에서의 오류에 대응하는 원칙은 뉴런이 이미 알고 있는 것으로 그 자체를 학습시킬 필요가 없으며, 이미 들은 바가 있는 것을 타겟 뉴런에게 말하는 것 은 바람직하지 않다는 것이다. 예측 오류의 장점은 문헌상 학습 강화 및 효율적인 코딩에 있다고 인식되었다. 도 1에서 뉴런의 출력은 예측 오류로 표현되는데, 왜냐하면 전압-게이트 칼륨 채널에 의하여 전달된 과거 출력 의 기억은 글루타메이트 게이트 채널에 의한 흥분에 반대작용을 하기 때문이다. 하지만, 인공 네트워크의 전형 적인 뉴런은 상기들과 같은 비-시냅스 이온 채널이 없었다. 상술한 예에서, 글루타메이트 농도는 뉴런의 자극제이다. 뉴런 자극제는 뉴런이 추정하고, 예측하는 세계의 일 측면 또는 부분으로 본 명세서에서 정의된다. 뉴런은 자연적으로 이것은 생물물리학적 구조 내에서 정보를 담고 있으며, 그 정보는 본 명세서에서 정의된 뉴런 자극제로 외부세계의 정보에 관한 것이어야 한다. 아래에서 보다상세히 설명되는 바와 같이, 뉴런 자극제는 뉴런의 제1 부류 입력의 가중 합에 의하여 정확하게 정의된다. 전형 적으로 뉴런 자극제는 거의 연속적인 방법으로 강도가 변화되지만, 원칙적으로는 적은 수의 가능한 상태들 중 어느 하나에서만 대신 존재할 수 있다. 전형적인 뉴런은 복수의 개개 글루타메이트 시냅스들로부터의 현재 정보 를 받으며, 따라서 이것의 자극제는 이들 시냅스를 가로질러 총합된 국소적인 글루타메이트 농도이다. 일반적으로, 뉴런은 복수 종류의 이온채널을 가지며, 이것은 선행 정보를 기여하게 된다. 상기 채널들은 현재 정보를 기여하는 채널들의 효과를 예측하고 없애는 기능을 수행한다. 선행 정보를 기여하는 채널은 제로 오류(0 오류)에 대응하는 중간지점으로 막 전압을 유도하는 방식으로 전도도를 조절하는 경향이 있다. 따라서, 뉴런의 선행 정보는 예측 오류를 최소화하는 기능을 수행한다. 이런 원칙은 망막 뉴런의 계산 모델로 예시된다. 공간적으로 국한된 광센서들을 단순히 발현시킴으로써 뉴런은 공간상 작은 영역에서의 현재 광 강도를 추정할 수 있다. 하지만, 자연적인 조건에서의 광 강도는 공간과 시간에서 인접한 지점들 사이의 강한 양성 상관관계를 표시한다. 따라서, 공간 상의 작은 지점(즉, 중간)에서의 현재의 광 강도를 추정하는 대체 방식은 현재의 과거 및 공간 상에 이웃하는 지점(즉, 주변)으로부터의 정보만을 이용하는 것이다. 본 발명은, 뉴런이 현재 정보를 기여하는 몇몇 입력을 관심 공간으로부터 직접 수신하는 것과, 선행 정보를 기여하는 다른 입력을 공간 및 시간 상으로 인접하는 지점들로부터 수신하는 방식이 제안된다. 뉴런은 이들 두 추정값 또는 정보집합들을 비교하며, 이들 출력은 두 개 사이의 차이에 대응된다(식 1)(도 2). 차이는 보다 일반적으로 예측 오류로 지칭된다. 뉴런의 출력이 예측 오류에 대응한다는 제안은 순전히 서술적이다. 예측 오류의 개념은 정보 전달의 흐름과 모 델 뉴런의 기능을 이해하는 데 유용하다. 이 용어의 이유는 아래에서 설명되는 유연성 규칙 관점에서 보다 명확 해진다. 하지만, 예측 오류 개념은 뉴런에 의하여 수행되는 과정-이것은 확립된 생물물리학을 통하여 결정되었 다- 에 대하여 어떠한 제한을 부여하지 않으며, 이것은 식 1에서 예시되었다. 뉴런에 의하여 보유된 정보에 관 해서도 예측 오류는 어떠한 제한을 부여하지 않는다. 뉴런의 예측은 단순히 뉴런이 가질 수 있는 어떠한 정보에 대해서도 조건부이다. 원칙적으로, 뉴런 예측은 무한대의 불확실성을 가질 수 있으며, 이것은 완벽한 정보 부재 에 해당한다. 하지만, 아래의 유연성 규칙은 보통 뉴런이 관계된 정보를 수집하여, 예측 오류 개념이 뉴런의 계 산 기능을 설명하는데 유용한 개념이 되도록 한다. 일 실시예에서, 뉴런 출력은 이것의 막 전압에 대응된다. 반대로, 가장 현실적인 뉴런은 막 전압이 문턱전압을 넘을 때 시작되는 양자택일(all or nothing) 방식의 거동 전위차(action potentials)를 통하여 다른 뉴런과 소 통한다. 이 경우, 짧은 시간 순간에서의 뉴런 출력은 이진수가 된다. 디지털화된 출력은 아날로그 전압 신호에 의하여 보유된 정보에 비하여, 정보 잃음을 나타낸다. 하지만, 디지털화된 출력은 보다 먼 거리에서 신뢰성 있 는 통신에 있어서는 유리하다. 장거리로 정보를 전달하지 않는 많은 뉴런은 거동 전위차를 방출하지 않고, 등급 화된 막 전위차를 사용하는 신호를 방출한다. 바람직한 실시예는 이와 같이 등급화된 전위차 뉴런에 기반한다. 하지만, 대신 이진수의 출력으로 뉴런을 자극하는 때에 본 발명의 원칙이 구현될 수 있음은 명백하다. 선행 정보 공급원의 선택 뉴런은 복수의 개별 입력을 수신하며, 이것의 각각은 공간에서의 상이한 지점 또는 과거의 다른 시간기간으로부 터의 정보이다. 이들 정보들 각각은 정보 공급원으로 기능한다. 어쨌든 뉴런은 이들 정보 공급원 중 몇몇을 선 택하며, 나머지는 버린다. 상술한 바와 같이, 뉴런은 2개 클래스의 입력을 수신하며, 이것들을 현재 정보 및 과 거 정보를 기여하게 된다. 현재 정보 공급원(정보원)은 뉴런의 예측 오류를 최대화시키도록 선택된다(즉, 막 전 압을 과분극과 탈분극 한계치까지 근접하게 한다). 반면, 과거 정보 공급원은 뉴런의 예측 오류를 최소화시키도 록 선택된다(즉, 막 전압을 중간값으로 근접하게 한다). 비록 이 결과를 달성시키는 다른 많은 방식이 존재하지 만, 본 발명의 바람직한 실시예에서는 뉴런이 Hebbian 또는 anti-Hebbina 유연성을 통하여 이것을 수행한다. 선행 정보를 기여하는 입력은 공간 및 시간 상 스펙트럼의 폭을 가질 수 있으며, 뉴런은 자극 강도를 예측하는 데 있어서 오류를 가장 최소화시킬 수 있는 입력을 선택하도록 제안된다. 채널이 표현하는 과거(이것의 기억) 중 어떤 때는 이것의 동역학적 특성에 의존한다. 가장 간단한 경우, 채널의 게이팅 동역학(gating kinetics)은 단일의 기하급수 과정에 의하여 지배되지만, 전형적으로 실제 채널의 게이팅은 상대적으로 복잡한 복수의 기하 급수 과정의 상호 작용에 의존한다. 전압 의존성뿐만 아니라, 동역학 특성을 달리하는 많은 종류의 비-시냅스 이온 채널이 있으며, 칼륨 채널은 특히 이러한 다양성이 최고이다. 상이한 종류의 채널은 과거의 상이한 기간으 로부터의 정보를 운반한다. 하지만, 뉴런은 이들 이온 채널의 하위 집합만을 표현한다. 이들 이온 채널의 하위 부분만을 발현함으로써, 뉴런은 과거의 몇몇 기간 중의 기억은 유지하고, 나머지는 유지하지 않도록 선택한다. 전압으로 매개된 오류 신호를 통하여 거동하는 뉴런 자극제의 패턴은 자극제 강도를 제일 잘 예측하는 과거의 관련 기간, 비-시냅스 이온 채널의 종류를 선택할 수 있다. 유사한 과정이 공간 도메인에서 또한 일어날 수 있 는데, 여기에서 비연속적인 시냅스는 비연속적인 공간 지점을 나타낸다. 예를 들면, 상술한 망막의 경우, 개별 입력은 (시냅스 이전 뉴런 자극제 또는 수신 영역 중심에 의하여 결정된) 주변 특정 영역에서의 광 강도를 나타 내는 저해 시냅스일 수 있다. 중심에서의 자극제 강도를 가장 잘 예측하는 주변으로부터의 이들 시냅스는 증가 될 수 있다. 어떤 방식으로 이 과정이 벌어지는 지를 예시하기 위하여, 본 발명자는 선행 정보를 기여하는 입력을 칼륨 채널 로만 구성된 뉴런을 다시 고려하였다. 이때 동역학적 특징이 다른 구별된 종류의 칼륨 채널들이 있다. 시간상 임의 순간에서의 전체 칼륨 전도도(Gk)는 각 성분의 활성 또는 칼륨 채널의 가중 합이다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "성분의 활성(Ui) 은 주어진 순간에서 그 종류의 채널이 개방될 시간- 및 전압-의존적 가능성이다(즉, 평균 채널 거동의 Hodgkin-Huxley 모델에서의 채널 개방 가능성이다). 성분의 가중치(wi)는 상기 종류의 기능성 채널의 수 에 대응된다. 막으로부터 채널들을 삽입하거나 제거함으로써 또는 채널의 기능을 다른 기능으로 바꾸게 할 수 있는 인산화반응과 같은 사건에 의하여 가중치는 조절될 수 있다. 오류의 최소화는 오류가 제로가 되는 영역의 중간값으로 막 전위차를 유도한다. 현재의 정보 공급원이 탈분극화 되는 전형적인 경우, 탈분극된 전위차는 양성 오류에 대응될 수 있으며, 과분급된 전위차는 음성 오류에 대응될 수 있다. 만약 막이 탈분극화되는 때에 탈분극-활성화 칼륨 채널이 개방된다면(양성 오류), 비록 뉴런의 선행 정보 공급원이 전체적으로 너무 낮다고 추측하였음에도 불구하고, 이것은 글루타메이트 농도가 높다라고 정확하 게 추측한다. 따라서, 칼륨 채널의 이런 종류 가중치는 증가되어야 한다(하기 식 3에서 특정된 바와 같이 보다 음성화 시킨다). 만약, 칼륨 채널이 개방될 때 막이 과분극된다면(음성 오류), 이런 종류의 채널은 제거되어야 하는데, 왜냐하면 이것들은 너무 높게 추측하고, 음성 오류에 기여하였기 때문이다. 만약 칼륨 채널이 폐쇄된다 면, 이것은 전압이 얼마인지에 대하여 어떠한 책임이 없으며, 이것의 해당 가중치는 실질적으로 변화되지 않아 야 한다. 개방되어, 전도를 수행하는 때에 막을 탈분극하는 종류의 채널 가중치를 조정하는데, 동일 원칙이 적 용할 수 있다. 이들 원칙은 하기와 같은 학습 공식을 제안한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기에서 개별 성분의 가중치(w)는 시간의 각 순간에서의 활성도(U), 막 전압(V) 및 학습도(α 및 β)에 따라 업데이트된다. 이 경우, 가중치는 임의 음의 실수일 수 있다. 최종 항(βwt)은 막으로부터 낮은 속도로 제거되는채널에 대응되며, 이것은 활성도가 실질적으로 막 전위차와 관련되지 않은 채널 종류의 가중치를 0으로 근접시 키는 데 유용하다. 항 θ는 범위의 중간에 근사한 전압을 지칭한다. 이것은 오류가 없는 막 전압의 널 포인트 (null point)로 기능한다. θ를 넘어선 탈분극화는 가중치를 증가시킬 수 있는 반면, 과분극화는 가중치를 감소 시킨다. 식 3은 막 전압에 과분극화 영향을 수행하는 칼륨 채널 등에 적적하다. 하지만, 선행 정보에 기여하는 몇몇 채 널 종류는 탈분극화 영향을 가질 수 있다(예를 들면 과분극화에 의하여 활성화되는 HCN 양이온 채널). 이런 종 류 채널의 가중치는 이들 채널이 뉴런이 과분극화됨과 동시에 이들 채널이 개방되는 때에 증가하는데, 왜냐하면 이때 채널이 음성 오류로 반작용할 수 있기 때문이다. 만약 탈분극화하는 입력에서 식 3에서 첫 번째 마이너스 부호(왼쪽으로 가장 먼 항)가 플러스 부호로 바뀌게 되면, 이것은 달성될 수 있다. 식 3과 같은 유연성 알고리즘은 종종 “anti-Hebbian”으로 지칭된다. Hebbian 규칙은 각각 탈분극 또는 과분극 과 쌍을 이루는 탈분극 또는 과분극 입력을 강화시키며, 따라서 이것은 양성(positive) 피드백을 포함한다. Anti-Hebbian 규칙은 단지 이것의 반대로서, 음성 피드백을 나타낸다. Anti-Hebbian 유연성은 생물학 및 인공 신경 네트워크에서 몇몇 시냅스 연결의 강도를 통제하도록 예전에 제안되었다. 본 발명의 인공 뉴런은 비-시냅 스 이온 채널 가운데에서의 선택에까지 이것의 적용을 확장한다. 본 발명의 체제에서 anti-Hebbian 규칙에 대하 여 기능적으로 관련된 항은 오류 최소화로 지칭된다. anti-Hebbian 유연성을 활용, 선행 정보 공급원 가운데서 선택함으로써, 상술한 뉴런은 외부 세계의 일부에 대 한 상태를 예측하도록 학습할 수 있다. 외부 세계의 어떤 부분을 뉴런이 학습하여 예측할 수 있는지는 이것의 현재 정보 공급원(이것은 뉴런 자극제를 정의한다)에 의존할 것이다. 뉴런이 이것의 선행 정보 공급원으로부터 선택함에 따라, 이것들은 또한 이것의 현재 정보 공급원으로부터 선택한다. 이런 선택 과정은 뉴런의 정보가 중 요하고 흥미로운 외부 세계의 일면에 관한 것이다라는 것을 보증하여야 한다. 현재 정보 공급원의 선택 상술한 뉴런은 어떠한 자극제가 현재 정보를 공급하는지를 학습하여 예측할 수 있다. 하지만, 실제 신경 시스템 은 미래 보상에 관련된 외부 시계의 측면들에 대해서만 관련된다. 본 명세서에서 사용되는 미래 보상은 주 목적 이 미래 보상의 합을 예측하는 강화 학습 용어 사용과 매우 유사하다. 신경 시스템의 일반적 기능은 미래 보상 을 예측하는 것이다(즉, 미래 보상에 대한 불확실성을 최소화는 것이다). 미래 보상은 결국 생물학적 적합성 관 점 또는 모든 생물들의 목적인 유전 정보를 영속화시키는 동물들의 능력 관점에서 정의된다. 인공 신경 네트워 크의 경우, 미래 보상은 (네트워크 설계자에 의하여 정의된 바에 따른) 목적을 달성하는 데 있어서의 네트워크 의 미래 성취에 대응될 수 있다. 시스템의 모든 출력은 미래 보상을 높이도록 선택되어야 하기 때문에, 모든 시 스템 정보는 미래 보상에 관한 것이어야 한다. 본 명세서에서 사용되는 “보상”이라는 용어는 좋은 결과값만을 지칭하는 것이 아니라, 나쁨에서 좋음에 이르는 영역의 값의 측정 또한 지칭한다. 이러한 넓고 포괄적인 미래 보상의 개념은 다소 추상적이고, 무형의 형태이다. 하지만, 강화 학습에서 같이, 시 스템은 그 자체가 미래 보상을 예측할 수 있는 구체적인 물리학적 자극제를 예측함으로써 미래 보상을 예측한다. 이러한 자극제들은 신경 시스템에 의하여 검출될 수 있는 외부 시계의 모든 측면을 포함할 수 있으며, 여기에서 자극제는 시스템의 내부 세계 또는 외부 시계의 일부에 해당될 수 있다. 예를 들면, 이것은 음식 모습이나 맛과 같은 강한 예측자 뿐만 아니라, 광강도와 같은 미래 보상의 약한 예측자를 일반적으로 포함 할 수 있다. 모토 시스템에서의 뉴런에 대하여, 보상-예측적 자극제는 대략적으로 거동에 대한 계획에 대응할 수 있다. 따라서, 세상의 현재 상태는 그 자체로 관심 대상이 되는 것이 아니라, 이것이 미래, 보다 구체적으로 는 미래 보상에 관한 정보를 제공하기 때문에 관심의 대상이 된다. 공간 및 시간에서의 어떤 지점이 자극제 강도를 예측하는 데 있어서 가장 유용한지를 뉴런이 선택할 수 있는 것 과 같이, 이것은 미래 보상에 관하여 가장 유용한 자극제를 또한 선택할 수 있다. 뉴런의 근위 자극제는 시냅스 하위 부분을 가로질러 합산된 글루타메이트 농도로 정의된다. 각 비연속적인 시냅스는 현재 정보의 개별 공급원 에 해당한다. 뉴런은 이것의 자극제 또는 현재 정보 공급원의 집합을 선택하는데, 이는 몇몇 시냅스에 대하여 높은 가중치를 주고, 나머지에 낮은 가중치를 줌으로써 달성된다. 현재 정보 공급원 중에서의 선택에 있어 중요한 두 개의 기준이 있다. 그 중 하나는 활성이 확립된 보상 예측자 (예를 들면 음식)에 대하여 예측적인 개별 입력(예를 들면 시냅스)는 강해야 한다는 것이다. 만약, 뉴런이 스스 로 어떤 입력이 보상에 대하여 가장 좋은 예측자인지를 학습하기 위해서, 보상 피드백에 관한 몇몇 형태가 요구 된다. 보상 피드백의 일 예는 중뇌 도파민 뉴런에 의하여 신호화되는 보상 예측 오류일 수 있으며, 이것은 시 스템의 광범위한 양의 정보를 통합하고, 세상의 현재 상태가 예측치보다 좋은지 또는 나쁜지에 관하여 신호를 준다. 보상 피드백의 또 다른 형태는 신피질에서 선택적인 집중을 중재하는 탑-다운 피드백 예측으로부터 얻을 수 있다. 보상 피드백 신호는 이 예들보다 다소 정교할 수 있는데, 가장 단순한 경우, 세대간 자연 선택에 의하 여 제공될 수 있다. 인공 신경 네트워크에서 보상 피드백은 네트워크의 다른 뉴런 또는 외부 통제 신호에 의하 여 제공될 수 있다. 본 발명 모델의 모든 측면에 대하여, 뉴런은 이것이 갖게 되는 어떠한 정보에 의하여 작동 한다. 아래에서 설명되는 바와 같이, 보상에 대한 관련성은 뉴런이 이것의 제 1 부류 입력을 선택하는데 뉴런이 사용하여야 하는 유일한 기준이 아니며, 모델 뉴런은 반드시 보상 피드백을 요구할 필요는 없다. 두 번째 기준은 미래 보상을 가장 잘 예측하기 위하여, 뉴런은 주어진 뉴런의 선행 정보에서 최소 예측 가능성 의 자극제(즉, 제 1 부류 입력의 집합)를 선택하여야 한다. 이것은 일 매개변수(예를 들면 광 강도) 변동이 커 질수록, 다른 매개변수(예를 들면, 물의 유연성)의 변동을 설명할 수 있는 가능성이 커진다는 통계학의 원칙과 유사하다. 하지만, 비록 자극제 강도가 큰 변동값을 가지고, 이것이 보상과 연관된다고 하여도, 만약 이것이 상 당히 예측 가능하다면 뉴런에 있어서 이것은 유용하지 않은데, 왜냐하면 이것은 단순히 뉴런에게 어떤 뉴런이 알고 있는지 만을 알려줄 뿐이기 때문이다. 따라서, 가장 예측 가능하지 않은 입력이 가장 유용한 정보를 준다. 따라서, 다른 사항이 동일하다면, 뉴런에게 미래 보상에 관하여 가장 많은 정보를 제공하는 것으로 기대되는 것 이 가장 예측 가능하지 않은, 즉 비예측적인 자극제이다. 미래 보상에 대하여 예측적이면서, 예측 가능하지 않은 자극제를 뉴런이 선택할 수 있는 일 방식은 상술한 식 3 과 유사한 학습 규칙을 적용하는 것이지만, 표시 변화에 의하여 가능한 임의의 보상 정보(R)을 포함한다:"}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "이 식에서 가중치(w)는 임의의 양인 실수일 수 있다. 만약 보상에 관한 유일한 피드백이 세대간 자연선택에 의 하여 제공된다면, R은 유기체 일생 동안 상수일 수 있으며, 이 원칙은 단순히 최소로 예측 가능한 자극제를 선 택하는 경향을 보인다. 비록 식 3에서는 나타나지 않았지만, 보상 정보는 또한 선행 정보를 기여하는 입력의 선 택을 결정지을 수 있다. 하지만, 비록 식 3에서 보상의 직접적인 영향 없이도, 식 4에서의 보상 영향은 뉴런의 선행 정보가 미래 보상에 대하여 예측적이다는 점을 보증한다. 또한, 가중치 변화가 일 순간의 시냅스전 활성과 잠시 후의 시냅스후 활성 사이의 연결성에 의존한다면, 이 유연성 규칙(식 4)의 목표는 달성될 수 있으며, 이것 은 스파이크-타이밍 의존적 유연성(spike-timing dependent plasticity) 경우와 같다. 식 4는 글루타메이트 시냅스와 같이 탈분극화시키는 전형적인 입력의 경우에 적용된다. 하지만, 만약 과분극화 시키는 영향을 입력이 갖게 된다면, 도 4의 플러스 부호는 마이너스 부호로 바뀔 수 있다. 이것은, 과분극화와 일치하며, 따라서 음 오류에 기여하는 과분화 입력이 커지는 것을 보증한다. 식 4의 유연성 알고리즘은 Hebbian-타입 규칙이며, 이것은 뉴런의 예측 오류를 최대화하는 경향을 보인다. 상기 종류의 규칙은 활성이 동시 발생하는 경향의 시냅스를 강화시킨다. 동시 발생하는 활성화는 반복적인 공간 패턴 또는 외부 세상의 객체에 의하여 구동되는 시냅스 하위부분에서 보다 종종 발생할 수 있다. 이들 시냅스는 강해 질 수 있으며, 이로써 뉴런이 맞추어지는 자극제를 결정짓는다. 비록 Hebbian 유연성이 종종 인공 신경 네트워크에서 활용되지만, 본 발명의 특징적 제안은 오류를 최대화하는 데 Hebbian 규칙이 기능한다는 것이며, 신경 시스템의 궁극적 목표인 미래 보상을 예측하도록 학습하는 데 왜 이것이 유용한지를 제안하는 것이다. 오류-최대화 규칙은 자극제가 뉴런이 기 보유하지 않은 정보를 기여한다는 것이다. 예를 들면, 뉴런이 시간적으로 정형화된 순서의 흥분 시냅스 입력을 수신받는다면, Hebbian 규칙은 선 택적으로 순서 중 첫 번째 입력을 강화시킬 것이다(왜냐하면 선행 정보는 마지막 자극 입력에 대한 반응을 억제 하는 경향을 보이기 때문이다). 순서 중 첫 번째 입력은 가장 많은 정보를 제공하는데, 왜냐하면 이것은 마지막 입력을 예측하기 때문이며, 이것은 예상된 마지막 사건에서 적절한 동작 출력을 준비하도록 네트워크를 준비시 킬 수 있기 때문이다. 따라서, 오류-최대화 규칙은 외부 환경을 조사하여, 미래 보상에 관하여 가장 최적의 외 부 정보 공급원을 식별하는 반면, 오류-최소화 규칙은 이 정보를 가장 잘 포착하고, 보유할 수 있는 내부 기재 를 식별하는 것이다. 이것들은 모두 함께 미래 보상에 관한 뉴런의 정보를 최대화시키도록 기능한다. 비록 상기 논의의 초점은 공간의 비연속적 지점(시냅스) 또는 과거의 비연속적인 기간(즉, 구분되는 동역학을 갖는 이온 채널 종류)으로부터의 정보 선택에 관한 것이지만, 이들 원칙은 뉴런의 이온 채널과 이들의 조절 단 백질의 다른 측면들의 선택에 사용될 수 있다. 예를 들면, 이온 채널의 장벽이 되는 신경전달물질 수용자는 이 들의 리간드 친화도가 상이하거나, 또는 탈감각화 속도가 상이할 수 있다. 하기 자극제에서, Hebbian 규칙을 통 하여 뉴런이 노출되는 리간드 농도의 실제 범위에 적당한 리간드 친화도의 하위종류 이온 채널을 선택하였다. 유사하게, 전압-게이트 칼륨 채널들은 (비록 유사한 동역학적 특성을 가져도) 전압 의존성을 달리하며, anti- Hebbian 규칙은 막 전압을 가장 잘 예측하고, 반작용하는 것을 선택할 수 있다. 원칙적으로, 입력이 상이할 수 있는 크기에 상관 없이, 뉴런에 대한 모든 입력은 이들 두 종류의 유연성 규칙 중 어느 하나에 의하여 선택될 수 있다. Hebbian 및 anti-Hebbian 유연성 규칙은 선행 기술 내에서 유연성 알고리즘의 클래스들로 식별되며, 상기 식 3 및 4에서 주어진 특정 알고리즘은 이들 클래스들의 예들로 제공된다. 뉴런이 학습할 수 있는 것은 명백히 가능한 정보 공급원 집합에 의존한다. 인공 네트워크를 설계할 때, 설계자 는 각 뉴런에게 어떤 정보 공급원을 사용할지를 결정할 필요가 있을 것이다. 뉴런은 (비록 몇몇 입력은 0의 가 중치를 가지지만) 명백히 네트워크의 다른 뉴런 각각으로부터 또는 뉴런의 하위집합만으로부터 입력을 수신할 수 있다. 유사하게, 설계자는 뉴런의 비-시냅스 이온 채널의 동역학을 결정하거나, 동등하게 선행 정보의 공급 원으로 과거 출력 중 어떤 메모리 궤적을 뉴런이 사용할지를 결정할 필요가 있을 것이다. 본 발명은 뉴런에 사 용가능한 정보 공급원으로 어떠한 제한을 두지 않으며, 이것은 네트워크 설계자에 의하여 선택될 수 있다. 예를 들면, 실제 이온 채널의 동역학 특성은 인공 뉴런이 사용가능한 메모리 궤적의 종류들을 제한하지 않는다. 더 나아가, 설계자는 시냅스 이전 뉴런을 배열하여 단일 시냅스 이후 뉴런에 대하여 두 개의 입력을 제공할 수 있 다. 이들 중 하나의 가중치는 Hebbian rule에 의하여 제어될 수 있는 반면, 나머지는 anti-Hebbian 규칙에 의하 여 제어될 수 있다. 유연성 규칙은 시냅스 이전 뉴런이 현재 정보 또는 선행 정보를 제공할지, 또는 아무런 정 보도 제공하지 않을지를 결정할 수 있다. 기본 원칙은 사용 가능한 어떠한 종류의 정보 공급원을 뉴런이 사용, 작동하며, 뉴런이 어떤 정보 공급원을 사용할지는 설계자에 의하여 결정된다. Hebbian 및 anti-Hebbian 유연성의 대안 본 발명의 주요 원리는 뉴런 입력 중 몇몇은 예측 오류를 최대화하도록 선택되는 반면, 다른 것은 예측 오류를 최소화하도록 선택되는 것이다. 유연성 규칙은 이러한 목적을 달성하는 유일한 수단이다. 유연성 규칙은 적어도 두 개의 특징적인 장점을 갖는다. 첫째, 각 가중치는 국소적으로 사용가능한 정보만을 사용하여 변형된다는 것 이다. 따라서, 이들 원칙은 생물논리 시스템을 포함하는 하드웨어에서 실현된다. 둘째, 유연성 규칙은 항상 가중치를 업데이트하며, 이로써 연속적으로 변화되는 환경에 뉴런이 적응되도록 한다. 이들 특징은 인공 네트워크 가 스스로 학습하는 것을 가능하게 하며, 외부 공급원으로부터 이것에 정보를 공급하여야 할 필요가 없다. Hebbian 및 anti-Hebbian 유연성 규칙은 따라서 바람직한 실시예로 활용될 수 있다. 본 발명의 대체예로서, 통계적 추정 방식이 사용되어, 뉴런의 최적 입력 가중치를 결정한다. 선형 평균 제곱 추 정이 사용되어, 오류를 최소화하는 선행 정보의 최적 공급원을 결정할 수 있다. 예를 들면, 이 기술을 사용하여 광 강도가 중심 영역에서 현재 빛에 대한 가장 효과적인 예측자(predictor)인 과거의 기간 및 주변 공간의 영역 을 선택할 수 있다. 이 기술은 오류의 제곱을 최소화하도록 설계되지만, 유사한 통계학적 기술이 사용되어 오류 의 제곱을 최대화하도록 제 1 부류 입력의 가중치(즉, 현재 정보 공급원)를 선택할 수 있다. 각 가중치는 공간 및 시간상의 많은 지점에서의 모든 다른 가중치 및 강도에 관한 정보를 사용하여 각 가중치가 결정될 수 있기 때문에, 이런 통계학적 방법 또는 다른 통계학적 방법은 잠재적으로 유연성 규칙에 의하여 얻어진 가중치 집합 보다 더 최적에 가까운 가중치 집합을 선택할 수 있다. 하지만, 이러한 통계학적 방법은 국부적인 정보에만 의 존적이지 않기 때문에, 이러한 접근은 네트워크로의 전문가 정보의 제공이 필요하다. 또 다른 실시예에서, 네트워크의 설계자는 본 명세서에서 설명된 원칙에 따라 뉴런의 입력을 선택할 수 있으나, 유연성 규칙 및 통계학적 기술의 조력 없이 할 수 있다. 예를 들면, 만약 뉴런 자극제가 물체의 위치인 경우, 우리는 물체가 질량을 가지며 따라서 관성 또한 갖는다는 알 수 있다. 이러한 간단한 지식에만 기초하여, 만약 선행 정보의 단일 공급원이 앞선 순간에서의 자극제 위치로부터 유도된다면 뉴런은 예측 오류를 최소화할 수 있 는데, 왜냐하면 가장 최근 과거는 언제나 더 먼 과거보다는 좋은 예측자가 되기 때문이다. 마지막 순간의 자극 제 위치와 위치가 상이할 때 이런 뉴런은 흥분되거나, 저해될 것이다. 유연성 규칙 또는 통계학적 방법은 동일 결과를 가질 수 있지만, 이런 경우에는 불필요하게 간주될 수 있다. 따라서, 뉴런의 예측 오류를 최소화 또는 최대화하는 원칙은 사용 가능한 지식에 기초하여 네트워크 설계자에 의하여 직접 실현될 수 있다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 5, "content": "도 2에 예시된 실시예의 요약 도 2는 바람직한 실시예의 단일 뉴런에 대한 입력을 나타낸다. 각 화살표는 개별 입력(즉, 이온 채널의 구별되 는 총수)을 나타내며, 대응되는 화살표는 정보 흐름의 방향을 나타낸다. 각 입력은 흥분성(즉, 탈분극)이거나 저해성(즉, 과분극)일 수 있다. 개별 입력의 수는 본 발명에 의하여 제한되지 않으며, 본 명세서에서 도시된 수 는 예시적인 목적으로 선택된 것이다. 각 개별 입력은 변형 가능한 가중치와 연관되며, 이는 뉴런의 막 전압에 대한 기여도의 강도를 나타낸다. 막 전압 200은 예측 오류에 해당하며, 뉴런의 출력으로 기능한다. 각 개별 입 력은 예측 오류를 최대화 또는 최소화시키기 위하여 가중치가 결정되었는지에 따라 꼬리표가 부가된다. 제 1 부류 입력(클래스 1, 제 1 클래스) 210와 연관된 가중치는 예측 오류를 최대화하기 위하여 선택된다. 본 명세서에서 이들 입력은 현재 정보를 기여하는 것으로 설명되며, 바람직한 실시예에서 이들의 가중치는 Hebbian 유연성에 의하여 결정된다. 제 2 부류 입력(클래스 2, 제 2 클래스) 220 및 230은 예측 오류를 최소화하기 위하 여 선택된다. 본 명세서에서 이들 입력은 선행 정보를 기여하는 것으로 설명되며, 바람직한 실시예에서 이들의 가중치는 anti-Hebbian 규칙에 의하여 결정된다. 선행 공간 정보 220 및 현재 정보를 기여하는 입력 210은 네트워크의 다른 뉴런 또는 네트워크 외부의 공급원 (즉, 시냅스 입력)으로부터 온다. 반대로, 선행 시간 정보를 기여하는 입력 230은 동일 뉴런의 막 전압에 의하 여 조절되는 이온 채널로부터 온다. 이들 입력의 활성 또는 전도도는 과거 막 전압 및 현재 막 전압의 영향력에 의존한다. 따라서 막 전압 200과 이들 입력 230의 활성 또는 전도도 사이에서 양 방향으로 정보가 흐른다. 선행 시간 정보를 기여하는 각 개별 입력은 구별되는 동역학 특성을 갖는 채널 종류에 대응된다. 채널의 동역학 특성 은 이것이 기억하는 과거의 어떤 기간을 결정하고, 따라서, 구별되는 채널들은 구별되는 선행 시간 정보(즉, 기 억)를 기여한다. 일 실시예에서 뉴런은 생물학적 뉴런에 적용되는 기존의 생물리학적 원칙에 따라 입력을 통합 한다. 네트워크 상술한 설명은 단일 뉴런의 입력에 관한 것이다. 단일 뉴런은 그 스스로가 몇몇 활용도를 가질 수 있으나, 뉴런 들의 네트워크에서 정보 처리 요소로 기능하도록 의도된다. 일 실시예에서 각 뉴런은 네트워크에서의 공간 위치, 출력 표시(즉, 흥분성 또는 저해성), 및 임의 선택된 입력 가중치만이 초기에 상이하다. 가중치의 변경을 통하여 네트워크는 이것의 목적을 달성하도록 학습할 수 있다. 미래 보상의 예측은 신경 시스템의 중심 기능이며, 상술한 뉴런은 이러한 기능을 수행한다. 단일 뉴런은 세상의 작은 부분에 대한 상태만을 예측하며, 상기 부분은 미래 보상과 밀접하게 연관되지 않을 수 있다. 하지만, 각 개별 뉴런이 이러한 중심 기능을 수행한다면, 뉴런 시스템은 통합되어 미래 보상을 보다 잘 예측하도록 작동한 다. 각 뉴런은 적어도 생물학적 특성이 대략적으로 유사하므로, 각 뉴런은 자극제에 관하여 유사한 양의 정보를 보유한다. 하지만, 몇몇 자극제는 미래 보상에 대하여 많은 정보를 주지만(예를 들면 음식 모습), 반면 다른 것 들은 미래 보상과는 약하게 연관된다(예를 들면, 광 강도). 따라서, 뉴런들은 미래 보상에 대하여 얼마나 많은 정보를 갖느냐에 있어서 상이하며, 이것은 네트워크 수준에서는 중요한 변수가 된다. 보상 피드백은 식 4를 통하여 각 뉴런 자극제의 선택에 기여하므로, 네트워크의 감각 입력으로부터 처리되는 일 련의 연속하는 각 뉴런의 자극제는 미래 보상과 보다 밀접하게 연관되고, 즉각적인 감각 세계와는 보다 덜 밀접 하게 연관된다. 더 나아가 감각 주변부로부터의 뉴런은 더 많은 정보를 가지며, 미래 보상에 대한 불확실성을 덜 갖는다. 이러한 현상은 막막으로부터 피질을 통하여 운동 뉴런에 이르는 긴 경로를 추적함으로써 예시될 수 있다. 시각 시스템에서 연속하는 뉴런의 자극제는 광 강도의 작은 원으로부터 배향된 바로 변환되며, 마침내 얼 굴로 변환된다. 양상에 상관없이 측두엽 및 전두엽 피질에서 보다 높은 뉴런은 관련된 정보에 반응한다. 이 경 로에서 더 나아가, 뉴런은 미래 보상에 대하여 보다 선택적이지만, 특정 수족 및 근육에 특별히 관련된 자기수 용성 전정(vestibular)정보를 통합시킴으로써 뉴론들은 보다 운동성을 가질 수 있다. 경로의 최종 뉴런으로, 운 동 뉴런에 의하여 예측된 자극제는 대략적으로 행동 계획에 대응될 것이다. 모든 업스트림 시냅스에서 보상 피 드백의 누적 효과는 운동 뉴런의 자극제를 결정하기 때문에, 운동 뉴런은 업스트림 뉴런의 어떤 것보다 미래 보 상에 관하여 낮은 불확실성을 갖는다. 유사하게, 운동 뉴런은 시스템의 결정을 만들게 된다. 뉴런 및 이것의 자극제에 의하여 보유된 보상 정보의 양은 또한 감각 양상에 따라 상이할 수 있다. 예를 들면, 맛은 광도 보다는 미래 보상과 더 강하게 관련되어 있기 때문에, 혀의 미각세포는 광수용체 보다 더 많은 정보 와 더 적은 미래 보상에 대한 불확실성을 가진다. 마찬가지로, 감각세포는 광수용체 보다는 (즉, 더 적은 시냅 스에 의해 분리된) 운동 뉴런에 더 가깝다. 감각 경로와 시각 경로가 모두 동일한 운동 뉴런에 모이면, 미래 보 상에 대한 불확실성에 동일한 감소를 야기한다. 그러나, 광도는 미래 보상의 더 적은 예측자이기 때문에, 긴 경 로의 시각계는 보상 불확실성에 동일한 감소를 야기시키기 위해서 짧은 경로의 감각계 보다 더 많은 일을 해야 한다. 그러나, 광도로부터 보상 정보를 추출하기 위한 실제적인 일을 함에 있어서, 긴 경로의 시각계는 대체로 세계에 대한 불확실성에 훨씬 더 큰 감소를 달성한다. 지능과 유용한 출력을 생성하도록 학습하는 이들 뉴런의 네트워크 능력은 대체로 하나 이상의 보상 피드백 신호 의 존재에 의존한다. 일 구체예에서, 보상 피드백의 적합한 형태는 한정되어 있지 않다. 오히려 인공뉴런은 네 트워크의 정보가 그것의 목적과 관련되도록 보상 피드백을 이용한다. 보상 피드백이 없는 경우, 이들 뉴런의 네 트워크는 예측하기 어려운 세계의 측면의 상태를 예측하도록 학습한다. 예를 들면, 이러한 네트워크는 시각 세 계를 보고 이해하고 예측하도록 학습할 수 있다. 이것은 물체를 인식하도록 학습할 수 있지만, 생체계와 다르게 다른 것들보다 더 중요한 어떤 물체를 보지는 못한다. 이러한 네트워크는 외부 관찰자에게 정보를 제공하므로 유용하지만, 생체계를 닮은 목적-관련 운동 출력을 생성하기에는 적합하지 않다. 이들 뉴런의 네트워크를 설계하는데 중요한 이슈는 보상 피드백의 형태에 관한 것이다. '보상 피드백'은 네트워 크의 목적이 얼마나 잘 달성되는지에 대한 정보를 말한다. 식 4에서 나타낸 바와 같이, Hebbian 유연성에 대한그것의 영향을 통하여, 이 보상 피드백은 뉴런과 네트워크가 획득하는 정보를 결정할 것이다. 보상 피드백은 모 든 네트워크의 정보가 네트워크의 목적과 관련된다는 것을 확인시키도록 돕는다. 상기에 몇 가지 예가 제공되었 지만, 보상 피드백의 형태는 이들에 한정되지 않는다. 그것은 단순히 흥분성 또는 억제성 보다는 신경전달물질 도파민과 유사한 조절 효과를 가진 네트워크 내에 특별한 뉴런으로부터 비롯될 수 있다. 그렇지 않으면, 그것은 네트워크의 외부 공급원으로부터 제공될 수 있는데, 이 경우 네트워크는 지도 방식으로 학습한다고 한다. 따라 서, 보상 피드백에는 상이한 많은 구체예가 있다. 이들 뉴런의 중요한 측면은 네트워크의 목적과 가장 관련된 세계의 측면을 예측하기 위하여 무슨 보상 피드백이 제공되든지 간에 이를 이용할 수 있다는 것이다. 생물학적 신경 네트워크는 동물의 목적에 잠재적 관련성이 있는 세계의 측면들만을 고려한다. 그러나, 어떤 특 정 행동의 목적의 부재시 세계의 상태를 예측하는 것에 관심이 있는 인공 네트워크를 상상할 수 있다(즉, 이해 만을 위한 이해). 따라서, 일 구체예에서, 아무런 보상 피드백이 제공되지 않는다. 이러한 네트워크가 어떻게 작동할 수 있는지를 상상하기 위해서, 유용한 비유가 시각계에 적용될 수 있다. 시각계의 말기는 보상 관련 정 보를 위하여 강하게 편향된다. 보상 피드백의 영향 하에서, 시각계는 대체로 바위와 같은 물체를 무시하면서 얼굴의 상세한 공정을 수행하도록 학습한다. 그러나 초기 시각계는 보상과의 관련성과 상관없이 모든 종류의 물 체를 인식 및 구별하는 것을 학습한다. 보상 피드백의 부재시에 학습하는 이들 뉴런의 네트워크는 비슷하게 물 체를 인식 및 구별하도록 예상된다. 예를 들면, Hebbian 유연성은 뉴런이 명암의 연장된 부위에 선택적이 되도 록 야기할 수 있다고 알려져 있다. 따라서, Hebbian 유연성은 일차 시각령(visual cortex)의 단순 세포의 자극 선택성을 설명할 수 있다. 따라서, 보상 피드백이 결핍된 네트워크는 보는 것을 학습할 수 있다(즉, 시각 세계 의 상태를 예측). 시뮬레이션 본 발명의 일 구체예에서, 하나 이상의 네트워크는 컴퓨터를 이용하여 시뮬레이션된다. 이러한 시뮬레이션의 결 과는 본 발명에서 제공된다. 종래 연구들은 Hebbian 또는 anti-Hebbian 시냅틱 유연성 규칙이 어떻게 네트워크를 형성할 수 있는지를 증명하 였다(비록 이들 법칙의 제시된 조합이 시뮬레이션되지는 않았지만). 본 발명의 구체예는 이와 동일한 종류의 유 연성 규칙을 비-시냅스 이온 채널의 선택성에 적용하도록 확대시킨다. 이 시뮬레이션에서, 비-시냅스 이온 채 널의 스펙트럼 중에서 선택된 단일 뉴런이 도 3A-H에 도시되어 있다. 시뮬레이션은 단일 구획, 등급 전위(즉, 비 활동전위), Hodgkin-Huxley 타입 모델 뉴런에 기초한다. 뉴런은 Hebbian 법칙(식 4)을 통하여 글루타메이트-게이트 양이온 채널의 4가지 서브타입과, anti-Hebbian 법칙(식 3) 을 통하여 전압-조절 칼륨 채널의 9개 서브타입 중에서 동시적으로 선택된다. 채널 활성(즉, 개발 가능성, 식 2-4에서 “U”)은 도 3C, 3E, 및 3G에 도시된 바와 같이 0 과 1 사이에서 연속적으로 다양하다. 각 유형의 채널 수(식 2-4에서 “W”)는 임의의 양의 실수를 가정할 수 있다. 각 채널의 단위 전도도는 1이었다. 식 3 및 4의 유연성 규칙을 위하여, θ가 칼륨과 비선택 양이온 역전위 사이에 중간인 -50 mV가 되도록 선택하 였다. 속도상수는 식 3의 anti-Hebbian 규칙과 식 4의 Hebbian 규칙에서 각각 0.1과 1.0이었다. 각 규칙에서, b는 0.0000001이었다. 글루타메이트 농도의 특정 패턴 외에, 유연성 속도는 시뮬레이션에서 유일한 자유 매개변 수였다. 다른 매개변수는 채널 서브타입의 경우에, 알려진 물리적 수치의 근사치를 계산하거나, 관련 동역학 또 는 글루타메이트 친화도의 스펙트럼을 포괄하도록 선택하였다. Hebbian 규칙에 대한 anti-Hebbian 규칙에서 a의 더 큰 값은 Hebbian 규칙의 양성 피드백에서 기인한 글루타메이트-게이트 채널의 수의 증가를 제한하도록 선택 하였다(anti-Hebbian 규칙의 음성 피드백을 더 빠르게 함으로써). “수동” 붕괴속도(b)는 “능동” 유연성 속 도(a) 보다 더 작게 선택하였으며, 결과적으로 채널 활성의 변화 속도와 비교하여 느리도록 선택하였다. 각 단계(time step)에서, 글루타메이트 농도는 도 3A에 도시된 바와 같이 평균 20%의 표준편차로 가우스 분포 (Gaussian distribution)에서 유도하였다. 평균 농도는 2000에서 시작한 10 단계와 2200에서 시작한 500 단계 에서 50 μM 에서 1000 μM으로 증가하였다. 5000 단계 후, 패턴을 총 20,000 사이클 반복하였다. 초기에는, 4 개의 서브타입 중에 균등하게 분리된 총 800 글루타메이트-게이트 비선택 양이온 채널 과, 9개의 서브타입 중에 균등하게 분리된 800 전압-게이트 K+ 채널이 있었다. 상이한 수와 비율의 채널로 2가지 다른 시뮬레이션을 시작 하였다(나타내지 않음). 각 서브타입의 채널의 최종 수는 시작 숫자와 상관없이 세가지 시뮬레이션 모두에서 동 일하였다. 4가지 타입의 글루타메이트-게이트 채널은 글루타메이트에 대한 친화도(KD)가 상이했다. 이것은 단일 사이클에서 각 채널 타입의 활성(즉, 개방 가능성)을 나타내는 도 3C에서 확인할 수 있다. 중간 친화도의 서브타입은 뉴런 이 노출된 실제 범위의 글루타메이트 농도에 가장 민감하였다. 이들 활성은 더 다양하고 덜 예측가능하며, 따라 서 이들 중 하나(KD = 1000 μM)가 식 4의 Hebbian 유연성 규칙에 의해 선택된 우세한 서브타입이었다. 도 3D는 각 타입의 글루타메이트-게이트 채널의 수(즉, 식 4에서 가중치 w)를 사이클 수의 함수로 나타내며, 각 사이클 의 값은 사이클에서 모든 5000 시점에 걸친 평균 가중치이다. Hebbian 유연성 규칙은 가장 유리한 채널 타입, 전압의 예측 오류 또는 편차를 최대화하고, 다른 것들을 없애는 것들을 선택하였다. 학습이 진행되고 더 높고 더 낮은 친화성 수용체가 제거되면서(도 3D), 뉴런의 막 전위는 글루타메이트 농도에 더 민감하게 되었고, 따라 서 보다 가변적이었다. 이는 도 3B에 도시되어 있는데, 마지막 사이클 동안 막 전압을 나타내는 하위 자취 (trace)가 첫 번째 사이클 동안 막 전압을 나타내는 상위 자취에 비해서 매우 가변적이다. 뉴런의 과거 정보(즉, 제2부류 입력)는 9가지 서브타입의 칼륨 채널을 포함하였다. 이들 중 4가지는 타입 1 채 널이고, 5가지는 타입 2 채널이다. 타입 1 채널은 평형시 -40mV에서 최대의 반(half-maximal)의 활성화를 가진 단일 2 상태 전압 센서에 의해 게이트되며, 따라서 각 순간에 이들의 활성은 단순히 과거 전압의 지수함수였다. 마지막 사이클 동안 각 타입의 채널의 활성은 도 3E에 도시되어 있다. 4가지 서브타입은 10 내지 333 시간 단위 의 -40 mV에서의 시간 상수 t를 비롯해서 동역학 특성이 상이했다. 첫번째 약 1000 사이클 동안, 각 타입의 칼 륨 채널의 수는 식 3에서 막 전압이 θ (-50 mV) 이상으로 거의 항상 탈분극되었기 때문에 그것의 시작 값 89로 부터 증가하였다. 이것은 총 칼륨 전도도가 막전압을 평균 오류가 0인 정지점(null point) θ가까이 이르게 하 는데 충분할 때가지 계속되었다. 식 3의 anti-Hebbian 규칙은 도 3F에 도시된 바와 같이 가장 빠른 동역학을 가 진(τ = 10 시점) 채널 타입을 선택하였다. 이 채널은 막 전압의 예측뿐만 아니라 글루타메이트 농도 예측시에 가장 최근의 과거가 더 먼 과거 보다 더 낫기 때문에 적합하였다. 이것은 막 시간 상수에 의해 도입된 막 전압에서 작은 범위의 상관관계와, 평균 글루타메이트 농도의 단계 변화 때문에 사실이다. 가장 빠른 동역학의 채널은 예측을 더 빠르게 적응할 수 있으며, 따라서 오류를 최소화할 수 있다. 실제로, 많은 실제 전압-게이트 칼륨 채널은 빠른 동역학을 가진다. 그러나, 가장 최근의 과거가 항상 현재의 가장 좋은 예측자인 것은 아니다. 마찬가지로, 글루타메이트 농도가 가우스 변이(Gaussian variation)를 나타냈 지만 다른 일시적 패턴이 결핍되었을 때, 채널은 가장 긴 기간 동안의 과거 전압을 평균화함으로써 가장 좋은 예측을 하기 때문에 anti-Hebbian 규칙은 가장 느린 동역학을 가진 채널 타입을 선택하였다(나타내지 않음). 타입 1 칼륨 채널은 글루타메이트의 두 번째 펄스가 첫 번째 펄스에 발생했을 때 예측할 수 없었지만, 원칙적으 로 일부 타입의 채널은 예측할 수 있다. 타입 2 칼륨 채널은 이 목적을 위하여 설계되었다. 타입 2 칼륨 채널의 각 5가지의 서브타입은 전술한 바와 같이 8개의 센서에 의해 게이트되었다. 상기 서브타입은 도 3G에 도신된 바와 같이 동역학 특성이 상이했다. 복수 센서에 의한 각 타입 2 채널의 게이팅(gating)은 단일 센서 타입 1 채 널에 비해서 훨씬 더 실제 채널과 같았다. 막 전압이 -25mV 이상으로 탈분극된 후 임의의 기간동안 채널이 열리 도록 채널 게이팅 및 센서 동역학의 규칙을 선택하였다. 거의 첫번째 2000 사이클 동안, 이 역치에는 이르지 않 았고, 따라서 타입 2 채널의 각 서브타입 수는 식 3의 수동 붕괴 구간 때문에 감소하였다(도 3H). 결국, 막 저 압은 더 민감한 글루타메이트-게이트 이온 채널의 선택 때문에 글루타메이트 농도에 더 민감하게 됨에 따라, 글 루타메이트의 첫 번째 펄스는 -25mV 이상으로 막을 탈분극시키기에 충분하게 되어 타입 2 채널을 활성화시켰다. 그 이후, anti-Hebbian 규칙은 글루타메이트의 두 펄스 간의 실제 간격(t =100)을 거의 일치시킨 동역학을 가진 채널 서브타입을 선택하였다(도 3H). 이 서브타입은 글루타메이트의 두 번째 펄스 전에 간단한 과분극(즉, 음성오류)을 야기함에도 불구하고, 글루타메이트-유래 탈분극(즉, 양성 오류)을 대응시키기 때문에 선택되었다(도 3B, 하위 자취). 따라서 이 타입의 채널은 글루타메이트의 첫 번째 펄스를 이용하여 글루타메이트의 두 번째 펄 스에 의해 야기된 과분극을 예측 및 대응시킬 수 잇다. 뉴런의 함수는 그것에 사용된 정보 공급원(즉, 채널 타입 또는 시냅스)에 의존한다. 이 실시예는 단지 설명하기 위해서 제공되는 것이며, 뉴런에 사용가능한 정보 공급원의 종류를 제한하는 것을 의미하지는 않는다. 컴퓨터 판독가능 매체 일 구체예에서, 인공뉴런은 의뢰인에 저장된 소프트웨어 프로그램을 이용하여 생성된다. 도 4는 인공뉴런 의 네트워크를 생성하기 위한 예시적 구성을 설명하는 간단한 도표이다. 예시적 구성은 의뢰인(예를 들면, 컴퓨터와 같이 의뢰인 장치로서 역할을 하도록 설정된 컴퓨팅 플랫폼), 디지털 미디어 플레이어, 개인용 디지털 보조자, 또는 RAM(random access memory) 또는 전자 프로세서에 결합된 자기 또는 광학 매체와 같이 컴퓨 터-판독가능 매체을 포함하도록 설정된 휴대폰이다. 프로세서는 컴퓨터-판독가능 매체에 저장된 프로그램 지시를 실행한다."}
{"patent_id": "10-2010-7011168", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "본 기술분야의 통상의 지식을 지닌 당업자에게 이해되는 바와 같이, 본 발명은 그것의 사상 또는 필수적 특징을 벗어나지 않고 다른 특정 형태로 구현될 수 있다. 마찬가지로, 구성, 특징, 속성 및 다른 양태의 특정 명명 및 분류는 필수적이거나 중요하지 않으며, 본 발명을 수행하는 메커니즘 또는 그것의 특징은 다른 이름, 분류 및/ 또는 형식을 가질 수 있다. 따라서, 본 발명의 개시는 하기 청구범위에 나타난 본 발명의 범위를 한정하려는 것 이 아니라 설명하기 위한 것이다."}
{"patent_id": "10-2010-7011168", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1A-1D는 종래 기술에서 알려진 바와 같이 글루타메이트의 제곱 파 펄스(square wave pulse)에 대한 단순화된 뉴런의 멤브레인 전압 및 전도도 반응이며, 여기에서 뉴런은 글루타메이트-게이트된 양이온 채널 및 전압-게이 트된 칼륨 채널을 갖는다; 도 2는 단일 인공 뉴런에 대한 정보 공급원 또는 입력을 나타내며, 여기에서 각 화살표는 본 발명의 일 구체예 에 따라 개별 입력에 대한 정보 입력 흐름의 방향을 나타낸다; 도 3A-3H는 Hebbian-anti-Hebbian 규칙에 따라 상이한 이온 채널들 중에서 선택된 Hodgkin-Huxley 형 모델 뉴 런 단일 구획, 구배 전위차, 단일 분획 시뮬레이션 결과이다; 그리고 도 4는 본 발명의 일 구체예에 따른 컴퓨터 판독 개체를 나타내는 블록도이다."}
