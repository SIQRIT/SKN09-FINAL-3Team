{"patent_id": "10-2020-0113328", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0029692", "출원번호": "10-2020-0113328", "발명의 명칭": "비디오 영상에 보케 효과를 적용하는 방법 및 기록매체", "출원인": "주식회사 날비컴퍼니", "발명자": "이용수"}}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법에 있어서,상기 비디오 영상에 포함된 이미지로부터 상기 이미지의 특성 정보를 추출하는 단계;상기 이미지의 추출된 특성 정보를 분석하는 단계;상기 이미지의 분석된 특성 정보를 기초로 상기 이미지에 적용될 보케 효과를 결정하는 단계; 및상기 결정된 보케 효과를 상기 이미지에 적용하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 이미지의 추출된 특성 정보를 분석하는 단계는,상기 이미지 내의 객체를 탐지하는 단계;상기 이미지 내의 객체에 대응하는 영역을 생성하는 단계;상기 이미지 내에서의 객체에 대응하는 영역의 위치, 크기 및 방향 중 적어도 하나를 결정하는 단계; 및상기 객체에 대응하는 영역의 위치, 크기 및 방향 중 적어도 하나에 대한 정보에 기초하여 이미지의 특성을 분석하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 이미지 내의 객체는, 상기 이미지 내에 포함된 인물 객체, 얼굴 객체, 랜드마크 객체 중 적어도 하나의 객체를 포함할 수 있으며,상기 이미지 내에서의 객체의 위치, 크기 및 방향 중 적어도 하나를 결정하는 단계는 상기 이미지의 크기와 상기 객체에 대응하는 영역의 크기의 비율을 결정하는 단계를 포함하고,상기 객체의 위치, 크기 및 방향 중 적어도 하나에 대한 정보에 기초하여 이미지의 특성을 분석하는 단계는 상기 이미지 내에 포함된 객체의 포즈(pose)를 분류하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 이미지의 추출된 특성 정보를 분석하는 단계는,상기 이미지 내에 포함된 점근선(지평선) 및 소실점의 높이 중 적어도 하나를 탐지하는 단계; 및상기 탐지된 점근선 및 소실점의 높이 중 적어도 하나를 기초로 상기 이미지 내의 심도(depth) 특성을 분석하는공개특허 10-2021-0029692-3-단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 이미지에 적용될 보케 효과를 결정하는 단계는,상기 이미지의 분석된 특성 정보를 기초로 상기 이미지의 적어도 일부분에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 비디오 영상에 대한 보케 효과의 강도에 대한 입력 정보를 수신하는 단계를 더 포함하고,상기 이미지에 보케 효과를 적용하는 단계는,상기 수신된 강도에 대한 입력 정보에 기초하여, 보케 효과의 강도를 결정해 상기 이미지에 적용하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 결정된 보케 효과를 상기 이미지에 적용하는 단계는,상기 이미지 내에서 블러(blur) 효과가 적용될 영역들에 대응하는 서브 이미지들을 생성하는 단계;상기 블러 효과를 서브 이미지들에 적용시키는 단계; 및상기 블러 효과가 적용된 서브 이미지들을 혼합하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 이미지를 다운샘플링하여 상기 이미지의 해상도보다 낮은 해상도의 이미지를 생성하는 단계를 더포함하고,상기 이미지 내에서 블러 효과가 적용된 영역들에 대응하는 서브 이미지들을 생성하는 단계는 상기 낮은 해상도의 이미지에서 상기 서브 이미지에 대응하는 영역에 블러 효과를 적용하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,공개특허 10-2021-0029692-4-상기 블러 효과가 적용된 서브 이미지들을 혼합하는 단계는,상기 낮은 해상도의 이미지 및 상기 블러 효과가 적용된 영역들에 대응하는 서브 이미지들을 혼합하는 단계;상기 이미지의 해상도와 동일하도록, 상기 서브 이미지들이 혼합된 낮은 해상도의 이미지를 업샘플링(upsampling)하는 단계; 및상기 이미지 및 상기 업샘플링된 이미지들을 혼합하여 상기 업샘플링된 이미지의 선명도를 보정하는 단계를 포함하는,보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법에 있어서,(a) 복수의 영상 프레임에 대한 정보를 수신하는 단계;(b) 상기 복수의 영상 프레임에 대한 정보를 제1 인공신경망 모델에 입력하여 상기 복수의 영상 프레임 내에 포함된 하나 이상의 객체에 대한 세그멘테이션 마스크를 생성하는 단계;(c) 상기 복수의 영상 프레임에 대한 정보를 제2 인공신경망 모델에 입력하여 상기 복수의 영상 프레임에 대한깊이 맵을 추출하는 단계; 및(d) 상기 생성된 세그멘테이션 마스크 및 상기 추출된 깊이 맵을 기초로, 상기 복수의 영상 프레임에 대한 심도효과를 적용하는 단계를 포함하는,비디오 영상에 보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 (d) 단계는,상기 생성된 세그멘테이션 마스크를 이용하여 상기 추출된 깊이 맵을 보정하는 단계; 및상기 보정된 깊이 맵을 기초로 상기 복수의 영상 프레임에 대한 심도 효과를 적용하는 단계를 포함하는,비디오 영상에 보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 (a) 단계 내지 상기 (d) 단계의 각각은 복수의 이기종 프로세서 중 어느 하나의 프로세서에 의해실행되는, 비디오 영상에 보케 효과를 적용하는 방법."}
{"patent_id": "10-2020-0113328", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항 내지 제12항 중 어느 한 항에 따른 사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법을 컴퓨터에서 실행하기 위한 컴퓨터 프로그램이 기록된, 컴퓨터로 판독 가능한 기록 매체."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법에 관한 것이다. 보케 효과를 적용하는 방 법은, 비디오 영상에 포함된 이미지로부터 이미지의 특성 정보를 추출하는 단계, 이미지의 추출된 특성 정보를 분석하는 단계, 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과를 결정하는 단계, 결정된 보케 효과를 이미지에 적용하는 단계를 포함한다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 비디오 영상에 보케 효과를 적용하는 방법 및 기록매체에 관한 것으로, 보다 상세하게는, 컴퓨터 비 전 기술을 이용하여, 조리개의 지름이 큰 SLR(Single Lens Reflex Camera)이나 DSLR(Digital Single-Lens Reflex Camera)로 촬영해야 구현 가능한 포커싱, 아웃 포커싱, 보케 효과를 실시간 비디오 영상에 적용하는 방 법 및 기록매체에 관한 것이다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "실시간(real-time) 비디오 입력에서 인물이 등장하는 경우, 인물의 중심에 초점을 두고 영상이 촬영되고 있다고 가정하는 것이 일반적이며, 인물에 대한 정보를 추가 분석하는 것을 통해 Bokeh 효과를 자동으로 자연스럽게 만 들 수 있다. 실시간으로 Bokeh 효과를 적용하기 위해 알고리즘 연산 속도가 충분히 빨라야 하기 때문에, 원본보다 낮은 해상 도(resolution)에서의 처리(image processing)가 권장되지만, 초점이 맞는 부분 혹은 선명해야 할 것으로 결정 된 부분에서는 원본과 동일한 수준의 선명도를 갖는 것이 심미적으로 중요하다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상기와 같은 문제점을 해결하기 위한 비디오 영상에 보케 효과를 적용하는 방법 및 기록매체를 제공 한다. 모바일 카메라 등 실시간 비디오 입력이 들어오는 환경에서 실제 깊이(depth) 정보가 주어지지 않을 경우 다양 한 방법으로 Bokeh 효과를 구현하여 자연스러운 영상을 만들 수 있다. 이 중 Segmentation mask를 이용하여 object 영역은 선명하고 background 영역은 흐리게 효과를 가하여 DSLR과 같은 포커싱 및 아웃포커싱 효과를 만드는 간단한 방식들이 이미 상용화 되어 있지만, background 영역에 일관 된 흐림(Blurring) 효과를 주는 방식들이 대부분으로, 자연스럽지 않고 강한 흐림 효과를 주지 못한다는 단점이 있다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방법은, 비디오 영상에 포함된 이미지로부터 이미지의 특성 정보를 추출하는 단계, 이미지의 추출된 특성 정보를 분석하는 단계, 이미지의 분석된 특성 정보 를 기초로 이미지에 적용될 보케 효과를 결정하는 단계, 결정된 보케 효과를 이미지에 적용하는 단계를 포함한 다. 일 실시예에 따르면, 이미지의 추출된 특성 정보를 분석하는 단계는, 이미지 내의 객체를 탐지하는 단계, 이미 지 내의 객체에 대응하는 영역을 생성하는 단계, 이미지 내의 객체에 대응하는 영역의 위치, 크기 및 방향 중 적어도 하나를 결정하는 단계, 객체에 대응하는 영역의 위치, 크기 및 방향 중 적어도 하나에 대한 정보에 기초 하여 이미지의 특성을 분석하는 단계를 포함한다. 일 실시예에 따르면, 이미지 내의 객체는 이미지 내에 포함된 인물 객체, 얼굴 객체, 랜드마크 객체 중 적어도 하나의 객체를 포함할 수 있으며, 이미지 내에서의 객체의 위치, 크기 및 방향 중 적어도 하나를 결정하는 단계 는 이미지의 크기와 객체에 대응하는 영역의 크기의 비율을 결정하는 단계를 포함하고, 객체의 위치, 크기 및 방향 중 적어도 하나에 대한 정보에 기초하여 이미지의 특성을 분석하는 단계는 이미지 내에 포함된 객체의 포 즈(pose)를 분류하는 단계를 포함한다. 일 실시예에 따르면, 이미지의 추출된 특성 정보를 분석하는 단계는, 이미지 내에 포함된 점근선(지평선) 및 소 실점의 높이 중 적어도 하나를 탐지하는 단계 및 탐지된 점근선 및 소실점의 높이 중 적어도 하나를 기초로 이미지 내의 심도(depth) 특성을 분석하는 단계를 포함한다. 일 실시예에 따르면, 이미지에 적용될 보케 효과를 결정하는 단계는, 이미지의 분석된 특성 정보를 기초로 이미 지의 적어도 일부분에 적용된 보케 효과의 종류 및 적용 방식을 결정하는 단계를 포함한다. 일 실시예에 따르면, 비디오 영상에 보케 효과를 적용하는 방법은, 비디오 영상에 대한 보케 효과의 강도에 대 한 입력 정보를 수신하는 단계를 더 포함하고, 이미지에 보케 효과를 적용하는 단계는, 수신된 강도에 대한 입 력 정보에 기초하여, 보케 효과의 강도를 결정해 이미지에 적용하는 단계를 포함한다. 일 실시예에 따르면, 결정된 보케 효과를 이미지에 적용하는 단계는, 이미지 내에서 블러(blur) 효과가 적용될 영역들에 대응하는 서브 이미지들을 생성하는 단계, 블러 효과를 서브 이미지들에 적용시키는 단계 및 블러 효 과가 적용된 서브 이미지들을 혼합하는 단계를 포함한다. 일 실시예에 따르면, 결정된 보케 효과를 이미지에 적용하는 단계는, 이미지를 다운샘플링하여 이미지의 해상도 보다 낮은 해상도의 이미지를 생성하는 단계를 더 포함하고, 이미지 내에서 블러 효과가 적용된 영역들에 대응 하는 서브 이미지들을 생성하는 단계는, 낮은 해상도의 이미지에서 서브 이미지에 대응하는 영역에 블러 효과를 적용하는 단계를 포함한다. 일 실시예에 따르면, 블러 효과가 적용된 서브 이미지들을 혼합하는 단계는, 낮은 해상도의 이미지 및 블러 효 과가 적용된 영역들에 대응하는 서브 이미지들을 혼합하는 단계, 이미지의 해상도와 동일하도록, 서브 이미지들 이 혼합된 낮은 해상도의 이미지를 업샘플링(upsampling)하는 단계 및 이미지 및 업샘플링된 이미지들을 혼합하 여 업샘플링된 이미지의 선명도를 보정하는 단계를 포함한다. 일 실시예에 따르면, (a) 복수의 영상 프레임에 대한 정보를 수신하는 단계, (b) 복수의 영상 프레임에 대한 정 보를 제1 인공신경망 모델에 입력하여 복수의 영상 프레임 내에 포함된 하나 이상의 객체에 대한 세그멘테이션 마스크를 생성하는 단계, (c) 복수의 영상 프레임에 대한 정보를 제2 인공신경망 모델에 입력하여 복수의 영상 프레임에 대한 깊이 맵을 추출하는 단계 및 (d) 생성된 세그멘테이션 마스크 및 추출된 깊이 맵을 기초로, 복수 의 영상 프레임에 대한 심도 효과를 적용하는 단계를 포함한다. 일 실시예에 따르면, (d) 단계는, 생성된 세그멘테이션 마스크를 이용하여 추출된 깊이 맵을 보정하는 단계 및 보정된 깊이 맵을 기초로 복수의 영상 프레임에 대한 심도 효과를 적용하는 단계를 포함한다. 일 실시예에 따르면, (a) 단계 내지 (d) 단계의 각각은 복수의 이기종 프로세서 중 어느 하나의 프로세서에 의 해 실행된다. 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방법을 컴퓨터에서 실행하기 위한 컴퓨터 프 로그램이 기록된, 컴퓨터로 판독 가능한 매체가 제공된다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 일부 실시예들에 따르면, Segmentation mask를 이용하여, 입력 영상의 특성과 사용자의 강도 (intensity) 조절에 따라 자동으로 효과의 정도, 범위, 방식이 조절되는 비디오 영상에 보케 효과를 적용하는 방법 및 기록매체를 제공할 수 있다. 본 개시의 일부 실시예들에 따르면, 이미지 내의 포커싱할 물체에 대한 정보 및 이미지의 깊이 맵을 이용하여 이미지 내에 보케 효과를 적용하기 때문에, 이미지의 배경 영역에 심도 효과를 차등적으로 적용할 수 있을 뿐만 아니라 이미지 내의 객체 영역에서도 심도 효과를 차등적으로 적용할 수 있다. 본 개시의 효과는 이상에서 언급한 효과로 제한되지 않으며, 언급되지 않은 다른 효과들은 청구범위의 기재로부 터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 실시를 위한 구체적인 내용을 첨부된 도면을 참조하여 상세히 설명한다. 다만, 이하의 설명에 서는 본 개시의 요지를 불필요하게 흐릴 우려가 있는 경우, 널리 알려진 기능이나 구성에 관한 구체적 설명은 생략하기로 한다. 첨부된 도면에서, 동일하거나 대응하는 구성요소에는 동일한 참조부호가 부여되어 있다. 또한, 이하의 실시예 들의 설명에 있어서, 동일하거나 대응하는 구성요소를 중복하여 기술하는 것이 생략될 수 있다. 그러나 구성요 소에 관한 기술이 생략되어도, 그러한 구성요소가 어떤 실시예에 포함되지 않는 것으로 의도되지는 않는다. 개시된 실시예의 이점 및 특징, 그리고 그것들을 달성하는 방법은 첨부되는 도면과 함께 후술되어 있는 실시예 들을 참조하면 명확해질 것이다. 그러나 본 개시는 이하에서 개시되는 실시예들에 한정되는 것이 아니라 서로 다른 다양한 형태로 구현될 수 있으며, 단지 본 실시예들은 본 개시가 완전하도록 하고, 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에게 발명의 범주를 완전하게 알려주기 위해 제공되는 것일 뿐이다. 본 개시에서 사용되는 용어에 대해 간략히 설명하고, 개시된 실시예에 대해 구체적으로 설명하기로 한다. 본 개시에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 관련 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 개시에서의 단수의 표현은 문맥상 명백하게 단수인 것으로 특정하지 않는 한, 복수의 표현을 포함한다. 또 한 복수의 표현은 문맥상 명백하게 복수인 것으로 특정하지 않는 한, 단수의 표현을 포함한다. 본 개시의 전체에서 어떤 부분이 어떤 구성요소를 '포함'한다고 할 때, 이는 특별히 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있음을 의미한다. 본 개시에서, 용어 '모듈'은 소프트웨어 또는 하드웨어 구성요소를 의미하며, '모듈'은 어떤 역할들을 수행한다. 그렇지만 '모듈'은 소프트웨어 또는 하드웨어에 한정되는 의미는 아니다. '모듈'은 어드레싱할 수 있는 저장 매체에 있도록 구성될 수도 있고 하나 또는 그 이상의 프로세서들을 재생시키도록 구성될 수도 있다. 따라서, 일 예로서 '모듈'은 소프트웨어 구성요소들, 객체지향 소프트웨어 구성요소들, 클래스 구성요소들 및 태스크 구성요소들과 같은 구성요소들과, 프로세스들, 함수들, 속성들, 프로시저들, 서브루틴들, 프로그램 코드 의 세그먼트들, 드라이버들, 펌웨어, 마이크로 코드, 회로, 데이터, 데이터베이스, 데이터 구조들, 테이블들, 어레이들 및 변수들을 포함한다. 구성요소들과 '모듈'들은 안에서 제공되는 기능은 더 작은 수의 구성요소들 및 '모듈'들로 결합되거나 추가적인 구성요소들과 '모듈'들로 더 분리될 수 있다. 소프트웨어 또는 하드웨어 구성요소를 의미하며, '모듈'은 어떤 역할들을 수행한다. 본 개시에서, '시스템'은 서버 장치와 클라우드 서버 장치 중 적어도 하나의 장치를 지칭할 수 있지만, 이에 한 정되는 것은 아니다. 본 개시에서, '사용자 단말'은 통신 모듈을 구비하여 네트워크 연결이 가능하고, 웹사이트, 애플리케이션 등을 접속하여 컨텐츠 출력이 가능한 임의의 전자기기(예를 들어, 스마트폰, PC, 태블릿 PC, 랩톱 PC 등)를 포함할 수 있다. 사용자는 사용자 단말을 통해 사용자 단말의 인터페이스(예를 들어, 터치 디스플레이, 키보드, 마우 스, 터치펜 또는 스틸러스, 마이크로폰, 동작인식 센서 등)를 통한 입력에 의해 네트워크를 통해 접속가능한 임 의의 컨텐츠를 제공받을 수 있다. 도 1은 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방 법의 예시도이다. 도 1에서는, 사용자 단말이 스마트 폰으로 도시되었으나, 이에 한정되는 것은 아니며, 카메라를 구비하여 촬영을 할 수 있고, CPU(중앙처리장치, Central Processing Unit) 또는 GPU(그래픽처리장치, Graphics Processing Unit) 또는 NPU(신경망처리장치, Neural Processing Unit) 등의 컴퓨터 시스템을 통제하고 프로그 램의 연산을 실행할 수 있는 장치를 구비하고, 비디오 영상 컨텐츠 출력이 가능한 임의의 전자기기(예를 들어, PC, 태플릿 PC, 랩톱 PC 등)일 수 있다. 사용자는 사용자 단말의 인터페이스(예를 들어, 터치 디스플레이, 키보드, 마우스, 터치펜 또는 스틸러스, 마이크로폰, 동작인식 센서)를 통한 입력에 의해 비디오 영상에 적용될 보케 효과의 강도를 제어할 수 있다. 또 다른 예로서, 사용자 단말은 임의의 서버가 제공 하는 애플리케이션을 통해 비디오 영상에 보케 효과를 적용하는 서비스를 제공받을 수 있다. 도 1에 도시된 바와 같이, 사용자 단말에서 비디오 영상에 보케 효과를 적용할 수 있다. 일 실시예에서, 배경 영역은 흐림 효과가 적용되고, 인물 객체 영역인 전경 영역은 흐림 효과가 적용되지 않는, 실시 간으로 보케 효과를 처리하는 것을 녹화와 동시에 화면으로 확인할 수 있다. 도 2는 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방법의 순서도이다. 사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법에 있어서, 비디오 영상에 포함된 이미지로부터 이미지의 특성 정보를 추출하는 단계, 이미지의 추출된 특성 정보를 분석하는 단계, 이미지의 분석된 특성 정보를 기초로 이미지에 적 용될 보케 효과를 결정하는 단계 및 결정된 보케 효과를 이미지에 적용하는 단계를 포함할 수 있다. 단계 S210에서 보케 효과 적용 시스템은 비디오 영상에 포함된 이미지로부터 이미지의 특성 정보를 추출할 수 있다. 특성 정보는 영상 내 픽셀의 RGB 값 등의 영상으로부터 추출할 수 있는 정보를 의미할 수 있으며, 이에 한정되지 않는다. 단계 S220에서 보케 효과 적용 시스템은 이미지의 추출된 특성 정보를 분석할 수 있다. 예를 들어, 입력 영상 으로부터 추출한 특성 정보를 수신해, 입력 영상에 적용될 보케 효과의 종류와 강도를 결정하기 위한 특성 정보 를 분석할 수 있다. 단계 S230에서 보케 효과 적용 시스템은 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과를 결 정할 수 있다. 결정하는 보케 효과는 Flat Bokeh 또는 Gradient Bokeh 등을 포함할 수 있으며, 이에 한정되지않는다. 단계 S240에서 보케 효과 적용 시스템은 결정된 보케 효과를 이미지에 적용할 수 있는데, 상세한 구성은 아래 도 3 및 4을 참조하여 설명된다. 도 3은 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 시스템의 블록도이다. 도 3에 도시된 바와 같이 보케 효과 적용 시스템은, 촬상부, 입력부, 출력부, 처리부, 저장부 및 통신부를 포함할 수 있다. 일 실시예에서, 촬상부는, 보케 효과를 적용하기 위한 입력 영상을 촬영해 저장부로 송신할 수 있다. 촬상부는, 카메라 등을 구비하여 사진 또는 영상을 촬영할 수 있다. 카메라는 하나의 렌즈와 하나의 센서 를 단안 카메라 또는 두 개 이상의 렌즈와 센서를 갖는 카메라로 구성될 수 있으며, 이에 한정되지 않는다. 일 실시예에서, 입력부는, 보케 효과 적용부에서 입력 영상에 보케 효과를 적용하는데 있어서 적용할 보케 효과의 종류, 강도 및 강도 분포를 결정하기 위해, 사용자로부터 입력 강도를 수신할 수 있다. 일 실시예에서, 출력부는, 입력 영상에 대해 보케 효과를 적용한 영상을 저장부로부터 수신해 출력할 수 있다. 다른 실시예에서, 출력부는, 입력 영상에 대해 보케 효과를 적용한 영상을 출력해 실시간으로 확인할 수 있다. 일 실시예에서, 처리부는, 입력 영상으로부터 특성 정보를 추출하고, 추출한 특성 정보에 기초하여 특성 정보를 분석할 수 있고, 분석된 특성 정보를 기초로 보케 효과를 결정할 수 있다. 또한, 결정된 보케 효과와 사용자로부터 수신한 보케 효과의 강도에 대한 입력 정보에 기초해, 적용하는 흐림 효과의 강도 또는 흐림 효과 강도의 분포를 결정할 수 있는데, 처리부의 상세한 구성은 아래 도 4 및 도 11을 참조하여 설명된다. 일 실시예에서. 저장부는, 촬상부로부터 촬상된 영상을 저장하거나, 처리부에서 입력 영상에 보 케 효과를 적용하는 일련의 과정에서 생성한 영상들(예를 들어 서브 이미지, 혼합 이미지, 다운 샘플링 영상 등) 및 최종 출력 영상을 저장할 수 있다. 또한, 통신부로부터 수신한 외부 입력 영상 등을 저장할 수 있 다. 저장부는 저장부에 저장된 영상을 출력부를 통해 출력하거나, 처리부에서 입력 영상 에 보케 효과를 적용하기 위해 사용하는 영상들을 통신부를 통해 송신할 수 있다. 일 실시예에서, 통신부는 보케 효과 적용 시스템 내부의 데이터 교환 또는, 외부의 서버와 통신하여 영상 등의 데이터를 송신 및 수신할 수 있다. 다른 실시예에서, 통신부는, 임의의 서버가 제공하는 애플 리케이션을 통해 비디오 영상에 보케 효과를 적용하는 서비스를 수신할 수 있다. 도 4는 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 시스템의 처리부의 영상 처리 방법의 순서도이다. 처리부는 촬상부로부터 수신한 촬영 중인 영상 또는, 저장부로부터 수신한 사용자 단말에 저장되어 있는 영상을 입력 영상으로 수신하여, 보케 효과를 적용시켜 출력할 수 있다. 도 4에 도 시된 바와 같이 처리부는, 특성 정보 추출부, 특성 정보 분석부, 보케 효과 결정부 및 보 케 효과 적용부를 포함할 수 있다. 처리부는, 룰 기반(Rule-base) 알고리즘의 인공지능부터 머신 러 닝(Machine Learning), CNN(Convolutional Neural Network), RNN(Recurrent Neural Network) 및 DNN(Deep Neural Network) 등의 종래에 알려진 인공지능 기술로 구현될 수 있다. 일 실시예에서, 특성 정보 추출부는, 특성 정보 분석부가 입력 영상에 대한 특성 정보를 분석하기 위 해 필요한, 영상 내 픽셀의 RGB 값 등의 영상으로부터 추출할 수 있는 정보를 의미할 수 있으며, 이에 한정되지 않는다. 일 실시예에서, 특성 정보 분석부는, 특성 정보 추출부에서 추출한 특성 정보를 수신해, 입력 영상에 적용될 보케 효과의 종류와 강도를 결정하기 위한 특성 정보를 분석할 수 있다. 특성 정보 분석부에서 특 성 정보를 분석해 생성하는 분석된 이미지 특성 정보는, 이미지 내 객체, 이미지 내 객체에 대응하는 영역 (bounding box), 이미지 내의 객체의 에지(edge)에 대응하는 세그멘테이션 마스크(segmentation mask), 이미지 내 객체의 얼굴 영역(head bounding box), 얼굴 영역에서 사람 얼굴의 눈, 코 및 입 등의 얼굴 특징점(facial feature) 또는 랜드마크(land mark), 이미지 내 객체의 비율 및 포즈(pose), 이미지 내 객체의 접지 영역, 이미 지 내에 포함된 배경의 점근선(지평선 또는 수평선), 소실점 및 각 분석된 요소의 위치, 크기 및 방향 등의 computer vision 정보를 의미할 수 있으며, 이에 한정되지 않는다. 또한, 특성 정보 분석부의 상세한 구성은 아래 도 5 내지 7을 참조하여 설명된다. 일 실시예에서, 보케 효과 결정부는, 특성 정보 분석부로부터 분석된 영상의 특성 정보를 기초로 입 력 영상에 적용할 보케 효과를 결정할 수 있다. 보케 효과 결정부에서 결정하는 보케 효과는 Flat Bokeh 또는 Gradient Bokeh 등을 포함할 수 있으며, 이에 한정되지 않는다. 또한, 특성 정보 분석부는 Rule- base 기반 인공지능 또는 Classification task를 수행하는 간단한 인공신경망 등의 종래에 널리 알려진 인공지 능 기술을 통해 구성될 수 있으며, 이에 한정되지 않는다. 그리고, 보케 효과 결정부의 상세한 구성은 아 래 도 8 및 9를 참조하여 설명된다. 일 실시예에서, 보케 효과 적용부는, 사용자로부터 수신한 보케 효과의 강도에 대한 입력 정보에 기초해 Flat Bokeh의 강도 또는 Gradient Bokeh의 흐림 효과의 강도의 분포를 결정할 수 있다. 처리부는, 후술하 는 도 11에 도시된 바와 같이, 또한, 보케 효과 적용부의 서브 이미지 생성 모듈은, 입력 영상을 입 력 영상보다 낮은 해상도로 다운샘플링한 서브 이미지와 생성된 서브 이미지에 블러 효과를 적용한 서브 이미지 들을 저장부에 저장할 수 있다. 일 실시예에서, 보케 효과 적용부의 서브 이미지 혼합 모듈은, 입력 영상에 Gradient Bokeh 적용 시, 서브 이미지 생성 모듈에 의해 생성된 흐림 효과가 적용된 서브 이미지들을 혼합하여, 혼합 이미지를 저장부에 저장할 수 있다. 일 실시예에서, 보케 효과 적용부의 혼합 이미지 업샘플링 모듈은, 서브 이미지 혼합 모듈에 서 혼합된 낮은 해상도의 이미지를 업샘플링 할 수 있다. 일 실시예에서, 보케 효과 적용부의 선명도 보정 모듈은, 블러 효과를 적용하기 전의 원본 입력 영 상을 이용하여, 보케 효과 적용 영상의 선명도를 보정할 수 있다. 도 5는 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정을 설명하기 위한 예시도이다. 도 5에 도시된 바와 같이, 특성 정보 분석부는 특성 정보 추출부에서 추출한 특성 정보를 수신해, 입 력 영상에 적용될 보케 효과의 종류와 강도를 결정하기 위한 특성 정보를 분석할 수 있다. 일 실시예에서, 특성 정보 분석부는 특성 정보 추출부에서 추출한 특성 정보를 기초로, 이미지(510, 530) 내의 객체를 탐지하여, 이미지 내의 객체에 대응하는 영역(bounding box)(515, 535) 및 이미지 내의 객체에 대 응하는 세그멘테이션 마스크(segmentation mask)(525, 545)를 생성할 수 있다. CNN(Convolutional Neural Network), DNN(Deep Neural Network) 등의 널리 알려진 다양한 종류의 인공 지능 기술로 객체 탐지(object detection)해 생성할 수 있다. 일 실시예에서, 특성 정보 분석부는 이미지 내에서의 객체에 대응하는 영역의 위치, 크기 및 방향 중 적어 도 하나를 결정할 수 있다. 또한, 특성 정보 분석부는 객체에 대응하는 영역의 위치, 크기 및 방향 중 적 어도 하나에 대한 정보에 기초하여 이미지의 특성을 분석하여 보케 효과 결정부에 분석된 특성 정보를 송 신할 수 있다. 예를 들어, 특성 정보 분석부는 이미지 내에서 객체에 대응하는 영역의 크기가 전체 이미지 내 절반 (50%) 이상의 크기이고, 이미지 내에서 객체에 대응하는 영역이 이미지의 모서리에 붙어 있으면, 셀 피(selfie)인 것으로 이미지 특성을 결정할 수 있다. 예를 들어, 특성 정보 분석부는 이미지 내에서 객체에 대응하는 영역의 크기가 전체 이미지 내 절반 (50%) 이하의 크기이거나, 이미지 내에서 객체에 대응하는 영역이 이미지 모서리에 붙어 있지 않으면, 전신 샷(full body shot)인 것으로 이미지 특성을 결정할 수 있다. 일 실시예에서, 이미지 특성이 셀피 인지 전신 샷 인지 여부는 보케 효과 결정부에서 이미지에 적용할 보 케 효과의 종류를 결정하는데 사용될 수 있다. 도 6은 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정에서 객체의 포즈를 분류하는 과정을 설명하기 위한 예시도이다. 도 6에 도시된 바와 같이, 이미지 내의 객체는, 이미지 내에 포함된 인물 객체, 얼굴 객체, 랜드마크 객체 중 적어도 하나의 객체를 포함할 수 있다. 또한, 이미지 내에서의 객체의 위 치, 크기 및 방향 중 적어도 하나를 결정하는 단계는 이미지의 크기와 객체에 대응하는 영역의 크기의 비율을 결정하는 단계를 포함할 수 있다. 또한, 객체의 위치, 크기 및 방향 중 적어도 하나에 대한 정보에 기초하여 이미지의 특성을 분석하는 단계는 이미지 내에 포함된 객체의 포즈(pose)를 분류하는 단계를 포함할 수 있다.일 실시예에서, 이미지 내의 객체는, 이미지 내에 포함된 인물 객체, 얼굴 객체, 랜드마크 객체 및 중 적어도 하나의 객체를 포함할 수 있다. 랜드마크 객체는 이미지 내의 얼굴 객체에서 사람 얼굴의 눈, 코 및 입 등의 얼굴 특징점을 의미할 수 있다. 일 실시예에서, 특성 정보 분석부는, 이미지 내의 객체 및 얼굴 객체 위치, 크기, 방향 및 비율 중 적어도 하나에 대한 정보에 기초하여 이미지 내에 포함된 객체의 비율 및 포즈(pose)를 분류할 수 있다. 예 를 들어, 이미지 내의 객체 내에서 얼굴 객체의 위치, 크기, 방향 및 비율 정보를 기초로 이미지 내 의 인물이 서있는지 앉아있는지를 판단할 수 있다. 일 실시예에서, 이미지 내의 객체 내에서 얼굴 객체의 반대편에, 접지 영역이 존재한다고 추론 할 수 있다. 접지 영역 정보는 보케 효과 결정부에서 적용할 보케 효과의 강도를 결정하는데 사용될 수 있다. 예를 들어, 접지 영역은 배경 영역 중 이미지 내에 포함된 인물 객체와 가장 거리상 가까 운 영역으로 추론될 수 있다. 따라서, 보케 효과 결정부에서 배경 영역 중 블러 효과가 가장 적게 들어갈 영역으로 결정될 수 있다. 도 7은 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정에서 이미지 내에 포함된 점근 선(지평선) 및 소실점의 높이 중 적어도 하나를 탐지하여, 이미지 내의 심도(depth) 특성을 분석하는 과정을 설 명하기 위한 예시도이다. 도 7에 도시된 바와 같이, 이미지의 추출된 특성 정보를 분석하는 단계는, 이미지 내 에 포함된 점근선(지평선) 및 소실점의 높이 중 적어도 하나를 탐지하는 단계, 탐지된 점근선 및 소실점의 높이 중 적어도 하나를 기초로 이미지 내의 심도(depth) 특성을 분석하는 단계를 포함할 수 있다. 일 실시예에서, 특성 정보 분석부는 소실점을 탐지할 수 있는 배경 요소가 있는 이미지에서 소실점 을 탐지해, 보케 효과 결정부로 송신할 수 있다. 소실점은, 이미지 내 에지(edge) 성분들이 일 정한 범위에 교차하는 점을 의미할 수 있다. 원근법에 의해 물체는 관찰자 시점에 가까울수록 크게 투영되고, 관찰자 시점에서 멀어질수록 작게 투영되어, 물체는 점점 작게 투영되며, 이를 선으로 표현하면 관찰자 시점에 서 멀어질수록 선과 선이 만나, 소실점을 형성할 수 있다. 즉, 소실점은 하늘 영역을 제외하면 이미 지 내 카메라로부터 실제 거리가 가장 먼 영역으로 추론될 수 있다. 따라서, 보케 효과 결정부에서 소실 점은 배경 영역 중 블러 효과가 가장 많이 들어갈 영역으로 결정될 수 있다. 일 실시예에서, 특성 정보 분석부는 점근선(지평선)을 탐지할 수 있는 배경 요소가 있는 이미지에서 점근선(지평선 또는 수평선)을 탐지해, 보케 효과 결정부로 송신할 수 있다. 점근선 또한 소실 점과 마찬가지로 하늘 영역을 제외하면 이미지 내 카메라로부터 실제 거리가 가장 먼 영역으로 추론될 수 있다. 따라서, 보케 효과 결정부에서 점근선은 배경 영역 중 블러 효과가 가장 많이 들어갈 영역으로 결정 될 수 있다. 이하에서는, 도 8 및 9를 참조하여 본 발명의 일 실시예에 따른 보케 효과 결정부에서 이미지 특성에 따라 보케 효과를 결정하는 과정에 대하여 설명한다. 도 8 및 9에 도시된 바와 같이 이미지에 적용될 보케 효과를 결정하는 단계는, 이미지의 분석된 특성 정보를 기초로 이미지의 적어도 일부분에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 단계를 포함할 수 있다. 보케 효과 결정부는 classification task를 수행하는 간단 한 인공신경망으로 구현될 수 있다. 배경 영역의 흐림 효과의 강도의 경우, 검은색에 가까울수록 흐림 강도가 강한 영역으로 도시되었고, 흰색에 가까울수록 흐림 강도가 약한 영역으로 도시하였다. 도 8은 본 개시의 일 실시예에 따른 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 과정을 설명하기 위한 예시도이다. 보케 효과 결정부에서는 입력 영상에 적용 할 보케 효과를 결정할 수 있다. 보케 효과 결정부에서 결정하는 보케 효과의 종류는 Flat Bokeh 또는 Gradient Bokeh 등을 포함할 수 있다. 보케 효과 결정부는 Flat Bokeh의 흐림 강도, Gradient Bokeh의 흐림 강도의 분포를 결정할 수 있다. 도 8에 도시된 바와 같이 Flat Bokeh가 적용된 영상은 이미지에서 배경 영역이 일괄적으로 같은 강도의 흐 림 효과가 적용될 수 있다. Gradient Bokeh가 적용된 영상은 이미지에서 배경 영역이 수평 또는 수직 축 을 따라 흐림 강도가 다르게 적용될 수 있다. 일 실시예에서, 도 8에 도시된 바와 같이 수직 방향으로 Gradient Bokeh가 적용된 영상은 수평 방향의 흐 림 강도는 동일하게 적용될 수 있다. Gradient Bokeh에서는 배경 영역의 초점이 맞는 부분(또는 초점이 맞았다 고 분석된 부분)이 가장 흐림 효과를 적게 받거나 흐림 효과를 받지 않을 수 있다. Gradient Bokeh에서는 배경 영역의 실제 거리가 가장 먼 부분(또는 거리가 가장 멀 것이라고 분석된 부분)이 가장 흐림 효과를 강하게 받을수 있다. 일 실시예에서, 도 8에 도시된 입력 영상은 특성 정보 분석부로부터 영상 특성이 셀피인 것으로 결정 된 영상일 수 있다. 셀피 영상은 전체 영상에서 인물 객체가 차지하는 부분이 크고, 배경이 차지하는 영역이 작을 수 있다. 또한, 배경 영역 간의 카메라로부터의 거리 차이가 크지 않을 수 있다. 따라서, 블러 강도를 일괄적으로 처리하는 Flat Bokeh를 적용해도 보케 적용 영상이 자연스러워 보일 수 있다. 따라서, 보케 효과 결정부에서는 영상 특성이 셀피인 영상은 Flat Bokeh를 적용하는 것이 적절하다고 결정할 수 있다. 다른 실시예에서, 보케 효과 결정부는 Gradient Bokeh에 비해, 동작 속도가 빠르고 연산량이 적은 Flat Bokeh를 입력 영상에 적용시키고, 셀피 영상의 보케 처리에서 심미적으로 중요한 부분인 인물 영역이 더욱 선명하고 정교한 에지(edge)를 가질 수 있도록, 흐림 효과가 적용되지 않을 영역인 인물 영역에 대응하는 세그 멘테이션 마스크(Segmentation Mask)가 다시 산출될 수 있다. 도 9는 본 개시의 일 실시예에 따른 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 과정을 설명하기 위한 예시도이다. 보케 효과 결정부에서는 입력 영상에 적용 할 보케 효과를 결정할 수 있다. 일 실시예에서, 도 9에 도시된 입력 영상은 특성 정보 분석부로부터 영상 특성이 전신 샷인 것으로 결정된 영상일 수 있다. 전신 샷 영상은 전체 영상에서 인물 객체가 차지하는 부분이 적고, 배경이 차지하는 영역이 클 수 있다. 또한, 배경 영역 간의 카메라로부터의 거리 차이가 클 수 있다. 따라서, 블러 강도를 일 괄적으로 처리하는 Flat Bokeh가 적용된 영상은 부자연스러워 보일 수 있다. Gradient Bokeh가 적용된 영 상은 Flat Bokeh가 적용된 영상에 비해 블러 효과가 배경 영역의 심도(depth) 특성을 유지하므로 자 연스러워 보일 수 있다. 따라서, 보케 효과 결정부에서는 영상 특성이 전신 샷인 영상은 Gradient Bokeh 를 적용하는 것이 적절하다고 결정할 수 있다. 일 실시예에서, Gradient Bokeh가 적용된 영상은 특성 정보 분석부에서 분석한 소실점, 점근선 등의 영역은 배경 영역 중 실제 거리가 가장 멀거나, 거리가 가장 멀 것으로 분석된 부분이므로, 도 9에 도시된 바와 같이 흐림 효과가 가장 강한 영역으로 결정될 수 있다. 일 실시예에서, Gradient Bokeh가 적용된 영상은 특성 정보 분석부에서 분석한 접지 영역은 배경 영 역 중 실제 거리가 가장 가깝거나, 거리가 가장 가까울 것으로 분석된 부분이므로, 도 9에 도시된 바와 같이 흐 림 효과가 가장 약하거나, 흐림 효과를 주지 않을 영역로 결정될 수 있다. 도 10은 본 개시의 일 실시예에 따른 보케 비디오 영상에 대한 보케 효과의 강도에 대한 입력 정보를 수신하는 과정을 설명하기 위한 예시도이다. 도 10에 도시된 바와 같이, 보케 효과 적용 시스템은, 비디오 영상에 대한 보케 효과의 강도에 대한 입력 정보를 수신하는 단계를 더 포함할 수 있다. 그리고, 이미지에 보케 효과를 적 용하는 단계는, 수신된 강도에 대한 입력 정보에 기초하여, 보케 효과의 강도를 결정해 이미지에 적용하는 단계 를 포함할 수 있다. 배경 영역의 흐림 효과의 강도의 경우, 검은색에 가까울수록 흐림 강도가 강한 영역으로 도시되었고, 흰색에 가까울수록 흐림 강도가 약한 영역으로 도시되었다. 일 실시예에서, 보케 효과 적용부는 사용자로부터 수신한 보케 효과의 강도에 대한 입력 정보(1015, 102 5)에 기초해, Flat Bokeh의 강도를 결정할 수 있다. 예를 들어, 보케 효과 적용부는 흐림 강도가 낮은 입 력 정보를 수신하면, 배경의 흐림 강도가 약하게 적용된 영상을 출력할 수 있다. 흐림 강도가 높 은 입력 정보를 수신하면, 배경의 흐림 강도가 강하게 적용된 영상을 출력할 수 있다. 다른 실시예에서, 보케 효과 적용부는 사용자로부터 Gradient Bokeh에서 흐림 효과가 강한 영역의 강도 및 위치, 흐림 효과가 낮은 영역의 강도 및 위치를 결정해 이미지에 적용할 수 있다. 도 11은 본 개시의 일 실시예에 따른 보케 효과를 이미지에 적용하는 단계를 설명하기 위한 순서도이다. 도 11 에 도시된 바와 같이, 결정된 보케 효과를 이미지에 적용하는 단계는, 이미지 내에서 블러 효과가 적용될 영역 들에 대응하는 서브 이미지들을 생성하는 단계, 블러 효과를 서브 이미지들에 적용시키는 단계 및 블러 효과가 적용된 서브 이미지들을 혼합하는 단계를 포함할 수 있다. 또한, 이미지를 다운샘플링하여 이미지의 해상도보 다 낮은 해상도의 이미지를 생성하는 단계를 더 포함할 수 있고, 이미지 내에서 블러 효과가 적용된 영역들에 대응하는 서브 이미지들을 생성하는 단계는 낮은 해상도의 이미지에서 서브 이미지에 대응하는 영역에 블러 효 과를 적용하는 단계를 포함할 수 있다. 또한, 블러 효과가 적용된 서브 이미지들을 혼합하는 단계는 낮은 해상 도의 이미지 및 블러 효과가 적용된 영역들에 대응하는 서브 이미지들을 혼합하는 단계, 이미지의 해상도와 동 일하도록 서브 이미지들이 혼합된 낮은 해상도의 이미지를 업샘플링하는 단계, 이미지 및 업샘플링된 이미지들을 혼합하여 업샘플링된 이미지의 선명도를 보정하는 단계를 포함할 수 있다. 도 11에 도시된 바와 같이, 보케 효과 적용부는 서브 이미지 생성 모듈, 서브 이미지 혼합 모듈 , 혼합 이미지 업샘플링 모듈 및 선명도 보정 모듈을 포함할 수 있다. 일 실시예에서, 서브 이미지 생성 모듈은, 입력 영상에 Flat Bokeh 적용 시, 빠른 동작 속도를 확보하기 위해, 입력 영상을 입력 영상보다 낮은 해상도로 다운샘플링한 서브 이미지 및 다운샘플링한 이미지에 블러 효 과가 적용된 서브 이미지를 생성해 저장부에 저장할 수 있다. 객체 영역에 해당하는 세그멘테이션 마스크 내부 영역은 흐림 효과를 적용하지 않고, 생략하는 방식으로 흐림 효과에 필요한 연산량을 줄일 수 있다. 또한, 블러 처리 속도 향상 및 품질 향상을 위해, 블러 처리 전후에 일련의 image processing 단계가 추가될 수 있다. 일 실시예에서, 서브 이미지 생성 모듈은, 입력 영상에 Gradient Bokeh 적용 시, 빠른 동작 속도를 확보 하기 위해, 입력 영상을 입력 영상보다 낮은 해상도로 다운샘플링한 이미지 및 다운샘플링한 이미지에 블러 효 과를 적용한 서브 이미지를 생성해 저장부에 저장할 수 있다. 서브 이미지는 Gradient Bokeh의 픽셀의 위 치에 따라 달라지는 흐림 강도를 갖는 이미지를 생성하는 대신에, 특정한 흐림 강도를 갖는 Flat Bokeh 적용 이 미지들을 생성할 수 있다. 예를 들어, 흐림 강도가 1~3단계가 있다고 하면, 자연스러운 보케 효과를 위해 흐림 강도가 1단계인 영역과 흐림 강도가 2단계인 영역 사이의 영역들은 1단계 흐림 강도를 갖는 영상과 2단계 흐림 강도를 갖는 영상을 혼합하여, 단계적 차이(Gradation)가 있는 흐림 강도를 갖도록 생성할 수 있다. 즉, 2단계 이상의 흐림 강도를 갖는 영역들은 1단계 흐림 강도를 갖는 영상을 준비할 필요가 없을 수 있다. 따라서, K단 계의 흐림 효과를 적용한 영상은 K-1 보다 크고, K+1 보다 작은 흐림 효과를 갖는 영역에 대해서만 계산하는 방 식으로 연산량을 줄일 수 있다. 또한, 인물 영역에 해당하는 세그멘테이션 마스크 내부 영역은 흐림 효과를 적 용하지 않고, 생략하는 방식으로 흐림 효과에 필요한 연산량을 줄일 수 있다. 또한, 블러 처리 속도 향상 및 품질 향상을 위해, 블러 처리 전후에 일련의 image processing 단계가 추가될 수 있다. 다른 실시예에서, 서브 이미지 생성 모듈은, 서브 이미지를 생성하는 과정 중, 블러 효과를 적용하는 단 계에서 빠른 동작 속도를 확보하기 위해, 블러 커널 5x5 계산 시, 블러 커널 3x3을 두 번 연산해 적용하는 방식 을 사용할 수 있다. 또한, 빠른 동작 속도를 확보하기 위해, 블러 커널 3x3 계산 시, 블러 커널 1x3과 3x1을 합성하는 방식을 사용할 수 있다. 일 실시예에서, 서브 이미지 혼합 모듈은, 입력 영상에 Flat Bokeh 적용 시, 생략할 수 있다. 일 실시예에서, 서브 이미지 혼합 모듈은, 입력 영상에 Gradient Bokeh 적용 시, 서브 이미지 생성 모듈 에 의해 생성된 흐림 효과가 적용된 이미지들을 혼합해 저장부에 저장할 수 있다. 예를 들어, 1단 계 흐림 강도를 갖는 영역과 2단계 흐림 강도를 갖는 영역 사이의 영역들은 1단계 흐림 강도를 갖는 영상과 2단 계 흐림 강도를 갖는 영상을 선형적으로 혼합할 수 있다. 다른 실시예에서, 자연스러운 Gradient Bokeh 효과 구현을 위해, 서브 이미지 혼합 모듈은, 1단계 흐림 강도를 갖는 영상과 2단계 흐림 강도를 갖는 영상을, 2차함수 형태의 곡선으로 표현될 수 있는, 점진적 가중치 를 갖는 비율로 혼합할 수 있다. 일 실시예에서, 혼합 이미지 업샘플링 모듈은, 서브 이미지 혼합 모듈에서 혼합된 낮은 해상도의 이미지를 업샘플링 할 수 있다. 일 실시예에서, 선명도 보정 모듈은, 블러 효과를 적용하기 전의 원본 영상을 이용하여 보케 효과 적용 영상의 선명도를 보정할 수 있다. 낮은 해상도에서 흐림 효과를 적용하지 않은 커널(1x1)과 흐림 효과를 적용 한 커널(예를 들어, 3x3, 5x5 등)을 혼합하는 경우, 이를 높은 해상도로 업샘플링 했을 때 흐림 효과를 적용하 지 않았던 위치라도, 다운샘플링 및 업샘플링 과정에서 손실되는 픽셀 값으로 인해, 이미지가 흐려져 보일 수 있다. 낮은 해상도에서 흐림 효과를 적용한 영상을 높은 해상도로 다시 업샘플링 후 선명도를 높이기 위하여, 흐림 효과를 적용하지 않은 커널(1x)에 해당하는 위치(예를 들어 세그멘테이션 마스크 내부의 인물 객체 영역) 에 높은 해상도의 흐림 효과를 적용하지 않은 영상을 혼합해야 입력 영상의 선명도를 유지할 수 있다. 따라서, 해당 위치는 높은 해상도의 원본 입력 영상, 낮은 해상도의 흐림 효과를 적용하지 않은 영상, 흐림 효과가 적용 되지 않은 낮은 해상도의 영상, 3가지가 혼합되기 때문에, 혼합 비율을 잘 못 설정하면, 영상에 잡음이나 픽셀 값이 변하는 문제가 생길 수 있다. 따라서, 원본 영상에 제곱근 비율을 사용하여, 처음에 적용한 흐림 효과의 비율을 유지하면서, 선명도를 보정할 수 있다. 예를 들어, 흐림 효과 적용 영상과 원본 영상의 비율이 0.7:0.3으로 혼합되어야 하는 경우, 먼저, 흐림 효과 적 용 영상과 낮은 해상도의 흐림 효과를 적용하지 않은 영상의 혼합은 sqrt(0.7):1-sqrt(0.7)의 비율로 혼합되어 1차 혼합 영상이 생성될 수 있다. 그리고, 1차 혼합 영상이 업샘플링될 수 있다. 그리고, 1차 혼합 영상과 높 은 해상도의 입력 영상의 혼합은 sqrt(0.7):1-sqrt(0.7)의 비율로 수행될 수 있다. 이에 따라, 흐림 효과 적용 영상과 낮은 해상도의 흐림 효과를 적용하지 않은 영상과 높은 해상도의 입력 영상의 혼합 비율은 약 0.7:0.14:0.16일 수 있다. 도 12는 본 개시의 일 실시예에 따른 깊이 맵을 세그멘테이션 마스크를 이용하여 깊이 맵을 보정하는 과정을 설 명하기 위한 예시도이다. 본 개시의 일 실시예에 따른 사용자 단말에서 비디오 영상에 보케 효과를 적용하는 방법은, (a) 복수의 영상 프레임에 대한 정보를 수신하는 단계, (b) 복수의 영상 프레임에 대한 정보를 제1 인 공신경망 모델에 입력하여 복수의 영상 프레임 내에 포함된 하나 이상의 객체에 대한 세그멘테이션 마스크 를 생성하는 단계, (c) 복수의 영상 프레임에 대한 정보를 제2 인공신경망 모델에 입력하여 복수의 영상 프레임에 대한 깊이 맵을 추출하는 단계 및 (d) 생성된 세그멘테이션 마스크 및 추출된 깊이 맵 을 기초로, 복수의 영상 프레임에 대한 심도 효과를 적용하는 단계를 포함할 수 있다. 다른 실시예에서, 이에 제한되지 않고, 촬상부는 깊이 카메라를 포함할 수 있으며, 깊이 카메라를 통해 획 득한 깊이 맵을 이용하여, 복수의 영상 프레임에 대한 심도 효과를 적용할 수도 있다. 예를 들어, 깊이 카메라는 ToF(Time of Flight) 센서, 구조광(Structured light) 센서를 포함할 수 있으나, 이에 제한되지 않고 본 개시에서는 스테레오 비전 방식(예를 들어, 듀얼 카메라로 깊이 값 계산)으로 깊이 맵을 획득하는 경 우에도 추가로 구비된 카메라 및 복수의 카메라를 이용해 깊이 값을 계산하는 프로세서를 깊이 카메라로 지칭할 수 있다. 본 개시의 일 실시예에 따른 보케 효과 적용 시스템은, 이미지에 보케 효과를 적용하는 이미지 버전과 비디오 영상에 보케 효과를 적용하는 비디오 버전을 포함할 수 있다. 이미지에 보케 효과를 적용하기 위한 보케 효과 적용 시스템의 이미지 버전은, 입력 데이터(예를 들어, 이미지 또는 복수의 영상 프레임)가 입력된 후, 사용자 입력을 통해 초점의 위치를 변경할 때 및/또는 사용자 입력을 통해 블러(blur)의 강도를 변경할 때, 입력 데이 터에 대한 보케 효과 적용이 실시간으로 처리될 수 있다. 또한, 범용성을 위해 ARM CPU만을 이용하여 처리할 때에도, 입력데이터에 대한 보케 효과 적용이 실시간으로 처리될 수 있다. 이미지에 보케 효과가 적용되는 이미지 버전은, 사용할 블러 커널(kernel)들에 대한 필터링 이미지(filtering image)를 미리 구하는 단계 및 휴먼 마스크(human mask)를 고려하여 마스크(mask)의 가장자리(edge) 주변 영역 (예를 들어, Dilate한 영역과 Erode한 영역의 차이)에 대해 사람 영역과 배경(background) 영역이 서로 번지지 않도록 특수한 필터링을 한 값으로 채워주는 단계(예를 들어, 업데이트하는 단계)를 포함할 수 있다. 예를 들 어, 이미지 전체에 보케 효과를 적용한다면, 사람 영역과 배경 영역의 경계선이 번져 보일 수 있다. 이에 따라, 사람 영역과 배경 영역을 분리하여, 배경 영역에 사람 영역이 분리된 공간에 사람 영역의 경계선 안쪽에 배경 영역의 픽셀을 채워 넣고 블러 효과를 적용한 다음, 미리 분리한 사람 영역을 다시 합성하여, 실제 DSLR 카메라로 촬영한 것과 유사한 고품질의 보케 효과 이미지를 획득할 수 있다. 또한, 이미지 버전은, 초점이 맞춰진 영역의 평균 깊이 맵(average depth map)과 강도(intensity)를 고려하여 얻은 정규화된 깊이 맵(normalized depth map)의 값에 따라 각 픽셀(pixel) 값을 미리 준비한 여러 필터링 이미 지(예를 들어, 특수한 필터링을 한 값으로 채워진 이미지)를 이용하여 보간(interpolation)한 결과를 얻어내는 단계를 포함할 수 있다. 이와 같은 보케 방법을 이미지에 적용하면, 사용자가 입력하는 곳에 따라 초점을 변경 할 때 및/또는 사용자 입력을 통해 블러의 강도를 변경할 때, 매번 필터링 과정을 수행할 필요 없이 정규화된 깊이 맵을 획득하는 단계 및 필터링 이미지로부터 보간하는 단계를 수행할 수 있다. 또한, 특수한 필터링을 한 값은, 이미지의 선명도(sharpness)를 향상시킬 수 있는 임의의 값을 지칭할 수 있으며, 예를 들어, 샤프닝 (sharpening) 효과를 줄 수 있는 라플라시안 커널(Laplacian kernel)을 적용한 값을 포함할 수 있다. 일 실시예에서, 보케 효과 적용 시스템은, 입력 데이터가 이미지인 경우, 필터링(filtering)이 먼저 적용되고, 정규화된 깊이 맵에 따라 보간법(interpolation)이 적용될 수 있다. 여기서, 여러 커널의 크기 및 타입 중 적 어도 하나에 따라 필터링 이미지를 미리 생성해 놓을 수 있다. 예를 들어, 입력 데이터가 이미지인 경우, 하나 의 이미지에 다양한 필터를 미리 적용해 놓고, 필요한 효과에 따라 필터가 적용된 이미지를 블랜딩할 수 있다. 예를 들어, 필터링 커널 사이즈가 1, 3, 7, 15인 경우, 커널 사이즈 7 및 15의 필터링 결과물을 블랜딩하여 필 터링 커널 사이즈 11의 결과물과 유사한 결과물을 출력할 수 있다. 일 실시예에서, 보케 효과 적용 시스템은, 입력 데이터가 비디오 영상인 경우, 정규화된 깊이 맵에 따라 필터링 및 보간법이 동시에 이루어지는 방식을 수행할 수 있다. 이때, 보케 효과 적용 시스템은, 예를 들어, 하나의픽셀에 대해 필터링을 한 번만 수행할 수 있어, 필터링 커널의 사이즈를 더 촘촘하게 구성할 수 있다. 예를 들 어, 이미지 버전에서 커널의 사이즈를 1, 3, 7, 15를 사용한다면, 비디오 버전에서는 커널의 사이즈를 1, 3, 5, 7, 9, 11, 13, 15를 사용할 수 있다. 즉, 비디오는 짧은 시간에 여러 개의 이미지를 출력해야 하기 때문에, 이 미지 버전과 같이 복수의 필터가 적용된 이미지를 블랜딩하는 것 보다는, 필요한 필터를 생성해 두고 한 번에 적용하는 것이 성능상 유리할 수 있다. 다른 실시예에서, 보케 효과 적용 시스템을 구성하는 하드웨어의 성능에 따라, 이미지 버전 및 비디오 버전의 수행 방법이 상호 수행될 수 있다. 예를 들어, 성능이 낮은 하드웨어로 구성된 보케 효과 적용 시스템에서는 이미지 버전에서, 본래의 비디오 버전의 수행 방법이 실행될 수 있고, 성능이 높은 하드웨어로 구성된 보케 효 과 적용 시스템에서는 비디오 버전에서, 본래의 이미지 버전의 수행 방법이 실행될 수 있으나, 이에 제한되지 않고, 다양한 방식의 필터링 과정이 수행될 수 있다. 일 실시예에서, 비디오 영상에 보케 효과가 적용되는 비디오 버전은, 프레임 컬러(frame color), 깊이(depth), 마스크(mask)가 입력될 때마다 초점을 맞추는 지점과 블러 강도가 바뀔 수 있고, 처리율(throughput)이 동영상 장치에서 프레임을 처리하는 속도에 맞도록 설정될 수 있다(예를 들어, 30fps(frame per second) 또는 60fps). 예를 들어, 처리율이 30fps 또는 60fps에 맞추어 처리하기 위해 컴퓨팅 유닛(computing unit)을 이용하여 파이 프라인(pipeline) 기술이 적용될 수 있다. 비디오 영상에 보케 효과가 적용되는 비디오 버전은, 초점이 맞춰진 영역의 평균 깊이 맵(average depth map)과 강도(intensity)를 고려하여 얻은 정규화된 깊이 맵(normalized depth map)의 값에 따라 각 픽셀 값의 깊이 값 을 얻어내는 단계 및 각 픽셀의 깊이 값 등에 따라 각 픽셀에서 사용할 커널을 결정하고 필터링하는 단계를 수 행할 수 있다. 여기서, 예를 들어, 깊이 값이 0.6인 픽셀의 경우 4/7일 수 있다. 또한, 도 12에 도시된 바와 같이, (d) 단계는, 생성된 세그멘테이션 마스크를 이용하여 추출된 깊이 맵 을 보정하는 단계 및 보정된 깊이 맵을 기초로 복수의 영상 프레임에 대한 심도 효과를 적용하는 단계를 포함할 수 있다. 일 실시예에서, 도 12에 도시된 바와 같이, 제2 인공신경망을 통해 추출된 깊이 맵은 깊이 정보가 부정확 할 수 있다. 예를 들어, 이미지 또는 복수의 영상 프레임 내에 포함된 인물의 손가락과 같은 작고 세밀한 부분 의 경계가 모호할 수 있다. 또는, 상의와 외투의 색상 차이로 인해, 깊이 정보가 부정확하게 추출될 수 있다. 따라서, 본 개시의 일 실시예에 따른 보케 효과 적용 시스템은, 제1 인공신경망을 통해 생성된 세그멘테이션 마 스크를 이용해 세그멘테이션 마스크의 내부 및 외부의 깊이 정보를 각각 정규화 할 수 있다. 심도 효과를 적용함에 있어, 초점을 맞추고자 하는 부분이 포커싱할 물체를 탐지 및 분할하는 과정에서 얻어낸 분할 영역에 포함되지 않는 경우, 깊이 맵을 보정할 때 사용할 세그멘테이션 마스크가 사용될 수 있다. 깊이 맵을 보정하는 방법은, 초점을 맞출 분할 영역 내부의 깊이 맵의 범위를 정해진 범위 이내로 정규화 (normalize)하는 단계를 포함할 수 있다. 또한, 깊이 맵을 개선하는 과정은, 선택되지 않은 분할 영역 내부의 깊이 맵을 균질하게 만드는 단계를 포함할 수 있다. 예를 들어, 평균 값으로 통일하거나, 아래 수학식 1을 통 해 분산을 작게 만들거나, 평균 필터링(median filtering)을 적용할 수 있다. [수학식 1]"}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "또한, 깊이 맵에서 초점을 맞출 분할 영역 내부의 깊이 맵의 대표값(예를 들어, 평균)을 빼고, 절대값을 취하여 평균화하는 단계를 포함할 수 있다. 도 13 내지 16을 통해 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 서술한다. 일 실시예 에서, (a) 단계 내지 (d) 단계의 각각은 복수의 이기종 프로세서 중 어느 하나의 프로세서에 의해 실행될 수 있 다. 도 13 내지 16에 도시되어 있는 프로세서 A, 프로세서 B, 프로세서 C는 각각 간단한 전처리 작업과 데이터 수행 의 중재 역할을 할 수 있는 프로세서, 화면을 그리는 작업을 담당하는 프로세서, 뉴럴 네트워크 연산을 수행하 기에 최적화된 프로세서(예를 들어, DSP, NPU, Neural Accelerator 등)일 수 있으나 이에 한정되는 것은아니다. 또한, 일 예로, 프로세서 A는 CPU, 프로세서 B는 GPU(예를 들어, GL 인터페이스를 가진 GPU 등), 프로 세서 C는 DSP일 수 있으나, 이에 한정되지 않으며, 프로세서 A 내지 C의 각각은 프로세서 구성을 실행할 수 있 는 알려진 프로세서 중 하나를 포함할 수 있다. 도 13은 카메라로부터 촬상된 이미지 데이터가 프로세서 C로 바로 입력되는 시스템을 도시하고 있으나, 이에 한 정되지 않으며, 카메라 입력을 바로 수신할 수 있는 프로세서는 하나 이상의 프로세서로 입력되어 처리될 수 있 다. 또한, 도 14에서는 뉴럴 네트워크가 두 프로세서(프로세서 A 및 C)에서 수행되는 것으로 도시되어 있으나, 이에 한정되지 않으며, 여러 프로세서에서 병렬적으로 수행될 수 있다. 도 15에서는 각각의 작업이 각 프로세 서에서 처리되는 것으로 도시되어 있으나, 각 작업이 복수의 프로세서에서 단계적으로 나누어져 처리될 수 있다. 예를 들어, 복수의 프로세서가 한 가지 작업을 직렬적으로 함께 처리할 수도 있다. 도 16에서 처리되는 데이터의 흐름도는 일 실시예로서, 이에 한정되지 않으며, 프로세서의 구성 및 기능에 따라 다양한 방식의 데이 터 흐름도가 구현될 수 있다. 도 13은 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도시된 바와 같이, 프로세서 B는 촬상부로부터 프레임 이미지를 수신할 수 있다(S1340). 일 실시예에서, 프로세 서 B는 수신한 프레임 이미지의 전처리를 수행할 수 있다(S1342). 일 실시예에서, 프로세서 C는 프로세서 B가 프레임 이미지를 수신하는 것과 동시에 프레임 이미지 를 수신할 수 있다(S1350). 일 실시예에서, 프로세서 C는 제1 인공신경망 모델을 포함할 수 있다. 일 실시예에서, 프로세서 C는 제1 인공신경망 모델을 이용해 단계(S1350)에서 수신한 프레임 이미지에 대응 하는 세그멘테이션 마스크를 생성할 수 있다(S1352). 일 실시예에서, 프로세서 C는 제2 인공신경망 모델 을 포함할 수 있다. 프로세서 C는 제2 인공신경망 모델을 이용해 단계(S1350)에서 수신한 프레임 이미지 에 대응하는 깊이 맵을 생성할 수 있다. 프로세서 C는 생성된 세그멘테이션 마스크 및 깊이 맵을 프로세 서 A로 전송할 수 있다(S1356). 일 실시예에서, 프로세서 A는 프로세서 C로부터 세그멘테이션 마스크 및 깊이 맵을 수신할 수 있다 (S1360). 일 실시예에서, 프로세서 A는 수신한 세그멘테이션 마스크 및 깊이 맵을 프로세서 B에게 전송할 수 있다(S1362). 일 실시예에서, 프로세서 B는 프로세서 A로부터 세그멘테이션 마스크 및 깊이 맵을 수신할 수 있다 (S1364). 일 실시예에서, 프로세서 B는, 수신한 깊이 맵을 전처리 할 수 있다(S1370). 일 실시예에서, 프로세서 B는, 단계(S1342)에서 프로세서 B가 전처리한 이미지에 프로세서 A로부터 수신한 세그멘테이션 마스크, 단계(S1370)에서 프로세서 B가 전처리한 깊이 맵을 이용해 보케 필터를 적용할 수 있다(S1372). 일 실시예에서, 프로세서 B는 출력부를 통해 보케 필터를 적용한 결과물에 해당하는 프레임 이미지를 출력할 수 있다(S1374). 도 14는 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도시된 바와 같이, 프로세서 B는 촬상부로부터 프레임 이미지를 수신할 수 있다(S1410). 일 실시예에서, 프로세 서 B는 수신한 프레임 이미지의 전처리를 수행할 수 있다(S1412). 프로세서 B는 전처리된 이미지 를 프로세서 A로 전송할 수 있다(S1414). 또한, 프로세서 B는 전처리된 이미지를 프로세서 C로 전송할 수 있다(S1416). 일 실시예에서, 프로세서 A는 프로세서 B로부터 전처리된 이미지를 수신할 수 있다(S1420). 일 실 시예에서, 프로세서 A는 제2 인공신경망 모델을 포함할 수 있다. 프로세서 A는 제2 인공신경망 모 델을 이용하여 단계(S1420)에서 수신한 전처리된 이미지에 대응하는 깊이 맵을 생성할 수 있다(S1422). 일 실 시예에서, 프로세서 A는 단계(S1422)에서 생성된 깊이 맵의 전처리를 수행할 수 있다(S1424). 일 실시예에서, 프로세서 C는 프로세서 B로부터 전처리된 이미지를 수신할 수 있다(S1430). 일 실 시예에서, 프로세서 C는 제1 인공신경망 모델을 포함할 수 있다. 프로세서 C는 제1 인공신경망 모 델을 이용하여 단계(S1430)에서 수신한 전처리된 이미지에 대응하는 세그멘테이션 마스크를 생성할 수 있다 (S1432). 또한, 프로세서 C는 생성한 세그멘테이션 마스크를 프로세서 A에게 전송할 수 있다 (S1434). 일 실시예에서, 프로세서 A는 프로세서 C로부터 세그멘테이션 마스크를 수신할 수 있다. 일 실시 예에서, 프로세서 A는 단계(S1420)에서 수신한 전처리된 이미지에 단계(S1440)에서 수신한 세그멘테이션 마스크 및 단계(S1424)에서 프로세서 A에 의해 전처리된 깊이 맵을 이용해 보케 효과 필터를 적용할 수있다(S1442). 또한, 프로세서 A는 보케 필터 적용 결과를 프로세서 B에게 전송할 수 있다(S1444). 일 실시예에서, 프로세서 B는 프로세서 A로부터 보케 필터 적용 결과를 수신할 수 있다(S1446). 일 실시예에서, 프로세서 B는 출력부를 통해 보케 필터를 적용한 결과물에 해당하는 프레임 이미지 를 출력할 수 있다(S1450). 도 15는 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도시된 바와 같이, 프로세서 B는 촬상부로부터 프레임 이미지를 수신할 수 있다(S1510). 일 실시예에서, 프로세 서 B는 수신한 프레임 이미지의 전처리를 수행할 수 있다(S1512). 프로세서 B는 전처리된 이미지 를 프로세서 A로 전송할 수 있다(S1514). 일 실시예에서, 프로세서 A는 프로세서 B로부터 전처리된 이미지를 수신할 수 있다(S1520). 일 실 시예에서, 프로세서 A는 프로세서 C에게 전처리된 이미지를 전송할 수 있다(S1522). 일 실시예에 서, 프로세서 C는 프로세서 A로부터 전처리된 이미지를 수신할 수 있다(S1524). 일 실시예에서, 프로세서 C는 제1 인공신경망 모델을 포함할 수 있다. 프로세서 C는 제1 인공신경 망 모델을 이용하여 단계(S1524)에서 수신한 전처리된 이미지에 대응하는 세그멘테이션 마스크를 생성할 수 있 다. 일 실시예에서 프로세서 C는 제2 인공신경망 모델을 포함할 수 있다. 프로세서 C는 제2 인공 신경망 모델을 이용하여 단계(S1524)에서 수신한 전처리된 이미지에 대응하는 깊이 맵을 생성할 수 있다. 또한, 프로세서 C는 생성한 세그멘테이션 마스크 및 깊이 맵을 프로세서 A에게 전송할 수 있다 (S1534). 일 실시예에서, 프로세서 A는 프로세서 C로부터 세그멘테이션 마스크 및 깊이 맵을 수신할 수 있다 (S1540). 일 실시예에서, 프로세서 A는 단계(S1540)에서 수신한 깊이 맵을 전처리할 수 있다(S1542). 또한, 프로세서 A는 단계(S1540)에서 수신한 세그멘테이션 마스크 및 단계(S1542)에서 전처리한 깊이 맵 을 프로세서 B에게 전송할 수 있다(S1544). 일 실시예에서, 프로세서 B는 프로세서 A로부터 세그멘테이션 마스크 및 깊이 맵을 수신할 수 있다 (S1546). 일 실시예에서, 프로세서 B는 프로세서 A에 의해 전처리된 깊이 맵을 다시 전처리할 수 있다(S1550). 일 실시예에서, 프로세서 B는 단계(S1512)에서 프로세서 B에 의해 전처리된 이미지 에 단계(S1544)에서 수신한 세그멘테이션 마스크 및 깊이 맵을 이용해 보케 효과 필터를 적용할 수 있다 (S1552). 또한, 프로세서 B는 출력부를 통해 보케 필터를 적용한 결과물에 해당하는 프레임 이미지 를 출력할 수 있다(S1554). 도 16은 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 블록도이다. 도 16에 도시 된 입출력 인터페이스는 도 3에서 전술한 촬상부, 입력부 및 출력부를 포함할 수 있다. 예를 들어, 입출력 인터페이스는 촬상부를 통해 이미지 또는 복수의 영상 프레임을 획득할 수 있다. 또한, 입출력 인터페이스는 입력부를 통해 사용자로부터 초점의 위치 및/또는 보케 효과의 강도를 변경하는 입력을 수신할 수 있다. 또한, 입출력 인터페이스는 출력부를 통해 프로세서 A, 프 로세서 B 및 프로세서 C에 의해 생성된 보케 필터 적용 결과물을 출력할 수 있다. 일 실시예에서, 프로세서 B는 보케 커널을 포함할 수 있다. 예를 들어, 프로세서 B는 화면 을 그리는 작업을 담당하는 GPU로 구성될 수 있으나, 이에 제한되지 않는다. 보케 커널은 도 13 내지 도 16에서 전술한 바와 같이, 이미지 또는 복수의 영상 프레임에 세그멘테이션 마스크 및 깊이 맵을 이용해 보케 효과 필터를 적용할 수 있다. 일 실시예에서, 프로세서 A는 데이터 브릿지 및 전처리부를 포함할 수 있다. 예를 들어, 프 로세서 A는 간단한 전처리 작업과 데이터 수행의 중재 역할을 할 수 있는 CPU로 구성될 수 있으나, 이에 제한되지 않는다. 일 실시예에서, 간단한 전처리 작업은 사람과 배경의 경계선을 부드럽게 처리하는 블러(blur)처리, 깊이 맵 노 이즈 등의 신호 잡음을 제거하는 미디안(median) 필터, 깊이 맵의 빈 공간을 채우는 깊이 맵 완성(completion) 및 깊이 맵의 해상도 품질을 높이는 깊이 맵 업샘플링 등을 포함할 수 있으나, 이에 제한되지 않고 출력물의 품 질을 향상시킬 수 있는 다양한 전처리 작업을 포함할 수 있다. 다른 실시예에서, 보케 효과 적용 시스템(예: 보케 효과 적용 시스템)은 깊이 맵 완성, 깊이 맵 업샘플링 등의 간단한 전처리 작업을 위한 별도의 인공신경망 모델을 포함할 수 있다. 데이터 브릿지는 입출력 인터페이스, 프로세서 B 및 프로세서 C 사이의 데이터 수행의 중재 역할을 할 수 있다. 예를 들어, 프로세서 A에서 연산을 통해 프로세서 B 및 프로세서 C에서 처리할 작업의 분배를 수행할 수 있으나, 이에 제한되지 않는다. 전처리부는 입출력 인터페이스, 프로세서 B 및 프로세서 C로부터 수신한 이미지, 복수 의 영상 프레임 또는 깊이 맵의 전처리를 수행할 수 있다. 일 실시예에서, 프로세서 C는 세그멘테이션 네트워크 및 깊이 네트워크를 포함할 수 있다. 예를 들어, 프로세서 C는 뉴럴 네트워크 연산을 수행하기에 최적화된 프로세서인 DPS, NPU, Neural Accelerator 등으로 구성될 수 있으나, 이에 제한되지 않는다. 세그멘테이션 네트워크는 이미지, 복수의 영상 프레임, 전처리된 이미지 또는 복수의 영상 프레임을 입력 받아 세그멘테이션 마스크를 생성할 수 있다. 또한, 세그멘테이션 네트워크는 제1 인공신경망 모델을 포 함할 수 있으며, 도 17에서 보다 상세히 후술한다. 깊이 네트워크는 이미지, 복수의 영상 프레임, 전처리된 이미지 또는 복수의 영상 프레임을 입력 받아 깊 이 맵을 추출할 수 있다. 또한, 깊이 네트워크는 제2 인공신경망 모델을 포함할 수 있으며, 도 17에서 보다 상세히 후술한다. 도 17은 본 개시의 일 실시예에 따른 인공신경망 모델을 나타내는 예시도이다. 인공신경망 모델은, 머신 러닝(Machine Learning) 기술과 인지과학에서, 생물학적 신경망의 구조에 기초하여 구현된 통계학적 학습 알고 리즘 또는 그 알고리즘을 실행하는 구조이다. 일 실시예에 따르면, 인공신경망 모델은, 생물학적 신경망 에서와 같이 시냅스의 결합으로 네트워크를 형성한 인공 뉴런인 노드(Node)들이 시냅스의 가중치를 반복적으로 조정하여, 특정 입력에 대응한 올바른 출력과 추론된 출력 사이의 오차가 감소되도록 학습함으로써, 문제 해결 능력을 가지는 머신러닝 모델을 나타낼 수 있다. 예를 들어, 인공신경망 모델은 머신 러닝, 딥러닝 등의 인공지능 학습법에 사용되는 임의의 확률 모델, 뉴럴 네트워크 모델 등을 포함할 수 있다. 일 실시예에 따르면, 인공신경망 모델은 적어도 하나의 객체를 포함하는 복수의 영상 프레임 및/또는 이 러한 복수의 영상 프레임으로부터 추출된 이미지 피처를 입력하여 세그멘테이션 마스크를 출력하도록 구성된 제 1 인공신경망 모델을 포함할 수 있다. 일 실시예에 따르면, 인공신경망 모델은 적어도 하나의 객체를 포함하는 복수의 영상 프레임 및/또는 이 러한 복수의 영상 프레임으로부터 추출된 이미지 피처를 입력하여 깊이 맵을 출력하도록 구성된 제2 인공신경망 모델을 포함할 수 있다. 인공신경망 모델은 다층의 노드들과 이들 사이의 연결로 구성된 다층 퍼셉트론(MLP: multilayer perceptron)으로 구현된다. 본 실시예에 따른 인공신경망 모델은 MLP를 포함하는 다양한 인공신경망 모 델 구조들 중의 하나를 이용하여 구현될 수 있다. 도 17에 도시된 바와 같이, 인공신경망 모델은, 외부 로부터 입력 신호 또는 데이터를 수신하는 입력층, 입력 데이터에 대응한 출력 신호 또는 데이터 를 출력하는 출력층, 입력층과 출력층 사이에 위치하며 입력층으로부터 신호를 받아 특성을 추출하여 출력층으로 전달하는 n개(여기서, n은 양의 정수)의 은닉층(1730_1 내지 1730_n)으 로 구성된다. 여기서, 출력층은 은닉층(1730_1 내지 1730_n)으로부터 신호를 받아 외부로 출력한다. 인공신경망 모델의 학습 방법에는, 교사 신호(정답)의 입력에 의해서 문제의 해결에 최적화되도록 학습하 는 지도 학습(Supervised Learning) 방법과, 교사 신호를 필요로 하지 않는 비지도 학습(Unsupervised Learning) 방법이 있다. 처리부는 복수의 학습 영상 프레임으로부터 세그멘테이션 마스크 및/또는 깊이 맵을 출력하기 위하여 지도 학습(Supervised Learning)을 이용하여 복수의 입력 영상 프레임에 대한 분석을 수 행하고, 복수의 영상 프레임에 대응되는 세그멘테이션 마스크 및/또는 깊이 맵이 추론될 수 있도록 인공신경망 모델을 학습시킬 수 있다. 이렇게 학습된 인공신경망 모델은 저장부에 저장될 수 있으며, 통 신부 및/또는 입력부 수신된 적어도 하나의 객체를 포함하는 복수의 영상 프레임의 입력에 응답하여 세그멘테이션 마스크 및/또는 깊이 맵을 출력할 수 있다. 일 실시예에 따르면, 도 17에 도시된 바와 같이, 깊이 정보(예를 들어, 깊이 맵)를 추출할 수 있는 인공신경망 모델의 입력변수는, 적어도 하나의 객체를 포함한 복수의 학습 영상 프레임이 될 수 있다. 예를 들어, 인공신경망 모델의 입력층에 입력되는 입력변수는, 학습 이미지를 하나의 벡터 데이터 요소로 구성한, 이미지 벡터가 될 수 있다. 적어도 하나의 객체를 포함한 학습 이미지의 입력에 응답하여, 인공신경 망 모델의 출력층에서 출력되는 출력변수는 세그멘테이션 마스크 및/또는 깊이 맵을 나타내는 벡터 가 될 수 있다. 본 개시에 있어서 인공신경망 모델의 출력변수는, 이상에서 설명된 유형에 한정되 지 않으며, 변형가능한 3D 움직임 모델을 나타내는 임의의 정보/데이터를 포함할 수 있다. 이와 같이 인공신경망 모델의 입력층과 출력층에 복수의 입력변수와 대응되는 복수의 출력변 수가 각각 매칭되고, 입력층, 은닉층(1730_1 내지 1730_n) 및 출력층에 포함된 노드들 사이의 시냅 스 값이 조정됨으로써, 특정 입력에 대응한 올바른 출력이 추출될 수 있도록 학습될 수 있다. 이러한 학습 과 정을 통해, 인공신경망 모델의 입력변수에 숨겨져 있는 특성을 파악할 수 있고, 입력변수에 기초하여 계 산된 출력변수와 목표 출력 간의 오차가 줄어들도록 인공신경망 모델의 노드들 사이의 시냅스 값(또는 가 중치)를 조정할 수 있다. 이렇게 학습된 인공신경망 모델을 이용하여, 입력된 적어도 하나의 객체를 포 함한 복수의 영상 프레임에 응답하여, 복수의 입력 영상 프레임에 대응하는 세그멘테이션 마스크 및/또는 깊이 맵에 대한 정보가 출력될 수 있다. 본원에 기술된 기법들은 다양한 수단에 의해 구현될 수도 있다. 예를 들어, 이러한 기법들은 하드웨어, 펌웨어, 소프트웨어, 또는 이들의 조합으로 구현될 수도 있다. 본원의 개시와 연계하여 설명된 다양한 예시적 인 논리적 블록들, 모듈들, 회로들, 및 알고리즘 단계들은 전자 하드웨어, 컴퓨터 소프트웨어, 또는 양자의 조 합들로 구현될 수도 있음을 당업자들은 더 이해할 것이다. 하드웨어 및 소프트웨어의 이러한 상호교환성을 명 확하게 설명하기 위해, 다양한 예시적인 컴포넌트들, 블록들, 모듈들, 회로들, 및 단계들이 그들의 기능성의 관 점에서 일반적으로 위에서 설명되었다. 그러한 기능이 하드웨어로서 구현되는지 또는 소프트웨어로서 구현되는 지의 여부는, 특정 애플리케이션 및 전체 시스템에 부과되는 설계 제약들에 따라 달라진다. 당업자들은 각각의 특정 애플리케이션을 위해 다양한 방식들로 설명된 기능을 구현할 수도 있으나, 그러한 구현 결정들은 본 개시 의 범위로부터 벗어나게 하는 것으로 해석되어서는 안된다. 하드웨어 구현에서, 기법들을 수행하는 데 이용되는 프로세싱 유닛들은, 하나 이상의 ASIC들, DSP들, 디지털 신 호 프로세싱 디바이스들 (digital signal processing devices; DSPD들), 프로그램가능 논리 디바이스들 (programmable logic devices; PLD들), 필드 프로그램가능 게이트 어레이들 (field programmable gate arrays; FPGA들), 프로세서들, 제어기들, 마이크로제어기들, 마이크로프로세서들, 전자 디바이스들, 본원에 설명된 기능 들을 수행하도록 설계된 다른 전자 유닛들, 컴퓨터, 또는 이들의 조합 내에서 구현될 수도 있다. 따라서, 본원의 개시와 연계하여 설명된 다양한 예시적인 논리 블록들, 모듈들, 및 회로들은 범용 프로세서, DSP, ASIC, FPGA나 다른 프로그램 가능 논리 디바이스, 이산 게이트나 트랜지스터 로직, 이산 하드웨어 컴포넌 트들, 또는 본원에 설명된 기능들을 수행하도록 설계된 것들의 임의의 조합으로 구현되거나 수행될 수도 있다. 범용 프로세서는 마이크로프로세서일 수도 있지만, 대안에서, 프로세서는 임의의 종래의 프로세서, 제어기, 마 이크로제어기, 또는 상태 머신 일 수도 있다. 프로세서는 또한 컴퓨팅 디바이스들의 조합, 예를 들면, DSP와 마이크로프로세서, 복수의 마이크로프로세서들, DSP 코어와 연계한 하나 이상의 마이크로프로세서들, 또는 임의 의 다른 그러한 구성의 조합으로써 구현될 수도 있다. 펌웨어 및/또는 소프트웨어 구현에 있어서, 기법들은 랜덤 액세스 메모리 (random access memory; RAM), 판독 전용 메모리 (read-only memory; ROM), 불휘발성 RAM (non-volatile random access memory; NVRAM), PROM (programmable read-only memory), EPROM (erasable programmable read-only memory), EEPROM (electrically erasable PROM), 플래시 메모리, 컴팩트 디스크 (compact disc; CD), 자기 또는 광학 데이터 스토리지 디바이 스 등과 같은 컴퓨터 판독가능 매체 상에 저장된 명령들로써 구현될 수도 있다. 명령들은 하나 이상의 프로세 서들에 의해 실행 가능할 수도 있고, 프로세서(들)로 하여금 본원에 설명된 기능의 특정 양태들을 수행하게 할 수도 있다. 소프트웨어로 구현되면, 상기 기능들은 하나 이상의 명령들 또는 코드로서 컴퓨터 판독 가능한 매체 상에 저장 되거나 또는 컴퓨터 판독 가능한 매체를 통해 전송될 수도 있다. 컴퓨터 판독가능 매체들은 한 장소에서 다른 장소로 컴퓨터 프로그램의 전송을 용이하게 하는 임의의 매체를 포함하여 컴퓨터 저장 매체들 및 통신 매체들 양자를 포함한다. 저장 매체들은 컴퓨터에 의해 액세스될 수 있는 임의의 이용 가능한 매체들일 수도 있다. 비제한적인 예로서, 이러한 컴퓨터 판독가능 매체는 RAM, ROM, EEPROM, CD-ROM 또는 다른 광학 디스크 스토리지, 자기 디스크 스토리지 또는 다른 자기 스토리지 디바이스들, 또는 소망의 프로그램 코드를 명령들 또 는 데이터 구조들의 형태로 이송 또는 저장하기 위해 사용될 수 있으며 컴퓨터에 의해 액세스될 수 있는 임의의 다른 매체를 포함할 수 있다. 또한, 임의의 접속이 컴퓨터 판독가능 매체로 적절히 칭해진다.예를 들어, 소프트웨어가 동축 케이블, 광섬유 케이블, 연선, 디지털 가입자 회선 (DSL), 또는 적외선, 무선, 및 마이크로파와 같은 무선 기술들을 사용하여 웹사이트, 서버, 또는 다른 원격 소스로부터 전송되면, 동축 케 이블, 광섬유 케이블, 연선, 디지털 가입자 회선, 또는 적외선, 무선, 및 마이크로파와 같은 무선 기술들은 매 체의 정의 내에 포함된다. 본원에서 사용된 디스크 (disk)와 디스크 (disc)는, CD, 레이저 디스크, 광 디스크, DVD (digital versatile disc), 플로피디스크, 및 블루레이 디스크를 포함하며, 여기서 디스크들 (disks) 은 보통 자기적으로 데이터를 재생하고, 반면 디스크들 (discs) 은 레이저를 이용하여 광학적으로 데이터를 재생한 다. 위의 조합들도 컴퓨터 판독가능 매체들의 범위 내에 포함되어야 한다. 소프트웨어 모듈은 RAM 메모리, 플래시 메모리, ROM 메모리, EPROM 메모리, EEPROM 메모리, 레지스터들, 하드 디스크, 이동식 디스크, CD-ROM, 또는 공지된 임의의 다른 형태의 저장 매체 내에 상주할 수도 있다. 예시적인 저장 매체는, 프로세가 저장 매체로부터 정보를 판독하거나 저장 매체에 정보를 기록할 수 있도록, 프로세서에 커플링 될 수 있다. 대안으로, 저장 매체는 프로세서에 통합될 수도 있다. 프로세서와 저장 매체는 ASIC 내에 존재할 수도 있다. ASIC은 유저 단말 내에 존재할 수도 있다. 대안으로, 프로세서와 저장 매체는 유저 단말에 서 개별 컴포넌트들로써 존재할 수도 있다. 본 개시의 앞선 설명은 당업자들이 본 개시를 행하거나 이용하는 것을 가능하게 하기 위해 제공된다. 본 개시 의 다양한 수정예들이 당업자들에게 쉽게 자명할 것이고, 본원에 정의된 일반적인 원리들은 본 개시의 취지 또 는 범위를 벗어나지 않으면서 다양한 변형예들에 적용될 수도 있다. 따라서, 본 개시는 본원에 설명된 예들에 제한되도록 의도된 것이 아니고, 본원에 개시된 원리들 및 신규한 특징들과 일관되는 최광의의 범위가 부여되도 록 의도된다. 비록 예시적인 구현예들이 하나 이상의 독립형 컴퓨터 시스템의 맥락에서 현재 개시된 주제의 양태들을 활용하 는 것을 언급할 수도 있으나, 본 주제는 그렇게 제한되지 않고, 오히려 네트워크나 분산 컴퓨팅 환경과 같은 임 의의 컴퓨팅 환경과 연계하여 구현될 수도 있다. 또 나아가, 현재 개시된 주제의 양상들은 복수의 프로세싱 칩 들이나 디바이스들에서 또는 그들에 걸쳐 구현될 수도 있고, 스토리지는 복수의 디바이스들에 걸쳐 유사하게 영 향을 받게 될 수도 있다. 이러한 디바이스들은 PC들, 네트워크 서버들, 및 핸드헬드 디바이스들을 포함할 수도 있다."}
{"patent_id": "10-2020-0113328", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "본 명세서에서는 본 개시가 일부 실시예들과 관련하여 설명되었지만, 본 발명이 속하는 기술분야의 통상의 기술 자가 이해할 수 있는 본 개시의 범위를 벗어나지 않는 범위에서 다양한 변형 및 변경이 이루어질 수 있다는 점 을 알아야 할 것이다. 또한, 그러한 변형 및 변경은 본 명세서에서 첨부된 특허청구의 범위 내에 속하는 것으 로 생각되어야 한다."}
{"patent_id": "10-2020-0113328", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시의 실시예들은, 이하 설명하는 첨부 도면들을 참조하여 설명될 것이며, 여기서 유사한 참조 번호는 유사 한 요소들을 나타내지만, 이에 한정되지는 않는다.도 1은 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방법의 예시도이다. 도 2는 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 방법의 순서도이다. 도 3은 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 시스템의 블록도이다. 도 4는 본 개시의 일 실시예에 따른 비디오 영상에 보케 효과를 적용하는 시스템의 처리부의 영상 처리 방법의 순서도이다. 도 5는 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정을 설명하기 위한 예시도이다. 도 6은 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정에서 객체의 포즈를 분류하는 과정을 설명하기 위한 예시도이다. 도 7은 본 개시의 일 실시예에 따른 이미지의 추출된 특성 정보를 분석하는 과정에서 이미지 내에 포함된 점근 선(지평선) 및 소실점의 높이 중 적어도 하나를 탐지하여, 이미지 내의 심도(depth) 특성을 분석하는 과정을 설 명하기 위한 예시도이다. 도 8은 본 개시의 일 실시예에 따른 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 과정을 설명하기 위한 예시도이다. 도 9는 본 개시의 일 실시예에 따른 이미지의 분석된 특성 정보를 기초로 이미지에 적용될 보케 효과의 종류 및 적용 방식을 결정하는 과정을 설명하기 위한 예시도이다. 도 10은 본 개시의 일 실시예에 따른 보케 비디오 영상에 대한 보케 효과의 강도에 대한 입력 정보를 수신하는 과정을 설명하기 위한 예시도이다. 도 11은 본 개시의 일 실시예에 따른 보케 효과를 이미지에 적용하는 단계를 설명하기 위한 순서도이다. 도 12는 본 개시의 일 실시예에 따른 깊이 맵을 세그멘테이션 마스크를 이용하여 깊이 맵을 보정하는 과정을 설 명하기 위한 예시도이다. 도 13은 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도 14는 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도 15는 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 개략도이다. 도 16은 본 개시의 일 실시예에 따른 비디오 보케 솔루션의 데이터 흐름을 나타내는 블록도이다. 도 17은 본 개시의 일 실시예에 따른 인공신경망 모델을 나타내는 예시도이다."}
