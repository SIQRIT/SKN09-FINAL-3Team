{"patent_id": "10-2024-7031904", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0023334", "출원번호": "10-2024-7031904", "발명의 명칭": "신경 프로세싱 유닛 및 이의 동작 방법", "출원인": "주식회사 딥엑스", "발명자": "박정부"}}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "신경 프로세싱 유닛(Neural Processing Unit, NPU)의 동작 방법으로서,컨트롤러에 의해, 제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성곱(Transpose Convolution) 연산임을 결정하는 단계;상기 컨트롤러에 의해, 상기 전치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분하는 단계;적어도 하나의 프로세싱 엘리먼트에 의해, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 상기 서브커널들 각각의 컨볼루션 연산을 수행하는 단계; 및각 프로세싱 엘리먼트에 의해 연산된 부분 합을 다음 프로세싱 엘리먼트로 각각 전달하는 단계;를 포함하는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 연산된 부분 합은,출력 특징맵, 가중치 및 입력 특징맵 중 어느 하나인,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 연산된 부분 합은,재사용을 위해 상기 다음 프로세싱 엘리먼트의 레지스터에 고정되는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 컨볼루션 연산을 수행하는 단계는,제로 스키핑(zero skipping) 방식을 기반으로 제로 패딩된 입력 특징맵에 대해서는 패딩된 제로를 연산하지 않는 단계를 포함하고,상기 복수 개의 서브 커널들은, 서로 다른 크기를 갖는 서브 커널들을 포함하고,상기 커널에 포함된 가중치 값들은, 복수 개의 상기 서브 커널들 중 적어도 하나에 포함되되, 서로 중복되지 않도록 포함되는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 입력 데이터의 크기 및 상기 입력 특징맵의 크기를 식별하는 단계;복수 개의 상기 서브 커널들에 대한 스트라이드(Stride)의 크기를 결정하는 단계; 및공개특허 10-2025-0023334-3-상기 입력 특징맵의 크기 또는 상기 스트라이드의 크기 중 적어도 하나에 기초하여, 출력 데이터의 크기를 결정하는 단계;를 더 포함하는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,복수 개의 상기 서브 커널들은 제1 서브 커널 및 제2 서브 커널을 포함하고,상기 제1 서브 커널은, 상기 입력 특징맵의 제1 윈도우에 상기 커널이 대응될 때 상기 제1 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되고,상기 제2 서브 커널은, 상기 입력 특징맵의 제2 윈도우에 상기 커널이 대응될 때 상기 제2 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,적어도 하나의 상기 프로세싱 엘리먼트에 의해, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 서브필터들 각각의 컨볼루션 연산을 수행하는 단계는,상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하는 단계; 및상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하고 난 후, 상기 입력 특징맵과 상기 제2 서브 커널의 컨볼루션 연산을 수행하는 단계;를 포함하는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산은, 상기 입력 특징맵에 포함된 상기 입력 데이터와 상기 제1 서브 커널의 컨볼루션 연산이고,상기 제1 컨볼루션 레이어의 출력은, 출력 특징맵(Output feature map)이고,상기 출력 특징맵은, 상기 입력 데이터보다 큰 사이즈인,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,컨트롤러에 의해, 상기 입력 특징맵의 크기, 상기 출력 특징맵의 크기, 상기 입력 특징맵에 포함된 적어도 하나의 상기 제로 또는 상기 입력 데이터의 위치, 상기 스트라이드의 크기 또는 상기 커널의 크기 중 적어도 하나에기초하여 복수의 상기 서브 커널들을 결정하는 단계;를 더 포함하는,신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서,상기 컨트롤러에 의해, 상기 출력 특징맵에 포함된 출력 데이터를 내부 메모리의 주소에 맵핑하는 단계; 및상기 내부 메모리에 의해, 상기 출력 특징맵에 포함된 상기 출력 데이터를 상기 맵핑된 주소에 저장하는 단계;를 더 포함하는,공개특허 10-2025-0023334-4-신경 프로세싱 유닛의 동작 방법."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "인공신경망모델을 저장하도록 구성된 내부 메모리;상기 내부 메모리에 액세스(access)하도록 구성되며, 상기 인공신경망모델의 합성곱 연산을 처리하도록 구성된적어도 하나의 프로세싱 엘리먼트(processing element; PE); 및상기 내부 메모리, 적어도 하나의 상기 프로세싱 엘리먼트와 동작 가능하게 연결된 컨트롤러(controller);를 포함하고,상기 인공신경망모델은 이미지를 입력으로 하여 이미지 향상(Image Enhancement)을 처리하도록 구성된 인공 신경망 기반의 모델이고, 상기 컨트롤러는,제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성곱(Transpose Convolution) 연산임을 결정하고, 상기 전치합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분하고, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 상기 서브 커널들 각각의 컨볼루션 연산을 수행하고, 연산된 부분 합을 다음 프로세싱 엘리먼트로각각 전달하도록 구성된,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 연산된 부분 합은,출력 특징맵, 가중치 및 입력 특징맵 중 어느 하나인,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 연산된 부분 합은,재사용을 위해 상기 다음 프로세싱 엘리먼트의 레지스터에 고정되는,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 컨트롤러는,상기 컨볼루션 연산을 수행할 시에, 제로 스키핑(zero skipping) 방식을 기반으로 제로 패딩된 입력 특징맵에대해서는 패딩된 제로를 연산하지 않고,상기 복수 개의 서브 커널들은, 서로 다른 크기를 갖는 서브 커널들을 포함하고,상기 커널에 포함된 가중치 값들은, 복수 개의 상기 서브 커널들 중 적어도 하나에 포함되되, 서로 중복되지 않도록 포함되는,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 컨트롤러는,공개특허 10-2025-0023334-5-상기 입력 데이터의 크기 및 상기 입력 특징맵의 크기를 식별하고, 복수 개의 상기 서브 커널들에 대한 스트라이드(Stride)의 크기를 결정하고, 상기 입력 특징맵의 크기 또는 상기 스트라이드의 크기 중 적어도 하나에 기초하여 출력 데이터의 크기를 결정하도록 추가로 구성된,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,복수 개의 상기 서브 커널들은 제1 서브 커널 및 제2 서브 커널을 포함하고,상기 제1 서브 커널은, 상기 입력 특징맵의 제1 윈도우에 상기 커널이 대응될 때 상기 제1 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되고,상기 제2 서브 커널은, 상기 입력 특징맵의 제2 윈도우에 상기 커널이 대응될 때 상기 제2 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되는,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서,상기 컨트롤러는,상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하고, 상기 제1컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하고 난 후 상기 입력 특징맵과 상기 제2 서브 커널의 컨볼루션 연산을 수행하도록 추가로 구성된,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산은, 상기 입력 특징맵에 포함된 상기 입력 데이터와 상기 제1 서브 커널의 컨볼루션 연산이고, 상기 제1 컨볼루션 레이어의 출력은 출력 특징맵(Output feature map)이고,상기 출력 특징맵은 상기 입력 데이터보다 큰 사이즈인,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 컨트롤러는,상기 입력 특징맵의 크기, 상기 출력 특징맵의 크기, 상기 입력 특징맵에 포함된 적어도 하나의 상기 제로 또는상기 입력 데이터의 위치, 상기 스트라이드의 크기 또는 상기 커널의 크기 중 적어도 하나에 기초하여 복수의상기 서브 커널들을 결정하도록 구성된,신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서,상기 컨트롤러는,상기 출력 특징맵에 포함된 출력 데이터를 내부 메모리의 주소에 맵핑하고, 상기 출력 특징맵에 포함된 상기 출력 데이터를 상기 맵핑된 주소에 저장하도록 상기 내부 메모리를 제어하도록 추가로 구성된,공개특허 10-2025-0023334-6-신경 프로세싱 유닛."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 예시에 따른 신경 프로세싱 유닛의 동작 방법이 제공된다. 상기 방법은, 컨트롤러에 의해, 제1 컨 볼루션 레이어에서 수행되는 연산이 전치 합성곱(Transpose Convolution) 연산임을 결정하는 단계; 상기 컨트롤 러에 의해, 상기 전치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분하는 단계; 적어도 하나의 프로세싱 엘리먼트에 의해, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 상기 서브 커널들 각각의 컨 볼루션 연산을 수행하는 단계; 및 각 프로세싱 엘리먼트에 의해 연산된 부분 합을 다음 프로세싱 엘리먼트로 각 각 전달하는 단계;를 포함할 수 있다."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 신경 프로세싱 유닛(Neural Processing Unit, NPU) 및 이의 동작 방법에 관한 것이다."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인간은 인식(Recognition), 분류(Classification), 추론(Inference), 예측(Predict), 조작/의사결정 (Control/Decision making) 등을 할 수 있는 지능을 갖추고 있다. 인공지능(artificial intelligence: AI)은 인간의 지능을 인공적으로 모방하는 것을 의미한다. 인간의 뇌는 뉴런(Neuron)이라는 수많은 신경세포로 이루어져 있다. 각각의 뉴런은 시냅스(Synapse)라고 불리는 연결부위를 통해 수 백에서 수 천 개의 다른 뉴런들과 연결되어 있다. 인간의 지능을 모방하기 위하여, 생물학 적 뉴런의 동작원리와 뉴런 간의 연결 관계를 모델링한 것을, 인공신경망(Artificial Neural Network, ANN) 모 델이라고 한다. 즉, 인공신경망은 뉴런들을 모방한 노드들을 레이어(Layer: 계층) 구조로 연결시킨, 시스템이다. 이러한 인공신경망모델은 레이어 수에 따라 '단층 신경망'과 '다층 신경망'으로 구분한다. 일반적인 다층신경망은 입력 레이어와 은닉 레이어, 출력 레이어로 구성된다. 입력 레이어(input layer)는 외부의 자료들을 받아들이는 레이어로서, 입력 레이어의 뉴런 수는 입력되는 변수의 수와 동일하다. 은닉 레이어(hidden layer)는 입력 레이어와 출력 레이어 사이에 위치하며 입력 레이어로부터 신호를 받아 특성을 추 출하여 출력층으로 전달한다. 출력 레이어(output layer)는 은닉 레이어로부터 신호를 받아 외부로 출력한 다. 뉴런 간의 입력신호는 0에서 1 사이의 값을 갖는 각각의 연결강도와 곱해진 후 합산된다. 합산 값이 뉴런의 임계치보다 크면 뉴런이 활성화되어 활성화 함수를 통하여 출력 값으로 구현된다. 한편, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은닉 레이어의 개수를 늘린 것을 심층 신경망 (Deep Neural Network, DNN)이라고 한다. DNN에는 여러 종류가 있다. 일 예로, 컨볼루션 신경망(Convolutional Neural Network, CNN)은 입력 데이터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. 컨볼루션 신경망(CNN)은 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼 루션 신경망은 영상처리에 적합한 것으로 알려져 있다. 컨볼루션 신경망은 분류(Classification), 객체 탐지 (Object Detection), 인스턴스 분할(Instance Segmentation), 이미지 향상(Image Enhancement) 등의 목적으로 사용될 수 있다. 컨볼루션 신경망은 컨볼루션 채널들과 풀링(pooling) 채널들이 반복되는 형태로 구성된다. 컨볼루션 신경망에서 대부분의 연산시간은 컨볼루션 동작이 차지한다. 컨볼루션 신경망은 행렬(Matrix) 형태의 커널(kernel)에 의해 각 채널의 영상의 특징을 추출하고, 풀링(Pooling)에 의해 이동이나 왜곡 등의 항상성을 제공하는 방식으로 사 물을 인식한다. 각 채널에서는 입력 데이터와 커널의 컨볼루션으로 특징맵(Feature Map)을 구한 후 ReLU(Rectified Linear Unit) 같은 활성함수를 적용하여 해당 채널의 활성화 맵을 생성한다. 이후 풀링이 적용 될 수 있다. 패턴을 실제로 분류하는 신경망은 특징 추출 신경망의 후단에 위치하며, 완전 연결 레이어(Fully Connected Layer)라고 한다. 컨볼루션 신경망의 연산 처리에서 대부분의 연산은 컨볼루션 또는 행렬곱을 통해 수행된다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국공개특허공보 제10-2022-0097161호 (공개일: 2022년 07월 07일)"}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시의 발명자는 하기의 사항들에 대하여 인식하였다. 이미지 향상(Image Enhancement)을 위한 인공신경망모델의 경우, 출력 이미지는 입력 이미지와 같은 사이즈로 출력되므로 특징(feature)에 대한 채널이 증가했다가 다시 감소하는 형태이다. 이미지 향상(Image Enhancement)을 위한 인공신경망모델의 경우, 채널이 감소할 때에는 특징맵(feature map)의 가로(width) 및 세로(height)의 길이가 증가하는 경향이 있다. 또한, 이미지 향상을 위한 인공신경망모델의 경 우, 전치 합성곱(Transpose Convolution) 연산을 포함하는 경향이 있다. NPU는 전치 합성곱 연산 수행 시, 특징맵에 zero(s)를 삽입하여 일반적인 합성곱 연산으로 대체하여 연산할 수 있다. 일반적인 합성곱 연산을 이용하여 전치 합성곱 연산을 수행하는 경우, 삽입된 zero(s)에 대한 MAC(multiply and accumulate) 연산을 수반하게 되므로 반응 속도(latency)가 증가한다. 또한, 일반적인 합성곱 연산을 이용하여 전치 합성곱 연산을 수행하는 경우, 삽입된 zero(s)에 대한 MAC 연산을 수반하게 되므로 불필요한 MAC 연산량이 증가한다. 이에, 본 개시가 해결하고자 하는 과제는 특징맵의 전치 합성곱 연산을 서로 다른 복수 개의 합성곱으로 분리하 여 연산하는 장치 및 방법을 제공하는 것이다. 따라서 본 개시에 따른 장치 및 방법에 의하면 zero(s)가 삽입된 특징맵의 zero(s)를 연산하지 않을 수 있는 효과가 있다. 단 본 개시는 이에 제한되지 않으며, 또 다른 과제들은 아래의 기재로부터 당업자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "전술한 바와 같은 과제를 해결하기 위하여 본 발명의 일 예시에 따른 신경 프로세싱 유닛의 동작 방법이 제공된 다. 상기 방법은, 컨트롤러에 의해, 제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성곱(Transpose Convolution) 연산임을 결정하는 단계; 상기 컨트롤러에 의해, 상기 전치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커 널들로 구분하는 단계; 적어도 하나의 프로세싱 엘리먼트에 의해, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 상기 서브 커널들 각각의 컨볼루션 연산을 수행하는 단계; 및 각 프로세싱 엘리먼트에 의해 연산된 부분 합을 다음 프로세싱 엘리먼트로 각각 전달하는 단계;를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 상기 연산된 부분 합은 출력 특징맵, 가중치 및 입력 특징맵 중 어느 하나일 수 있다. 본 발명의 일 실시예에 따르면, 상기 연산된 부분 합은 재사용을 위해 상기 다음 프로세싱 엘리먼트의 레지스터 에 고정될 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨볼루션 연산을 수행하는 단계는 제로 스키핑(zero skipping) 방식을 기 반으로 제로 패딩된 입력 특징맵에 대해서는 패딩된 제로를 연산하지 않는 단계를 포함하고, 상기 복수 개의 서 브 커널들은, 서로 다른 크기를 갖는 서브 커널들을 포함하고, 상기 커널에 포함된 가중치 값들은, 복수 개의 상기 서브 커널들 중 적어도 하나에 포함되되, 서로 중복되지 않도록 포함될 수 있다. 본 발명의 일 실시예에 따르면, 상기 입력 데이터의 크기 및 상기 입력 특징맵의 크기를 식별하는 단계; 복수 개의 상기 서브 커널들에 대한 스트라이드(Stride)의 크기를 결정하는 단계; 및 상기 입력 특징맵의 크기 또는 상기 스트라이드의 크기 중 적어도 하나에 기초하여, 출력 데이터의 크기를 결정하는 단계;를 더 포함할 수 있 다. 본 발명의 일 실시예에 따르면, 복수 개의 상기 서브 커널들은 제1 서브 커널 및 제2 서브 커널을 포함하고, 상 기 제1 서브 커널은, 상기 입력 특징맵의 제1 윈도우에 상기 커널이 대응될 때 상기 제1 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되고, 상기 제2 서브 커널은, 상기 입력 특징맵의 제2 윈도우에 상기 커널이 대 응될 때 상기 제2 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정될 수 있다. 본 발명의 일 실시예에 따르면, 적어도 하나의 상기 프로세싱 엘리먼트에 의해, 상기 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 서브 필터들 각각의 컨볼루션 연산을 수행하는 단계는 상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하는 단계; 및 상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하고 난 후, 상기 입력 특징맵과 상기 제2 서브 커널의 컨볼루션 연산을 수행하는 단계;를 포함할 수 있다. 본 발명의 일 실시예에 따르면, 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산은 상기 입력 특징맵에 포함된 상기 입력 데이터와 상기 제1 서브 커널의 컨볼루션 연산이고, 상기 제1 컨볼루션 레이어의 출력은 출력 특징맵(Output feature map)이고, 상기 출력 특징맵은 상기 입력 데이터보다 큰 사이즈일 수 있다. 본 발명의 일 실시예에 따르면, 컨트롤러에 의해, 상기 입력 특징맵의 크기, 상기 출력 특징맵의 크기, 상기 입 력 특징맵에 포함된 적어도 하나의 상기 제로 또는 상기 입력 데이터의 위치, 상기 스트라이드의 크기 또는 상 기 커널의 크기 중 적어도 하나에 기초하여 복수의 상기 서브 커널들을 결정하는 단계;를 더 포함할 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러에 의해, 상기 출력 특징맵에 포함된 출력 데이터를 내부 메모리 의 주소에 맵핑하는 단계; 및 상기 내부 메모리에 의해, 상기 출력 특징맵에 포함된 상기 출력 데이터를 상기 맵핑된 주소에 저장하는 단계;를 더 포함할 수 있다. 전술한 바와 같은 과제를 해결하기 위하여 본 발명의 일 예시에 따른 신경 프로세싱 유닛이 제공된다. 신경 프로세싱 유닛은, 인공신경망모델을 저장하도록 구성된 내부 메모리; 상기 내부 메모리에 액세스(access)하도록 구성되며, 상기 인공신경망모델의 합성곱 연산을 처리하도록 구성된 적어도 하나의 프로세싱 엘리먼트(processing element; PE); 및 상기 내부 메모리, 적어도 하나의 상기 프로세 싱 엘리먼트와 동작 가능하게 연결된 컨트롤러(controller);를 포함하고, 상기 인공신경망모델은 이미지를 입력 으로 하여 이미지 향상(Image Enhancement)을 처리하도록 구성된 인공 신경망 기반의 모델이고, 상기 컨트롤러 는 제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성곱(Transpose Convolution) 연산임을 결정하고, 상기 전 치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분하고, 상기 제1 컨볼루션 레이어에서 입력 특 징맵과 복수 개의 상기 서브 커널들 각각의 컨볼루션 연산을 수행하고, 연산된 부분 합을 다음 프로세싱 엘리먼 트로 각각 전달하도록 구성될 수 있다. 본 발명의 일 실시예에 따르면, 상기 연산된 부분 합은 출력 특징맵, 가중치 및 입력 특징맵 중 어느 하나일 수 있다. 본 발명의 일 실시예에 따르면, 상기 연산된 부분 합은 재사용을 위해 상기 다음 프로세싱 엘리먼트의 레지스터 에 고정될 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러는 상기 컨볼루션 연산을 수행할 시에, 제로 스키핑(zero skipping) 방식을 기반으로 제로 패딩된 입력 특징맵에 대해서는 패딩된 제로를 연산하지 않고, 상기 복수 개의 서브 커널들은 서로 다른 크기를 갖는 서브 커널들을 포함하고, 상기 커널에 포함된 가중치 값들은 복수 개의 상기 서브 커널들 중 적어도 하나에 포함되되, 서로 중복되지 않도록 포함될 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러는 상기 입력 데이터의 크기 및 상기 입력 특징맵의 크기를 식별 하고, 복수 개의 상기 서브 커널들에 대한 스트라이드(Stride)의 크기를 결정하고, 상기 입력 특징맵의 크기 또 는 상기 스트라이드의 크기 중 적어도 하나에 기초하여 출력 데이터의 크기를 결정하도록 추가로 구성될 수 있 다. 본 발명의 일 실시예에 따르면, 복수 개의 상기 서브 커널들은 제1 서브 커널 및 제2 서브 커널을 포함하고, 상 기 제1 서브 커널은, 상기 입력 특징맵의 제1 윈도우에 상기 커널이 대응될 때 상기 제1 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정되고, 상기 제2 서브 커널은, 상기 입력 특징맵의 제2 윈도우에 상기 커널이 대 응될 때 상기 제2 윈도우에 포함된 상기 입력 데이터의 위치에 따라 결정될 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러는 상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산을 수행하고, 상기 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널 의 컨볼루션 연산을 수행하고 난 후 상기 입력 특징맵과 상기 제2 서브 커널의 컨볼루션 연산을 수행하도록 추 가로 구성될 수 있다. 본 발명의 일 실시예에 따르면, 상기 입력 특징맵과 상기 제1 서브 커널의 컨볼루션 연산은 상기 입력 특징맵에 포함된 상기 입력 데이터와 상기 제1 서브 커널의 컨볼루션 연산이고, 상기 제1 컨볼루션 레이어의 출력은 출력특징맵(Output feature map)이고, 상기 출력 특징맵은 상기 입력 데이터보다 큰 사이즈일 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러는 상기 입력 특징맵의 크기, 상기 출력 특징맵의 크기, 상기 입 력 특징맵에 포함된 적어도 하나의 상기 제로 또는 상기 입력 데이터의 위치, 상기 스트라이드의 크기 또는 상 기 커널의 크기 중 적어도 하나에 기초하여 복수의 상기 서브 커널들을 결정하도록 구성될 수 있다. 본 발명의 일 실시예에 따르면, 상기 컨트롤러는 상기 출력 특징맵에 포함된 출력 데이터를 내부 메모리의 주소 에 맵핑하고, 상기 출력 특징맵에 포함된 상기 출력 데이터를 상기 맵핑된 주소에 저장하도록 상기 내부 메모리 를 제어하도록 추가로 구성될 수 있다. 기타 실시예의 구체적인 사항들은 상세한 설명 및 도면들에 포함되어 있다."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시에 따르면, 저해상도의 이미지를 고해상도로 이미지를 만들기 위한 인공신경망모델의 전치 합성곱 연산 을 여러 타입의 컨볼루션으로 분리하여 수행할 수 있다. 본 개시에 따르면, 저해상도의 이미지를 고해상도로 이미지를 만들기 위한 인공신경망모델의 전치 합성곱 연산 에서 불필요한 제로들 간의 연산을 제거할 수 있다. 본 개시에 따르면, 저해상도의 이미지를 고해상도로 이미지를 만들기 위한 인공신경망모델의 전치 합성곱 연산 에서 MAC(multiply and accumulate) 연산량을 감소시킬 수 있다. 본 개시에 따른 효과는 이상에서 예시된 내용에 의해 제한되지 않으며, 더욱 다양한 효과들이 본 발명 내에 포 함되어 있다."}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 명세서 또는 출원에 개시되어 있는 본 개시의 개념에 따른 실시 예들에 대해서 특정한 구조적 내지 단계적 설명들은 단지 본 개시의 개념에 따른 실시 예를 설명하기 위한 목적으로 예시된 것이다. 본 개시의 개념에 따른 실시 예들은 다양한 형태로 실시될 수 있으며 본 개시의 개념에 따른 실시 예들은 다양 한 형태로 실시될 수 있으며 본 명세서 또는 출원에 설명된 실시 예들에 한정되는 것으로 해석되어서는 아니 된다. 본 개시의 개념에 따른 실시 예는 다양한 변경을 가할 수 있고 여러 가지 형태를 가질 수 있으므로 특정 실시 예들을 도면에 예시하고 본 명세서 또는 출원에 상세하게 설명하고자 한다. 그러나, 이는 본 개시의 개념에 따 른 실시 예를 특정한 개시 형태에 대해 한정하려는 것이 아니며, 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제 1 및/또는 제 2 등의 용어는 다양한 구성 요소들을 설명하는데 사용될 수 있지만, 상기 구성 요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성 요소를 다른 구성 요소로부터 구별하는 목적으로만, 예컨대 본 개시의 개념에 따른 권리 범위로부터 이탈되지 않은 채, 제 1 구성요소는 제 2 구성요소로 명명될 수 있고, 유사하게 제 2 구성요소 는 제 1 구성요소로도 명명될 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃하는\"과 \"~에 직 접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 본 문서에서, \"A 또는 B,\" \"A 또는/및 B 중 적어도 하나,\" 또는 \"A 또는/및 B 중 하나 또는 그 이상\" 등의 표현 은 함께 나열된 항목들의 모든 가능한 조합을 포함할 수 있다. 예를 들면, \"A 또는 B,\" \"A 및 B 중 적어도 하나,\" 또는 \"A 또는 B 중 적어도 하나\"는, 적어도 하나의 A를 포함, 적어도 하나의 B를 포함, 또는 적어도 하나의 A 및 적어도 하나의 B 모두를 포함하는 경우를 모두 지칭할 수 있다. 본 문서에서 사용된 \"제 1,\" \"제 2,\" \"첫째,\" 또는 \"둘째,\" 등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 예를 들면, 제 1 사용자 기기와 제 2 사용자 기기는, 순서 또는 중요도와 무관하게, 서로 다른 사용자 기기를 나타낼 수 있다. 예를 들면, 본 문서에 기재된 권리범위를 벗어나지 않으면서 제 1 구성요 소는 제 2 구성요소로 명명될 수 있고, 유사하게 제 2 구성요소도 제 1 구성요소로 바꾸어 명명될 수 있다. 본 문서에서 사용된 용어들은 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 다른 예시의 범위를 한정하 려는 의도가 아닐 수 있다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함할 수 있다. 기술적이거나 과학적인"}
{"patent_id": "10-2024-7031904", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "용어를 포함해서 여기서 사용되는 용어들은 본 문서에 기재된 기술분야에서 통상의 지식을 가진 자에 의해 일반 적으로 이해되는 것과 동일한 의미를 가질 수 있다. 본 문서에 사용된 용어들 중 일반적인 사전에 정의된 용어들은, 관련 기술의 문맥상 가지는 의미와 동일 또는 유사한 의미로 해석될 수 있으며, 본 문서에서 명백하게 정의되지 않는 한, 이상적이거나 과도하게 형식적인 의 미로 해석되지 않는다. 경우에 따라서, 본 문서에서 정의된 용어일지라도 본 문서의 실시 예들을 배제하도록 해 석될 수 없다. 본 명세서에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도 가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 명세서에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 서술된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함 을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 개시가 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥상 가지는 의미와 일치하는 의미 를 가지는 것으로 해석되어야 하며, 본 명세서에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적 인 의미로 해석되지 않는다.본 발명의 여러 예시들의 각각 특징들이 부분적으로 또는 전체적으로 서로 결합 또는 조합 가능하며, 당업자가 충분히 이해할 수 있듯이 기술적으로 다양한 연동 및 구동이 가능하며, 각 예시들이 서로에 대하여 독립적으로 실시 가능할 수도 있고 연관 관계로 함께 실시 가능할 수도 있다. 실시 예를 설명함에 있어서 본 개시가 속하는 기술 분야에 익히 알려져 있고 본 개시와 직접적으로 관련이 없는 기술 내용에 대해서는 설명을 생략한다. 이는 불필요한 설명을 생략함으로써 본 개시의 요지를 흐리지 않고 더 욱 명확히 전달하기 위함이다. <용어의 정의> 이하, 본 명세서에서 제시되는 개시들의 이해를 돕고자, 본 명세서에서 사용되는 용어들에 대하여 간략하게 정 리하기로 한다. NPU: 신경 프로세싱 유닛(Neural Processing Unit)의 약어로서, CPU(Central processing unit)과 별개로 인공 신경망모델의 연산을 위해 특화된 프로세서를 의미할 수 있다. 인공신경망 가속기로 지칭되는 것도 가능하다. 인공신경망의 구조에 대한 정보: 레이어의 개수에 대한 정보, 레이어 내의 노드의 개수, 각 노드의 값, 연산 처 리 방법에 대한 정보, 각 노드에 적용되는 가중치 행렬에 대한 정보 등을 포함하는 정보이다. 인공신경망모델의 데이터 지역성: 학습이 완료된 인공신경망(ANN)의 구조가 확정되면, 인공신경망모델을 컴파일 하여 확정된 모든 연산순서 및 연산 종류를 포함하는 정보이다. DNN: 심층 신경망(Deep Neural Network)의 약어로서, 보다 높은 인공 지능을 구현하기 위하여, 인공신경망의 은 닉 레이어의 개수를 늘린 것을 의미할 수 있다. CNN: 컨볼루션 신경망(Convolutional Neural Network)의 약어로서, 인간 뇌의 시각 피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망이다. 컨볼루션 신경망은 영상처리에 적합한 것으로 알려져 있으며, 입력 데이 터의 특징들을 추출하고, 특징들의 패턴을 파악하기에 용이한 것으로 알려져 있다. CNN에서의 가중치는 N x M 크기의 커널을 지칭할 수 있다. 이하, 첨부한 도면을 참조하여 본 개시의 실시 예를 설명함으로써, 본 개시를 상세히 설명한다. 이하, 본 개시 의 실시 예를 첨부된 도면을 참조하여 상세하게 설명한다. 도 1은 본 개시의 일 실시예에 따른 신경 프로세싱 유닛이 포함된 장치를 설명하는 개략적인 개념도이다. 도 1을 참조하면 NPU이 포함된 장치(B)는 온칩 영역(A)을 포함한다. 온칩 영역 외부에는 메인 메모리 가 포함될 수 있다. 메인 메모리는 예를 들어 DRAM등과 같은 시스템 메모리일 수 있다. 도시되지 않 았으나, 온칩 영역(A) 외부에는 ROM등을 포함하는 저장부가 포함될 수 있다. 온칩 영역(A)에는 중앙 프로세싱 유닛(CPU)와 같은 범용 프로세싱 유닛과 온칩 메모리 그리고 NPU가 배치된다. CPU는 NPU와 온칩 메모리 그리고 메인 메모리에 동작 가능하 게 연결된다. 단 본 개시의 장치(B)는 독립형(standalone) 장치로 구성될 수 있으며, 이러한 경우 중앙 프로세싱 유닛 이 배제되도록 구성되는 것도 가능하다. 단, 본 개시는 이에 제한되지 않으며, CPU 내부에 NPU가 포함되도록 구성되는 것도 가능하다. 온칩 메모리는 반도체 다이에 실장된 메모리로 메인 메모리 액세스와 별도로 캐싱을 위한 메모리일 수 있다. 예를 들면, 온칩 메모리는 다른 온칩 반도체들이 액세스하도록 설정된 메모리일 수도 있다. 예를 들면, 온칩 메모리는 캐시 메모리 또는 버퍼 메모리 일 수 있다. NPU는 내부 메모리를 포함하며, 내부 메모리는 예를 들어 SRAM을 포함할 수 있다. 내부 메모리 는 실질적으로 NPU에서의 연산에만 사용되는 메모리 일 수 있다. 내부 메모리는 NPU 내부 메모 리로 지칭될 수 있다. 여기서 실질적이란, 내부 메모리에는 NPU가 처리하는 인공신경망과 관련된 데 이터를 저장하도록 구성된 것을 의미할 수 있다. 일 예로, 인공신경망과 관련된 데이터는 입력 특징맵, 커널 및 /또는 출력 특징맵에 대한 데이터일 수 있다. 예를 들면, 내부 메모리는 NPU 연산에 필요한 가중치, 커널 및/또는 특징맵을 저장하도록 구성된 버 퍼 메모리 및/또는 캐시 메모리 일 수 있다. 단, 이에 제한되지 않는다. 예를 들면, 내부 메모리는 SRAM, MRAM, 레지스터 파일(Register file) 등의 읽고 쓰기가 메인 메모리 보다 상대적으로 더 빠른 메모리 소자로 구성될 수 있다. 단, 이에 제한되지 않는다. 예를 들면, 내부 메모리는 NPU 내부에 배치된 복수의 메모리들을 포함하도록 구성될 수 있다. 예를 들면, 내부 메모리는 컨트롤러를 제어하기 위한 적어도 하나의 명령어 및/또는 인스트럭션(instruction)을 저장하기 위한 제1 내부 메모리(미도시), 컴파일된 인공신경망모델의 연산 단계가 결정된 스케줄링(scheduling) 정보를 저장하기 위한 제2 내부 메모리(미도시), 및/또는 인공신경망모델의 파라미터(예를 들면, 입력 특징맵, 출력 특징맵, 가중치, 커널, 부분합(partial sum) 등)을 저장하도록 구성된 제3 내부 메모리(미도시) 등을 포함 하도록 구성되는 것도 가능하다. NPU가 포함된 장치(B)는 내부 메모리, 온칩 메모리, 메인 메모리 중 적어도 하나를 포 함한다. 이하에서 설명하는 “적어도 하나의 메모리”는 내부 메모리, 및 온칩 메모리중 적어도 하나를 포함 하도록 의도된다. 또한, 온칩 메모리의 기재는 NPU의 내부 메모리 또는 NPU의 외부에 있으나 온칩 영역 (A)에 있는 메모리를 포함하도록 의도될 수 있다. 다만, 적어도 하나의 메모리를 지칭하는 내부 메모리 및/또는 온칩 메모리는 위치적 특성이 아닌 메 모리의 대역폭(bandwidth) 기준 또는 버스의 구성 방식에 따라 메인 메모리와 구분하는 것도 가능하다. 통상적으로 메인 메모리는 대용량의 데이터를 저장하기 용이하나, 메모리 대역폭이 상대적으로 낮고, 전 력 소모가 상대적으로 큰 메모리를 지칭한다. 통상적으로 내부 메모리와 온칩 메모리는 메모리 대역폭이 상대적으로 높고, 전력 소모가 상대적으 로 낮으나, 대용량의 데이터를 저장하기에 비효율 적인 메모리를 지칭한다. NPU이 포함된 장치(B)의 각각의 구성요소는 버스를 통해서 통신할 수 있다. 장치(B)의 버스 는 적어도 하나일 수 있다. 버스는 통신 버스 및/또는 시스템 버스 등으로 지칭될 수 있다. NPU의 내부 메모리와 온칩 메모리는 인공신경망모델의 가중치와 특징맵 처리를 위해 특정 대 역폭 이상을 보장하기 위해서 별도의 전용 버스를 더 구비하는 것도 가능하다. 온 칩 메모리와 메인 메모리 사이에는 특정 대역폭 이상을 보장하기 위해서 별도의 전용 버스를 더 구비하는 것도 가능하다. 상기 특정 대역폭은 NPU의 프로세싱 엘리먼트 어레이의 처리 성능을 기준으로 결정될 수 있다. NPU의 내부 메모리와 메인 메모리 사이에는 특정 대역폭 이상을 보장하기 위해서 별도의 전용 버스를 더 구비하는 것도 가능하다. 상기 특정 대역폭은 NPU의 프로세싱 엘리먼트 어레이의 처리 성능을 기준으로 결정될 수 있다. NPU이 포함된 장치(B)는 DMA(Direct Memory Access) 모듈을 더 포함하여, 내부 메모리, 온 칩 메모 리 및/또는 메인 메모리를 직접 제어하도록 구성되는 것도 가능하다. 예를 들면, DMA 모듈은 버스를 직접 제어하여 NPU와 온칩 메모리의 데이터 전송을 직접 제어 하도록 구성될 수 있다. 예를 들면, DMA 모듈은 버스를 직접 제어하여 온칩 메모리와 메인 메모리의 데이터 전송을 직접 제어하도록 구성될 수 있다. 예를 들면, DMA 모듈은 버스를 직접 제어하여 내부 메모리와 메인 메모리의 데이터 전송을 직 접 제어하도록 구성될 수 있다. 신경 프로세싱 유닛(neural processing unit, NPU)은 인공신경망을 위한 동작을 수행하도록 특화된 프로 세서이다. NPU는 AI 가속기로 지칭될 수 있다. 인공신경망은 여러 입력 또는 자극이 들어오면 각각 가중치를 곱해 더해주고, 추가적으로 편차를 더한 값을 활 성화 함수를 통해 변형하여 전달하는 인공 뉴런들이 모인 네트워크를 의미한다. 이렇게 학습된 인공신경망은 입력 데이터로부터 추론(inference) 결과를 출력하는데 사용될 수 있다. 상기 NPU은 전기/전자 회로로 구현된 반도체일 수 있다. 상기 전기/전자 회로라 함은 수많은 전자 소자, (예컨대 트렌지스터, 커패시터)를 포함하는 것을 의미할 수 있다. 상기 NPU은 프로세싱 엘리먼트 (processing element: PE) 어레이, NPU 내부 메모리, NPU 컨트롤러, 특수 기능 유닛(SFU) 및 NPU 인터페 이스를 포함할 수 있다. 프로세싱 엘리먼트 어레이, NPU 내부 메모리, NPU 스케줄러, 및 NPU 인터페이스 각각은 수많은 트렌지스터들이 연결된 반도체 회로일 수 있다. 따라서, 이들 중 일부는 육안으로는 식별되어 구분되기 어려울 수 있고, 동작에 의해서만 식별될 수 있다. 예컨 대, 임의 회로는 프로세싱 엘리먼트 어레이로 동작하기도 하고, 혹은 NPU 컨트롤러로 동작될 수도 있다. 상기 NPU은 프로세싱 엘리먼트 어레이, 프로세싱 엘리먼트 어레이에서 추론될 수 있는 인공신경망모델의 적어도 일부를 저장하도록 구성된 NPU 내부 메모리, 및 인공신경망모델의 데이터 지역성 정보 또는 인공신 경망모델의 구조에 대한 정보에 기초하여 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리를 제어하도록 구 성된 NPU 컨트롤러를 포함할 수 있다. 인공신경망모델은 인공신경망모델의 데이터 지역성 정보 또는 구조에 대한 정보를 포함할 수 있다. 인공신경망모델은 이미지 향상 기능을 수행하도록 학습된 AI 인식모델을 의미할 수 있다. 프로세싱 엘리먼트 어레이는 인공신경망을 위한 동작을 수행할 수 있다. 예를 들어, 입력 데이터가 입력되었을 때, 프로세싱 엘리먼트 어레이는 인공신경망이 학습을 수행하도록 할 수 있다. 학습이 완료된 이후, 입력 데이 터가 입력되었을 때, 프로세싱 엘리먼트 어레이는 학습 완료된 인공신경망을 통해 추론 결과를 도출하는 동작을 수행할 수 있다. 예를 들면, NPU은 NPU 인터페이스를 통해서 메인 메모리에 저장된 인공신경망모델의 데이터를 NPU 내부 메모리으로 불러올 수 있다. NPU 인터페이스는 버스를 통해서 메인 메모리와 통신할 수 있다. NPU 컨트롤러는 NPU의 추론 연산을 위한 프로세싱 엘리먼트 어레이의 연산 및 NPU 내부 메모리의 읽 기 및 쓰기 순서를 제어하도록 구성된다. NPU 컨트롤러는 입력 데이터와 복수의 서브 커널들(또는, 필터들) 각 각에 대한 합성곱 연산의 순서를 제어하도록 구성된다. NPU 컨트롤러는 합성곱 연산들의 결과로 출력되는 특징 맵에 있어서, 특징맵에 포함된 각 요소들(또는, 출력 값들, 출력 데이터)을 NPU 내부 메모리의 대응되는 주소에 맵핑하도록 구성된다. NPU 컨트롤러는 인공신경망모델의 구조를 분석하거나 또는 인공신경망모델의 구조를 제공받는다. 다음으로, NPU 컨트롤러는 각 레이어 별 연산순서를 순차적으로 결정한다. 즉, 인공신경망모델의 구조가 확정될 경우, 레이어 별 연산순서가 정해질 수 있다. 이러한 인공신경망모델의 구조에 따른 연산의 순서 또는 데이터 흐름의 순서를 알고리즘 레벨에서의 인공신경망모델의 데이터 지역성으로 정의할 수 있다. NPU 컨트롤러는 상기 인공신경망모델의 구조를 반영하여 각 레이어 별 연산순서를 순차적으로 결정한다. 즉, 인 공신경망모델의 구조가 확정될 경우, 레이어 별 연산순서가 정해질 수 있다. 예시적으로, NPU 컨트롤러는 특정 컨볼루션 레이어에서 컨볼루션 전치 연산이 수행되는 경우, 서브 레이어 별 연산 순서가 정해질 수 있다. 입력 데이터와 복수 개의 서브 필터들 각각의 컨볼루션 연산이 수행될 때, 각각의 컨볼루션 연산을 서브 레이어에서 수행되는 연산으로 정의할 수 있다. 인공신경망모델이 NPU에서 실행되도록 컴파일러가 인공신경망모델을 컴파일 할 경우, 신경 프로세싱 유닛 -메모리 레벨에서의 인공신경망모델의 인공신경망 데이터 지역성이 재구성될 수 있다. 예를 들어, 컴파일러는 CPU, 온라인 서버 또는 별도의 외부 시스템에 의해 실행될 수 있다. NPU가 해당 인공신경망모델을 연산하는 방식, 예를 들면, 특징맵 타일링(feature map tiling), 프로세싱 엘리먼트의 스테이셔너리(Stationary) 기법, NPU의 프로세싱 엘리먼트 개수, NPU 내 특징맵 및 가 중치의 크기, 내부 메모리 용량, NPU내의 메모리 계층 구조, 및 해당 인공신경망모델을 연산 처리하기 위 한 NPU의 연산 동작의 순서를 결정해 주는 컴파일러의 알고리즘 특성 등에 따라, 동일한 인공신경망모델 의 경우에도 처리하고자 하는 인공신경망모델의 계산 방법이 상이하게 구성될 수 있다. 왜냐하면, 상술한 요인 들에 의해서 동일한 인공신경망모델을 연산 처리하더라도 NPU가 클럭 단위로 매 순간 필요한 데이터의 순 서를 상이하게 결정할 수 있기 때문이다.도 2는 본 개시의 일 실시예에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 신경 프로세싱 유닛(NPU)은 프로세싱 엘리먼트 어레이(Processing Element Array, PE Array), 내부 메모리 및/또는 컨트롤러를 포함할 수 있다. 내부 메모리에는 본 개시의 예시적인 실시예에 따 른 인공신경망모델이 저장될 수 있다. 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트를 포함 한다. 따라서 프로세싱 엘리먼트 어레이는 복수의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 컨트롤러는 인공신경망모델의 가중치 값들의 크기, 특징맵(feature map)의 크기, 가중치 값들 및/또 는 특징맵의 계산 순서 등을 고려하여 프로세싱 엘리먼트 어레이 및 NPU 내부 메모리을 제어하도록 구성될 수 있다. 컨트롤러는 프로세싱 엘리먼트 어레이에서 계산될 가중치 값들의 크기, 특징맵의 크기, 및 가중치 값 들과 특징맵의 계산 순서 등을 수신할 수 있다. 인공신경망모델이 포함할 수 있는 인공신경망의 데이터는 각 레 이어의 노드 데이터 또는 특징맵, 및 각각의 레이어의 노드를 연결하는 연결망 각각의 가중치 데이터를 포함할 수 있다. 일 예로, 인공신경망모델이 포함할 수 있는 인공신경망과 관련된 데이터는 입력 특징맵, 커널 및/또는 출력 특징맵에 대한 데이터일 수 있다. 인공신경망의 데이터 또는 파라미터들 중 적어도 일부는 컨트롤러 내부에 제공되는 메모리 또는 NPU 내부 메모리에 저장될 수 있다. 컨트롤러는 제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성곱 연산임을 결정할 수 있다. 제1 컨볼루 션 레이어에서 수행되는 연산은 채널이 감소하면서 특징(feature)(또는, 특징맵)의 크기가 증가하게 되는 전치 합성곱 연산일 수 있다. 이렇게, 채널이 감소하면서 특징의 크기가 증가하게 되는 연산은 업 스케일링 (upscaling), 업 컨볼루션(up convolution), 전치 합성곱(Transpose Convolution) 연산으로 지칭될 수 있다. 컨트롤러는 복수 개의 컨볼루션 레이어들의 연산 중 해당 컨볼루션 레이어에서 수행되는 연산이 전치 합성 곱 연산임을 결정할 수 있다. 컨트롤러는 전치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분할 수 있다. 일 실시 예에 따르면, 복수 개의 서브 커널들은 제1 서브 커널, 제2 서브 커널, 제3 서브 커널, 제4 서브 커널을 포함할 수 있다. 커널의 크기는 가로(width)와 세로(height)의 곱(width x height)으로 나타낼 수 있다. 일 실시예에 따르 면, 커널의 크기는 3x3이고, 제1 서브 커널의 크기는 1x1이고, 제2 서브 커널의 크기는 2x1이고, 제3 서브 커널 의 크기는 1x2이고, 제4 서브 커널의 크기는 2x2일 수 있다. 프로세싱 엘리먼트는 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 서브 커널들 각각의 컨볼루션 연 산을 수행할 수 있다. 프로세싱 엘리먼트은 입력 특징맵과 제1 서브 커널의 컨볼루션 연산을 수행할 수 있다. 프로세싱 엘리먼트 은 입력 특징맵과 제2 서브 커널의 컨볼루션 연산을 수행할 수 있다. 프로세싱 엘리먼트은 입력 특징 맵과 제3 서브 커널의 컨볼루션 연산을 수행할 수 있다. 프로세싱 엘리먼트은 입력 특징맵과 제4 서브 커 널의 컨볼루션 연산을 수행할 수 있다. 본 개시의 NPU에서 MAC 연산량을 감소시키는 구성은 특히 제로 패딩된 입력 데이터와의 합성곱 연산 시 패딩된 제로를 연산하지 않는 방식을 택함으로써, 도 6 이하에서 설명할 전치 합성곱 연산에서 연산량을 최소화 하는 것이다. 일반적인 CPU의 스케줄링은 공평성, 효율성, 안정성, 반응 시간 등을 고려하여, 최상의 효율을 낼 수 있도록 동 작한다. 즉, 우선 순위, 연산 시간 등을 고려해서 동일 시간내에 가장 많은 프로세싱을 수행하도록 스케줄링 된 다. 종래의 CPU는 각 프로세싱의 우선 순서, 연산 처리 시간 등의 데이터를 고려하여 작업을 스케줄링 하는 알고리 즘을 사용하였다. 이와 다르게 NPU의 컨트롤러는 인공신경망모델의 파라미터들의 계산 방식에 기초하여 프로세싱 순서 를 결정할 수 있다. 프로세싱 엘리먼트 어레이는 인공신경망의 노드 데이터(예를 들면, 특징맵)와 연결망의 가중치 데이터(예 를 들면, 커널)를 연산하도록 구성된 복수의 프로세싱 엘리먼트들(PE1, PE2, …)을 포함하도록 구성된다. 각각 의 프로세싱 엘리먼트는 MAC (multiply and accumulate) 연산기 및/또는 ALU (Arithmetic Logic Unit) 연산기를 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 도 2에서는 예시적으로 복수의 프로세싱 엘리먼트들(PE1, PE2, …)이 도시되었지만, 하나의 프로세싱 엘리먼트 내부에 MAC을 대체하여, 복수의 곱셈기(multiplier) 및 가산기 트리(adder tree)로 구현된 연산기들이 병렬로 배치되어 구성되는 것도 가능하다. 이러한 경우, 프로세싱 엘리먼트 어레이는 복수의 연산기를 포함하는 적어도 하나의 프로세싱 엘리먼트로 지칭되는 것도 가능하다. 또한, 도 2에 도시된 복수의 프로세싱 엘리먼트들(PE1, PE2, …)은 단지 설명의 편의를 위한 예시이며, 복수의 프로세싱 엘리먼트들(PE1, PE2, …)의 개수는 제한되지 않는다. 복수의 프로세싱 엘리먼트들(PE1, PE2, …)의 개수에 의해서 프로세싱 엘리먼트 어레이의 크기 또는 개수가 결정될 수 있다. 프로세싱 엘리먼트 어레이의 크 기는 N x M 행렬 형태로 구현될 수 있다. 여기서 N 과 M은 0보다 큰 정수이다. 이에, 프로세싱 엘리먼트 어레이 는 N x M 개의 프로세싱 엘리먼트를 포함할 수 있다. 즉, 프로세싱 엘리먼트는 1개 이상일 수 있다. 또한, 프로세싱 엘리먼트 어레이는 복수 서브 모듈로 구성되는 것도 가능하다. 이에, 프로세싱 엘리먼트 어레이는 N x M x L 개의 서브 모듈로 구성된 프로세싱 엘리먼트를 포함할 수 있다. 부연 설명하면 L개는 프로세싱 엘리먼트 어레이의 서브 모듈의 개수로, 코어, 엔진 또는 쓰레드 등으로 지칭될 수 있다. 프로세싱 엘리먼트 어레이의 크기는 NPU이 작동하는 인공신경망모델의 특성을 고려하여 설계할 수 있다. 부연 설명하면, 프로세싱 엘리먼트의 개수는 작동할 인공신경망모델의 데이터 크기, 요구되는 동작 속도, 요구되는 소비 전력 등을 고려하여 결정될 수 있다. 인공신경망모델의 데이터 크기는 인공신경망모델의 레이어 수와 각각의 레이어의 가중치 데이터 크기에 대응되어 크기가 결정될 수 있다. 따라서, 본 개시의 일 예시에 따른 프로세싱 엘리먼트 어레이의 크기는 제한되지 않는다. 프로세싱 엘리먼 트 어레이의 프로세싱 엘리먼트들(PE1…)의 개수가 증가할수록 작동하는 인공신경망모델의 병렬 연산 능력이 증가되나, NPU의 제조 비용 및 물리적인 크기가 증가될 수 있다. 예를 들면, NPU에서 작동되는 인공신경망모델은 저해상도의 이미지를 고해상도 이미지로 출력하도록 학습 된 인공신경망, 즉 이미지 향상을 위한 AI 모델일 수 있다. 이러한 경우, 엘리먼트 어레이의 크기는 인공 신경망모델의 연산량 특성을 고려하여 N x M로 설계될 수 있다. 다르게 설명하면, 엘리먼트 어레이는 12개 의 프로세싱 엘리먼트들을 포함할 수 있다. 단, 이에 제한되지 않으며, 복수의 프로세싱 엘리먼트들(PE1 …)의 개수는 예를 들면, 8개 내지 16,384 범위 내에서 선택되는 것도 가능하다. 즉, 본 개시의 실시예들 에서 프로세싱 엘리먼트의 개수는 제한되지 않는다. 프로세싱 엘리먼트 어레이는 인공신경망 연산에 필요한 덧셈, 곱셈, 누산 등의 기능을 수행하도록 구성된 다. 다르게 설명하면, 프로세싱 엘리먼트 어레이는 MAC(multiplication and accumulation) 연산을 수행하 도록 구성될 수 있다. 내부 메모리는 휘발성 메모리일 수 있다. 휘발성 메모리는 전원이 공급된 경우에만 데이터를 저장하고, 전 원 공급이 차단되면 저장된 데이터가 소멸되는 메모리일 수 있다. 휘발성 메모리는 정적 랜덤 액세스 메모리 (Static Random Access Memory; SRAM), 동적 랜덤 액세스 메모리 (Dynamic Random Access Memory; DRAM) 등을 포함할 수 있다. 내부 메모리는 바람직하게는 SRAM일 수 있으나, 이에 한정되지 않는다. 이하에서는 인공신경망 중에서 심층 신경망(DNN, Deep Neural Network)의 한 종류인 컨볼루션 신경망(CNN, Convolutional Neural Network)에 대해서 집중적으로 설명하기로 한다. 컨볼루션 신경망은 하나 또는 여러 개의 컨볼루션 레이어(convolutional layer)과 통합 레이어(pooling layer), 완전하게 연결된 레이어(fully connected layer)들의 조합일 수 있다. 컨볼루션 신경망은 2차원 데이터 의 학습 및 추론에 적합한 구조를 가지고 있으며, 역전달(Backpropagation algorithm)을 통해 학습될 수 있다. 본 개시의 예시에서, 컨볼루션 신경망에는 레이어마다 복수의 채널을 포함한다. 채널마다 채널의 입력 영상의 특징을 추출하는 커널이 존재한다. 커널은 2차원 행렬로 구성될 수 있으며, 입력 데이터를 순회하면서 컨볼루션 연산 수행한다. 커널의 크기는 임의로 결정될 수 있으며, 커널이 입력 데이터를 순회하는 간격(stride) 또한 임 의로 결정될 수 있다. 커널 하나당 입력 데이터 전체에 대한 커널의 일치 정도는 특징맵(feature map) 또는 활 성화 맵일 수 있다. 이하에서 커널은 일 세트의 가중치 값들 또는 복수의 세트의 가중치 값들을 포함할 수 있다. 프로세싱 엘리먼트 어레이는 인공신경망의 컨벌루션 연산을 처리하도록 구성되고, 활성화 함수 연산은 별 도의 활성화 함수 처리 모듈에서 처리하도록 구성될 수 있다. 이러한 경우, 프로세싱 엘리먼트 어레이는컨벌루션 연산만을 위해서 동작될 수 있다. 특히 이러한 경우, 프로세싱 엘리먼트 어레이는 정수 타입의 데이터만 처리하도록 구성되어, 방대한 합성곱 연산 시 연산 효율을 극대화하는 것도 가능하다. 이처럼 컨볼루션 연산은 입력 데이터와 커널의 조합으로 이루어진 연산이므로, 이후 비선형성을 추가하기 위한 활성화 함수가 적용될 수 있다. 컨볼루션 연산의 결과인 특징맵에 활성화 함수가 적용되면 활성화 맵으로 지칭 될 수 있다. 일반적인 컨볼루션 신경망은 Alexnet, Squeezenet, VGG16, Resnet152, Moblienet 등이 있는데, 각 인공신경망 모델은 한번의 추론을 위해 각각 727 MFLOPs(Mega Floating-point Operations per Second), 837 MFLOPs, 16 MFLOPs, 11 MFLOPs, 11 MFLOPs, 579 MFLOPs의 곱셈 횟수가 필요하고, 커널을 포함한 모든 가중치가 각각 233 MB, 5 MB, 528 MB, 230 MB, 16 MB의 저장 크기를 갖는다. 따라서, 이러한 컨볼루션 신경망은 연산을 위해 상당 히 많은 양의 하드웨어 리소스와 전력 소모량을 요구함을 알 수 있다. 도 3은 본 개시에 적용될 수 있는 프로세싱 엘리먼트 어레이 중 하나의 프로세싱 엘리먼트를 설명하는 개략적인 개념도이다. 이하에서, 도 2를 참조하여 설명될 수 있다. 도 2를 참조하면, 프로세싱 엘리먼트 어레이는 MAC 연산을 수행하도록 구성되고, MAC 연산 결과를 양자화 해서 출력하도록 구성될 수 있다. 단, 본 개시의 예시들은 이에 제한되지 않는다. 내부 메모리는 메모리 크기와 인공신경망모델의 데이터 크기에 따라 인공신경망모델의 전부 또는 일부를 저장할 수 있다. 도 3을 참조하면, 제1 프로세싱 엘리먼트(PE1)는 곱셈기(Multiplier), 가산기(Adder), 및 누산기 (Accumulator)를 포함할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않으며, 프로세싱 엘리먼트 어레이는 인공신경망모델의 연산 특성을 고려하여 변형 실시될 수도 있다. 곱셈기는 입력 받은 (N)bit 데이터와 (M)bit 데이터를 곱한다. 곱셈기의 연산 값은 (N+M)bit 데이터 로 출력된다. 여기서 N과 M은 0보다 큰 정수이다. (N)bit 데이터를 입력 받는 제1 입력부는 변수 같은 특성을 가지는 값(예를 들면, 입력 특징맵)을 입력 받도록 구성될 수 있고, (M)bit 데이터를 입력 받는 제2 입력부는 상수 같은 특성을 가지는 값(예를 들면, 학습이 완료된 가중치)을 입력 받도록 구성될 수 있다. 단, 곱셈기 의 입력 데이터는 상수 값과 변수 값에 제한되지 않는다. 한편, 제1 프로세싱 엘리먼트(PE1)는 곱셈기의 제1 입력부 및 제 2 입력부 중 하나의 입력부에 0이 입력될 때, 연산을 하지 않더라도 연산 결과가 0인 것을 인지하고 있기 때문에, 곱셈기가 연산을 하지 않 도록 동작을 제한할 수 있다. 예를 들면, 곱셈기의 제1 입력부 및 제2 입력부 중 하나의 입력부에 0이 입력될 때, 곱셈기는 제로 스키핑(zero skipping) 방식으로 동작하도록 구성될 수 있다. 곱셈기의 제1 입력부 및 제2 입력부에 입력되는 데이터는 인공신경망모델의 각각의 레이어의 노드 데이터 및 가중치 데이터의 양자화에 따라서 비트 폭(bit width)이 결정될 수 있다. 예를 들면, 제1 레이어의 노드 데 이터가 5bit로 양자화 되고 제1 레이어의 가중치 데이터가 7bit로 양자화되는 경우 제1 입력부는 5bit의 데이터 를 입력 받도록 구성되고, 제2 입력부는 7bit의 데이터를 입력 받도록 구성될 수 있다. 도 2 내지 도 3을 참조하면, NPU는 내부 메모리에 저장된 양자화된 데이터가 제1 프로세싱 엘리먼트 (PE1)의 입력부들에 입력될 때 양자화된 비트 폭이 실시간으로 변환되도록 제1 프로세싱 엘리먼트(PE1)를 제어 할 수 있다. 가산기는 곱셈기의 연산 값과 누산기의 연산 값을 가산한다. (L)loops가 0일 경우, 누산된 데이 터가 없으므로, 가산기의 연산 값은 곱셈기의 연산 값과 동일할 수 있다. (L)loops가 1일 경우, 곱셈 기의 연산 값과 누산기의 연산 값이 가산된 값이 가산기의 연산 값일 수 있다. 누산기는 가산기의 연산 값과 곱셈기의 연산 값이 (L)loops 횟수만큼 누산되도록 가산기의 출력부에서 출력된 데이터를 임시 저장한다. 구체적으로, 가산기의 출력부에서 출력된 가산기의 연산 값은 누산기의 입력부에 입력되고, 입력된 연산 값은 누산기에 임시 저장되었다가 누산기의 출 력부에서 출력된다. 출력된 연산 값은 루프에 의해 가산기의 입력부에 입력된다. 이때, 가산기의 입력부 에는 곱셈기의 출력부에서 새롭게 출력된 연산 값이 함께 입력된다. 즉, 누산기의 연산 값과 곱 셈기의 새로운 연산 값이 가산기의 입력부에 입력되고, 이 값들이 가산기에서 가산되어 가산기 의 출력부를 통해 출력된다. 가산기의 출력부에서 출력된 데이터, 즉 가산기의 새로운 연산 값 은 누산기의 입력부에 입력되며, 이후 동작들은 상술한 동작들과 실질적으로 동일하게 루프 횟수만큼 수행된다. 이처럼, 누산기는 곱셈기의 연산 값과 가산기의 연산 값을 루프 횟수만큼 누산하기 위해 가산기 의 출력부에서 출력된 데이터를 임시 저장하므로, 누산기의 입력부에 입력되는 데이터 및 출력부에서 출력되는 데이터는 가산기의 출력부에서 출력된 데이터와 같은 (N+M+log2(L))bit의 비트 폭을 가질 수 있 다. 여기서 L은 0보다 큰 정수이다. 누산기는 누산이 종료되면, 초기화 신호(initialization reset)를 인가받아서 누산기 내부에 저장된 데이터를 0으로 초기화 할 수 있다. 단, 본 개시에 따른 예시들은 이에 제한되지 않는다. 누산기의 출력 데이터(N+M+log2(L))bit는 다음 레이어의 노드 데이터 또는 컨볼루션의 입력 데이터가 될 수 있다. 도 4는 본 개시의 일 실시예에 따른 인공신경망모델의 구조를 나타내는 개략도이다. 도 4는 본 개시의 일 실시에에 따른 인공신경망모델의 파라미터들의 크기를 레이어(layer)별로 도시한다. 인공 신경망모델은 인코더-디코더(encoder-decoder) 기반의 합성곱 신경망(Convolution Neural Network) 구조를 가 질 수 있다. 실시예에서, 인공신경망모델은 이미지 향상을 처리하도록 구성된 인공신경망모델을 의미할 수 있다. 도 4를 참조하면, 인공신경망모델은 축소 경로(contracting path)와 확장 경로(expending path)를 포함할 수 있다. 축소 경로(contracting path)는 입력 이미지의 특징을 포착할 수 있도록 채널의 수를 늘리면서 이미지의 차원을 축소하는 경로를 의미한다. 확장 경로(expending path)는 채널의 수를 줄이고 이미지의 차원을 늘리면서 고차원의 이미지를 복원하는 경로를 의미한다. 확장 경로는 저차원으로 인코딩 된 정보를 디코딩 단계의 각 레 이어에 합치는(concatenation) 방법(즉, 축소 경로의 레이어와 확장 경로의 레이어를 직접 연결(skip connection)하는 방법)을 이용하여 고차원의 이미지를 복원한다. 일 실시예에서, 572x572x1 사이즈의 입력 이미지 타일(input image tile)이 인공 신경망에 입력될 수 있다. 인 공신경망모델에는 이미지 전체가 입력될 수도 있고, 이미지의 일부인 이미지 타일로 나눠서 입력될 수도 있으며, 본 개시는 입력 이미지의 형태에 제한되지 않는다. 572x572는 맵(map)의 차원을 의미하고, 1은 채널의 수를 의미한다. 즉, 1개의 채널을 갖고 크기가 572x572인 입력 이미지 타일이 인공신경망모델에 입력될 수 있다. 입력 이미지 타일(tile)이 입력되면, 3x3 컨볼루션 연산 및 ReLU 활성화 함수, Max pooling이 반복적으로 적용 될 수 있다. 확장 경로는 복수 개의 전치 합성곱 연산을 포함할 수 있다. 실시예에서, 전치 합성곱 연산은 2x2 업 컨볼루션 (up-conv 2x2)일 수 있다. 일 예로, 임의의 컨볼루션 레이어에서, 100x100x256 크기의 특징맵은 2x2 업 컨볼루 션(up-conv 2x2) 연산을 수행하여 200x200x128 크기의 특징맵으로 출력될 수 있다. 보다 높은 해상도의 이미지를 얻기 위해서는 출력 이미지가 입력 이미지와 같은 크기이거나 적어도 유사한 크기 로 출력되어야 하기 때문에, 확장 경로에서 특징맵을 확장하면서 진행되는 전치 합성곱 연산이 필요하다. 전치 합성곱 연산은 특징맵에 zero(s)를 삽입하여 일반적인 합성곱 연산으로 대체하여 연산할 수 있다. 이하의 도 5에서는 특징맵에 zero(s)를 삽입하여 일반적인 합성곱 연산으로 대체하여 연산하는 전치 합성곱에 대해 설명하기로 한다. 도 5는 본 개시의 비교예에 따른 전치 합성곱 연산을 나타내는 개략도이다. 도 5는 입력 데이터, 입력 특징맵, 커널 및 출력 특징맵이 도시된다. 입력데이터, 입력 특징맵 , 커널 및 출력 특징맵은 각각 2차원 행렬(N x M, N과 M은 양의 정수)로 구성될 수 있다. 실시예에서, 입력 데이터는 요소 데이터 a를 포함하고, 3x3 크기일 수 있다. 입력데이터에 입력 특징맵은 zero(s)가 패딩된 입력 데이터일 수 있고, 7x7 크기일 수 있다. 커널은 3x3 크기일 수 있다. 출력 특징맵 은 요소 데이터 b를 포함하고, 5x5 크기일 수 있다. 출력 특징맵에 포함된 요소 데이터들은 '출력 데이 터'로 지칭될 수 있다. (a) 내지 (d)를 참조하면, 임의의 컨볼루션 레이어에서 커널은 입력 특징맵을 순회하면서 합성곱 연산 을 수행할 수 있다. 즉, 합성곱 연산은 커널의 윈도우를 일정한 간격으로 이동해가면서 계산한다. 윈도우란지정한 크기의 영역을 의미하며, 합성곱 연산 시 커널 크기에 대응되는 입력 특징맵 상의 영역을 의미할 수 있 다. 실시예에서, 커널이 입력 특징맵을 순회하는 간격(stride)는 1로 설정될 수 있다. 본 비교예에 따른 일반적인 합성곱 연산을 이용하여 전치 합성곱 연산을 수행하는 경우, 삽입된 zero(s)에 대한 MAC 연산을 수반하게 된다. (a)를 참조하면, 입력 특징맵의 9개의 요소 데이터와 커널의 컨볼루션 연산 수행 시, 8번의 연산이 zero(s)에 대한 MAC 연산에 해당된다. 커널과 상관없이 zero와의 연산에 대한 출력 값은 zero이므로, 삽입된 zero(s)에 대한 MAC 연산은 불필요한 MAC 연산량에 해당된다. 본 개시의 실시예에 따른 인공 신경망의 동작 방법에 따르면 제로 패딩된 입력 특징맵과의 합성곱 연산 시 패딩 된 제로를 연산하지 않음으로써 전치 합성곱 연산에서 MAC 연산량을 최소화할 수 있다. 상기 MAC 연산은 프로세싱 엘리먼트에서 수행될 수 있다. 도 6은 본 개시의 실시예에 따른 전치 합성곱 연산을 나타내는 개략도이다. 도 6을 참조하면, 임의의 컨볼루션 레이어에서, 입력 이미지를 제로 패딩한 후, 제로 패딩된 입력 이미지를 특 정 커널을 이용하여 탐색하면서 이미지의 특징들을 추출하고, 추출한 특징들을 특징맵으로 생성하는 전치 컨볼 루션 연산을 도시한다. 입력 이미지(또는, 입력 데이터)는 복수 개의 요소 데이터들(또는, 픽셀들)(a1,1 내지 a3,3)을 포함할 수 있다. 입력 이미지는 3x3 크기일 수 있다. 본 개시의 실시예에 따른 전치 합성곱 연산에서, 입력 이미지는 제로 패딩(zero-padding)될 수 있다. 제로 패딩은 입력 이미지의 요소 데이터들 사이 및/또는 가장자리에 제로 값으로 채운 픽셀들을 추가함으로써 합성곱 연산 후 출력되는 특징맵의 크기가 입력 이미지와 같거나 입력 이미지보다 크게 만들기 위함이다. 제로 패딩 된 입력 이미지는 7x7 크기일 수 있다. 제로 패딩 된 입력 이미지는 '입력 특징맵'으로 지칭 될 수 있다. 3x3 크기의 커널이 입력 특징맵을 간격(stride) 1로 순회하면서 합성곱 연산을 수행함으로써, 특징이 추출될 수 있다. 3x3 크기의 커널이 입력 특징맵을 간격(stride) 1로 순회하면서 추출한 특징들은 b1,1 내지 b5,5로 표현될 수 있다. 추출한 특징들은 5x5 크기의 출력 특징맵으로 출력될 수 있다. 도 7은 본 개시의 실시예에 따른 신경 프로세싱 유닛이 동작하는 방법을 설명하는 순서도이다. 도 8은 본 개시 의 실시예에 따른 복수 개의 서브 커널들 및 이에 대응되는 입력 데이터를 나타내는 개략도이다. 이하에서, 도 6, 도 9를 참조하여 설명된다. 도 7을 참조하면, 신경 프로세싱 유닛(또는, 컨트롤러)은 제1 컨볼루션 레이어에서 수행되는 연산이 전치 합성 곱(Transpose Convolution) 연산임을 결정할 수 있다(S702). 신경 프로세싱 유닛(또는, 컨트롤러)은 입력 데이터의 크기 및/또는 입력 특징맵의 크기를 식별할 수 있다. 신경 프로세싱 유닛(또는, 컨트롤러)은 전치 합성곱 연산에 사용되는 커널을 복수 개의 서브 커널들로 구분할 수 있다(S704). 제1 컨볼루션 레이어의 입력은 일정한 크기를 갖는 입력 특징맵(Input feature map)이고, 입력 특징맵은 입력 데이터에 적어도 하나의 제로가 패딩된 특징맵일 수 있다. 도 6 및 도 8을 참조하면, 커널은 신경 프로세싱 유닛(또는, 컨트롤러)에 의해 4개의 서브 커널들(82_1, 82_2, 82_3, 82_4)로 구분될 수 있다. 커널은 가중치 값들(k0 내지 k8)을 포함할 수 있다. 커널에 포함된 가중치 값들은 4개의 서브 커널들(82_1, 82_2, 82_3, 82_4) 중 하나에 포함된다. 즉, 커널에 포함된 가중치 값 은 복수 개의 서브 커널들(82_1, 82_2, 82_3, 82_4) 중 하나에 필수적으로 포함되되, 중복되어 포함되지는 않는 방식으로 복수 개의 서브 커널들(82_1, 82_2, 82_3, 82_4)이 구분되어 설정될 수 있다. 단, 복수 개의 서브 커 널로 구분하는 방식은 이에 제한되지 않는다. 실시예에서, 4개의 서브 커널들은 각각 제1 서브 커널(82_1), 제2 서브 커널(82_2), 제3 서브 커널(82_3), 제4 서브 커널(82_4)로 지칭될 수 있다. 제1 서브 커널(82_1)은 입력 특징맵의 제1 윈도우에 커널이 대응될 때, 제1 윈도우에 포함된 상기 입력 데이터 의 위치에 따라 결정될 수 있다.도 6 및 도 9를 참조하면, 입력 특징맵(91_1)의 제1 윈도우(94_1)에 커널이 대응될 때, 즉, 입력 특징맵 (91_1)의 제1 윈도우(94_1)와 커널 사이에 합성곱 연산이 수행될 때, 제1 윈도우(94_1)에 포함된 입력 데이 터는 a1,1이고 커널의 k4에 대응된다. 이 때 제1 서브 커널(82_1)은 k4로 구성될 수 있다. 이로써, 입력 특 징맵(91_1)과 제1 서브 커널(82_1)의 컨볼루션 연산은, 입력 특징맵(91_1)에 포함된 상기 입력 데이터(a1,1)와 제1 서브 커널(82_1)의 컨볼루션 연산일 수 있다. 즉, 입력 특징맵(91_1)과 제1 서브 커널(82_1)의 컨볼루션 연 산은 입력 특징맵(91_1)에 포함된 zero(s)와의 연산을 포함하지 않도록 제1 서브 커널(82_1)이 결정된다. 제2 서브 커널(82_2)은 입력 특징맵의 제2 윈도우에 커널이 대응될 때, 특정 윈도우에 포함된 상기 입력 데이터 의 위치에 따라 결정될 수 있다. 도 6 및 도 9를 참조하면, 입력 특징맵(91_1)의 제2 윈도우(94_2)에 커널이 대응될 때, 즉, 입력 특징맵 (91_1)의 제2 윈도우(94_2)와 커널 사이에 합성곱 연산이 수행될 때, 제2 윈도우(94_2)에 포함된 입력 데이 터는 a1,1 및 a1,2이고, 각각 커널의 k3 및 k5에 대응된다. 이 때 제2 서브 커널(82_2)은 k3 및 k5로 구성 될 수 있다. 이로써, 입력 특징맵(91_2)과 제2 서브 커널(82_2)의 컨볼루션 연산은, 입력 특징맵(91_1)에 포함 된 상기 입력 데이터(a1,1 및 a1,2)와 제2 서브 커널(82_2)의 컨볼루션 연산일 수 있다. 즉, 입력 특징맵(91_ 1)과 제2 서브 커널(82_2)의 컨볼루션 연산은 입력 특징맵(91_1)에 포함된 zero(s)와의 연산을 포함하지 않도록 제2 서브 커널(82_2)이 결정된다. 제3 서브 커널(82_3) 및 제4 서브 커널(82_4)은 상술한 제1 서브 커널(82_1) 또는 제2 서브 커널(82_2)과 유사 한 방식으로 구성될 수 있으므로, 이하 생략한다. 신경 프로세싱 유닛(또는, 컨트롤러)은 복수 개의 상기 서브 커널들에 대한 스트라이드(Stride)의 크기를 결정 할 수 있다. 신경 프로세싱 유닛(또는, 컨트롤러)은 입력 특징맵의 크기 또는 스트라이드의 크기 중 적어도 하나에 기초하여, 출력 데이터의 크기를 결정할 수 있다. 신경 프로세싱 유닛(또는, 컨트롤러)은 입력 특징맵의 크기, 출력 특징맵의 크기, 입력 특징맵에 포함된 적어도 하나의 제로 또는 입력 데이터의 위치, 상기 스트라이드의 크기 또는 커널의 크기 중 적어도 하나에 기초하여 복수의 서브 커널들을 결정할 수 있다. 도 7을 다시 참조하면, 신경 프로세싱 유닛(또는, 컨트롤러)은 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개 의 상기 서브 커널들 각각의 컨볼루션 연산을 수행할 수 있다(S706). 신경 프로세싱 유닛(또는, 컨트롤러)은 입력 특징맵과 복수의 서브 커널들 각각에 대한 합성곱 연산의 순서를 제어할 수 있다. 신경 프로세싱 유닛(또는, 컨트롤러)은 제1 컨볼루션 레이어에서 상기 입력 특징맵과 상기 제1 서브 커널의 컨 볼루션 연산을 수행하고 난 후, 입력 특징맵과 상기 제2 서브 커널의 컨볼루션 연산을 수행할 수 있다. 제1 컨볼루션 레이어에서 입력 특징맵과 복수 개의 상기 서브 커널들 각각의 컨볼루션 연산을 수행한 결과 출력 되는 제1 컨볼루션 레이어의 출력은, 일정한 크기를 갖는 출력 특징맵(Output feature map)이다. 출력 특징맵은 입력 데이터보다 큰 사이즈일 수 있다. 신경 프로세싱 유닛(또는, 컨트롤러)은 출력 특징맵에 포함된 출력 데이터를 내부 메모리의 주소에 맵핑할 수 있다. 내부 메모리는 출력 특징맵에 포함된 출력 데이터를 상기 맵핑된 주소에 저장할 수 있다. 도 8의 (a)를 참조하면, 제1 서브 커널(82_1)이 스트라이드 2로 이동하면서 입력 특징맵과의 합성곱 연산이 9번 수행될 때, 제1 서브 커널(82_1)과 요소별 곱(element-wise multiplication)이 수행되는 입력 데이터의 요 소 데이터를 나타낸다. 본 개시의 실시예에 따른 제1 서브 커널(82_1)에 따르면, 제로 패딩 된 입력 특징맵과 제1 서브 커널(82_1)의 합성곱 연산으로, 입력 특징맵에 패딩 된 제로는 연산하지 않을 수 있는 효과가 있다. 도 8의 (b)를 참조하면, 제2 서브 커널(82_2)이 스트라이드 2로 이동하면서 입력 특징맵과의 합성곱 연산이 6번 수행될 때, 제1 서브 커널(82_1)과 요소별 곱(element-wise multiplication)이 수행되는 입력 데이터의 요 소 데이터를 나타낸다. 본 개시의 실시예에 따른 제2 서브 커널(82_2)에 따르면, 제로 패딩 된 입력 특징맵과 제2 서브 커널(82_2)의 합성곱 연산으로, 입력 특징맵에 패딩 된 제로는 연산하지 않을 수 있는 효과가 있다. 도 8의 (c)를 참조하면, 제3 서브 커널(82_3)이 스트라이드 2로 이동하면서 입력 특징맵과의 합성곱 연산이 6번 수행될 때, 제3 서브 커널(82_3)과 요소별 곱(element-wise multiplication)이 수행되는 입력 데이터의 요소 데이터를 나타낸다. 본 개시의 실시예에 따른 제3 서브 커널(82_3)에 따르면, 제로 패딩 된 입력 특징맵과 제3 서브 커널(82_3)의 합성곱 연산으로 입력 특징맵에 패딩 된 제로는 연산하지 않을 수 있는 효과가 있다. 도 8의 (d)를 참조하면, 제4 서브 커널(82_4)이 스트라이드 2로 이동하면서 입력 특징맵과의 합성곱 연산이 4번 수행될 때, 제4 서브 커널(82_4)과 요소별 곱(element-wise multiplication)이 수행되는 입력 데이터의 요 소 데이터를 나타낸다. 본 개시의 실시예에 따른 제4 서브 커널(82_4)에 따르면, 제로 패딩 된 입력 특징맵과 제4 서브 커널(82_4)의 합성곱 연산으로 입력 특징맵에 패딩 된 제로는 연산하지 않을 수 있는 효과가 있다. 신경 프로세싱 유닛(또는, 컨트롤러)에 의해 복수 개의 서브 커널들(92_1 내지 92_4)로 구분됨에 따라, 제1 컨 볼루션 레이어의 합성 곱 연산은 도 8의 (a)에 대응되는 제1 타입의 합성곱 연산, 도 8의 (b)에 대응되는 제2 타입의 합성곱 연산, 도 8의 (c)에 대응되는 제3 타입의 합성곱 연산, 도 8의 (d)에 대응되는 제4 타입의 합성 곱 연산으로 구분될 수 있다. 본 개시의 실시예에 따른 신경 프로세싱 유닛의 전치 합성곱 연산 방법에 따르면, 스트라이드 2의 합성곱 연산 을 네가지 타입으로 구분하여 수행함으로써, 패딩 된 zero(s) 연산을 생략할 수 있고 반응 속도(latency)를 1/4 감소시킬 수 있는 효과가 있다. 도 9는 본 개시의 실시예에 따른 신경 프로세싱 유닛에서 복수 개의 서브 커널들을 이용하여 수행되는 합성곱 연산을 나타내는 개략도이다. 이하에서, 도 6 및 도 8을 참조하여 설명된다. 도 6 및 도 9를 참조하면, 3x3 크기의 입력 데이터에 제로 패딩 된 7x7 크기의 입력 특징맵(91_1 내지 91_25)을 복수 개의 서브 커널들(92_1 내지 92_4)을 이용하여 25번의 합성곱 연산을 수행하고, 5x5 크기의 출력 특징맵 을 출력하는 과정이 도시된다. 도 8 내지 도 9를 참조하면, 도 8의 (a)에 대응되는 제1 타입의 합성곱 연산은, 도 9에서 제1 서브 커널(92_1) 이 스트라이드 2로 이동하면서 입력 특징맵(91_1, 91_3, 91_5, 91_11, 91_13, 91_15, 91_21, 91_23, 91_25)과 수행되는 합성곱 연산에 대응된다. 또한, 도 8의 (b)에 해당하는 제2 타입의 합성곱 연산은, 도 9에서 제2 서브 커널(92_2)이 스트라이드 2로 이동 하면서 입력 특징맵(91_2, 91_4, 91_12, 91_14, 91_22, 91_24)과 각각 수행되는 합성곱 연산에 대응된다. 도 8의 (c)에 해당하는 제3 타입의 합성곱 연산은, 도 9에서 제3 서브 커널(92_3)이 스트라이드 2로 이동하면서 입력 특징맵(91_6, 91_8, 91_10, 91_16, 91_18, 91_20)과 각각 수행되는 합성곱 연산에 대응된다. 도 8의 (d)에 해당하는 제4 타입의 합성곱 연산은, 도 9에서 제4 서브 커널(92_4)이 스트라이드 2로 이동하면서 입력 특징맵(91_7, 91_9, 91_17, 91_19)과 각각 수행되는 합성곱 연산에 대응된다. 도 9에 도시된 바와 같이, 입력 특징맵(91_1 내지 91_25)이 복수 개의 서브 커널들(92_1 내지 92_4) 중 하나와 특정 윈도우에서 합성곱 연산을 수행하면, 출력 특징맵의 출력 데이터 1개가 출력될 수 있다. 일 예로, 도 6 및 도 9를 참조하면, 입력 특징맵(91_1)이 제1 서브 커널(92_1)과 특정 윈도우에서 합성곱 연산 을 수행하면, 출력 특징맵의 1개의 출력 데이터(b1,1)가 출력될 수 있다. 도 6 및 도 9를 참조하면, 입력 특징맵(91_1~91_25)이 복수 개의 서브 커널들(92_1~92_4) 중 하나와 모든 윈도 우에서 합성곱 연산을 수행하면, 출력 특징맵의 모든 출력 데이터(b1,1 내지 b5,5)(또는, 5x5 크기의 출력 특징 맵)가 출력될 수 있다. 도 10은 본 개시의 비교예에 따른 systolic array 구조들을 나타내는 개략도이다. 도 10은 프로세싱 엘리먼트의 스테이셔너리(Stationary) 기법에 따른 NPU의 systolic array 구조의 비교예들을 나타낸다. Systolic array 구조를 갖는 NPU는 MAC 연산을 수행하는 복수 개의 프로세싱 엘리먼트들을 포함하는 프로세싱 엘리먼트 어레이를 포함할 수 있다. Systolic array 구조는 데이터를 재사용하여 병렬적으로 연산하는 구조를 의미하며, 이러한 방식을 dataflow로 지칭할 수 있다. 각 프로세싱 엘리먼트의 로컬 메모리에 어떤 값이 저장되는지에 따라 세가지 종류로 나뉠 수 있다. Systolic array 구조에 따르면, 데이터를 레지스터 파일에서 가져올 필요 없이, 임의의 프로세싱 엘리먼트에서 사용한 데이터를 위, 아래, 또는 오른쪽에 위치한 다음 프로세싱 엘리먼트로 전달하여, 다음 프로세싱 엘리먼트 에서 해당 데이터를 바로 사용할 수 있다. 도 10의 (a)를 참조하면, Output Stationary(OS) 모드에 따른 systolic array 구조를 나타낸다. Output Stationary(OS) 기법에 따른 systolic array는 출력 특징맵(Output Feature map, OF), 즉, 부분합 (partial sum)을 각 PE의 로컬 메모리에 저장하여 재사용할 수 있는 구조로, 특징(feature) 값과 가중치 (weight) 값 모두 재사용할 수 있다. 각 프로세싱 엘리먼트에 특징(feature) 값과 커널에 포함된 가중치 (weight) 값은 FIFO를 통해 로딩될 수 있다. 각 프로세싱 엘리먼트는 특징 값과 가중치 값을 이용하여 MAC 연산 을 수행할 수 있다. 이 때 연산된 부분 합(partial sum)은 각 프로세싱 엘리먼트에 저장하고, 다음에 발생된 부 분 합을 누적시킬 수 있다. 여기서 특징 값은 특징맵을 구성하는 파라미터를 지칭할 수 있다. 도 10의 (b)를 참조하면, Weight Stationary(WS) 모드에 따른 Systolic array 구조를 나타낸다. Weight stationary 기법에 따른 Systolic array는 가중치(weight) 값을 각 PE의 로컬 메모리에 저장하여, 컨볼 루션 연산 시 재사용할 수 있는 구조이다. Weight Stationary 기법에 따르면, DRAM에서 가중치 값을 가져오는 것을 최소화함으로써 가중치 값을 읽어올 때의 에너지 소모를 최소화할 수 있다. 각 프로세싱 엘리먼트에 특징 (feature) 값은 FIFO를 통해 로딩될 수 있다. 프로세싱 엘리먼트 어레이에 가중치가 프리 로딩(pre-loading)되 고, 프로세싱 엘리먼트 어레이에 가중치가 맵핑된 후 해당 가중치와 연산되는 입력 데이터(feature)가 가장 왼 쪽 열(column)부터 계속해서 피딩(feeding)하는 방식을 통해, 컨볼루션 연산을 수행할 수 있다. 이 때 시스톨릭 어레이 내부에서 매 사이클(cycle)마다 입력 데이터(feature)가 왼쪽에서 오른쪽으로 시프트되고, 연산된 부분 합(partial sum)은 매 사이클(cycle)로 이동하면서 위에서 아래로 프로세싱 엘리먼트에 전달할 수 있다. 도 10의 (c)를 참조하면, Input stationary(IS) 모드에 따른 Systolic array 구조를 나타낸다. Input stationary(IS) 기법에 따른 Systolic array 구조는 Weight stationary 기법에 따른 Systolic array 구 조와 tensor flow가 동일하다. 다만, Weight stationary 기법에 따른 Systolic array 구조와는 달리, 입력 특 징맵(Input Feature map, IF)을 각 PE의 로컬 메모리에 저장하여 재사용할 수 있는 구조이다. 각 Stationary 모드에 따른 Systolic array 구조는 인공신경망의 종류 및 특성에 따라 계산 시간에 큰 차이를 보일 수 있다. 인공신경망의 레이어 종류에 따라 입력 및 가중치의 높이(height), 넓이(width), 깊이(depth), 채널(channel) 등이 다양하기 때문이다. 예를 들어, 채널 수가 작은 레이어에서, Output stationary 기법에 따른 systolic array 구조를 사용하는 경우 MAC 이용률(utilization)이 현저히 낮을 수 있다. 채널 수가 작은 레이어에서는 Output stationary 기법에 따 른 Systolic array 구조 대신 Input stationary 모드에 따른 Systolic array 구조를 사용한다면 MAC 이용률 (utilization)을 높일 수 있다. Output stationary 모드 따른 Systolic array 구조로 고정된 NPU를 사용하는 경우, 채널의 수를 줄이고 이미지 의 차원을 늘리면서 고차원의 이미지를 복원하는 경로가 필요한 이미지 향상을 위한 인공신경망모델을 사용한다 면 MAC 이용률이 현저히 낮아지므로 이를 해결할 Dataflow의 개선이 필요할 수 있다. 본 개시의 실시예에 따른 신경 프로세싱 유닛의 동작 방법에 따르면, 인공신경망의 각 레이어의 특성에 따라 프 로세싱 엘리먼트 어레이에서의 MAC 연산 시간을 계산하고, 계산된 MAC 연산 시간에 기초하여 레이어 별로 MAC 연산 구조를 변경(switch)할 수 있다. 상기 연산 시간 계산은 인공신경망모델의 컴파일 단계에서 수행될 수 있 다. 단 이에 제한되지 않는다. 본 개시의 다른 실시예에 따른 신경 프로세싱 유닛의 동작 방법에 따르면, 인공신경망의 입력 특징맵의 특성에 따라 프로세싱 엘리먼트 어레이에서의 MAC 연산 시간을 계산하고, 계산된 MAC 연산 시간에 기초하여 입력 특징 맵의 타일(tile)(또는, 패치(patch)) 별로 MAC 연산 구조를 변경(switch)할 수 있다. 상기 연산 시간 계산은 인 공신경망모델의 컴파일 단계에서 수행될 수 있다. 단 이에 제한되지 않는다. 인공신경망의 각 레이어의 특성은 입력 특징맵(Input Feature map, IF) 및/또는 가중치(weight)의 모양(shap e)을 포함할 수 있다. 입력 특징맵(Input Feature map, IF) 및/또는 가중치(weight)의 모양(shape)이란 높이 (height), 넓이(width), 깊이(depth), 채널(channel)으로 결정될 수 있다. 인공신경망의 입력 특징맵은 복수 개의 작은 타일(tile)로 분할될 수 있고, 각 타일은 커널과의 연산을 통해 특 징을 추출할 수 있다. 타일 별로 MAC 연산 시간을 최소화할 수 있는 Systolic array 구조가 상이할 수 있다. 신경 프로세싱 유닛의 프로세싱 엘리먼트 어레이의 MAC 연산 구조는 Input stationary 모드에 따른 Systolic array 구조, Output stationary 모드에 따른 Systolic array 구조, Weight stationary 모드에 따른 Systolicarray 중 하나일 수 있다. 컨볼루션 신경망에서, 대부분의 연산 시간은 컨볼루션 동작이 차지한다. 컨볼루션 동작을 위한 MAC 연산의 수는 아래의 수학식 1과 같은 알고리즘으로 계산될 수 있다. [수학식 1] for (m=0; m<M; m++) // Num filters for (e=0; e<E; e++) // num_conv_window(height) for (f=0; f<F; f++) // num_conv_window(width) for (c=0; c<C; c++) // Elements per filter(channel) for (r=0; r<R; r++) // Elements per filter(height) for (s=0; s<S; s++) // Elements per filter(width) { output[m][e][f] = input[c][e+r][f+s] * weight[m][c][r][s] time = time + 1; } 수학식 1을 참조하면, m은 필터의 개수, e는 출력 특징맵의 높이, f는 출력 특징맵의 넓이, c는 채널의 수, r은 필터의 높이, s는 필터의 넓이를 각각 나타내는 변수이다. M은 필터의 개수, E는 출력 특징맵의 높이, F는 출력 특징맵의 넓이, C는 채널의 수, R은 필터의 높이, S는 필터의 넓이를 각각 나타내는 상수이다. 수학식 1에 따른 알고리즘은 각 stationary 기법에 따라 각기 다른 for-loop를 병렬로 연산할 수 있다. Output stationary 모드에 따른 systolic array 구조인 경우, 수학식 1에 따른 알고리즘에서 첫 번째 내지 세 번째 for-loop를 루프 언롤링(loop-unrolling)을 수행할 수 있다. 구체적으로, Output stationary 모드에 따른 Systolic array 구조인 경우, 첫 번째의 for-loop(Num filters)를 프로세싱 엘리먼트 어레이의 행(row)의 수만 큼 병렬로 연산하고, 두 번째 for-loop(num_conv_window(height))와 세 번째 for- loop(num_conv_window(width))를 프로세싱 엘리먼트 어레이의 열(column)의 수만큼 병렬로 연산할 수 있다. 본 개시의 실시예에 따른 신경 프로세싱 유닛은, 입력 특징맵과 1개의 커널을 입력으로 하는 MAC 연산을 1 cycle이라고 했을 때, 프로세싱 엘리먼트의 개수, 입력 특징맵 및/또는 가중치를 이용하여 총 MAC 연산 시간을 계산할 수 있다. 도 11은 본 개시의 실시예에 따른 변경가능한 MAC 아키텍쳐를 나타내는 구성도이다. 도 11을 참조하면, 4개의 MAC 연산기(1100_1, 1100_2, 1100_3, 1100_4)를 포함하는 MAC 아키텍쳐가 도시된다. 제1 MAC 연산기(1100_1), 제2 MAC 연산기(1100_2), 제3 MAC 연산기(1100_3) 및 제4 MAC 연산기(1100_4) 각각은 변경 가능한 MAC(Configurable MAC)의 구조를 가진다. 변경 가능한 MAC이란, 컴파일러 또는 컨트롤러의 제어 신 호에 의해 stationary 모드를 변경할 수 있는 구조의 MAC을 의미한다. 도 11에 도시된 MAC 아키텍쳐는 일 실시 예에 불과하고, MAC 연산기의 개수는 본 개시의 기술적 범위를 제한하지 않는다. 도 11을 참조하면, 제1 MAC 연산기(1100_1)는 제1 멀티플렉서, 제2 멀티플렉서, 레지스터 파일 및/또는 디코더(dec)를 포함할 수 있다. 제1 멀티플렉서는 가중치(w)(1101_1) 또는 레지스터 파일로부터 전달된 데이터 중 어느 하나를 선택 하여 출력할 수 있다. 제2 멀티플렉서는 부분 합(p)(1102_1) 또는 레지스터 파일로부터 전달된 데이터 중 어느 하나를 선택 하여 출력할 수 있다. 본 개시의 실시예에 따른 인공 프로세싱 유닛은 제1 멀티플렉서의 출력 값과 입력 특징맵(f)(1104_1) 간의 곱셈을 연산하고, 연산된 결과 값과 제2 멀티플렉서의 출력 간의 덧셈을 연산할 수 있다. 디코더는 덧셈의 결과 값을 수신하여 제3 MAC 연산기(1100_3) 또는 제1 MAC 연산기(1100_1)의 레지스터 파일에 전달할 수 있다. 도 12는 본 개시의 실시예에 따른 Output stationary 모드에서의 MAC 아키텍쳐를 나타내는 구성도이다. 도 12를 참조하면, Output stationary 모드에서의 4개의 MAC 연산기(1200_1, 1200_2, 1200_3, 1200_4)를 포함 하는 MAC 아키텍쳐가 도시된다. 도 12에서, 점선은 Output stationary 모드에서 실제 데이터가 전달되는 경로를 의미한다. 제1 MAC 연산기(1200_1)는 제1 멀티플렉서, 제2 멀티플렉서, 레지스터 파일, 디코더(dec)(120 5)를 포함할 수 있다. 제1 멀티플렉서는 가중치(w)(1201_1) 또는 레지스터 파일로부터 전달된 데이터 중 가중치(w)(1201_ 1)를 선택하여 출력할 수 있다. 가중치(w)(1201_1)는 제1 멀티플렉서에 전달됨과 동시에, 제3 MAC 연산기(1200_3)로 전달되어 재사용될 수 있다. 실시예에 따라, 가중치(w)(1201_1)는 파이프라인(pipeline) 형식으로 같은 열(column)에 위치한 MAC 연산 기들로 전달될 수 있다. 제2 멀티플렉서는 부분 합(p)(1102_1) 또는 레지스터 파일로부터 전달된 데이터 중 레지스터 파일 로부터 전달된 데이터를 선택하여 출력할 수 있다. 도 12에서, 레지스터 파일은 부분 합(partial sum)이 저장되는 누산기(accumulator)일 수 있다. 따라서, 레지스터 파일로부터 제2 멀티플렉서에 전 달된 데이터는 부분 합일 수 있다. 본 개시의 실시예에 따른 인공 프로세싱 유닛은 제1 멀티플렉서의 출력 값인 가중치(w)와 입력 특징맵 (f)(1204_1) 간의 곱셈을 연산하고, 연산된 결과 값과 제2 멀티플렉서의 출력 값인 부분합 간의 덧셈을 연 산할 수 있다. 디코더는 덧셈의 결과 값을 수신하여, 제1 MAC 연산기(1200_1)의 레지스터 파일에 전달할 수 있다. 입력 특징맵(f)(1204_1)은 제1 멀티플렉서의 출력 값인 가중치(w)와 곱셈을 연산하기 위해 전달됨과 동시에, 제2 MAC 연산기(1200_2)로 전달되어 재사용될 수 있다. 실시예에 따라, 입력 특징맵(f)(1204_1)은 파이 프라인(pipeline) 형식으로 같은 행(row)에 위치한 MAC 연산기들로 전달될 수 있다. 도 13은 본 개시의 실시예에 따른 Weight stationary 또는 Input stationary 모드에서의 MAC 아키텍쳐를 나타 내는 구성도이다. 도 13을 참조하면, Weight stationary 또는 Input stationary 모드에서의 4개의 MAC 연산기(1300_1, 1300_2, 1300_3, 1300_4)를 포함하는 MAC 아키텍쳐가 도시된다. 도 13에서, 점선은 Weight stationary 또는 Input stationary 모드에서 실제 데이터가 전달되는 경로를 의미한다. 제1 MAC 연산기(1300_1)는 제1 멀티플렉서, 제2 멀티플렉서, 레지스터 파일, 디코더(dec)(130 5)를 포함할 수 있다. 제1 멀티플렉서는 가중치(w)(1301_1) 또는 레지스터 파일로부터 전달된 데이터 중 레지스터 파일 로부터 전달된 데이터를 선택하여 출력할 수 있다. 제2 멀티플렉서는 부분 합(p)(1302_1) 또는 레지스터 파일로부터 전달된 데이터 중 부분 합 (p)(1302_1)을 선택하여 출력할 수 있다. 도 12에서, 레지스터 파일에는 Weight stationary 모드인 경우 에는 가중치, Input stationary 모드인 경우에는 입력 특징맵이 저장될 수 있다. Stationary 모드에 따라 저장 된 가중치 또는 입력 특징맵은 'Preload data'로 지칭될 수 있다. 따라서, 레지스터 파일로부터 제2 멀티 플렉서에 전달된 데이터는 가중치 또는 입력 특징맵일 수 있다. 본 개시의 실시예에 따른 인공 프로세싱 유닛은 제1 멀티플렉서의 출력 값인 Preload data와 입력 특징맵 (f)(1204_1) 간의 곱셈을 연산하고, 연산된 결과 값과 제2 멀티플렉서의 출력 값인 부분 합(p)(1302_1) 간 의 덧셈을 연산할 수 있다. 디코더는 덧셈의 결과 값을 수신하여, 제3 MAC 연산기(1200_1)의 부분합(p)(1302_3)으로 재사용할 수 있 도록 전달할 수 있다. 입력 특징맵(f)(1304_1)은 제1 멀티플렉서의 출력 값인 Preload data와 곱셈을 연산하기 위해 전달됨과 동 시에, 제2 MAC 연산기(1200_2)로 전달되어 재사용될 수 있다. 실시예에 따라, 입력 특징맵(f)(1204_1)은 파이프라인(pipeline) 형식으로 같은 행(row)에 위치한 MAC 연산기들로 전달될 수 있다. 본 개시의 실시예에 따른 신경 프로세싱 유닛의 동작 방법에 따르면, 인공신경망의 각 레이어의 특성에 따라 프 로세싱 엘리먼트 어레이에서의 MAC 연산 시간을 계산하고, 계산된 MAC 연산 시간에 기초하여 레이어 별로 MAC 연산 구조를 변경(switch)할 수 있다. 따라서, 레이어 별로 3개 모드의 MAC 연산 구조 중 최적의 MAC 연산 구조 를 채택함으로써 신경 프로세싱 유닛의 MAC MAC 연산량을 최소화할 수 있다. 본 개시의 다른 실시예에 따른 신경 프로세싱 유닛의 동작 방법에 따르면, 인공신경망의 입력 특징맵의 특성에 따라 프로세싱 엘리먼트 어레이에서의 MAC 연산 시간을 계산하고, 계산된 MAC 연산 시간에 기초하여 입력 특징 맵의 타일(tile)(또는, 패치(patch)) 별로 MAC 연산 구조를 변경(switch)할 수 있다. 따라서, 레이어 별로 3개 모드의 MAC 연산 구조 중 최적의 MAC 연산 구조를 채택함으로써 신경 프로세싱 유닛의 MAC 연산량을 최소화할 수 있다. 본 명세서와 도면에 게시된 본 개시의 예시들은 본 개시의 기술내용을 쉽게 설명하고 본 개시의 이해를 돕기 위 해 특정 예를 제시한 것뿐이며, 본 명의 범위를 한정하고자 하는 것은 아니다. 여기에 게시된 예시들 이외에도 발명의 기술적 사상에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은 본 개시가 속하는 기술 분야에서 통상 의 지식을 가진 자에게 자명한 것이다."}
{"patent_id": "10-2024-7031904", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 신경 프로세싱 유닛이 포함된 장치를 설명하는 개략적인 개념도이다. 도 2는 본 개시의 일 실시예에 따른 신경 프로세싱 유닛을 설명하는 개략적인 개념도이다. 도 3은 본 개시에 적용될 수 있는 프로세싱 엘리먼트 어레이 중 하나의 프로세싱 엘리먼트를 설명하는 개략적인 개념도이다. 도 4는 본 개시의 일 실시예에 따른 인공신경망의 구조를 나타내는 개략도이다. 도 5는 본 개시의 비교예에 따른 전치 합성곱 연산을 나타내는 개략도이다. 도 6은 본 개시의 실시예에 따른 전치 합성곱 연산을 나타내는 개략도이다. 도 7은 본 개시의 실시예에 따른 신경 프로세싱 유닛이 동작하는 방법을 설명하는 순서도이다. 도 8은 본 개시의 실시예에 따른 복수 개의 서브 커널들 및 이에 대응되는 입력 데이터를 나타내는 개략도이다. 도 9는 본 개시의 실시예에 따른 신경 프로세싱 유닛에서 복수 개의 서브 커널들을 이용하여 수행되는 합성곱 연산을 나타내는 개략도이다. 도 10은 본 개시의 비교예에 따른 systolic array 구조들을 나타내는 개략도이다. 도 11은 본 개시의 실시예에 따른 변경가능한 MAC 아키텍쳐를 나타내는 구성도이다. 도 12는 본 개시의 실시예에 따른 Output stationary 모드에서의 MAC 아키텍쳐를 나타내는 구성도이다. 도 13은 본 개시의 실시예에 따른 Weight stationary 또는 Input stationary 모드에서의 MAC 아키텍쳐를 나타 내는 구성도이다."}
