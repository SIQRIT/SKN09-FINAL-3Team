{"patent_id": "10-2019-0125202", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0042537", "출원번호": "10-2019-0125202", "발명의 명칭": "대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법 및 이를 구현하는 로봇과 클라우드", "출원인": "엘지전자 주식회사", "발명자": "주진우"}}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "로봇이 이동하는 공간을 촬영하는 카메라 센서; 상기 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기와 상기 로컬영역 각각에 대응하여 상기로봇의 위치를 추정하는 다수의 위치 추정기를 포함하며, 상기 로컬영역 분류기에 상기 카메라 센서가 촬영한이미지를 입력하여 상기 로봇이 위치한 로컬영역을 식별하며, 상기 식별된 로컬영역에 대응하는 위치 추정기에상기 카메라 센서가 촬영한 이미지를 입력하여 상기 로컬영역 내의 위치 정보를 산출하는 제어부; 및상기 로봇을 이동시키는 이동부를 포함하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 로컬영역 분류기가 둘 이상의 로컬영역을 출력할 경우, 상기 제어부는 상기 로봇이 이전에 위치했던 로컬영역에 대한 정보 또는 상기 출력된 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하여 출력된둘 이상의 로컬영역에서 하나의 로컬 영역을 선택하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 로컬 영역 분류기는 제1시점에서 입력된 이미지에 기반하여 제1예측 결과를 산출하며, 상기 로컬 영역분류기는 제2시점에서 입력된 이미지에 기반하여 제2예측 결과를 산출하며, 상기 제1시점 이후 상기 제2시점이 후행하며, 상기 식별한 로컬 영역과 상기 제1예측 결과 사이의 유사도는 상기 식별한 로컬 영역과 상기 제2예측 결과 사이의 유사도보다 낮거나 동일한 것을 특징으로 하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 제어부는 상기 로봇의 현재 위치가 교차로인지 판단한 후, 교차로인 경우, 해당 교차로를 중심으로 클로즈루프를 형성하며 이동하며 상기 카메라 센서가 촬영하고, 상기 로컬영역 분류기는 상기 촬영된 이미지들을 이용하여 현재 로봇이 위치하는 로컬 영역을 식별하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항에 있어서, 상기 제어부는 상기 로봇의 현재 위치가 교차로인지 판단한 후, 교차로가 아닌 경우, 상기 로봇의 현재 위치인제1위치에서 제2위치로 이동 후 다시 제1위치로 복귀하며 상기 카메라 센서가 촬영하거나 상기 제1위치에서 상기 카메라 센서가 회전하며 촬영한 후, 공개특허 10-2021-0042537-3-상기 로컬영역 분류기는 상기 촬영된 이미지들을 이용하여 현재 로봇이 위치하는 로컬 영역을 식별하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 제어부는 상기 로봇이 이동한 과정에서 촬영한 이미지 또는 미리 저장된 맵을 이용하여 교차로를식별하고, 상기 식별된 교차로 사이의 거리를 계산하여 상기 교차로를 중심으로 하는 로컬영역 또는 상기 교차로 사이의 영역을 포함하는 로컬영역들로 상기 로봇이 이동한 공간을 구분하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 로컬영역은 인접한 로컬영역과 중첩하는 중첩 영역을 포함하는, 대면적의 공간에서 로컬 영역별로 위치를추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 로컬영역 분류기는 상기 로컬영역별로 수집된 이미지를 입력받아 로컬영역에 대한 학습을 수행하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "로봇이 이동하는 공간을 촬영한 이미지를 상기 로봇으로부터 수신하는 통신부; 및상기 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기 또는 상기 로컬영역 각각에 대응하여상기 로봇의 위치를 추정하는 다수의 위치 추정기 중 어느 하나 이상을 포함하며,상기 통신부가 수신한 이미지에 대응하는 로컬영역에 대한 식별 정보 또는 상기 로컬영역 내의 위치 정보를 산출하는 서버 제어부를 포함하며상기 통신부는 상기 로컬영역에 대한 식별정보 또는 상기 로컬영역 내의 위치정보를 상기 로봇에게 전송하는,대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드 서버."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 로컬영역 분류기가 둘 이상의 로컬영역을 출력할 경우, 상기 서버 제어부는 상기 로봇이 이전에 위치했던로컬영역에 대한 정보 또는 상기 출력된 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하여 출력된 둘 이상의 로컬영역에서 하나의 로컬 영역을 선택하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는클라우드 서버."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 공개특허 10-2021-0042537-4-상기 로컬 영역 분류기는 제1시점에서 입력된 이미지에 기반하여 제1예측 결과를 산출하며, 상기 로컬 영역분류기는 제2시점에서 입력된 이미지에 기반하여 제2예측 결과를 산출하며, 상기 제1시점 이후 상기 제2시점이 후행하며, 상기 식별한 로컬 영역과 상기 제1예측 결과 사이의 유사도는 상기 식별한 로컬 영역과 상기 제2예측 결과 사이의 유사도보다 낮거나 동일한 것을 특징으로 하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드서버."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제9항에 있어서, 상기 서버 제어부는 상기 로봇이 이동한 과정에서 촬영하여 전송한 이미지 또는 미리 저장된 맵을 이용하여 교차로를 식별하고, 상기 식별된 교차로 사이의 거리를 계산하여 상기 교차로를 중심으로 하는 로컬영역 또는 상기 교차로 사이의 영역을 포함하는 로컬영역들로 상기 로봇이 이동한 공간을 구분하는, 대면적의 공간에서 로컬영역별로 위치를 추정하는 클라우드 서버."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제6항에 있어서, 상기 로컬영역은 인접한 로컬영역과 중첩하는 중첩 영역을 포함하며, 상기 로컬영역 분류기는 상기 로컬영역별로 수집된 이미지를 입력받아 로컬영역에 대한 학습을 수행하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드 서버."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "로봇의 카메라 센서가 상기 로봇이 이동하는 공간을 촬영하는 단계; 상기 로봇의 제어부가 포함하는 로컬영역 분류기가 상기 이미지를 입력받아 상기 로봇이 위치한 로컬영역을 식별하는 단계; 상기 제어부는 다수의 위치 추정기를 포함하며, 상기 제어부는 상기 다수의 위치 추정기 중에서 상기 식별된 로컬 영역에 대응하는 위치 추정기를 선택하는 단계; 및상기 제어부는 상기 위치추정기에게 상기 카메라 센서가 촬영한 이미지를 입력하고 상기 위치추정기가 상기 로컬영역 내의 위치 정보를 산출하는 단계를 포함하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 로컬영역 분류기가 둘 이상의 로컬영역을 출력할 경우, 상기 제어부는 상기 로봇이 이전에 위치했던 로컬영역에 대한 정보 또는 상기 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하여 출력된 둘 이상의 로컬영역에서 하나의 로컬 영역을 선택하는 단계를 더 포함하는, 대면적의 공간에서 로컬 영역별로 위치를추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 로컬 영역 분류기는 제1시점에서 입력된 이미지에 기반하여 제1예측 결과를 산출하며, 상기 로컬 영역분류공개특허 10-2021-0042537-5-기는 제2시점에서 입력된 이미지에 기반하여 제2예측 결과를 산출하는 단계를 더 포함하며, 상기 제1시점 이후 상기 제2시점이 후행하며, 상기 식별한 로컬 영역과 상기 제1예측 결과 사이의 유사도는 상기 식별한 로컬 영역과 상기 제2예측 결과 사이의 유사도보다 낮거나 동일한 것을 특징으로 하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16항에 있어서, 상기 제어부는 상기 로봇이 전송한 이미지를 분석하여 상기 로봇의 현재 위치가 교차로인지 판단한 후, 교차로인 경우, 해당 교차로를 중심으로 클로즈 루프를 형성하며 이동하도록 상기 로봇에게 지시하며, 교차로가 아닌 경우, 상기 로봇의 현재 위치인 제1위치에서 제2위치로 이동 후 다시 제1위치로 복귀하거나 또는상기 제1위치에서 상기 로봇의 카메라 센서의 회전을 지시하며, 상기 로컬영역 분류기는 상기 로봇이 상기 지시 이후 촬영한 이미지들을 수신하여 상기 로봇이 위치하는 로컬영역을 식별하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제14항에 있어서, 상기 제어부는 상기 로봇이 이동한 과정에서 촬영한 이미지 또는 미리 저장된 맵을 이용하여 교차로를식별하고, 상기 식별된 교차로 사이의 거리를 계산하여 상기 교차로를 중심으로 하는 로컬영역 또는 상기 교차로 사이의 영역을 포함하는 로컬영역들로 상기 로봇이 이동한 공간을 구분하는 단계를 더 포함하는, 대면적의공간에서 로컬 영역별로 위치를 추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서, 상기 로컬영역은 인접한 로컬영역과 중첩하는 중첩 영역을 포함하는, 대면적의 공간에서 로컬 영역별로 위치를추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제18항에 있어서, 상기 로컬영역 분류기는 상기 로컬영역별로 수집된 이미지를 입력받아 로컬영역에 대한 학습을 수행하는 단계를더 포함하는, 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법 및 이를 구현하는 로봇과 클라우드 서버에 관 한 것으로, 본 발명의 일 실시예에 따른 로봇은 상기 로봇이 이동하는 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기와 상기 로컬영역 각각에 대응하여 상기 로봇의 위치를 추정하는 다수의 위치 추정기를 포함한다."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법 및 이를 구현하는 로봇과 클라우드 서버에 관한 기술이다."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "대면적 공간 예를 들어, 대면적의 실내 공간에서 로봇이 주행할 경우, 로봇의 현재 위치를 확인하는 과정이 필 요하다. 특히, 로봇이 자율 주행하여 다양한 작업(배송, 안내, 청소, 보안 등)을 수행하는 과정에서는 로봇의현재 위치 정보를 정밀하게 확보해야 한다. 종래에 대면적의 공간에서 위치 정보의 확보하기 위해서는 GPS 등을 이용하여 위치를 확인할 수 있었으나, GPS 장치가 가지는 정밀도가 로봇의 작업 수행에 필요한 수준을 만족시키지 못하는 경우가 있다. 또한, GPS 장치는 실내에서 정확도가 떨어지거나 신호 수신이 불가능할 때가 있다. 이에, 본 명세서에서는 대면적의 공간에서 로봇이 정밀하게 위치 정보를 확보하는 방안에 대해 살펴본다."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 명세서에서는 대면적의 공간에서 로봇 또는 클라우드 서버가 로봇의 위치를 1차적으로 로컬 영역으로 식별한 후, 해당 로컬 영역에 정밀하게 위치를 추정하는 방안을 제공하고자 한다. 또한, 본 명세서에서는 로봇 또는 클라우드 서버가 대면적의 공간을 로컬 영역으로 분할하여 각각의 로컬 영역 에 최적화된 위치추정 딥러닝 네트워크를 이용하여 위치 정보를 확보하는 방안을 제공하고자 한다. 또한, 본 명세서에서는 로봇 또는 클라우드 서버가 대면적의 공간의 위치 정보 획득에 최적화된 로컬 영역의 구 성에 기반하여 위치 정보를 확보하는 방안을 제공하고자 한다. 본 발명의 목적들은 이상에서 언급한 목적으로 제한되지 않으며, 언급되지 않은 본 발명의 다른 목적 및 장점들 은 하기의 설명에 의해서 이해될 수 있고, 본 발명의 실시예에 의해 보다 분명하게 이해될 것이다. 또한, 본 발 명의 목적 및 장점들은 특허 청구 범위에 나타낸 수단 및 그 조합에 의해 실현될 수 있음을 쉽게 알 수 있을 것 이다."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇은 로봇이 이동하는 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기와 상기 로컬영역 각각에 대응하여 상기 로봇의 위치 를 추정하는 다수의 위치 추정기를 포함한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇은 로컬영역 분류기 또는 위치추정기 중 어느 하나 이상을 포함하며, 로컬영역 분류기에 상기 카메라 센서가 촬영한 이미지를 입력하여 상기 로봇이 위치한 로컬영역을 식별하며, 상기 식별된 로컬영역에 대응하는 위치 추정기에 상기 카메라 센서가 촬영한 이미지를 입력하여 상기 로컬영역 내의 위치 정보를 산출하는 제어부를 포함한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇은 상기 로컬영역 분류기가 둘 이상의 로컬영역을 출력할 경우, 상기 제어부는 상기 로봇이 이전에 위치했던 로컬영역에 대한 정보 또는 상 기 출력된 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하여 출력된 둘 이상의 로컬영역에서 하 나의 로컬 영역을 선택한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 로봇의 로컬 영역 분류기는 제1 시점에서 입력된 이미지에 기반하여 제1예측 결과를 산출하며, 상기 로컬 영역분류기는 제2시점에서 입력된 이 미지에 기반하여 제2예측 결과를 산출하며, 상기 제1시점 이후 상기 제2시점이 후행하며, 상기 식별한 로컬 영 역과 상기 제1예측 결과 사이의 유사도는 상기 식별한 로컬 영역과 상기 제2예측 결과 사이의 유사도보다 낮거 나 동일하다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드 서버는 로봇이 이동하 는 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기 또는 상기 로컬영역 각각에 대응하여 상 기 로봇의 위치를 추정하는 다수의 위치 추정기 중 어느 하나 이상을 포함한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드 서버는 로컬영역 분류 기 또는 위치추정기 중 어느 하나 이상을 포함하며, 상기 통신부가 수신한 이미지에 대응하는 로컬영역에 대한 식별 정보 또는 상기 로컬영역 내의 위치 정보를 산출하는 서버 제어부를 포함한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 클라우드 서버의 로컬영역 분류 기가 둘 이상의 로컬영역을 출력할 경우, 상기 서버 제어부는 상기 로봇이 이전에 위치했던 로컬영역에 대한 정 보 또는 상기 출력된 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하여 출력된 둘 이상의 로컬영역에서 하나의 로컬 영역을 선택한다. 본 발명의 일 실시예에 따른 대면적의 공간에서 로컬 영역별로 위치를 추정하는 방법은 로봇의 카메라 센서가 상기 로봇이 이동하는 공간을 촬영하는 단계와, 상기 로봇의 제어부가 포함하는 로컬영역 분류기가 상기 이미지 를 입력받아 상기 로봇이 위치한 로컬영역을 식별하는 단계와, 상기 제어부는 다수의 위치 추정기를 포함하며, 상기 제어부는 상기 다수의 위치 추정기 중에서 상기 식별된 로컬 영역에 대응하는 위치 추정기를 선택하는 단 계와 상기 제어부는 상기 위치추정기에게 상기 카메라 센서가 촬영한 이미지를 입력하고 상기 위치추정기가 상 기 로컬영역 내의 위치 정보를 산출하는 단계를 포함한다."}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 실시예들을 적용할 경우, 로봇 또는 클라우드 서버가 로봇의 위치를 1차적으로 로컬 영역으로 식별한 후, 해당 로컬 영역에 정밀하게 위치를 추정할 수 있다. 본 발명의 실시예들을 적용할 경우, 로봇 또는 클라우드 서버가 대면적의 공간을 로컬 영역으로 분할하여 각각 의 로컬 영역에 최적화된 위치추정 딥러닝 네트워크를 이용하여 위치 정보를 확보할 수 있다. 본 발명의 실시예들을 적용할 경우, 로봇 또는 클라우드 서버가 대면적의 공간의 위치 정보 획득에 최적화된 로 컬 영역의 구성에 기반하여 위치 정보를 확보할 수 있다. 상술한 효과와 더불어 본 발명의 구체적인 효과는 이하"}
{"patent_id": "10-2019-0125202", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "전술한 목적, 특징 및 장점은 첨부된 도면을 참조하여 상세하게 후술되며, 이에 따라 본 발명이 속하는 기술분 야에서 통상의 지식을 가진 자가 본 발명의 기술적 사상을 용이하게 실시할 수 있을 것이다. 본 발명을 설명함 에 있어서 본 발명과 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 불필요하게 흐릴 수 있다고 판단되는 경우에는 상세한 설명을 생략한다. 이하, 첨부된 도면을 참조하여 본 발명에 따른 바람직한 실시예를 상세히 설명하기로 한다. 도면에서 동일한 참조부호는 동일 또는 유사한 구성요소를 가리키는 것으로 사용된다. 이하, 도면을 참조하여 본 발명의 실시예에 대하여 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시예에 한정되지 않는다. 본 발명을 명확하게 설명하기 위해서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 동일 또는 유사한 구성요소에 대해서는 동일한 참조 부호를 붙이도록 한다. 또한, 본 발명의 일부 실시예들을 예시적인 도 면을 참조하여 상세하게 설명한다. 각 도면의 구성요소들에 참조부호를 부가함에 있어서, 동일한 구성요소들에 대해서는 비록 다른 도면상에 표시되더라도 가능한 한 동일한 부호를 가질 수 있다. 또한, 본 발명을 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략할 수 있다. 본 발명의 구성 요소를 설명하는 데 있어서, 제 1, 제 2, A, B, (a), (b) 등의 용어를 사용할 수 있다. 이러한 용어는 그 구성 요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 용어에 의해 해당 구성 요소의 본질, 차례, 순서 또는 개수 등이 한정되지 않는다. 어떤 구성 요소가 다른 구성요소에 \"연결\", \"결합\" 또는 \"접속\"된 다고 기재된 경우, 그 구성 요소는 그 다른 구성요소에 직접적으로 연결되거나 또는 접속될 수 있지만, 각 구성 요소 사이에 다른 구성 요소가 \"개재\"되거나, 각 구성 요소가 다른 구성 요소를 통해 \"연결\", \"결합\" 또는 \"접 속\"될 수도 있다고 이해되어야 할 것이다. 비록 제1, 제2 등이 다양한 구성요소들을 서술하기 위해서 사용되나, 이들 구성요소들은 이들 용어에 의해 제한 되지 않음은 물론이다. 이들 용어들은 단지 하나의 구성요소를 다른 구성요소와 구별하기 위하여 사용하는 것으 로, 특별히 반대되는 기재가 없는 한, 제1 구성요소는 제2 구성요소일 수도 있음은 물론이다. 명세서 전체에서, 특별히 반대되는 기재가 없는 한, 각 구성요소는 단수일 수도 있고 복수일 수도 있다. 본 명세서에서 사용되는 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"구성된다\" 또는 \"포함한다\" 등의 용어는 명세서 상에 기재된 여러 구성 요소들, 또는 여러 단계들을 반드시 모두 포함하는 것으로 해석되지 않아야 하며, 그 중 일부 구성 요소들 또는 일부 단계들은 포함되지 않 을 수도 있고, 또는 추가적인 구성 요소 또는 단계들을 더 포함할 수 있는 것으로 해석되어야 한다. 명세서 전체에서, \"A 및/또는 B\" 라고 할 때, 이는 특별한 반대되는 기재가 없는 한, A, B 또는 A 및 B 를 의미 하며, \"C 내지 D\" 라고 할 때, 이는 특별한 반대되는 기재가 없는 한, C 이상이고 D 이하인 것을 의미한다. 또한, 본 발명을 구현함에 있어서 설명의 편의를 위하여 구성요소를 세분화하여 설명할 수 있으나, 이들 구성요 소가 하나의 장치 또는 모듈 내에 구현될 수도 있고, 혹은 하나의 구성요소가 다수의 장치 또는 모듈들에 나뉘 어져서 구현될 수도 있다. 이하, 본 명세서에서 로봇은 다양한 형상을 가지며, 특정한 목적(배송, 청소, 보안, 모니터링, 안내 등)을 가지 거나 혹은 로봇이 이동하는 공간의 특성에 따른 기능을 제공하며 이동하는 장치를 포함한다. 따라서, 본 명세서 에서의 로봇은 소정의 정보와 센서를 이용하여 이동할 수 있는 이동수단을 보유하며 소정의 기능을 제공하는 장 치를 통칭한다. 본 명세서에서 로봇은 맵을 보유하면서 이동할 수 있다. 맵은 공간에서 이동하지 않는 것으로 확인된 거리의 이 미지, 고정된 벽, 건물, 계단 등 고정 객체에 대한 정보를 의미한다. 또한, 주기적으로 배치되는 이동 장애물, 즉 동적인 객체들에 대한 정보도 맵 상에 저장될 수 있다. 본 명세서에서 로봇은 대면적의 공간을 이동하며 정밀한 위치정보를 확보하기 위해 클라우드 서버와 통신으로 정보를 송수신할 수 있다. 그리고 이 과정에서 로봇은 클라우드 서버로부터 맵 정보를 수신할 수도 있다. 또는 로봇이 위치한 영역을 포함한 일정 범위 내에서 로봇이 취득한 정보를 클라우드 서버에게 전송할 수 있다. 그리고, 클라우드 서버는 수신된 정보를 처리하여 새로운 맵 정보를 생성하고 이를 로봇에게 전송할 수 있다. 로봇은 수신된 정보 또는 로봇 자체적으로 보유하는 정보들을 이용하여 로봇의 현재 위치 정보를 확인할 수 있 다. 대면적의 공간에서 로봇이 이동하거나 위치 정보를 확보하기 위해서는 로봇이 위치 정보를 확보할 수 있어야 한 다. 특히, 대면적의 실내공간에서 로봇이 주행 중이던 위치를 이탈해 현재 새로 놓여진 위치를 정확하게 확인하는데 에는 많은 시간이나 컴퓨팅 자원을 필요로 한다. . 딥러닝 기술을 통해 입력한 로봇의 영상으로 로봇의 현재 위치 좌표를 추정하는 기술 역시 대면적의 공간에 적 용 시 많은 시간이나 컴퓨팅 자원을 필요로 한다. 특히, 공항, 쇼핑몰 등 대면적 실내 공간에서 하나의 딥러닝 네트워크를 이용하여 위치를 확인하는데에는 한계가 있다. 따라서, 본 명세서에서는 맵 및 이 맵을 구성하는 딥러닝 네트워크를 여러 개로 분할하여 구성하는 실시예를 제 시한다. 이하, 실내 공간을 중심으로 설명하지만, 본 발명이 이에 한정되는 것은 아니다. 본 발명은 대규모 공 간에서 로봇의 위치 추정을 위해 로컬영역 각각에 별도의 딥러닝 네트워크를 적용하는 기술의 다양한 실시예를 포함한다. 도 1은 본 발명의 일 실시예에 의한 로컬영역을 보여준다. 로봇 또는 클라우드 서버는 대면적 실내공간을 일정 한 로컬영역(local area)으로 나누어 각 로컬영역 별로 위치를 추정하는 딥러닝 네트워크를 개별적으로 학습한 다. 로컬영역을 구별하는 딥러닝에 기반한 분류기를 통해 해당하는 로컬영역의 딥러닝 네트워크를 선택하여 이 를 통해 최종적으로 로봇의 현위치를 추정한다. 도 1에 제시된 바와 같이, 로봇이 이동하는 대면적의 영역은 다수의 로컬영역(area01~area06)로 구분된다. 로봇 또는 클라우드 서버는 원형, 사각형 등 다양한 방식으로 경계를 구분하여 로컬영역들을 구분한다. 이때, 각 로컬영역은 다른 로컬영역과 일부 중첩될 수 있다. 도 1에서 각 로컬 영영에 로봇이 있을 경우 로봇이 사용하게 될 딥러링 네트워크는 각각 DNN_Pose_Net01~06이다. 이를 정리하면 표 1과 같다. 표 1 로봇의 현재 위치 적용할 딥러닝 네트워크 Area01 DNN_Pose_Net01 Area02 DNN_Pose_Net02 Area03 DNN_Pose_Net03 Area04 DNN_Pose_Net04 Area05 DNN_Pose_Net05 Area06 DNN_Pose_Net06 도 1과 같이 로봇 또는 클라우드 서버는 위치정보를 알아내려는 대상공간을 여러 개의 로컬영역으로 분류한다. 그리고 이들 나누어진 각각의 로컬영역 별로 로봇 또는 클라우드 서버는 위치정보를 추정하는 딥러닝 네트워크 (로컬영역별 딥러닝 네트워크)를 여러 개 학습하여 대응한다. 또한, 로봇 또는 클라우드 서버는 현재 위치가 전체 대면적에서 어떤 로컬영역에 해당하는지 알아내는 분류기의 기능을 수행하는 딥러닝 네크워크를 이용하여 로컬영역을 판단하기 위해 딥러닝 네트워크(로컬영역 식별을 위한 딥러닝 네트워크)를 별도로 학습한다. 그리고 로봇 또는 클라우드 서버는 로컬영역 식별을 위한 딥러닝 네트워크를 적용하여 현재 로봇이 위치해 있는 로컬영역을 1차적으로 알아낸다. 그리고 다음으로 각각의 로컬영역 별로 학습한 위치 인식을 위한 딥러닝 네크 워크를 대응하는 로컬영역과 매칭하여 현위치의 정확한 좌표를 추론한다. 정확도를 높이기 위해 로봇 또는 클라우드 서버는 시계열 딥러닝 네트워크를 사용할 수 있다. 이에 대해 보다 상세히 살펴본다. 도 2는 본 발명의 일 실시예에 의한 계층적 딥러닝 네트워크의 구성을 보여준다. 로봇이 주변의 영상을 촬영하여 획득한 이미지를 로컬영역 분류기(Local area classifier)(도 3, 4의 256 참 조)인 딥러닝 뉴럴 네트워크(Deep neural network, DNN, 10)에 입력한다(S3). 로컬영역 분류기인 DNN은 입 력된 이미지를 이용하여 현재 로봇의 위치가 도 1의 로컬영역들 중 어디에 위치하는지를 판단한다. 판단 결과 로컬영역 분류기인 DNN은 어느 하나의 로컬영역을 지시하는 정보를 출력한다(S4). 또는 S4의 프 로세스에서 두 개 이상의 로컬영역을 지시하는 정보가 출력될 수 있다. 위치 추정 속도를 높이기 위해 로컬영역 분류기인 DNN은 두 개 이상의 로컬영역을 지시하되, 로컬영역들에 대한 우선 순위도 함께 출력할 수 있다. S4에서 지시된 특정 로컬영역이 Area02인 경우를 가정한다. 이 경우 Area 02에 적용 가능한 위치 추정기(pose estimator)인 DNN에 이미지가 입력된다. 이미지는 Area 02의 위치 추정을 위한 딥러닝 네트워크에 입력되어 Area02에 대응하는 로컬맵(localmap02)에서의 위치를 출력한다(S5b). 그리고 로봇 또는 클라우드 서버는 Area02의 로컬 맵에서의 위치를 글로벌 좌표로 보정한다(S6). 이를 세분화 하면, 로봇 또는 클라우드 서버는 Area 02에 해당하는 로컬 맵 내에서 로봇의 위치를 지시하는 로컬 좌표 (localmap02 coordinate)를 글로벌 오프셋(global offset)을 이용하여 캘리브레이션한다(S6a). 그리고, 로봇 또는 클라우드 서버는 캘리브레이션한 결과로 글로벌 좌표(global coordinate)를 산출한다(S6b). 도 2의 과정을 정리하면 다음과 같다. 대면적의 공간, 특히 GPS 신호를 수신하기 어려운 실내공간에서 로봇 또 는 클라우드 서버는 입력된 영상을 이용하여, 현재 로봇이 위치하고 있는 로컬영역을 식별한다. 그리고 로봇 또는 클라우드 서버는 식별한 로컬영역 내에서의 위치좌표를 추론하는 딥러닝 네트워크를 검색한다. 전술한 과정에서 검색된 딥러닝 네트워크에 입력된 영상을 입력하여 정확한 로컬영역의 위치좌표를 추론하고 로 컬좌표를 전체 글로벌 좌표로 보정하여 로봇의 글로벌 좌표를 산출할 수 있다. 도 2에서 11 내지 16과 같이 특정 로컬영역에 대응하는 위치 추정기들(도 4의 257 참조)인 딥러닝 네트워크들은 해당 영역 내에서만 입력된 영상을 이용하여 로컬 위치를 산출한다. 한편, 로컬영역 분류기(local area classifier)인 딥러닝 네트워크는 대규모 공간에서 로봇의 위치를 로컬 영역으로 확인하기 위해 11 내지 16의 딥러닝 네트워크와 상이한 구성을 가질 수 있다. 로컬영역 분류기 역시 로봇 또는 클라우드 서버 내에서 소프트웨어 또는 하드웨어의 형태로 구현될 수 있다. 이하, 특정 로컬영역에 대응하도록 학습된 딥러닝 네트워크를 위치 추정기(pose estimator)로 지칭한다. 그리고 로컬영역 분류기는 로봇이 어느 로컬영역에 위치하는지를 식별하도록 학습된 것을 지칭한다. 도 3은 본 발명의 일 실시예에 의한 시계열적인 DNN으로 구성된 로컬영역 분류기를 보여준다. 로컬영역 분류기 는 CNN 백본 네트워크를 포함하며, LSTM, FC 계층(layer)를 포함한다. 계열적으로 하나의 로컬영역분류기 가 시계열적으로 동작하는 것을 나타내기 위해 t 시점의 로컬영역분류기는 256a로 지시한다. t+1 시점의 로컬영역분류기는 256b로 지시한다. 마찬가지로 t+n 시점의 로컬영역분류기는 256n으로 지시한다. 로컬영역 분류기의 세부 구성을 살펴본다. 로컬영역 분류기를 구성하는 CNN(Convolution Neural Network) 백본 네트워크는 VGG, LeNet, ResNet, YoLo 등을 일 실시예로 한다. 여기서 CNN은 인공신경망의 일종으로 이미지 인 식에 주로 사용된다. CNN은 다중 계층(Layer)으로 구성될 수 있다. CNN은 입력된 이미지를 일부로 분할하여 일정한 크기의 컨벌루션 (convolution)을 학습하는 모델이다. CNN은 컨벌루션 계층과 풀링(pooling) 계층을 포함할 수 있다. 그리고, 로봇 또는 클라우드 서버에 포함되는 로컬영역 분류기는, 분류의 정확도를 위해 CNN 백본 네트워크와 LSTM(Long-short term memory) 계층을 결합하고 시계열 데이터를 사용할 수 있다. 여기서 LSTM 계층은 시계열 데이터를 이용하여 이전 단계의 정보를 일정 범위 내에서 제거하거 새로운 입력값을 반영하여 시간에 따라 달라 지는 정보들을 반영할 수 있다. 그리고 LSTM에서 산출된 결과는 FC(fully connected) 계층으로 입력되고, FC 계층이 출력한 결과는 Softmax에 입력된다. 그 결과 Softmax는 입려된 이미지가 어떠한 로컬영역에 해당하는지를 예측한 결과(predicted local area)를 출력한다. 즉, 로컬영역 분류기(Local Area Classifier)가 구현하는 프로세스는 Convolution Neural Network(CNN)과 LSTM 계층 구조를 사용하므로 시계열 데이터를 사용할 수 있으며 분류의 정확도를 높일 수 있다. 그리고 FC 계층의 최종 출력의 개수는 분류하고자 하는 로컬영역(Local Area)의 총 개수 C와 동일하다. 또한, 가장 마지막 계층에 서는 Softmax를 이용해 분류하고자 하는 로컬영역들의 점수(Score)를 입력으로 받아 산정하여 가장 높은 확률 값을 가진 로컬영역을 선택한다. 도 3에서 매시간 입력되는 이미지들은 시계열 데이터로 적용되므로, 로봇이 위치한 장소에서 N 개의 연속 프레 임을 받아서 마지막에 출력되는 값이 현재 로봇이 있는 것으로 추정되는 최종 로컬영역을 지시하게 된다. 최종 로컬영역에 대한 지시 정보를 이용하여 도 2에 도시된 바와 같이 로봇 또는 클라우드 서버는 11 내지 16 중 특정한 딥러닝 네트워크인 위치 추정기에 이미지를 입력한다. 그 결과 현재 로봇의 위치를 최종적으로 산출 할 수 있다. 정리하면, 로컬영역 분류기는 제1시점에서 입력된 이미지에 기반하여 제1예측 결과를 산출하며, 상기 로컬 영역 분류기는 제2시점에서 입력된 이미지에 기반하여 제2예측 결과를 산출한다. 여기서 제1시점 이후 제2시점이 후 행한다. 그리고 로컬영역 분류기가 식별한 로컬 영역과 상기 제1예측 결과 사이의 유사도는 상기 식별한 로컬 영역과 상 기 제2예측 결과 사이의 유사도보다 낮거나 동일할 수 있다. 이는 로컬영역 분류기가 시간이 누적될수록 더 정 확도가 높은 분류결과를 출력함을 의미한다. 전술한 실시예들을 적용할 경우, 위치 추정 과정에서 하나의 알고리즘으로 전체 면적을 커버하지 않으므로 오히 려 위치 추정의 정확도 및 추정 속도를 높일 수 있다. 즉, 로봇 또는 클라우드 서버는 로컬영역별로 위치추정의정확도가 높은 각각의 위치인식 딥러닝 알고리즘을 선별적으로 사용할 수 있다. 그 결과 위치 추정의 정확도를 높이고 실행 속도를 높여 실시간성을 확보할 수 있다. 또한, 로컬영역에서 위치 추론 오류가 발생하더라도 로컬영역 내에서의 오차범위에서 발생하므로 갑자기 전혀 다른 위치의 큰 오류좌표가 발생하는 현상을 방지 할 수 있다. 예를 들어, 로컬영역 내에서의 오차는 최대로 확장될 경우라 하여도 해당 로컬영역 이내이므로, 로봇의 현재 위 치로부터 크게 벗어난 위치로 추정하는 오류가 발생하지 않는다. 따라서, 본 발명의 실시예들을 적용할 경우, 대면적의 공간, 특히 GPS 신호 수신에 불리한 실내 공간에서 로봇 의 위치추정의 정확도 및 신속도를 높일 수 있다. 도 4는 본 발명의 일 실시예에 의한 로봇의 구성을 보여준다. 도 4의 로봇의 구성에서 제어부의 기능은 클 라우드 서버 역시 제공할 수 있다. 여기서 로봇은 대면적의 실내 공간을 이동하며 소정의 기능(배송, 카트, 수납, 안내, 보안 등)의 수행할 수 있 다. 특히 대면적의 실내 공간에서 로봇은 배송, 보안, 안내, 수납 등의 기능을 제공하기 위해 정확한 위치 정보 의 획득을 필요로 한다. 도 4의 로봇은 대면적의 실내 공간을 이동하며 로봇의 현재 위치에 따라 1차적으로 로컬영역에 대한 정보를 취 득하고 로컬영역 정보에 대응하는 위치정보를 획득 딥러닝 네트워크를 선택하여 로컬영역 내에서의 정밀한 로봇 의 위치 정보를 획득한다. 또한, 로봇은 로컬영역 내에서의 로봇의 위치 정보를 이용하여 글로벌 좌표에 기반한 로봇의 위치를 산출할 수 있다. 로봇은 수납부를 선택적으로 포함한다. 수납부는 사용자에 의해 사물이 수납되거나 적재되는 공 간이다. 또한, 로봇은 수납부를 가지고 사용자를 추종하여 이동할 수도 있다. 또는 수납부는 배송 기 능을 제공하는 로봇에게 필수적으로 탑재되어 물건을 적재할 수 있다. 특히, 배송 로봇은 적재된 물 건을 지정된 위치에 하차시키거나, 혹은 외부에 배치된 물건을 수납부로 적재할 수 있다. 수납부는 자동으로 물품을 적재하거나 적재된 물품을 하차시키거나, 또는 적재된 물품의 적재 순서를 변경할 수 있다. 기능부는 로봇에게 부여된 소정의 기능을 수행하는 구성요소이다. 청소 기능의 로봇에 있어서 기능부(12 0)는 청소를 위한 물걸레, 흡입기 등을 포함한다. 배송을 위한 로봇에 있어서 기능부는 수납공간, 수납된 짐을 이동시키는 운반부 등을 포함한다. 보안을 위한 로봇에 있어서 기능부는 안전에 필요한 검사기(공기 질 검사, 폭발물 검사 등)를 포함한다. 배터리는 로봇이 동작하는데 필요한 전기적 에너지를 제공한다. 이동부는 로봇의 이동 기능을 제공한다. 로봇은 액츄에이터 또는 모터를 포함하는 이동부를 구비하여 로봇 관절을 움직이는 등의 다양한 물리적 동 작을 수행할 수 있다. 또한, 이동 가능한 로봇은 이동부에 휠, 브레이크, 프로펠러 등이 포함되어, 이동부를 통 해 지상에서 주행하거나 공중에서 비행할 수 있다. 또한 로봇은 자율주행을 수행할 수 있다. 자율 주행은 스스로 주행하는 기술을 의미하며, 자율 주행 로봇은 사 용자의 조작 없이 또는 사용자의 최소한의 조작으로 주행한다. 예컨대, 자율 주행에는 주행 중인 공간의 다른 장애물과 거리를 유지하는 기술, 어댑티브 크루즈 컨트롤과 같이 속도를 자동으로 조절하는 기술, 정해진 경로를 따라 자동으로 주행하는 기술, 목적지가 설정되면 자동으로 경 로를 설정하여 주행하는 기술 등이 모두 포함될 수 있다. 로봇은 자율 주행을 위해 내연 기관 또는 전기 모터를 구비할 수 있으며 이들은 이동부의 하위 구성요소가 된다. 또한, 로봇은 전술한 구성요소들을 제어하는 제어부를 포함한다. 제어부는 인공지능부를 더 포함할 수 있다. 또한, 제어부는 로봇의 구성요소인 저장부, 라이다 센서, 카메라 센서, 통신부를 제 어한다. 저장부는 로봇이 이동하는 대면적의 실내 공간을 구분하는 둘 이상의 로컬영역 정보 및 로컬영역 별 위치 정보를 획득하는 딥러닝 네트워크 메커니즘을 저장한다. 저장부는 제어부의 하위 구성요소가 될 수있다. 예를 들어, 제어부는 저장부를 하나의 메모리로 포함하는 하나의 구성요소가 될 수 있다. 저장부는 전술한 로컬영역 식별기와 위치 추정기를 저장할 수 있다. 또한, 저장부는 로봇이 이동하는 공간에 대한 전체 맵을 저장하거나, 전체 맵을 로컬영역으로 분류한 정보를 저장할 수 있다. 그리고 제어부는 이동 과정에서 저장부에 저장된 로컬영역 식별기와 위치 추정기를 로딩하여 위치 추 정 프로세스를 수행할 수 있다. 라이다 센서(LiDAR Sensor)는 2차원 또는 3차원으로 주변의 사물들을 센싱할 수 있다. 2차원 라이다 센서 의 경우 로봇을 중심으로 360도 범위 또는 그보다 작은 범위 내의 사물의 위치를 센싱할 수 있다. 특정 위치에 서 센싱한 라이다 정보는 센서 데이터의 일 실시예가 된다. 또는 라이다 센서로 확보한 센서 데이터를 라이다 프레임이라고 지칭할 수 있다. 즉, 라이다 센서는 로봇의 외부에 배치된 사물과 로봇 사이의 거리를 센싱하여 라이다 프레임을 생성한다. 카메라 센서는 일반 카메라를 일 실시예로 한다. 시야각의 제약을 해결하기 위해 둘 이상의 카메라 센서 를 사용할 수 있다. 특정 위치에서 촬영한 영상은 이미지 정보를 구성한다. 즉, 카메라 센서는 로봇 의 외부에 배치된 사물을 촬영하여 생성한 이미지 정보는 센서 데이터의 일 실시예가 된다. 또는, 카메라 센서로 확보한 센서 데이터를 비주얼 프레임을 프레임이라고 지칭할 수 있다. 즉, 카메라 센 서는 로봇의 외부를 촬영하여 비주얼 프레임을 생성한다. 이하 본 발명을 적용하는 로봇은 라이다 센서 또는 카메라 센서 중 어느 하나 또는 둘을 이용하 여 SLAM(simultaneous localization and mapping)을 수행한다. SLAM 과정에서 로봇은 라이다 프레임과 비주얼 프레임을 각각 이용하거나 또는 이들을 결합하여 맵을 생성 하거나 위치를 추정할 수 있다. 인터페이스부는 사용자로부터 정보를 입력받는다. 터치 입력, 음성 입력 등 다양한 정보를 사용자로부터 입력받고, 이에 대한 결과를 출력한다. 인터페이스부는 또한 이동 과정에서 사람들에게 로봇이 접근 함을 알리는 소리를 출력할 수 있다. 즉, 인터페이스부는 외부의 사람들에게 소정의 시각적 또는 청각적 정보를 제공할 수 있다. 그리고 외부의 사람들로부터 음성 명령을 입력받거나 터치 스크린을 통한 입력을 받을 수 있다. 제어부는 실내 공간 중에 로봇이 위치하는 로컬영역에 대한 정보를 획득하고 획득한 로컬영역 정보에 대응 하도록 설정된 위치 추정기를 로딩하여 위치 추정기에 센서(라이다 센서 및/또는 카메라 센서)가 획득한 정보를 입력한다. 그 결과 로봇의 정확한 위치 정보가 산출된다. 그리고 이에 기반하여 제어부는 이동부를 제어하고, 이동부는 제어에 따라 로봇을 이동시킨다. 또한, 제어부는 휠 인코더(Wheel Encoder)를 포함할 수 있다. 휠 인코더는 로봇의 이동부를 구성하는 바퀴 의 회전이나 방향 등의 정보를 취합하여 휠 오도메트리 정보를 생성한다. 제어부 휠 인코더가 생성한 정보 에 기반하여 이동 거리나 이동 방향 등을 산출할 수 있다. 통신부는 로봇이 다른 로봇 또는 외부의 클라우드 서버와 통신하여 정보를 송수신할 수 있도록 한다. 또한, 통신부는 로봇이 로컬영역을 식별하데 필요한 외부의 통신 환경에 대한 정보를 획득할 수 있다. 예를 들어, 통신부는 실내 공간에 배치되어 신호를 출력하는 송신기로부터 신호를 수신할 수 있다. 예를 들어 실내 공간에는 다수의 매장들이나 사무실 등이 위치할 수 있으며, 이들 매장이나 사무실이 WiFi를 사용할 수 있다. 따라서, 통신부는 WiFi 식별 정보를 획득하고, 제어부는 획득한 정보를 이용하여 현재 로봇 의 로컬영역 또는 위치를 추정함에 있어 정확도를 높일 수 있다. 제어부에 포함된 인공지능부는 딥러 닝에 기반하여 동작하는 로컬영역 분류기 및 위치추정기를 포함한다. 도 4의 구성은 다음과 같다. 카메라 센서가 로봇이 이동하는 공간을 촬영한다. 제어부는 소프트 웨어적 구성요소 또는 하드웨어적 요소로 딥러닝에 기반하여 동작하는 로컬 영역 분류기와 위치추정기 를 포함한다. 로컬 영역 분류기는 로봇이 이동하는 공간을 다수의 로컬영역 중 어느 하나로 식별한다. 제어부는 로 컬영역 분류기에 카메라 센서가 촬영한 이미지를 입력하고 로컬영역 분류기는 입력된 이미지를이용하여 로봇이 위치한 로컬영역을 식별한다. 위치추정기는 로컬영역 각각에 대응하여 로봇의 위치를 추정하며, 제어부는 다수의 위치추정기 를 포함할 수 있다. 제어부는 식별된 로컬영역에 대응하는 위치 추정기를 선택하여 선택한 위치추정 기에 카메라 센서가 촬영한 이미지를 입력하여 로컬영역 내의 위치 정보를 산출한다. 로컬영역 내의 위치 정보 산출이 완료되면 제어부는 로컬영역을 기준으로 글로벌 좌표를 산출할 수 있다. 도 5는 본 발명의 일 실시예에 의한 실내 공간을 로컬영역으로 구분하는 과정을 보여준다. 매장이나 사무실 등이 다수 배치된 실내 공간을 로컬영역으로 구분하기 위한 일 실시예로 로봇 또는 클라우드 서버는 교차로를 판단할 수 있다. 즉, 로봇 또는 클라우드 서버는 교차로를 중심으로 로컬영역을 판단할 수 있 다. 이하, 로봇을 중심으로 설명하지만, 실시예들은 클라우드 서버에도 적용될 수 있다. 이 경우, 클라우드 서버는 로봇이 센싱한 센서 데이터를 이용한다. 로봇의 제어부는 실내 공간 내의 주요 거리들이 교차하는 교차로를 식별한다(S21). 로봇의 제어 부는 실내 공간에 대해 이미 작성된 맵을 이용하거나, 혹은 실내 공간에 대한 안내도를 포함하는 이미지를 이용하여 교차로를 식별할 수 있다. 또는 로봇이 이동하는 과정에서 교차로를 식별할 수도 있다. 일 실시예에 의하면 로봇의 카메라센서는 로봇이 이동하는 거리에서 양쪽의 매장과 직진 방향의 거리 를 촬영하고, 거리 끝단에서 분기되는 교차로를 촬영할 수 있다. 로봇의 제어부는 이미지 프로세싱 및 이미지 내의 객체 분석을 통해 교차로를 식별할 수 있다. 그리고 로봇의 제어부는 주행과정 또는 맵이나 이미지를 이용하여 식별된 다수의 교차로들 사이의 거 리를 계산한다(S22). 계산 후, 제어부는 교차로 사이의 거리와 로컬영역의 최대 허용 반경을 기준으로 로컬영역을 구분한다 (S23). 여기서 로컬영역의 최대 허용 반경이란, 위치 정보를 획득함에 있어서 딥러닝 네트워크가 효율적으로 처 리할 수 있는 로컬영역의 최대 크기를 의미한다. 최대 허용 반경을 넘어서 로컬영역을 정할 경우 로컬영역 내에서 로봇이 위치를 추정함에 있어서 오류가 발생하 거나, 시간이 증가할 수 있다. 최대 허용 반경의 크기는 로봇이나 클라우드 서버가 처리할 수 있는 컴퓨팅 자원 이나, 공간의 특징, 로봇의 수, 또는 매장들의 밀집도 등에 따라 다양하게 설정될 수 있다. 그리고 로봇의 제어부는 설정된 로컬영역들 사이에서 중첩 영역을 설정한다. 이는 로컬영역의 경계에 가까 울 겨우 로컬영역 분류기가 산출한 로컬영역 정보가 실제 로봇이 위치하는 로컬영역과 일치하지 않을 수 있기 때문에, 경계에 대해서는 둘 이상의 로컬영역들이 각각 독자적인 딥러닝 네트워크로 커버할 수 있도록 중 첩 영역을 설정한다(S24). 그리고 로봇 또는 클라우드 서버는 설정된 로컬영역 별로 취득한 이미지 정보를 이용하여 로컬영역 분류기와 로 컬영역별 위치추정기를 학습시킨다(S25) 여기에서 정확도를 높이기 위해 카메라 센서 외의 다른 센서들이 취득한 센서 정보 역시 학습 과정에 입력될 수 있다. 도 6은 본 발명의 일 실시예에 의한 대규모 공간을 로컬영역으로 구분한 것을 보여준다. 로봇은 대면적의 영역 을 직접 주행하여 교차로 및 주변 영역에 대한 위치 정보들을 확보할 수 있다. 또는 로봇은 대면적의 영역에 대 응하도록 미리 제공된 맵에 기반하여 교차로 및 주변 영역에 대한 위치 정보를 확보할 수 있다. 대면적의 영역에서 로봇은 굵은 선으로 표시된 바와 같이 교차로를 식별한다. 도 6에서 3개의 교차로가 식별 되었다. 각 교차로의 중심점은 원으로 표시되었다. 중심점 사이의 거리, 교차로에서 교차하는 경로들의 수 등에 기반하여 로봇의 제어부는 도 6의 31, 32, 33과 같은 로컬영역들을 설정한다. 여기서 제1로컬영역(Area01, 31)과 제2로컬영역(Area02, 32)의 경계가 중첩된다. 그리고 제1로컬영역(Area01, 31)과 제3로컬영역(Area03, 33)의 경계가 중첩된다. 마찬가지로, 제2로컬영역(Area02, 32)과 제3로컬영역 (Area03, 33)의 경계가 중첩된다. 그리고 각 로컬영역에 대응하여 로봇 또는 클라우드 서버는 로컬영역 분류기와 로컬영역별 위치추정기를 학습시킨다. 도 5 및 도 6에서 살펴본 바와 같이, 로봇 또는 클라우드 서버는 실내에서 교차로가 있는 곳을 중심으로 로컬 영역을 설정할 수 있다. 그리고 로컬 영역의 크기는 공간 내에 매장이나 유동인구의 크기 등의 환경 요소와 로 봇의 계산능력 등에 따라 변동될 수 있다. 또한 매장들의 크기가 크거나, 매장들이 쉽게 식별되는 공간인 경우에는 로컬 영역의 크기를 크게 하여 하나의 로컬영역별 위치추정기가 넓은 범위를 커버하여 위치 추정을 수행할 수 있도록 한다. 반대로, 매장들의 크기가 작거나, 매장들이 쉽게 식별되지 않는 공간인 경우에는 로컬 영역의 크기를 작게 하여 하나의 로컬영역별 위치추정기가 좁은 범위를 커버하여 위치 추정을 수행할 수 있도록 한다. 하나의 대면적의 공간 내에서도 로컬 영역의 크기는 다양하게 설정될 수 있다. 또한, 로봇 또는 클라우드 서버는 인접한 로컬영역들은 서로 일정 영역이 겹치도록 하여 로컬영역별 위치추정기 또는 로컬영역 분류기를 구성하는 딥러닝 네트워크를 학습시킨다. 로컬 영역들이 중첩될 경우, 로봇이 어느 하나의 로컬 영역에서 다른 로컬 영역으로 이동하는 과정에서도 정확 하게 로컬 영역과 위치를 추정할 수 있다. 도 5 또는 도 6의 과정을 정리하면 다음과 같다. 제어부는 로봇이 이동한 과정에서 촬영한 이미지 또는 미 리 저장된 맵을 이용하여 교차로를 식별한다(S21). 그리고 제어부는 식별된 교차로 사이의 거리를 계산하 여 교차로를 중심으로 하는 로컬영역 또는 교차로 사이의 영역을 포함하는 로컬영역들로 상기 로봇이 이동한 공 간을 구분한다(S22~S24). 이때, 로컬영역은 인접한 로컬영역과 중첩하는 중첩 영역을 포함할 수 있다. 이후 로컬영역 분류기는 로컬영역별로 수집된 이미지를 입력받아 로컬영역에 대한 학습을 수행한다(S25). 또한, 로컬 영역별로 위치추정기들 역시 학습을 수행한다(S25). 도 5 또는 도 6의 과정은 매퍼(mapper) 로봇 또 는 클라우드 서버가 수행할 수 있다. 도 7은 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 식별하는 과정을 보여준다. 로봇은 현재 로컬 영역에 대한 정보를 획득하기 위해 일정한 범위 내에서 이동하며 이미지 프레임을 획득한다. 예를 들어, 로봇은 클로즈드 루프(closed loop) 경로를 이동하고 카메라 센서가 이미지 프레임을 생성한다. 이때 클로즈드 루프는 일정한 크기의 폐곡선 형태가 될 수 있다. 또한, 교차로 중심일 경우 로봇은 교차로 영역을 포함하는 클로즈드 루프 형태로 경로를 생성하여 이동할 수 있다. 또는, 로봇은 교차로가 아닌 곳에서는 길을 따라 전후 또는 좌우로 이동하고 이미지 프레임을 획득할 수 있다. 이 경우, 로봇은 카메라 센서를 360도 방향으로 회전시키거나 혹은 로봇이 직접 360도로 회전하여 주 변의 모든 방향의 이미지를 촬영할 수 있다. 그리고 로봇의 제어부는 획득한 이미지 프레임을 로컬 영역 분류기에 입력한다(S35). 도 3에서 살펴 본 바와 같이 이미지는 로봇의 이동에 따라 시간차를 두고 획득된다. 따라서 도 3에 도시된 바와 같이 t 시점의 이미지가 입력되어 1차로 로컬 영역에 대한 예측 결과가 산출된다. 이후 다시 t+1 시점에서 이미지가 입력되어 2차로 로컬 영역에 대한 예측 결과가 산출된다. 이는 계속 반복되어 로봇이 경로 이동을 완료할 때까지 반복된다. 즉, 로봇은 반복하여 이동하는 과정에서 누적하여 획득한 이미지 를 로컬영역 분류기에 반복하여 입력한다. 그리고 로컬영역 분류기는 시간 순으로 이미지를 누적하며 로컬 영역에 대한 예측을 수행한다(S36). 경로에 대한 이동이 완료되면, 로컬영역 분류기는 마지막 이미지의 예측 결과에 기반하여 로컬 영역 정보 를 출력한다(S37). 이때, 로컬 영역 정보는 하나의 로컬 영역을 지시할 수 있다. 또는 로컬 영역 정보는 다수의 로컬 영역을 지시할 수 있다. 로컬영역 분류기는 다수의 로컬 영역을 출력할 경우, 각각의 로컬 영역에 대한 확률 역시 출력할 수 있다. 예를 들어, 로컬영역 분류기는 총 6개의 로컬영역에 대해 다음 표2와 같이 확률을 출력할 수 있다. 표 2 예측된 로컬 영역 확률 Area01 20% Area02 75%Area03 5% Area04 0% Area05 0% Area06 0% 표 2에 도시된 바와 같이 로컬영역분류기가 각 로컬 영역 별로 확률을 출력하면, 제어부는 이들 중에 서 확률이 가장 높은 \"Area02\"에 로봇이 위치한 것으로 판단할 수 있다. 또는 로봇의 이전 위치 정보를 이용하여 표 2와 같이 산출된 결과 중에서 제어부가 특정 로컬 영역을 선택 할 수 있다. 도 1을 참조하여, 로봇이 이전에 Area04의 위치에 있었다고 가정한다. 그렇다면 Area 04에서 로봇 이 이동할 수 있는 로컬 영역은 Area02 및 Area06이다. 그런데 표 2와 같이 Area06은 0%이며 Area02는 75%이므로 현재 로봇의 위치를 Area02로 판단할 수 있다. 그리고 로봇의 제어부는 \"DNN_Pose_Net02에 이미지를 입력하여 Area02에서의 로봇의 로컬 좌표를 산출한다. 그리 고 로봇의 제어부는 로컬 좌표와 Area02에 대한 정보를 이용하여 글로벌 좌표를 생성한다. 즉, 로컬영역 분류기가 표 2와 같이 둘 이상의 로컬영역을 출력할 경우, 제어부는 로봇이 이전 에 위치했던 로컬영역에 대한 정보 또는 출력된 둘 이상의 로컬영역의 각각의 확률 중 어느 하나 이상을 이용하 여 출력된 둘 이상의 로컬영역에서 하나의 로컬 영역을 선택할 수 있다. 만약, 로컬 영역을 선택한 후, 해당 로컬영역의 위치추정기를 이용하여 위치 정보를 산출하는 과정에서 위치추 정기가 위치 정보를 출력하지 못할 경우, 제어부는 앞서 선택 과정에서 선택되지 않은 로컬영역들 중에서 하나를 선택할 수 있다. 도 8은 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 확인하기 위해 이동하는 과정을 보여준다. 도 8에서 로 봇(100, R)은 화살표 경로로 이동한다. 그리고 로봇의 카메라 센서는 t시점, t+1시점, ..., t+n 시점 에서 이미지를 촬영한다. 촬영된 이미지는 로컬영역 분류기로 입력된다. 로컬영역 분류기는 t시점, t+1시점, t+n시점의 이미지 들을 학습하여 최종적인 t+n에서 예측된 로컬 영역에 대한 정보를 출력한다. 로컬영역 분류기의 출력 결과 는 어느 하나의 로컬 영역에 대한 정보가 될 수 있다. 또는 로컬영역 분류기의 출력결과는 어느 둘 이상의 로컬 영역에 대한 정보가 될 수 있다. 그리고 제어부 는 출력된 정보를 이용하여 현재 로봇이 위치한 곳을 확인하고 그에 해당하는 로컬영역별 위치추정기를 실 행한다. 로봇은 반드시 클로즈드 루프를 이동하지 않을 수도 있다. 즉, 주변을 360도로 촬영하기 위해 전후 또는 좌우 이동할 수 있으며 이 과정에서 촬영된 이미지를 이용하여 로컬 영역을 예측할 수 있다. 특히 로컬영역 분류기 내의 LSTM은 연속된 이미지의 특징을 누적하여 추론 결과를 출력하므로, 로컬영역을 식별하는데 있어 정확도를 높일 수 있다. 도 8을 정리하면 다음과 같다. 제어부는 로봇의 현재 위치가 교차로인지 판단한 후, 교차로인 경우, 해당 교차로를 중심으로 클로즈 루프를 형성하며 이동한다. 이 과정에서 카메라 센서가 주변을 촬영한다. 그리고 로컬영역 분류기는 촬영된 이미지들을 이용하여 현재 로봇이 위치하는 로컬 영역을 식별한다. 도 9는 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 확인하기 위해 이동하는 과정을 보여준다. 도 9에서 로 봇(100, R)은 화살표 경로로 이동한다. 그리고 로봇의 카메라 센서는 t시점, t+1시점, ..., t+n 시점 에서 이미지를 촬영한다. 촬영된 이미지는 로컬영역 분류기로 입력된다. 로컬영역 분류기는 t시점, t+1시점, t+n시점의 이미지 들을 학습하여 최종적인 t+n에서 예측된 로컬 영역에 대한 정보를 출력한다. 로컬영역 분류기의 출력 결과 는 어느 하나의 로컬 영역에 대한 정보가 될 수 있다. 또는 로컬영역 분류기의 출력결과는 어느 둘 이상의 로컬 영역에 대한 정보가 될 수 있다. 도 9는 도 8과 달리, 로봇이 교차로에 위치하지 않는 경우의 실시예이다. 즉, 제어부가 로봇의 현재 위치가 교차로인지 판단한 후, 교차로가 아닌 경우, 로봇의 현재 위치인 제1위치(p1)에서 제2위치(p2)로 이동후 다시 제1위치(p1)로 복귀한다(화살표). 이 과정에서 카메라 센서가 주변을 촬영할 수 있다. 또는 로봇이 제1위치(p1)에 계속 머무른 상태에 서 카메라 센서가 회전하며 주변을 촬영할 수 있다. 그리고 로컬영역 분류기는 상기 촬영된 이미지들을 이 용하여 현재 로봇이 위치하는 로컬 영역을 식별한다. 도 10은 본 발명의 일 실시예에 의한 클라우드 서버의 구성을 보여준다. 클라우드 서버는 로봇과 통신하는 통신부와 저장부, 그리고 서버 제어부로 구성된다. 저장부는 구현 과정에서 서버제어부에 포함될 수 있다. 서버 제어부의 구성은 도 4의 로봇의 제어부의 구성과 유사하다. 클라우드 서버가 인공지 능부의 기능을 제공할 때, 클라우드 서버는 인공지능 서버로 지칭될 수 있다. 클라우드 서버의 인공 지능부 또는 로봇의 인공지능부는 로컬영역 분류기 및 로컬영역별 위치추정기(257, 257a~257n)의 딥러닝 네트워크를 제공한다. 클라우드 서버의 통신부는 로봇이 이동하는 공간을 촬영한 이미지를 로봇으로부터 수신한다. 클 라우드 서버의 서버제어부는 공간을 다수의 로컬영역 중 어느 하나로 식별하는 로컬영역 분류기 또는 로컬영역 각각에 대응하여 로봇의 위치를 추정하는 다수의 위치 추정기(257a~257n) 중 어느 하나 이상을 포함한다. 로봇의 계산 성능이 느릴 경우에는 클라우드 서버의 로컬영역 분류기 및 위치 추정기 (257a~257n)가 로봇의 로컬영역 식별 및 글로벌 좌표 생성을 수행한다. 만약 로봇의 계산 성능이 인공지능을 수행하기에 충분히 확보된 경우에는 로봇은 클라우드 서버와 통 신 없이 로봇의 로컬영역 식별 및 글로벌 좌표 생성을 수행할 수 있다. 그리고 로봇과 클라우드 서버 사이의 기능 분배는 로봇과 클라우드 서버 사이의 통신 속도, 로봇의 처리 성능 등에 따라 달라질 수 있다. 서버 제어부는 통신부가 수신한 이미지에 대응하는 로컬영역에 대한 식별 정보 또는 로컬영역 내의 위치 정보를 산출한다. 그리고 통신부는 로컬영역에 대한 식별정보 또는 로컬영역 내의 위치정보를 로봇에 게 전송한다. 클라우드 서버의 로컬영역 분류기 및 위치 추정기(257a~257n)는 앞서 로봇과 동일하므로 이전의 설명을 참조한다. 로봇 또는 클라우드 서버에 포함되는 인공지능부(255, 355)에 대해 간략히 살펴본다. 인공 지능은 인공적인 지능 또는 이를 만들 수 있는 방법론을 연구하는 분야를 의미하며, 머신 러닝(기계 학습, Machine Learning)은 인공 지능 분야에서 다루는 다양한 문제를 정의하고 그것을 해결하는 방법론을 연구하는 분야를 의미한다. 머신 러닝은 어떠한 작업에 대하여 꾸준한 경험을 통해 그 작업에 대한 성능을 높이는 알고리 즘으로 정의하기도 한다. 인공 신경망(ANN: Artificial Neural Network)은 머신 러닝에서 사용되는 모델로써, 시냅스의 결합으로 네트워 크를 형성한 인공 뉴런(노드)들로 구성되는, 문제 해결 능력을 가지는 모델 전반을 의미할 수 있다. 인공 신경 망은 다른 레이어의 뉴런들 사이의 연결 패턴, 모델 파라미터를 갱신하는 학습 과정, 출력값을 생성하는 활성화 함수(Activation Function)에 의해 정의될 수 있다. 인공 신경망은 입력층(Input Layer), 출력층(Output Layer), 그리고 선택적으로 하나 이상의 은닉층(Hidden Layer)를 포함할 수 있다. 각 층은 하나 이상의 뉴런을 포함하고, 인공 신경망은 뉴런과 뉴런을 연결하는 시냅 스를 포함할 수 있다. 인공 신경망에서 각 뉴런은 시냅스를 통해 입력되는 입력 신호들, 가중치, 편향에 대한 활성 함수의 함숫값을 출력할 수 있다. 모델 파라미터는 학습을 통해 결정되는 파라미터를 의미하며, 시냅스 연결의 가중치와 뉴런의 편향 등이 포함된 다. 그리고, 하이퍼파라미터는 머신 러닝 알고리즘에서 학습 전에 설정되어야 하는 파라미터를 의미하며, 학습 률(Learning Rate), 반복 횟수, 미니 배치 크기, 초기화 함수 등이 포함된다. 인공 신경망의 학습의 목적은 손실 함수를 최소화하는 모델 파라미터를 결정하는 것으로 볼 수 있다. 손실 함수 는 인공 신경망의 학습 과정에서 최적의 모델 파라미터를 결정하기 위한 지표로 이용될 수 있다.머신 러닝은 학습 방식에 따라 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 분류할 수 있다. 지도 학습은 학습 데이터에 대한 레이블(label)이 주어진 상태에서 인공 신경망을 학습시키는 방법을 의미하며, 레이블이란 학습 데이터가 인공 신경망에 입력되는 경우 인공 신경망이 추론해 내야 하는 정답(또는 결과 값)을 의미할 수 있다. 비지도 학습은 학습 데이터에 대한 레이블이 주어지지 않는 상태에서 인공 신경망을 학습시키 는 방법을 의미할 수 있다. 강화 학습은 어떤 환경 안에서 정의된 에이전트가 각 상태에서 누적 보상을 최대화 하는 행동 혹은 행동 순서를 선택하도록 학습시키는 학습 방법을 의미할 수 있다. 인공 신경망 중에서 복수의 은닉층을 포함하는 심층 신경망(DNN: Deep Neural Network)으로 구현되는 머신 러닝 을 딥 러닝(심층 학습, Deep Learning)이라 부르기도 하며, 딥 러닝은 머신 러닝의 일부이다. 이하에서, 머신 러닝은 딥 러닝을 포함하는 의미로 사용된다. 로봇은 앞서 살펴보았던 제어부를 구성하는 하위 구성요소인 인공지능부가 인공지능 기능을 수 행할 수 있다. 제어부 내에 인공지능부가 소프트웨어 또는 하드웨어로 구성될 수 있다. 또는 클라우 드 서버의 서버 제어부를 구성하는 하위 구성요소인 인공지능부가 인공지능 기능을 수행할 수 있다. 이 경우, 로봇의 통신부는 유무선 통신 기술을 이용하여 다른 AI 기능을 제공하는 로봇이나 또는 도 13에서 살펴본 클라우드 서버 등의 외부 장치들과 데이터를 송수신할 수 있다. 예컨대, 통신부는 외 부 장치들과 센서 정보, 사용자 입력, 학습 모델, 제어 신호 등을 송수신할 수 있다. 이때, 통신부가 이용하는 통신 기술에는 GSM(Global System for Mobile communication), CDMA(Code Division Multi Access), LTE(Long Term Evolution), 5G, WLAN(Wireless LAN), Wi-Fi(Wireless-Fidelity), 블 루투스(Bluetooth), RFID(Radio Frequency Identification), 적외선 통신(Infrared Data Association; IrDA), ZigBee, NFC(Near Field Communication) 등이 있다. 인터페이스부는 다양한 종류의 데이터를 획득할 수 있다. 이때, 인터페이스부는 영상 신호 입력을 위한 카메라, 오디오 신호를 수신하기 위한 마이크로폰, 사용자로 부터 정보를 입력 받기 위한 사용자 입력부 등을 포함할 수 있다. 여기서, 라이다 센서, 카메라 센서 또는 마이크로폰이 획득한 정보들은 센싱 데이터 또는 센서 정보 등을 지칭한다. 인터페이스부 및 각종 센서들과 이동부의 휠 인코더 등은 모델 학습을 위한 학습 데이터 및 학습 모델을 이용하여 출력을 획득할 때 사용될 입력 데이터 등을 획득할 수 있다. 전술한 구성요소들은 가공되지 않은 입력 데이터를 획득할 수도 있으며, 이 경우 제어부 또는 인공지능부는 입력 데이터에 대하여 전처리로써 입력 특징점(input feature)을 추출할 수 있다. 인공지능부는 학습 데이터를 이용하여 인공 신경망으로 구성된 모델을 학습시킬 수 있다. 여기서, 학습된 인공 신경망을 학습 모델이라 칭할 수 있다. 학습 모델은 학습 데이터가 아닌 새로운 입력 데이터에 대하여 결 과 값을 추론해 내는데 사용될 수 있고, 추론된 값은 로봇이 어떠한 동작을 수행하기 위한 판단의 기초로 이용될 수 있다. 이때, 로봇의 인공지능부는 클라우드 서버의 인공지능부와 함께 AI 프로세싱을 수행할 수 있다. 이때, 로봇의 인공지능부는 로봇에 통합되거나 구현된 메모리를 포함할 수 있다. 또는, 로봇 의 인공지능부는 별도의 메모리 또는 로봇에 결합된 외부 메모리 또는 외부 장치에서 유지되는 메모리를 사용하여 구현될 수도 있다. 로봇은 다양한 센서들을 이용하여 로봇의 내부 정보, 로봇의 주변 환경 정보 및 사용자 정보 중 적어도 하나를 획득할 수 있다. 이때, 로봇에 포함되는 센서에는 근접 센서, 조도 센서, 가속도 센서, 자기 센서, 자이로 센서, 관성 센서, RGB 센서, IR 센서, 지문 인식 센서, 초음파 센서, 광 센서, 마이크로폰, 라이다 센서, 장애물 센서 , 카메라 센서, 레이더 등이 있다. 또한, 앞서 살펴본 인터페이스부는 시각, 청각 또는 촉각 등과 관련된 출력을 발생시킬 수 있다. 이때, 인터페이스부는 시각 정보를 출력하는 디스플레이부, 청각 정보를 출력하는 스피커, 촉각 정보를 출 력하는 햅틱 모듈 등이 포함될 수 있다. 로봇에 내장된 메모리는 로봇의 다양한 기능을 지원하는 데이터를 저장할 수 있다. 예컨대, 로봇 에 내장된 각종 센서들이나 인터페이스부 등이 획득한 입력 데이터, 학습 데이터, 학습 모델, 학습 히스토리 등을 저장할 수 있다. 제어부는 데이터 분석 알고리즘 또는 머신 러닝 알고리즘을 사용하여 결정되거나 생성된 정보에 기초하여, 로봇의 적어도 하나의 실행 가능한 동작을 결정할 수 있다. 그리고, 제어부는 로봇의 구성 요소 들을 제어하여 결정된 동작을 수행할 수 있다. 이를 위해, 제어부는 인공지능부 또는 메모리의 데이터를 요청, 검색, 수신 또는 활용할 수 있고, 적어도 하나의 실행 가능한 동작 중 예측되는 동작이나, 바람직한 것으로 판단되는 동작을 실행하도록 로봇의 구 성 요소들을 제어할 수 있다. 이때, 제어부는 결정된 동작을 수행하기 위하여 외부 장치의 연계가 필요한 경우, 해당 외부 장치를 제어 하기 위한 제어 신호를 생성하고, 생성한 제어 신호를 해당 외부 장치에 전송할 수 있다. 제어부는 사용자 입력에 대하여 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 사용자의 요구 사항 을 결정할 수 있다. 이때, 제어부는 음성 입력을 문자열로 변환하기 위한 STT(Speech To Text) 엔진 또는 자연어의 의도 정보 를 획득하기 위한 자연어 처리(NLP: Natural Language Processing) 엔진 중에서 적어도 하나 이상을 이용하여, 사용자 입력에 상응하는 의도 정보를 획득할 수 있다. 이때, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 적어도 일부가 머신 러닝 알고리즘에 따라 학습된 인 공 신경망으로 구성될 수 있다. 그리고, STT 엔진 또는 NLP 엔진 중에서 적어도 하나 이상은 인공지능부에 의해 학습된 것이나, 인공지능 서버의 기능을 제공하는 클라우드 서버의 인공지능부에 의해 학습된 것이거 나, 또는 이들의 분산 처리에 의해 학습된 것일 수 있다. 한편, 제어부는 이미지 센서 데이터 또는 라이다 센서 데이터와 같이 실시간으로 획득한 센서 데이터에서 특징점을 추출할 수 있다. 이를 위해 인공지능부는 머신 러닝 알고리즘에 따라 학습된 인공 신경망으로 구 성될 수 있다. 그리고, 로봇의 인공지능부는 학습된 것이나, 클라우드 서버의 인공지능부 에 의해 학습된 것이거나, 또는 이들의 분산 처리에 의해 학습된 것일 수 있다. 제어부는 로봇의 동작 내용이나 동작에 대한 사용자의 피드백 등을 포함하는 이력 정보를 수집하여 메모리 또는 인공지능부에 저장하거나, 클라우드 서버 등의 외부 장치에 전송할 수 있다. 수집된 이 력 정보는 학습 모델을 갱신하는데 이용될 수 있다. 도 10의 클라우드 서버는 인공지능 서버로 동작할 수 있다. 인공 지능 서버, 즉 AI 서버의 기능을 제공하는 클라우드 서버는 머신 러닝 알고리즘을 이용하여 인공 신 경망을 학습시키거나 학습된 인공 신경망을 이용하는 장치를 의미할 수 있다. 여기서, 클라우드 서버는 복 수의 서버들로 구성되어 분산 처리를 수행할 수도 있고, 5G 네트워크로 정의될 수 있다. 클라우드 서버는 통신부, 서버제어부, 인공지능부 등을 포함하며 각각에 대해 도 3에서 살 펴본 바 있다. 한편 클라우드 서버는 메모리를 더 포함할 수 있다. 메모리는 인공지능부을 통하여 학습 중인 또는 학습된 모델(또는 인공 신경망)로 구성된 로컬영역 분류기 , 위치추정기(257a~257n)을 저장할 수 있다. 인공지능부는 학습 데이터를 이용하여 인공 신경망(331a)을 학습시킬 수 있다. 학습 모델은 인공 신경망의 클라우드 서버에 탑재된 상태에서 이용되거나, 로봇등의 외부 장치에 탑재되어 이용될 수도 있다. 학습 모델은 하드웨어, 소프트웨어 또는 하드웨어와 소프트웨어의 조합으로 구현될 수 있다. 학습 모델의 일부 또는 전부가 소프트웨어로 구현되는 경우 학습 모델을 구성하는 하나 이상의 명령어(instruction)는 메모리에 저장될 수 있다. 서버제어부는 학습 모델을 이용하여 새로운 입력 데이터에 대하여 결과 값을 추론하고, 추론한 결과 값에 기초한 응답이나 제어 명령을 생성할 수 있다.로봇은 AI 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터테인먼트 로봇, 펫 로 봇, 무인 비행 로봇 등으로 구현될 수 있다. 로봇은 동작을 제어하기 위한 로봇 제어 모듈을 포함할 수 있고, 로봇 제어 모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩을 의미할 수 있다. 로봇은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 로봇(100a)의 상태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계획을 결정하거나, 사용 자 상호작용에 대한 응답을 결정하거나, 동작을 결정할 수 있다. 여기서, 로봇은 이동 경로 및 주행 계획을 결정하기 위하여, 라이다, 레이더, 카메라 중에서 적어도 하나 이상의 센서에서 획득한 센서 정보를 이용할 수 있다. 로봇은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수행할 수 있 다. 예컨대, 로봇은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있고, 인식된 주변 환경 정보 또는 객체 정보를 이용하여 동작을 결정할 수 있다. 여기서, 학습 모델은 로봇에서 직접 학습되거나, 클라 우드 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 로봇은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, 클라우드 서버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수도 있다. 로봇은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적 어도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로와 주행 계 획에 따라 로봇을 주행시킬 수 있다. 맵 데이터에는 로봇이 이동하는 공간에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예컨대, 맵 데이터에는 벽, 문 등의 고정 객체들과 화분, 책상 등의 이동 가능한 객체들에 대한 객체 식별 정보 가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위치 등이 포함될 수 있다. 또한, 로봇은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거나 주행할 수 있다. 이때, 로봇은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정보를 획득하고, 획득한 의도 정보에 기초하여 응답을 결정하여 동작을 수행할 수 있다. 또한, 로봇은 자율 주행을 수행하며, 이 과정에서 AI 기술이 적용되어, 이동형 로봇, 차량, 무인 비행체 등으로 구현될 수 있다. 자율 주행하는 로봇은 자율 주행 기능을 제어하기 위한 자율 주행 제어 모듈을 포함할 수 있고, 자율 주행 제어 모듈은 소프트웨어 모듈 또는 이를 하드웨어로 구현한 칩을 의미할 수 있다. 자율 주행 제어 모듈은 자율 주행하는 로봇의 구성으로써 내부에 포함될 수도 있지만, 자율 주행하는 로봇의 외부에 별도의 하드 웨어로 구성되어 연결될 수도 있다. 자율 주행하는 로봇은 다양한 종류의 센서들로부터 획득한 센서 정보를 이용하여 자율 주행하는 로봇(10 0)의 상태 정보를 획득하거나, 주변 환경 및 객체를 검출(인식)하거나, 맵 데이터를 생성하거나, 이동 경로 및 주행 계획을 결정하거나, 동작을 결정할 수 있다. 여기서, 자율 주행하는 로봇은 이동 경로 및 주행 계획을 결정하기 위하여, 로봇과 마찬가지로, 라이 다, 레이더, 카메라 중에서 적어도 하나 이상의 센서에서 획득한 센서 정보를 이용할 수 있다. 특히, 자율 주행하는 로봇은 시야가 가려지는 영역이나 일정 거리 이상의 영역에 대한 환경이나 객체는 외 부 장치들로부터 센서 정보를 수신하여 인식하거나, 외부 장치들로부터 직접 인식된 정보를 수신할 수 있다. 자율 주행하는 로봇은 적어도 하나 이상의 인공 신경망으로 구성된 학습 모델을 이용하여 상기한 동작들을 수행할 수 있다. 예컨대, 자율 주행하는 로봇은 학습 모델을 이용하여 주변 환경 및 객체를 인식할 수 있 고, 인식된 주변 환경 정보 또는 객체 정보를 이용하여 주행 동선을 결정할 수 있다. 여기서, 학습 모델은 자율 주행 차량(100b)에서 직접 학습되거나, AI 서버 등의 외부 장치에서 학습된 것일 수 있다. 이때, 자율 주행하는 로봇은 직접 학습 모델을 이용하여 결과를 생성하여 동작을 수행할 수도 있지만, 클 라우드 서버 등의 외부 장치에 센서 정보를 전송하고 그에 따라 생성된 결과를 수신하여 동작을 수행할 수 도 있다.자율 주행하는 로봇은 맵 데이터, 센서 정보로부터 검출한 객체 정보 또는 외부 장치로부터 획득한 객체 정보 중에서 적어도 하나 이상을 이용하여 이동 경로와 주행 계획을 결정하고, 구동부를 제어하여 결정된 이동 경로와 주행 계획에 따라 자율 주행하는 로봇을 주행시킬 수 있다. 맵 데이터에는 자율 주행 차량(100b)이 주행하는 공간(예컨대, 도로)에 배치된 다양한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 예컨대, 맵 데이터에는 가로등, 바위, 건물 등의 고정 객체들과 차량, 보행자 등의 이 동 가능한 객체들에 대한 객체 식별 정보가 포함될 수 있다. 그리고, 객체 식별 정보에는 명칭, 종류, 거리, 위 치 등이 포함될 수 있다. 또한, 자율 주행하는 로봇은 사용자의 제어/상호작용에 기초하여 구동부를 제어함으로써, 동작을 수행하거 나 주행할 수 있다. 이때, 자율 주행하는 로봇은 사용자의 동작이나 음성 발화에 따른 상호작용의 의도 정 보를 획득하고, 획득한 의도 정보에 기초하여 응답을 결정하여 동작을 수행할 수 있다. 또한, 로봇은 AI 기술 및 자율 주행 기술이 적용되어, 안내 로봇, 운반 로봇, 청소 로봇, 웨어러블 로봇, 엔터테인먼트 로봇, 펫 로봇, 무인 비행 로봇 등으로 구현될 수 있다. 자율 주행 기능을 가진 로봇은 사용자의 제어 없이도 주어진 동선에 따라 스스로 움직이거나, 동선을 스스 로 결정하여 움직이는 장치들을 통칭할 수 있다. 자율 주행 기능을 가진 로봇은 이동 경로 또는 주행 계획 중 하나 이상을 결정하기 위해 공통적인 센싱 방 법을 사용할 수 있다. 예를 들어, 자율 주행 기능을 가진 로봇은 라이다, 레이더, 카메라를 통해 센싱된 정보를 이용하여, 이동 경로 또는 주행 계획 중 하나 이상을 결정할 수 있다. 본 발명의 실시예를 구성하는 모든 구성 요소들이 하나로 결합되거나 결합되어 동작하는 것으로 설명되었다고 해서, 본 발명이 반드시 이러한 실시예에 한정되는 것은 아니며, 본 발명의 목적 범위 내에서 모든 구성 요소들 이 하나 이상으로 선택적으로 결합하여 동작할 수도 있다. 또한, 그 모든 구성 요소들이 각각 하나의 독립적인 하드웨어로 구현될 수 있지만, 각 구성 요소들의 그 일부 또는 전부가 선택적으로 조합되어 하나 또는 복수 개 의 하드웨어에서 조합된 일부 또는 전부의 기능을 수행하는 프로그램 모듈을 갖는 컴퓨터 프로그램으로서 구현 될 수도 있다. 그 컴퓨터 프로그램을 구성하는 코드들 및 코드 세그먼트들은 본 발명의 기술 분야의 당업자에 의해 용이하게 추론될 수 있을 것이다. 이러한 컴퓨터 프로그램은 컴퓨터가 읽을 수 있는 저장매체(Computer Readable Media)에 저장되어 컴퓨터에 의하여 읽혀지고 실행됨으로써, 본 발명의 실시예를 구현할 수 있다. 컴 퓨터 프로그램의 저장매체로서는 자기 기록매체, 광 기록매체, 반도체 기록소자를 포함하는 저장매체를 포함한 다. 또한 본 발명의 실시예를 구현하는 컴퓨터 프로그램은 외부의 장치를 통하여 실시간으로 전송되는 프로그램 모듈을 포함한다. 이상에서는 본 발명의 실시예를 중심으로 설명하였지만, 통상의 기술자의 수준에서 다양한 변경이나 변형을 가 할 수 있다. 따라서, 이러한 변경과 변형이 본 발명의 범위를 벗어나지 않는 한 본 발명의 범주 내에 포함되는 것으로 이해할 수 있을 것이다."}
{"patent_id": "10-2019-0125202", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 의한 로컬영역을 보여준다. 도 2는 본 발명의 일 실시예에 의한 계층적 딥러닝 네트워크의 구성을 보여준다. 도 3은 본 발명의 일 실시예에 의한 시계열적인 DNN으로 구성된 로컬영역 분류기를 보여준다. 도 4는 본 발명의 일 실시예에 의한 로봇의 구성을 보여준다. 도 5는 본 발명의 일 실시예에 의한 실내 공간을 로컬영역으로 구분하는 과정을 보여준다. 도 6은 본 발명의 일 실시예에 의한 대규모 공간을 로컬영역으로 구분한 것을 보여준다. 도 7은 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 식별하는 과정을 보여준다. 도 8은 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 확인하기 위해 이동하는 과정을 보여준다. 도 9는 본 발명의 일 실시예에 의한 로봇이 로컬 영역을 확인하기 위해 이동하는 과정을 보여준다. 도 10은 본 발명의 일 실시예에 의한 클라우드 서버의 구성을 보여준다."}
