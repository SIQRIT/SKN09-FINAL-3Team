{"patent_id": "10-2023-0022550", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0130172", "출원번호": "10-2023-0022550", "발명의 명칭": "인공지능을 이용한 창업스토리텔링 제공방법", "출원인": "박은민", "발명자": "박은민"}}
{"patent_id": "10-2023-0022550", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시스템서버가 이미지를 입력받는 이미지입력단계;상기 이미지입력단계에서 입력받은 상기 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하고, 추출된 상기 특징에 대응되는 사물로 분류하는 이미지특징추출 및 분류단계;상기 특징추출 및 분류단계에서 추출된 상기 이미지의 특징에 관한 키워드를 설정하고, 상기 키워드를 기반으로하여 특징키워드문장을 생성하는 문장생성단계;상기 문장생성단계에서 생성된 상기 특징키워드문장과 유사한 창업스토리를 생성하여 출력하는 창업스토리생성단계; 및상기 특징키워드문장으로부터 핵심키워드를 추출하는 키워드추출단계; 를 포함하는 것을 특징으로 하는, 인공지능을 이용한 창업스토리텔링 제공방법."}
{"patent_id": "10-2023-0022550", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1항에 있어서,상기 이미지입력단계 이후의 단계로서,상기 이미지입력단계에서 입력된 상기 이미지의 사이즈를 소정의 규격에 맞게 리사이징하는이미지리사이징단계;를 더 포함하고,상기 이미지특징추출 및 분류단계는, 상기 이미지입력단계에서 입력받은 상기 이미지 대신에 상기 이미지리사이징단계에서 리사이징된 상기 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하는 것을 특징으로 하는,인공지능을 이용한 창업스토리텔링 제공방법."}
{"patent_id": "10-2023-0022550", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 1항에 있어서,상기 시스템서버는,이미지를 입력받는 이미지입력부;상기 이미지입력부로 입력된 상기 이미지의 사이즈를 소정의 규격에 맞게 리사이징하는 이미지리사이징부;이미지 분류 딥러닝 알고리즘 모델을 이용하여 상기 이미지수신부 또는 상기 이미지리사이징부로부터 전달받은상기 이미지로부터 특징을 추출하고, 추출된 상기 특징에 대응되는 사물로 분류하는 특징추출 및 분류부;상기 특징추출 및 분류부에서 추출된 상기 이미지의 특징에 관한 키워드를 설정하고, 상기 키워드를 기반으로하여 특징키워드문장을 생성하는 문장생성부;상기 문장생성부에서 생성된 상기 특징키워드문장과 유사한 창업스토리를 생성하여 출력하는 창업스토리생성부;및상기 특징키워드문장으로부터 핵심키워드를 추출하는 키워드추출부; 를 포함하는 것을 특징으로 하는, 공개특허 10-2024-0130172-3-인공지능을 이용한 창업스토리텔링 제공방법."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 인공지능을 이용한 창업스토리텔링 제공방법에 관한 것으로, 본 발명에 따르면, 시스템서버가 이미지 를 입력받는 이미지입력단계; 이미지입력단계에서 입력받은 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하고 대응되는 사물로 분류하는 이미지특징추출 및 분류단계; 추출된 이미지의 특징에 관한 키워드를 설정하고, 키워드를 기반으로 하여 특징키워드문장을 생성하는 문장생성단계; 생성된 특징키워드문장과 유사한 창업스토리를 생성하여 출력하는 창업스토리생성단계 및 특징키워드문장으로부터 핵심키워드를 추출하는 키워드추출단계;를 포함함으로써 이미지에 연관된 창업스토리텔링을 제공하는 기술이 개시된다."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 창업에 관한 스토리텔링을 제공하는 시스템 및 제공방법에 관한 것으로서, 보다 상세하게는 사진을 기반으로 하여 인공지능을 이용한 창업스토리텔링을 제공하는 시스템 및 이를 이용한 제공방법에 관한 것이다."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스토리텔링은 이야기(Stroy)를 말하다(Telling) 라는 뜻의 합성어로서, 이야기를 말한다는 뜻이다. 이러한 스토 리텔링은 자신이 알고 있는 정보 즉, 이야기를 상대방에게 말하는 전달방식을 의미한다. 오늘날 스토리텔링은 인공지능 분야에서 대두되고 있는 화제이다. 현재까지는 소정의 단어를 입력하면 그 단어에 관한 간단한 문장을 서술하거나 개요를 작성하는 스토리텔링 인공지능 Ai(Artificial intelligence)등 간단한 문장을 출력해주는 인 공지능 Ai는 출시되었다. 그러나 이미지에 연동된 스토리텔링에 관한 기술은 아직 구현되지 않았기에 이미지로는 스토리텔링을 하기 어려 웠으며 기존의 스토리텔링 인공지능 Ai가 출력해주는 문장이 너무 단순하여 실제로 활용하기에는 부족한 점이 많이 있다는 문제점이 있었다. 따라서, 인공지능 Ai를 이용하여 이미지를 분석하고 이미지에 관련된 스토리텔링을 제공하여줄 수 있는 기술에 관한 개발이 요청되고 있었다. 선행기술문헌 특허문헌 (특허문헌 0001) 대한민국 등록특허공보 제10-2466449호"}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 목적은 상기한 바와 같은 종래의 문제점을 해결하기 위한 것으로, 이미지를 기반으로 하여 인공지능 을 이용한 창업스토리텔링을 제공하는 시스템 및 제공방법을 제공함에 있다."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기와 같은 목적을 달성하기 위한 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법은 시 스템서버가 이미지를 입력받는 이미지입력단계; 상기 이미지입력단계에서 입력받은 상기 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하고, 추출된 상기 특징에 대응되는 사물로 분류하는 이미지 특징추출 및 분류단계; 상기 특징추출 및 분류단계에서 추출된 상기 이미지의 특징에 관한 키워드를 설정하고, 상기 키워드를 기반으로 하여 특징키워드문장을 생성하는 문장생성단계; 상기 문장생성단계에서 생성된 상기 특 징키워드문장과 유사한 창업스토리를 생성하여 출력하는 창업스토리생성단계 및 상기 특징키워드문장으로부터 핵심키워드를 추출하는 키워드추출단계를 포함하는 것을 하나의 특징으로 할 수도 있다. 여기서, 상기 이미지입력단계 이후의 단계로서, 상기 이미지입력단계에서 입력된 상기 이미지의 사이즈를 소정 의 규격에 맞게 리사이징하는 이미지리사이징단계;를 더 포함하고, 상기 이미지특징추출 및 분류단계는, 상기 이미지입력단계에서 입력받은 상기 이미지 대신에 상기 이미지리사이징단계에서 리사이징된 상기 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하는 것을 또 하나의 특징으로 할 수도 있다. 상기와 같은 목적을 달성하기 위한 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공시스템 서 버는 접속되는 단말기 또는 정보기록매체로부터 이미지를 입력받는 이미지입력부; 입력받은 이미지를 리사이징 하는 이미지리사이징부; 이미지 분류 딥러닝 알고리즘 모델인 VIT를 사용한 모델로서 리사이징된 이미지의 특징을 추출하는 이미지추출및분류부, OPEN AI사에서 제공하는 GPT3모델로서 Transformer기반의 언어 생성 모델로 상기 이미지의 특징의 키워드를 기반으로 간단한 문장을 생성하는 문장생성부; 센텐스버트(Sentence Bert) 알고 리즘을 이용해 만든 창업 스토리 AI 모델을 기반으로 문장생성부에서 추출한 문장과 가장 유사한 창업 스토리를 선택하여 출력하는 창업스토리생성부; 문장으로부터 핵심 키워드를 추출하는 키워드추출부; 상기 생성한 이야기 를 번역하는 번역부; 를 포함하는 것을 하나의 특징으로 할 수도 있다. 여기서, 이미지리사이징부는 이미지입력부에서 입력받은 이미지를 이미지 분류 모델에 입력하기 전에 이미지 특 징 추출 모델에 맞게 사이즈 및 해상도를 조정해 줄 수 있는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 이미지특징추출 및 분류부는 VIT기반의 이미지 분류 AI 모델 시스템으로서, 리사이징된 이미지를 전 처 리 해주는 전 처리 단계; 특징을 추출한 후 사진 내에서 해당 특징을 검출하는 제 1 이미지 특징 추출 단계; 및 특징을 검출한 후 이미지를 분류하는 분류 단계;를 수행하는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 문장생성부는 GPT3기반의 AI 모델 시스템으로서, 이미지특징 추출 및 분류부에서 분류된 단어를 입력받 아, 인코더에 입력 후 키워드에 맞는 문장을 생성하여 출력, 출력된 여러 문장들을 하나의 문장으로 합치는 것 을 또 하나의 특징으로 할 수도 있다. 여기서, 창업스토리생성부는 창업에 관한 이야기로 구성되어 있는 센텐스버트(Sentence-Bert)를 기반으로 하는 인공지능 모델로서, 센텐스버트(Sentence-Bert)를 기반으로 학습된 창업스토리와 문장생성부에서 출력된 스토리 를 기반으로 코사인 함수를 통해 유사성을 구한 후, 정밀도를 계산하여 높은 정밀도를 가진 문장을 출력하는 것 을 또 하나의 특징으로 할 수도 있다. 여기서, 키워드 추출부는, 창업 스토리생성부로 부터 입력 받은 문장을 토큰화 하는 단계; 어간 및 표제어를 추 출하는 단계; 불용어를 제거하는 단계; 자주 쓰이는 토큰을 찾는 단계; 결과 값 및 상기 검출 이미지, 상기 검 출한 키워드, 상기 생성한 문장, 생성된 창업 스토리, 키워드 추출부에서 생성되는 키워드를 데이터베이스에 저 장 하는 단계;를 수행하는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 관리자 페이지 인터페이스는 이미지 해상도 조절, 사이즈 조절이 운영자가 입력한 대로 조절 가능한 시 스템으로 관리자 사이트와 데이터 베이스와 연결되어 있는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 상기 검출된 키워드, 생성된 문장 및 생성된 창업 스토리, 키워드 추출부에서 생성되는 키워드를 데이 터 베이스에 저장 하는 단계는, 상기 출력된 키워드, 문장을 입력 받아 창업에 적합한 문장을 만든 후 기존 임 시 데이터베이스에 저장된 데이터를 가져와서 데이터 베이스에 저장하는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 이미지특징추출 및 분류부는 입력된 이미지의 특징을 찾아 특정 사물로 분류하기 위해 이미지를 일련의 고정 크기의 패치(patch)로 분할 한 후, 각 패치를 왼쪽 상단에서 오른쪽 하단의 순서로 나열하여 시퀀 스 데이터처럼 만들어 벡터로 변환하고, 선형 연산을 거쳐 임베딩 작업의 결과를 통해 특정 사물을 예측하는 클 래스 토큰을 추가 ,추가한 클래스 토큰 및 각 이미지 패치에 포지션 임베딩을 더해, 입력값을 만들어 준 후 트 랜스포머 인코더를 반복하여 동일한 크기의 출력 값을얻는 MLP Head를 이용하여 소프트맥스를 적용함으로써 특 정 사물로 분류되는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 문장 생성부는 GPT-3 모델은 transformer 모델 기반으로 transformer에서 사용하는 인코더 및 디코더를 여러 번 중첩하여 사용하며, 인코더 및 디코더는 multi-head attention 및 masked multi-head attention과 Feed forward Neural Network 등을 포함하고 있는 것을 또 하나의 특징으로 할 수도 있다. 여기서, 창업 스토리생성부는, 자체적으로 창업 아이디어에 도움되는 스토리를 수집하고 센텐스버트(Sentence Bert ;SBERT) 인공지능 알고리즘을 이용하여 만든 자체 인공지능 모듈로서 문자 생성부에서 출력된 문장을 바탕 으로 학습시킨 문장 중 문자 생성 시스템과 유사도가 가장 높은 문장을 찾아서 출력하는 것을 또 하나의 특징으 로 할 수도 있다."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에 따른 인공지능을 이용한 창업스토리텔링 제공 시스템 및 창업스토리텔링 제공방법은, 창업스토리텔링 을 위해 제공되는 이미지에 기초하여 스토리텔링 및 주요 키워드를 제공하여 주므로 소정의 이미지에 관련된 창 업스토리텔링을 마련하고자 하는 사용자의 시간과 노력을 절감시켜줄 수 있는 효과가 있다."}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명은 다양한 변환을 가할 수 있고, 여러 가지 실시 예를 가질 수 있는 바, 특정 실시 예들을 도면에 예시 하고 상세하게 설명하고자 한다. 그러나 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변환, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 본 실시 예들은 당해 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에게 본 발명을 더욱 상세하게 설명하기 위해서 제공되는 것이다. 따라서 도면에 나타난 각 요소의 형상은 보다 분명한 설명을 강조하기 위하여 과장될 수 있으며, 본 발명을 설명함에 있어서, 관련된 공지 기술에 대한 구체적인 설명이 본 발명의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략할 수 있다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들은 용어들에 의해 한정되 어서는 안 된다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하여 설명하고 이해하는 목적으로 사용 된다. 본 발명에서 사용한 용어는 단지 특정한 실시 예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 발명에서, \"포함하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 이하, 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 본 발명의 실시 예 에 대하여 첨부한 도면을 참고로 하여 상세히 설명한다. 그러나 본 발명은 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 명세서 전체를 통하여 유사한 부분에 대해서는 동일한 도면 부호를 사용할 수도 있다. 이하 도면을 참조하여 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에 대하여 설명하 기로 한다. 본 발명의 실시 예에 따른 창업 스토리텔링 제공방법을 수행하는 시스템을 통해 생성되는 결과물은 이미지에 관 한 창업 아이디어 스토리이다. 여기서 이미지란, 인공지능을 이용한 창업스토리텔링 제공방법을 실행하는 시스템을 사용하는 사용자에 의해 입 력되는 사물 및 공간에 대한 이미지이며, 창업 아이디어 스토리는 사용자가 입력한 이미지를 바탕으로 인공지능 을 이용한 창업스토리텔링 제공방법을 수행하는 시스템(이하 간략히 창업스토리텔링 제공시스템이라 함)이 만들 어낸 창업에 관한 이야기를 말한다. 따라서 본 발명은 사용자가 입력한 이미지에 대한 창업 스토리라인을 자동으로 완성함으로써, 사용자가 창업에 관한 아이디어를 좀 더 쉽게 생각할 수 있도록 도와줄 수 있는 것이다. 도 1은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법을 개략적으로 나타낸 흐름도이다. 도 1을 참조하면, 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법은 이미지입력단계 (S110), 이미지특징추출 및 분류단계(S130), 문장생성단계(S140) 창업스토리생성단계(S150) 및 키워드추출단계 (S160)를 포함하며 이미지리사이징단계(S120)를 더 포함할 수도 있다. << S110 >> 이미지입력단계(S110)는 시스템의 서버가 이미지를 입력받는 단계이다. 좀 더 구체적으로, 이미지입력단계(S110)에서는 창업스토리텔링 제공시스템의 서버(이하 간략히 서버)가 USB메 모리장치와 같은 전자정보저장기록매체에 저장된 이미지를 입력받거나 또는 서버에 접속된 사용자의 단말기로부 터 전송되는 이미지를 수신하여 입력받는다. << S120 >> 이미지입력단계(S110)에서 서버에 입력된 이미지의 사이즈 또는 해상도에 대한 보정 또는 조정이 필요한 경우에 는 입력받은 이미지에 대한 리사이징이 요구될 수도 있다. 따라서 이미지리사이징단계(S120)는 이미지의 사이즈의 조정 필요 여부에 따라서 추가적으로 더 포함될 수 있는 단계이다. 즉, 이미지입력단계(S110) 이후의 단계로서, 이미지입력단계(S110)에서 입력된 이미지의 사이즈를 소정의 규격 에 맞게 리사이징하는 이미지리사이징단계(S120)를 더 포함할 수도 있다는 것이다. 여기서 리사이징 시킬 규격 은 관리자로부터 사전에 설정 입력될 수 있다. 이미지리사이징단계(S120)를 통해 리사이징된 이미지는 사이즈와 용량이 감소될 수 있다. <<S130>> 기본적으로 이미지특징추출 및 분류단계(S130)는 이미지입력단계(S110)에서 입력받은 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출하고, 추출된 특징에 대응되는 사물로 분류하는 단계이다. 그러나 좀 더 구체적으로 앞서 언급한 이미지리사이징단계(S120)을 더 포함하는 경우, 이미지특징추출 및 분류 단계(S130)에서는, 이미지입력단계(S110)에서 입력받은 이미지 대신에 이미지리사이징단계(S120)에서 리사이징 된 이미지로부터 이미지 분류 딥러닝 알고리즘 모델을 이용하여 특징을 추출한다. 그리고 추출된 특징에 대응되 는 사물로 분류한다. 여기서 도 2를 더 참조한다. 도 2는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 이미지특징추출 및 분류단계에서 이용될 수 있는 VIT알고리즘을 개략적으로 나타낸 개념도이다. 도 2에서 참조되는 바와 같이, 비전트랜스포머(VIT; Vision Transformer)는 트랜스포머(Transformer)라는 기술 에서 파생된 기술로서 트랜스포머에서 사용하는 인코더(Encoder)를 사용한다. 여기서 트랜스포머(Transformer)란 2017년 구글사에서 발표한 알고리즘으로서, 자연어처리(NLP; Natural Language Processing)를 목적으로 개발된 알고리즘이다. 이 자연어처리 알고리즘은 굳이 자연어 처리에만 국한 되는 것은 아니며 이 자연어처리 알고리즘을 이미지 처리에 활용한 알고리즘이 비전트랜스포머(VIT; Vision Transformer)이다. 여기서 인코더(Encoder)란 소스(정보)를 압축하는 과정이다. 도 3에 도시된 트랜스포머 인코더(Transformer Encoder)은 창업스토리텔링 제공방법에서 사용되는 센텐스버트(sentence-bert) 알고리즘에서도 사용되므로 여기 서 도 3을 더 참조하여 자세히 설명하고 넘어가고자 한다. 도 3은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 트랜스포머의 인코딩 과정 을 설명하기 위하여 트랜스포머 인코더 및 디코더의 구조를 개략적으로 나타낸 개념도이다. 도 3에서 트랜스포머 인코더의 흐름도를 보면 다음과 같다. 먼저 단어를 입력하기 전에 전처리된 단어들을 임베 딩(Embedding)해준다. 이 때 도 3에서 언급된 인풋 임베딩(Inputs Embedding)에 들어가는 것은 언어 문장의 토 큰(Token) 인덱스(Index)이다. 이 때 단어 수준 임베딩행렬에서 입력된 각 토큰 인덱스에 대응하는 벡터를 참조 (lookup)해서 가져온다. 그 후 포지셔널 인코딩(Positional Encoding)처리를 해주는데 이는 입력 임베딩에 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법이다. 여기서 임베딩(Embedding) 이란 입력한 자료를 기계가 이해할 수 있도록 벡터로 변환한 과정 또는 결과를 의미 한다. 도 3에 도시된 멀티헤드 어텐션(Multi-Head Attention)은 입력값을 여러 개로 쪼개서 어텐션(Attention)을 계산 하는 기법이다. 이 때 입력값을 N개로 쪼갠다면 어텐션 계산을 N번하게 되는데, N번이 순차적으로 진행되는 것 이 아니라 동시에 병렬적으로 진행된다. 멀티 헤드 어텐션의 출력은 입력 단어들에 대응하는 벡터 시퀀스이다. 여기서, 어텐션(Attention)이란 딥러닝 학습시 집중해야 하는 특정 부분에만 집중할 수 있도록 가중치(weight) 를 주는 것이다. 기존의 시퀀스 투 시퀀스(sequence to sequence)에서의 문제점을 해결하기 위해 개발되었다. 여기서 말하는 문제점이란 다음과 같다. 첫 번째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려는 과정에 서 정보 손실이 발생된다는 점, 두 번째, 기존의 언어 학습 모델인 RNN(Recurrent Neural Network), LSTM(Long Short-Term Memory)에서 많이 발생하던 기울기 소실(vanishing gradient)문제이다. 여기서 기울기 소실 문제란 신경망의 활성함수의 도함수 값이 계속 곱해지다 보면 가중치에 따른 결과값의 기울 기가 0이 되어 버려서, 경사 하강법을 이용할 수 없게 되는 문제이다. 기존 어텐션(Attention)이 나오기 전에는 입력 시퀀스가 길면 출력 시퀀스의 정확도가 떨어졌다. 어텐션"}
{"patent_id": "10-2023-0022550", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "(Attention)은 이에 대한 대안책으로 나왔다. 요약하자면 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예 측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점이다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단 어 부분을 좀 더 집중(attention)해서 보게 한다. 도 3에 도시된 애드놈(Add Norm)에서 애드(Add)는 잔차 연결(residual connection)을 의미하고 놈(Norm)은 레 이어 정규화(layer normalization)를 의미한다. 여기서 애드(Add) 즉 잔차 연결(residual connection)이란, 블록(block) 계산을 건너뛰는 경로를 하나 두는 것 을 말한다. 입력을 x, 이번 계산 대상 블록을 F라고 할 때 잔차 연결은 F(x)+x로 간단히 실현한다. 만약 잔차 연결이 없다면 모든 함수를 연속적으로 수행하는 경로 한 가지만 존재한다. 하지만 잔차 연결을 수행 함으로서 다양한 관점으로 블록 계산이 수행 가능하게 된다. 여기서 놈(Norm) 즉 레이어 정규화(layer normalization)란, 미니 배치의 인스턴스별로 평균을 빼주고 표준편차 로 나누어 주어서 정규화(normalization)을 수행하는 기법이다. 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과가 있다. 도 3에 도시된 피드 포워드(Feed Forward)는 신경망(neural network)의 한 종류로 입력층(input layer), 은닉 층(hidden layer), 출력층(ouput layer) 3개 계층으로 구성되어 있고 완전 연결학습(전부 연결된 학습)으로 진 행된다. 피드포워드 뉴럴네트워크의 학습 대상은 가중치와 바이어스다. 이들은 태스크(예: 기계 번역)를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트된다. 여기서 가중치(Weight)란 전체 집단에서 개별 구성요소가 차지하는 비중이나 중요도를 수치로 나타낸 값을 말한 다. 딥러닝에서는 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수라고 볼 수 있다. 여기서 바이어스(bias)란 통계적 추정결과가 체계적으로 한 쪽으로 치우치는 경향을 보임으로써 발생하는 오차 다. 바이어스(bias)를 사용하는 이유는 활성화 함수 기울기 조절 때문이다. 즉 뉴런의 결과 값을 어떻게 변형하 여 출력하느냐를 조정하는 매개변수라 볼 수 있다. 여기서 피드 포워드(Feed Forward)의 역할은 멀티헤드 어텐션(Multi-Head Attention)에서 나온 결과를 취합하여 전달하는 역할이다. 도 2에서 도시된 바와 같이, 알고리즘의 흐름은 ①~⑤까지 순서대로 진행된다. 도 2에서 도시된 ①은, 비전트랜스포머(VIT) 알고리즘이 이미지를 처리하기 위하여 고객이 입력한 이미지를 처 리하기 쉽게 쪼개는 과정이다. 이 때 알고리즘은 이미지를 여러 개의 패치(작은 크기의 조각)로 자른다. 도 2에서 도시된 ②은, ①에서 패치 단위로 자른 이미지를 인코더(Encoder)가 처리할 수 있도록 평평(flatten) 하게 1차원 배열로 나열해주는 과정이다.도 2에서 도시된 ③은, ②에서 나열한 이미지에 위치(position)를 표시해서 임베딩(Embedding)하고 클래스 토큰 (class token)을 추가 해주는 과정이다. 여기서 클래스 토큰(class token)은 MSA(Multi-head Self Attention)을 사용하여 모든 패치에서 정보를 수집하 는 역할을 담당한다. 도 2에서 도시된 ④은, ③에서 임베딩된 자료를 인코딩해주는 과정으로서 트랜스 포커 인코더를 사용한다. 자세 한 인코딩 과정은 옆에 도시되었다. 도 2에서 도시된 ④-①은, 레이어 정규화(layer Normalization)과정으로 미니 배치의 인스턴스별로 평균을 빼주 고 표준편차로 나눠줘 정규화(normalization)를 수행하는 기법이다.이로 인해 학습이 안정되고 빨라진다. 도 2에서 도시된 ④-②은, 멀티헤드 어텐션(Multi-Head Attention)은 위의 도6을 설명하며 언급된 멀티헤드 어 텐션(Multi-Head Attention)과 동일하다. 도 2에서 도시된 ④-③은, 멀티 레이어 퍼셉트론(Multi Layer Perceptron ; MLP)으로서 여러 계층의 노드(신경 망)이 완전히 연결된 인공신경망 클래스이다. 멀티레이어 퍼셉트론(MLP)를 끝내면 가우시안 에러 선형 유닛 (GELU ; Gaussian Error Linear Unit)라는 활성화 함수로 출력값을 계산한다. 도 2에서 도시된 ⑤는, ④에서 인코딩된 결과가 구현된 최종값이다. 도 2에서 도시된 ⑥는, ⑤의 멀티 레이어 퍼셉트론 헤드(MLP Head)값을 기반으로 클래스(class)를 분류하는 것 을 나타낸 것이다. 도 2에서 도시된 바와 같이, 이미지특징추출 및 분류단계(S130)에서 다음과 같은 VIT기반의 프로세스를 거쳐 이 미지를 분류할 수 있다. 도 2에서 출력되는 결과물은 리사이징단계(S120)에서 리사이징된 이미지를 이미지특징추출 및 분류단계(S130)에 서 특징 추출 및 분류 프로세스를 거쳐 분류된 키워드이다. 예를 들어 도 1에 나타낸 바와 같이 카페 사진이미 지를 입력받으면 \"cafe\"라는 사진 내 객체에 관한 단어가 추출되고 만약 강아지가 산책하다가 고양이를 마주친 사진을 입력하면 \"dog\",\"cat\"이라는 사진 내 객체에 두 가지에 관한 단어가 추출된다. 이와 같이, 이미지특징추출 및 분류단계(S130)에서는 이미지로부터 특징을 추출해내고, 추출된 이미지의 특징에 관한 키워드를 설정한다. << S140 >> 다음으로 문장생성단계(S140)는, 이미지특징추출 및 분류단계(S130)에서 이미지로부터 추출되어 설정된 키워드 를 기반으로 하여 특징키워드문장을 생성하는 단계이다. 도 1에서 참조되는 바와 같이, 이미지특징추출 및 분류단계(S130)에서 추출된 키워드에 관한 간단한 문장을 생 성한다. 예를 들어, GPT3를 포함하는 서버의 문장생성부에서 키워드에 관한 간단한 문장을 생성할 수 있다. 여기서 GPT3 역시 트랜스포머(Transformer) 알고리즘 기반의 딥러닝 알고리즘 모델이다. OPEN AI사에서 개발한 세계적인 NLP모델로서 2020년에 출시 되었다. GPT3모델은 키워드를 기반으로 간단한 문장을 생성할 수 있는 모 듈이 포함되어 있으며 우리는 이 모듈을 기반으로 키워드에 대한 간단한 설명을 제시한다. GPT3에서 생성된 문장은 하나의 주제가 아닐 수 있다. 창업 스토리생성부에 입력할 때 키워드를 조합한 창업 스 토리를 얻기 위하여 문장을 합쳐준다. <<S150>> 창업스토리생성단계(S150)는 문장생성단계(S140)에서 생성된 특징키워드문장과 유사한 창업스토리를 생성하여 출력하는 단계이다. 여기서 도 4를 더 참조한다. 도 4는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 창업스토리생성단계(S150) 에서 이용될 수 있는 센텐스버트(Sentence BERT)의 알고리즘을 개략적으로 나타낸 도면이다. 여기서 센텐스버트(Sentence-Bert)알고리즘은 2019년 구글에서 발표한 알고리즘으로서, 버트(Bert) 알고리즘에 서 문장 임베딩 성능을 중점적으로 개선한 모델이다. 여기서 버트(Bert)모델은 구글에서 개발한 자연어처리(NLP; Natural Language Processing) 사전 훈련 기술이 며 트랜스포머(Transformer) 알고리즘 기반의 딥러닝 알고리즘 모델이다. 또한 버트(Bert)는 레이블이 없는 데 이터로부터 레이블을 스스로 생성해 사전 학습을 진행해서 만들어진(pre-trained) 모델이다. 버트(Bert)의 가장 큰 특징은 위키피디아(25억 단어) 및 북코퍼스(BookCorpus)(8억 단어)와 같은 레이블 없는 텍스트 데이터로 사전 훈련된 언어 모델이라는 점으로, 이렇게 방대한 레이블 없는 데이터로 사전 훈련된 모델 을 가지고 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용 하여 성능을 높인다는 것이다. 여기서 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하는 작업을 파인 튜닝 (Fine-tuning)이라 한다. 여기서 수집한 창업 스토리가 한 문장에 의미가 담길 수 있도록 전처리한 후 파인 튜닝(Fine-tuning)을 진행하 였다. 창업 스토리를 되도록 한 문장으로 정의한 이유는 사진 속 객체가 여러 개여서 키워드가 여러 개 나왔을 때 해당 키워드를 조합한 창업 문장이 자연스럽게 출력되도록 하기 위함이다. 여기서 파인 튜닝시 사용한 함수는 MNR loss(Multiple Negatives Ranking loss)이다. MNR loss란 앵커(ancho r)와 포지티브(positive) 사이의 거리는 가까워지도록, 앵커(anchor)와 네거티브(negative) 사이의 거리는 멀어 지도록 학습을 유도하는 함수이다. 도 5는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 버트(BERT)의 흐름을 개략 적으로 나타낸 개념도이다. 버트(Bert)의 입력값은 임베딩층(Embedding layer)를 지난 임베딩 벡터이고 평균적으로 768차원으로 정의된다. 이 의미는 버트(BERT)가 내부적인 연산을 거친 후, 동일하게 각 단어에 대해서 768차원의 벡터를 출력한다는 것 을 의미한다. 여기서 버트(BERT)의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 된다. 여기서 버트(Bert)는 하나의 단어가 모든 단어를 참고하는 연산은 사실 버트(BERT)의 모든 계층에서 전부 이루 어지는 연산을 하는데 이게 가능한 이유는 도 5에 기재된 버트(Bert)인코더 내 중첩되어 있는 트랜스포머 (Transformer) 인코더 때문이다. 여기서 버트(Bert)는 총 3번의 임베딩(Embedding)과정을 거친다. 첫 번째는 도 5에 기재된 ① 워드피스 (WordPiece)임베딩, 두 번째는 ②에 기재된 포지션임베딩(Position Embedding), 세 번째는 ③에 기재된 세그먼 트임베딩(Segment Embedding)이다. 도 5에 기재된 ① 워디피스(WordPiece) 임베딩은 버트(Bert)에서 실질적인 입력이 되는 임베딩으로서 임베딩 벡 터의 종류는 단어 집합의 크기로 환산하며 총 30,522개이다. 자주 등장하는 단어는 그대로 단어 집합에 등장하 나, 자주 등장하지 않는 단어는 서브 워드로 분리되어 단어 집합에 추가된다는 내용이다. 여기서 서브 워드란 하나의 단어에 있는 더 작은 단위의 의미 있는 단어로서 예를 들면 birthday에서 birth와 day는 각 각 의미가 있는 단어다. 여기서 birth와 day를 서브워드라 한다. 도 5에 기재된 ② 포지션 임베딩(Position Embedding)은 ① 워드피스(WordPiece)임베딩을 거친 단어 임베딩에 위치 정보를 더해주는 임베딩이다. 도 5에 도시된 바와 같이 위치정보는 순차적으로 더해지며 문장의 최대 길이 인 512개까지 가능하다. 도 5에 기재된 ③ 세그먼트 임베딩(Segment Embedding)은 두 개의 문장을 구분하기 위한 임베딩으로서 첫번째 문장에는 센텐스(Sentence) 0 임베딩, 두번째 문장에는 센텐스(Sentence) 1 임베딩을 더해주는 방식이다. 이 때 세그먼트 임베딩 벡터는 두 개만 사용된다. 여기서 버트(Bert)는 하나의 텍스트에 대한 텍스트 분류 유형, 하나의 텍스트에 대한 태깅작업(Tagging), 텍스 트의 쌍에 대한 분류 또는 회귀 문제, 질의 응답등 여러 문제를 해결할 수 있는데 우리는 텍스트의 쌍에 대한 분류 또는 회귀 문제로 진행한다. 이는 자연어 추론(Natural language inference)문제를 해결하는데 적합하다. 여기서 자연어 추론 문제란, 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는 지를 분류하는 것이다. 여기서 회귀란 통계학에서 사용하는 자료 분석 방법 중 하나로, 간략히 표현해 여러 자료들 간의 관계성을 수학 적으로 추정, 설명하는 것을 말한다. 도 4에서 센텐스버트(Sentence-Bert) 알고리즘의 흐름도를 보면 버트(Bert)가 끝나면 풀링(Pooling)이라는 과정 을 거치는 것을 확인할 수 있다. 버트(Bert)로부터 나온 임베딩된 문장 즉 벡터들은 풀링(Pooling)이라는 과정 을 거쳐야 하는데 해당 풀링(Pooling)에는 두 가지가 있다. 즉, 평균 풀링(Mean Pooling)과 맥스 풀링(Max Pooling)이 있는데 이 두 가지의 풀링(Pooling)을 통해 문장이 가지는 의미는 달라진다. 여기서 평균 풀링(Mean Pooling)으로 얻는 문장 벡터는 문장 내 모든 단어의 의미를 반영한다. 여기서 맥스 풀링(Max Pooling)으로 얻는 문장 벡터는 중요한 단어의 의미를 반영한다. 우리 서비스에서는 버트 (Bert)로 얻은 문장 벡터에 맥스풀링(Max Pooling)을 진행하여 중요한 단어의 의미를 중점적으로 반영하는 벡터 를 얻는다. 여기서 맥스 풀링(Max Pooling)으로 풀링(Pooling)을 진행한 이유는( 설명기재 없음.) 도 4에서 맥스풀링(Max Pooling)으로 얻은 벡터의 결과 값은 각각 u,v 이다. 이 두 벡터의 코사인 유사도를 계 산한다. 이 때 u,v간에 코사인 유사성을 구한 다음 평균 제곱 오차 손실(MSE ; Mean Squared Error) 최적화를 진행한다. 여기서 평균 제곱 오차란 오차(error)를 제곱한 값의 평균이다. 오차란 알고리즘이 예측한 값과 실제 정답과의 차이를 의미를 말한다. 즉, 알고리즘이 정답을 잘 맞출수록 평균제곱오차 값은 작다. 평균 제곱 오차 손실을 구한 후 u와 v 문장들 사이에서 오차가 적은대로 가져와서 각 문장의 정밀도 점수를 계 산한다. 여기서 계산된 정밀도 점수가 가장 큰 문장을 GPT3 출력문장과 가장 비슷한 문장으로 간주하고 출력합 니다. <<S160>> 다음으로 키워드추출단계(S160)는 상기 특징키워드문장으로부터 핵심키워드를 추출하는 단계이다. 여기서 도 6을 더 참조한다. 도 6은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 키워드추출단계에서의 데이터흐름을 개략적으로 나타낸 도면이다. 도6에서 도시된 바와 같이 키워드 추출부 레머터제이션(Lemmatization), 스톱워드(Stopword), 원-핫 인코딩(One-Hot Encoding) 의 흐름순으로 진행된다. 도 6에 표시된 키워드 추출부의 흐름도는 데이터 전처리와 유사하다. 데이터 전처리란 토큰화,정제,정규화 를 포함한다. 토큰화란 단어를 의미 있는 단위로 나누는 것을 말한다. 정제(Cleaning)는 가지고 있는 데이터로부터 노이즈 데이터를 제거한다는 의미이고 정규화(Normalization)는 표 현 방법이 다른 단어를 통합시켜 같은 단어로 만든다는 의미다. 이때 규칙 기반 표기가 다른 단어(ex:USA-US) 대, 소문자 통합, 불필요한 단어 제거(등장 빈도가 적음, 길이가 짧은 단어)를 시행한다. 도 6에 표시된 레머티제이션(Lemmatization)은 표제어 추출(Lemmatization)이라는 의미다. 이 둘의 목적 은 단어의 원형을 찾는 것에 있다. 단어의 원형이란 다음과 같다. 영어에 play라는 말은 상태에 따라 playing, played, plays로도 나타날 수 있다. 이 때 앞의 playing, played, plays의 원형은 play이다. 즉 우리는 표제어 추출을 통해 단어의 원형인 play를 찾는 것이다. 여기서 레머티제이션(Lemmatization) 사용시 단어의 원형을 찾는 예시는 다음과 같다. 예를 들어, \"The striped bats are hanging on their feet for best\" 다음과 같은 문장에서 표제어 추출시 결 과 값은 ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best＇]과 같이 출력된다. 도 6에 표시된 스톱워드(stopword)는 '불용어'라는 의미다. 이는 데이터에서 유의미한 단어만 선택하기 위 해 큰 의미가 없는 단어를 제거하는 작업이다. 스톱워드(stopword) 사용 시 불용어를 찾는 예시는 다음과 같다. 예를 들어, \"Family is not an important thing. It's everything.\" 다음과 같은 문장에서 불용어를 제거하면 ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.'] 다음과 같은 결과 값이 출력된다. 도 6에 기재된 원-핫 인코딩(One-Hot Encoding)는 문자를 숫자로 바꾸는 기법 중 가장 기본적인 방법이다. 원-핫 인코딩(One-Hot Encoding)는 단어 집합의 크기를 벡터의 차원으로 표기하고 표현하고 싶은 단어의 인덱스에 숫자를 부여한다. 여기서 단어 집합이란 서로 다른 언어들의 집합을 의미한다. 원-핫 인코딩(One-Hot Encoding) 사용 시 예시는 다음과 같다.\"I want to go home now\" 다음과 같은 문장 에서 원-핫 인코딩(One-Hot Encoding) 사용 시 { \"I\":0, \"want\":1, \"go\":2, \"home\":3 , \"now\":4 }와 같 은 결과 값이 나온다. 이어서, 도 6에 표시된 대로 원-핫 인코딩(One-Hot Encoding) 결과값 기반으로 많이 출력된 단어를 찾는다. << S170 >> 번역단계(S170)에서는 출력된 창업스토리텔링 문장과 키워드를 사용자가 사용하고자 하는 언어로 번역하는 단계 이다. 아울러, 이상에서 설명한 바와 같은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법을 실행할 수 있는 시스템서버는, 이미지를 입력받는 이미지입력부; 상기 이미지입력부로 입력된 상기 이미지의 사 이즈를 소정의 규격에 맞게 리사이징하는 이미지리사이징부; 이미지 분류 딥러닝 알고리즘 모델을 이용하여 상 기 이미지수신부 또는 상기 이미지리사이징부로부터 전달받은 상기 이미지로부터 특징을 추출하고, 추출된 상기 특징에 대응되는 사물로 분류하는 특징추출 및 분류부; 상기 특징추출 및 분류부에서 추출된 상기 이미지의 특 징에 관한 키워드를 설정하고, 상기 키워드를 기반으로 하여 특징키워드문장을 생성하는 문장생성부; 상기 문장 생성부에서 생성된 상기 특징키워드문장과 유사한 창업스토리를 생성하여 출력하는 창업스토리생성부; 및 상기 특징키워드문장으로부터 핵심키워드를 추출하는 키워드추출부; 를 포함할 수 있다. 이상에서 설명된 바와 같이, 본 발명에 대한 구체적인 설명은 첨부된 도면을 참조한 실시 예들에 의해서 이루어 졌지만, 상술한 실시 예들은 본 발명의 바람직한 실시 예를 들어 설명하였을 뿐이기 때문에, 본 발명이 상기의 실시 예에만 국한되는 것으로 이해되어져서는 아니되며, 본 발명의 권리범위는 후술하는 청구범위 및 그 등가개 념으로 이해되어져야 할 것이다."}
{"patent_id": "10-2023-0022550", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법을 개략적으로 나타낸 흐름도이다. 도 2는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 이미지특징추출 및 분류단 계에서 이용될 수 있는 VIT알고리즘을 개략적으로 나타낸 개념도이다. 도 3은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 트랜스포머의 인코딩 과정 을 설명하기 위하여 트랜스포머 인코더 및 디코더의 구조를 개략적으로 나타낸 개념도이다. 도 4는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 창업스토리생성단계에서 이 용될 수 있는 센텐스버트(Sentence BERT)의 알고리즘을 개략적으로 나타낸 도면이다. 도 5는 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 버트(BERT)의 흐름을 개략 적으로 나타낸 개념도이다. 도 6은 본 발명의 실시 예에 따른 인공지능을 이용한 창업스토리텔링 제공방법에서, 키워드추출단계에서의 데이 터흐름을 개략적으로 나타낸 도면이다."}
