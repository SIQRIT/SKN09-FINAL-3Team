{"patent_id": "10-2023-0103620", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0022493", "출원번호": "10-2023-0103620", "발명의 명칭": "시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법", "출원인": "(주)에이아이매틱스", "발명자": "채정훈"}}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집하는 영상 수집부;상기 차량 영상의 시인성을 결정하는 영상 분석부; 및상기 시인성에 기반한 신뢰도와 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델을 통해 상기 차량 영상의시공간적 변화 정보로부터 상기 차량의 사고 상황을 탐지하는 주행상황 탐지부;를 포함하는 시공간심층신경망을활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 영상 수집부는상기 차량 영상으로부터 특정 시간 간격으로 샘플링된 복수의 차량 이미지들을 기초로 정규화된 영상 클립(clip)을 생성하는 것을 특징으로 하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 영상 분석부는상기 차량 영상의 화면 밝기(Luminance), 대비(Contrast), 구조(Structural), 잡음(Noise) 및 카메라 노출 파라미터 중 적어도 하나를 기초로 산출된 영상의 정보량을 이용하여 상기 시인성을 결정하는 것을 특징으로 하는시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 주행상황 탐지부는상기 시인성에 기반하여 상기 시공간심층신경망 기반의 상황 탐지 모델과 가속도 센서에 따른 충격 감지 기능각각의 신뢰도를 조절하는 것을 특징으로 하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서, 상기 주행상황 탐지부는상기 영상 클립을 시간 순서에 따라 상기 상황 탐지 모델의 입력으로 연속하여 제공하는 경우 메모리에 저장된이전 영상 클립 중 현재 영상 클립과 중복되는 이미지들의 특징 데이터를 반복하여 사용하는 것을 특징으로 하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서, 상기 주행상황 탐지부는상기 차량 영상으로부터 상기 시공간적 변화 정보에 관한 특징 데이터를 추출하여 합성곱 연산(convolution공개특허 10-2025-0022493-3-operation) 만으로 구성된 상기 상황 탐지 모델의 입력으로 제공하는 것을 특징으로 하는 시공간심층신경망을활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 상황 탐지 모델은1차원 또는 2차원 합성곱 연산만으로 상기 특징 데이터를 반복적으로 처리하는 복수의 시공간 합성곱 레이어들을 포함하는 것을 특징으로 하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서, 상기 주행상황 탐지부는상기 차량 영상으로부터 상기 특징 데이터를 추출하는 특징 추출 신경망을 상기 상황 탐지 모델에 연결하고, 상기 차량 영상의 연속된 프레임 사이의 차영상 정보를 상기 특징 추출 신경망의 입력으로 제공하는 것을 특징으로 하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치."}
{"patent_id": "10-2023-0103620", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "영상 수집부를 통해, 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집하는 단계;영상 분석부를 통해, 상기 차량 영상의 시인성을 결정하는 단계; 및주행상황 탐지부를 통해, 상기 시인성에 기반한 신뢰도와 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델을 통해 상기 차량 영상의 시공간적 변화 정보로부터 상기 차량의 사고 상황을 탐지하는 단계;를 포함하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명은 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법에 관한 것으로, 상 기 장치는 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집하는 영상 수집부; 상기 차량 영상의 시인성을 결정하는 영상 분석부; 및 상기 시인성에 기반한 신뢰도와 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델 을 통해 상기 차량 영상의 시공간적 변화 정보로부터 상기 차량의 사고 상황을 탐지하는 주행상황 탐지부;를 포 함한다."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 영상분석 기반의 주행 상황 판단 기술에 관한 것으로, 보다 상세하게는 합성곱 연산만으로 구성된 시 공간심층신경망을 기초로 차량 영상의 시공간적 변화를 학습하여 차량의 주행 상황을 탐지하는 시공간심층신경 망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법에 관한 것이다."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "영상 인식 기술은 카메라에서 촬영되는 영상으로부터 객체의 위치를 인식하고, 인식된 객체의 종류를 판별하도 록 구현될 수 있다. 영상 인식 기술은 자율 주행 차량, 감시 카메라 시스템, 로봇 제어 등 다양한 산업분야에서 활용되고 있다. 예를 들어, 자율 주행 차량에서는 정적 객체(차선, 신호등, 표지판, 건물, 보도, 장애물 등)를 인식하고 인식 결과를 토대로 차량의 주행 자세를 제어할 수 있다. 또한, 자율 주행 차량이 동적 객체(주변 차량, 보행자, 자 전거, 이륜차 등)를 인식하는 경우 동적 객체와의 충돌을 예측하고 경고하거나 충돌을 회피하기 위한 자세 제어 를 수행할 수 있다. 차량의 ADAS(Advanced Driver Assistance System), 자율 주행 장치, 원격의 차량 관제 서버 등은 운전 중 발생 할 수 있는 수많은 상황들을 인지하고, 각 상황에 대한 판단을 내려 차량에 장착된 기계장치들을 제어할 수 있 다. 또한, 이들 장치들은 상황 판단 결과를 디스플레이를 통해 출력하여 운전자가 시인할 수 있도록 할 수 있다.차량의 주행 상황에 대한 판단 근거는 대부분 차량에 장착된 카메라에서 촬영된 영상에 기초할 수 있다. 한편, 최근에는 ADAS, 자율 주행 장치, 차량 관제 서버 등에 인공지능(AI: Artificial Intelligence) 기술이 이용되 고 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허 제10-2095152호 (2020.03.24)"}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명의 일 실시예는 합성곱 연산만으로 구성된 시공간심층신경망을 기초로 차량 영상의 시공간적 변화를 학 습하여 차량의 주행 상황을 탐지하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법을 제공하고자 한다. 본 발명의 일 실시예는 개별 이미지의 공간성과 시간의 흐름을 함께 고려하여 특정한 핵심정보를 가정하지 않고 시공간심층신경망이 영상변화의 특성을 최적으로 탐색함으로써 관측 범위 밖의 사고 및 작은 충격의 사고 등도 탐지하는 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법을 제공하고자 한다."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "실시예들 중에서, 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치는 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집하는 영상 수집부; 상기 차량 영상의 시인성을 결정하는 영상 분석부; 및 상기 시인성에 기반한 신뢰도와 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델을 통해 상기 차량 영 상의 시공간적 변화 정보로부터 상기 차량의 사고 상황을 탐지하는 주행상황 탐지부;를 포함한다. 상기 영상 수집부는 상기 차량 영상으로부터 특정 시간 간격으로 샘플링된 복수의 차량 이미지들을 기초로 정규 화된 영상 클립(clip)을 생성할 수 있다. 상기 영상 분석부는 상기 차량 영상의 화면 밝기(Luminance), 대비(Contrast), 구조(Structural), 잡음(Noise) 및 카메라 노출 파라미터 중 적어도 하나를 기초로 산출된 영상의 정보량을 이용하여 상기 시인성을 결정할 수 있다. 상기 주행상황 탐지부는 상기 시인성에 기반하여 상기 시공간심층신경망 기반의 상황 탐지 모델과 가속도 센서 에 따른 충격 감지 기능 각각의 신뢰도를 조절할 수 있다. 상기 주행상황 탐지부는 상기 영상 클립을 시간 순서에 따라 상기 상황 탐지 모델의 입력으로 연속하여 제공하 는 경우 메모리에 저장된 이전 영상 클립 중 현재 영상 클립과 중복되는 이미지들의 특징 데이터를 반복하여 사 용할 수 있다. 상기 주행상황 탐지부는 상기 차량 영상으로부터 상기 시공간적 변화 정보에 관한 특징 데이터를 추출하여 합성 곱 연산(convolution operation) 만으로 구성된 상기 상황 탐지 모델의 입력으로 제공할 수 있다. 상기 상황 탐지 모델은 1차원 또는 2차원 합성곱 연산만으로 상기 특징 데이터를 반복적으로 처리하는 복수의 시공간 합성곱 레이어들을 포함할 수 있다. 상기 주행상황 탐지부는 상기 차량 영상으로부터 상기 특징 데이터를 추출하는 특징 추출 신경망을 상기 상황 탐지 모델에 연결하고, 상기 차량 영상의 연속된 프레임 사이의 차영상 정보를 상기 특징 추출 신경망의 입력으 로 제공할 수 있다. 실시예들 중에서, 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법은 영상 수집부를 통 해, 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집하는 단계; 영상 분석부를 통해, 상기 차량 영상의 시인성을 결정하는 단계; 및 주행상황 탐지부를 통해, 상기 시인성에 기반한 신뢰도와 사전 학습된 시공간심층 신경망 기반의 상황 탐지 모델을 통해 상기 차량 영상의 시공간적 변화 정보로부터 상기 차량의 사고 상황을 탐 지하는 단계;를 포함한다."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "개시된 기술은 다음의 효과를 가질 수 있다. 다만, 특정 실시예가 다음의 효과를 전부 포함하여야 한다거나 다 음의 효과만을 포함하여야 한다는 의미는 아니므로, 개시된 기술의 권리범위는 이에 의하여 제한되는 것으로 이 해되어서는 아니 될 것이다. 본 발명의 일 실시예에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법 은 실시간 단말 인공지능 구현에 적합하도록 종단간처리 방식과 단말기 메인 프로세서 내의 관련된 기능 요소 등을 유기적으로 활용할 수 있다. 본 발명의 일 실시예에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 장치 및 방법 은 영상 AI 기술로 사고 발생 시 차량에 장착된 카메라의 영상변화를 학습하여 사고 순간을 감지하고 가속도 센 서를 보조적으로 활용하여 주행상황 탐지의 신뢰도를 향상시킬 수 있다."}
{"patent_id": "10-2023-0103620", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명에 관한 설명은 구조적 내지 기능적 설명을 위한 실시예에 불과하므로, 본 발명의 권리범위는 본문에 설 명된 실시예에 의하여 제한되는 것으로 해석되어서는 아니 된다. 즉, 실시예는 다양한 변경이 가능하고 여러 가 지 형태를 가질 수 있으므로 본 발명의 권리범위는 기술적 사상을 실현할 수 있는 균등물들을 포함하는 것으로 이해되어야 한다. 또한, 본 발명에서 제시된 목적 또는 효과는 특정 실시예가 이를 전부 포함하여야 한다거나 그러한 효과만을 포함하여야 한다는 의미는 아니므로, 본 발명의 권리범위는 이에 의하여 제한되는 것으로 이해 되어서는 아니 될 것이다. 한편, 본 출원에서 서술되는 용어의 의미는 다음과 같이 이해되어야 할 것이다. \"제1\", \"제2\" 등의 용어는 하나의 구성요소를 다른 구성요소로부터 구별하기 위한 것으로, 이들 용어들에 의해 권리범위가 한정되어서는 아니 된다. 예를 들어, 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다.어떤 구성요소가 다른 구성요소에 \"연결되어\"있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결될 수 도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\"있다고 언급된 때에는 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 한편, 구성요소들 간의 관계를 설명하는 다른 표현들, 즉 \"~사이에\"와 \"바로 ~사이에\" 또는 \"~에 이웃 하는\"과 \"~에 직접 이웃하는\" 등도 마찬가지로 해석되어야 한다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한 복수의 표현을 포함하는 것으로 이해되어야 하고, \"포함 하다\"또는 \"가지다\" 등의 용어는 실시된 특징, 숫자, 단계, 동작, 구성요소, 부분품 또는 이들을 조합한 것이 존재함을 지정하려는 것이며, 하나 또는 그 이상의 다른 특징이나 숫자, 단계, 동작, 구성요소, 부분품 또는 이 들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 각 단계들에 있어 식별부호(예를 들어, a, b, c 등)는 설명의 편의를 위하여 사용되는 것으로 식별부호는 각 단 계들의 순서를 설명하는 것이 아니며, 각 단계들은 문맥상 명백하게 특정 순서를 기재하지 않는 이상 명기된 순 서와 다르게 일어날 수 있다. 즉, 각 단계들은 명기된 순서와 동일하게 일어날 수도 있고 실질적으로 동시에 수 행될 수도 있으며 반대의 순서대로 수행될 수도 있다. 본 발명은 컴퓨터가 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 코드로서 구현될 수 있고, 컴퓨터가 읽을 수 있는 기록 매체는 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기록 장치를 포함 한다. 컴퓨터가 읽을 수 있는 기록 매체의 예로는 ROM, RAM, CD-ROM, 자기 테이프, 플로피 디스크, 광 데이터 저장 장치 등이 있다. 또한, 컴퓨터가 읽을 수 있는 기록 매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산 방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수 있다. 여기서 사용되는 모든 용어들은 다르게 정의되지 않는 한, 본 발명이 속하는 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가진다. 일반적으로 사용되는 사전에 정의되어 있는 용어들은 관 련 기술의 문맥상 가지는 의미와 일치하는 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한 이 상적이거나 과도하게 형식적인 의미를 지니는 것으로 해석될 수 없다. 도 1은 본 발명에 따른 주행 상황 판단 시스템을 설명하는 도면이다. 도 1을 참조하면, 주행 상황 판단 시스템은 차량, 주행 상황 판단 장치 및 데이터베이스를 포함할 수 있다. 여기에서는, 설명의 편의를 위하여 차량, 주행 상황 판단 장치 및 데이터베이스 를 독립된 장치들로 구분하여 설명하지만, 반드시 이에 한정되지 않고, 실시간 주행 상황 판단을 위한 다 양한 실시예에 따라 서로 다른 적어도 2개의 장치들이 하나의 장치로 통합되어 구현될 수 있음은 물론이다. 차량은 엔진에 의해 생산된 동력을 이용하여 승객이나 화물을 운반하는 교통수단에 해당할 수 있고, 대표 적인 예로서 자동차에 해당할 수 있다. 차량은 자동차뿐만 아니라 오토바이, 자전거 등을 포함할 수 있고, 세그웨이(Segway), 전동킥보드, 전동휠체어 등의 퍼스널 모빌리티(person mobility) 등을 포함할 수 있으며, 반 드시 이에 한정되지 않고, 동력을 이용하여 움직일 수 있는 다양한 운송 수단을 포함할 수 있다. 또한, 차량 은 주행 상황 판단 장치와 유선 또는 무선 네트워크를 통해 연결될 수 있으며, 네트워크를 통해 데이 터를 송·수신할 수 있다. 일 실시예에서, 차량은 차량의 주변 영상을 촬영하는 적어도 하나의 카메라를 포함하여 구현될 수 있다. 카메라는 차량의 전방, 후방 및 양측방의 영상을 촬영할 수 있는 위치에 결합될 수 있으며, 필요에 따라 복수개로 형성될 수 있다. 예를 들어, 카메라는 렌즈가 하나인 단안 카메라(mono camera) 또는 렌즈가 두개 이 상인 다안 카메라(stereo camera 등)로 구현될 수 있다. 차량은 카메라를 통해 특정 시간 구간 동안 촬영 된 영상을 저장할 수 있으며, 네트워크로 연결된 주행 상황 판단 장치에게 실시간으로 또는 주기적으로 전 송할 수 있다. 일 실시예에서, 차량은 부품들의 상태를 모니터링하거나 또는 운전자 및 탑승자의 상태를 모니터링하기 위 하여 관련 데이터를 측정할 수 있는 다양한 센서들을 포함하여 구현될 수 있다. 예를 들어, 차량은 차량 움직임에 관한 가속 센서, 브레이크 센서, 가속도 센서, 진동 센서, GPS(Global Positioning System) 센서, 유 량 센서 및 조향각 센서 등을 포함할 수 있고, 운전자 및 차량 환경에 관한 심박동 센서, 온/습도 센서, 가스 센서 및 공기질 센서 등을 포함할 수 있다. 주행 상황 판단 장치는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판 단 방법을 수행하도록 구현된 컴퓨팅 장치 또는 서버에 해당할 수 있다. 주행 상황 판단 장치는 차량(11 0)과 블루투스, WiFi, LTE 등과 같은 무선 네트워크로 연결되어 데이터를 송·수신할 수 있으며, 복수의 차량 들과 동시에 연결되어 동작하도록 구현될 수 있다. 또한, 주행 상황 판단 장치는 본 발명에 따른 영상분석 기반의 실시간 주행 상황 판단을 위하여 별도의 외 부 시스템(도 1에 미도시함)과 연동하여 동작하도록 구현될 수도 있다. 예를 들어, 외부 시스템은 차량 영상과 데이터 분류 정보를 포함하는 학습 데이터셋을 구축하는 데이터 구축 시스템, 차량의 주행상황 예측을 위한 상 황 탐지 모델을 구축하는 학습 시스템(또는 서버) 등을 포함할 수 있다. 즉, 주행 상황 판단 장치는 실제 차량 영상에 기반하여 학습 및 추론을 위한 목적으로 활용될 수 있을 뿐 만 아니라 외부 시스템과의 연동을 통해 사고 상황 영상의 수집 및 저장, 충격 이벤트 감지 및 충격 영상의 저 장 및 보존을 위한 기능을 제공할 수 있다. 또한, 주행 상황 판단 장치는 실시간 사고 영상전송 및 신속한 사고 대응을 처리할 수 있으며, 효율적인 모니터링 운용으로 통신비용을 최소화하여 경제성을 확보할 수 있다. 데이터베이스는 주행 상황 판단 장치의 동작 과정에서 필요한 다양한 정보들을 저장하는 저장장치에 해당할 수 있다. 예를 들어, 데이터베이스는 차량으로부터 수집된 영상 정보를 저장하거나 또는 모델 학습 및 추론을 위한 정보를 저장할 수 있으며, 반드시 이에 한정되지 않고, 주행 상황 판단 장치가 본 발 명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 과정에서 다양한 형태로 수집 또 는 가공된 정보들을 저장할 수 있다. 한편, 도 1에서, 데이터베이스는 주행 상황 판단 장치와 독립적인 장치로서 도시되어 있으나, 반드시 이에 한정되지 않고, 주행 상황 판단 장치의 논리적인 저장장치로서 주행 상황 판단 장치에 포함되어 구현될 수 있음은 물론이다. 도 2는 도 1의 주행 상황 판단 장치의 시스템 구성을 설명하는 도면이다. 도 2를 참조하면, 주행 상황 판단 장치는 프로세서, 메모리, 사용자 입출력부 및 네트워크 입출력부를 포함할 수 있다. 프로세서는 본 발명의 실시예에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법을 수행하기 위한 프로시저를 실행할 수 있고, 이러한 과정에서 읽혀지거나 작성되는 메모리를 관리할 수 있으며, 메모리에 있는 휘발성 메모리와 비휘발성 메모리 간의 동기화 시간을 스케줄 할 수 있다. 프로 세서는 주행 상황 판단 장치의 동작 전반을 제어할 수 있고, 메모리, 사용자 입출력부 및 네트워크 입출력부와 전기적으로 연결되어 이들 간의 데이터 흐름을 제어할 수 있다. 프로세서는 주 행 상황 판단 장치의 CPU(Central Processing Unit), GPU(Graphics Processing Unit) 또는 NPU(Neural Processing Unit)로 구현될 수 있다. 메모리는 SSD(Solid State Disk) 또는 HDD(Hard Disk Drive)와 같은 비휘발성 메모리로 구현되어 주행 상 황 판단 장치에 필요한 데이터 전반을 저장하는데 사용되는 보조기억장치를 포함할 수 있고, RAM(Random Access Memory)과 같은 휘발성 메모리로 구현된 주기억장치를 포함할 수 있다. 또한, 메모리는 전기적으로 연결된 프로세서에 의해 실행됨으로써 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법을 실행하는 명령들의 집합을 저장할 수 있다. 사용자 입출력부은 사용자 입력을 수신하기 위한 환경 및 사용자에게 특정 정보를 출력하기 위한 환경을 포함하고, 예를 들어, 터치 패드, 터치 스크린, 화상 키보드 또는 포인팅 장치와 같은 어댑터를 포함하는 입력 장치 및 모니터 또는 터치 스크린과 같은 어댑터를 포함하는 출력장치를 포함할 수 있다. 일 실시예에서, 사용 자 입출력부은 원격 접속을 통해 접속되는 컴퓨팅 장치에 해당할 수 있고, 그러한 경우, 주행 상황 판단 장치는 해당 컴퓨팅 장치가 연결된 네트워크의 독립적인 노드에 대응될 수 있다. 네트워크 입출력부은 네트워크를 통해 다른 장치와 연결되기 위한 통신 환경을 제공하고, 예를 들어, LAN(Local Area Network), MAN(Metropolitan Area Network), WAN(Wide Area Network) 및 VAN(Value Added Network) 등의 통신을 위한 어댑터를 포함할 수 있다. 또한, 네트워크 입출력부는 데이터의 무선 전송을 위해 WiFi, 블루투스 등의 근거리 통신 기능이나 4G 이상의 무선 통신 기능을 제공하도록 구현될 수 있다.도 3은 도 1의 주행 상황 판단 장치의 기능적 구성을 설명하는 도면이다. 도 3을 참조하면, 주행 상황 판단 장치는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실 시간 주행 상황 판단 방법을 수행할 수 있다. 이를 위하여, 주행 상황 판단 장치는 영상 수집부, 영 상 분석부, 주행상황 탐지부 및 제어부를 포함할 수 있다. 이때, 본 발명의 실시예는 상기의 구성들을 동시에 모두 포함해야 하는 것은 아니며, 각각의 실시예에 따라 상 기의 구성들 중 일부를 생략하거나, 상기의 구성들 중 일부 또는 전부를 선택적으로 포함하여 구현될 수도 있다. 이하, 각 구성들의 동작을 구체적으로 설명한다. 영상 수집부는 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집할 수 있다. 이를 위하여, 영 상 수집부는 차량과 연동하여 동작할 수 있으며, 차량에 설치된 적어도 하나의 카메라와 직접 연결되어 차량 영상을 수신할 수도 있다. 영상 수집부는 카메라가 설치된 방향에 따라 차량의 전방, 후방 또는 양측방 영상을 차량 영상으로서 수신할 수 있다. 한편, 차량에 설치된 카메라는 독립된 모듈로 서 동작할 수 있으나, 반드시 이에 한정되지 않고, 차량에 설치된 시스템의 일부로 포함되어 동작할 수도 있다. 또한, 카메라는 렌즈의 개수에 따라 단안 카메라(mono camera) 또는 다안 카메라(stereo camer) 등으로 구현될 수 있으며, 설치 방향을 기준으로 특정 범위의 시야각(FOV)을 형성하여 FOV 영역 내 영상을 촬영할 수 있다. 일 실시예에서, 영상 수집부는 차량 영상으로부터 특정 시간 간격으로 샘플링된 복수의 차량 이미지들을 기초로 정규화된 영상 클립(clip)을 생성할 수 있다. 차량의 사고 발생 여부를 판단하기 위한 데이터는 단 일 또는 몇 개의 이미지보다는 일정 시간 동안 이어지는 연속적인 이미지들의 집합이 보다 적합할 수 있다. 따 라서, 영상 수집부는 차량 영상을 일정한 간격으로 샘플링하여 추출된 이미지들의 집합을 영상 클립으로서 생성할 수 있다. 예를 들어, 영상 수집부는 0.1초 간격의 16장 이미지로 구성된 1.5초 동안의 영상을 영상 클립으로 생성할 수 있다. 또한, 영상 수집부는 샘플링을 통해 추출된 영상에 대한 전처리 동작을 수행하여 정규화(Normalization)할 수 있다. 영상 전처리 동작에는 영상 부분제거(crop), 크기조정(resizing) 등이 포함될 수 있으며, 이에 대해서 는 도 6을 통해 보다 자세히 설명한다. 일 실시예에서, 영상 수집부는 영상 클립을 사전 정의된 클래스(class)에 따라 분류할 수 있다. 즉, 영상 수집부는 라벨링(labeling) 동작을 통해 수집된 차량 영상에 대해 사고 상황인지 또는 비사고 상황인지를 식별할 수 있다. 한편, 차량 영상(또는 영상 클립)에 부여되는 클래스 정보는 크게 사고 클래스와 비사고 클래 스로 정의될 수 있다. 또한, 영상 수집부는 클래스를 보다 세분화하여 일부 클래스 또는 천이 구간을 학습에서 제외하는 방식으 로 모델의 성능을 개선할 수 있다. 예를 들어, 사고 클래스는 다시 충돌 방향을 기준으로 세분화될 수 있고, 비 사고 클래스는 충격, 진동 또는 흔들림의 원인을 기준으로 세분화될 수 있다. 차량 영상에 관한 데이터 클래스 에 대해서는 도 7을 통해 보다 자세히 설명한다. 영상 분석부는 차량 영상의 시인성을 결정할 수 있다. 여기에서, 차량 영상의 시인성은 영상 내 객체의 식 별이 쉬운 정도를 나타내는 특성에 해당할 수 있다. 특히, 영상 분석부는 AI 모델을 통해 사고 상황을 탐 지하는 과정에서 모델 관점의 시인성을 결정할 수 있다. 이때, 차량 영상의 시인성은 다양한 요인에 의해 결정 될 수 있다. 일 실시예에서, 영상 분석부는 차량 영상의 화면 밝기(Luminance), 대비(Contrast), 구조(Structural), 잡음(Noise) 및 카메라 노출 파라미터 중 적어도 하나를 기초로 산출된 영상의 정보량을 이용하여 시인성을 결 정할 수 있다. 예를 들어, 영상 분석부는 차량 영상에 블러(blur)를 적용한 영상과 원래의 영상 간의 구조 적 유사 지수(SSIM, Structural Similarity Index)를 통해 유사도를 산출할 수 있으며, 유사도가 클수록 시인 성을 낮게 결저하고 유사도가 작을수록 시인성을 높게 결정할 수 있다. 주행상황 탐지부는 시인성에 기반한 신뢰도와 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델을 통 해 차량 영상의 시공간적 변화 정보로부터 차량의 사고 상황을 탐지할 수 있다. 여기에서, 상황 탐지 모델 은 차량 영상으로부터 사고 상황을 탐지하는 인공지능 모델에 해당할 수 있으며, 시공간심층신경망을 기반으로 생성될 수 있다. 상황 탐지 모델은 차량 영상을 입력으로 수신하고 해당 차량 영상 내 사고 상황의 발생 확률을 출력으로 생성할 수 있다. 이때, 상황 탐지 모델의 입력은 차량 영상에서 추출된 특징 정보가 사용될 수 있다.즉, 상황 탐지 모델은 영상 내 변화를 학습한 시공간심층신경망 기반으로 구축된 결과 객체(object), 추정 거리 (estimated distance) 및 광학 흐름(optical flow) 등의 중간 정보를 가정하지 않더라도 차량의 주행 상 황을 탐지할 수 있다. 또한, 주행상황 탐지부는 상황 탐지 모델에 의해 출력된 사고 확률에 기초하여 사고 또는 비사고 상황을 탐지할 수 있고, 모델 학습에 사용된 데이터셋의 세부 클래스 분류에 따라 보다 세분화된 주행상황을 탐지할 수도 있다. 또한, 주행상황 탐지부는 차량으로부터 수집된 차량 영상에 관한 학습 데이터셋을 기초로 상황 탐지 모델을 사전에 학습하여 구축할 수 있다. 상황 탐지 모델의 학습 과정은 학습 데이터의 영상 클립으로부터 추출 된 특징 정보를 상황 탐지 모델에 입력하여 사고 확률을 획득하고, 학습 데이터의 정답(Ground Truth)과 사고 확률을 기초로 손실 함수(loss function)를 계산한 다음, 상황 탐지 모델에 관한 전체 손실을 최소화하는 방향 으로 모델의 가중치를 갱신하는 반복 과정을 통해 수행될 수 있다. 또한, 주행상황 탐지부는 시인성에 기반하여 신뢰도를 결정할 수 있으며, 신뢰도를 기준으로 상황 탐지 모 델 또는 가속도 센서에 따른 충격 감지 기능을 독립적으로 활성화하여 차량 영상으로부터 차량의 사고 상 황을 탐지할 수 있다. 한편, 주행 상황 판단 장치는 모델 구축을 위한 별도의 독립된 모듈을 포함하여 구현될 수 있으며, 주행상 황 탐지부는 해당 모듈에 의해 구축된 상황 탐지 모델을 이용하여 주행 상황 탐지 동작을 수행할 수 있다. 예를 들어, 상황 탐지 모델은 단말 인공지능으로 NPU에 이식되어 동작할 수 있다. 일 실시예에서, 주행상황 탐지부는 시인성에 기반하여 시공간심층신경망 기반의 상황 탐지 모델과 가속도 센서에 따른 충격 감지 기능 각각의 신뢰도를 조절할 수 있다. 즉, 주행상황 탐지부는 영상의 시인성에 따 라 AI 모델과 함께 가속도 센서를 사용하여 차량의 사고 상황을 탐지할 수 있다. 특히, 주행상황 탐지부 는 시인성에 기반하여 상황 탐지 모델과 가속도 센서의 신뢰도를 동적으로 조절할 수 있으며, 이를 기반으 로 주행 상황 탐지의 인식 결과를 생성할 수 있다. 이에 대해서는 도 10을 통해 보다 자세히 설명한다. 한편, 주행상황 탐지부는 주행 상황 탐지를 위하여 가속도 센서 외에 다양한 센서 데이터를 추가적으로 활 용할 수도 있다. 예를 들어, 주행상황 탐지부는 차량에 설치된 다양한 센서들로부터 OBD(On-Board Diagnostics) 신호, IMU(Inertial Measurement Unit) 신호 및 GPS(Global Positioning System) 신호 등을 수신 할 수 있으며, 해당 신호들을 기초로 차량의 움직임 정보(예: 위치, 차속, 회전각 등)를 생성할 수 있다. 일 실시예에서, 주행상황 탐지부는 영상 클립을 시간 순서에 따라 상황 탐지 모델의 입력으로 연속하여 제 공하는 경우 메모리에 저장된 이전 영상 클립 중 현재 영상 클립과 중복되는 이미지들의 특징 데이터를 반복하 여 사용할 수 있다. 주행상황 탐지부는 중복적인 데이터 처리 과정을 방지하기 위하여 연속적인 영상 클립 간의 동일한 이미지들에 대해 특징 추출 과정이 반복되지 않도록 할 수 있다. 이에 대해서는 도 9에서 보다 자 세히 설명한다. 일 실시예에서, 주행상황 탐지부는 차량 영상으로부터 시공간적 변화 정보에 관한 특징 데이터를 추출하여 합성곱 연산(convolution operation) 만으로 구성된 상황 탐지 모델의 입력으로 제공할 수 있다. 이를 위하여, 주행상황 탐지부는 차량 영상으로부터 특징 데이터를 추출하기 위한 특징 추출 신경망을 활용할 수 있다. 예를 들어, 특징 추출 신경망은 차량 영상 내에 포함된 객체의 공간적 정보와 시간의 흐름에 따른 변화에 관한 시공간적 변화 정보를 해당 차량 영상의 특징 정보로서 출력할 수 있다. 이때, 특징 추출 신경망의 입력은 차량 영상의 연속된 프레임(frame) 사이의 차영상 정보가 사용될 수 있다. 또한, 주행상황 탐지부는 상황 탐지 모델을 트랜스포머(Transformer) 모델에서 주로 활용되는 어텐션 (attention) 등의 신경망을 사용하지 않고 합성곱 연산으로만 구성할 수 있다. 즉, 상황 탐지 모델은 차량 영상 내 각 이미지의 공간성과 시간의 흐름을 함께 고려하여 사고 발생 순간을 탐색하는 시공간 합성곱신경망 형태의 심층신경망으로 구현될 수 있다. 특히, 차량 사고는 물리적으로 실재하는 객체 간의 충돌인 점에서 일정 범위의 속도로 진행될 수 있고, 사고 발 생 시 차량에 설치된 카메라를 통해 화면의 움직임이 발생하는 점에서 상황 탐지 모델은 화면 전체의 일방 향 움직임과 떨림, 그리고 화면상 객체의 급격한 움직임 등에 민감한 경향을 나타낼 수 있다. 이에 따라, 주행 상황 탐지부는 지역적 상관관계를 잘 반영하는 합성곱신경망 구조로 상황 탐지 모델을 구축할 수 있고, 전 역적 상관관계를 잘 반영할 수 있도록 보완된 구조를 적용하여 상황 탐지 모델을 구축할 수 있다. 일 실시예에서, 상황 탐지 모델은 1차원 또는 2차원 합성곱 연산만으로 특징 데이터를 반복적으로 처리하는 복 수의 시공간 합성곱 레이어(Spatio-Temporal Convolution Layer)들을 포함할 수 있다. 즉, 시공간 합성곱 레이 어는 합성곱 연산 단계에서 3차원 구조의 합성곱 커널을 사용하는 대신에 합성곱 커널의 차원을 2차원 또는 1차 원으로 분리하여 합성곱을 적용하도록 설계될 수 있다. 이를 통해, 상황 탐지 모델의 전체 복잡도와 연산량을 감소될 수 있다. 예를 들어, 시공간 합성곱 레이어는 합성곱 신경망의 전역 정보 처리의 한계를 극복하기 위하여 공간축과 시간 축이 결합된 차원에서 모든 노드 간의 상관관계를 가정하는 완전연결계층(Fully Connected Layer)이 적용될 수 있다. 또한, 시공간 합성곱 레이어는 완전연결계층에서 공간축과 시간축을 결합할 때 모든 차원을 동시에 결합 하지 않고 세로축과 시간축, 가로축과 시간축, 그리고 세로축과 가로축으로 2차원 단위로 결합할 수 있으며, 이 를 통해 고차원의 정보를 처리함과 동시에 연산량을 큰 폭으로 줄일 수 있다. 일 실시예에서, 주행상황 탐지부는 차량의 사고 상황을 탐지하고 주행 상황 판단 결과를 시각적으로 표시할 수 있다. 이를 위하여, 주행상황 탐지부는 디스플레이 모듈과 연동하여 동작할 수 있다. 예를 들어, 주행상황 탐지부는 사고 상황이 탐지된 경우 디스플레이 모듈을 통해 사고 발생을 알리는 텍스트 또 는 그래픽을 표시할 수 있으며, 정상 상황인 경우에도 사고 확률이 높은 경우 디스플레이 모듈을 통해 경고 및 알림을 제공할 수 있다. 제어부는 주행 상황 판단 장치의 전체적인 동작을 제어하고, 영상 수집부, 영상 분석부 및 주행상황 탐지부 간의 제어 흐름 또는 데이터 흐름을 관리할 수 있다. 도 4는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법을 설명하는 순 서도이다. 도 4를 참조하면, 주행 상황 판단 장치는 영상 수집부를 통해 특정 시간 구간 동안 차량에서 촬 영된 차량 영상을 수집할 수 있다(단계 S410). 주행 상황 판단 장치는 영상 분석부를 통해 차량 영상 의 시인성을 결정할 수 있다(단계 S420). 주행 상황 판단 장치는 주행상황 탐지부를 통해 영상의 시 인성이 정상인지 여부를 결정할 수 있다(단계 S430). 또한, 주행 상황 판단 장치는 주행상황 탐지부를 통해 시인성이 기 설정된 기준 이하인 불량 상태인 경우 가속도 센서에 따른 충격 감지 기능을 활성화할 수 있다(단계 S440). 이에 따라, 주행 상황 판단 장치 는 안전 모드(Fail Safety mode)로 동작할 수 있으며, 차량 내 가속도 센서에 의해 수집된 센서 데이 터를 활용하여 차량의 충격 이벤트를 검출하고, 충격에 따른 사고 상황 알림 및 대응 동작을 수행할 수 있 다(S460). 또한, 주행 상황 판단 장치는 주행상황 탐지부를 통해 시인성이 기 설정된 기준을 충족하는 정상 상 태인 경우 사전 학습된 시공간심층신경망 기반의 상황 탐지 모델을 통해 차량 영상의 시공간적 변화 정보로부터 차량의 사고 상황을 탐지할 수 있다(단계 S450, S460). 즉, 주행 상황 판단 장치는 모델 기반의 탐지 동작을 수행하는 AI 모드로 동작할 수 있다. 도 5는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 과정을 설명하는 도 면이다. 도 5를 참조하면, 주행 상황 판단 장치는 개별 이미지의 공간성과 시간의 흐름을 함께 고려하여 사고발생 순간을 탐색하는 시공간 합성곱신경망 형태의 심층신경망을 구축할 수 있고, 이를 활용하여 차량 영상으로부터 사고 상황을 탐지할 수 있다. 주행 상황 판단 장치는 확보된 다수의 학습용 데이터(Ground Truth)를 이용 한 훈련을 통해 심층신경망 모델의 파라미터를 최적화할 수 있으며, 사고발생 상황의 판별에 최적인 영상변화의 특성을 알고리즘이 직접 탐색하게 함으로써 기존 기술의 한계를 극복할 수 있다. 예를 들어, 주행 상황 판단 장치는 시간의 흐름에 따라 촬영된 차량 영상(영상 #1 ~ #10) 내에서 영상 변 화에 따른 사고상황(영상 #5, #6)을 학습하여 심층신경망 모델을 구축할 수 있다. 주행 상황 판단 장치는 심층신경망 모델에 의해 출력된 추론값을 임계치(threshold)와 비교하여 임계치보다 큰 경우를 사고 상황(영상 #6, True)으로 탐지할 수 있다. 한편, 주행 상황 판단 장치는 심층신경망 모델이 특정한 핵심정보(객체, 거리추정, 광학흐름 등)를 가정하 지 않도록 설계된 결과 영상 내 변화를 기초로 카메라 관측 범위(FOV) 밖에서 발생하는 사고 및 작은 충격의 사 고 등을 탐지할 수도 있다. 도 6은 본 발명에 따른 데이터 전처리 과정의 일 실시예를 설명하는 도면이다. 도 6을 참조하면, 주행 상황 판단 장치는 특정 시간 구간 동안 차량에서 촬영된 차량 영상을 수집할 수 있다. 특히, 주행 상황 판단 장치는 차량 영상으로부터 일정 시간 동안 이어지는 연속적인 이미지들을 추출하여 영상 클립(Clip)을 생성할 수 있다. 예를 들어, 영상 클립은 이미지들의 집합으로서 0.1초 간격의 16 장 이미지로 구성된 1.5초 동안의 영상에 해당할 수 있다. 즉, 주행 상황 판단 장치는 차량 영상에 대한 서브샘플링(Sub-sampling)을 통해 시간축 상 정보를 축약할 수 있으며, 서브샘플링 주기와 클립의 길이는 빠른 속도에서의 사고와 느린 속도에서의 사고를 모두 탐지할 수 있는 범위 내에서 사고 현상의 진행속도를 기초로 결정될 수 있다. 또한, 주행 상황 판단 장치는 차량 영상에 관한 전처리 동작을 수행하여 데이터 용량을 줄이면서 효과적이 고 신뢰성 있는 학습 데이터를 생성할 수 있으며, 이를 통해 효율적인 인공지능 학습이 수행되도록 할 수 있다. 예를 들어, 주행 상황 판단 장치는 차량 영상으로부터 샘플링을 통해 추출된 각 이미지의 불필요한 부분을 제거하기 위한 잘라내기(Crop) 동작과 적절한 크기로 축소하여 영상 내 공간적 정보를 축약하는 크기조정 (Resizing) 동작을 수행할 수 있다. 도 6에서, 영상 클립은 현재 시점을 포함하여 최근 16장의 연속된 이미지들을 집합으로 묶어 생성될 수 있다. 즉, 주행 상황 판단 장치는 영상 클립을 학습 데이터로 사용하여 인공지능 학습을 수행할 수 있으며, 구축 된 인공지능 모델을 활용하여 현재 시점 기준 최근 1.5초 동안의 데이터로 사고발생 여부를 판별함으로써 단말 인공지능의 실시간성을 확보할 수 있다. 또한, 시간순으로 연속된 영상 클립들은 0.1초 단위로 중첩되어 생성될 수 있다. 즉, 주행 상황 판단 장치는 연속적인 추론을 통해 사고 현상의 정점을 놓치지 않고 탐지할 수 있 다. 이때, 사고 현상은 물리적으로 실재하는, 즉 관성을 갖는 객체들 간의 충돌 현상으로서, 객체들의 질량과 절대속도, 상대속도 등에 따라 시간을 두고 진행되는 현상에 해당할 수 있다. 도 7은 본 발명에 따른 차량 영상 분류 클래스의 일 실시예를 설명하는 도면이다. 도 7을 참조하면, 주행 상황 판단 장치는 학습 데이터로 사용되는 영상 클립을 사전 정의된 클래스(clas s)에 따라 분류할 수 있다. 특히, 주행 상황 판단 장치는 데이터 클래스를 세분화하여 일부 클래스 또는 천이 구간을 학습에서 제외하는 등의 기법을 통해 모델 성능을 개선할 수 있다. 예를 들어, 도 7에서, 주행 상황 판단 장치는 충격-비사고 클래스를 세분화하여 차량 영상을 분류할 수 있 다. 즉, 주행 상황 판단 장치는 비사고 요인으로 인한 충격의 사고 오판 억제를 위해 충격-비사고 클래스 (예: 과속방지턱, 인도 단차 등)를 세분화하여 분류할 수 있고, 이를 통해 상황 탐지 모델이 비사고 충격을 구 분하도록 할 수 있다. 또한, 주행 상황 판단 장치는 진동 흔들림 클래스를 세분화하여 차량 영상을 분류할 수 있다. 즉, 주행 상 황 판단 장치는 사고 모델링 범위의 명확화를 위해 기타 진동 흔들림(예: 도로 균열, 포트홀, 급정지 등) 을 세분화하여 분류할 수 있고, 이를 통해 상황 탐지 모델이 사고 충돌만을 탐지하도록 유도할 수 있다. 또한, 주행 상황 판단 장치는 회피 불가능 클래스를 세분화하여 차량 영상을 분류할 수 있다. 즉, 주행 상 황 판단 장치는 사고 발생 직전 회피 불가능 클래스를 별도로 세분화하여 분류할 수 있고, 학습 과정에서 제외시킴으로써 저속 충돌 및 근접 거리 충돌 감지의 정밀도를 향상시킬 수 있다. 도 8은 본 발명에 따른 차량 탐지 모델의 일 실시예를 설명하는 도면이다. 도 8을 참조하면, 주행 상황 판단 장치는 시공간심층신경망 기반의 상황 탐지 모델을 구축할 수 있고, 이 를 활용하여 차량 영상의 시공간적 변화 정보로부터 차량의 사고 상황을 탐지할 수 있다. 이때, 주행 상황 판단 장치는 합성곱 연산으로만 구성된 상황 탐지 모델을 구성할 수 있다. 예를 들어, 도 8에서, 주행 상황 판단 장치는 NPU(Neural Processing Unit)에 적합한 연산처리방식을 구 현하기 위하여 (Batch, Channel, Height, Width) 등의 4차원 정보를 처리하는 2차원 합성곱 연산만으로 구성된 상황 탐지 모델을 구축할 수 있다. 즉, 차량 영상과 같은 비디오 정보는 (Batch, Time, Channel, Height, Width)의 5차원 공간 정보로 표현될 수 있으며, 이를 합성곱 연산하기 위해서는 3차원 구조의 합성곱 연산이 사 용될 수 있다. 주행 상황 판단 장치는 3차원 구조의 합성곱 연산이 NPU에서 지원되지 않는다는 점을 고려 하여, 2차원 합성곱 연산만으로 비디오 정보를 처리하기 위한 모델 구조를 적용할 수 있다. 보다 구체적으로, 상황 탐지 모델은 복수의 시공간 합성곱 레이어(Spatio-Temporal Convolution Layer)들이 반 복적으로 연결된 구조로 구현될 수 있다. 하나의 시공간 합성곱 레이어는 영상 내 시공간적 변화 정보의 지역적 상관관계를 학습하기 위한 분할 합성곱 레이어(Separable Convolution Layer)와 영상 내 시공간적 변화 정보의 전역적 상관관계를 학습하기 위한 완전 연결 레이어(Fully Connected Layer)가 연결된 구조로 구현될 수 있다. 이때, 분할 합성곱 레이어와 완전 연결 레이어 각각은 2차원 합성곱 연산을 수행하는 레이어들로 구성될 수 있 다. 또한, 상황 탐지 모델은 영상 클립을 입력으로 수신하여 해당 영상 내 사고 확률을 출력으로 생성하도록 설계될 수 있다. 특히, 상황 탐지 모델은 영상 클립으로부터 추출된 특징 정보를 입력으로 수신할 수 있으며, 이를 위 하여 영상 클립(Frames)으로부터 특징 정보를 추출하는 특징 추출 신경망(예를 들어, ResNet18)에 연결될 수 있 다. 이때, 특징 추출 신경망은 영상 클립의 연속된 이미지 간 차영상(FrameDiffs)을 입력으로 수신할 수 있다. 도 9는 본 발명에 따른 중복 데이터 처리 과정의 일 실시예를 설명하는 도면이다. 도 9를 참조하면, 주행 상황 판단 장치는 영상 클립을 시간 순서에 따라 상황 탐지 모델의 입력으로 연속 하여 제공하는 경우 메모리에 저장된 이전 영상 클립 중 현재 영상 클립과 중복되는 이미지들의 특징 데이터를 반복하여 사용할 수 있다. 예를 들어, 도 9에서, 이전 영상 클립 Ct-1은 30fps로 촬영된 차량 영상에서 10fps로 샘플링된 이미지들의 집합 ({Frame t-16, ..., Frame t-1})이고, 현재 영상 클립 Ct는 현재 시점 기준 최근 이미지들의 집합({Frame t-15, ..., Frame t})에 해당할 수 있다. 즉, 이전 영상 클립 Ct-1과 현재 영상 클립 Ct은 총 15장의 이미지들({Frame t-15, ..., Frame t-1})이 중복될 수 있다(도 9의 중복 구간). 이때, 메모리에는 이전 영상 클립 Ct-1의 특징 정보들({Feature t-15, ..., Feature t-1})이 저장될 수 있고, 주행 상황 판단 장치는 현재 영상 클립 Ct의 처리를 위하여 메모리에 저장된 이전 영상 클립 Ct-1의 특징 정보들을 활용하여 중복적인 특징 추출 과정을 생략할 수 있다. 구체적으로, 주행 상황 판단 장치는 이전 영상 클립 Ct-1의 가장 오래된 이미지(Frame t-16)와 연관된 특징 정보(Feature t-15)를 메모리에서 삭제하고 현 재 영상 클립 Ct의 가장 최근(즉, 현재 시점)의 이미지(Frame t)와 연관된 특징 정보(Feature t)를 산출하여 메 모리에 삽입할 수 있다. 이에 따라, 메모리에 저장된 기존의 특징 정보들은 인덱스만 갱신될 수 있다. 이후, 주행 상황 판단 장치는 메모리에 저장된 현재 영상 클립 Ct의 특징 정보들({Feature t-14, ..., Feature t})을 상황 탐지 모델에 입력으로 제공하여 사고 상황을 탐지할 수 있으며, 이후 연속적으로 입력되는 차량 영상에 대해서도 사고 상황 탐지 동작을 반복적으로 수행할 수 있다. 한편, 주행 상황 판단 장치는 종단간처리(End-to-End) 방식으로 동작하도록 구현될 수 있다. 이에 따라, CPU의 관여 없이 DMA(Direct Memory Access)제어를 통해 카메라에서 입력된 차량 영상은 NPU로 직접 전달될 수 있고, NPU 내부에서 모든 처리가 일괄로 이루어진 후 사고 확률만이 CPU로 전달될 수 있다. 따라서, 주행 상황 판단 장치는 사고 확률을 기반으로 사고 상황 탐지 동작을 수행할 수 있으며, 비사고 상황에 대한 선별 및 제외를 통해 효율성을 확보하여 LTE 통신망을 활용한 실시간 사고 영상 전송 및 사고대응이 가능할 수 있다. 도 10은 본 발명에 따른 시인성에 기반한 신뢰도 조절의 다양한 실시예를 설명하는 도면이다. 도 10을 참조하면, 주행 상황 판단 장치는 주행상황 탐지부를 통해 시인성에 기반하여 시공간심층신 경망 기반의 상황 탐지 모델과 가속도 센서에 따른 충격 감지 기능 각각의 신뢰도를 조절할 수 있다. 예를 들어, 그림 (a)와 같이 시인성을 임계값(threshold) 기준으로 판단할 경우 임계값 이상이면 시공간심층신 경망 기반의 상황 탐지 모델만이 사용되어 주행 상황 탐지 동작이 수행될 수 있고, 임계값 이하이면 가속도 센 서에 따른 충격 감지 기능만이 사용되어 주행 상황 탐지 동작이 수행될 수 있다. 다른 예로서, 그림 (b) 또는 그림 (c)와 같이 특정 함수로 설정된 가중치가 적용되어 신뢰도 조절 동작이 수행 될 수도 있다. 즉, 주행 상황 판단 장치는 시공간심층신경망 기반의 상황 탐지 모델과 가속도 센서에 따른 충격 감지 기능의 결과에 대해 시인성에 연동되는 가중치를 적용할 수 있고, 이를 통해 두 개의 결과를 모두 반 영하여 차량이 주행 상황을 탐지하고 인식 결과를 생성할 수 있다. 상기에서는 본 발명의 바람직한 실시예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특 허 청구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0103620", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명에 따른 주행 상황 판단 시스템을 설명하는 도면이다. 도 2는 도 1의 주행 상황 판단 장치의 시스템 구성을 설명하는 도면이다. 도 3은 도 1의 주행 상황 판단 장치의 기능적 구성을 설명하는 도면이다. 도 4는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 방법을 설명하는 순 서도이다. 도 5는 본 발명에 따른 시공간심층신경망을 활용한 영상분석 기반의 실시간 주행 상황 판단 과정을 설명하는 도 면이다. 도 6은 본 발명에 따른 데이터 전처리 과정의 일 실시예를 설명하는 도면이다. 도 7은 본 발명에 따른 차량 영상 분류 클래스의 일 실시예를 설명하는 도면이다. 도 8은 본 발명에 따른 차량 탐지 모델의 일 실시예를 설명하는 도면이다. 도 9는 본 발명에 따른 중복 데이터 처리 과정의 일 실시예를 설명하는 도면이다. 도 10은 본 발명에 따른 시인성에 기반한 신뢰도 조절의 다양한 실시예를 설명하는 도면이다."}
