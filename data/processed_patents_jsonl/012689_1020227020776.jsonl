{"patent_id": "10-2022-7020776", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0104769", "출원번호": "10-2022-7020776", "발명의 명칭": "다수의 데이터 소스들을 사용한 스피치 전사", "출원인": "페이스북 테크놀로지스, 엘엘씨", "발명자": "청 빈센트 찰스"}}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "시스템에 있어서,복수의 화자(speaker)들과 연관된 오디오 데이터를 캡처하도록 구성된 오디오 캡처 시스템; 상기 복수의 화자들 중 하나 이상의 이미지들을 캡처하도록 구성된 이미지 캡처 시스템; 및 스피치 프로세싱 엔진(speech processing engine)을 포함하고, 상기 스피치 프로세싱 엔진은:상기 오디오 데이터에서 복수의 스피치 세그먼트들을 인식하고,상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 그리고 상기 이미지들에 기반하여, 상기 스피치 세그먼트와 연관된 화자를 식별하고,상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 상기 스피치 세그먼트와 연관된 상기 화자의표시를 포함하는 상기 복수의 스피치 세그먼트들의 전사(transcription)를 생성하기 위해, 상기 복수의 스피치세그먼트들 각각을 전사하고, 상기 전사에서 도출된 추가 데이터를 생성하기 위해 상기 전사를 분석하도록 구성되는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제 1 항에 있어서, 상기 복수의 스피치 세그먼트들을 인식하기 위해, 상기 스피치 프로세싱 엔진은 상기 이미지들에 기반하여, 상기 복수의 스피치 세그먼트들을 인식하도록 추가로 구성되고;바람직하게는, 상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 상기 화자를 식별하기 위해,상기 스피치 프로세싱 엔진은 상기 이미지들에서 하나 이상의 얼굴들을 검출하도록 추가로 구성되는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제 2 항에 있어서, 상기 스피치 프로세싱 엔진은 각각의 스피치 세그먼트와 연관된 상기 화자의 아이덴티티에기반하여, 하나 이상의 스피치 인식 모델들을 선택하도록 추가로 구성되는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제 3 항에 있어서, 상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 상기 화자를 식별하기 위해, 상기 스피치 프로세싱 엔진은 상기 이미지들에서 입술들이 움직이는 하나 이상의 얼굴들을 검출하도록 추가로 구성되는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제 1 항 내지 제 4 항 중 어느 한 항에 있어서, 상기 스피치 프로세싱 엔진은 외부 데이터에 액세스하도록 추가로 구성되고; 상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 상기 화자를 식별하기 위해, 상기 스피치 프로세싱 엔진은:상기 외부 데이터에 기반하여 상기 화자를 식별하도록 추가로 구성되고;바람직하게는 외부 데이터는 캘린더 정보 및 위치 정보 중 하나 이상을 포함하는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제 3 항 내지 제 5 항 중 어느 한 항에 있어서, 사용자에 의해 착용될 수 있는 HMD(head-mounted display)를더 포함하고, 상기 하나 이상의 스피치 인식 모델들은 상기 사용자에 대한 음성 인식 모델을 포함하고;바람직하게는, 상기 HMD는 인공 현실 콘텐츠를 출력하도록 구성되고, 상기 인공 현실 콘텐츠는 비디오 스트림공개특허 10-2022-0104769-3-및 오디오 스트림을 포함하는 가상 회의 애플리케이션을 포함하는 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제 3 항 내지 제 5 항 중 어느 한 항에 있어서, 사용자에 의해 착용될 수 있는 HMD(head-mounted display)를더 포함하고, 상기 스피치 프로세싱 엔진은 상기 복수의 스피치 세그먼트들의 속성들에 기반하여 상기 HMD의 사용자를 상기 복수의 스피치 세그먼트들의 화자로서 식별하도록 추가로 구성되는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제 1 항 내지 제 7 항 중 어느 한 항에 있어서, 상기 오디오 캡처링 시스템은 마이크로폰 어레이를 포함하고;바람직하게는 상기 추가 데이터는 상기 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 포함하는 오디오 스트림을 포함하는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제 1 항 내지 제 8 항 중 어느 한 항에 있어서, 상기 추가 데이터는 상기 전사에서 설명된 회의 또는 이벤트에대한 캘린더 초대(calendar invitation), 상기 전사에서 식별된 주제들과 관련된 정보, 또는 상기 전사에서 식별된 작업들을 포함하는 작업 목록 중 하나 이상을 포함하는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제 1 항 내지 제 9 항 어느 한 항에 있어서, 상기 추가 데이터는: 상기 화자에 의해 말해진 단어들의 수, 상기화자의 톤, 상기 화자에 의해 사용된 필러 단어(filler word)들에 관한 정보, 상기 화자가 말한 시간의 백분율,사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사의 요약 또는 상기 화자의 감정을 포함하는상기 전사에 관한 통계 중 적어도 하나를 포함하는, 시스템."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "방법에 있어서,복수의 화자들과 연관된 오디오 데이터를 캡처하는 단계; 상기 복수의 화자들 중 하나 이상의 이미지들을 캡처하는 단계; 상기 오디오 데이터에서 복수의 스피치 세그먼트들을 인식하는 단계;상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 그리고 상기 이미지들에 기반하여, 상기 스피치 세그먼트와 연관된 화자를 식별하는 단계;상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 상기 스피치 세그먼트와 연관된 상기 화자의표시를 포함하는 상기 복수의 스피치 세그먼트들의 전사를 생성하기 위해 상기 복수의 스피치 세그먼트들 각각을 전사하는 단계; 및 상기 전사에서 도출된 추가 데이터를 생성하기 위해 상기 전사를 분석하는 단계를 포함하는, 방법."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제 11 항에 있어서,외부 데이터에 액세스하는 단계; 및 상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 상기 외부 데이터에 기반하여 상기 화자를식별하는 단계를 더 포함하고;바람직하게는 상기 외부 데이터는 캘린더 정보 및 위치 정보 중 하나 이상을 포함하는, 방법."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제 11 항 또는 제 12 항에 있어서, 상기 추가 데이터는 상기 전사에서 설명된 회의 또는 이벤트에 대한 캘린더초대, 상기 전사에서 식별된 주제들과 관련된 정보, 또는 상기 전사에서 식별된 작업들을 포함하는 작업 목록공개특허 10-2022-0104769-4-중 하나 이상을 포함하는, 방법."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제 11 항 내지 제 13 항 어느 한 항에 있어서, 상기 추가 데이터는: 상기 화자에 의해 말해진 단어들의 수, 상기 화자의 톤, 상기 화자에 의해 사용된 필러 단어들에 관한 정보, 상기 화자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사의 요약 또는 상기 화자의 감정을 포함하는 상기 전사에 관한 통계 중 적어도 하나를 포함하는, 방법."}
{"patent_id": "10-2022-7020776", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "명령들을 포함하는 컴퓨터-판독가능 저장 매체에 있어서,상기 명령들은, 실행될 때:복수의 화자들과 연관된 오디오 데이터를 캡처하고; 상기 복수의 화자들 중 하나 이상의 이미지들을 캡처하고; 상기 오디오 데이터에서 복수의 스피치 세그먼트들을 인식하고;상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 그리고 상기 이미지들에 기반하여, 상기 스피치 세그먼트와 연관된 화자를 식별하고;상기 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 상기 스피치 세그먼트와 연관된 상기 화자의표시를 포함하는 상기 복수의 스피치 세그먼트들의 전사를 생성하기 위해 상기 복수의 스피치 세그먼트들 각각을 전사하고; 상기 전사에서 도출된 추가 데이터를 생성하기 위해 상기 전사를 분석하도록 컴퓨팅 시스템의 프로세싱 회로를구성하는, 컴퓨터-판독가능 저장 매체."}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시내용은 오디오, 이미지, 및 다른 데이터를 사용하여 스피치를 전사하는 것을 설명한다. 복수의 화자들과 연관된 오디오 데이터를 캡처하도록 구성된 오디오 캡처 시스템, 복수의 화자들 중 하나 이상의 이미지들을 캡처 하도록 구성된 이미지 캡처 시스템, 및 스피치 프로세싱 엔진을 포함하는 시스템이 설명된다. 스피치 프로세싱 엔진은 오디오 데이터에서 복수의 스피치 세그먼트를 인식하고, 복수의 스피치 세그먼트들의 각 스피치 세그먼트 에 대해 그리고 이미지들에 기반하여, 스피치 세그먼트와 연관된 화자를 식별하고, 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 스피치 세그먼트와 연관된 화자의 표시를 포함하는 복수의 스피치 세그먼트들의 전사를 생성하기 위해 복수의 스피치 세그먼트들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석하도록 구성될 수 있다."}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시내용은 일반적으로 스피치 전사 시스템(speech transcription system)들, 및 보다 구체적으로 다수의 사 람의 스피치를 전사하는 것에 관한 것이다."}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "스피치 인식은 점점 대중화되고 있고 텔레비전(TV)들, 컴퓨터들, 태블릿들, 스마트폰들 및 스피커들에 점점 더 추가되고 있다. 예를 들어, 많은 스마트 디바이스들은 사용자가 말하는 커맨드들이나 질문들에 기반하여 서비스 들을 수행할 수 있다. 이러한 디바이스들은 스피치 인식을 사용하여 캡처된 오디오에 기반하여 사용자의 커맨드 들 및 질문들을 식별하고 이어서 조치를 수행하거나 응답 정보를 식별한다. 일반적으로, 본 개시내용은 오디오, 이미지, 및 다른 데이터를 사용하여 스피치를 전사하기 위한 시스템 및 방 법을 설명한다. 일부 예들에서, 시스템은 스피치 인식, 화자 식별 및 시각적 패턴 인식 기법들을 결합하여 2명 이상의 사용자들 간의 상호작용의 전체 전사를 생성할 수 있다. 예를 들어, 이러한 시스템은 오디오 데이터 및 이미지 데이터를 캡처하고, 오디오 데이터에서 복수의 스피치 세그먼트를 인식하고, 이미지 데이터에 기반하여 각 스피치 세그먼트와 연관된 화자를 식별하고, 각 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 복수의 스피치 세그먼트들 각각을 전사할 수 있다. 일부 예들에서, 인공 지능(AI)/기계 학습(ML) 모델들은 하나 이상의 식별된 화자들의 스피치를 인식하고 전사하도록 훈련될 수 있다. 일부 예들에서, 시스템 은 스피치를 인식하고/하거나 이미지 데이터에서 입술들이 움직이는 하나 이상의 얼굴들을 검출하는 것에 기반 하여 화자들을 식별할 수 있다. 이러한 시스템은 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대, 전사"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "에서 식별된 주제와 관련된 정보, 전사에서 식별된 작업들을 포함하는 작업 목록, 요약, 통지들(예를 들어, 대 화에 참석하지 않은 사람에게, 대화에서 논의된 주제 또는 사람들에 관해 사용자에게), 통계(예를 들어, 화자가 말한 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어(filler word)들에 관한 정보, 각 화자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, \"필러들\"이 사용된 횟수, 화자의 볼륨 또는 화자의 감정 등)를 포함하는 전사로부터 추가 데이터를 생성하기 위해 전사를 추가로 분석할 수 있다. 일부 예들에서, 스피치 전사는 스피치, 대화들, 또는 상호작용들이 거의 또는 거의 겉보기에 거의 실시간 으로 발생하는 동안 수행된다. 다른 예들에서, 스피치 전사는 스피치, 대화들, 또는 상호작용들이 종료된 후에 수행된다. 일부 예들에서, 본원에 설명된 기법들은 머리 장착 디스플레이(HMD) 또는 이미지 데이터를 캡처하기 위한 이미 지 캡처 디바이스들(예를 들어, 카메라들) 및 오디오 데이터를 캡처하기 위한 오디오 캡처 디바이스들(예를 들 어, 마이크로폰들)을 갖는 컴퓨팅 디바이스에 의해 수행된다. 일부 예들에서, HMD 또는 컴퓨팅 디바이스는 사용 자들 간의 상호작용 동안 모든 사용자에 대해 캡처된 모든 스피치 세그먼트들을 전사할 수 있다. 다른 예들에서, HMD는 HMD를 착용한 사용자만을 위한 스피치 세그먼트들을 전사할 수 있고, HMD, 컴퓨팅 디바이스 및 /또는 전사 시스템은 선택적으로 다른 HMD들 및/또는 컴퓨팅 디바이스들로부터 수신된 개별 전사들을 결합할 수 있다. 본 발명의 제1 양태에 따르면, 복수의 화자들과 연관된 오디오 데이터를 캡처하도록 구성된 오디오 캡처 시스템; 복수의 화자들 중 하나 이상의 이미지들을 캡처하도록 구성된 이미지 캡처 시스템; 및 스피치 프로세싱 엔진을 포함하는 시스템이 제공되고, 스피치 프로세싱 엔진은: 오디오 데이터에서 복수의 스피치 세그먼트들을 인식하고, 복수의 스피치 세그먼트들 각각의 스피치 세그먼트에 대해 그리고 이미지들에 기반하여, 스피치 세그 먼트와 연관된 화자를 식별하고, 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 스피치 세그먼트 와 연관된 화자의 표시를 포함하는 복수의 스피치 세그먼트들의 전사를 생성하기 위해 복수의 스피치 세그먼트 들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석하도록 구성된다. 복수의 스피치 세그먼트들을 인식하기 위해, 스피치 프로세싱 엔진은 이미지에 기반하여, 복수의 스피치 세그먼 트들을 인식하도록 추가로 구성될 수 있다. 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 화자를 식별하기 위해, 스피치 프로세싱 엔진은 이 미지들에서 하나 이상의 얼굴들을 검출하도록 추가로 구성될 수 있다. 스피치 프로세싱 엔진은 각각의 스피치 세그먼트와 연관된 화자의 아이덴티티에 기반하여, 하나 이상의 스피치 인식 모델들을 선택하도록 추가로 구성될 수 있다. 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 화자를 식별하기 위해, 스피치 프로세싱 엔진은 입 술들이 움직이는 이미지들에서 하나 이상의 얼굴들을 검출하도록 추가로 구성될 수 있다. 스피치 프로세싱 엔진은 외부 데이터에 액세스하도록 추가로 구성될 수 있다. 복수의 스피치 세그먼트들의 각각 의 스피치 세그먼트에 대해 화자를 식별하기 위해, 스피치 프로세싱 엔진은 외부 데이터에 기반하여 화자를 식 별하도록 추가로 구성될 수 있다. 외부 데이터는 캘린더 정보 및 위치 정보 중 하나 이상을 포함할 수 있다. 시스템은 사용자에 의해 착용될 수 있는 HMD(head-mounted display)를 더 포함할 수 있다. 하나 이상의 스피치 인식 모델들은 사용자에 대한 스피치 인식 모델을 포함할 수 있다. 스피치 프로세싱 엔진은 복수의 스피치 세그 먼트들의 속성들에 기반하여 복수의 스피치 세그먼트들의 화자로서 HMD의 사용자를 식별하도록 추가로 구성될 수 있다. HMD는 인공 현실 콘텐츠를 출력하도록 구성될 수 있다. 인공 현실 콘텐츠는 비디오 스트림 및 오디오 스트림을 포함하는 가상 회의 애플리케이션을 포함할 수 있다. 오디오 캡처링 시스템은 마이크로폰 어레이를 포함할 수 있다. 추가 데이터는 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대, 전사에서 식별된 주제들과 관련된 정보, 및/또는 전사에서 식별된 작업들을 포함하는 작업 목록 중 하나 이상을 포함할 수 있다. 추가 데이터는: 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어들에 관한 정보, 화"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 3, "content": "자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사의 요약 또는 화 자의 감정을 포함하는 전사에 관한 통계 중 적어도 하나를 포함할 수 있다. 추가 데이터는 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 포함하는 오디오 스 트림을 포함할 수 있다. 방법은: 외부 데이터에 액세스하는 단계; 및 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 외부 데이터에 기반하여 화자를 식별하는 단계를 더 포함할 수 있다. 외부 데이터는 캘린더 정보 및 위치 정보 중 하 나 이상을 포함할 수 있다. 추가 데이터는 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대, 전사에서 식별된 주제들과 관련된 정보, 및/또는 전사에서 식별된 작업들을 포함하는 작업 목록 중 하나 이상을 포함할 수 있다. 추가 데이터는: 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어들에 관한 정보, 화"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 4, "content": "자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사의 요약 또는 화 자의 감정을 포함하는 전사에 관한 통계 중 적어도 하나를 포함할 수 있다. 본 발명의 제2 양태에 따르면, 복수의 화자들과 연관된 오디오 데이터를 캡처하는 단계; 복수의 화자들 중 하나 이상의 이미지들을 캡처하는 단계; 오디오 데이터에서 복수의 스피치 세그먼트를 인식하는 단계; 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 그리고 이미지들에 기반하여, 스피치 세그먼트와 연관된 화자를 식별하는 단계; 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 스피치 세그먼트와 연관된 화자의 표시를 포함하는 복수의 스피치 세그먼트들의 전사를 생성하기 위해 복수의 스피치 세그먼트들 각각을 전사하는 단계; 및 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석하는 단계를 포함하는 방법이 제공된다. 본 발명의 제3 양태에 따르면, 실행될 때: 복수의 화자들과 연관된 오디오 데이터를 캡처하고; 복수의 화자들 중 하나 이상의 이미지들을 캡처하고; 오디오 데이터에서 복수의 스피치 세그먼트들을 인식하고; 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해 그리고 이미지들에 기반하여, 스피치 세그먼트와 연관된 화자를 식별하고; 복수의 스피치 세그먼트들의 각각의 스피치 세그먼트에 대해, 스피치 세그먼트와 연관된 화자의 표시 를 포함하는 복수의 스피치 세그먼트들의 전사를 생성하기 위해 복수의 스피치 세그먼트들 각각을 전사하고; 전 사에서 도출된 추가 데이터를 생성하기 위해 전사를 분석하도록 컴퓨팅 시스템의 프로세싱 회로를 구성하는 명 령들을 포함하는 컴퓨터-판독가능 저장 매체가 제공된다. 이러한 기법들은 다양한 기술적 장점과 실용적인 애플리케이션들을 갖는다. 예를 들어, 본 개시내용의 하나 이 상의 양태들에 따른 기법들은 전사로부터 추가 데이터를 생성할 수 있는 스피치 전사 시스템을 제공할 수 있다. 추가 데이터를 자동으로 생성함으로써, 본 개시내용의 기법들에 따른 시스템은 커맨드 또는 질문이 있었거나 있 을 것이라고 시스템에게 신호하는 특정 단어들(예를 들어, \"깨우기\" 단어들)을 사용자 말할 필요 없이, 그리고 가능하게 특정 커맨드들 또는 명령들 없이 사용자에게 서비스들을 제공할 수 있다. 이것은 시스템과의 사용자 상호작용을 가능하게 하고, 사용자가 다른 사용자와 상호작용하는 벙법과의 상호작용들을 더 일관되게 하고, 이 에 의해 시스템과의 상호작용들이 더 자연스럽게 할 수 있다. 본 개시내용의 기법들의 하나 이상의 예들의 세부사항들은 아래의 첨부 도면들 및 설명에서 설명된다. 기법들의 다른 특징들, 목적들, 및 장점들은 설명, 도면들 및 청구항들로부터 자명하게 될 것이다."}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "item": 1, "content": "도 1a는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 시스템(10A)을 묘사하는 예시이다. 도 1a의 예 에서, 시스템(10A)은 HMD(Head Mounted Device)를 포함하는 인공 현실 시스템이다. 도시된 바와 같이, HMD는 일반적으로 사용자에 의해 착용되고 인공 현실 콘텐츠를 사용자에게 제시하기 위한 전자 디스플레이 및 광학 어셈블리를 포함한다. 또한, HMD는 HMD의 모션을 추적하기 위한 하나 이상 의 모션 센서들(예를 들어, 가속도계들), 주변 물리적 환경의 오디오 데이터를 캡처하기 위한 하나 이상의 오디 오 캡처 디바이스들(예를 들어, 마이크로폰들), 및 주변 물리적 환경의 이미지 데이터를 캡처하기 위한 하나 이 상의 이미지 캡처 디바이스들(예를 들어, 카메라들, 적외선(IR) 검출기들, 도플러 레이더, 라인 스캐너들)을 포 함한다. HMD는 임의의 형태의 컴퓨팅 리소스에 대응할 수 있는 전사 시스템과 네트워크를 통해 통신하는 것으로 예시된다. 예를 들어, 전사 시스템은 물리적 컴퓨팅 디바이스일 수 있거나 클라이언트 디 바이스들 및 다른 디바이스들 또는 시스템들에 서비스를 제공하는 클라우드 컴퓨팅 시스템, 서버 팜, 및/또는 서버 클러스터(또는 그 일부)의 구성요소일 수 있다. 따라서, 전사 시스템은 하나 이상의 물리적 컴퓨팅 디바이스들, 가상 컴퓨팅 디바이스들, 가상 기계들, 컨테이너들, 및/또는 다른 가상화된 컴퓨팅 디바이스를 나 타낼 수 있다. 일부 예시적인 구현들에서, HMD는 독립형 모바일 인공 현실 시스템으로서 동작한다. 네트워크는 인터넷일 수 있거나, 임의의 공공 또는 사설 통신 네트워크 또는 다른 네트워크를 포함하거나 나타낼 수 있다. 예를 들어, 네트워크는 셀룰러, Wi-Fi1®;, ZigBee, 블루투스, 근거리 통신(NFC), 위성, 기업, 서비스 제공자, 및/또는 컴퓨팅 시스템들, 서버들 및 컴퓨팅 디바이스들 사이에서 송신 데이터의 전송을 가능하게 하는 다른 유형의 네트워크일 수 있거나 이들을 포함할 수 있다. 클라이언트 디바이스들, 서버 디바이 스들, 또는 다른 디바이스들 중 하나 이상은 임의의 적합한 통신 기법들을 사용하여 네트워크를 통해 데이 터, 커맨드들, 제어 신호들, 및/또는 다른 정보를 송신 및 수신할 수 있다. 네트워크는 하나 이상의 네트 워크 허브들, 네트워크 스위치들, 네트워크 라우터들, 위성 접시들, 또는 임의의 다른 네트워크 장비를 포함할 수 있다. 이러한 디바이스들 또는 구성요소들은 동작가능하게 상호결합되어, 컴퓨터들, 디바이스들 또는 다른 구성요소들 사이(예를 들어, 하나 이상의 클라이언트 디바이스들 또는 시스템들과 하나 이상의 서버 디바이스들 또는 시스템들 사이)의 정보 교환을 제공할 수 있다. 도 1b에 예시된 각각의 디바이스들 또는 시스템들은 하나 이상의 네트워크 링크들을 사용하여 네트워크에 동작가능하게 결합될 수 있다. 일반적으로, 인공 현실 시스템(10A)은 사용자에게 디스플레이하기 위해 인공 현실 콘텐츠를 렌더링하 기 위해 현실-세계 3D 물리적 환경으로부터 캡처된 정보를 사용한다. 도 1a의 예에서, 사용자는 HMD 상에서 실행되는 인공 현실 애플리케이션에 의해 구성되고 렌더링된 인공 현실 콘텐츠를 본다. 인공 현실 콘텐츠(122A)는 가상 또는 비디오 회의 애플리케이션, 소셜 상호작용 애플리케이션, 움직임 명령 애플리케이션, 대안 세계 애플리케이션, 내비게이션 애플리케이션, 교육 애플리케이션, 게임 애플리케이션, 훈련 또는 시뮬레 이션 애플리케이션들, 증강 현실 애플리케이션, 가상 현실 애플리케이션, 또는 인공 현실을 구현하는 다른 유형 의 애플리케이션들에 따라 렌더링된 콘텐츠에 대응할 수 있다. 일부 예들에서, 인공 현실 콘텐츠는 현실- 세계 이미지와 가상 객체들, 예를 들어 혼합 현실 및/또는 증강 현실의 혼합을 포함할 수 있다. 동작 동안, 인공 현실 애플리케이션은 기준 프레임, 일반적으로 HMD의 보기 관점에 대한 포즈 정보를 추적 및 계산함으로써 사용자에게 디스플레이하기 위한 인공 현실 콘텐츠를 구성한다. HMD를 기준 프 레임으로 사용하고, HMD의 현재 추정된 포즈에 의해 결정된 현재 시야에 기반하여, 인공 현실 애플리 케이션은 일부 예들에서 적어도 부분적으로 사용자의 현실-세계 3D 물리적 환경에 오버레이될 수 있는 3D인공 현실 콘텐츠를 렌더링한다. 이 프로세스 동안, 인공 현실 애플리케이션은 움직임 정보 및 사용자 커맨드들 과 같은 HMD로부터 수신된 감지 데이터를 사용하고, 일부 예들에서는 사용자에 의한 모션과 같은, 현 실 세계 물리적 환경 내의 3D 정보를 캡처하기 위해 외부 카메라들과 같은 임의의 외부 센서들로부터의 데이터 를 사용한다. 감지된 데이터에 기반하여, 인공 현실 애플리케이션은 HMD의 참조 프레임에 대한 현재 포즈 를 결정하고, HMD의 현재 포즈에 따라 인공 현실 콘텐츠를 렌더링한다. 보다 구체적으로, 본원에 추가로 설명되는 바와 같이, HMD의 이미지 캡처 디바이스들은 이미지 캡처 디바 이스들의 시야 내에 있는 현실 세계 물리적 환경의 객체들을 나타내는 이미지 데이터를 캡처한다. 이 들 객체들은 사람들(101A 및 102A)을 포함할 수 있다. 시야는 일반적으로 HMD의 보기 관점에 대응한 다. 도 1a는 사용자가 사람(101A 및 102A)과 상호작용하는 장면을 묘사한다. 사람(101A 및 102A)들 둘 모두는 HMD의 시야에 있으므로, HMD가 사람들(101A 및 102A)의 오디오 데이터 및 이미지 데이터를 캡처 하게 한다. HMD(112A)는 사람들(101A 및 102A)에 각각 대응하는 사용자에게 인공 현실 콘텐츠의 사람 들(101B 및 102B)을 디스플레이할 수 있다. 일부 예들에서, 사람들(101B 및/또는 102B)은 각각 사람들(101A 및 102A)의 변경되지 않은 이미지들일 수 있다. 다른 예들에서, 사람(101B) 및/또는 사람(102B)은 사람(101B) 및/ 또는 사람(102B)에 대응하는 아바타(또는 임의의 다른 가상 표현)일 수 있다. 도 1a에 도시된 예에서, 사용자는 \"Hello Jack and Steve. How's it going?\"라고 말하고, 사람(101A)은 \"Where is Mary?\"라고 응답한다. 그 장면 동안, HMD는 이미지 데이터 및 오디오 데이터를 캡처하고 HMD(도시되지 않음)의 스피치 프로세싱 엔진은 캡처된 오디오 데이터에서 스피치 세그먼트들을 인식하고 각각의 스피치 세그먼트와 연관된 화자를 식별하도록 구성될 수 있다. 예를 들어, 스피치 프로세싱 엔진은 오디 오 데이터에서 \"Hello Jack and Steve. How's it going?\" 및 \"Where is Mary?\"라는 스피치 세그먼트들을 인식 할 수 있다. 일부 예들에서, 스피치 프로세싱 엔진은 개별 단어들(예를 들어, \"Hello,\" \"Jack,\" \"and,\" \"Steve\" 등) 또는 하나 이상의 단어들의 임의의 조합을 스피치 세그먼트로 인식할 수 있다. 일부 예들에서, 스 피치 프로세싱 엔진은 사용자에 대한 저장된 음성 인식 모델에 기반하여(예를 들어, 저장된 스피치 인식 모델과 유사한 스피치 세그먼트들의 속성들에 기반하여) 및/또는 사운드 강도(예를 들어, 볼륨)에 기반하여 사 용자를 \"Hello Jack and Steve. How's it going?\"의 화자로서 식별할 수 있다. 일부 예들에서, 스피치 프로세싱 엔진은 스피치 세그먼트들(예를 들어, 스피치 세그먼트의 시작 및 끝)을 인식 하고/하거나 화자를 식별하기 위해 이미지 데이터에서 입술들이 움직이는 얼굴들을 검출하도록 구성될 수 있다. 예를 들어, 스피치 프로세싱 엔진은 사람들(101A 및 102A)의 얼굴들을 검출할 수 있고 사람(101A)의 입이 스피치 세그먼트 \"Where is Mary?\"와 연관된 오디오를 캡처하는 동안 움직이고 있음을 검출할 수 있다. 이 정보 에 기반하여, 스피치 프로세싱 엔진은 그 스피치 세그먼트의 화자로 사람(101A)을 결정할 수 있다. 다른 예에서, 스피치 프로세싱 엔진은 사용자가 사람(101A)이 말하고 있는 동안(예를 들어, 사람(101A)의 입술 들이 움직이고 오디오 데이터가 캡처되는 동안) 사람(101A)에 초점을 맞추고 있기 때문에 사람이 화자인 것으로 결정할 수 있다. 일부 예들에서, 스피치 프로세싱 엔진은 또한 예를 들어 위치 정보(예를 들어, GPS 좌 표) 또는 캘린더 정보와 같은 다른 정보를 획득하여 화자들을 식별하거나 잠재적인 화자 모델들을 식별한다. 예 를 들어, 스피치 프로세싱 엔진은 사람들(101A 및 102A)을 식별하기 위해 캘린더 회의 정보를 사용할 수 있다. 스피치 프로세싱 엔진은 각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피 치 세그먼트들 각각을 전사할 수 있다. 스피치 프로세싱 엔진은 또한 전사를 분석하여 전사에서 도출된 추가 데 이터를 생성할 수 있다. 예를 들어, 도 1에 도시된 예에서, 스피치 프로세싱 엔진은 \"Where is Mary?\"라는 스피 치 세그먼트를 전사하고, 캘린더 정보를 분석하고, 메리(Mary)가 회의 초대를 거절했다고 결정할 수 있다. 이어 서, 스피치 프로세싱 엔진은 경고를 생성하고 인공 현실 콘텐츠에서 사용자에게 그 경고를 디스 플레이할 수 있다. 이러한 방식으로, 스피치 프로세싱 엔진은 사용자가 사람(101A)에 응답하는 것을 도울 수 있다. 스피치 프로세싱 엔진은 전사에 설명된 회의 또는 이벤트에 대한 캘린더 초대, 전사에서 식별된 주제와 관련된 정보, 또는 전사에서 식별된 작업들을 포함하는 작업 목록과 같은 다른 추가 데이터를 생성할 수 있다. 일부 예 들에서, 스피치 프로세싱 엔진은 통지들을 생성할 수 있다. 예를 들어, 프로세싱 엔진은 사람(101A)이 메리에 관해 묻고 있음을 나타내는 통지를 생성하고 그 통지를 메리에게 송신할 수 있다. 일부 예들에서, 스피치 프로 세싱 엔진은 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자 볼륨, 화자에 의해 사용된 필러 단어들에 관한"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "정보, 각 화자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사 요약또는 화자의 감정을 포함하는 전사에 관한 통계를 생성할 수 있다. 스피치 프로세싱 엔진은 또한 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 생성할 수 있다. 예를 들어, 스피치 프로세싱 엔 진은 하나 이상의 화자들의 음성들이 다른 음성(예를 들어, 만화 캐릭터의 음성 또는 유명인의 음성)으로 대체 되거나 오디오 또는 비디오 파일의 하나 이상의 스피치 세그먼트들을 대체한 오디오 또는 비디오 파일을 생성할 수 있다. 일부 예들에서, 스피치 프로세싱 엔진은 전사 시스템에 포함될 수 있다. 예를 들어, HMD는 오디오 및 이미지 데이터를 캡처하고 네트워크를 통해 오디오 및 이미지 데이터를 전사 시스템에 송신할 수 있 다. 전사 시스템은 오디오 데이터에서 스피치 세그먼트들을 인식하고, 스피치 세그먼트들 각각과 연관된 화자를 식별하고, 각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피치 세그 먼트들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석할 수 있다. 본원에 설명된 기법들 중 하나 이상은 다양한 기술적 장점들 및 실용적인 애플리케이션들을 가질 수 있다. 예를 들어, 본 개시내용의 하나 이상의 양태들에 따른 스피치 전사 시스템은 전사로부터 추가 데이터를 생성할 수 있 다. 추가 데이터를 자동으로 생성함으로써, 본 개시내용의 기법들에 따른 시스템은 사용자가 \"깨우기\"라는 단어 들을 말하거나 심지어 커맨드들 또는 명령들을 입력하지 않고 사용자에게 서비스들을 제공할 수 있다. 이것은 시스템과의 사용자 상호작용을 가능하게 하고, 사용자가 다른 사용자와 상호작용하는 벙법과의 상호작용들을 더 일관되게 하고, 이에 의해 시스템과의 상호작용들이 더 자연스럽게 할 수 있다. 도 1b는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 예시적인 시스템을 묘사하는 예시이다. 이 예 에서, 사용자는 이 예에서, 112A를 착용하고, 사람(101A)은 HMD(112B)를 착용하고, 사람(102A)은 112C를 착용하고 있다. 일부 예들에서, 사용자들(110, 101A, 및/또는 103A)은 동일한 물리적 환경 또는 상이한 물리적 환경에 있을 수 있다. 도 1b에서, HMD(112A)는 사용자에게 인공 현실 콘텐츠의 사람들(101B 및 102B)을 디스플레이할 수 있다. 이 예에서, 인공 현실 콘텐츠는 HMD(112B 및 112C) 각각으로부터의 비디오 스트림 및 오디오 스트림을 포함하는 가상 회의 애플리케이션을 포함한다. 일부 예들에서, 사람들(101B 및/또는 102B)은 각각 사람들(101A 및 102A)의 변경되지 않은 이미지들일 수 있다. 다른 예들에서, 사람(101B) 및/또는 사람(102B)은 사람(101B) 및/또는 사람(102B)에 대응하는 아바타(또는 임의의 다른 가상 표현)일 수 있다. 도 1b에 도시된 예에서, HMD들(112A, 112B, 및 112C)(집합적으로 \"HMD들\")은 서로 무선으로(예를 들어, 직접적으로 또는 네트워크를 통해) 통신한다. HMD들 각각은 스피치 프로세싱 엔진(도시되지 않음)을 포함할 수 있다. 일부 예들에서, HMD들 각각은 도 1a의 HMD와 실질적으로 동일한 방식으로 동작할 수 있다. 일부 예들에서, HMD(112A)는 사용자에 대응하는 제1 스피치 인식 모델을 저장할 수 있고, HMD(112 B)는 사용자(101A)에 대응하는 제2 스피치 인식 모델을 저장할 수 있고, HMD(112C)는 사용자(102A)에 대응하는 제3 스피치 인식 모델을 저장할 수 있다. 일부 예들에서, HMD들 각각은 제1, 제2, 및 제3 스피치 인식 모 델들의 사본들을 공유하고 저장할 수 있다. 일부 예들에서, HMD들 각각은 오디오 데이터 및/또는 이미지 데이터를 획득한다. 예를 들어, HMD들 각각은 물리적 환경으로부터 오디오 데이터 및 이미지 데이터를 캡처하고/하거나 다른 HMD들로부터 오디오 데이터 및/또는 이미지 데이터를 획득할 수 있다. 일부 예들에서, 각각의 HMD는 HMD를 착용한 사용자에 대 응하는 스피치 세그먼트들을 전사할 수 있다. 예를 들어, HMD(112A)는 사용자에 대응하는 하나 이상의 스 피치 세그먼트만을 전사할 수 있고, HMD(112B)는 사용자(101A)에 대응하는 하나 이상의 스피치 세그먼트만을 전 사할 수 있고, HMD(112C)는 사용자(102A)에 대응하는 하나 이상의 스피치 세그먼트만을 전사할 수 있다. 예를 들어, 그러한 예에서, HMD(112A)는 그 물리적 환경으로부터 오디오 데이터 및/또는 이미지 데이터를 캡처하고, 오디오 데이터에서 스피치 세그먼트들을 인식하고, 사용자에 대응하는 스피치 세그먼트들을 식별하고(예를 들어, 사용자에 대해 저장된 스피치 인식 모델에 기반하여), 사용자에 대응하는 스피치 세그먼트들 각각을 전사한다. 각각의 HMD들은 그들의 개별적인 전사들을 전사 시스템으로 송신할 것이다. 시스템 은 개별 전사들을 결합하여 완전한 전사를 생성하고 전체 전사를 분석하여 전체 전사에서 도출된 추가 데 이터를 생성한다. 이런 방식으로, HMD들 각각은 다른 사용자들을 위한 스피치 인식 모델을 저장할 필요가 없다. 또한, 대응하는 사용자로부터의 스피치를 전사하는 각각의 HMD는 전사 및/또는 화자 식별 정확도를 개선할 수 있다. 다른 예들에서, HMD들 각각은 오디오 및 이미지 데이터를 캡처하고 오디오 및 이미지 데이터를 네트워크 를 통해 (예를 들어, 오디오 및 비디오 스트림들로) 전사 시스템에 송신할 수 있다. 전사 시스템 은 오디오 데이터에서 스피치 세그먼트들을 인식하고, 스피치 세그먼트들 각각과 연관된 화자를 식별하고,각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피치 세그먼트들 각각을 전 사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석할 수 있다. 도 1c는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 예시적인 시스템(10B)을 묘사하는 예시이다. 이 예에서, 사용자들(110, 101, 및 102)은 동일한 물리적 환경에 있고 컴퓨팅 디바이스는 오디오 및/또는 이미지 데이터를 캡처한다. 다른 예들에서, 상이한 물리적 환경에 위치된 하나 이상의 다른 사용자들은 컴퓨팅 디바이스에 의해 사용자들(110, 101, 및 102)과의 상호작용의 일부일 수 있다. 도 1c의 컴퓨팅 디바이스 는 모바일 폰, 태블릿, 스마트 워치, 게임 콘솔, 워크스테이션, 데스크톱 컴퓨터, 랩톱, 어시스턴트 디바 이스, 특수-목적 태블릿톱 디바이스, 또는 다른 컴퓨팅 디바이스에 대응할 수 있는 단일 컴퓨팅 디바이스로서 도시되어 있다. 다른 예들에서, 컴퓨팅 디바이스는 복수의 컴퓨팅 디바이스들에 걸쳐 분산될 수 있다. 일부 예들에서, 컴퓨팅 디바이스는 도 1a 및 도 1b의 HMD들을 참조하여 위에서 설명된 것과 유사한 전사 동작들을 수행할 수 있다. 예를 들어, 컴퓨팅 디바이스(도시되지 않음)의 스피치 프로세싱 엔진은 오 디오 데이터에서 스피치 세그먼트들을 인식하고, 스피치 세그먼트들 각각과 연관된 화자를 식별하고, 각각의 스 피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피치 세그먼트들 각각을 전사하고, 전 사에서 도출된 추가 데이터를 생성하기 위해 전사를 분석한다. 다른 예에서, 컴퓨팅 디바이스는 오디오 및 /또는 이미지 데이터를 캡처하고, 오디오 및/또는 이미지 데이터를 전사 시스템으로 송신하고, 이어서 전사 시 스템의 스피치 프로세싱 엔진은 오디오 데이터에서 스피치 세그먼트들을 인식하고, 스피치 세그먼트들 각 각과 연관된 화자를 식별하고, 각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피치 세그먼트들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석한다. 컴퓨팅 디바이스가 원격 사용자들 및/또는 상이한 물리적 환경들의 사용자들을 포함하는 상호작용을 촉진 하는 예들에서, 컴퓨팅 디바이스는 오디오 스트림(들)에서 스피치 세그먼트들을 인식하기 위해 원격 사용 자들에 대응하는 디바이스들로부터의 오디오 정보 및 이미지 또는 비디오 정보의 임의의 표시들(예를 들어, 오 디오 및/또는 비디오 스트림들)을 사용하고, 오디오 스트림(들)에서 스피치 세그먼트들 각각과 연관된 화자(예 를 들어, 원격 사용자)를 식별하고, 각각의 스피치 세그먼트와 연관된 화자(원격 화자들을 포함)의 표시를 포함 하는 전사를 생성하기 위해 스피치 세그먼트들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위 해 전사를 분석할 수 있다. 도 2a는 본 개시내용의 하나 이상의 기법들에 따라 동작하도록 구성된 예시적인 HMD를 묘사하는 예시이다. 도 2a의 HMD는 도 1a의 HMD 또는 도 1b의 HMD들(112A, 112B, 및 112C)의 예일 수 있다. HMD는 본원에 설명된 기법들을 구현하도록 구성된 독립형 모바일 인공 부동산 시스템으로서 동작할 수 있거나 도 1a, 도 1b의 시스템(10A)과 같은 시스템의 일부일 수 있다. 이 예에서, HMD는 전면 강성체 및 HMD를 사용자에게 고정하기 위한 밴드를 포함한다. 또한, HMD(11 2)는 인공 현실 콘텐츠를 사용자에게 제시하도록 구성된 내부-지향 전자 디스플레이를 포함한다. 전자 디 스플레이는 액정 디스플레이들(LCD), 양자점 디스플레이, 도트 매트릭스 디스플레이, 발광 다이오드(LED) 디스플레이들, 유기 발광 다이오드(OLED) 디스플레이들, 음극선관(CRT) 디스플레이들, 전자-잉크, 또는 흑백, 컬러 또는 시각적 출력을 생성할 수 있는 임의의 다른 유형의 디스플레이 같은 임의의 적합한 디스플레이 기술 일 수 있다. 일부 예들에서, 전자 디스플레이는 사용자의 각각의 눈에 별도의 이미지들을 제공하기 위한 입체 디스플레이이다. 일부 예들에서, HMD의 전면 강성체에 관련한 디스플레이의 알려진 배향 및 포지션은 HMD 및 사용자의 현재 보기 관점에 따라 인공 현실 콘텐츠를 렌더링하기 위해 HMD의 포지션 및 배향 을 추적할 때, 로컬 원점으로 또한 지칭되는 참조 프레임으로 사용된다. 참조 프레임은 또한 HMD의 포지션 및 배향을 추적하는 데 사용될 수 있다. 다른 예들에서, HMD는 안경 또는 고글 같은 다른 착용가능 머리 장착 디스플레이들의 형태를 취할 수 있다. 도 2a에 추가로 도시된 바와 같이, 이 예에서 HMD는 HMD의 현재 가속도를 나타내는 데이터를 출력하 는 하나 이상의 가속도계들(또한 관성 측정 유닛들 또는 \"IMU들\"로 지칭됨), HMD의 위치를 나타내는 데이 터를 출력하는 GPS 센서들, 다양한 객체들로부터 HMD의 거리들을 나타내는 데이터를 출력하는 레이더 또는 소나 센서들, 또는 물리적 환경 내에서 HMD 또는 다른 객체들의 위치 또는 배향의 표시들을 제공하는 다른 센서들과 같은 하나 이상의 모션 센서들을 더 포함한다. 또한, HMD는 원시 이미지 및 오디오 데이터 를 각각 캡처하도록 구성된 통합된 이미지 캡처 디바이스들(208A 및 208B)(총칭하여 \"이미지 캡처 시스템(20 8)\"으로, 이는 임의의 수의 이미지 캡처 디바이스들을 포함할 수 있음)(예를 들어, 비디오 카메라들, 스틸 카메 라들, IR 스캐너들, UV 스캐너들, 레이저 스캐너들, 도플러 레이더 스캐너들, 깊이 스캐너들) 및 오디오 캡처시스템(예를 들어, 마이크로폰들)을 포함할 수 있다. 일부 양태들에서, 이미지 캡처 시스템은 가시 스펙트럼 및 전자기 스펙트럼(예를 들어, IR 광)의 비가시 스펙트럼으로부터 이미지 데이터를 캡처할 수 있다. 이미지 캡처 시스템은 가시 스펙트럼으로부터 이미지 데이터를 캡처하는 하나 이상의 이미지 캡처 디바이 스들 및 비가시 스펙트럼으로부터 이미지 데이터를 캡처하는 하나 이상의 개별 이미지 캡처 디바이스들을 포함 할 수 있거나, 이들은 동일한 하나 이상의 이미지 캡처 디바이스들에 결합될 수 있다. 더 구체적으로, 이미지 캡처 시스템은 일반적으로 HMD의 보기 관점에 대응하는 이미지 캡처 시스템의 시야 내에 있는 물리적 환경의 객체들을 나타내는 이미지 데이터를 캡처하고, 오디오 캡처 시스템은 HMD 주변 (예를 들어, 오디오 캡처 디바이스들의 360도 범위 내)의 오디오 데이터를 캡처한다. 일부 예들에서, 오디오 캡 처 시스템은 HMD에 대한 오디오 소스의 방향성에 관한 정보를 캡처할 수 있는 마이크로폰 어레이를 포함할 수 있다. HMD는 내부 전원 및 감지 데이터를 프로세싱하고 인공-현실 콘텐츠를 디스플레이에 제시하기 위한 프로그래밍가능 동작들을 실행하기 위한 운영 환경을 제공하기 위해 하나 이상의 프로세서들, 메 모리 및 하드웨어를 갖는 하나 이상의 인쇄-회로 기판들을 포함할 수 있는 내부 제어 유닛을 포함한다. 일 예에서, 본원에 설명된 기법들에 따라, 제어 유닛은 오디오 캡처 시스템으로 캡처된 오디오 데이 터에서 스피치 세그먼트들을 인식하고, 각각의 스피치 세그먼트와 연관된 화자를 식별하고, 각각의 스피치 세그 먼트와 연관된 화자의 표시를 포함하는 복수의 스피치 세그먼트들의 전사를 생성하기 위해 스피치 세그먼트들 각각을 전사하고, 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석하도록 구성된다. 일부 예들에서, 제어 유닛은 오디오 데이터 및/또는 이미지 데이터가 네트워크를 통해 (예를 들어, 오디오 데이터 및/또는 이미지 데이터가 캡처되는 것과 거의 실시간으로, 또는 겉보기에 거의 실시간으로, 또는 상호작 용이 완료된 후) 전사 시스템에 송신되게 한다. 도 2b는 본 개시내용의 기법들에 따른 예시적인 HMD를 묘사하는 예시이다. 도 2b에 도시된 바와 같이, HMD는 안경 형태를 취할 수 있다. 도 2a의 HMD는 도 1a 및 도 1b의 HMD 중 임의의 것의 예일 수 있다. HMD는 도 1a, 도 1b의 시스템(10A)과 같은 시스템의 일부일 수 있거나, 본원에 설명된 기법들을 구현하도록 구성된 독립형 모바일 인공 현실 시스템으로서 동작할 수 있다. 이 예에서, HMD는 HMD가 사용자의 코 및 사용자에게 HMD를 고정하기 위해 사용자의 귀 위로 연 장되는 관자놀이(또는 \"팔\")에 놓이게 하도록 하는 브리지를 포함하는 전면 프레임을 포함하는 안경이다. 또한, 도 2b의 HMD는 인공 현실 콘텐츠를 사용자에게 제시하도록 구성된 내부-지향 전자 디스플레이들(203A 및 203B)(집합적으로, \"전자 디스플레이들\")을 포함한다. 전자 디스플레이들은 액정 디스플레이들(LCD), 양자점 디스플레이, 도트 매트릭스 디스플레이, 발광 다이오드(LED) 디스플레이들, 유기 발광 다이오드(OLED) 디스플레이들, 음극선관(CRT) 디스플레이들, 전자-잉크, 또는 흑백, 컬러 또는 시각적 출력을 생성할 수 있는 임의의 다른 유형의 디스플레이 같은 임의의 적합한 디스플레이 기술일 수 있다. 도 2b에 도시된 일부 예들에서, 전자 디스플레이들은 사용자의 각각의 눈에 별도의 이미지들을 제공하기 위한 입체 디스플레이 를 형성한다. 일부 예들에서, HMD의 전면 프레임에 관련한 디스플레이의 알려진 배향 및 포지션은 HMD 및 사용자의 현재 보기 관점에 따라 인공 현실 콘텐츠를 렌더링하기 위해 HMD의 포지션 및 배향 을 추적할 때, 로컬 원점으로 또한 지칭되는 참조 프레임으로 사용된다. 도 2b에 추가로 도시된 바와 같이, 이 예에서 HMD는 HMD의 현재 가속도를 나타내는 데이터를 출력하 는 하나 이상의 가속도계들(또한 관성 측정 유닛들 또는 \"IMU들\"로 지칭됨), HMD의 위치를 나타내는 데이 터를 출력하는 GPS 센서들, 다양한 객체들로부터 HMD의 거리들을 나타내는 데이터를 출력하는 레이더 또는 소나 센서들, 또는 물리적 환경 내에서 HMD 또는 다른 객체들의 위치 또는 배향의 표시들을 제공하는 다른 센서들과 같은 하나 이상의 모션 센서들을 더 포함한다. 또한, HMD는 이미지 오디오 데이터를 각각 캡처하도록 구성된 통합된 이미지 캡처 디바이스들(208A 및 208B)(총칭하여 \"이미지 캡처 시스템\")(예를 들어, 비디오 카메라들, 스틸 카메라들, IR 스캐너들, UV 스캐너들, 레이저 스캐너들, 도플러 레이더 스캐너들, 깊이 스캐너들) 및 오디오 캡처 시스템(예를 들어, 마이크로폰들)을 포함할 수 있다. 일부 양태들에서, 이 미지 캡처 시스템은 가시 스펙트럼 및 전자기 스펙트럼(예를 들어, IR 광)의 비가시 스펙트럼으로부터 이 미지 데이터를 캡처할 수 있다. 이미지 캡처 시스템은 가시 스펙트럼으로부터 이미지 데이터를 캡처하는 하나 이상의 이미지 캡처 디바이스들 및 비가시 스펙트럼으로부터 이미지 데이터를 캡처하는 하나 이상의 개별 이미지 캡처 디바이스들을 포함할 수 있거나, 이들은 동일한 하나 이상의 이미지 캡처 디바이스들에 결합될 수 있다. 더 구체적으로, 이미지 캡처 시스템은 일반적으로 HMD의 보기 관점에 대응하는 이미지 캡처 시 스템의 시야 내에 있는 물리적 환경의 객체들을 나타내는 이미지 데이터를 캡처하고, 오디오 캡처 시 스템은 HMD 주변(예를 들어, 오디오 캡처 디바이스들의 360도 범위 내)의 오디오 데이터를 캡처한다.HMD는 내부 전원 및 감지 데이터를 프로세싱하고 인공-현실 콘텐츠를 디스플레이에 제시하기 위한 프 로그래밍가능 동작들을 실행하기 위한 운영 환경을 제공하기 위해 하나 이상의 프로세서들, 메모리 및 하드웨어 를 갖는 하나 이상의 인쇄-회로 기판들을 포함할 수 있는 내부 제어 유닛을 포함한다. 본원에 설명된 기법 들에 따라, 도 2b의 제어 유닛은 도 2a의 제어 유닛과 유사하게 동작하도록 구성된다. 도 3은 본 개시내용의 기법들에 따른, 도 1a, 도 1b의 인공 현실 시스템들의 HMD의 예시적인 인스턴스에 의해 스피치 전사가 수행되는 예를 묘사하는 블록도이다. 도 3의 예에서, HMD는 본원에 설명된 기법들에 따라 이미지 및 오디오 데이터 캡처, 화자 식별, 전사, 및 분석 동작들을 수행한다. 이 예에서, HMD는 하나 이상의 프로세서들 및 메모리를 포함하고, 이는 일부 예들에서 예를 들 어 내장형 실시간 멀티태스킹 운영 체제 또는 다른 유형의 운영 체제일 수 있는 운영 체제를 실행하기 위 한 컴퓨터 플랫폼을 제공한다. 차례로, 운영 체제는 하나 이상의 소프트웨어 구성요소들을 실행하기 위한 멀티태스킹 운영 환경을 제공한다. 프로세서들은 디스플레이 디바이스들, 이미지 캡처 디바이스들, 다른 HMD들 등과 같은 다른 디바이스들과 통신하기 위한 하나 이상의 I/O 인터페이스들을 제공하는 I/O 인터페 이스들에 결합된다. 또한, 하나 이상의 I/O 인터페이스들은 네트워크와 같은 네트워크와 통신하 기 위한 하나 이상의 유선 또는 무선 네트워크 인터페이스 제어기(NIC)들을 포함할 수 있다. 또한, 프로세서(들)는 전자 디스플레이, 모션 센서들, 및 이미지 캡처 시스템, 및 오디오 캡처 시스템에 결합된다. 일부 예들에서, 프로세서들 및 메모리는 별개의 이산 구성요소들일 수 있다. 다른 예들에서, 메모리는 단일 집적 회로 내에서 프로세서들과 함께 배치된 온-칩 메모리일 수 있다. 이미지 캡처 시스템 및 오디오 캡처 시스템은 각각 이미지 데이터 및 오디오 데이터를 획득하 도록 구성된다. 일반적으로, 애플리케이션 엔진은 인공 현실 애플리케이션, 예를 들어 전사 애플리케이션, 음성 어시스턴 트 애플리케이션, 가상 회의 애플리케이션, 게이밍 애플리케이션, 내비게이션 애플리케이션, 교육 애플리케이션, 훈련 또는 시뮬레이션 애플리케이션들 등을 제공하고 제시하는 기능을 포함한다. 애플리케이션 엔진은 예를 들어 HMD 상에서 인공 현실 애플리케이션을 구현하기 위한 하나 이상의 소프트웨어 패키 지들, 소프트웨어 라이브러리들, 하드웨어 드라이버들, 및/또는 애플리케이션 프로그램 인터페이스(API)들을 포 함할 수 있다. 애플리케이션 엔진에 의한 제어에 응답하여, 렌더링 엔진은 HMD의 애플리케이션 엔진에 의해 사용자에게 디스플레이하기 위한 3D 인공 현실 콘텐츠를 생성한다. 애플리케이션 엔진 및 렌더링 엔진은 포즈 추적기에 의해 결정된 바와 같이, 참조 프레임, 일반 적으로 HMD의 보기 관점 내의 HMD에 대한 현재 포즈 정보에 따라 사용자에게 디스플레이하기 위 한 인공 콘텐츠를 구성한다. 현재 보기 관점에 기반하여, 렌더링 엔진은 일부 경우들에서 사용자의 현실-세계 3D 환경에 적어도 부분적으로 오버레이될 수 있는 3D 인공 현실 콘텐츠를 구성한다. 이 프로세스 동 안, 포즈 추적기는 사용자에 의한 모션 및/또는 사용자에 대한 특징 추적 정보와 같은 현실 세 계 환경 내의 3D 정보를 캡처하기 위해 HMD 및 사용자 커맨드들로부터 수신된 감지 데이터에 대해 동작한 다. 일부 예들에서, 애플리케이션 엔진 및 렌더링 엔진은 본 개시내용의 기법들에 따라 전사 애플리 케이션 또는 음성 어시스턴트 애플리케이션을 위한 하나 이상의 사용자 인터페이스들을 디스플레이하기 위해 생 성 및 렌더링할 수 있다. 예를 들어, 애플리케이션 엔진 및 렌더링 엔진은 전사 및/또는 추가 데이터 를 디스플레이하기 위한 사용자 인터페이스를 디스플레이하기 위해 생성하고 렌더링할 수 있다. HMD의 소프트웨어 애플리케이션들은 전사 애플리케이션을 포함하는 전체 인공 현실 애플리케이션을 제공하도록 동작한다. 이 예에서, 소프트웨어 애플리케이션은 렌더링 엔진, 애플리케이션 엔진, 포즈 추적기, 스피치 프로세싱 엔진, 이미지 데이터, 오디오 데이터, 화자 모델들, 및 전사를 포함한다. 일부 예들에서, HMD는 위치 정보, 사용자에 대한 캘린더 이벤트 데이터(예를 들 어, 초대된 사람들, 확인된 사람들, 회의 주제) 등을 포함하는 다른 데이터를 (예를 들어, 메모리에) 저장 할 수 있다. 일부 예들에서, 이미지 데이터, 오디오 데이터, 화자 모델들, 및/또는 전사는 저장소 또는 캐시를 나타낼 수 있다. 스피치 프로세싱 엔진은 본 개시내용의 기법들에 따라 오디오 데이터에서 스피치를 전사하는 것과 관 련된 기능들을 수행하고 전사를 분석한다. 일부 예들에서, 스피치 프로세싱 엔진은 스피치 인식 엔진 , 화자 식별자, 스피치 전사기, 및 음성 어시스턴트 애플리케이션을 포함한다. 스피치 인식 엔진은 오디오 데이터에서 하나 이상의 스피치 세그먼트를 인식하는 것과 관련된 기능을 수행한다. 일부 예들에서, 스피치 인식 엔진은 오디오 데이터(예를 들어, 원시 아날로그 데이터와 별개)의 하나 이상의 스피치 세그먼트들을 저장한다. 스피치 세그먼트는 하나 이상의 말해진 단어들을 포함할 수 있다. 예를 들어, 스피치 세그먼트는 단일 단어들, 2개 이상의 단어들 또는 심지어 구들 또는 완전한 문장들일 수 있다. 일부 예들에서, 스피치 인식 엔진은 오디오 데이터에서 하나 이상의 스피치 세그먼트들을 인식하기 위해 임의의 스피치 인식 기법들을 사용한다. 예를 들어, 오디오 데이터는 아날로그 데이터를 포 함할 수 있고 스피치 인식 엔진은 아날로그-투-디지털 변환기(ADC)를 사용하여 아날로그 데이터를 디지털 데이터로 변환하고, 디지털화된 오디오 데이터에서 노이즈를 필터링하고, 하나 이상의 통계적 모델들(예를 들어, 은닉 마르코프 모델(Hidden Markov Model) 또는 신경망)을 필터링된 디지털화된 오디오 데이터에 적용하 여 하나 이상의 스피치 세그먼트들을 인식할 수 있다. 일부 예들에서, 스피치 인식 엔진은 하나 이상의 특 정 사용자(예를 들어, 도 1a-도 1c의 사용자)에 대한 스피치를 인식하도록 훈련된 인공 지능(AI)/기계 학 습(ML) 모델을 적용할 수 있다. 일부 예들에서, AI/ML 모델은 스피치 인식 결정들을 조정하기 위해 사용자로부 터 훈련 피드백을 수신할 수 있다. 일부 예들에서, 스피치 인식 엔진은 이미지 데이터에 기반하여 오 디오 데이터에서 하나 이상의 스피치 세그먼트들을 인식할 수 있다. 예를 들어, 스피치 인식 엔진은 스피치 세그먼트들(예를 들어, 스피치 세그먼트의 시작 및 끝)을 인식하기 위해 이미지 데이터에서 입술들이 움 직이는 얼굴들을 검출하도록 구성될 수 있다. 화자 식별자는 스피치 인식 엔진에 의해 인식되는 하나 이상의 스피치 세그먼트들 각각과 연관된 화 자를 식별하는 것과 관련된 기능들을 수행한다. 예를 들어, 화자 식별자는 화자 또는 잠재적 화자들을 식 별하기 위해 이미지 데이터에서 입술들이 움직이는 얼굴들을 검출하도록 구성될 수 있다. 다른 예에서, 오 디오 캡처 시스템은 HMD에 대한 오디오 소스의 방향성에 관한 정보를 캡처할 수 있는 마이크로폰 어 레이를 포함할 수 있고, 화자 식별자는 그 방향성 정보 및 이미지 데이터에 기반하여 화자 또는 잠재 적인 화자들을 식별할 수 있다(예를 들어, 화자 식별자는 \"Where is Mary?\"라는 스피치 세그먼트에 관한 방향성 정보에 기반하여 도 1의 사람(101A)을 식별할 수 있음). 또 다른 예에서, 화자 식별자는 사용자가 누구에게 초점을 맞추는지에 기반하여(예를 들어, HMD의 시야에 기반하여) 화자를 식별할 것이다. 일부 예 들에서, 화자 식별자는 각각의 스피치 세그먼트에 대한 해시 값 또는 임베딩 값을 결정할 수 있고, (예를 들어, 화자 모델들로부터) 잠재적 화자 모델들을 획득하고, 해시 값을 잠재적 화자 모델들과 비교하고, 해 시 값에 가장 가까운 화자 모델을 식별할 수 있다. 화자 식별자는 외부 데이터, 이미지 데이터(예를 들어, 입술들이 움직이는 검출된 얼굴들에 기반), 및/또는 사용자 입력에 기반하여 잠재적인 화자 모델들을 식 별할 수 있다. 예를 들어, 화자 식별자는 캘린더 정보(예를 들어, 확인된 또는 잠재적인 회의 초대 대상자 들에 관한 정보), 이미지 데이터에서 식별된 하나 이상의 얼굴들, 위치 정보(예를 들어, HMD에 관하 여 다른 사람들과 연관된 사람들 또는 디바이스들의 근접 정보)에 기반하여, 및/또는 사용자 입력을 통해 선택 된 잠재적인 화자 모델들에 기반하여 잠재적인 화자들을 식별할 수 있다. 일부 예들에서, 스피치 세그먼트에 대 한 해시 값과 가장 가까운 화자 모델들 간의 차이가 임계 차이 이상이면, 화자 식별자는 해시 값에 기반하 여 새로운 화자 모델을 생성하고 새로운 화자 모델을 스피치 세그먼트에 연관시킬 수 있다. 스피치 세그먼트에 대한 해시 값과 가장 가까운 화자 모델들 간의 차이가 임계 차이 미만인 경우, 화자 식별자는 가장 가까운 화자 모델과 연관된 화자를 스피치 세그먼트의 화자로 식별할 수 있다. 일부 예들에서, 화자 모델들은 상 이한 화자들에 대한 해시 값들(또는 다른 음성 속성들)을 포함할 수 있다. 일부 예들에서, 화자 모델들은 하나 이상의 화자들(예를 들어, 도 1a-도 1c의 사람들(110, 101, 102))에 대한 스피치를 식별하도록 훈련된 AI/ML 모델들을 포함할 수 있다. 일부 예들에서, AI/ML 모델들은 화자 식별 결정들을 조정하기 위해 사용자로부 터 훈련 피드백을 수신할 수 있다. 화자 모델들은 또한 화자 식별자(예를 들어, \"화자 1\", \"화자 2\" 등)에 의해 자동으로 생성되거나 I/O 인터페이스들을 통해 사용자(예를 들어, \"Jack,\" \"Steve\", \"boss 등)에 의해 수동으로 입력되는 화자 식별자(ID), 이름, 또는 레이블을 포함할 수 있다. 일부 예들에서, 화자 모 델들은 각각 화자의 하나 이상의 이미지들 및/또는 화자의 얼굴들에 대한 해시 값을 포함할 수 있다. 일부 예들에서, 화자 식별자는 HMD의 사용자에게 기인된 스피치 세그먼트들을 식별하도록 구성될 수 있다. 예를 들어, 화자 식별자는 HMD의 사용자(예를 들어, 사용자)에 특정한 화자 모델을 적용 하여 사용자와 연관된 하나 이상의 스피치 세그먼트들을 식별할 수 있다(예를 들어, 스피치 세그먼트들의 속성 에 기반하여 사용자에 의해 말해진 스피치 세그먼트들이 사용자 화자 모델과 유사함을 식별). 다시 말해서, 화자 식별자는 HMD의 사용자에 의해 말해진 스피치 세그먼트(들)에 대해 스피치 인식 엔진 에 의해 인식된 하나 이상의 화자 세그먼트를 필터링할 수 있다. 스피치 전사기는 스피치 인식 엔진에 의해 인식된 스피치 세그먼트들을 전사하는 것과 관련된 기능들 을 수행한다. 예를 들어, 스피치 전사기는 화자 식별자에 의해 식별된 하나 이상의 화자들의 표시와 함께 스피치 인식 엔진에 의해 인식된 하나 이상의 스피치 세그먼트들의 텍스트 출력을 생성한다. 일부 예들에서, 스피치 전사기는 HMD의 사용자(예를 들어, 사용자)와 연관된 스피치 인식 엔진에 의해 인식되는 하나 이상의 스피치 세그먼트들의 텍스트 출력을 생성한다. 다시 말해서, 일부 예들에서, 스피치 전사기는 화자 식별자에 의해 식별되는 바와 같이 HMD의 사용자에 의해 말해진 하나 이상의 스 피치 세그먼트들에 대한 텍스트 출력만을 생성한다. 어느 쪽이든, 스피치 전사기는 텍스트 출력을 전사들 에 저장한다. 음성 어시스턴트 애플리케이션은 전사 분석과 관련된 기능들을 수행하여 전사로부터 도출된 추가 데이터를 생성한다. 예를 들어, 음성 어시스턴트 애플리케이션은 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대(예를 들어, \"Let's touch base again first thing Friday morning\"라는 스피치 세그먼트에 해당), 전사에 서 식별된 주제들에 관련된 정보(예를 들어, 회의 초대 대상자가 도 1a에 도시된 바와 같이 회의 초대를 거부했 다는 통지, 상호작용에 존재하지 않는 사람에 대한 통지), 또는 전사에서 식별된 작업들을 포함하는 작업 목록 (예를 들어, 작업 항목은 \"Please send out the sales report for last month after the meeting\"라는 스피치 세그먼트에 해당) 같은 추가 데이터를 생성할 수 있다. 일부 예들에서, 음성 어시스턴트 애플리케이션은 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어들(예를 들어, um, hmm, uh, like 등)에 관한 정보, 각 화자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "정보, 전사 요약 또는 화자의 감정을 포함하는 전사에 관한 통계를 생성할 수 있다. 음성 어시스턴트 애플리케 이션은 또한 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 생성할 수 있다. 예를 들어, 음성 어시스턴트 애플리케이션은 하나 이상의 화자들의 음성이 다른 음성(예를 들어, 만화의 음성 또는 유명인의 음성)으로 대체되거나 오디오 또는 비디오 파일에서 하나 이상의 스피치 세그먼트들의 언어 를 대체한 오디오 또는 비디오 파일을 생성할 수 있다. 전술된 바와 같이, 화자 모델들은 다양한 AI/ML 모델을 포함할 수 있다. 이러한 AI/ML 모델들은 인공 신경 망들(ANN), 판단 트리들, 지원 벡터 네트워크들, 베이지안 네트워크들, 유전 알고리즘들, 선형 회귀, 로지스틱 회귀, 선형 판별 분석, 나이브 베이즈, k-최근접 이웃들, 학습 벡터 양자화, 지원 벡터 기계들, 랜덤 결정 포리 스트들, 또는 임의의 다른 알려진 AI/ML 수학적 모델들을 포함할 수 있다. 이러한 AI/ML 모델들은 오디오 데이 터를 프로세싱하고 스피치 세그먼트들을 인식하고/하거나 스피치 세그먼트들의 화자를 식별하도록 훈련될 수 있 다. 예를 들어, 이러한 AI/ML 모델들은 오디오 데이터에서 스피치 및/또는 특정 음성들을 인식하도록 훈련 될 수 있다. 일부 예들에서, 이러한 AI/ML 모델들은 이미지 데이터에서 잠재적 화자들을 식별하도록 훈련될 수 있다. 예를 들어, 이러한 AI/ML 모델들은 이미지 데이터에서 사람들(예를 들어, 얼굴들) 및/또는 움직이는 입술들을 인식하도록 훈련될 수 있다. 일부 예들에서, 화자 모델들은 하나 이상의 사용자들에 대한 스피치 데이터 세트 및/또는 하나 이상의 사용자들에 대응하는 이미지들 세트로 훈련될 수 있다. 하나 이상의 양태들에 서, 이미지 데이터, 오디오 데이터, 화자 모델들, 및/또는 전사들 각각에 저장된 정보는 저장소, 데이터베이스, 맵, 검색 트리, 또는 임의의 다른 데이터 구조에 저장될 수 있다. 일부 예들에서, 이미 지 데이터, 오디오 데이터, 화자 모델들, 및/또는 전사들은 HMD와 별개일 수 있다(예 를 들어, 도 1a의 네트워크를 통해 HMD와 통신하는 별개의 데이터베이스(들)일 수 있음). 모션 센서들은 HMD의 현재 가속도를 나타내는 데이터를 출력하는 하나 이상의 가속도계들(또한 관성 측정 유닛들 또는 \"IMU들\"로 지칭됨), 다양한 객체들로부터 HMD의 거리들을 나타내는 데이터를 출력하는 레이더 또는 소나, 또는 HMD 또는 물리적 환경 내의 다른 객체들의 위치 또는 배향의 표시들을 제공하는 다른 센서들과 같은 센서들을 포함할 수 있다. 도 4는 본 개시내용의 기법들에 따른, 도 1a, 도 1b의 인공 현실 시스템의 HMD 및 전사 시스템의 예시적인 인스 턴스들에 의해 스피치 전사가 수행되는 예시적인 구현들을 도시하는 블록도이다. 도 4의 예에서, HMD는 오 디오 및/또는 이미지 데이터를 캡처하고 오디오 및/또는 이미지 데이터를 전사 시스템으로 송신한다. 전사 시스템의 스피치 인식 엔진은 본원에 기술된 하나 이상의 기법들에 따라 오디오 데이터에서 스피치 세그먼트들을 인식하고, 각각의 스피치 세그먼트들과 연관된 화자를 식별하고, 각 스피치 세그먼트와 연관된 화 자의 표시를 포함하는 전사를 생성하기 위해 각각의 스피치 세그먼트들을 전사하고, 전사로부터 도출된 추가 데 이터를 생성하기 위해 전사를 분석한다. 이 예에서, 도 3과 유사한 방식으로, HMD는 하나 이상의 프로세서들 및 메모리를 포함하고, 이 는 일부 예들에서 예를 들어 내장형 실시간 멀티태스킹 운영 체제 또는 다른 유형의 운영 체제일 수 있는 운영 체제를 실행하기 위한 컴퓨터 플랫폼을 제공한다. 차례로, 운영 체제는 하나 이상의 소프트웨어 구성 요소들을 실행하기 위한 멀티태스킹 운영 환경을 제공한다. 또한, 프로세서(들)는 전자 디스플레이 , 모션 센서들, 및 이미지 캡처 시스템, 및 오디오 캡처 시스템에 결합된다. 일부 예들에서, HMD는 도 3에 도시된 임의의 다른 구성요소들 중 임의의 것을 더 포함한다. 예를 들어, HMD는 스 피치 프로세싱 엔진(스피치 인식 엔진, 화자 식별자, 스피치 전사기 및 음성 어시스턴트 애플리케이션 포함), 이미지 데이터, 오디오 데이터, 화자 모델들, 및 전사들을 포함 할 수 있다. 일반적으로, 전사 시스템은 HMD로부터 수신된 오디오 및/또는 이미지 데이터를 프로세싱하여 오디오 데이터에 포함된 스피치 세그먼트들에서 하나 이상의 화자들의 표시를 포함하는 전사를 생성하고 전사로부터 도 출된 추가 데이터로부터 추가 데이터를 생성하는 디바이스이다. 일부 예들에서, 전사 시스템은 서버, 워크 스테이션, 데스크톱 컴퓨터, 랩톱 또는 게임 시스템과 같은 단일 컴퓨팅 디바이스이다. 다른 예들에서, 프로세 서들 및/또는 메모리와 같은 전사 시스템의 적어도 일부는 컴퓨팅 시스템들, 서버들, 및 컴퓨팅 디바이스들 사이에서 데이터를 송신하기 위해, 클라우드 컴퓨팅 시스템, 데이터 센터에 걸쳐, 또는 인터넷, 다 른 공용 또는 사설 통신 네트워크, 예를 들어, 광대역, 셀룰러, Wi-Fi, 및/또는 다른 유형들의 통신 네트워크들 같은 네트워크에 걸쳐 분산될 수 있다. 도 4의 예에서, 전사 시스템은 하나 이상의 프로세서들 및 메모리를 포함하고, 이는 일부 예들 에서 예를 들어 내장형 실시간 멀티태스킹 운영 체제 또는 다른 유형의 운영 체제일 수 있는 운영 체제를 실행하기 위한 컴퓨터 플랫폼을 제공한다. 차례로, 운영 체제는 하나 이상의 소프트웨어 구성요소들 을 실행하기 위한 멀티태스킹 운영 환경을 제공한다. 프로세서들은 키보드, 마우스, 게임 제어기들, 디스 플레이 디바이스들, 이미지 캡처 디바이스들, HMD들 등과 같은 다른 디바이스들과 통신하기 위한 하나 이상의 I/O 인터페이스들을 제공하는 I/O 인터페이스들에 결합된다. 또한, 하나 이상의 I/O 인터페이스들은 네트워크와 같은 네트워크와 통신하기 위한 하나 이상의 유선 또는 무선 네트워크 인터페이스 제어기(NI C)들을 포함할 수 있다. 프로세서들(302, 412) 각각은 멀티-코어 프로세서, 제어기, 디지털 신호 프로세서 (DSP), 주문형 집적 회로(ASIC), 필드-프로그램 가능 게이트 어레이(FPGA), 또는 동등한 이산 또는 집적 논리 회로 중 임의의 하나 이상을 포함할 수 있다. 메모리(304, 414)는 RAM(random-access memory), ROM(read-only memory), PROM(programmable read-only memory), EPROM(erasable programmable read-only memory), EEPROM(electronically erasable programmable read-only memory), 및 플래시 메모리 같은 데이터 및 실행가능 소프트웨어 명령들을 저장하기 위한 임의의 형태의 메모리를 포함할 수 있다. 전사 시스템의 소프트웨어 애플리케이션들은 전사 애플리케이션을 제공하도록 동작한다. 이 예에서, 소프트웨어 애플리케이션은 렌더링 엔진, 애플리케이션 엔진, 포즈 추적기, 스피치 프로세 싱 엔진, 이미지 데이터, 오디오 데이터, 화자 모델들, 및 전사를 포함한다. 도 3의 스피치 프로세싱 엔진과 유사하게, 스피치 프로세싱 엔진은 스피치 인식 엔진, 화자 식별자 , 스피치 전사기, 및 음성 어시스턴트 애플리케이션을 포함한다. 일반적으로, 애플리케이션 엔진은 인공 현실 애플리케이션, 예를 들어 전사 애플리케이션, 음성 어시스턴 트 애플리케이션, 가상 회의 애플리케이션, 게이밍 애플리케이션, 내비게이션 애플리케이션, 교육 애플리케이션, 훈련 또는 시뮬레이션 애플리케이션들 등을 제공하고 제시하는 기능을 포함한다. 애플리케이션 엔진은 예를 들어 컴퓨터 시스템은 상에서 인공 현실 애플리케이션을 구현하기 위한 하나 이상의 소 프트웨어 패키지들, 소프트웨어 라이브러리들, 하드웨어 드라이버들, 및/또는 애플리케이션 프로그램 인터페이 스(API)들을 포함할 수 있다. 애플리케이션 엔진에 의한 제어에 응답하여, 렌더링 엔진은 HMD의 애플리케이션 엔진에 의해 사용자에게 디스플레이하기 위한 3D 인공 현실 콘텐츠를 생성한다. 애플리케이션 엔진 및 렌더링 엔진은 포즈 추적기에 의해 결정된 바와 같이, 참조 프레임, 일반 적으로 HMD의 보기 관점 내의 HMD에 대한 현재 포즈 정보에 따라 사용자에게 디스플레이하기 위 한 인공 콘텐츠를 구성하는 것과 관련된 기능들을 수행한다. 현재 보기 관점에 기반하여, 렌더링 엔진은 일부 경우들에서 사용자의 현실-세계 3D 환경에 적어도 부분적으로 오버레이될 수 있는 3D 인공 현실 콘텐 츠를 구성한다. 이 프로세스 동안, 포즈 추적기는 사용자에 의한 모션, 및/또는 사용자에 관련 한 특징 추적 정보와 같은 현실 세계 환경 내의 3D 정보를 캡처하기 위해 HDM 상의 센서들로부터의 이미지 데이터, 및 일부 예들에서 외부 카메라들 같은 외부 센서들로부터의 데이터 같은 HMD로부터 수신된 감지 데이터에 대해 동작할 수 있다. 감지된 데이터에 기반하여, 컴퓨터 시스템은 사용자에게 디스플 레이하기 위해 하나 이상의 I/O 인터페이스들(315, 415)을 통해 HMD로 통신하기 위한 가상 현실 콘텐츠를 구성한다. 일부 예들에서, 애플리케이션 엔진 및 렌더링 엔진은 본 개시내용의 기법들에 따라 멀티미 디어 질의 애플리케이션을 위한 하나 이상의 사용자 인터페이스들을 디스플레이하기 위해 생성 및 렌더링할 수 있다. 예를 들어, 애플리케이션 엔진 및 렌더링 엔진은 전사 및/또는 추가 데이터를 디스플레이하기위한 사용자 인터페이스를 디스플레이하기 위해 생성하고 렌더링할 수 있다. 스피치 인식 엔진은 (예를 들어, 도 3의 스피치 인식 엔진을 참조하여 전술된 바와 같이) HMD로 부터 수신된 오디오 데이터에서 하나 이상의 스피치 세그먼트들을 인식하는 것에 관련된 기능들을 수행한 다. 일부 예들에서, 스피치 인식 엔진은 오디오 데이터(예를 들어, 원시 아날로그 데이터와 별개)의 하나 이상의 스피치 세그먼트들을 저장한다. 스피치 세그먼트는 하나 이상의 말해진 단어들을 포함할 수 있다. 예를 들어, 스피치 세그먼트는 단일 단어들, 2개 이상의 단어들 또는 심지어 구들 또는 완전한 문장들일 수 있 다. 화자 식별자는 스피치 인식 엔진에 의해 인식되는 하나 이상의 스피치 세그먼트들 각각과 연관된 화 자를 식별하는 것과 관련된 기능들을 수행한다. 예를 들어, 화자 식별자는 화자 또는 잠재적 화자들을 식 별하기 위해 이미지 데이터에서 입술들을 움직이는 얼굴들을 검출하도록 구성될 수 있다. 다른 예에서, HMD의 오디오 캡처 시스템은 HMD에 대한 오디오 소스의 방향성에 관한 정보를 캡처할 수 있는 마이크로폰 어레이를 포함할 수 있고, 화자 식별자는 그 방향성 정보 및 이미지 데이터에 기반하여 화자 또는 잠재적인 화자들을 식별할 수 있다(예를 들어, 화자 식별자는 \"Where is Mary?\"라는 스피치 세 그먼트에 관한 방향성 정보에 기반하여 도 1의 사람(101A)을 식별할 수 있음). 또 다른 예에서, 화자 식별자 는 사용자가 누구에게 초점을 맞추는지에 기반하여(예를 들어, HMD의 시야에 기반하여) 화자를 식별 할 것이다. 일부 예들에서, 화자 식별자는 각각의 스피치 세그먼트에 대한 해시 값 또는 임베딩 값을 결정할 수 있고, (예를 들어, 화자 모델들로부터) 잠재적 화자 모델들을 획득하고, 해시 값을 잠재적 화자 모델들과 비교하 고, 해시 값에 가장 가까운 화자 모델을 식별할 수 있다. 화자 식별자는 외부 데이터, HMD로부터 수 신된 이미지 데이터(예를 들어, 움직이는 입술을 갖는 검출된 얼굴들에 기반), 및/또는 사용자 입력에 기 반하여 잠재적인 화자 모델들을 식별할 수 있다. 예를 들어, 화자 식별자는 캘린더 정보(예를 들어, 확인 된 또는 잠재적인 회의 초대 대상자들에 관한 정보), HMD에서 수신된 이미지 데이터에서 식별된 하나 이상의 얼굴들, 위치 정보(예를 들어, HMD에 관하여 다른 사람들과 연관된 사람들 또는 디바이스들의 근접 정보)에 기반하여, 및/또는 사용자 입력을 통해 선택된 잠재적인 화자 모델들에 기반하여 잠재적인 화자들을 식 별할 수 있다. 일부 예들에서, 스피치 세그먼트에 대한 해시 값과 가장 가까운 화자 모델들 간의 차이가 임계 차이 이상이면, 화자 식별자는 해시 값에 기반하여 새로운 화자 모델을 생성하고 새로운 화자 모델을 스피 치 세그먼트에 연관시킬 수 있다. 스피치 세그먼트에 대한 해시 값과 가장 가까운 화자 모델들 간의 차이가 임 계 차이 미만인 경우, 화자 식별자는 가장 가까운 화자 모델과 연관된 화자를 스피치 세그먼트의 화자로 식별한다. 일부 예들에서, 화자 모델들은 상이한 화자들에 대한 해시 값들을 포함할 수 있다. 일부 예들에서, 화자 식별자는 HMD의 사용자에게 기인된 스피치 세그먼트들을 식별하도록 구성될 수 있다. 예를 들어, 화자 식별자는 HMD의 사용자(예를 들어, 사용자)에 특정한 화자 모델을 적용 하여 사용자와 연관된 하나 이상의 스피치 세그먼트들을 식별할 수 있다(예를 들어, 스피치 세그먼트들의 속성 에 기반하여 사용자에 의해 말해진 스피치 세그먼트들이 사용자 화자 모델과 유사함을 식별). 도 3에 관하여 전술된 스피치 전사기와 유사하게, 스피치 전사기는 스피치 인식 엔진에 의해 인 식된 하나 이상의 스피치 세그먼트들을 전사하는 것과 관련된 기능들을 수행한다. 예를 들어, 스피치 전사기 는 화자 식별자에 의해 식별된 하나 이상의 화자들의 표시와 함께 스피치 인식 엔진에 의해 인 식된 하나 이상의 스피치 세그먼트들의 텍스트 출력을 생성하고, 텍스트 출력을 전사들에 저장한다. 일부 예들에서, 스피치 전사기는 화자 식별자에 의해 식별되는 바와 같이 HMD의 사용자에 의해 말해 진 하나 이상의 스피치 세그먼트들에 대한 텍스트 출력만을 생성한다. 일부 예들에서, 스피치 프로세싱 엔진 은 텍스트 출력을 HMD에 송신한다. 음성 어시스턴트 애플리케이션은 전사 분석과 관련된 기능들을 수행하여 전사로부터 도출된 추가 데이터를 생성한다. 예를 들어, 음성 어시스턴트 애플리케이션은 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대(예를 들어, \"Let's touch base again first thing Friday morning\"라는 스피치 세그먼트에 해당), 전사에 서 식별된 주제들에 관련된 정보(예를 들어, 회의 초대 대상자가 도 1a에 도시된 바와 같이 회의 초대를 거부했 다는 통지, 상호작용에 존재하지 않는 사람에 대한 통지), 또는 전사에서 식별된 작업들을 포함하는 작업 목록 (예를 들어, 작업 항목은 \"Please send out the sales report for last month after the meeting\"라는 스피치 세그먼트에 해당) 같은 추가 데이터를 생성할 수 있다. 일부 예들에서, 음성 어시스턴트 애플리케이션은 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어들(예를 들어, um, hmm, uh, like등)에 관한 정보, 각 화자가 말한 시간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "정보, 전사 요약 또는 화자의 감정을 포함하는 전사에 관한 통계를 생성할 수 있다. 음성 어시스턴트 애플리케 이션은 또한 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 생성할 수 있다. 예를 들어, 음성 어시스턴트 애플리케이션은 하나 이상의 화자들의 음성이 다른 음성(예를 들어, 만화의 음성 또는 유명인의 음성)으로 대체되거나 오디오 또는 비디오 파일에서 하나 이상의 스피치 세그먼트들의 언어 를 대체한 오디오 또는 비디오 파일을 생성할 수 있다. 일부 예들에서, 스피치 프로세싱 엔진은 추가 데이 터를 HMD에 송신한다. 도 3과 관련하여 전술된 화자 모델들들과 유사하게, 화자 모델들은 다양한 AI/ML 모델들을 포함할 수 있다. 이러한 AI/ML 모델들은 오디오 데이터를 프로세싱하고 스피치 세그먼트들을 인식하고/하거나 스피치 세그 먼트들의 화자를 식별하도록 훈련될 수 있다. 예를 들어, 이러한 AI/ML 모델들은 오디오 데이터에서 스피 치 및/또는 특정 음성들을 인식하도록 훈련될 수 있다. 일부 예들에서, 이러한 AI/ML 모델들은 이미지 데이터에 서 잠재적 화자들을 식별하도록 훈련될 수 있다. 예를 들어, 이러한 AI/ML 모델들은 이미지 데이터에서 사 람들(예를 들어, 얼굴들) 및/또는 움직이는 입술들을 인식하도록 훈련될 수 있다. 일부 예들에서, 화자 모델들 은 하나 이상의 사용자들에 대한 스피치 데이터 세트 및/또는 하나 이상의 사용자들에 대응하는 이미지들 세트로 훈련될 수 있다. 일부 예들에서, AI/ML 모델들은 화자 식별 결정들을 조정하기 위해 사용자로부터 (예를 들어, I/O 인터페이스들을 통해) 훈련 피드백을 수신할 수 있다. 화자 모델들은 또한 화자 식별자 (예를 들어, \"화자 1\", \"화자 2\" 등)에 의해 자동으로 생성되거나 I/O 인터페이스들을 통해 사용자 (예를 들어, \"Jack,\" \"Steve\", \"boss 등)에 의해 수동으로 입력되는 화자 식별자, 이름, 또는 레이블을 포함할 수 있다. 일부 예들에서, 화자 모델들은 각각 화자의 하나 이상의 이미지들 및/또는 화자의 얼굴들에 대한 해시 값을 포함할 수 있다. 일부 예들에서, 전사 시스템은 2개 이상의 HMD들(예를 들어, 도 1b의 HMD들)로부터 오디오 및/또는 이미지 데이터를 수신한다. 일부 예들에서, 각각의 HMD는 동일한 물리적 환경 또는 상이한 물리적 환경(예를 들 어, 도 1b에 도시된 바와 같이)의 오디오 및/또는 이미지 데이터를 송신할 수 있다. 2개 이상의 상이한 소스들 에서 동일한 환경에 관한 오디오 및/또는 이미지 데이터를 캡처함으로써, 더 많은 양의 정보가 캡처될 수 있다. 예를 들어, 이미지 데이터는 2개 이상의 상이한 관점에서 캡처될 수 있거나 오디오 데이터는 환경의 2개의 상이 한 지점들에서 캡처될 수 있고, 이는 상이한 사운드들이 캡처되게 할 수 있다. 일부 예들에서, 전사 시스템 은 모든 HMD들로부터 수신된 데이터로부터 단일 전사를 생성한다. 도 5는 본 개시내용의 기법들에 따른, 도 1c의 시스템의 컴퓨팅 디바이스의 예시적인 인스턴스에 의해 스 피치 전사가 수행되는 예시적인 구현들을 도시하는 블록도이다. 도 5의 예에서, 컴퓨팅 디바이스는 도 3의 HMD를 참조하여 전술한 바와 같이 이미지 및 오디오 데이터 캡처, 화자 식별, 전사 및 분석 동작들을 수행 한다. 이 예에서, 컴퓨팅 디바이스는 하나 이상의 프로세서들 및 메모리를 포함하고, 이는 일부 예들 에서 예를 들어 내장형 실시간 멀티태스킹 운영 체제, 또는 다른 유형의 운영 체제일 수 있는 운영 체제를 실행하기 위한 컴퓨터 플랫폼을 제공한다. 차례로, 운영 체제는 하나 이상의 소프트웨어 구성요소들 을 실행하기 위한 멀티태스킹 운영 환경을 제공한다. 프로세서들은 키보드, 마우스, 게임 제어기들, 디스 플레이 디바이스들, 이미지 캡처 디바이스들, 다른 HMD들 등과 같은 다른 디바이스들과 통신하기 위한 하나 이 상의 I/O 인터페이스들을 제공하는 I/O 인터페이스들에 결합된다. 또한, 하나 이상의 I/O 인터페이스들 은 네트워크와 같은 네트워크와 통신하기 위한 하나 이상의 유선 또는 무선 네트워크 인터페이스 제 어기(NIC)들을 포함할 수 있다. 또한, 프로세서(들)는 전자 디스플레이, 및 이미지 캡처 시스템 , 및 오디오 캡처 시스템에 결합된다. 이미지 캡처 시스템 및 오디오 캡처 시스템은 각각 이미지 데이터 및 오디오 데이터를 획득하도록 구성된다. 도 5의 컴퓨팅 디바이스는 모바일 폰, 태블릿, 스마트 워치, 게임 콘솔, 워크스테이션, 데스크톱 컴퓨터, 랩톱, 또는 다른 컴퓨팅 디바이스에 대응할 수 있는 단일 컴퓨팅 디바이스로서 도시되어 있다. 다른 예들에서, 컴퓨팅 디바이스는 분산 컴퓨팅 네트워크, 데이터 센터, 또는 클라우드 컴퓨팅 시스템과 같은 복수의 컴퓨 팅 디바이스들에 걸쳐 분산될 수 있다. 컴퓨터 시스템의 소프트웨어 애플리케이션들은 전사 애플리케이션을 제공하도록 동작한다. 각각 도 3 및 도 4의 소프트웨어 애플리케이션들(317 및 417)과 유사하게, 소프트웨어 애플리케이션들은 렌더링 엔진 , 애플리케이션 엔진, 스피치 프로세싱 엔진, 이미지 데이터, 오디오 데이터, 화자모델들, 및 전사들을 포함한다. 각각 도 3 및 도 4의 스피치 프로세싱 엔진(341, 441)과 유사하게, 스피치 프로세싱 엔진은 스피치 인식 엔진, 화자 식별자, 스피치 전사기 및 음성 어시스턴 트 애플리케이션을 포함한다. HMD가 오디오 및/또는 이미지 데이터를 프로세싱하는 방식과 유사하게(예를 들어, 도 3과 관련하여 전술된 바와 같이), 컴퓨팅 시스템은 오디오 및/또는 이미지 데이터를 캡처하고 오디오 및/또는 이미지 데이터를 전사 시스템에 송신하고, 전사 시스템의 스피치 인식 엔진은 오디오 데이터의 스피치 세그먼트 들을 인식하고, 각각의 스피치 세그먼트들과 연관된 화자를 식별하고, 각각의 스피치 세그먼트와 연관된 화자의 식별을 포함하는 전사를 생성하기 위해 각각의 스피치 세그먼트들을 전사하고, 전사로부터 도출된 추가 데이터 를 생성하기 위해 전사를 분석한다. 일부 예들에서, 도 5의 컴퓨팅 디바이스는 단순히 이미지 데이터 및 오디오 데이터를 캡처하고 그 데이터를 전사 시스템으로 송신한다. 전사 시스템은 오디오 데이터에 포함된 스피치 세그먼트들에 서 하나 이상의 화자들의 표시를 포함하는 전사를 생성하고 전사로부터 도출된 추가 데이터로부터 추가 데이터 를 생성하기 위해 HMD로부터 수신된 오디오 및/또는 이미지 데이터를 프로세싱하는 것과 동일한 방식으로 컴퓨팅 디바이스로부터 수신된 오디오 및/또는 이미지 데이터를 프로세싱한다(예를 들어, 도 4와 관련하여 전술된 바와 같이). 일부 예들에서, 전사 시스템은 도 4의 HMD 및 도 5의 컴퓨팅 디바이스 둘 모두로부터 오디오 및 /또는 이미지 데이터를 수신한다. 일부 예들에서, HMD 및 컴퓨팅 디바이스는 동일한 물리적 환경 또 는 상이한 물리적 환경들의 오디오 및/또는 이미지 데이터를 송신할 수 있다. 2개 이상의 상이한 소스들에서 동 일한 환경에 관한 오디오 및/또는 이미지 데이터를 캡처함으로써, 더 많은 양의 정보가 캡처될 수 있다. 예를 들어, 이미지 데이터는 2개 이상의 상이한 관점에서 캡처될 수 있거나 오디오 데이터는 환경의 2개의 상이한 지 점들에서 캡처될 수 있고, 이는 상이한 사운드들이 캡처되게 할 수 있다. 일부 예들에서, 전사 시스템은 컴퓨팅 디바이스로부터의 데이터를 프로세싱하는 것과 동일하거나 유사한 방식으로 HMD로부터의 데이 터를 프로세싱하고, 그 반대도 마찬가지이고, HMD 및 컴퓨팅 디바이스로부터 수신된 데이터로부터 단 일 전사를 생성한다. 도 6은 본 개시내용의 양태들에 따라 스피치를 전사하고 분석하기 위한 방법의 예시적인 동작들을 예시하는 흐 름도이다. 일부 예들에서, 도 6에 도시된 하나 이상의 동작들은 HMD, 컴퓨팅 디바이스, 및/또는 전사 시스템에 의해 수행될 수 있다. HMD의 오디오 캡처 시스템 및 이미지 캡처 시스템 및/또는 컴퓨팅 디바이스의 오디오 캡처 시스템 및 이미지 캡처 시스템은 오디오 및 이미지 데이터를 캡처한다. 일부 예들에서, 오디오 및/또는 이미지 데이터는 자동으로 또는 수동으로 캡처된다. 예를 들어, HMD 및/또는 컴퓨팅 시스템 의 오디오 및/또는 이미지 캡처 시스템은 전원이 켜져 있을 때 항상 오디오 및/또는 이미지 데이터를 캡처하도 록 구성될 수 있다. 일부 예들에서, HMD의 멀티미디어 캡처 시스템 및/또는 컴퓨팅 시스템의 멀 티미디어 시스템은 데이터 캡처를 개시하는 사용자 입력에 응답하여 및/또는 전사, 가상 회의, 또는 음성 어시스턴트 애플리케이션을 개시하는 것에 응답하여 멀티미디어 데이터를 캡처하도록 구성될 수 있다. 일부 예 들에서, HMD 및/또는 컴퓨팅 디바이스는 (예를 들어, 실시간으로, 거의 실시간으로, 또는 상호작용이 종료된 후에) 오디오 및/또는 이미지 데이터를 전사 시스템에 송신할 수 있다. 스피치 프로세싱 엔진(341, 441, 또는 541)은 이미지 데이터를 사용하여 오디오 데이터를 전사한다. 예를 들어, 스피치 프로세싱 엔진(341, 441, 또는 541)은 오디오 데이터에서 스피치 세그먼트들을 인식하고, 스피치 세그먼트들 각각과 연관된 화자를 식별하고, 각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생성하기 위해 스피치 세그먼트 각각을 전사할 수 있다. 이어서, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 전사로부터 도출된 추가 데이터를 생성하기 위해 전사를 분석한다. 예를 들어, 음성 어시스턴트 애플리케이션(348, 448, 548)은 전사에서 설명된 회의 또는 이벤트에 대한 캘린더 초대(예를 들어, \"Let's touch base again first thing Friday morning\"라는 스피치 세 그먼트에 해당), 전사에서 식별된 주제들에 관련된 정보(예를 들어, 회의 초대 대상자가 도 1a에 도시된 바와 같이 회의 초대를 거부했다는 통지, 상호작용에 존재하지 않는 사람에 대한 통지), 또는 전사에서 식별된 작업 들을 포함하는 작업 목록(예를 들어, 작업 항목은 \"Please send out the sales report for last month after the meeting\"라는 스피치 세그먼트에 해당) 같은 추가 데이터를 생성할 수 있다. 일부 예들에서, 추가 데이터는 (예를 들어, 세그먼트 또는 전체 전사물당) 화자에 의해 말해진 단어들의 수, 화 자의 톤, 화자에 의해 사용된 필러 단어들(예를 들어, um, hmm, uh, like 등)에 관한 정보, 각 화자가 말한 시"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "간의 백분율, 사용된 욕설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사 요약 또는 화자의 감정을 포 함하는 전사에 관한 통계를 포함할 수 있다. 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 또한 복수의 화자들 중 적어도 하나와 연관된 스피치 세그먼트들의 수정된 버전을 생성할 수 있다. 예를 들어, 음성 어시스 턴트 애플리케이션(348, 448, 또는 548)은 하나 이상의 화자들의 음성이 다른 음성(예를 들어, 만화의 음성 또 는 유명인의 음성)으로 대체되거나 오디오 또는 비디오 파일에서 하나 이상의 스피치 세그먼트들의 언어를 대체 한 오디오 또는 비디오 파일을 생성할 수 있다. 일부 예들에서, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 실시간으로(예를 들어, 오디오 및 이미지 데이터가 캡처될 때), 거의 실시간으로, 상호작용이 종료된 후, 또는 HMD 또는 컴퓨팅 디바이스가 이미지 또는 이미지 데이터 캡처를 중지한 후 전사를 분석한다. 도 7은 본 개시내용의 기법들에 따른 오디오 데이터 및 전사를 예시한다. 도 7에 도시된 예에서, 오 디오 데이터는 HMD의 오디오 캡처 시스템 또는 컴퓨팅 디바이스의 오디오 캡처 시스템 에 의해 캡처된 아날로그 데이터에 대응한다. 스피치 인식 엔진(342, 442 또는 552)은 오디오 데이터(70 2)에서 스피치 세그먼트들(704A, 704B, 704C)(집합적으로 \"스피치 세그먼트들\")을 인식하고 대응하는 전사 된 스피치 세그먼트들(706A, 706B, 및 706C)(집합적으로 \"전사\")을 생성한다. 스피치 세그먼트들이 각각 전체 문장들을 포함하지만, 스피치 세그먼트들은 하나 이상의 단어들을 포함할 수 있다. 예를 들어, 스피 치 세그먼트들은 항상 전체 문장들을 포함하지 않을 수 있고 단일 단어들 또는 구들로 구성될 수 있다. 일부 예 들에서, 스피치 인식 엔진(342, 442, 또는 552)은 도 7에 도시된 바와 같이 완전한 문장들을 포함하는 스피치 세그먼트들을 형성하기 위해 하나 이상의 단어들을 결합할 수 있다. 도 7에 도시된 예에서, (예를 들어, 도 3-도 5를 참조하여 전술된 바와 같이 화자 모델들 및/또는 이미지 데이 터에 기반하여) 화자 식별자(344, 444 또는 544)는 \"화자 1\"을 스피치 세그먼트들(706A 및 706B)의 화자로 식별 하고 \"화자 2\"를 스피치 세그먼트(706C)의 화자로 식별한다. 일부 예들에서, 레이블들 또는 식별자들(\"화자 1\" 및 \"화자 2\")(결과적인 전사에 삽입됨)은 화자 식별자(344, 444 또는 544)에 의해 자동으로 생성될 수 있다. 다 른 예들에서, 이러한 식별자들 또는 레이블들은 사용자에 의해 수동으로 입력될 수 있고 이름(예를 들어, \"Jack\", \"Steve\", \"boss\" 등)을 포함할 수 있다. 어느 쪽이든, 이러한 레이블들, 식별자들 또는 이름들은 전사 에서 스피치 세그먼트들의 소스였던 화자의 표시를 제공할 수 있다. 일부 예들에서, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 추가 데이터를 생성하기 위해 전사 를 분석할 수 있다. 예를 들어, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 통지(예를 들어, 도 1a에 도시된 바와 같이 \"Mary declined the meeting invitation\"는 통지)를 생성할 수 있다. 일부 예들에서, 추가 데이터는 (예를 들어, 세그먼트 또는 전체 전사물당) 화자에 의해 말해진 단어들의 수, 화자의 톤, 화자에 의해 사용된 필러 단어들(예를 들어, um, hmm, uh, like 등)에 관한 정보, 각 화자가 말한 시간의 백분율, 사용된 욕"}
{"patent_id": "10-2022-7020776", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "설에 관한 정보, 사용된 단어들의 길이에 관한 정보, 전사 요약 또는 화자의 감정을 포함하는 전사에 관한 통계 를 포함할 수 있다. 다른 예에서, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 화자 1 및/또는 화자 2 의 음성들이 다른 음성(예를 들어, 만화의 음성 또는 유명인의 음성)으로 대체되거나 오디오 또는 비디오 파일 에서 임의의 스피치 세그먼트들의 언어를 대체한 오디오 또는 비디오 데이터를 생성할 수 있다. 도 8은 본 개시내용의 양태들에 따른 스피치를 전사하기 위한 방법의 예시적인 동작들을 예시하는 흐름도 이다. 흐름도는 도 6의 흐름도의 요소에서 스피치 프로세싱 엔진(341, 441, 또는 541)에 의해 수행되는 기능들의 일 예이다. 처음에, 스피치 인식 엔진(342, 442, 또는 542)은 오디오 데이터(예를 들어, 오디오 데이터(332, 432, 532, 또 는 702))에서 하나 이상의 스피치 세그먼트를 인식한다. 예를 들어, 스피치 인식 엔진(342, 442 또는 54 2)은 아날로그-투-디지털 변환기(ADC)를 사용하여 아날로그 오디오 데이터를 디지털 데이터로 변환하고, 디지털화된 오디오 데이터에서 노이즈를 필터링하고, 도 7의 스피치 세그먼트(706A)를 인식하기 위해 하나 이상 의 통계 모델들(예를 들어, 은닉 마르코프 모델 또는 신경망)을 필터링된 디지털화된 오디오 데이터에 적용할 수 있다. 일부 예들에서, 스피치 인식 엔진(342, 442, 또는 542)은 하나 이상의 특정 사용자들(예를 들어, 도 1a-도 1c의 사용자)에 대한 스피치를 인식하도록 훈련된 AI/ML 모델을 오디오 데이터에 적용할 수 있 다. 예를 들어, 스피치 인식 엔진(342, 442, 또는 542)은 HMD의 사용자(사용자)에 대한 스피치만 인 식하도록 훈련된 AI/ML 모델을 적용할 수 있다. 일부 예들에서, AI/ML 모델은 스피치 인식 결정들을 조정하기 위해 사용자로부터 훈련 피드백을 수신할 수 있다. 일부 예들에서, 스피치 인식 엔진(342, 442, 또는 542)은 이미지 데이터(330, 430, 또는 530)에 기반하여 오디오 데이터(332, 432, 또는 532)에서 하나 이상의 스피치 세그 먼트들을 인식할 수 있다. 예를 들어, 스피치 인식 엔진(342, 442, 또는 542)은 스피치 세그먼트들(예를 들어, 스피치 세그먼트의 시작 및 끝)을 인식하기 위해 이미지 데이터에서 입술들이 움직이는 얼굴들을 검출하도록 구 성될 수 있다. 화자 식별자(344, 444 또는 544)는 인식된 스피치 세그먼트와 연관된 화자를 식별한다. 예를 들어, 화자 식별자(344, 444, 또는 544)는 스피치 세그먼트(704A)의 사운드 강도(예를 들어, 볼륨)에 기반하여 도 7의 세그 먼트(704A)의 화자로서 화자 1을 식별할 수 있다(예를 들어, 사운드 강도는 도 1b의 HMD(112A)의 사용자로부터 발생하는 스피치에 대해 더 큼). 다른 예에서, 화자 식별자(344, 444, 또는 544)는 HMD의 이미지 캡처 시 스템 및/또는 컴퓨팅 디바이스의 이미지 캡처 시스템에 의해 캡처된 이미지 데이터를 사용하여 도 7의 세그먼트(704C)의 화자로서 화자 2를 식별할 수 있다. 예를 들어, 화자 식별자(344, 444 또는 544)는 이 미지 데이터(330, 430, 또는 530)에서 입술들이 움직이는 얼굴들을 검출하여 화자를 식별하도록 구성될 수 있고, 입술들이 움직이는 검출된 얼굴 및/또는 이미지 데이터의 초점(예를 들어, 사용자가 화자를 보고 있 음을 암시)에 기반하여 화자를 식별할 수 있다. 다른 예에서, HMD 또는 컴퓨팅 시스템의 오디오 캡처 시스템(209 또는 509)은 각각 HMD 또는 컴퓨팅 디바이스에 관하여 오디오 소스의 방향성에 관한 정보 를 캡처할 수 있는 마이크로폰을 포함할 수 있고, 화자 식별자(344, 444 또는 544)는 방향 정보 및 이미지 데이 터(330, 430 또는 530)에 기반하여 화자 또는 잠재적인 화자를 식별할 수 있다. 화자 식별자(344, 444 또는 544)는 인식된 스피치 세그먼트에 화자 식별자를 태깅(tag)한다. 예를 들어, 화자 식별자(344, 444, 또는 544)는 도 7의 식별자(\"화자 1\")로 스피치 세그먼트(704A)를 태깅한다. 도 7에 관 하여 전술된 바와 같이, 일부 예들에서, 화자 식별자(344, 444, 또는 544)는 전사에 포함하기 위해 식별자 (\"화자 1\")를 자동으로 생성한다. 다른 예들에서 사용자, 관리자 또는 다른 소스는 하나 이상의 세그먼트들에 대한 식별자, 레이블 또는 이름을 입력한다. 이러한 레이블들, 식별자들 또는 이름들은 전사에서 스피치 세그먼 트들의 화자 표시를 제공할 수 있다. 스피치 전사기(346, 446 또는 546)는 스피치 인식 엔진(342, 442 또는 542)에 의해 인식된 스피치 세그먼트를 전사한다. 예를 들어, 스피치 전사기(346, 446, 또는 546)는 도 7의 세그먼트(704A)에 대한 텍스트 출력 (706A)을 생성한다. 다음으로, 스피치 프로세싱 엔진(341, 441 또는 541)은 스피치 인식 엔진(342, 442 또는 542)이 오디오 데이터(예를 들어, 오디오 데이터(332, 432, 532 또는 702))에서 하나 이상의 추가 스피치 세그 먼트들을 인식하는지 여부를 결정한다. 스피치 인식 엔진(342, 442 또는 542)이 하나 이상의 추가 스피치 세그먼트들을 인식하면(810의 예 분기), 요소들(804 내지 810)은 반복된다. 예를 들어, 스피치 인식 엔진(342, 442 또는 542)은 스피치 세그먼트(704B)를 인식하고, 이어서 화자 식별자(344, 444 또는 544)는 화자 1을 스피치 세그먼트(704B)의 화자로서 식별하고 스피치 세그먼트(704B)를 화자 1이 화자라는 표시로 태깅하고, 이어서 스피치 전사기(346, 446 또는 546)는 스피치 세그먼트(704B)를 전사한다. 이 프로세스는 추가 스피치 세그먼트들이 인식되지 않을 때까지 계속될 수 있다(예를 들어, 상호작용이 종료될 때, 오디오/이미지 데이터가 더 이상 캡처되지 않을 때, 또는 전체 오디오 데이터가 프로세싱되었을 때)(810의 아니오 분기). 전사 가 완료된다(예를 들어, 흐름도는 도 6의 606으로 계속될 수 있음). 일부 예들에서, 흐름도는 2개 이상의 소스들(예를 들어, 2개 이상의 HMD 및/또는 컴퓨팅 디바이스 로부터 수신됨)로부터의 오디오 및/또는 이미지 데이터(예를 들어, 오디오 및/또는 비디오 스트림들 또는 파일들)를 프로세싱한다. 그 예에서, 흐름도의 동작들은 각각의 오디오 데이터 스트림 또는 파일에 대해 반복될 수 있다. 일부 예들에서, 흐름도는 각 오디오 데이터 스트림 또는 파일의 전사들을 결합하고 전사 에서 각 스피치 세그먼트의 화자의 표시를 포함하는 단일 전체 전사를 생성할 것이다. 예를 들어, 흐름도 는 각각의 오디오 데이터 파일 또는 스트림으로부터의 타임 스탬프들을 사용하여 전사를 결합할 수 있다. 도 9는 본 개시내용의 양태들에 따른 스피치 세그먼트의 화자를 식별하기 위한 방법의 예시적인 동작들을 예시 하는 흐름도이다. 흐름도는 도 8의 흐름도의 요소에서 화자 식별자(344, 444, 또는 544) 에 의해 수행되는 기능들의 일 예이다. 화자 식별자(344, 444, 544)는 스피치 세그먼트에 대한 스피치 세그먼트 해시 값을 결정할 수 있다. 예를 들어, 스피치 프로세싱 엔진(341, 441, 또는 541)은 각각의 인식된 스피치 세그먼트를 별도의 파일들(예를 들어, 임시 파일)에 저장할 수 있다. 이러한 파일들은 아날로그 오디오 데이터 또는 오디오 데이터의 디지털화 된 버전(예를 들어, 필터링된 스피치 이외의 노이즈들 포함)을 포함할 수 있다. 화자 식별자는 이러한 개별 파 일들에 해시 함수를 적용하여 각 스피치 세그먼트에 대한 스피치 세그먼트 해시 값을 결정할 수 있다. 화자 식별자(344, 444, 544)는 화자 모델들(334, 434, 534)로부터 잠재적 화자 모델들을 획득하고 스피치 세그먼 트 해시 값을 잠재적인 화자 모델들의 해시 값들과 비교할 수 있다. 화자 식별자(344, 444, 544)는 스피치 세그먼트 해시 값에 가장 가까운 해시 값으로 가장 가까운 화자 모델을 식별한다. 스피치 세그먼트 해시 값과 가장 가까운 화자 모델들 간의 차이가 임계 차이 이상이면(910의 아니오 분기), 화 자 식별자(344, 444 또는 544)는 스피치 세그먼트 해시 값에 기반하여 새로운 화자 모델을 생성할 수 있다 . 예를 들어, 화자 식별자(344, 444 또는 544)는 스피치 세그먼트 해시 값에 대한 새로운 화자 식별자(I D)를 결정하고 새로운 화자 ID 및 스피치 세그먼트 해시 값을 화자 모델들(334, 434 또는 534)에 새로운 화자 모델로 저장할 것이다. 이어서, 화자 식별자(344, 444, 또는 544)는 스피치 세그먼트에 대한 화자로서 새로운 화자 ID를 반환할 것이다(예를 들어, 흐름도는 새로운 화자 ID로 도 8의 806으로 계속될 수 있음). 스피치 세그먼트에 대한 스피치 세그먼트 해시 값과 가장 가까운 화자 모델들의 해시 값 간의 차이가 임계 차이 미만이면(910의 예 분기), 화자 식별자(344, 444 또는 544)는 스피치 세그먼트 해시 값에 기반하여 가장 가까운 화자 모델을 업데이트한다. 예를 들어, 가장 가까운 화자 모델의 해시 값은 그 화자와 연관된 모든 스피치 세그먼트들의 평균 해시 값을 포함할 수 있고 화자 식별자(344, 444 또는 544)는 스피치 세그먼트 해시 값을 그 평균에 통합할 수 있다. 이어서, 화자 식별자(344, 444, 또는 544)는 가장 가까운 화자 모델의 화자 ID를 스피 치 세그먼트에 대한 화자로서 반환할 것이다(예를 들어, 흐름도는 가장 가까운 화자 모델과 연관된 화자 ID로 도 8의 806으로 계속될 수 있음). 도 10은 본 개시내용의 양태들에 따른 잠재적인 화자 모델들을 식별하기 위한 방법의 예시적인 동작들을 예시하 는 흐름도이다. 흐름도는 도 9의 흐름도의 요소에서 화자 식별자(344, 444, 또는 544) 에 의해 수행되는 기능들의 일 예이다. 화자 식별자(344, 444 또는 544)는 많은 입력들에 기반하여 잠재적인 화자 모델들을 식별할 수 있다. 예 를 들어, 화자 식별자(344, 444 또는 544)는 외부 데이터를 획득하고, 그 외부 데이터를 프로세싱하여 하 나 이상의 잠재적인 화자 모델들을 식별할 수 있다. 일부 예들에서, 외부 데이터는 한 명 이상의 사용자 들의 위치 정보(예를 들어, GPS 좌표들)를 포함할 수 있다. 예를 들어, 화자 식별자(344, 444, 또는 544)는 HMD 또는 컴퓨팅 디바이스의 부근(예를 들어, 50피트 이내) 내에서 한 명 이상의 사용자들(또는 하나 이상의 사용자들과 연관된 디바이스들)을 결정하고 해당 정보를 사용하여 (예를 들어, 화자 모델들(334, 434 또 는 534)로부터) 해당 사용자들/디바이스들과 연관된 화자 모델들을 획득할 수 있다. 일부 예들에서, 외부 정보 는 회의에 대한 초대받은 사람 정보, 회의에 대한 위치 정보, 및 각 초대받은 사람이 회의에 참석할 계획인지의 표시를 포함하는 캘린더 정보를 포함할 수 있다. 일부 예들에서, 화자 식별자(344, 444, 또는 544)는 캘린더 정 보의 모든 초대받은 사람들에 대응하는 화자 모델들을 식별할 것이다. 다른 예들에서, 화자 식별자(344, 444 또 는 544)는 회의에 참석할 계획인 캘린더 정보의 모든 초대받은 사람들에 대응하는 화자 모델들을 식별할 것이다. 일부 예들에서, 화자 식별자(344, 444 또는 544)는 이미지 데이터를 획득하고 그 외부 데이터를 프로세싱 하여 하나 이상의 잠재적인 화자 모델들을 식별할 수 있다. 예를 들어, 화자 식별자(344, 444 또는 544) 는 이미지 데이터에서 얼굴들을 검출하고 (예를 들어, 화자 모델들(334, 434 또는 534)로부터) 검출된 얼굴들과 연관된 화자 모델들을 식별하도록 구성될 수 있다. 다른 예들에서, 화자 식별자(344, 444 또는 544)는 오디오 데이터에서 인식된 스피치 세그먼트에 대응하는 이미지 데이터의 입술들이 움직이는 얼굴들을 검출하고 (예를 들어, 화자 모델들(334, 434 또는 534)로부터) 입술들이 움직이는 검출된 얼굴들과 연관된 화자 모델들을 식별 하도록 구성될 수 있다. 일부 예들에서, 화자 식별자(344, 444 또는 544)는 이미지에서 얼굴들 및/또는 입술들 이 움직이는 얼굴들을 식별하도록 훈련된 AI/ML 모델들을 이미지 데이터에 적용할 수 있다. 다른 예에서, HMD 또는 컴퓨팅 시스템의 오디오 캡처 시스템(209 또는 509)은 각각 HMD 또는 컴퓨팅 디바이스 각각에 관하여 오디오 소스의 방향성에 관한 정보를 캡처할 수 있는 마이크로폰 어레이를 포함할 수 있고, 화자 식별자(344, 444, 또는 544)는 방향 정보 및 이미지 데이터에서 검출된 얼굴들에 기반하여 화자 또 는 잠재적인 화자들을 식별할 수 있다. 예를 들어, 화자 식별자(344, 444, 또는 544)는 스피치 세그먼트에 관한 방향성 정보 및 도 1c의 사람(101A)의 얼굴에 그 방향성의 대응에 기반하여 도 7의 스피치 세그먼트(704 C)의 화자로서 화자 2를 식별할 수 있다. 또 다른 예에서, 화자 식별자(344, 444, 또는 544)는 사용자가 초점을 맞추는 사람에 기반하여(예를 들어, HMD의 시야에 기반하여) 화자를 식별할 것이다. 일부 예들에서, 화자 식별자(344, 444 또는 544)는 사용자 입력을 수신하고 그 사용자 입력을 프로세싱하 여 하나 이상의 잠재적인 화자 모델들을 식별할 수 있다. 예를 들어, (예를 들어, 화자 모델들(334, 434또는 534)로부터) 화자 또는 화자 모델들을 식별할 수 있다. 다른 예들에서, 사용자는 외부 데이터 또는 이미지 데이터에 기반하여 식별된 잠재적인 화자 모델들을 확인할 수 있다. 도 11은 본 개시내용의 양태들에 따른 분산 디바이스들에 대한 스피치를 전사하기 위한 방법의 예시적인 동작들 을 예시하는 흐름도이다. 일부 예들에서, 도 11에 도시된 하나 이상의 동작들은 HMD, 컴퓨팅 디바이 스, 및/또는 전사 시스템에 의해 수행될 수 있다. HMD의 오디오 캡처 시스템 및 이미지 캡처 시스템 및/또는 컴퓨팅 디바이스의 오디오 캡처 시스템 및 이미지 캡처 시스템은 오디오 및 이미지 데이터를 캡처한다. 예를 들어, 2개 이상의 HMD들 및/또는 컴퓨팅 디바이스는 (예를 들어, 동일하거나 상이한 물리적 환경들로부터) 오디오 및/ 또는 이미지 데이터를 캡처할 수 있다. 스피치 프로세싱 엔진(341, 441, 또는 541)은 사용자 화자 모델(예를 들어, 디바이스의 사용자에 특정한 화자 모델)을 사용하여 각 디바이스에 대한 이미지 데이터를 사용하여 오디오 데이터를 전사한다. 예를 들어, 도 1b에서 HMD(112A)의 스피치 프로세싱 엔진은 (예를 들어, 사용자에 특정한 화자 모델을 사용하여) 사용 자에 대응하는 스피치 세그먼트들을 전사하고, HMD(112B)의 스피치 프로세싱 엔진은 (예를 들어, 사용자 (101A)에 특정한 화자 모델을 사용하여) 사용자(101A)에 대응하는 스피치 세그먼트들을 전사하고, HMD(112C)의 스피치 프로세싱 엔진은 (예를 들어, 사용자(102A)에 특정한 화자 모델을 사용하여) 사용자(102A)에 대응하는 스피치 세그먼트를 전사한다. 일부 예들에서, 사용자는 HMD 또는 컴퓨팅 디바이스에 로그인하거나 그 렇지 않으면 그 또는 그녀 자신을 사용자로 식별한다. 다른 예들에서, HMD 또는 컴퓨팅 디바이스는 (예를 들어, 전술된 음성 및/또는 얼굴 인식 기법들을 사용하여) 자동으로 사용자를 식별한다. 예를 들어, 스피 치 프로세싱 엔진(341, 441, 또는 541)은 각각의 스피치 세그먼트와 연관된 화자의 표시를 포함하는 전사를 생 성하기 위해 스피치 세그먼트들 각각을 전사할 수 있다. 일부 예들에서, 도 1c의 HMD(112A, 112B, 및/또는 112C) 중 임의의 것은 오디오 및 이미지 데이터를 캡처하고 그 오디오 및 이미지 데이터를 전사를 위해 전사 시 스템으로 송신할 수 있다(예를 들어, 도 4를 참조하여 전술된 바와 같이). 예를 들어, 전사 시스템은 도 1c의 HMD(112A, 112B, 및/또는 112C) 중 하나 이상으로부터 오디오 및 이미지 데이터를 수신하고 각 디바이 스에 대한 오디오 데이터를 전사한다. 이어서, 스피치 프로세싱 엔진(341, 441 또는 541)은 화자/각 전사된 스피치 세그먼트와 연관된 사용자의 표시 를 포함하는 하나의 전체 전사를 생성하기 위해 2개 이상의 HMD들 및/또는 컴퓨팅 디바이스들에 의해 캡처된 오디오 데이터의 스피치 세그먼트들에 대응하는 모든 전사들을 결합한다. 예를 들어, HMD들(112A, 112B, 및 112C) 각각은 사용자들(110, 101A, 및 102A)로부터 각각 캡처된 스피치의 개별 전사들을 개별 전사들 을 결합할 전사 시스템으로 송신할 수 있다. 다른 예에서, HMD들(112B 및 112C)은 사용자들(101A 및 102 A)로부터 각각 캡처된 스피치의 개별 전사들을 HMD(112A)로 송신할 수 있고, 이는 개별 전사들을 결합할 것이다. 이어서, 일부 예들에서, 음성 어시스턴트 애플리케이션(348, 448, 또는 548)은 전사로부터 도출된 추가 데이터를 생성하기 위해 개별 및/또는 전체 전사들을 선택적으로 분석한다(예를 들어, 도 6을 참조하여 전술된 바와 같음). 본 개시내용에서 설명된 기법들은 적어도 부분적으로 하드웨어, 소프트웨어, 펌웨어 또는 이들의 임의의 조합으 로 구현될 수 있다. 예를 들어, 설명된 기법들의 다양한 양태들은 하나 이상의 마이크로프로세서들, DSP들, 주 문형 집적 회로(ASIC)들, 현장 프로그래밍가능 게이트 어레이(FPGA)들, 또는 임의의 다른 등가 집적 또는 이산 논리 회로, 및 그러한 구성요소들의 임의의 조합을 포함하는 하나 이상의 프로세서들 내에서 구현될 수 있다. \"프로세서\" 또는 \"프로세싱 회로\"라는 용어는 일반적으로 단독으로 또는 다른 논리 회로, 또는 임의의 다른 등 가 회로와 조합하여 전술한 논리 회로 중 임의의 것을 지칭할 수 있다. 하드웨어를 포함하는 제어 유닛은 또한 본 개시내용의 기법들 중 하나 이상을 수행할 수 있다. 이러한 하드웨어, 소프트웨어, 및 펌웨어는 본 개시내용에서 설명된 다양한 동작들 및 기능들을 지원하기 위해 동일한 디바이스 내에서 또는 별도의 디바이스들 내에서 구현될 수 있다. 또한, 설명된 유닛들, 모듈들 또는 구 성요소들 중 임의의 것은 개별적이지만 상호운용 가능한 논리 디바이스들로서 함께 또는 별도로 구현될 수 있다. 모듈들 또는 유닛들과 같은 상이한 피처(feature)들의 묘사는 다른 기능적 양태들만을 강조하기 위한 것 이고 그러한 모듈들이나 유닛들이 별도의 하드웨어 또는 소프트웨어 구성요소들에 의해 실현되어야 함을 반드시 의미하지는 않는다. 오히려, 하나 이상의 모듈들 또는 유닛들과 연관된 기능은 별도의 하드웨어 또는 소프트웨 어 구성요소들에 의해 수행되거나 공통 또는 별도의 하드웨어 또는 소프트웨어 구성요소들 내에 통합될 수 있다.본 개시내용에서 설명된 기법들은 또한 명령들을 포함하는 컴퓨터-판독가능 저장 매체와 같은 컴퓨터-판독가능 매체에서 구현되거나 인코딩될 수 있다. 컴퓨터-판독 가능 저장 매체에 내장되거나 인코딩된 명령들은 예를 들 어 명령들이 실행될 때 프로그램가능 프로세서, 또는 다른 프로세서가 방법을 수행하게 할 수 있다. 컴퓨터 판 독 가능 저장 매체는 RAM(Random Access Memory), ROM(Read Only Memory), PROM(Programmable Read Only Memory), EPROM(Erasable Programmable Read Only Memory), EEPROM(Electronically Erasable Programmable Read Only Memory), 플래시 메모리, 하드 디스크, CD-ROM, 플로피 디스크, 카세트, 자기 매체, 광학 매체 또는 다른 컴퓨터 판독가능 매체를 포함할 수 있다. 본원의 다양한 예들에 의해 설명된 바와 같이, 본 개시내용의 기법들은 인공 현실 시스템을 포함하거나 이와 함 께 구현될 수 있다. 설명된 바와 같이, 인공 현실은 예를 들어, 가상 현실(VR), 증강 현실(AR), 혼합 현실(MR), 하이브리드 현실, 또는 이들의 일부 조합 및/또는 파생물들을 포함할 수 있는, 사용자에게 제시 전에 일부 방식 으로 조정된 현실 형태이다. 인공 현실 콘텐츠는 완전히 생성된 콘텐츠 또는 캡처된 콘텐츠(예를 들어, 현실-세 계 사진들)와 결합된 생성된 콘텐츠를 포함할 수 있다. 인공 현실 콘텐츠는 비디오, 오디오, 햅틱 피드백, 또는 이의 일부 조합을 포함할 수 있고, 이들 중 임의의 것은 단일 채널 또는 다중 채널들(이를테면 뷰어에게 3-차원 효과를 생성하는 스테레오 비디오)로 제시될 수 있다. 또한, 일부 실시예들에서, 인공 현실은 예를 들어 인공 현실의 콘텐츠를 생성하는 데 사용되고/되거나 인공 현실에 사용되는(예를 들어, 인공 현실의 활동들을 수행하 는) 애플리케이션들, 제품들, 액세서리들, 서비스들, 또는 이들의 일부 조합과 연관될 수 있다. 인공 현실 콘텐 츠를 제공하는 인공 현실 시스템은 호스트 컴퓨터 시스템에 연결된 머리장착 디바이스(HMD), 독립형 HMD, 모바 일 디바이스 또는 컴퓨팅 시스템, 또는 인공 현실 콘텐츠를 하나 이상의 뷰어들에게 제공할 수 있는 임의의 다 른 하드웨어 플랫폼을 포함하여 다양한 플랫폼들에서 구현될 수 있다. 특정 실시예들에서, 컴퓨팅 시스템의 하나 이상의 객체들(예를 들어, 콘텐츠 또는 다른 유형들의 객체들)은 하 나 이상의 프라이버시 설정들과 연관될 수 있다. 하나 이상의 객체들은 예를 들어 소셜-네트워킹 시스템, 클라 이언트 시스템, 제3자 시스템, 소셜-네트워킹 애플리케이션, 메시징 애플리케이션, 사진-공유 애플리케이션, 또 는 임의의 다른 적합한 컴퓨팅 시스템 또는 애플리케이션 같은 임의의 적합한 컴퓨팅 시스템 또는 애플리케이션 에 저장되거나 달리 연관될 수 있다. 본원에서 논의된 예들이 온라인 소셜 네트워크의 맥락에 있지만, 이러한 프라이버시 설정들은 임의의 다른 적합한 컴퓨팅 시스템에 적용될 수 있다. 객체에 대한 개인 프라이버시 설정 들(또는 \"액세스 설정들\")은 예를 들어 객체와 연관하여, 권한부여 서버의 인덱스에, 다른 적합한 방식으로, 또 는 이들의 임의의 적합한 조합과 같은 임의의 적합한 방식으로 저장될 수 있다. 객체에 대한 프라이버시 설정은 객체(또는 객체와 연관된 특정 정보)가 온라인 소셜 네트워크 내에서 액세스, 저장 또는 달리 사용(예를 들어, 보기, 공유, 수정, 복사, 실행, 표시 또는 식별)될 수 있는 방법을 지정할 수 있다. 객체에 대한 프라이버시 설 정이 특정 사용자 또는 다른 엔티티가 해당 객체에 액세스하게 하는 경우, 객체는 해당 사용자 또는 다른 엔티 티에 대해 \"가시적인\" 것으로 설명될 수 있다. 예로서, 제한 없이, 온라인 소셜 네트워크의 사용자는 사용자-프 로필 페이지의 업무-경험 정보에 액세스할 수 있는 사용자들 세트를 식별하는 사용자-프로필 페이지에 대한 프 라이버시 설정을 지정할 수 있어서, 다른 사용자들 해당 정보에 액세스하지 못하도록 한다. 특정 실시예들에서, 객체에 대한 프라이버시 설정들은 객체와 연관된 소정 정보에 액세스하는 것이 허용되지 않 아야 하는 사용자들 또는 다른 엔티티들의 \"차단된 목록\"을 지정할 수 있다. 특정 실시예들에서, 차단 목록은 제3자 엔티티들을 포함할 수 있다. 차단 목록은 객체가 보이지 않는 하나 이상의 사용자들 또는 엔티티들을 지 정할 수 있다. 예로서 제한 없이, 사용자는 사용자와 연관된 사진 앨범들에 액세스할 수 없는 사용자들 세트를 지정할 수 있으므로, (또한 가능하게 지정된 사용자들 세트 내에 있지 않은 소정 사용자들이 사진 앨범들에 액 세스할 수 있게 하면서) 해당 사용자들이 사진 앨범에 액세스하는 것을 제외할 수 있다. 특정 실시예들에서, 프 라이버시 설정들은 특정 소셜-그래프 요소들과 연관될 수 있다. 노드 또는 에지와 같은 소셜-그래프 요소의 프 라이버시 설정들은 소셜-그래프 요소, 소셜-그래프 요소와 연관된 정보, 또는 소셜-그래프 요소와 연관된 객체 들이 온라인 소셜 네트워크를 사용하여 액세스할 수 있는 방법을 지정할 수 있다. 예로서 제한 없이, 특정 사진 에 대응하는 특정 개념 노드는 사진에 태그된 사용자들 및 사진에 태그된 사용자들의 친구들만이 사진에 액세스 할 수 있음을 지정하는 프라이버시 설정을 가질 수 있다. 특정 실시예들에서, 프라이버시 설정들은 사용자들이 소셜-네트워킹 시스템에 의해 저장/로그인되거나 다른 시스템들(예를 들어, 제3자 시스템)과 공유되는 자신의 콘텐츠, 정보, 또는 행동들을 갖는 것을 옵트인(opt in) 또는 옵트아웃(opt out)하도록 허용할 수 있다. 본 개 시내용이 특정 방식으로 특정 프라이버시 설정들을 사용하는 것을 설명하지만, 본 개시내용은 임의의 적합한 방 식으로 임의의 적합한 프라이버시 설정들을 사용하는 것을 고려한다. 특정 실시예들에서, 프라이버시 설정들은 소셜 그래프의 하나 이상의 노드들 또는 에지들에 기반할 수 있다. 프 라이버시 설정은 소셜 그래프의 하나 이상의 에지들 또는 에지-유형들에 대해, 또는 소셜 그래프의 하나 이상의 노드들 또는 노드-유형들에 대해 지정될 수 있다. 2개의 노드들을 연결하는 특정 에지에 적용된 프라이버시 설 정들은 노드들에 해당하는 2개의 엔티티들 간의 관계가 온라인 소셜 네트워크의 다른 사용자들에게 보이는지 여 부를 제어할 수 있다. 유사하게, 특정 노드에 적용된 프라이버시 설정들은 노드에 대응하는 사용자 또는 개념이 온라인 소셜 네트워크의 다른 사용자들에게 보이는지 여부를 제어할 수 있다. 예로서 제한 없이, 제1 사용자는 소셜-네트워킹 시스템에 객체를 공유할 수 있다. 객체는 에지에 의해 제1 사용자의 사용자 노드에 연결된 개념 노드와 연관될 수 있다. 제1 사용자는 객체의 개념 노드에 연결되는 특정 에지에 적용되는 프라이버시 설정들을 지정하거나, 개념 노드에 연결되는 모든 에지들에 적용되는 프라이버시 설정들을 지정할 수 있다. 다른 예로서 제한 없이, 제1 사용자는 특정 객체-유형의 객체들 세트(예를 들어, 이미지들 세트)를 공유할 수 있다. 제1 사 용자는 특정 객체-유형의 제1 사용자와 연관된 모든 객체들에 대한 프라이버시 설정들을 특정 프라이버시 설정 을 갖는 것으로 지정할 수 있다(예를 들어, 제1 사용자가 게시한 모든 이미지들이 제1 사용자의 친구들 및/또는 이미지들에서 태깅된 사용자들에게만 보이도록 지정). 특정 실시예들에서, 소셜-네트워킹 시스템은 \"프라이버시 마법사\"(예를 들어, 웹페이지, 모듈, 하나 이상의 대 화 상자들, 또는 임의의 다른 적합한 인터페이스 내에서)를 제시하여 제1 사용자가 하나 이상의 프라이버시 설 정들을 지정하는 것을 도울 수 있다. 프라이버시 마법사는 명령들, 적합한 프라이버시-관련 정보, 현재 프라이 버시 설정들, 프라이버시 설정들의 변경 또는 확인을 지정하는 제1 사용자로부터의 하나 이상의 입력들을 수용 하기 위한 하나 이상의 입력 필드들, 또는 이들의 임의의 적절한 조합을 디스플레이할 수 있다. 특정 실시예들 에서, 소셜-네트워킹 시스템은 제1 사용자에게 제1 사용자의 현재 프라이버시 설정들을 디스플레이할 수 있는 \"대시보드\" 기능을 제1 사용자에게 제공할 수 있다. 대시보드 기능은 임의의 적절한 시간에 제1 사용자에게 디 스플레이될 수 있다(예를 들어, 대시보드 기능을 호출하는 제1 사용자로부터의 입력에 따라, 특정 이벤트 또는 트리거 동작의 발생에 따라). 대시보드 기능은 제1 사용자가 제1 사용자의 현재 프라이버시 설정들 중 하나 이 상을 언제든지 임의의 적합한 방식으로 수정하게 할 수 있다(예를 들어, 제1 사용자를 프라이버시 마법사로 재 지향). 객체와 연관된 프라이버시 설정들은 허용된 액세스 또는 액세스 거부의 임의의 적합한 입도를 지정할 수 있다. 예로서 제한 없이, 액세스 또는 액세스 거부는 특정 사용자들(예를 들어, 나만, 내 룸메이트들, 내 상사), 특정 정도의 분리 내 사용자들(예를 들어, 친구, 친구들의 친구들), 사용자 그룹들(예를 들어, 게임 클럽, 내 가족), 사용자 네트워크들(예를 들어, 특정 고용주들의 직원들, 학생들 또는 특정 대학 졸업생), 모든 사용자들(\"공 용\"), 사용자 없음(\"비공개\"), 제3자 시스템들의 사용자들, 특정 애플리케이션들(예를 들어, 제3자 애플리케이 션들, 외부 웹사이트들), 다른 적합한 엔티티들 또는 이들의 임의의 적합한 조합에 대해 지정될 수 있다. 본 개 시내용이 허용된 액세스 또는 액세스 거부의 특정 입도들을 설명하지만, 본 개시내용은 허용된 액세스 또는 액 세스 거부의 임의의 적합한 입도를 고려한다. 특정 실시예들에서, 하나 이상의 서버들은 프라이버시 설정들을 시행하기 위한 인가/프라이버시 서버들일 수 있 다. 데이터 저장소에 저장된 특정 객체에 대한 사용자(또는 다른 엔티티)의 요청에 응답하여, 소셜-네트워킹 시 스템은 객체에 대한 요청을 데이터 저장소에 전송할 수 있다. 요청은 요청과 연관된 사용자를 식별할 수 있고 객체는, 사용자가 객체와 연관된 프라이버시 설정들에 기반하여 객체에 액세스하도록 인가됨을 인가 서버가 결 정하는 경우 사용자(또는 사용자의 클라이언트 시스템)에게만 전송될 수 있다. 요청하는 사용자가 객체에 액세 스하도록 인가되지 않으면, 인가 서버는 요청된 객체가 데이터 저장소에서 검색되는 것을 방지하거나 요청된 객 체가 사용자에게 전송되는 것을 방지할 수 있다. 검색-질의 컨텍스트에서, 질의하는 사용자가 객체에 액세스하 도록 인가되는 경우, 예를 들어, 객체에 대한 프라이버시 설정들이 질의 사용자에게 표시되거나, 발견되거나, 다른 방식으로 보이도록 허용하는 경우에만 객체는 검색 결과로 제공될 수 있다. 특정 실시예들에서, 객체는 사 용자의 뉴스피드를 통해 사용자에게 보이는 콘텐츠를 나타낼 수 있다. 예로서 제한 없이, 하나 이상의 객체들은 사용자의 \"트렌딩\" 페이지에 보여질 수 있다. 특정 실시예들에서, 객체는 특정 사용자에게 대응할 수 있다. 객 체는 특정 사용자와 연관된 콘텐츠일 수 있거나, 소셜-네트워킹 시스템 또는 다른 컴퓨팅 시스템에 저장된 특정 사용자의 계정 또는 정보일 수 있다. 예로서 제한 없이, 제1 사용자는 온라인 소셜 네트워크의 \"People You May Know\" 기능을 통해, 또는 제1 사용자의 친구들 목록을 봄으로써 온라인 소셜 네트워크의 한 명 이상의 제2 사용 자를 볼 수 있다. 예로서 제한 없이, 제1 사용자는 자신의 뉴스피드 또는 친구들 목록에서 특정 제2 사용자와 연관된 객체들을 보고 싶지 않다고 지정할 수 있다. 객체에 대한 프라이버시 설정들이 사용자에게 표시되거나 발견되거나 볼 수 없게 하는 경우, 객체는 검색 결과들에서 제외될 수 있다. 본 개시내용이 특정 방식으로 프라 이버시 설정들을 시행하는 것을 설명하지만, 본 개시내용은 임의의 적합한 방식으로 프라이버시 설정들을 시행하는 것을 고려한다. 특정 실시예들에서, 사용자와 연관된 동일한 유형의 다른 객체들은 상이한 프라이버시 설정들을 가질 수 있다. 사용자와 연관된 상이한 유형들의 객체는 상이한 유형들의 프라이버시 설정들을 가질 수 있다. 예로서 제한 없 이, 제1 사용자는 제1 사용자의 상태 업데이트들이 공개되도록 지정할 수 있지만, 제1 사용자가 공유한 모든 이 미지들은 온라인 소셜 네트워크에서 제1 사용자의 친구들에게만 보인다. 다른 예로서 제한 없이, 사용자는 개별 사용자들, 친구들의 친구들, 팔로워들, 사용자 그룹들 또는 기업 엔티티들과 같은 상이한 유형들의 엔티티들에 대해 상이한 프라이버시 설정들을 지정할 수 있다. 다른 예로서 제한 없이, 제1 사용자는 제1 사용자의 고용주 에게 비디오가 보이도록 유지하면서 제1 사용자가 제1 사용자에 의해 포스팅된 비디오들을 볼 수 있는 사용자들 그룹을 지정할 수 있다. 특정 실시예들에서, 상이한 사용자 그룹들 또는 사용자 인구통계에 대해 상이한 프라이 버시 설정들이 제공될 수 있다. 예로서 제한 없이, 제1 사용자는 제1 사용자와 같은 대학에 다니는 다른 사용자 들이 제1 사용자의 사진들을 볼 수 있지만, 제1 사용자의 가족 구성원들인 다른 사용자는 이러한 동일한 사진들 을 볼 수 없도록 지정할 수 있다. 특정 실시예들에서, 소셜-네트워킹 시스템은 특정 객체-유형의 각 객체에 대한 하나 이상의 디폴트 프라이버시 설정들을 제공할 수 있다. 디폴트로 설정된 객체에 대한 프라이버시 설정은 해당 객체와 연관된 사용자가 변경 할 수 있다. 예로서 제한 없이, 제1 사용자에 의해 포스팅된 모든 이미지들은 제1 사용자의 친구들에게만 보이 는 디폴트 프라이버시 설정을 가질 수 있고, 특정 이미지에 대해 제1 사용자는 친구들 및 친구들의 친구들에게 보이도록 이미지에 대한 프라이버시 설정을 변경할 수 있다. 특정 실시예들에서, 프라이버시 설정들은 소셜-네트워킹 시스템이 임의의 목적을 위해 사용자와 연관된 특정 객 체들 또는 정보를 수신, 수집, 기록 또는 저장할 수 있는지 여부를 제1 사용자가 (예를 들어, 옵트아웃하여, 옵 트인하지 않음으로써) 지정할 수 있게 한다. 특정 실시예들에서, 프라이버시 설정들은 제1 사용자가, 특정 애플 리케이션들 또는 프로세스들이 사용자와 연관된 특정 객체들 또는 정보에 액세스, 저장 또는 사용할 수 있는지 여부를 지정하게 할 수 있다. 프라이버시 설정들은 제1 사용자가, 객체들 또는 정보가 특정 애플리케이션들 또 는 프로세스들에 의해 액세스, 저장 또는 사용되도록 옵트인 또는 옵트아웃하게 할 수 있다. 소셜-네트워킹 시 스템은 소셜-네트워킹 시스템이 임의의 다른 목적들을 위해 해당 정보에 액세스하지 않고도 제1 사용자에게 특 정 기능 또는 서비스를 제공하기 위해 이러한 정보에 액세스할 수 있다. 이러한 객체들 또는 정보에 액세스, 저 장 또는 사용하기 전에, 소셜-네트워킹 시스템은 사용자에게 임의의 이러한 동작을 허용하기 전에, 존재한다면 애플리케이션들 또는 프로세스들이 객체 또는 정보에 액세스, 저장, 또는 사용할 수 있는 것을 지정하는 프라이 버시 설정들을 제공하도록 그러한 객체 또는 정보를 허용하기 전에 객체 또는 정보에 액세스, 저장 또는 사용할 수 있는 애플리케이션 또는 프로세스(있는 경우)를 지정하는 프라이버시 설정을 제공하도록 촉발할 수 있다. 예 로서 제한 없이, 제1 사용자는 온라인 소셜 네트워크와 관련된 애플리케이션(예를 들어, 메시징 앱)을 통해 제2 사용자에게 메시지를 송신할 수 있고, 이러한 메시지들이 소셜-네트워킹 시스템에 의해 저장되지 않아야 하는 프라이버시 설정들을 지정할 수 있다. 특정 실시예들에서, 사용자는 제1 사용자와 연관된 특정 유형들의 객체들 또는 정보가 소셜-네트워킹 시스템에 의해 액세스, 저장 또는 사용될 수 있는지 여부를 지정할 수 있다. 예로서 제한 없이, 제1 사용자는 소셜-네트 워킹 시스템을 통해 제1 사용자에 의해 전송된 이미지들이 소셜-네트워킹 시스템에 의해 저장되지 않을 수 있음 을 지정할 수 있다. 다른 예로서 제한 없이, 제1 사용자는 제1 사용자로부터 특정 제2 사용자에게 전송된 메시 지들이 소셜-네트워킹 시스템에 의해 저장되지 않을 수 있음을 지정할 수 있다. 제한이 아닌 또 다른 예로서, 제1 사용자는 특정 애플리케이션을 통해 전송된 모든 객체들이 소셜-네트워킹 시스템에 의해 저장될 수 있음을 지정할 수 있다. 특정 실시예들에서, 프라이버시 설정들은 제1 사용자와 연관된 특정 객체들 또는 정보가 특정 클라이언트 시스 템들 또는 제3자 시스템들로부터 액세스될 수 있는지 여부를 제1 사용자가 지정하게 할 수 있다. 프라이버시 설 정들은 제1 사용자가 특정 디바이스(예를 들어, 사용자 스마트폰의 전화번호부), 특정 애플리케이션(예를 들어, 메시징 앱), 또는 특정 시스템(예를 들어, 이메일 서버)에서 객체들 또는 정보가 액세스되도록 옵트인 또는 옵 트아웃하게 할 수 있다. 소셜-네트워킹 시스템은 각 디바이스, 시스템 또는 애플리케이션에 대한 디폴트 프라이 버시 설정들을 제공할 수 있고/있거나, 제1 사용자는 각 컨텍스트에 대한 특정 프라이버시 설정을 지정하도록 촉발될 수 있다. 예로서 제한 없이, 제1 사용자는 소셜-네트워킹 시스템의 위치-서비스 피처(feature)를 활용하 여 사용자에게 근접한 레스토랑들 또는 다른 장소들에 대한 추천을 제공할 수 있다. 제1 사용자의 디폴트 프라 이버시 설정들은 소셜-네트워킹 시스템이 위치-기반 서비스들을 제공하기 위해 제1 사용자의 클라이언트 디바이 스에서 제공된 위치 정보를 사용할 수 있지만, 소셜-네트워킹 시스템이 제1 사용자의 위치 정보를 저장하거나이를 임의의 제3자 시스템에 제공할 수 없음을 지정할 수 있다. 이어서, 제1 사용자는 위치 정보가 사진들을 지 오-태깅(geo-tag)하기 위해 제3자 이미지-공유 애플리케이션에 의해 사용되게 하도록 프라이버시 설정들을 업데 이트할 수 있다."}
{"patent_id": "10-2022-7020776", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1a는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 예시적인 시스템을 묘사하는 예시이다. 도 1b는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 예시적인 시스템을 묘사하는 예시이다. 도 1c는 본 개시내용의 기법들에 따른 스피치 전사들을 수행하는 예시적인 시스템을 묘사하는 예시이다. 도 2a는 본 개시내용의 기법들에 따른 예시적인 HMD를 묘사하는 예시이다. 도 2b는 본 개시내용의 기법들에 따른 예시적인 HMD를 묘사하는 예시이다. 도 3은 본 개시내용의 기법들에 따른, 도 1a, 도 1b의 인공 현실 시스템들의 HMD의 예시적인 인스턴스에 의해 스피치 전사가 수행되는 예를 묘사하는 블록도이다. 도 4는 본 개시내용의 기법들에 따른, 도 1a, 도 1b의 인공 현실 시스템의 HMD 및 전사 시스템의 예시적인 인스 턴스들에 의해 스피치 전사가 수행되는 예시적인 구현들을 도시하는 블록도이다. 도 5는 본 개시내용의 기법들에 따른, 도 1c의 시스템의 컴퓨팅 디바이스의 예시적인 인스턴스에 의해 스피치 전사가 수행되는 예시적인 구현들을 도시하는 블록도이다. 도 6은 본 개시내용의 양태들에 따른 스피치를 전사하고 분석하기 위한 방법의 예시적인 동작들을 예시하는 흐 름도이다. 도 7은 본 개시내용의 기법들에 따른 오디오 데이터 및 전사를 예시한다. 도 8은 본 개시내용의 양태들에 따른 스피치를 전사하기 위한 방법의 예시적인 동작들을 예시하는 흐름도이다. 도 9는 본 개시내용의 양태들에 따른 스피치 세그먼트의 화자를 식별하기 위한 방법의 예시적인 동작들을 예시 하는 흐름도이다. 도 10은 본 개시내용의 양태들에 따른 잠재적인 화자 모델들을 식별하기 위한 방법의 예시적인 동작들을 예시하 는 흐름도이다. 도 11은 본 개시내용의 양태들에 따른 분산 디바이스들에 대한 스피치를 전사하기 위한 방법의 예시적인 동작들 을 예시하는 흐름도이다. 유사한 참조 문자들은 도면들 및 설명 전반에 걸쳐 유사한 요소들을 지칭한다."}
