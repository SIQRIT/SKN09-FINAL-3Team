{"patent_id": "10-2019-0156162", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0066653", "출원번호": "10-2019-0156162", "발명의 명칭": "전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "김동현"}}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서, 영상의 업스케일링(upscaling) 정보를 획득하도록 학습된 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득하고, 상기 획득된 업스케일링 정보에 기초하여 상기 입력 영상을 다운스케일링(downscaling)하고, 출력 해상도에 기초하여 상기 다운스케일링된 영상을 업스케일링하여 출력 영상을 획득하는 프로세서;를 포함하는 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 획득된 업스케일링 정보는, 상기 입력 영상의 업스케일링 비율 정보 또는 상기 입력 영상의 원본 해상도 정보 중 적어도 하나를 포함하는,전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 프로세서는, 상기 다운 스케일링된 영상에 대해 제1 화질 처리를 수행하고, 상기 제1 화질 처리된 영상을 업스케일링하며,상기 업스케일링된 영상에 대해 제2 화질 처리를 수행하여 상기 출력 영상을 획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서, 상기 프로세서는,초해상도 처리(super resolution)를 수행하도록 학습된 제2 인공 지능 모델을 이용하여 상기 다운스케일링된 영상을 업스케일링하여 상기 출력 영상을 획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 제1 인공 지능 모델은, 상기 영상의 특성 정보에 기초하여 상기 영상의 업스케일링 정보를 획득하도록 학습된 모델이며, 상기 프로세서는, 상기 입력 영상으로부터 획득된 특성 정보를 상기 제2 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링정보를 획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 프로세서는, 상기 입력 영상에서 에지 영역을 식별하고, 상기 식별된 에지 영역에 포함된 픽셀에 대한 특성 정보를공개특허 10-2021-0066653-3-획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 프로세서는, 상기 식별된 에지 영역을 블러링 처리하고, 상기 블러링된 에지 영역에 포함된 중심 픽셀을 기준으로 설정된 마진(margin) 영역 외의 픽셀에 대한 상기 특성 정보를 획득하고, 상기 획득된 특성 정보를 상기 제1 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링 정보를 획득하는,전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6항에 있어서,상기 프로세서는, 상기 입력 영상에서 관심 영역을 식별하고, 상기 식별된 관심 영역에 포함된 상기 에지 영역을 식별하는, 전자장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 제1 인공 지능 모델은, 상기 영상 또는 상기 영상의 에지 영역에 기초하여 상기 영상의 업스케일링 정보를 식별하도록 학습된모델이며, 상기 프로세서는, 상기 입력 영상 또는 상기 입력 영상의 에지 영역을 포함하는 영상을 상기 제1 인공 지능 모델에 입력하여 상기입력 영상의 업스케일링 정보를 획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제1항에 있어서, 상기 프로세서는, 상기 입력 영상의 씬이 변경되는 시점에 기초하여 상기 입력 영상의 다운스케일링 여부를 결정하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서, 상기 프로세서는, 상기 입력 영상의 제1 씬 구간에서 업스케일링 정보가 획득되는 경우, 상기 제1 씬 구간 이후의 제2 씬 구간에포함된 프레임부터 상기 획득된 업스케일링 정보에 기초하여 다운스케일링하고, 상기 다운스케일링된 프레임을업스케일링하여 출력 프레임을 획득하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서는, 상기 입력 영상의 기설정된 프레임 구간 단위로 업스케일링 정보를 획득하고, 상기 제1 씬 구간 이후의 임계 개수의 씬 구간에서 동일한 업스케일링 정보가 유지되는 경우 상기 임계 개수의공개특허 10-2021-0066653-4-씬 구간 이후의 상기 제2 씬 구간이 시작되는 프레임부터 다운스케일링하는, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제1항에 있어서, 상기 제1 인공 지능 모델은, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처리 여부, 압축 및 업스케일링 간 순서 또는 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정보에 기초하여 학습된 인공 지능 모델인, 전자 장치."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "전자 장치의 제어 방법에 있어서,영상의 업스케일링(upscaling) 정보를 획득하도록 학습된 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득하는 단계; 상기 획득된 업스케일링 정보에 기초하여 상기 입력 영상을 다운스케일링(downscaling)하는 단계; 및 출력 해상도에 기초하여 상기 다운스케일링된 영상을 업스케일링하여 출력 영상을 획득하는 단계;를 포함하는제어 방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 획득된 업스케일링 정보는, 상기 입력 영상의 업스케일링 비율 정보 또는 상기 입력 영상의 원본 해상도 정보 중 적어도 하나를 포함하는,제어 방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제14항에 있어서, 상기 출력 영상을 획득하는 단계는, 상기 다운 스케일링된 영상에 대해 제1 화질 처리를 수행하는 단계; 상기 제1 화질 처리된 영상을 업스케일링하는 단계; 및 상기 업스케일링된 영상에 대해 제2 화질 처리를 수행하여 상기 출력 영상을 획득하는 단계;를 포함하는, 제어방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제14항에 있어서, 상기 출력 영상을 획득하는 단계는, 초해상도 처리(super resolution)를 수행하도록 학습된 제2 인공 지능 모델을 이용하여 상기 다운스케일링된 영상을 업스케일링하여 상기 출력 영상을 획득하는, 제어 방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제14항에 있어서, 상기 제1 인공 지능 모델은, 상기 영상의 특성 정보에 기초하여 상기 영상의 업스케일링 정보를 획득하도록 학습된 모델이며, 상기 입력 영상의 업스케일링 정보를 획득하는 단계는, 상기 입력 영상으로부터 획득된 특성 정보를 상기 제2 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링공개특허 10-2021-0066653-5-정보를 획득하는, 제어 방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제14항에 있어서, 상기 입력 영상을 다운스케일링하는 단계는,상기 입력 영상의 씬이 변경되는 시점에 기초하여 상기 입력 영상의 다운스케일링 여부를 결정하는, 제어 방법."}
{"patent_id": "10-2019-0156162", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제14항에 있어서, 상기 제1 인공 지능 모델은, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처리 여부, 압축 및 업스케일링 간 순서 또는 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정보에 기초하여 학습된 인공 지능 모델인, 제어 방법."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 전자 장치는, 영상의 업스케일링(upscaling) 정보를 획득하도록 학습된 제1 인공 지능 모 델을 이용하여 입력 영상의 업스케일링 정보를 획득하고, 획득된 업스케일링 정보에 기초하여 입력 영상을 다운 스케일링(downscaling)하고, 출력 해상도에 기초하여 다운스케일링된 영상을 업스케일링하여 출력 영상을 획득하 는 프로세서를 포함한다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 전자 장치 및 그 제어 방법에 관한 것으로, 더욱 상세하게는 인공 지능 모델을 이용하여 영상 처리를 수행하는 전자 장치 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "전자 기술의 발달에 힘입어 다양한 유형의 전자 기기가 개발 및 보급되고 있다. 특히, 가정, 사무실, 공공 장소 등 다양한 장소에서 이용되는 디스플레이 장치는 최근 수년 간 지속적으로 발전하고 있다. 최근에는 고해상도 영상 서비스에 대한 요구가 크게 증가하고 있다. 이에 따라 고해상도 영상 지원이 가능한 TV 와 같은 디스플레이 장치의 보급이 확대됨에 따라 STB, 공중파 등 다양한 환경에서 자체적으로 영상의 업스케일 링을 수행하여 TV와 같은 디스플레이 장치로 전송하는 경우가 증가하고 있다. 하지만, 다양한 업스케일링 기술 및 방송 환경에 따라 업스케일링된 영상들이 상이한 화질을 가지며, TV에 구비 된 업스케일링 기술 대비 열세한 성능을 보인다는 문제점이 있다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 상술한 필요성에 따른 것으로, 본 개시의 목적은 외부에서 업스케일링된 영상의 원본 해상도를 추정 하여 다운스케일링한 후, 고성능의 업스케일링 기술을 이용하여 업스케일링함으로써 고화질의 영상을 제공하는 전자 장치 및 그 제어 방법을 제공함에 있다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "이상과 같은 목적을 달성하기 위한 일 실시 예에 따른 전자 장치는, 영상의 업스케일링(upscaling) 정보를 획득 하도록 학습된 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득하고, 상기 획득된 업스케일 링 정보에 기초하여 상기 입력 영상을 다운스케일링(downscaling)하고, 출력 해상도에 기초하여 상기 다운스케 일링된 영상을 업스케일링하여 출력 영상을 획득하는 프로세서를 포함한다. 여기서, 상기 획득된 업스케일링 정보는, 상기 입력 영상의 업스케일링 비율 정보 또는 상기 입력 영상의 원본 해상도 정보 중 적어도 하나를 포함할 수 있다. 또한, 상기 프로세서는, 상기 다운 스케일링된 영상에 대해 제1 화질 처리를 수행하고, 상기 제1 화질 처리된 영상을 업스케일링하며, 상기 업스케일링된 영상에 대해 제2 화질 처리를 수행하여 상기 출력 영상을 획득할 수 있다. 또한, 상기 프로세서는, 초해상도 처리(super resolution)를 수행하도록 학습된 제2 인공 지능 모델을 이용하여 상기 다운스케일링된 영상을 업스케일링하여 상기 출력 영상을 획득할 수 있다. 여기서, 상기 제1 인공 지능 모델은, 상기 영상의 특성 정보에 기초하여 상기 영상의 업스케일링 정보를 획득하 도록 학습된 모델이며, 상기 프로세서는, 상기 입력 영상으로부터 획득된 특성 정보를 상기 제2 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링 정보를 획득할 수 있다. 또한, 상기 프로세서는, 상기 입력 영상에서 에지 영역을 식별하고, 상기 식별된 에지 영역에 포함된 픽셀에 대 한 특성 정보를 획득할 수 있다. 또한, 상기 프로세서는, 상기 식별된 에지 영역을 블러링 처리하고, 상기 블러링된 에지 영역에 포함된 중심 픽 셀을 기준으로 설정된 마진(margin) 영역 외의 픽셀에 대한 상기 특성 정보를 획득하고, 상기 획득된 특성 정보 를 상기 제1 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링 정보를 획득할 수 있다. 또한, 상기 프로세서는, 상기 입력 영상에서 관심 영역을 식별하고, 상기 식별된 관심 영역에 포함된 상기 에지 영역을 식별할 수 있다. 또한, 상기 제1 인공 지능 모델은, 상기 영상 또는 상기 영상의 에지 영역에 기초하여 상기 영상의 업스케일링 정보를 식별하도록 학습된 모델이며, 상기 프로세서는, 상기 입력 영상 또는 상기 입력 영상의 에지 영역을 포 함하는 영상을 상기 제1 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링 정보를 획득할 수 있다. 또한, 상기 프로세서는, 상기 입력 영상의 씬이 변경되는 시점에 기초하여 상기 입력 영상의 다운스케일링 여부 를 결정할 수 있다. 또한, 상기 프로세서는, 상기 입력 영상의 제1 씬 구간에서 업스케일링 정보가 획득되는 경우, 상기 제1 씬 구 간 이후의 제2 씬 구간에 포함된 프레임부터 상기 획득된 업스케일링 정보에 기초하여 다운스케일링하고, 상기 다운스케일링된 프레임을 업스케일링하여 출력 프레임을 획득할 수 있다. 또한, 상기 프로세서는, 상기 입력 영상의 기설정된 프레임 구간 단위로 업스케일링 정보를 획득하고, 상기 제1 씬 구간 이후의 임계 개수의 씬 구간에서 동일한 업스케일링 정보가 유지되는 경우 상기 임계 개수의 씬 구간 이후의 상기 제2 씬 구간이 시작되는 프레임부터 다운스케일링할 수 있다. 또한, 상기 제1 인공 지능 모델은, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처 리 여부, 압축 및 업스케일링 간 순서 또는 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정 보에 기초하여 학습된 인공 지능 모델일 수 있다. 한편, 일 실시 예에 따른 전자 장치의 제어 방법은, 영상의 업스케일링(upscaling) 정보를 획득하도록 학습된 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득하는 단계, 상기 획득된 업스케일링 정보에 기초하여 상기 입력 영상을 다운스케일링(downscaling)하는 단계 및, 출력 해상도에 기초하여 상기 다운스케일 링된 영상을 업스케일링하여 출력 영상을 획득하는 단계를 포함할 수 있다. 여기서, 상기 획득된 업스케일링 정보는, 상기 입력 영상의 업스케일링 비율 정보 또는 상기 입력 영상의 원본 해상도 정보 중 적어도 하나를 포함할 수 있다. 또한, 상기 출력 영상을 획득하는 단계는, 상기 다운 스케일링된 영상에 대해 제1 화질 처리를 수행하는 단계, 상기 제1 화질 처리된 영상을 업스케일링하는 단계 및, 상기 업스케일링된 영상에 대해 제2 화질 처리를 수행하 여 상기 출력 영상을 획득하는 단계를 포함할 수 있다. 또한, 상기 출력 영상을 획득하는 단계는, 초해상도 처리(super resolution)를 수행하도록 학습된 제2 인공 지 능 모델을 이용하여 상기 다운스케일링된 영상을 업스케일링하여 상기 출력 영상을 획득할 수 있다. 여기서, 상기 제1 인공 지능 모델은, 상기 영상의 특성 정보에 기초하여 상기 영상의 업스케일링 정보를 획득하 도록 학습된 모델이며, 상기 입력 영상의 업스케일링 정보를 획득하는 단계는, 상기 입력 영상으로부터 획득된 특성 정보를 상기 제2 인공 지능 모델에 입력하여 상기 입력 영상의 업스케일링 정보를 획득할 수 있다. 또한, 상기 입력 영상을 다운스케일링하는 단계는, 상기 입력 영상의 씬이 변경되는 시점에 기초하여 상기 입력 영상의 다운스케일링 여부를 결정할 수 있다. 또한, 상기 제1 인공 지능 모델은, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처 리 여부, 압축 및 업스케일링 간 순서 또는 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정 보에 기초하여 학습된 인공 지능 모델일 수 있다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "상술한 다양한 실시 예에 따르면, 외부에서 업스케일링된 영상의 원본 해상도를 추정하여 다운스케일링한 후, 고성능의 업스케일링 기술을 이용하여 업스케일링함으로써 고화질의 영상을 제공할 수 있게 된다. 또한, 업스케일링된 영상에 대해 하드웨어 비용의 이유로 수행하기 어려운 화질 처리를 다운스케일링된 영상에 대해 수행함으로써, 낮은 복잡도의 하드웨어를 이용할 수 있고 그에 따라 하드웨어 구현 비용을 저감시킬 수 있 는 효과가 있다."}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 명세서에서 사용되는 용어에 대해 간략히 설명하고, 본 개시에 대해 구체적으로 설명하기로 한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 구성요소들은 용어들에 의해 한정되 어서는 안 된다. 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. A 또는 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 아래에서는 첨부한 도면을 참고하여 본 개시의 실시 예에 대하여 본 개시가 속하는 기술 분야에서 통상의 지식 을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 그러나 본 개시는 여러 가지 상이한 형태로 구현될 수 있으며 여기에서 설명하는 실시 예에 한정되지 않는다. 그리고 도면에서 본 개시를 명확하게 설명하기 위해 서 설명과 관계없는 부분은 생략하였으며, 명세서 전체를 통하여 유사한 부분에 대해서는 유사한 도면 부호를 붙였다. 도 1은 본 개시의 이해를 돕기 위한 전자 장치의 화질 처리 동작을 설명하기 위한 도면이다. 도 1에 도시된 전자 장치의 화질 처리 동작에 따르면, 입력 영상의 해상도의 기초하여 서로 다른 화질 처리 패 스를 적용하게 된다. 여기서, 입력 영상의 해상도는 720x480의 SD(Standard Definition), 1280x720의 HD(High Definition), 1920x1080의 FHD(Full High Definition), 2560x1440의 QHD(Quad High Definition), 3840x2160의 4K UHD(Ultra High Definition), 7680x4320의 8K UHD(Ultra High Definition) 뿐 아니라, 그 이상의 해상도(예 를 들어, 16K, 32K)를 포함할 수 있다. 일 실시 예에 따라 전자 장치는 임계 해상도 미만의 영상이 입력되면, 해당 영상을 입력 해상도 수준에서 제1 화질 처리한 후, 제1 화질 처리된 영상을 출력 해상도, 예를 들어 최대 출력 해상도로 업스케일링 처리 하고, 업스케일링된 영상을 제2 화질 처리하여 출력 영상을 획득할 수 있다. 다만, 전자 장치는 임계 해상도 이상의 영상이 입력되면, 해상 영상을 제1 화질 처리한 후, 제1 화질 처리된 영상을 제2 화질 처리하여 출력 영상을 획득할 수 있다. 여기서, 임계 해상도는 전자 장치의 최대 출력 해상도에 기초하여 결정될 수 있다. 예를 들어 전자 장치가 4K UHD가 최대 출력 해상도인 TV로 구현되는 경우 임계 해상도는 FHD 또는 QHD로 결정될 수 있다. 예를 들어, 4K UHD가 최대 출력 해상도인 TV는 FHD 영상이 입력되면, 이를 4K UHD로 업스케일링 처리하지만, 4K UHD 영상이 입 력되면 별도의 업스케일링 처리를 하지 않게 된다. 즉, 상술한 실시 예에 따르면, 전자 장치는 임계 해상도 이상의 영상에 대해서는 별도의 업스케일링 처리를 하지 않을 수 있다. 예를 들어, 입력 영상이 출력 해상도의 영상인 경우 출력을 위한 별도의 업스케일링 처리가 필요하지 않기 때문이며, 고해상도 영상의 화질을 개선하기 위한 하드웨어(예를 들어, ASIC)의 고속 동작을 위 해서는 고비용 연산기가 사용되어야 하며 상당히 큰 용량의 내부 버퍼를 요구할 수 있기 때문이다. 예를 들어, 입력 영상의 해상도와 동일한 라인 버퍼가 구현되어야 하므로 예를 들어, 4K UHD 입력 영상인 경우 SD 입력 영 상보다 3840/720 = 5.33 배 긴 라인 버퍼가, 8K UHD 입력 영상인 경우 SD 입력 영상보다 7680/720 = 10.66 배 긴 라인 버퍼가 요구될 수 있다. 또한 화질 개선 IP(Intellectual Property) 즉, 화질 개선을 위한 기능 블럭들 은 원본 해상도의 영상이 입력되었을 때 최적의 화질을 출력하도록 설계되었으며, 화질 레지스터 값들이 이미 설정되어 있기 때문에 이미 업스케일링 처리된 영상에 대해서는 최적의 화질 개선을 보장할 수 없게 된다. 이에 따라 방송국 또는 Set-top box 등 외부 장치에서 일반적인 업스케일링 기술을 통해 생성된 저화질의 임계 해상 도 이상(예를 들어, 4K 또는 8K 해상도)의 영상이 전자 장치로 입력되면, 전자 장치에 구현된 업스케일링 처리 를 위한 기능 블럭을 거치지 않게 되므로 저화질의 출력 영상을 제공할 수 밖에 없게 된다. 이에 따라 임계 해상도 이상의 영상이 전자 장치로 입력되는 경우라도, 전자 장치에 구현된 업스케일링 처리 를 위한 기능 블럭을 이용하여 고화질의 출력 영상을 제공하는 본 개시의 다양한 실시 예에 대해 설명하도록 한다. 도 2a 및 도 2b는 일 실시 예에 따른 전자 장치의 구성을 나타내는 블럭도들이다. 도 2a에 따르면, 전자 장치(100')는 프로세서를 포함하고, 도 2b에 따르면, 전자 장치는 프로세서 및 메모리를 포함한다. 일 실시 예에 따른 인공 지능 모델에 관한 정보가 프로세서 내부 메모 리에 저장되거나, 외부 메모리, 즉 메모리에 저장될 수 있으므로, 도 2a 및 도 2b를 구분하여 도시하였으 나, 이하에서는, 도 2b를 기준으로 설명하도록 한다. 전자 장치는 TV 또는 set-top box 로 구현될 수 있으나, 이에 한정되는 것은 아니며 스마트 폰, 태블릿 PC, 노트북 PC, HMD(Head mounted Display), NED(Near Eye Display), LFD(large format display), Digital Signage(디지털 간판), DID(Digital Information Display), 비디오 월(video wall), 프로젝터 디스플레이, 카 메라, 캠코더, 프린터, 서버 등으로 구현될 수 있다. 또는 전자 장치는 클라우딩 컴퓨팅 환경이 구축된 시 스템 자체일 수도 있다. 이에 한정되는 것은 아니며, 인공 지능 모델을 이용하여 데이터를 처리하는 장치라면 한정되지 않고 적용 가능하다. 일 예에 따라 전자 장치는 다양한 해상도의 영상, 다양한 압축 영상을 수신할 수 있다. 예를 들어, 전자 장치는 SD(Standard Definition), HD(High Definition), FHD, UHD 영상, UHD 이상의 해상도 영상 중 적어 도 하나의 영상을 수신할 수 있다. 또한, 전자 장치는 MPEG(Moving Picture Experts Group)(예를 들어, MP2, MP4, MP7 등), JPEG(joint photographic coding experts group), AVC(Advanced Video Coding), H.264, H.265, HEVC(High Efficiency Video Codec), VC-1, VP8, VP9 및 AV1(AOMedia Video 1) 등으로 압축된 형태로 영상을 수신할 수 있다. 메모리는 본 개시의 다양한 실시 예를 위해 필요한 데이터를 저장할 수 있다. 메모리는 데이터 저장 용도에 따라 전자 장치에 임베디드된 메모리 형태로 구현되거나, 전자 장치에 탈부착이 가능한 메모 리 형태로 구현될 수도 있다. 예를 들어, 전자 장치의 구동을 위한 데이터의 경우 전자 장치에 임베 디드된 메모리에 저장되고, 전자 장치의 확장 기능을 위한 데이터의 경우 전자 장치에 탈부착이 가능 한 메모리에 저장될 수 있다. 한편, 전자 장치에 임베디드된 메모리의 경우 휘발성 메모리(예: DRAM(dynamic RAM), SRAM(static RAM), 또는 SDRAM(synchronous dynamic RAM) 등), 비휘발성 메모리(non- volatile Memory)(예: OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메 모리(예: NAND flash 또는 NOR flash 등), 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 중 적어도 하나로 구현될 수 있다. 또한, 전자 장치에 탈부착이 가능한 메모리의 경우 메모리 카드(예를 들어, CF(compact flash), SD(secure digital), Micro-SD(micro secure digital), Mini-SD(mini secure digital), xD(extreme digital), MMC(multi-media card) 등), USB 포트에 연결가능한 외부 메모리(예를 들어, USB 메모리) 등과 같은 형태로 구현될 수 있다. 일 예에 따라 메모리는 전자 장치를 제어하기 위한 적어도 하나의 인스트럭션(instruction) 또는 인 스트럭션들을 포함하는 컴퓨터 프로그램을 저장할 수 있다. 다른 예에 따라, 메모리는 복수의 레이어를 포함하는 인공 지능 모델에 관한 정보를 저장할 수 있다. 여기 서, 인공 지능 모델에 관한 정보를 저장한다는 것은 인공 지능 모델의 동작과 관련된 다양한 정보, 예를 들어 인공 지능 모델에 포함된 복수의 레이어에 대한 정보, 복수의 레이어 각각에서 이용되는 파라미터(예를 들어, 필터 계수, 바이어스 등)에 대한 정보 등을 저장한다는 것을 의미할 수 있다. 예를 들어, 메모리는 일 실 시 예에 따라 입력 영상의 업스케일링 정보를 획득하도록 학습된 제1 인공 지능 모델에 대한 정보를 저장할 수 있다. 또한, 메모리는 일 실시 예에 따라 영상을 업스케일링 처리하도록 학습된 제2 인공 지능 모델에 대 한 정보를 저장할 수 있다. 여기서, 업스케일링 처리는 예를 들어, 초해상도(super resolution) 처리를 포함할 수 있다. 다만, 프로세서가 인공 지능 모델 전용 하드웨어로 구현되는 경우, 인공 지능 모델에 관한 정보 는 프로세서 내부 메모리에 저장될 수도 있다. 또 다른 예에 따라, 메모리는 외부 장치(예를 들어, 소스 장치), 외부 저장 매체(예를 들어, USB), 외부 서버(예를 들어 웹 하드) 등으로부터 수신된 영상을 저장할 수 있다. 여기서, 영상은 디지털 동영상이 될 수 있 으나 이에 한정되는 것은 아니다. 또 다른 예에 따라, 메모리는 화질 처리에 필요한 다양한 정보, 예를 들어 Noise Reduction, Detail Enhancement, Tone Mapping, Contrast Enhancement, Color Enhancement 또는 Frame rate Conversion 중 적어 도 하나를 수행하기 위한 정보, 알고리즘, 화질 파라미터 등을 저장할 수 있다. 또한, 메모리는 영상 처리 에 의해 생성된 최종 출력 영상을 저장할 수도 있다. 일 실시 예에 따르면, 메모리는 본 개시에 따른 다양한 동작들에서 생성되는 데이터를 저장하는 단일 메모 리로 구현될 수 있다. 다만, 다른 실시 예에 따르면, 메모리는 상이한 타입의 데이터를 각각 저장하거나, 상이한 단계에서 생성되는 데이터를 각각 저장하는 복수의 메모리를 포함하도록 구현될 수도 있다. 상술한 실시 예에서는 다양한 데이터가 프로세서의 외부 메모리에 저장되는 것으로 설명하였으나, 상 술한 데이터 중 적어도 일부는 전자 장치 또는 프로세서 중 적어도 하나의 구현 예에 따라 프로세서 내부 메모리에 저장될 수도 있다. 프로세서는 메모리와 전기적으로 연결되어 전자 장치의 전반적인 동작을 제어한다. 프로세서 는 하나 또는 복수의 프로세서로 구성될 수 있다. 구체적으로, 프로세서는 메모리에 저장된 적 어도 하나의 인스트럭션(instruction)을 실행함으로써, 본 개시의 다양한 실시 예에 따른 전자 장치의 동 작을 수행할 수 있다. 일 실시 예에 따라 프로세서는 디지털 영상 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP), 마이크로 프로세서(microprocessor), GPU(Graphics Processing Unit), AI(Artificial Intelligence) 프로세서, NPU (Neural Processing Unit), TCON(Time controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), 또 는 커뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, ASIC(application specific integrated circuit), FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 또한, 일 실시 예에 따른 인공 지능 모델을 실행하기 위한 프로세서는 CPU, AP, DSP(Digital Signal Processor) 등과 같은 범용 프로세서, GPU, VPU(Vision Processing Unit)와 같은 그래픽 전용 프로세서 또는 NPU와 같은 인공 지능 전용 프로세서과 소프트웨어의 조합을 통해 구현될 수 있다. 프로세서는, 메모리 에 저장된 기 정의된 동작 규칙 또는 인공 지능 모델에 따라, 입력 데이터를 처리하도록 제어할 수 있다. 또는, 프로세서가 전용 프로세서(또는 인공 지능 전용 프로세서)인 경우, 특정 인공 지능 모델의 처리에 특화된 하드웨어 구조로 설계될 수 있다. 예를 들어, 특정 인공 지능 모델의 처리에 특화된 하드웨어는 ASIC, FPGA 등의 하드웨어 칩으로 설계될 수 있다. 프로세서가 전용 프로세서로 구현되는 경우, 본 개시의 실시 예를 구현하기 위한 메모리를 포함하도록 구현되거나, 외부 메모리를 이용하기 위한 메모리 처리 기능을 포함하 도록 구현될 수 있다. 프로세서는 입력 데이터를 처리하여 출력 데이터를 획득한다. 여기서, 입력 데이터는 텍스트, 이미지 또는 사용자 음성 중 적어도 하나를 포함할 수 있다. 예를 들어, 입력 데이터는, 외부 장치와 통신 가능한 통신부, 키보드 또는 터치 패드 등과 같은 사용자 입력부, 카메라, 마이크 등을 통해 입력될 수 있다. 출력 데이터는 인 공 지능 모델의 종류에 따라 다양한 형태가 될 수 있다. 예를 들어 출력 데이터는, 해상도가 향상된 이미지, 이 미지 내에 포함된 오브젝트 관련 정보, 음성에 대응되는 텍스트 등이 될 수 있다. 일 예에 따라, 프로세서는 입력 영상을 영상 처리하여 출력 영상을 획득한다. 여기서, 입력 영상 또는 출 력 영상은 정지 영상, 복수의 연속된 정지영상(또는 프레임), 또는 비디오를 포함할 수 있다. 영상 처리는 영상 개선(image enhancement), 영상 복원(image restoration), 영상 변환(image transformation), 영상 분석(image analysis), 영상 인식(image understanding) 또는 영상 압축(image compression) 중 적어도 하나를 포함하는 디지털 영상 처리가 될 수 있다. 일 예에 따라 입력 영상이 압축 영상인 경우 프로세서는 압축 영상을 디 코딩한 후 영상 처리할 수 있다. 일 실시 예에 따라, 프로세서는 인공 지능 모델을 이용하여 입력 영상을 영상 처리할 수 있다. 예를 들어, 프로세서는 인공 지능 모델을 이용하기 위하여, 메모리, 예를 들어 DRAM과 같은 외부 메모리에 저장된 인공 지능 모델 관련 정보를 로딩하여 이용할 수 있다. 도 3은 일 실시 예에 따른 프로세서의 동작을 설명하기 위한 흐름도이다. 일 실시 예에 따르면, 프로세서는 입력 영상의 업스케일링 정보를 획득하고(S310), 획득된 업스케일링 정 보에 기초하여 입력 영상을 다운스케일링하고(S320), 다운스케일링된 영상을 업스케일링하여 출력 영상을 획득할 수 있다(S330). 여기서, 입력 영상의 업스케일링 정보는 입력 영상의 업스케일링 비율 정보 또는 입력 영상 의 원본 해상도 정보 중 적어도 하나를 포함할 수 있다. 입력 영상의 업스케일링 비율이란 입력 영상이 원본 영 상을 업스케일링하여 획득된 영상인 경우, 해당 업스케일링 비율을 의미할 수 있다. 다만, 업스케일링되지 않은 영이상의 경우 업스케일링 비율이 1이 될 수 있으므로, 업스케일링 비율이 반드시 업스케일링이 수행된 영상에 만 적용되는 용어는 아니다. 일 실시 예에 따르면, 하나의 프로세서가 S310, S320, S330 단계의 동작을 모 두 수행할 수도 있으나, 적어도 일부 단계의 적어도 일부 동작은 적어도 하나의 다른 프로세서에 의해 수행될 수도 있다. 일 예에 따라 프로세서는 영상의 업스케일링 정보를 획득하도록 학습된 제1 인공 지능 모델을 이용하여 입 력 영상의 업스케일링 정보를 획득할 수 있다. 다만, 반드시 이에 한정되는 것은 아니며 제1 인공 지능 모델을 이용하지 않고 입력 영상의 업스케일링 정보를 획득하는 것도 가능하다. 예를 들어, 프로세서는 입력 영상 의 픽셀 정보에 기초한 다양한 방식으로 입력 영상의 업스케일링 정보를 획득할 수도 있다. 예를 들어, 에지 영 역 주변의 픽셀 값 분포를 확인하여 업스케일링 비율 또는 원본 해상도 정보를 식별할 수도 있다. 다만, 일 실 시 예에 따라 정확한 업스케일링 정보를 획득하기 위하여 제1 인공 지능 모델을 이용하는 경우를 상정하여 설명 하도록 한다. 일 실시 예에 따라 프로세서는 입력 영상의 특성 정보를 제1 인공 모델에 입력하여 입력 영상의 업스케일 링 정보를 획득할 수 있다. 일 예로, 프로세서는 입력 영상의 특성 정보를 제1 인공 모델에 입력하여 복수의 상이한 업스케일링 비율 각각에 대한 확률 정보를 획득할 수 있고, 획득된 확률 정보 중 최대 값 또는 임계값 이상의 값 중 적어도 하나 에 기초하여 입력 영상의 업스케일링 비율을 식별할 수 있다. 다른 예로, 프로세서는 입력 영상의 특성 정 보를 제1 인공 모델에 입력하여 복수의 상이한 원본 해상도 정보 각각에 대한 확률 정보를 획득할 수 있고, 획 득된 확률 정보 중 최대 값 또는 임계값 이상의 값 중 적어도 하나에 기초하여 입력 영상의 원본 해상도 정보를 획득할 수 있다. 이 경우, 제1 인공 지능 모델은, 영상의 특성 정보에 기초하여 영상의 업스케일링 정보를 획득하도록 학습된 모 델일 수 있다. 여기서, 영상의 특성 정보는 영상의 특정 영역에서 획득된 정보일 수 있다. 예를 들어 영상의 특 성 정보는 영상 내의 에지를 포함하는 에지 영역에서 획득된 정보일 수 있다. 이는 에지 영역이 영상의 업스케 일링에 의해 픽셀 값 변화가 큰 영역이기 때문이다. 다만, 이에 한정되는 것은 아니며, 영상의 특성 정보는, 경 우에 따라 텍스처(texture) 영역 및 평탄(flat) 영역 중 적어도 하나에서 획득되는 것도 가능하다. 여기서, 제1 인공 지능 모델은 복수의 훈련 영상에서 획득된 특성 정보 및 복수의 훈련 영상 각각의 업스케일링 정보(예를 들어, 업스케일링 비율 또는 원본 해상도)를 각각 입출력 훈련 데이터 쌍으로 이용하여 학습될 수 있 다. 예를 들어, 도 4a에 도시된 바와 같이 (제1 훈련 영상에서 획득된 제1 특성 정보, 제1 훈련 영상의 업스케 일링 정보), (제2 훈련 영상에서 획득된 제2 특성 정보, 제2 훈련 영상의 업스케일링 정보), ..., (제n 훈련 영 상에서 획득된 제n 특성 정보, 제n 훈련 영상의 업스케일링 정보)를 입출력 훈련 데이터 쌍으로 이용하여 학습 될 수 있다. 다른 실시 예에 따라 프로세서는 입력 영상 전체 또는 입력 영상 일부를 제1 인공 모델에 입력하여 입력 영상의 업스케일링 정보를 획득할 수 있다. 이 경우, 제1 인공 지능 모델은, 영상 전체 또는 영상 일부에 기초 하여 영상의 업스케일링 정보를 획득하도록 학습된 모델일 수 있다. 여기서, 일상 일부는 관심 영역을 포함하는 영상 일부, 특정 픽셀 특성(예를 들어 에지)을 포함하는 영상 일부 중 적어도 하나를 포함할 수 있다. 여기서, 제1 인공 지능 모델은 영상 전체(또는 영상 일부) 및 대응되는 영상의 업스케일링 비율 정보를 각각 입 출력 훈련 데이터 쌍으로 이용하여 학습될 수 있다. 예를 들어, 도 4b에 도시된 바와 같이 (제1 훈련 영상, 제1 훈련 영상의 업스케일링 정보), (제2 훈련 영상, 제2 훈련 영상의 업스케일링 정보), ..., (제n 훈련 영상, 제n 훈련 영상의 업스케일링 정보)를 입출력 훈련 데이터 쌍으로 이용하여 학습될 수 있다. 여기서, 인공 지능 모델이 학습된다는 것은, 기본 인공 지능 모델(예를 들어 임의의 랜덤한 파라미터를 포함하 는 인공 지능 모델)이 학습 알고리즘에 의하여 다수의 훈련 데이터들을 이용하여 학습됨으로써, 원하는 특성(또 는, 목적)을 수행하도록 설정된 기 정의된 동작 규칙 또는 인공 지능 모델이 만들어짐을 의미한다. 이러한 학습 은 별도의 서버 및/또는 시스템을 통해 이루어질 수 있으나, 이에 한정되는 것은 아니며 전자 장치에서 이루어 질 수도 있다. 학습 알고리즘의 예로는, 지도형 학습(supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또는 강화 학습(reinforcement learning)이 있으나, 전술한 예에 한정되지 않는다. 여기서, 제1 인공 지능 모델은, 예를 들어, CNN (Convolutional Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등으로 구현될 수 있으나, 이에 한정되지 않는 다. 일 실시 예에 따르면, 프로세서는 획득된 업스케일링 정보에 기초하여 입력 영상의 다운 스케일링 비율을 획득할 수 있다. 일 예로, 프로세서는 업스케일링 비율이 획득된 경우, 획득된 업스케일링 비율의 역수를 다운스케일링 비 율로 결정할 수 있다. 예를 들어, 프로세서는 업스케일링 비율이 2인 경우, 업스케일링 비율의 역수인 1/2 를 다운스케일링 비율로 결정하고, 획득된 업스케일링 비율이 2.1인 경우, 업스케일링 비율의 역수인 1/2.1를 다운스케일링 비율로 결정할 수 있다. 또는 프로세서는 원본 해상도 정보가 획득된 경우, 획득된 원본 해 상도와 입력 영상의 해상도의 비율 즉, \"원본 해상도/입력 영상의 해상도\"의 비율을 다운스케일링 비율로 결정 할 수 있다. 다른 예로, 프로세서는 업스케일링 비율이 획득된 경우, 획득된 업스케일링 비율을 임계 범위 내에서 조정 한 후, 조정된 업스케일링 비율의 역수를 다운스케일링 비율로 결정할 수 있다. 예를 들어, 프로세서는 획 득된 업스케일링 비율이 2.1인 경우, 업스케일링 비율을 2(예를 들어 정수배)로 조정한 후, 조정된 업스케일링 비율의 역수인 1/2를 다운스케일링 비율로 결정할 수 있다. 유사한 방식으로, 원본 해상도 정보가 획득된 경우 \"원본 해상도/입력 영상의 해상도\"의 비율을 조정할 수 있다. 또 다른 예로, 프로세서는 업스케일링 비율이 획득된 경우, 획득된 업스케일링 비율에 기초하여 원본 영상 의 해상도를 추정하고, 추정된 해상도에 기초하여 입력 영상의 다운 스케일링 비율을 획득할 수 있다. 구체적으 로, 프로세서는 제1 인공 지능 모델의 출력에 기초하여 업스케일링 비율이 획득되면 획득된 업스케일링 비 율에 기초하여 원본 영상의 해상도를 추정할 수 있다. 예를 들어, 프로세서는 원본 영상의 해상도 및 획득 된 업스케일링 비율에 기초하여 기설정된 복수의 해상도 중 하나를 원본 영상의 해상도로 추정할 수 있다. 예를 들어, 기설정된 복수의 해상도는 현재 제작되고 있는 영상의 해상도 예를 들어, 720x480, 1280x720, 1920x1080, 2560x1440, 3840x2160, 7680x4320 등의 다양한 해상도를 포함할 수 있다. 일 예로, 프로세서는 입력 영상의 해상도가 3840x2160의 4K UHD이고 식별된 업스케일링 비율이 2인 경우, 원본 영상의 해상도가 1920x1080의 FHD인 것으로 추정할 수 있다. 다른 예로, 프로세서는 입력 영상의 해 상도가 3840x2160의 4K UHD이고, 식별된 업스케일링 비율이 2.1인 경우 원본 영상의 해상도가 1920x1080의 FHD 인 것으로 추정할 수 있다. 상술한 바와 같이 다운스케일링 비율이 결정되면, 프로세서는 다운스케일링 비율에 기초하여 입력 영상을 다운스케일링(또는 다운샘플링)한 후, 다운스케일링된 영상을 출력 해상도에 기초하여 업스케일링할 수 있다. 예를 들어, 입력 영상의 해상도가 3840x2160의 4K UHD이고 1/2 다운스케일링 비율에 따라 다운스케일링된 경우, 출력 해상도가 8K UHD인 경우라면, 다운스케일링된 영상을 2배 업스케일링하여 출력 영상을 획득할 수 있다. 여 기서, 다운스케일링 방식으로는 Sub-Sampling을 포함한 다양한 종래의 방식이 이용될 수 있다. 예를 들어, RGB 데이터를 YUV 데이터(예를 들어, Y'UV, YUV, YCbCr, YPbPr)로 변환하고, Y 컴포넌트(밝기 정보) 대비 U, V 컴 포넌트(색차 정보)를 감소시켜 다운스케일링을 수행할 수 있다. 일 실시 예에 따라 프로세서는 고화질 처리를 위한 업스케일링 기능 블럭을 이용하여 다운스케일링된 영상 을 업스케일링할 수 있다. 일 예에 따라 프로세서는 초해상도 처리(super resolution)를 수행하도록 학습된 제2 인공 지능 모델을 이 용하여 다운스케일링된 영상을 업스케일링할 수 있다. 구체적으로, 프로세서는 다운스케일링된 영상을 제2 인공 지능 모델에 입력하여 업스케일링된 영상을 획득할 수 있다. 이 경우, 제2 인공 지능 모델은, 복수의 훈련 영상 및 훈련 영상 각각에 대응되는 업스케일드 영상을 각각 입출력 훈련 데이터 쌍으로 이용하여 학습될 수 있 다. 여기서, 복수의 훈련 영상은 다양한 해상도의 영상이 될 수 있다. 여기서, 제2 인공 지능 모델은 예를 들어, CNN 기반의 VDSR 기술(Jiwon Kim, et al. , Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR 2016), EDSR(Enhanced Deep Residual Networks for Single Image Super- Resolution), DRCN(Deeply-Recursive Convolutional Network for Image Super-Resolution.”Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.), MDSR(Multi-scale deep super-resolution system) 등이 이용될 수 있으나, 이에 한정되는 것은 아니다. 한편, 일 실시 예에 따르면, 프로세서는 입력 영상에서 기설정된 영역을 식별하고, 식별된 영역에 포함된 픽셀에 대한 특성 정보를 획득하여, 제1 인공 지능 모델의 입력 데이터로 이용할 수 있다. 여기서, 기설정된 영 역은, 에지(edge) 영역, 텍스처(texture) 영역 또는 평탄(flat) 영역 중 적어도 하나를 포함할 수 있다. 일 예에 따라 프로세서는 입력 영상에서 우선 관심 영역을 식별한 후, 관심 영역에서 기설정된 특성 영역 을 식별할 수 있다. 예를 들어, 실제 영상에서 뉴스나 홈쇼핑처럼 영상 내 영상이 존재하는 경우 또는 자막이나 얼굴이 포함된 영상의 경우, 각 영역의 화질 차이가 있을 수 있기 때문에 관심 영역을 우선적으로 식별한 후 관 심 영역에서 기설정된 특성 영역을 식별할 수 있다. 여기서, 관심 영역은 특정 오브젝트(예를 들어 건물, 사람, 텍스트 등과 같이 뚜렸한 에지 위주의 오브젝트)를 포함하는 영역, 특정 픽셀 값 분포(예를 들어 픽셀 영역들 간의 픽셀 값 차이가 큰 분포)를 만족하는 영역 등이 될 수 있다. 다만, 이에 한정되는 것은 아니며, 입력 영상 에서 관심 영역을 식별하지 않고 바로 기설정된 특성 영역을 식별할 수 있음은 물론이다. 일 실시 예에 따라 프로세서는 에지 영역에 포함된 픽셀에 대한 특성 정보를 획득할 수 있다. 여기서, 에 지는 공간적으로 인접한 픽셀 값들이 급격하게 변하는 영역으로, 인접 픽셀 값 간의 차이가 임계 값 이상인 영 역이 될 수 있다. 예를 들어, 에지는 영상의 밝기가 낮은 값에서 높은 값으로 또는 높은 값에서 낮은 값으로 급 격하게 변화하는 영역이 될 수 있다. 예를 들어, 인접 픽셀 값 간의 차이가 일정 기준 값 이상일 경우 에지로 판단될 수 있다. 주로, 객체의 경계(boundary)나 텍스트 등의 영역이 에지 영역으로 판단될 수 있다. 일 실시 예에 따른 에지 영역은 에지에 대응되는 에지 픽셀 및 에지 픽셀 주변 픽셀 영역을 포함한 영역이 될 수 있다. 일 예로, 프로세서는 식별된 에지 영역에 포함된 중심 픽셀을 기준으로 마진(margin) 영역 외의 픽셀에 기 초하여 특성 정보를 획득할 수 있다. 다른 예로, 프로세서는 식별된 에지 영역을 블러링 처리하고, 블러링 된 에지 영역에 포함된 중심 픽셀을 기준으로 마진(margin) 영역을 설정하고, 설정된 마진 영역 외의 픽셀에 기 초하여 특성 정보를 획득할 수 있다. 이하에서는 프로세서는 식별된 에지 영역을 블러링 처리한 후, 특성 정보를 획득하는 경우를 상정하여 설명하도록 한다. 도 5는 일 실시 예에 따라 제1 인공 지능 모델에 입력되는 특성 정보를 획득하는 방법을 설명하기 위한 도면이 다. 도 5에 따르면, 프로세서는 입력 영상에서 에지 영역을 검출한다(S510). 여기서, 에지 영역은 관심 영역 (관심 객체 또는 텍스트)에 포함된 영역일 수 있으나, 반드시 이에 한정되는 것은 아니다. 일 예에 따라, 프로세서는 에지 검출 필터를 이용하여 에지 영역을 검출할 수 있다. 예를 들어 프로세서 는 입력 영상에 1차 또는 2차 에지 검출 필터를 적용하여 에지 강도 및 에지 방향 정보(그래디언트에 수직 방향)를 포함하는 필터링된 신호를 획득할 수 있고, 이에 기초하여 에지 영역을 검출할 수 있다. 프로세서는 검출된 에지 영역을 블러링(또는 스무딩) 처리하여 검출된 에지 영역을 확장할 수 있다(S520). 예를 들어, 가우시안 분포는 도 6a에 도시된 바와 같이 x축의 0은 가중치가 크고, +/- 부분로 갈수록 가중치가 적어지는 형태가 될 수 있고, 이러한 가우시안 분포를 3*3 형태의 마스크에 적용하면 마스크 중심은 가 중치가 크고, 마스크의 가장자리로 갈수록 가중치가 적어지는 형태가 될 수 있다. 다만 도 6a에 도시된 수 치는 예로 든 것이며, 필터링 수치는 가우시안 함수의 시그마 값에 따라 달라짐은 물론이다. 프로세서는 도 6b에 도시된 바와 같이 가우시안 마스크를 검출된 에지 영역에 적용하여 에지 영역을 블러링 처리할 수 있다. 일반적으로, 가우시안 필터는 스무딩을 통한 정규 분포, 확률 분포에 의해 생성된 잡음을 제거하기 위한 필터로 쓰이지만, 일 실시 예에서는 영상 내 에지로 식별된 픽셀 영역을 스무딩시켜 에지 픽셀을 포함한 그 주 변 픽셀 영역까지 대상 영역을 확장시키는 기능을 할 수 있다. 예를 들어, 도 7a에 도시된 바와 같이 스무딩을 통해 점선 영역까지 에지 영역이 확장될 수 있어 대상 영역 확장이 가능해지므로 좀더 정확한 업스케일링 비율 을 검출할 수 있다. 이어서, 프로세서는 블러링된 영역에서 마진 영역을 설정하고, 마진 영역 외의 픽셀을 검출하여 특성 정보 를 획득할 수 있다(S530). 구체적으로, 프로세서는 도 7a에 도시된 바와 같이 검출된 에지 픽셀을 기준으 로 설정된 마진 영역(점선) 외의 영역에 존재하는 픽셀에 대한 정보를 획득할 수 있다. 여기서, 마진 영역의 크 기는 기 정해진 크기로 설정되거나, 블러링을 위해 이용되는 필터의 크기, 필터의 계수값 등에 기초하여 설정될 수 있다. 예를 들어, 기 정해진 크기는 일반적으로 영상에 가장 흔하게 수행되는 업스케일링 비율을 고려하거나, 현재 존재하는 영상에서 최대로 가능한 업스케일링 비율 등을 고려하여 설정될 수 있다. 이와 같이 판단하는 이유는 영상의 업스케일링 비율에 따라 에지 픽셀의 픽셀 값(또는 유사 픽셀 값)이 에지 주변 영역으로 다양하게 확장될 수 있기 때문이다. 일 예에 따라 프로세서는 도시된 바와 같이 대상 픽셀이 마진 영역에 속하는 경우 제1 식별 정보를 맵핑하 고, 속하지 않는 경우 제2 식별 정보를 맵핑하여 특성 정보를 생성할 수 있다. 예를 들어, 이진 식별 정보를 이 용하는 경우, 제1 식별 정보는 0, 제2 식별 정보는 1이 될 수 있으나, 이에 한정되는 것은 아니다. 다만, 이하 에서는 설명의 편의를 위하여 제1 식별 정보를 0, 제2 식별 정보를 1로 상정하여 설명하도록 한다. 일 실시 예에 따라 프로세서는 영상에 포함된 픽셀들에 기설정된 크기의 윈도우를 적용하여 윈도우 내의 픽셀들이 마진 영역에 속하는지 여부를 식별하여 각 픽셀에 식별 정보를 부여하고, 식별 정보를 기설정된 순서 로 배열하여 특성 정보를 획득할 수 있다. 구체적으로, 프로세서는 기설정된 크기의 윈도우의 위치를 적어 도 하나의 픽셀 간격으로 이동시키면서 윈도우 내의 픽셀들이 마진 영역에 속하는지 여부를 식별할 수 있다. 예 를 들어, 프로세서는 윈도우의 위치를 하나의 픽셀 간격으로 이동시키면서 각 위치에 대응되는 특성 값을 획들할 수 있으나, 이에 한정되는 것은 아니며 두 개 이상의 픽셀 간격으로 이동시키면서 각 위치에 대응되는 특성 값을 획득하는 것도 가능하다. 일 예에 따라, 도 7b에 도시된 바와 같이 5 픽셀 크기의 윈도우를 적용하는 경우 중심 픽셀을 기준으로 우 측 가장 자리에 있는 픽셀의 식별 정보 > 좌측 가장 자리에 있는 픽셀의 식별 정보 > 우측에 인접한 픽셀의 식별 정보 > 좌측에 인접한 픽셀의 식별 정보 > 중심 픽셀의 식별 정보 순으로 bit0, bit1, bit2, bit3, bit4 위치에 정렬하여 01001을 획득할 수 있다. 이 후, 프 로세서는 해당 식별 정보를 십진수로 변환(2^4*0+2^3*1+2^2*0+2^1*0+2^0*1=9)여 특성 값 \"9\"를 획득할 수 있다. 여기서, 프로세서는 획득된 특성 값들 각각의 빈도수에 기초하여 히스토그램 정보를 획득할 수 있다. 예를 들어, 특성 값 각각은 bin 즉, 구간을 나타내고, 각 특성 값 즉, 각 bin의 빈도수가 산출될 수 있다. 예를 들어, 특성 값 \"9\"는 bin9가 될 수 있으며, 각 특성 값 즉, 각 bin의 빈도 수가 즉, 히스토그램 정보가 입력 영 상의 특성 정보로 획득될 수 있다. 다른 실시 예에 따라 프로세서는 윈도우 크기를 상이하게 설정하고 윈도우 크기 별로 상이한 스트라이드를 적용하여 특성 정보를 획득할 수 있다. 여기서, 스트라이드란 윈도우 내에서 식별 정보를 생성하는 픽셀 간 간 격이 될 수 있다. 예를 들어, 프로세서는 도 7c에 도시된 바와 같이 윈도우의 상이한 크기를 상이한 타입으로 설정하여 특성 정보를 획득할 수 있다. 구체적으로, 프로세서는 도 7c의 첫번째 라인에 도시된 바와 같이 5 픽셀 크기의 윈도우를 적용하여 획득 된 특성 값을 type 1으로 설정하고, 도 7c의 두번째 라인에 도시된 바와 같이 7 픽셀 크기의 윈도우를 적용하여 획득된 특성 값을 type 2 로, 도 7d의 세번째 라인에 도시된 바와 같이 9 픽셀 크기의 윈도우를 적용하여 획득 된 특성 값을 type 3 로, 그 외 다른 다른 크기의 윈도우를 적용하여 획득된 특성 값을 각각 다른 타입(예를 들 어, type 4, type 5)으로 설정할 수 있다. 도 7c의 첫번째 라인에 도시된 type1의 특성 값을 획득하기 위하여, 프로세서는 5 픽셀 크기의 윈도우, 스 트라이드 0을 적용할 수 있다. 해당 특성 값을 획득하는 방법은 도 7b에 도시된 바와 동일하므로 자세한 설명은 생략하도록 한다. 도 7c의 두번째 라인에 도시된 type2의 특성 값을 획득하기 위하여, 프로세서는 9 픽셀 크기의 윈도우, 스 트라이드 1을 적용할 수 있다. 구체적으로 프로세서는 중심 픽셀 기준으로 양 측으로 각각 4개의 인접한 픽셀을 1 픽셀 이격 간격으로 확인하여 각각의 식별 정보를 기설정된 순서로 배열하여 5 비트 크기의 특성 정보 를 획득할 수 있다. 예를 들어 도 7c의 두번째 라인에 도시된 바와 같이 9 픽셀 크기의 윈도우를 적용하는 경우 중심 픽셀을 기준으로 우측 제1 픽셀의 식별 정보 > 좌측 제2 픽셀의 식별 정보 > 우측 제3 픽셀의 식별 정보 > 좌측 제4 픽셀의 식별 정보 > 중심 픽셀의 식별 정보 순으로 bit0, bit1, bit2, bit3, bit4 위치에 정렬하여 00110을 획득할 수 있다. 즉, 프로세서 는 9 픽셀 크기의 윈도우 내에서 기설정된 순서에 따라 1 픽셀 간격으로 이격된 픽셀들의 식별 정보를 정 렬하여 type2의 특성 값을 획득할 수 있다. 이 후, 프로세서는 십진수로 변환 (2^4*0+2^3*0+2^2*1+2^1*1+2^0*1=9)여 type2의 특성 값 \"6\"를 획득할 수 있다. 이와 유사한 방법으로 프로세서는 윈도우 크기 및 스트라이트 크기를 상이하게 설정하여 다른 타입, 예를 들어, type3, type4의 특성 값을 획득할 수 있다. 여기서, 프로세서는 획득된 타입 특성 값들 각각의 빈도수에 기초하여 히스토그램 정보를 획득할 수 있다. 예를 들어, 프로세서는 type1bin9, type2bin6, type3bin0, type4bin8 과 같은 타입 별 특성 정보를 획득 할 수 있다. 다만, 상술한 실시 예에서는 중심 픽셀을 기준으로 좌-우-좌-우의 순서로 식별 정보를 정렬하여 특성 정보를 획 득하는 것으로 설명하였으나, 이는 일 실시 예에 불과함은 물론이다. 예를 들어, 최우측 픽셀의 식별 정보부터 순서대로 bit0, bit1, bit2, bit3, bit4 위치에 정렬하거나, 최좌측 픽셀의 식별 정보부터 순서대로 bit0, bit1, bit2, bit3, bit4 위치에 정렬하는 등 다양한 규칙에 따라 특성 정보 를 획득할 수 있음은 물론이다. 또한, 상술한 실시 예에서는 윈도우 크기 및 스트라이드 크기를 조정하여 각 타입 별로 동일한 크기(예를 들어, 5자리 이진수)의 특성 값을 획득하는 것으로 설명하였지만, 반드시 이에 한정되는 것은 아니며 타입 별 특성 값 의 크기가 달라지는 것도 가능하다. 예를 들어 윈도우 크기를 증가시키더라도 스트라이드 크기를 유지하여 좀 더 큰 크기(예를 들어, 6자리 이진수)의 특성 값을 획득할 수도 있다. 한편, 상술한 실시 예에서는 프로세서가 식별된 에지 영역을 블러링 처리한 후 특성 정보를 획득하는 경우 를 상정하여 설명하였지만, 식별된 에지 영역을 블러링 처리하지 않고 특성 정보를 획득하는 경우에도 블러링 처리 외의 과정이 동일하게 적용될 수 있음은 물론이다. 도 2로 돌아와서, 다른 실시 예에 따르면, 프로세서는 다운 스케일링된 영상을 업스케일링하기 전에 제1 화질 처리를 수행할 수 있다. 또한, 프로세서는 업스케일링된 영상에 대해 제2 화질 처리를 수행하여 출력 영상을 획득할 수 있다. 예를 들어, 제1 화질 처리는 Noise Reduction, Noise Reduction Simple, Detail Enhancement 또는 Detail Enhancement Simple 중 적어도 하나를 포함할 수 있고, 제2 화질 처리는 Tone Mapping, Contrast Enhancement, Color Enhancement 또는 Frame rate Conversion 중 적어도 하나를 포함할 수 있으나, 반드시 이에 한정되지 않는다. 예를 들어, 프로세서는 Noise Reduction 처리를 위해 입력 영상에 대한 프리 필터링을 수행할 수 있다. 일반적으로, 영상을 압축하는 과정, 전송하는 과정 등에서 Noise(잡음)이 발생된다. 이에 따라 프로세서는 넌 로컬 필터링(non-local filtering) 및 자기 유사성(self- similarity)을 이용한 방식, 로우 패스 필터링(low pass filtering)에 의한 스무딩(smoothing) 필터 등을 이용 하여 Noise Reduction 처리를 수행할 수 있다. 이와 같이 영상을 다운스케일링한 후 제1 화질 처리를 수행하게 되면, 제1 화질 처리시 낮은 복잡도의 하드웨어 를 이용할 수 있고 그에 따라 하드웨어 구현 비용을 저감시킬 수 있는 효과가 있다. 한편, 프로세서는 입력 영상이 업스케일링된 영상이 아닌 원본 영상으며 별도의 업스케일링 처리가 필요없 는 경우, 입력 영상을 화질 처리한 후, 제2 화질 처리하여 출력 영상을 획득할 수 있다. 여기서, 입력 영상에 대해 수행되는 화질 처리는 Noise Reduction Simple 또는 Detail Enhancement Simple 중 적어도 하나를 포함할 수 있다. 즉, 원본 영상에 대해 수행되는 화질 처리는, 업스케일링된 영상에 대해 수행되는 제1 화질 처리 (Noise Reduction 또는 Detail Enhancement 중 적어도 하나)와 화질 처리 방식이 다소 차이가 있을 수 있으나, 동일한 화질 처리 방식을 이용할 수 있음은 물론이다. 다만, 제2 화질 처리는 입력 영상의 해상도, 업스케링일 처리 유무에 관계없이 동일한 화질 처리 방식이 이용될 수 있다. 한편, 일 실시 예에 따르면, 프로세서는 입력 영상에서 씬이 변경되는 시점을 식별하고, 씬이 변경되는 시 점에 기초하여 다운 스케일링을 수행할 수 있다. 일 예에 따라, 프로세서는 입력 영상의 특정 씬 구간에서 의미가 있는 업스케일링 정보가 획득되는 경우 해당 업스케일링 정보에 기초하여 다음 씬부터 다운스케일링을 수행할 수 있다. 예를 들어 프로세서는 제1 씬 구간에서 획득된 업케일링 비율에 기초하여 제1 씬 구간 이후의 제2 씬 구간에 포함되는 프레임부터 다운스 케일링하고, 다운스케일링된 프레임을 업스케일링하여 출력 프레임을 획득할 수 있다. 구체적으로, 프로세서 는 업스케일링 정보가 획득되는 경우 바로 다음 프레임부터 다운스케일링일 수행하는 것이 아니라, 씬 체 인지 시점을 기준으로 다운스케일링을 수행하여 잦은 업스케일링 비율 변화로 인한 해상도 변화를 방지할 수 있 다. 일 예에 따라, 프로세서는 영상 내에 삽입된 씬 체인지 정보, 예를 들어, scene change flag에 기초하여 씬이 변하는 시점을 식별할 수 있다. 예를 들어, scene change flag는 메타 데이터의 일 영역에 포함될 수 있고, 씬이 유지되는 경우에는 \"0\" 값을 가지지만, 씬이 변하는 경우 \"1\" 값을 가질 수 있다. 다만, 이에 한정 되는 것은 아니며 종래의 씬을 식별할 수 있는 다양한 방법이 이용될 수 있다. 여기서, 씬은 컨텐츠 제작자에 의하여 구분되어진 시나리오 상의 공간 변화에 따른 씬이 될 수 있다. 다만, 경우에 따라서는 씬은 유사한 화질 특성을 가지는 구간을 의미할 수 있고, 이 경우 컨텐츠 제작자에 의하여 구분되어진 시나리오 상의 동일한 공간 에서도 영상의 밝기, 색상 등에 따라 상이한 씬으로 구분될 수도 있다. 또한, 프로세서는 입력 영상의 기설정된 프레임 구간 단위로 업스케일링 정보(예를 들어 업스케일링 비율 정보, 원본 해상도 정보)를 획득하고, 제1 씬 구간 이후의 임계 개수의 씬 구간에서 동일한 업스케일링 정보가 유지되는 경우 임계 개수의 씬 구간 이후의 제2 씬이 시작되는 프레임부터 다운스케일링을 수행할 수 있다. 이 는, 씬 체인지 정보나 업스케일링 정보가 완벽하게 정확하지 않을 수 있고, 잦은 업스케일링 비율 변화로 인해 해상도가 불필요하게 잦은 횟수로 변화되는 것을 방지하기 위함이다. 여기서, 프레임 구간은 적어도 하나의 프 레임을 포함하는 시간 구간이 될 수 있으며, 예를 들어 하나의 프레임 단위가 될 수 있다. 즉, 프로세서는 프레임 단위로 업스케일링 정보를 식별하고, 제1 씬 구간 이후의 임계 개수의 씬 구간에서 각 프레임마다 식별 된 업스케일링 정보가 일정하게 유지되는 경우 임계 개수의 씬 구간 이후의 제2 씬이 시작되는 프레임부터 다운 스케일링을 수행할 수 있다. 도 8은 일 실시 예에 따른 씬 기반 다운스케일링 방법을 자세히 설명하기 위한 도면이다. 본 개시의 일 예에 따라 도 8에 도시된 바와 같이 전자 장치로 10 개의 scene을 포함하는 입력 영상이 수 신되는 것으로 가정한다. 또한, 도 8의 첫번째 라인에 도시된 바와 같이 scene 1 내지 scene 3는 4K 영상(4K), scene 4 내지 scene 8은 업스케일링된 4K 영상(UP4K), scene 9는 4K 영상(4K), scene 10은 업스케일링된 4K 영 상(UP4K)인 것으로 가정하도록 한다. 이 경우, 프로세서는 도 8의 두번째 라인에 도시된 바와 같이 입력 영상에서 scene change flag 정보를 검 출하면서, 도 8의 세번째 라인에 도시된 바와 같이 각 프레임 별로 업스케일링 정보를 획득할 수 있다. 예를 들 어 프로세서는 상술한 바와 같이 제1 인공 지능 모델의 출력에 기초하여 각 프레임 별 업스케일링 정보를 획득할 수 있다. 다만, 일반적으로 씬이 변경되는 시점에서 scene change flag \"1\"이 검출되어 프로세서가 씬 체인지 시점을 판단할 수 있으나, 실제로는 scene change flag 정보가 잘못 검출되는 경우가 생길 수 있으므 로 본 실시 예에서는 scene 4에 포함된 중간 프레임에서 scene change flag \"1\"이 잘못 검출되고 scene 6 및 scene 7로 변하는 시점에서 scene change flag \"1\"이 아닌 \"0\"이 검출되는 경우를 가정하였다. 또한, 프로세서 가 각 프레임 별 업스케일링 정보를 잘못 판단하는 경우가 있을 수 있으므로 프레임 별 업스케일링 정보가 일부 프레임에서 잘못 검출되는 경우를 가정하였다. 이 경우, 프로세서는 scene change flag \"1\"이 검출되는 시점에서 동일한 업스케일링 정보가 임계 개수(예 를 들어 프레임 3개) 이상 지속적으로 유지되는 경우 업스케일링을 수행할 수 있다. 예를 들어 도 8에 도시된 실시 예에서 scene 4부터 업스케일링된 4K 영상(UP4K)이 입력될 수 있지만, 실제로 다운스케일링은 동일한 업스 케일링 정보가 임계 개수 이상 유지된 이후인 scene 6부터 수행될 수 있다. 또한, scene 9의 영상은 4K 영상이 지만 이를 무시하고 업스케일링된 4K 영상(UP4K)에 대한 처리, 즉, 다운 스케일링 처리를 scene 9에서도 유지할 수 있다. 상술한 바와 같이 프로세서는 씬 체인지 시점에서만 다운스케일링 수행 여부를 변경하고, 동일한 업스케일 링 정보가 임계 개수(예를 들어 프레임 3개) 이상 지속적으로 검출되는 경우 이 후 씬 체인지 시점의 프레임부 터 다운스케일링을 수행할 수 있다. 이는 잦은 해상도 변화는 시청자에게 과도하게 인식될 수 있고, 씬 체인지 정보나 업스케일링 정보 또한 완벽하게 정확하지는 않기 때문에 최대한 보수적으로 실시 예를 적용하기 위함이 다. 즉, 하나의 씬에서도 매 프레임 상이한 업스케일링 비율 및 상이한 확률 값이 추정될 수 있는데 이를 그대 로 적용할 수 없기 때문이다. 실제 뉴스나 홈쇼핑, 광고 등이나 채널 변경에 따라 업스케일링 정보가 매우 빈번 히 변경될 수 있고 이를 그대로 적용하는 경우 잦은 해상도 변화로 변화로 인해 부작용을 발생시킬 수 있기 때 문이다. 도 9a 및 도 9b는 일 실시 예에 따른 제1 인공 지능 모델의 구조를 설명하기 위한 도면들이다. 도 9a에 따르면, 제1 인공 지능 모델은 복수의 신경망 레이어를 포함하는 신경망 네트워크로 구현될 수 있다. 예를 들어, 제1 인공 지능 모델은 Classification Network로 구현될 수 있다. 일 예에 따라 신경망 네트워크의 입력은 도 7a 내지 도 7c에 도시된 방법에 따라 획득된 입력 영상의 특성 정보 가 될 수 있다. 다른 예에 따라 신경망 네트워크의 입력은 입력 영상 자체 또는 입력 영상 일부가 될 수도 있다. 다만, 이하에서는 일 예에 따라 신경망 네트워크의 입력이 입력 영상의 특성 정보인 경우를 상정하여 설 명하도록 한다. 일 예에 따라, 도 9b에 도시된 바와 같이 복수의 신경망 레이어, 예를 들어 Dense Layer 4개가 순차적으로 연결 되도록 구현될 수 있다. 다만, Dense Layer의 개수는 이에 한정되지 않음은 물론이다. 여기서, Dense Layer란 도 9b에 도시된 바와 같이 이전 계층의 모든 뉴런들과 결합되어 있는 형태의 layer를 말 하며 fully-connected layer라고도 불리운다. 일 예에 따라, 입력 데이터는 각각의 Dense Layer를 거치면 서 예를 들어, 128채널->64채널->16채널->5채널의 출력으로 출력 채널 개수가 줄어들게 되고 마지막 출력 채널 개수인 5개 class가 각각이 업스케일링 비율을 나타낼 수 있다. 예를 들어, 입력 영상의 특성 정보가 도 7c에 도시된 바와 같이 각 타입 별 특성값으로 획득되는 경우, 각 타입 별 특성값이 Dense Layer의 입력 데이터가 될 수 있다. 각 타입 별 특성값이 128 채널로 첫번째 Dense Layer로 입력되고, 마지막 Dense Layer에서 5개의 class, 즉 5개의 업스케일링 비율값(예를 들어,x1, x2, x3, x4, x4 이상) 및 5개의 class에 대한 확률 값, 즉 5개의 업스케일링 비율 값에 대응되는 확률 값이 출력될 수 있다. 일 예에 따라 각 타입 별 특성값이 128개의 채널로 랜덤하게 분산되어 입력될 수도 있다. 다른 예에 따라, 각 타입 별 특성 값이 각 타입에 기초하여 상이한 채널로 그룹핑되어 입력되는 것도 가능하다. 예를 들어, 제1 타 입 특성값은 1 내지 25번째 채널로, 제2 타입 특성값은 26 내지 50 번째 채널로, 제3 타입 특성 값은 51 내지 75번째 채널로, 제4 타입 특성 값은 76 내지 100번째 채널로, 제5 타입 특성 값은 100 내지 128번째 채널로 입 력될 수도 있다. 이 경우, 각 채널에 대응되는 파라미터는 대응되는 타입에 대응되도록 학습될 수 있다. 예를 들어, 첫번째 Dense Layer에서 1 내지 25번째 채널에 대응되는 필터 계수는 제1 타입 특성 값의 특성이 반영되 도록 학습될 수 있다. 한편, 신경망 네트워크의 출력 부분은 도 9b에 도시된 바와 같이 argmax 처리 및 softmax 처리가 가 능하도록 구현될 수 있다. 여기서, softmax 는 입력받은 값을 0 ~ 1 사이 값으로 모두 정규화하며 출력값들의 총합을 항상 1로 만드는 함수로, 각 class에 해당할 확률, 즉 업스케일링 비율 별 확률값을 출력하는 기능을 할 수 있다. 또한, Argmax는 다수의 label 중에서 가장 가능성 높은 것을 선택해 주는 함수로, 여기서는 업스케일 링 비율 별 확률 값 중 가장 큰 값을 가지는 비율을 선정해주는 기능을 할 수 있다. 즉, 최종적으로 원본 영상 이 업스케일링된 비율을 출력할 수 있다. 한편, 일 실시 예에 따르면, 제1 인공 지능 모델은 다양한 훈련 영상과 관련된 훈련 데이터에 기초하여 학습될 수 있다. 예를 들어, 제1 인공 지능 모델은, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처리 여부 또는 압축 및 업스케일링 간 순서 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정보 에 기초하여 학습될 수 있다. 예를 들어, SD, HD, Full HD, UHD와 같은 다양한 해상도, 10Mbps, 15Mbps, 20Mbps와 같은 다양한 비트레이트. 다양한 비트레이트 타입(예를 들어, variable bitrate 타입, constant bitrate 타입 또는 average bitrate 타입 등), MPEG(Moving Picture Experts Group)(예를 들어, MP2, MP4, MP7 등), JPEG(joint photographic coding experts group), AVC(Advanced Video Coding), H.264, H.265, HEVC(High Efficiency Video Codec), VC-1, VP8, VP9 및 AV1(AOMedia Video 1)와 같은 다양한 압축 방법에 기 초하여 획득된 다양한 타입의 훈련 영상이 학습에 이용될 수 있다. 도 10a 내지 도 10f는 일 실시 예에 따른 제1 인공 지능 모델의 학습을 위한 훈련 데이터를 획득하는 방법을 설 명하기 위한 도면들이다. 일 실시 예에 따르면, 제1 인공 지능 모델의 학습을 위한 학습 DB는 다양한 방법에 따라 생성된 훈련 데이터를 포함할 수 있다. 예를 들어, 제1 인공 지능 모델의 학습을 수행하는 외부 서버는 실제 방송 환경 시나리오을 고 려하여 다양한 방법으로 훈련 데이터를 생성할 수 있다. 다만, 경우에 따라 제1 인공 지능 모델의 학습이 전자 장치에서 이루어지고, 훈련 데이터 또한 전자 장치에서 생성될 수도 있음은 물론이다. 예를 들어, 해상도, 압축 방식, 압축률, 데이터 전송 속도, 업스케일링 비율, 인핸스 처리 여부, 압축 및 업스 케일링 간 순서 또는, 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상을 포함하는 학습 DB가 마련될 수 있다. 일 예로, 도 10a에 도시된 바와 같이 원본 영상을 AVC(Advanced Video Coding) 방식에 따라 압축한 후, 압축된 영상을 업스케일링하여 훈련 영상을 획득할 수 있다. 다른 예로, 도 10b에 도시된 바와 같이 원본 영상을 HEVC(High Efficiency Video Codec) 방식에 따라 압축한 후, 압축된 영상을 업스케일링하여 훈련 영상을 획득할 수 있다. 또 다른 예로, 도 10c에 도시된 바와 같이 원본 영상을 인핸스 처리하고 인핸스 처리된 영상을 AVC 방식에 따라 압축한 후, 압축된 영상을 업스케일링하여 훈련 영상을 획득할 수 있다. 또 다른 예로, 도 10d에 도시된 바와 같이 원본 영상을 인핸스 처리하고 인핸스 처리된 영상을 HEVC 방식에 따 라 압축한 후, 압축된 영상을 업스케일링하여 훈련 영상을 획득할 수 있다. 또 다른 예로, 도 10e에 도시된 바와 같이 원본 영상을 AVC 방식에 따라 압축한 후, 압축된 영상을 업스케일링 하고 업스케일링된 영상을 인핸스 처리하여 훈련 영상을 획득할 수 있다. 또 다른 예로, 도 10f에 도시된 바와 같이 원본 영상을 HEVC 방식에 따라 압축한 후, 압축된 영상을 업스케일링 하고 업스케일링된 영상을 인핸스 처리하여 훈련 영상을 획득할 수 있다. 일 실시 예에 따른 제1 인공 지능 모델은 상술한 바와 같은 다양한 타입의 훈련 영상으로터 획득된 훈련 데이터 (예를 들어, 특성 정보 및 업스케일링 비율)에 기초하여 학습될 수 있다. 다만, 도 10a 내지 도 10f에 도시된 실시 예는 이해를 돕기 위하여 특정 예를 든 것이며, 압축 방식은 MPEG(Moving Picture Experts Group)(예를 들어, MP2, MP4, MP7 등), JPEG(joint photographic coding experts group), AVC(Advanced Video Coding), H.264, H.265 및 HEVC(High Efficiency Video Codec) 중 적어 도 하나를 포함하는 다양한 방식이 적용될 수 있다. 또한, 인핸스 처리는 Noise Reduction, Noise Reduction Simple, Detail Enhancement, Detail Enhancement Simple, Tone Mapping, Contrast Enhancement, Color Enhancement 또는 Frame rate Conversion 중 적어도 하나를 포함할 수 있다. 또한, 영상 타입은 크게 broadcast/STB 두 가지 환경 기준으로 압축 방식, 압축률, enhance여부, 업스케일링과 압축 순서 등의 조건으로 추가 파생을 진행하였다. 또한, 원본 영상은 SD(Standard Definition), HD(High Definition), Full HD 또는 Ultra HD 영상 중 적어도 하나를 포함하는 다양한 해상도의 영상이 이용될 수 있다. 또한 원본 영상은 뉴스, 드 라마, 영화 중 다큐멘터리 중 적어도 하나를 포함하는 다양한 타입의 영상이 될 수 있다. 이는 영상 컨텐츠의 타입에 따라서도 영상 특성이 달라질 수 있기 때문이다. 그 외, 영상 복원(image restoration), 영상 변환 (image transformation)등 영상 특성에 영향을 줄 수 있는 다양한 영상 처리를 적용하여 훈련 영상을 생성할 수 있다. 상술한 바와 같이 실제 방송 환경에서 이용될 수 있는 다양한 훈련 영상을 이용하여 제1 인공 지능 모델을 학습 시킴으로써, 어떠한 입력 영상에 대해서도 최대한 정확한 업스케일링 비율을 획득할 수 있게 된다. 도 11a 내지 도 11c는 일 실시 예에 따른 제2 인공 지능 모델을 이용한 업스케일링 처리 방법을 자세히 설명하 기 위한 도면이다. 일 실시 예에 따른 제2 인공 지능 모델, 즉 다운스케일링된 영상에 대한 업스케일링을 위한 인공 지능 모델은 Super Resolution 처리를 위한 학습 네트워크 모델로 구현될 수 있다. Super Resolution이란 해상도가 낮은 영 상을 일련의 미디어 처리를 통해 높은 해상도로 변환하는 처리를 의미한다. 일 예에 따라 프로세서는 도 11a에 도시된 바와 같이 복수의 신경망 레이어들로 구성된 제2 인공 지능 모 델을 이용하여 다운스케일링된 영상을 업스케일링 처리할 수 있다. 복수의 신경망 레이어들 각각은 복수의 파라미터(parameters)(또는 복수의 가중치(weight values))를 포함하며, 이전(previous) 레이어의 연산 결과와 복수의 파라미터들 간의 연산을 통해 신경망 연산을 수행할 수 있다. 복수의 신경망 레이어들에 포함된 파라미 터들은 인공지능 모델의 학습 결과에 의해 최적화될 수 있다. 예를 들어, 학습 과정 동안 인공 지능 모델에서 획득한 로스(loss) 값 또는 코스트(cost) 값이 감소 또는 최소화되도록 파라미터들이 갱신될 수 있다. 인공 신 경망은 심층 신경망(DNN:Deep Neural Network)를 포함할 수 있으며, 예를 들어, CNN (Convolutional Neural Network), RNN (Recurrent Neural Network), GAN(Generative Adversarial Networks), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 또는 심층 Q-네트워크 (Deep Q-Networks) 등이 있으나, 전술한 예에 한정되지 않는다.다른 예에 따라 프로세서는 도 11b에 도시된 바와 같이 다운스케일링된 영상에 대해 보간 처리를 수행하고, 보간 처리된 영상을 제2 인공 지능 모델에 입력하여 잔차 영상을 획득할 수 있다. 즉, 제2 인공 지능 모델은 Residual neural network로 구현될 수 있다. 여기서, Residual neural network를 구 성하는 복수의 레이어 각각은 상이한 파라미터를 포함하는 필터를 이용하여 보간 처리된 영상에 대한 잔차 영상 을 생성할 수 있다. 여기서, 파라미터는 필터의 가중치(또는 계수)와 동일한 의미일 수 있다. 이 경우, 제2 인 공 지능 모델은 Identity Function, Logistic Sigmoid Function, Hyperbolic Tangent(tanh) Function, ReLU Function, Leaky ReLU Function 등 다양한 유형의 활성화 함수(Activation function)를 이용하여 연산을 수행할 수 있다. 다만, 제2 인공 지능 모델이 반드시 잔차 영상 만을 생성하는 것은 아니며, 제2 인공 지능 모델의 구현 예에 따라 다양한 방식으로 입력된 영상을 처리하고, 처리된 영상을 출력할 수 있다. 이 경우, 프로세서는 보간 처리된 영상을 잔차 영상과 결합하여 출력 영상, 예를 들어 고해상 도 영상을 획득할 수 있다. 여기서, 보간 처리는 해상도가 낮은 영상을 높은 해상도로 스케일링하는 처리를 의 미하며, 예를 들어 bilinear interpolation, nearest neighbor interpolation, bicubic interpolation, deconvolution interpolation, subpixel convolution interpolation, polyphase interpolation, trilinear interpolation, linear interpolation 중 적어도 하나의 보간 기법이 이용될 수 있다. 또한, 잔차 영상 (residual image)은 잔차 정보 만을 포함하는 영상을 의미할 수 있다. 여기서, 잔차 정보는 입력 영상과 기준 영상의 차이에 따른 정보로서, 예를 들어, 에지(edge) 방향, 에지 강도, 노이즈 정보 또는 텍스처(texture) 정 보 중 적어도 하나를 포함할 수 있으나, 이에 한정되는 것은 아니다. 다른 예에 따라 잔차 정보는 계조 정보, 밝기 정보 또는 감마 정보 중 적어도 하나를 포함할 수 있다. 다른 예에 따라 프로세서는 도 11c에 도시된 바와 같이 다운스케일링된 영상을 제2 인공 지능 모델 에 입력하여 잔차 영상(12’)을 획득하고, 잔차 영상(12')을 보간 처리하여 보간 처리된 잔차 영상 (12\")을 획득할 수 있다. 또한, 프로세서는 다운스케일링된 영상에 대해 보간 처리를 수행하여 보 간 처리된 영상을 획득할 수 있다. 이어서, 프로세서는 보간 처리된 영상을 보간 처리된 잔차 영 상(12\")과 결합하여 출력 영상, 예를 들어 고해상도 영상을 획득할 수 있다. 즉, 도 11c에 도시된 실시 예 에 따르면, 도 11b에 도시된 실시 예와 달리 다운스케일링된 영상을 제2 인공 지능 모델에 입력하여 잔 차 영상(12’)을 획득할 수도 있다. 다만, 다른 실시 예에 따르면, 제2 인공 지능 모델 외의 타 인공 지능 모델, 예를 들어 제3 인공 지능 모델 을 추가로 포함할 수 있음은 물론이다. 이 경우 제2 인공 지능 모델 및 제3 인공 지능 모델은 순차적으로 동작 하거나, 병렬적으로 동작할 수 있다. 일 예로, 프로세서는 제2 인공 지능 모델에 다운스케일링된 영상(1 0)을 입력하고, 제2 인공 지능 모델의 출력을 제3 인공 지능 모델에 입력한 후 제3 인공 지능 모델로부터 출력 되는 영상에 기초하여 출력 영상을 획득할 수 있다. 다른 예로 프로세서는 제2 및 제3 인공 지능 모델 각 각에 다운스케일링된 영상을 입력하고, 제2 및 제3 인공 지능 모델로부터 병렬적으로 출력되는 복수의 영상 에 기초하여 출력 영상을 획득할 수 있다. 예를 들어, 제2 인공 지능 모델은 제1 잔차 영상을 생성하는 모델이 고, 제3 인공 지능 모델은 제2 잔차 영상을 생성하는 모델을 포함할 수 있다. 또는 제2 인공 지능 모델은 해상 도의 업 스케일링을 위한 모델이고, 제3 인공 지능 모델은 상술한 다양한 영상 처리 중 하나(예를 들어 Noise reduction)를 위한 모델일 수 있다. 또는 제2 인공 지능 모델은 객체 영역 처리를 위한 모델이고, 제3 인공 지 능 모델은 배경 영역 처리를 위한 모델일 수 있다. 도 12는 일 실시 예에 따른 전자 장치의 동작을 설명하기 위한 도면이다. 도 12에서는 실시 예의 이해를 돕기 위하여 전자 장치가 4K UHD 디스플레이 장치로 구현되는 경우를 상정 하여 설명하도록 한다. 도 12에서는 설명의 편의를 위하여, 프로세서가 영상 분석 모듈, 다운스케일 러 및 업스케일러을 포함하는 것으로 도시하였다. 여기서, 영상 분석 모듈, 다운스케일러 및 업스케일러는 프로세서 내의 적어도 하나의 소프트웨어, 적어도 하나의 하드웨어 또는 이 들의 조합으로 구현될 수 있다. 도 12에서는 설명의 편의를 위하여 전자 장치로 입력되는 영상의 해상도는 예를 들어, SD, HD, FHD 및 UHD 중 어느 하나인 것으로 가정하였다. 전자 장치로 입력되는 영상에는 해상도 정보가 포함되어 있을 수 있으며, 프로세서는 입력 영상의 해 상도가 SD인 경우 업스케일링이 필요하다고 판단하고, 수신된 영상을 업스케일러로 제공할 수 있다.다만, 일 실시 예에 따르면, 프로세서는 입력 영상의 해상도가 HD, FHD, UHD 중 하나인 경우 입력 영상이 원본 영상인지 업스케일링된 영상인지 판단할 필요가 있다. 이 경우 프로세서는 입력 영상을 영상 분석 모 듈로 제공할 수 있다. 영상 분석 모듈은 입력 영상을 분석하여 입력 영상이 원본 영상인지 업스케일링된 영상인지 판단할 수 있 다. 예를 들어 상술한 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득하고, 획득된 업스케 일링 정보에 기초하여 입력 영상이 원본 영상인지 업스케일링된 영상인지 판단할 수 있다. 영상 분석 모듈은 입력 영상이 HD 또는 FHD의 원본 영상인 경우 입력 영상을 업스케일러로 제공할 수 있다. 출력 해상도가 UHD이기 때문에 해상도 업스케일링이 필요한 경우이기 때문이다. 또한, 영상 분석 모듈 은 입력 영상이 UHD의 원본 영상인 경우 별도의 업스케일링이 필요없다고 판단하고, 수신된 영상을 업스 케일러로 제공하지 않을 수 있다. 영상 분석 모듈은 입력 영상이 HD, FHD, UHD 중 하나의 업스케일 링된 영상인 경우 본 개시의 일 실시 예에 따른 다운 스케일링 및 업스케일링이 필요하다고 판단하고, 입력 영 상을 대응되는 다운스케일링 정보(예를 들어 다운스케일링 비율)와 함께 다운스케일러로 제공할 수 있다. 예를 들어, 영상 분석 모듈은 획득된 업스케일링 정보에 기초하여 다운스케일링 정보를 획득할 수 있다. 다운스케일러는 영상 분석 모듈로부터 제공된 다운스케일링 정보(예를 들어 다운스케일링 비율)에 기초하여 입력 영상을 다운스케일링할 수 있다. 다만, 영상 분석 모듈이 업스케일링 정보를 다운스케일러 로 제공하고 다운스케일러가 업스케일링 정보에 기초하여 다운스케일링 정보를 획득하는 것도 가능 하다. 다운스케일러는 입력 영상에 대한 다운스케일링을 수행하고, 다운스케일링된 영상을 업스케일러로 제공할 수 있다. 예를 들어, 다운 스케일러는 입력 영상을 원본 해상도로 다운스케일링할 수 있다. 업스케일러는 다운스케일러로부터 수신된 다운스케일링된 영상 즉, 원본 해상도로 다운스케일링된 영상을 출력 해상도에 해상도에 기초하여 업스케일링함으로써, 출력 영상을 획득할 수 있다. 예를 들어, 업스케 일러는 고화질 처리를 위한 업스케일링 기능 블럭을 이용하여 다운스케일링된 영상을 업스케일링하여 고 화질의 출력 영상을 획득할 수 있다. 예를 들어, 프로세서는 DLSR(dictionary learning and sparse representation) 기술을 이용하여 화질 처리가 수행된 영상을 업스케일링 처리할 수 있다. 여기서, DLSR(dictionary learning and sparse representation) 기술은 고해상도의 원본 영상 및 저해상도 영상을 포함 하는 학습 DB에 기초하여 학습된 CNN 모델을 이용하여 입력 영상의 해상도를 증대시키는 기술이다. 일 예에 따 라 DLSR 기술은 GAN(Generative Adversarial Networks)을 포함하여 DLSR 화질 처리를 거치면서 영상 내 texture 부분 생성 효과를 더 극대화시킬 수 있다. 일반적으로 GAN(Generative Adversarial Networks)은 생성 자(Generator)에서는 없는 데이터를 만들어내고, 구분자(Discriminator)는 진짜 데이터와 가짜 데이터를 구분해 내는 학습으로 둘을 경쟁적으로 함께 학습시켜 진짜와 비슷한 데이터를 만들어내도록 동작한다. 영상 화질 개선 에 이러한 GAN을 적용하게 되면, 원본 영상에는 없었으나 기존과 비슷한 영상 특징에 해당하는 texture를 자연 스럽게 생성할 수 있어, texture 영역에서의 디테일 향상을 기대할 수 있다. 이에 따라 고화질의 출력 영상을 획득할 수 있게 된다. 다만, DLSR은 일반적으로 x2, x3, x4의 정수배 비율로 업스케일링하는 구조로 설계될 수 있다. 이에 따라, 720x480의 해상도가 3840x2160으로 변환되기 위해서는 소수점 배율의 업스케일링이 필요하게 된다. 이를 위해, 비정수배 비율의 업스케일링을 위한 다양한 종래의 업스케일링 방식이 이용될 수 있다. 도 13은 도 12에 도시된 전자 장치 동작의 변형 실시 예를 설명하기 위한 도면이다. 도 13의 동작 중 도 12에 도시된 동작과 중복되는 구성에 대해서는 자세한 설명을 생략하도록 한다. 프로세서는 입력 영상을 영상 분석하여 입력 영상의 업스케일링 비율을 판단한다. 도 12에서 설명한 바와 같이 프로세서는 입력 영상이 HD, FHD, UHD와 같이 업스케일링이 수행되었을 가능성이 있는 영상에 대해서는 영상 분석을 수행하고, SD 영상에 대해서는 별도의 영상 분석을 수행하지 않을 수 있다. 예를 들어 프로세서는 상술한 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 여부 및 업스케일링 정보 를 식별할 수 있다. 프로세서는 입력 영상이 원본 영상을 업스케일링하여 획득한 영상으로 판단되면, 입력 영상의 다운스케일 링 비율에 기초하여 해당 영상을 다운스케일링할 수 있다. 예를 들어, 프로세서는 업스케일링된 영 상을 추정된 원본 영상의 해상도로 다운스케일링할 수 있다. 예를 들어, 프로세서는 4K UHD 해상도의 입력 영상이 SD 해상도의 원본 영상을 4K UDH 영상으로 업스케일링한 것으로 판단되면, 입력 영상을 SD 해상도의 영상으로 다운스케일링할 수 있다. 이어서, 프로세서는 추정된 원본 해상도로 다운스케일링된 영상에 대해 제1 화질 처리를 수행할 수 있다. 여기서. 제1 화질 처리는 원본 해상도(예를 들어, SD 해상도) 기준 화질 처리(예를 들어, Noise Reduction, Detail Enhancement)가 될 수 있다. 이 경우, 다운스케일링된 영상 예를 들어 저해상도 영상에 대해 화질 처리를 수행하기 때문에 화질 처리를 위한 평균값/히스토그램 등의 통계 값을 낮은 복잡도의 하드웨어 및/ 또는 소프트웨어로 산출할 수 있게 된다. 이어서, 프로세서는 제1 화질 처리가 수행된 영상을 출력 해상도(예를 들어 4K UDH)로 업스케일링 처리 할 수 있다. 이어서, 프로세서는 출력 해상도로 업스케일링된 영상에 대해 제2 화질 처리를 수행할 수 있다. 여 기서 제2 화질 처리는, 여기서. 제2 화질 처리는 출력 해상도 기준 화질 처리(예를 들어, Tone Mapping, Contrast Enhancement, Color Enhancement)가 될 수 있다. 이 후, 프로세서는 제2 화질 처리된 영상에 대해 제3 화질 처리(예를 들어, Frame rate Conversion)를 수 행하여 출력 영상을 획득할 수 있다. 다만, 프로세서는 영상 분석 결과 입력 영상이 업스케일링되지 않은 4K UHD 원본 영상인 경우 해당 영상에 대해 원본 해상도 기준 화질 처리(예를 들어, Noise Reduction Simple, Detail Enhancement Simple)를 수행한 후, 출력 해상도 기준 제2 화질 처리 및 제3 화질 처리 만을 수행하여 출력 영상을 획득할 수 있 다. 이는 입력 영상이 출력 해상도와 동일한 해상도의 4K UHD 원본 영상인 경우 별도의 업스케일링 처리가 필요 하지 않기 때문이다. 또한, 프로세서는 입력 영상이 SD 영상이 경우 제1 화질 처리, 업스케일링 , 제2 화질 처리, 제3 화질 처리를 수행할 수 있음은 물론이다. 다만, 상술한 입력 해상도 기준 화질 처리, 출력 해상도 기준 화질 처리는 일 예를 든 것이며, 해당 화질 처리 중 일부가 생략되거나, 해당 화질 처리 외 추가 화질 처리가 수행될 수도 있음은 물론이다. 도 14는 도 12 및 도 13에 도시된 영상 분석 동작을 자세히 설명하기 위한 도면이다. 도 14에 따르면, 프로세서는 입력 영상에서 우선 관심 영역을 식별한 후, 식별된 관심 영역에서 특 징(feature) 값을 추출할 수 있다. 다만, 관심 영역 식별은 경우에 따라 생략될 수 있다. 이어서, 프로세서는 추출된 특징 값을 Classfication Network에 입력하여 업스케일링 정보, 예를 들 어 업스케일링 비율을 획득할 수 있다. 여기서, Classfication Network는 상술한 제1 인공 지능 모델로 구현될 수 있다. 예를 들어, 프로세서는 각 프레임 단위로 대응되는 업스케일링 비율을 획득할 수 있다. 한편, 프로세서는 입력 영상에서 씬 체인지 시점을 검출하고, 획득된 업스케일링 비율 및 검출된 씬 체인지 시점에 기초하여 다운스케일링 비율을 결정하여 다운스케일링을 수행할 수 있다. 예를 들어, 프로 세서는 씬 체인지 시점에서 대응되는 프레임 즉, 다음 씬이 시작되는 프레임에 대응되는 다운스케일링 비 율을 결정하여 해당 프레임부터 다운스케일링을 수행할 수 있다. 또한, 프로세서는 동일한 업스케일링 비 율이 임계 개수(예를 들어 프레임 3개) 이상 지속적으로 검출되는 경우 이 후 씬 체인지 시점의 프레임부터 다 운스케일링을 수행할 수 있다. 만약, 하나의 scene이 재생되는 동안, 프레임마다 업스케일링 비율의 값이 다르고 업스케일링 비율이 달라질 때 마다 다운스케일링을 수행하게 된다면 사용자의 화질 변화 시인을 피할 수 없게 된다. 이러한 문제를 방지하기 위하여, 씬 검출을 수행하고, 씬 체인지 시점에서만 다운샘플링을 시작할 수 있다. 도 15는 일 실시 예에 따른 전자 장치의 하드웨어 구조를 설명하기 위한 도면이다. 도 15는 일 실시 예에 따른 프로세서 칩 구조를 나타내는 도면으로, 프로세서 칩은 다양한 프로세싱 을 위한 IP를 포함할 수 있다. 여기서, IP는, 재이용 가능한 기능블록을 지칭하며 하드웨어 또는 소프트웨어 기 능 블록이 될 수 있다. 이하에서는 설명의 편의를 위하여 IP가 하드웨어 IP로 구현되는 경우를 상정하여 설명한 다. 예를 들어, 영상 처리를 위한 프로세서 칩은 도 15에 도시된 바와 같이 메모리, CPU, Video Decoder, DS, NR, DE, UP, FRC의 하드웨어 IP가 버스(Bus)를 통해 연결되도록 구현될 수 있다. 도 15에서는 설명의 편의를 위하여, 도 13에 도시된 구성 중 제2 화질 처리 부분에 대응되는 구성은 생략하였다. 여기서, Video Decoder는 디코딩 처리를 위한 하드웨어 IP이다. 일 예에 따라 입력 영상은 압축 영상 데이 터일 수 있고, Video Decoder는 압축된 영상에 대한 디코딩 처리를 수행할 수 있다. 예를 들어, 입력 영상 은 주파수 변환 기반의 영상 압축 방법에 의해 부호화 처리된 영상 데이터일 수 있다. Video Decoder는 압 축 영상 데이터를 엔트로피 복호화하여 양자화된 잔차 데이터를 생성하는 과정, 양자화된 잔차 데이터를 역양자 화하는 과정, 주파수 영역 성분의 잔차 데이터를 공간 영역 성분으로 변환하는 과정, 예측 데이터를 생성하는 과정 또는 예측 데이터와 잔차 데이터를 이용하여 영상을 복원하는 과정 중 적어도 하나의 과정을 통해 압축 영 상 데이터를 복호화할 수 있다. 이와 같은 복호화 과정은 영상 압축시 수행된 부호화 과정에서 사용된 MPEG-2, H.264, MPEG-4, HEVC, VC-1, VP8, VP9 및 AV1 등 주파수 변환을 이용한 영상 압축 방법 중의 하나에 대응되는 영상 복원 방법으로 구현될 수 있다. DS는 Downscaling 처리를 위한 하드웨어 IP이고, NR은 Noise Reduction 처리를 위한 하드웨어 IP이 며, DE는 Detail Enhance 처리를 위한 하드웨어 IP이고, 여기서, UP은 Upscaling, 예를 들어 Super Resolution 처리를 위한 하드웨어 IP이고, FRC는 Frame Rate Conversion을 위한 하드웨어 IP가 될 수 있 다. 예를 들어, CPU는 메모리에 저장된 제1 인공 지능 모델에 대한 정보에 기초하여 DS의 동작 을 제어할 수 있다. 또한, CPU는 메모리에 저장된 제2 인공 지능 모델에 대한 정보에 기초하여 UP의 동작을 제어할 수 있다. 다만, 이에 한정되는 것은 아니며, DS, UP의 동작은 NPU와 같은 다른 프로세서에 의해 제어될 수 있음은 물론이다. 다만, 다른 실시 예에 따르면, 프로세서 칩 내에 포함된 다양한 하드웨어 IP들은 적어도 하나의 소프트웨 어 또는, 적어도 하나의 소프트웨어 및 적어도 하나의 하드웨어의 조합으로 구현될 수 있다. 예를 들어, Video Decoder의 일부 기능에 대응되는 로직은 Video Decoder 내에 구현되고, Video Decoder의 다른 기능에 해당하는 로직은 CPU에 의해 실행 가능한 소프트웨어로 구현될 수도 있다. 도 16은 일 실시 예에 따른 전자 장치의 일 구현 예를 나타내는 도면이다. 도 16에 따르면, 전자 장치(100\")는 프로세서, 메모리, 입력부, 디스플레이, 출력부 및 사용자 인터페이스를 포함한다. 도 10에 도시된 구성 중 도 2에 도시된 구성과 중복되는 구성에 대해서 는 자세한 설명을 생략하도록 한다. 입력부는 다양한 타입의 컨텐츠를 입력받는다. 예를 들어 입력부는 AP 기반의 Wi-Fi(와이파이, Wireless LAN 네트워크), 블루투스(Bluetooth), 지그비(Zigbee), 유/무선 LAN(Local Area Network), WAN(Wide Area Network), 이더넷(Ethernet), IEEE 1394, HDMI(High-Definition Multimedia Interface), USB(Universal Serial Bus), MHL(Mobile High-Definition Link), AES/EBU(Audio Engineering Society/ European Broadcasting Union), 옵티컬(Optical), 코액셜(Coaxial) 등과 같은 통신 방식을 통해 외부 장치(예를 들어, 소스 장치), 외부 저장 매체(예를 들어, USB 메모리), 외부 서버(예를 들어 웹 하드) 등으로부터 스트리밍 또는 다운로드 방식으로 영상 신호를 입력받을 수 있다. 여기서, 영상 신호는 SD(Standard Definition), HD(High Definition), Full HD 또는 Ultra HD 영상 중 어느 하나의 디지털 영상 신호가 될 수 있으나 이에 한정되는 것 은 아니다. 디스플레이는 자발광 소자를 포함하는 디스플레이 또는, 비자발광 소자 및 백라이트를 포함하는 디스플레 이로 구현될 수 있다. 예를 들어, LCD(Liquid Crystal Display), OLED(Organic Light Emitting Diodes) 디스플 레이, LED(Light Emitting Diodes), 마이크로 LED(micro LED), Mini LED, PDP(Plasma Display Panel), QD(Quantum dot) 디스플레이, QLED(Quantum dot light-emitting diodes) 등과 같은 다양한 형태의 디스플레이 로 구현될 수 있다. 디스플레이 내에는 a-si TFT, LTPS(low temperature poly silicon) TFT, OTFT(organic TFT) 등과 같은 형태로 구현될 수 있는 구동 회로, 백라이트 유닛 등도 함께 포함될 수 있다. 한 편, 디스플레이는 터치 센서와 결합된 터치 스크린, 플렉시블 디스플레이(flexible display), 롤러블 디스 플레이(rollable display), 3차원 디스플레이(3D display), 복수의 디스플레이 모듈이 물리적으로 연결된 디스 플레이 등으로 구현될 수 있다. 프로세서는 상술한 다양한 실시 예에 따라 획득된 출력 영상을 출력하도록 디스플레이를 제어할 수 있다. 여기서, 출력 영상은, 4K 또는 8K 이상의 고해상도 영상일 수 있다. 출력부는 음향 신호를 출력한다. 예를 들어, 출력부는 프로세서에서 처리된 디지털 음향 신호를 아날로그 음향 신호로 변환하고 증폭하여 출력할 수 있다. 예를 들어, 출력부는 적어도 하나의 채널을 출 력할 수 있는, 적어도 하나의 스피커 유닛, D/A 컨버터, 오디오 앰프(audio amplifier) 등을 포함할 수 있다. 일 예에 따라 출력부는 다양한 멀티 채널 음향 신호를 출력하도록 구현될 수 있다. 이 경우, 프로세서 는 입력 영상의 인핸스 처리에 대응되도록 입력된 음향 신호를 인핸스 처리하여 출력하도록 출력부를 제어할 수 있다. 예를 들어, 프로세서는 입력된 2채널 음향 신호를 가상의 멀티 채널(예를 들어, 5.1 채널) 음향 신호로 변환하거나, 전자 장치(100\")가 놓인 위치를 인식해 공간에 최적화된 입체 음향 신호로 처리 하거나, 입력 영상의 타입(예를 들어 컨텐츠 장르)에 따라 최적화된 음향 신호를 제공할 수 있다. 사용자 인터페이스는 버튼, 터치 패드, 마우스 및 키보드와 같은 장치로 구현되거나, 상술한 디스플레이 기능 및 조작 입력 기능도 함께 수행 가능한 터치 스크린, 리모콘 송수신부 등으로 구현될 수 있다. 리모콘 송 수신부는 적외선 통신, 블루투스 통신 또는 와이파이 통신 중 적어도 하나의 통신 방식을 통해 외부 원격 제어 장치로부터 리모콘 신호를 수신하거나, 리모콘 신호를 송신할 수 있다. 전자 장치 (100\")는 구현 예에 따라 튜너 및 복조부를 추가적으로 포함할 수 있다. 튜너(미도시)는 안테나를 통 해 수신되는 RF(Radio Frequency) 방송 신호 중 사용자에 의해 선택된 채널 또는 기 저장된 모든 채널을 튜닝하 여 RF 방송 신호를 수신할 수 있다. 복조부(미도시)는 튜너에서 변환된 디지털 IF 신호(DIF)를 수신하여 복조하 고, 채널 복호화 등을 수행할 수도 있다. 일 실시 예에 따라 튜너를 통해 수신된 입력 영상은 복조부(미도시)를 통해 처리된 후, 일 실시 예에 따른 영상 처리를 위해 프로세서로 제공될 수 있다. 도 17은 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 도 17에 도시된 전자 장치의 제어 방법에 따르면, 우선 제1 인공 지능 모델을 이용하여 입력 영상의 업스케일링 정보를 획득한다(S1710). 여기서, 제1 인공 지능 모델은 영상의 업스케일링(upscaling) 정보를 획득하도록 학습 된 모델일 수 있다. 여기서, 입력 영상의 업스케일링 정보는 입력 영상의 업스케일링 비율 정보 또는 입력 영상 의 원본 해상도 정보 중 적어도 하나를 포함할 수 있다. 이어서, 획득된 업스케일링 정보에 기초하여 입력 영상을 다운스케일링(downscaling)한다(S1620). 이 후, 출력 해상도에 기초하여 다운스케일링된 영상을 업스케일링하여 출력 영상을 획득한다(S1630). 또한, 출력 영상을 획득하는 S1630 단계에서는, 다운 스케일링된 영상에 대해 제1 화질 처리를 수행하고, 제1 화질 처리된 영상을 업스케일링하며, 업스케일링된 영상에 대해 제2 화질 처리를 수행하여 출력 영상을 획득할 수 있다. 또한, 출력 영상을 획득하는 S1630 단계에서는, 제2 인공 지능 모델을 이용하여 다운스케일링된 영상을 업스케 일링할 수 있다. 여기서, 제2 인공 지능 모델은 초해상도 처리(super resolution)를 수행하도록 학습된 모델일 수 있다. 일 실시 예에 따르면 제1 인공 지능 모델은, 영상의 특성 정보에 기초하여 영상의 업스케일링 정보를 획득하도 록 학습된 모델일 수 있다. 이 경우, 입력 영상의 업스케일링 정보를 획득하는 S1610 단계에서는 입력 영상으로 부터 획득된 특성 정보를 제2 인공 지능 모델에 입력하여 입력 영상의 업스케일링 정보를 획득할 수 있다. 이 경우, 입력 영상의 업스케일링 정보를 획득하는 S1610 단계에서는, 입력 영상에서 에지 영역을 식별하고, 식 별된 에지 영역에 포함된 픽셀에 대한 특성 정보를 획득할 수 있다. 또한, 입력 영상의 업스케일링 정보를 획득하는 S1610 단계에서는, 식별된 에지 영역을 블러링 처리하고, 블러 링된 에지 영역에 포함된 중심 픽셀을 기준으로 마진(margin) 영역 외의 픽셀에 대한 특성 정보를 획득하고, 획 득된 특성 정보를 제1 인공 지능 모델에 입력하여 입력 영상의 업스케일링 정보를 획득할 수 있다. 또한, 입력 영상의 업스케일링 정보를 획득하는 S1610 단계에서는, 입력 영상에서 관심 영역을 식별하고, 식별 된 관심 영역에 포함된 에지 영역을 식별할 수 있다. 다른 실시 예에 따르면, 제1 인공 지능 모델은, 영상 또는 영상의 에지 영역에 기초하여 영상의 업스케일링 정 보를 획득하도록 학습된 모델일 수 있다. 이 경우, 입력 영상의 업스케일링 정보를 획득하는 S1610 단계에서는 입력 영상 또는 상기 입력 영상의 에지 영역을 포함하는 영상을 제1 인공 지능 모델에 입력하여 입력 영상의 업스케일링 비율을 획득할 수 있다. 한편, 입력 영상을 다운스케일링하는 S1620 단계에서는, 입력 영상의 씬이 변경되는 시점에 기초하여 입력 영상 의 다운스케일링 여부를 결정할 수 있다. 또한, 입력 영상을 다운스케일링하는 S1620 단계에서는, 입력 영상의 제1 씬 구간에서 업스케일링 정보가 획득 되는 경우, 제1 씬 구간 이후의 제2 씬 구간에 포함된 프레임부터 업스케일링 정보에 기초하여 다운스케일링할 수 있다. 또한, 입력 영상을 다운스케일링하는 S1620 단계에서는, 입력 영상의 기설정된 프레임 구간 단위로 업스케일링 정보를 획득하고, 제1 씬 구간 이후의 임계 개수의 씬 구간에서 동일한 업스케일링 정보가 유지되는 경우 임계 개수의 씬 구간 이후의 제2 씬 구간이 시작되는 프레임부터 다운스케일링할 수 있다. 일 실시 예에 따르면, 제1 인공 지능 모델은, 압축 방식, 압축률, 업스케일링 비율, 인핸스 처리 여부, 압축 및 업스케일링 간 순서 또는 영상 타입 중 적어도 하나가 상이한 복수의 훈련 영상과 관련된 정보에 기초하여 학습 될 수 있다. 상술한 다양한 실시 예에 따르면, 외부에서 업스케일링된 영상의 원본 해상도를 추정하여 다운스케일링한 후, 고성능의 업스케일링 기술을 이용하여 업스케일링함으로써 고화질의 영상을 제공할 수 있게 된다. 또한, 업스케일링된 영상에 대해 하드웨어 비용의 이유로 수행하기 어려운 화질 처리를 다운스케일링된 영상에 대해 수행함으로써, 낮은 복잡도의 하드웨어를 이용할 수 있고 그에 따라 비용 측면에서 이득을 볼 수 있다. 한편, 상술한 본 개시의 실시 예를 적용하게 되면, 입력 영상의 해상도와 출력 영상의 해상도가 동일할지라도 화질에는 뚜렷한 차이가 발생하게 된다. 이 경우 본 개시의 실시 예가 적용된 것으로 판단할 수 있다. 본 개시의 다양한 실시 예들은 디스플레이 장치 뿐 아니라, 셋탑 박스와 같은 영상 수신 장치, 영상 처리 장치 등 영상 처리가 가능한 모든 전자 장치에 적용될 수 있음은 물론이다. 또한, 상술한 본 개시의 다양한 실시 예 들은 전자 장치에 구비된 임베디드 서버, 또는 영상 처리 장치의 외부 서버를 통해 수행되는 것도 가능하다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 설치 가능한 어플리케이션 또는 소프트웨어 형태로 구현될 수 있다. 또한, 상술한 본 개시의 다양한 실시 예들에 따른 방법들은, 기존 전자 장치에 대한 소프트웨어 업그레이드, 또 는 하드웨어 업그레이드 만으로도 구현될 수 있다. 한편, 상술한 본 개시의 실시예들은 컴퓨터에서 실행될 수 있는 프로그램 또는 인스트럭션으로 작성가능하고, 작성된 프로그램 또는 인스트럭션은 매체에 저장될 수 있다. 매체는 컴퓨터로 실행 가능한 프로그램 또는 인스트럭션을 계속 저장하거나, 실행 또는 다운로드를 위해 임시 저장하는 것일 수도 있다. 또한, 매체는 단일 또는 수개 하드웨어가 결합된 형태의 다양한 기록수단 또는 저장 수단일 수 있는데, 어떤 컴퓨터 시스템에 직접 접속되는 매체에 한정되지 않고, 네트워크 상에 분산 존재하는 것일 수도 있다. 매체의 예시로는, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체, CD-ROM 및 DVD와 같은 광기록 매체, 플롭티컬 디스크(floptical disk)와 같은 자기-광 매체(magneto-optical medium), 및 ROM, RAM, 플래시 메모리 등을 포함하여 프로그램 명령어가 저장되도록 구성된 것이 있을 수 있다. 또한, 다른 매체의 예시로, 애플리케이션을 유통하는 앱 스토어나 기타 다양한 소프트웨어를 공급 내지 유통하는 사이트, 서버 등에서 관리하는 기록매체 내지 저장매체도 들 수 있다. 또한, 일 실시 예에 따르면, 이상에서 설명된 다양한 실시 예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래 될 수 있다. 컴퓨터 프로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD- ROM))의 형태로, 또는 어플리케이션 스토어(예: 플레이 스토어TM)를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 한편, 상술한 인공 지능 모델은, 소프트웨어 모듈로 구현될 수 있다. 소프트웨어 모듈(예를 들어, 명령어 (instruction)를 포함하는 프로그램 모듈)로 구현되는 경우, 인공 지능 모델은 컴퓨터로 읽을 수 있는 판독 가 능한 기록매체에 저장될 수 있다.또한, 인공 지능 모델은 다운로드 가능한 소프트웨어 형태로 제공될 수도 있다. 컴퓨터 프로그램 제품은 제조사 또는 전자 마켓을 통해 전자적으로 배포되는 소프트웨어 프로그램 형태의 상품(예를 들어, 다운로드 가능한 애 플리케이션)을 포함할 수 있다. 전자적 배포를 위하여, 소프트웨어 프로그램의 적어도 일부는 저장 매체에 저장 되거나, 임시적으로 생성될 수 있다. 이 경우, 저장 매체는 제조사 또는 전자 마켓의 서버, 또는 중계 서버의 저장매체가 될 수 있다. 또한, 상술한 다양한 실시 예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구 성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요 소가 다양한 실시 예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로 그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시 예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동 작들은 순차적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2019-0156162", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진 자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술 적 사상이나 전망으로부터 개별적으로 이해되어서는 안될 것이다."}
{"patent_id": "10-2019-0156162", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 이해를 돕기 위한 전자 장치의 화질 처리 동작을 설명하기 위한 도면이다. 도 2는 일 실시 예에 따른 전자 장치의 구성을 나타내는 블럭도이다. 도 3은 일 실시 예에 따른 전자 장치의 동작을 설명하기 위한 흐름도이다. 도 4a 및 도4b는 일 실시 예에 따른 제1 인공 지능 모델의 학습 방법을 설명하기 위한 도면들이다. 도 5는 일 실시 예에 따라 제1 인공 지능 모델에 입력되는 특성 정보를 획득하는 방법을 설명하기 위한 도면이 다. 도 6a 및 도 6b는 일 실시 예에 따른 블러링 처리를 설명하기 위한 도면들이다. 도 7a 및 도 7b는 일 실시 예에 따른 특성값 획득 방법을 설명하기 위한 도면이다. 도 8은 일 실시 예에 따른 씬 기반 다운스케일링 방법을 설명하기 위한 도면이다. 도 9a 및 도 9b는 일 실시 예에 따른 제1 인공 지능 모델의 구조를 설명하기 위한 도면들이다. 도 10a 내지 도 10f는 일 실시 예에 따른 제1 인공 지능 모델의 학습을 위한 훈련 데이터를 획득하는 방법을 설 명하기 위한 도면들이다. 도 11a 내지 도 11c는 일 실시 예에 따른 제2 인공 지능 모델을 이용한 업스케일링 처리 방법을 설명하기 위한 도면이다. 도 12는 일 실시 예에 따른 전자 장치의 동작을 설명하기 위한 도면이다. 도 13은 도 12에 도시된 전자 장치 동작의 변형 실시 예를 설명하기 위한 도면이다. 도 14는 도 12, 도 13에 도시된 영상 분석 동작을 자세히 설명하기 위한 도면이다. 도 15는 일 실시 예에 따른 전자 장치의 하드웨어 구조를 설명하기 위한 도면이다. 도 16은 일 실시 예에 따른 전자 장치의 일 구현 예를 나타내는 도면이다. 도 17은 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다."}
