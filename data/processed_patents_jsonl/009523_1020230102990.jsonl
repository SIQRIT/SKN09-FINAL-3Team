{"patent_id": "10-2023-0102990", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0021836", "출원번호": "10-2023-0102990", "발명의 명칭": "발화자의 음성 및 감정을 인식하는 방법 및 시스템", "출원인": "한양대학교 산학협력단", "발명자": "장준혁"}}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "음성 신호에 기반하여 발화자의 음성 및 감정을 인식하는 방법에 있어서,발화자의 음성 데이터를 수신하는 단계;상기 음성 데이터를 GST 모듈에 적용하여 스타일 임베딩을 추출하는 단계;상기 스타일 임베딩을 이용하여 음성 인식 모델을 컨디셔닝하는 단계;상기 음성 인식 모델에 상기 음성 데이터를 입력하여 상기 음성 데이터에 대응하는 문자 토큰 및 감정 토큰을출력하는 단계;를 포함하는발화자의 음성 및 감정을 인식하는 방법."}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서,상기 음성 인식 모델은,컨포머 기반 AED 모델인 것을 특징으로 하는,발화자의 음성 및 감정을 인식하는 방법."}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서,상기 컨디셔닝하는 단계는,하기 수식에 따르는 것을 특징으로 하는,발화자의 음성 및 감정을 인식하는 방법.Se는 스타일 임베딩, FEN은 feed-forward network, 과 은 m번째 컨포머 인코더 블록의 숨겨진 표현"}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2 항에 있어서,상기 AED 모델은,하기 수식에 따른 손실 함수로 학습되는 것을 특징으로 하는,공개특허 10-2025-0021836-3-발화자의 음성 및 감정을 인식하는 방법.α하이퍼파라미터(0 ≤ α ≤ 1), x 입력 음성 발화, c 레이블의 목표 시퀀스"}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2 항에 있어서,상기 문자 토큰 및 감정 토큰을 출력하는 단계는,상기 음성 데이터를 인식하여 상기 문자 토큰을 생성하는 단계;상기 음성 데이터 및 상기 문자 토큰에 기반하여 발화자의 감정 상태를 분류하고, 상기 감정 상태에 대응하는상기 감정 토큰을 출력하는 단계;를 포함하는,발화자의 음성 및 감정을 인식하는 방법."}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "음성 신호에 기반하여 발화자의 음성 및 감정을 인식하는 장치에 있어서,발화자의 음성 데이터를 수신하는 수신부;상기 음성 데이터를 입력데이터로 하여 스타일 임베딩을 추출하는 GST 모듈;상기 스타일 임베딩을 이용하여 컨디셔닝하며, 상기 음성 데이터를 입력데이터로 하여 STT(Speech to text) 음성인식 방법에 따라 문자 토큰을 생성하고, 상기 음성 데이터 및 상기 문자 토큰에 기반하여 발화자의 감정 토큰을 출력하는 AED 모델;을 포함하는,음성 및 감정을 인식하는 장치."}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 AED 모델은,하기 수식에 따라 컨디셔닝 되는 것을 특징으로 하는,음성 및 감정을 인식하는 장치.Se는 스타일 임베딩, FEN은 feed-forward network, 과 은 m번째 컨포머 인코더 블록의 숨겨진 표현공개특허 10-2025-0021836-4-청구항 8 제6 항에 있어서,상기 AED 모델은,하기 수식에 따른 손실 함수로 학습되는 것을 특징으로 하는,음성 및 감정을 인식하는 장치.α하이퍼파라미터(0 ≤α ≤ 1), x 입력 음성 발화, c 레이블의 목표 시퀀스"}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6 항에 있어서,상기 AED 모델은,상기 문자 토큰과 상기 감정 토큰을 직렬로 출력하는 것을 특징으로 하는,음성 및 감정을 인식하는 장치."}
{"patent_id": "10-2023-0102990", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "컴퓨터를 이용하여 제1 항 내지 제5 항 방법 중 어느 하나의 방법을 실행시키기 위하여 매체에 저장된 컴퓨터프로그램."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 음성 신호에 기반하여 발화자의 음성 및 감정을 인식하는 방법은 발화자의 음성 데 이터를 수신하는 단계, 음성 데이터를 GST 모듈에 적용하여 스타일 임베딩을 추출하는 단계, 스타일 임베딩을 이 용하여 음성 인식 모델을 컨디셔닝하는 단계, 음성 인식 모델에 음성 데이터를 입력하여 음성 데이터에 대응하는 (뒷면에 계속)"}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 발화자의 음성 및 감정을 인식하는 방법 및 시스템에 관한 것으로서, 더욱 상세하게는 GSTs 모듈을 통해 추출한 스타일 임베딩을 이용하여 기학습된 컨포머 기반 음성 인식 모델을 컨디셔닝하고, 음성 인식 모델 을 통해 문자 토큰 및 감정 토큰을 출력하는 방법 및 시스템에 관한 것이다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "사람의 음성을 통해 발화자의 감정을 인식하는 기술에 대한 연구가 진행되고 있다. 특히, 인공 지능 기술이나 기계 학습 기술을 이용하는, 스마트 음성 감정 인식(SER, Speech Emotion Recognition) 기술은 디지털 오디오 신호 처리의 새로운 분야로 알려지고 있으며, 인간-컴퓨터 상호 작용(HCI, Human Computer Interface) 기술과 관련된 많은 응용 프로그램에서 중요한 역할을 할것으로 기대하고 있다. 기존의 연구는 음성 데이터에서 감정인식을 모델링 하기위해 다양한 수의 심층 신경망(DNN)을 도입하고 있다. 예를 들어, 원본 오디오 샘플에서 중요한 신호를 감지하는 DNN 모델이 제안되거나, 오디오 녹음의 특정 표현을 사용하여 모델에 대한 입력을 제공하는 기술이 제안되었다. 특히, 연구자들은 다양한 유형의 컨볼루션 연산을 통해 숨겨진 신호를 추출하고 선, 곡선, 점, 모양 및 색상을 인식하고 있다. 예를 들면, CNN(convolution neural networks), RNN(recurrent neural networks), LSTM(long short-term memory), DBN(deep belief networks) 등을 포함하는 중간 수준의 종단 간 모델을 활용하고 있다. 다만, 이러한 다양한 인공 신경망 모델의 구성이 여전히 부실하기 때문에 정확도 수준과 인식률이 낮다는 문제 가 존재한다. CNN을 이용한 모델의 경우 감정 인식의 정확도를 높이는 역할이 부족하다. 또한, 시간에 있어서 장기적인 변화요소를 학습하고, 감정을 인식하기 위해 RNN과 LSTM을 활용하고 있는데, 정 확도를 크게 향상시키지 못하면서도 전체 모델의 계산 및 학습 시간을 증가시키는 문제가 있다. 이에 감정 인식 의 정확도를 향상시키기 위한 다양한 방법이 제안되고 있다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상기 설명한 문제점을 해결하기 위해 고안된 발명으로서, 감정인식의 정확도 향상을 위해 GSTs 모듈 을 통해 추출한 스타일 임베딩을 이용하여 기학습된 컨포머 기반 음성 인식 모델을 컨디셔닝하고, 음성 인식 모 델을 통해 문자 토큰 및 감정 토큰을 출력하는 방법 및 시스템을 제공한다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 음성 신호에 기반하여 발화자의 음성 및 감정을 인식하는 방법은 발화자의 음성 데 이터를 수신하는 단계, 음성 데이터를 GST 모듈에 적용하여 스타일 임베딩을 추출하는 단계, 스타일 임베딩을 이용하여 음성 인식 모델을 컨디셔닝하는 단계, 음성 인식 모델에 음성 데이터를 입력하여 음성 데이터에 대응 하는 문자 토큰 및 감정 토큰을 출력하는 단계를 포함하는 것을 특징으로 한다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명에서는 음성신호로부터 발화자의 음성 및 감정을 모두 인식할 수 있는 장치를 제공한다. 본 발명에 따른 장치는 GSTs(Global style token) 모듈과 컨포머 기반 음성 인식 모델을 함께 이용하는 방법을 통해 과제를 해 결한다. 즉, 본 발명은 GSTs 모듈을 통해 추출한 스타일 임베딩을 이용하여 기학습된 컨포머 기반 음성 인식 모 델을 컨디셔닝하고, 음성 인식 모델을 통해 문자 토큰 및 감정 토큰을 출력하도록 한다. 본 발명은 낮은 단어오 류율과 높은 감정 분류 정확도를 보이는 발화자의 음성 및 감정을 인식하는 장치를 제공한다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 참조하여 본 명세서에 개시된 실시예를 상세히 설명하되, 도면 부호에 관계없이 동일하거 나 유사한 구성요소는 동일한 참조 번호를 부여하고 이에 대한 중복되는 설명은 생략하기로 한다. 이하의 설명 에서 사용되는 구성요소에 대한 접미사 \"모듈\" 및 \"부\"는 명세서 작성의 용이함만이 고려되어 부여되거나 혼용 되는 것으로서, 그 자체로 서로 구별되는 의미 또는 역할을 갖는 것은 아니다. 또한, 본 명세서에 개시된 실시 예를 설명함에 있어서 관련된 공지 기술에 대한 구체적인 설명이 본 명세서에 개시된 실시예의 요지를 흐릴 수 있다고 판단되는 경우 그 상세한 설명을 생략한다. 또한, 첨부된 도면은 본 명세서에 개시된 실시예를 쉽게 이해할 수 있도록 하기 위한 것일 뿐, 첨부된 도면에 의해 본 명세서에 개시된 기술적 사상이 제한되지 않으며, 본 명세서의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2 등과 같이 서수를 포함하는 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소 들은 상기 용어들에 의해 한정되지는 않는다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함한다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되 어야 한다. 도 1은 본 명세서의 일 실시예에 따른 AI 장치의 블록도이다. AI 장치는 AI 프로세싱을 수행할 수 있는 AI 모듈을 포함하는 전자 기기 또는 상기 AI 모듈을 포함하는 서 버 등을 포함할 수 있다. 또한, AI 장치는 전자기기의 적어도 일부의 구성으로 포함되어 AI 프로세싱 중 적 어도 일부를 함께 수행하도록 구비될 수도 있다. AI 장치는 AI 프로세서, 메모리 및/또는 통신부를 포함할 수 있다. AI 장치는 신경망을 학습할 수 있는 컴퓨팅 장치로서, 서버, 데스크탑 PC, 노트북 PC, 태블릿 PC 등과 같은 다양한 전자 장치로 구현될 수 있다. AI 프로세서는 메모리에 저장된 프로그램을 이용하여 신경망을 학습할 수 있다. 특히, AI 프로세서(2 1)는 주식 종목의 할당 비중인 액션 예측 정보를 추정하기 위한 인공지능 모델을 생성할 수 있고, 수집된 주식 가격 데이터 및 기술적 지표 데이터를 이용하여, 이러한 인공지능 모델을 강화학습시킬 수 있다. 한편, 전술한 바와 같은 기능을 수행하는 AI 프로세서는 범용 프로세서(예를 들어, CPU)일 수 있으나, 인공 지능 학습을 위한 AI 전용 프로세서(예를 들어, GPU, graphics processing unit)일 수 있다. 메모리는 AI 장치의 동작에 필요한 각종 프로그램 및 데이터를 저장할 수 있다. 메모리는 비 휘발 성 메모리, 휘발성 메모리, 플래시 메모리(flash-memory), 하드디스크 드라이브(HDD) 또는 솔리드 스테이트 드 라이브(SDD) 등으로 구현할 수 있다. 메모리는 AI 프로세서에 의해 액세스되며, AI 프로세서에 의 한 데이터의 독취/기록/수정/삭제/갱신 등이 수행될 수 있다. 또한, 메모리는 본 명세서의 일 실시예에 따른 데이터 분류/인식을 위한 학습 알고리즘을 통해 생성된 신경 망 모델(예를 들어, 딥 러닝 모델)을 저장할 수 있다. 한편, AI 프로세서는 데이터 분류/인식을 위한 신경망을 학습하는 데이터 학습부를 포함할 수 있다. 예를 들어, 데이터 학습부는 학습에 이용될 학습 데이터를 획득하고, 획득된 학습데이터를 딥러닝 모델에 적용함으로 써, 딥러닝 모델을 학습할 수 있다. 통신부는 AI 프로세서에 의한 AI 프로세싱 결과를 외부 전자 기기로 전송할 수 있다. 여기서 외부 전자 기기는 다른 단말, 서버를 포함할 수 있다. 한편, 도 1에 도시된 AI 장치는 AI 프로세서와 메모리, 통신부 등으로 기능적으로 구분하여 설 명하였지만, 전술한 구성요소들이 하나의 모듈로 통합되어 AI 모듈 또는 인공지능(AI) 모델로 호칭될 수도 있다.이하에서는 도 2 내지 도 5를 참고하여, 본 발명의 발화자의 음성 및 감정을 인식하는 방법 및 장치에 관하여 설명한다. 도 2는 본 명세서의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 장치의 구성도이다. 자동 음성 인식(Automatic speech recognition, 이하 ASR)과 음성 감정 인식 (speech emotion recognition, 이 하 SER)은 음성의 음향적 특징에 밀접한 관련이 있으며, 음향적 특징에 해당하는 음성의 음조(pitch), 억양 (tone) 및 세기(intensity)는 발화자의 감정 상태에 따라 변할 수 있다. 본 발명은 음성 및 감정을 인식하는 장 치에 관한 것으로, 음성 데이터로부터 텍스트와 함께 감정 토큰이 인식되는 공동 ASR 및 SER 작업(음성 및 감정 공동 인식 작업)에 초점을 맞추고 있다. 음성 및 감정의 공동 인식 성능을 더욱 향상시키기 위해 본 발명은 글 로벌 스타일 토큰(global style token, 이하 GSTs)을 적용한다. 즉, 획득한 음성 신호를 GSTs 모듈에 입력하여 스타일 임베딩을 추출하고, 이를 이용해 음성으로부터 감정 토큰을 추출하는 음성 인식 모델을 컨디셔닝한다. 본 발명의 음성 인식 모델은 ASR와 SER 모델을 병합한 모델이다. 구체적으로, 대규모 데이터셋에서 사전 학습된 컨포머 기반 음성 인식 모델을 음성신호의 여러 정보를 포함하고 있는 스타일 임베딩을 이용하여 피드 포워드 (Feed forward)를 통해 컨디셔닝한다. 컨디셔닝한 음성 인식 모델에 음파 형태의 음성 데이터를 입력하여 음성 데이터에 대응되는 문자 토큰을 추출하고, 음성 데이터 및 문자 토큰을 이용하여 감정 토큰을 출력하도록 한다. 본 발명의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 장치는 낮은 단어오류율과 높은 감정 분류 정확 도를 보인다. 본 발명에서는 음성신호로부터 발화자의 음성 및 감정을 모두 인식할 수 있는 장치를 제공한다. 본 발명에 따른 장치는 GSTs(Global style token) 모듈과 컨포머 기반 음성 인식 모델을 함께 이용하는 방법을 통해 과제를 해 결한다. 즉, 본 발명은 GSTs 모듈을 통해 추출한 스타일 임베딩을 이용하여 기학습된 컨포머 기반 음성 인식 모 델을 컨디셔닝하고, 음성 인식 모델을 통해 문자 토큰 및 감정 토큰을 출력하도록 한다. 도 2를 참조하면, 본 발명의 일 실시예에 따른 음성 및 감정 인식 장치는 GTSs 모듈과 음성인식 모델을 포함할 수 있다. 본 발명에 따른 GTSs모듈은 스타일을 모델링하는 강력한 E2E(end to end)시스템으로, 직관적이고 구현하기 쉬우 며 명백한 라벨들 없이 학습이 가능하다는 특징을 가지고 있다. 특히 표현이 풍부한 데이터로 학습시킬 때, GST 모델은 스타일을 제어하고 변환하는데 쓰이는 해석 가능한 임베딩들을 생성하여 준다. 즉, GSTs 모듈은 입력된 음성 데이터로부터 스타일 임베딩(style embedding)을 추출하여 Test-To-Speech(TTS)에서 음향 특징 표현 (acoustic feature representation)과 연관시켜(concatenate) 최종적으로 음성 인식 모델이 높은 정확도로 감 정 토큰을 생성하는데 기여하게 된다. GSTs 아키텍처는 추론 모드에서 강력하고 유연한 제어를 위해 설계되었다. 다만, 본 발명에서 사용되는 GSTs 모 듈은 종래 공지기술에 따른 GSTs 모듈은 무엇이든 사용 가능한 바, 이에 대한 자세한 설명은 생략하도록 한다. 음성 인식 모델은 도 2의 왼쪽에 그려진 컨포머(Conformer) 기반의 AED 모델을 사용할 수 있다. 음성 인식 모델 은 사전 훈련된 모델을 적용하여 효율적인 수렴과 공동 ASR 및 SER 훈련의 최적 성능을 얻을 수 있다. AED 모델 은 일반적인 end-to-end ASR에서 사용되며, 본 발명에 따른 모델은 전체 손실 함수를 AED와 CTC 손실 함수의 가 중 합으로 설계하여 학습하였으며, 전체 손실 함수는 다음과 같다. 여기서 α는 0 ≤ α ≤ 1을 만족하는 하이퍼파라미터이며, x ∈ RT''와 c∈ RL+1은 각각 입력 음성 발화와 해당 레이블의 목표 시퀀스를 나타낸다. 여기서 x의 해당하는 단어 시퀀스는 c' ∈ RL로 표시되며, T', L 및 N은 각 각 원시 음성, 단어 시퀀스의 길이 및 배치 크기를 나타낸다. 본 발명의 목표 단어 시퀀스는 해당하는 음성 x의 감정 토큰 e를 포함하여 다음과 같이 정의됩니다: c = c'∪{e}, 여기서 e ∈{e1, ..., en}이며, n개의 감정 클 래스 중 하나이다. 마지막으로, PAED와 PCTC는 각각 AED와 CTC 손실에 대한 입력 x에 대한 c의 사후 확률을 나타 낸다. 특히, PAED는 다음과 같이 추가 상태 토큰인 문장의 끝을 나타내는 기호인 eos를 사용하여 수식화될 수 있 다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 cL+2 = eos, cL+1 = e, c0 = sos로 정의되며, v는 어텐션 방법 내에서 인코더와 디코더 출력의 관련 부분 을 집계하는 컨텍스트 벡터를 나타낸다. 시작-문장 기호인 sos는 디코더가 어떠한 출력도 생성하기 전에 사용되 며, 디코더의 초기 입력으로 사용된다. PCTC는 CTC에 대한 사후 확률을 나타내며, CTC에 대한 모든 가능한 정렬 에 대해 주변화하여 수식화될 수 있다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서 aT = e이며, 는 모든 유효한 정렬의 집합을 나타낸다. 각 시간 프레임 t의 정렬은 at로 표시된다. 또 한, hm ∈ RTХD는 m번째 컨포머 인코더 블록의 출력을 나타내며, M, T, D는 각각 인코더 블록의 수, 프레임 길이 및 hm의 표현 특징 크기를 나타낸다. 사전 훈련된 모델을 사용하여 초기화된 컨포머 기반 AED 모델을 기반으로 하는 본 발명의 기준선 모델은 손실 함수로 세밀하게 조정하여 이전 공동 ASR 및 SER 연구와 비교해 확장된 방 법이다. 특히, 사전 훈련된 모델을 사용하면 효율적인 수렴을 통해 학습 과정이 개선되며, 단어 시퀀스의 끝에 감정 토큰을 배치함으로써 디코더의 AED 능력을 활용하여 감정 상태를 더 정확하게 분류할 수 있습니다. 본 발명은 말의 속성을 모델링하는 데 일반적으로 사용되는 GSTs(GSTs module) 모듈을 사용한다. 감정적인 발화 와 화자의 감정 상태를 공동으로 훈련할 때 이 모듈을 사용합니다. 스타일 임베딩 se ∈ RD'는 발화 x에 대한 GSTs 모듈의 출력값이다. 그런 다음 모든 컨포머 인코더 레이어의 반스텝 FFN 출력에 이를 추가하기 위해 하나 의 feed-forward network(FFN)를 사용한다. 상기 언급한 상세한 수식은 다음과 같습니다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서 과 은 각각 m번째 컨포머 인코더 블록의 숨겨진 표현을 나타내며, 그 차원은 RTХD이다. Se는 스타일 임베딩, FEN은 feed-forward network 이다. 그림 2에 설명된 것처럼, 컨포머 인코더 블록의 마지막 부분은 종래 기술과 동일하게 구성됩니다. 하나의 FFN 모듈에 의해 매핑된 스타일 임베딩은 모든 인코더 블록에 여러 번 조 건을 주는 데 사용된다.이하, 도 3 및 도 4는 실험적 설정과 본 발명이 ASR 시스템의 정확성과 견고성을 향상시키는 효과를 확인하기 위해 얻은 결과를 나타낸다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "본 발명은 먼저 스타일 임베딩에 조건을 주지 않은 컨포머 기반 AED 모델에서 음성 및 감정 공동 인식 작업을 평가했다. 그 후, 타깃 감정 레이블의 원핫 인코딩을 사용하여 컨포머 기반 AED 모델에 조건을 주는 실험을 수 행하고 오라클 환경을 나타내는 것으로 확인했다. 표 1과 표 2는 각각 WER(단어 오류율)과 감정 예측 정확도를 보여준다. 표의 각 줄에서 첫 번째와 두 번째 줄에 표시된 것처럼, 오라클 환경의 모든 성능이 기준선보다 우수 했다. 따라서 비사전 훈련된 GSTs 모듈에서 추출된 스타일 임베딩을 컨포머 기반 AED 모델에 조건을 주었을 때 음성 및 감정 공동 인식 작업의 성능 변화를 비교했다. 비사전 훈련된 GSTs 모듈에서 추출된 스타일 임베딩을 컨포머 기반 AED 모델과 조건화하는 방법은 성능을 개선시켰지만, 성능이 크게 향상되지는 않았다. 그러나 사전 훈련된 GSTs 모듈에서 추출된 스타일 임베딩을 컨포머 기반 AED 모델과 조건화하는 방법은 기준 성능에 비해 WW(weighted WER)와 UW(unweight WER)에서 각각 4.6%와 4.2%의 성능 향상을 보였다. 마찬가지로, WA(weighted accuracy)와 UA(unweighted accuracy)는 각각 3.2%와 3.2% 상대적으로 개선되었다. 사전 훈련된 GSTs 모듈에서 추출된 스타일 임베딩은 감정 예측을 유용하게 하고 감정적인 발화의 전사를 개선하는 데 도움이 된다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "또한 감정 토큰을 제외한 상태에서 본 발명이 제안하는 모델의 출력에서 계산한 WER을 조사하고 표 3에 나타냈 다. 본 발명이 제안하는 모델은 IEMOCAP 데이터셋에서 감정 토큰 없이 WER을 계산할 때 15.8%의 결과를 보여주 었다. 도 3은 기준선과 제안된 실험환경에서의 감정 예측 성능에 대한 혼동 행렬을 보여줍니다. 제안된 음성 인 식 모델은 특히 happy 감정 클래스에 대해 모든 네 가지 감정 클래스에 대한 예측 성능이 향상되었다.음성 및 감정 인식 장치에서 다른 인코더 레이어를 조건화하는 효과를 조사했다. 구체적으로, 각 인코더 레이어 를 조건화할 때 WER의 추이를 분석했다. 도 4는 WER이 상위 레이어보다 하위 레이어를 조건화할 때 점진적으로 개선되는 것을 보여준다. 특히, 모든 레이어를 조건화했을 때 최상의 성능을 얻었다. 본 발명의 결과는 표현의 여러 레이어에 조건을 주는 것이 음성 인식의 정확도를 효과적으로 개선시킬 수 있다는 것을 시사한다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "표 4에서 감정 토큰의 위치에 따른 ASR과 SER의 성능 변화를 비교했다. 실험 결과, 감정 토큰이 텍스트 전사 이 후에 위치할 때 ASR과 SER 모두 더 좋은 성능을 보였다. 감정 토큰을 텍스트 전사 앞에 두면 예측된 감정이 부 정확할 경우 잘못된 전사를 가져올 수 있다. 그러나 감정 토큰을 텍스트 전사의 끝에 배치하면 감정이 감정 예 측에 사용되기 때문에 더 나은 결과를 얻을 수 있다. 표 5는 IEMOCAP 데이터셋에서 이전 연구와 본 발명의 ASR과 SER의 비교를 보여준다. 본 발명은 GSTs 모듈에서 추출된 스타일 임베딩을 이용하여 AED 모델을 컨디셔닝하는 방법을 제안하였다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "본 발명의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 장치는 발화자의 음성 데이터를 수신하는 수신 부, 음성 데이터를 입력데이터로 하여 스타일 임베딩을 추출하는 GST 모듈, 스타일 임베딩을 이용하여 컨디셔닝 하며, 음성 데이터를 입력데이터로 하여 STT(Speech to text) 음성인식 방법에 따라 문자 토큰을 생성하고, 음 성 데이터 및 문자 토큰에 기반하여 발화자의 감정 토큰을 출력하는 AED모델을 포함할 수 있다. 본 발명의 일 실시예에 따른 AED 모델은 스타일 임베딩으로 컨디셔닝될 수 있으며, 문자 토큰과 상기 감정 토큰 을 직렬로 출력할 수 있다. 이하, 본 발명의 또 다른 실시예인 발화자의 음성 및 감정을 인식하는 방법에 대해 설명한다. 도 5를 참고하면, 본 발명의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 방법은 음성 데이터 수신단계 (S1000), 스타일 임베딩 추출단계(S2000), 음성 인식 모델 컨디셔닝 단계(S3000), 음성 데이터에 대응하는 문자 토큰 및 감정 토큰 출력단계(S4000)를 포함할 수 있다. 음성 데이터 수신단계(S1000)는 장치가 외부로부터 음파(waveform) 형태의 음성 데이터를 획득하는 단계를 의미 한다. 음성 데이터 수신단계는 장치의 수신부를 통해 외부로부터 음성 데이터를 수신하거나, 마이크를 이용하여 음성 데이터를 직접 획득하는 단계를 의미할 수 있다. 즉, 장치가 음파(waveform) 형태의 음성 데이터를 수신하 는 방법이면 무엇이든지 가능한 바, 이에 대한 자세한 설명은 생략한다. 스타일 임베딩 추출단계(S2000)는 수신한 음성 데이터를 GST 모듈에 적용하여 스타일 임베딩을 추출하는 단계를 의미한다. 도 2에 도시된 바와 같이, 학습을 통해 모듈 내의 가중치를 조절함으로써 적합한 스타일 임베딩을 추 출하도록 GST 모듈을 학습시킬 수 있다. 음성 인식 모델 컨디셔닝 단계(S3000)는 GST 모듈에서 추출한 스타일 임베딩을 이용하여 음성 인식 모델을 컨디 셔닝하는 단계를 의미한다. 보다 구체적으로, 음성 인식 모델은 컨포머 기반 AED 모델일 수 있다. 본 발명의 AED 모델은 하기 수식에 따른 전체 손실 함수로 학습될 수 있으며, 전체 손실 함수는 AED와 CTC 손실 함수의 가중 합으로 설계되었다. 전체 손실 함수에 대한 설명은 이미 설명한 바, 이에 대한 자세한 설명은 생략하도록 한다."}
{"patent_id": "10-2023-0102990", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "음성 인식 모델은 GST 모듈에서 추출한 스타일 임베딩을 이용하여 컨디셔닝할 수 있으며, 컨디셔닝 단계는 하기 수식에 따라 진행될 수 있다. 컨디셔닝 단계는 모든 컨포머 인코더 레이어의 반스텝 FFN 출력에 이를 추가하기 위해 하나의 feed-forward network(FFN)를 사용하며, 이에 대한 설명은 이미 설명한 바, 이에 대한 자세한 설명 은 생략하도록 한다. 음성 데이터에 대응하는 문자 토큰 및 감정 토큰 출력단계(S4000)는 학습 및 컨디셔닝 된 음성 인식 모델이 음 성 데이터를 입력하여 음성 데이터에 대응하는 문자 토큰 및 감정 토큰을 출력하는 단계를 의미한다. 보다 구체적으로, 음성 인식 모델은 입력된 음성 데이터를 인식 및 STT 변환하여 문자 토큰을 생성할 수 있다. 필요에 따라, 음성 인식 모델은 음성 데이터에 기반하여 발화자의 감정 상태를 분류하거나, 음성 데이터와 문자 토큰에 기반하여 발화자의 감정 상태를 분류할 수 있다. 감정 상태를 분류하는 방법은 이진분류 또는 다중분류 일 수 있다. 즉 감정상태는 기쁨, 슬픔, 화남, 놀람, 흥분, 실망, 중립 등 다양한 상태를 가질 수 있으며, 본 기재에 제한되지 않는다. 본 발명의 구체적 일 실시예에 따르면, 감정 상태를 4가지(happy, sad, angry, neutral)로 분류할 수 있다. 음성 인식 모델은 분류된 감정 상태를 이용하여 감정 토큰을 생성하고 이를 출력할 수 있다. 필요에 따라, 음성 인식 모델은 문자 토큰과 감정 토큰을 동시에 출력할 수 있으며, 문자 토큰과 감정 토큰을 직렬로 출력할 수 있다. 본 발명의 또 다른 실시예는 본 발명의 발화자의 음성 및 감정을 인식하는 방법을 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록 매체일 수 있다. 이상에서 실시 형태들에 설명된 특징, 구조, 효과 등은 본 발명의 적어도 하나의 실시 형태에 포함되며, 반드시 하나의 실시 형태에만 한정되는 것은 아니다. 나아가, 각 실시 형태에서 예시된 특징, 구조, 효과 등은 실시 형 태들이 속하는 분야의 통상의 지식을 가지는 자에 의해 다른 실시 형태들에 대해서도 조합 또는 변형되어 실시가능하다. 따라서 이러한 조합과 변형에 관계된 내용들은 본 발명의 범위에 포함되는 것으로 해석되어야 할 것 이다. 또한, 이상에서 실시 형태를 중심으로 설명하였으나 이는 단지 예시일 뿐 본 발명을 한정하는 것이 아니 며, 본 발명이 속하는 분야의 통상의 지식을 가진 자라면 본 실시 형태의 본질적인 특성을 벗어나지 않는 범위 에서 이상에 예시되지 않은 여러 가지의 변형과 응용이 가능함을 알 수 있을 것이다. 즉, 실시 형태에 구체적으 로 나타난 각 구성 요소는 변형하여 실시할 수 있는 것이다. 그리고 이러한 변형과 응용에 관계된 차이점들은 첨부된 청구 범위에서 규정하는 본 발명의 범위에 포함되는 것으로 해석되어야 할 것이다."}
{"patent_id": "10-2023-0102990", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 AI 장치의 블록도이다. 도 2는 본 발명의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 장치의 구성도이다. 도 3 및 도 4는 실험적 설정과 본 발명이 ASR 시스템의 정확성과 견고성을 향상시키는 효과를 확인하기 위해 얻 은 결과를 나타낸다. 도 5는 본 발명의 일 실시예에 따른 발화자의 음성 및 감정을 인식하는 방법의 순서도이다."}
