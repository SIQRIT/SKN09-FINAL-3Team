{"patent_id": "10-2023-0110677", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0029578", "출원번호": "10-2023-0110677", "발명의 명칭": "분할연산 기반 응용 서비스를 지원하기 위한 방법 및 장치", "출원인": "한국전자통신연구원", "발명자": "나태흠"}}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "분할연산을 지원하는 통신 시스템에서, 제1 네트워크 엔티티에 의해 수행되는 방법으로서, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청받는 과정; 상기 제1 및 제2 네트워크 엔티티와 구별되는 제3 네트워크 엔티티로부터 네트워크 분석정보를 수신하는 과정;및상기 네트워크 분석정보를 기초로, 상기 분할연산 기반 인공지능 응용에 대한 지원 서비스를 제공하는 과정을포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 분할연산 기반 인공지능 응용은,헤드모델 및 테일모델을 포함하는 병목-삽입된 모델(Bottleneck-injected model)을 이용하여 추론을 수행하는응용인, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 분할연산 기반 인공지능 응용에 대한 지원 서비스는,상기 헤드모델의 구조 설계 보조, 상기 헤드모델의 선택 보조, 상기 헤드모델의 선택, 상기 테일모델의 엔드포인트 선택 보조, 상기 테일모델의 엔드포인트 선택 및 사용자 단말과 상기 엔드포인트 간 라우팅 중 하나 이상을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서,상기 요청받는 과정은, 상기 제2 네트워크 엔티티로부터 상기 헤드모델의 구조 설계 보조를 요청받는 과정을 포함하고,상기 네트워크 분석정보는, 상기 헤드모델에 대한 연산을 수행할 사용자 단말이 상기 테일모델에 대한 연산을수행할 엔드포인트에 도달하기 위해 경유해야하는 네트워크 구간들의 전부 또는 일부, 상기 네트워크 구간들 중병목지점, 계약에 의한 QoS, 상기 사용자 단말과 상기 엔드포인트 간 보장받을 수 있는 QoS, 및 네트워크 성능의 지속 가능성 중 하나 이상에 관한 분석결과를 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제4항에 있어서,상기 제공하는 과정은, 상기 네트워크 분석정보를 기초로, 상기 헤드모델의 구조 설계에 필요한 정보를 가공하는 과정; 및상기 가공된 정보를 상기 제2 네트워크 엔티티에게 전송하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제2항에 있어서,공개특허 10-2025-0029578-3-상기 요청받는 과정은, 상기 제2 네트워크 엔티티로부터 상기 헤드모델의 선택 보조 또는 상기 헤드모델의 선택을 요청받는 과정을 포함하고,상기 네트워크 분석정보는, 상기 헤드모델에 대한 연산을 수행할 사용자 단말에 대한 관측된 서비스 경험(observed service experience, OSE), 상기 사용자 단말과 기지국 사이의 무선구간 성능 및 상기 사용자 단말에 대한 네트워크 성능의 지속 가능성 중 하나 이상에 대한 분석결과를 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제공하는 과정은, 상기 네트워크 분석정보를 기초로, 상기 헤드모델의 선택에 필요한 정보를 가공하는 과정; 및상기 가공된 정보를 상기 제2 네트워크 엔티티에게 전송하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 제공하는 과정은, 상기 네트워크 분석정보를 기초로, 서로 다른 구조를 갖는 복수개의 기훈련된 헤드모델들 중에서 상기 사용자 단말이 연산을 수행할 헤드모델을 선택하는 과정; 및선택된 상기 헤드모델에 관한 정보를 상기 제2 네트워크 엔티티에게 전송하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 제공하는 과정 이전에,상기 제2 네트워크 엔티티로부터 상기 사용자 단말의 연산성능 및 상태 정보 중 하나 이상을 수집하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제2항에 있어서,상기 요청받는 과정은, 상기 제2 네트워크 엔티티로부터 상기 테일모델에 대한 연산을 수행할 엔드포인트의 선택 보조 또는 상기 엔드포인트의 선택을 요청받는 과정을 포함하고,상기 네트워크 분석정보는, 상기 테일모델이 배포된 하나 이상의 후보 엔드포인트들과 상기 헤드모델에 대한 연산을 수행할 사용자 단말 사이의 네트워크 구간들의 전부 또는 일부에 대한 분석 결과를 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서,상기 요청받는 과정은, 상기 사용자 단말에 의한 추론이 개시되기 이전, 또는 상기 사용자 단말에 의한 추론이 수행되는 도중에 미리설정된 시간 간격으로 상기 제2 네트워크 엔티티로부터, 상기 선택 보조 또는 상기 선택을 요청받는 것인,방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서,상기 제1 및 제3 네트워크 엔티티와 구별되는 제4 네트워크 엔티티로부터, 상기 하나 이상의 후보 엔드포인트들에 대한 배포 정보를 수신하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "공개특허 10-2025-0029578-4-제10항에 있어서,상기 제공하는 과정은, 상기 네트워크 분석정보를 기초로 상기 엔드포인트의 선택에 필요한 정보를 가공하는 과정; 및상기 가공된 정보를 상기 제2 네트워크 엔티티에게 전송하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제10항에 있어서,상기 제공하는 과정은,상기 네트워크 분석정보를 기초로, 상기 하나 이상의 후보 엔드포인트들 중에서 상기 사용자 단말과 분할 추론을 수행할 엔드포인트를 선택하는 과정;상기 제2 및 제3 네트워크 엔티티와 구별되는 제4 네트워크 엔티티에게 상과 사용자 단말과 상기 선택된 엔드포인트 사이의 서비스 트래픽 라우팅을 요청하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제10항에 있어서,상기 제공하는 과정은,상기 선택된 엔드포인트 및 상기 사용자 단말과 상기 엔드포인트 간 적용된 QoS 중 하나 이상에 대한 정보를 포함하는, 응답 메시지를 상기 제2 네트워크 엔티티로 전송하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제1항에 있어서,상기 제공하는 과정은, 상기 제3 네트워크 엔티티로부터 수신한 분석정보를 상기 제2 네트워크 엔티티에게 전달하는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제1항에 있어서,상기 수신하는 과정은,상기 제3 네트워크 엔티티가 제공하는 인터페이스를 호출하여, 네트워크 분석 서비스를 구독하는 과정; 및상기 제3 네트워크로부터 구독한 네트워크 분석 서비스에 대한 분석결과를 보고받는 과정을 포함하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제1항에 있어서,상기 제2 네트워크 엔티티는, AF(Application Function)이고,상기 제3 네트워크 엔티티는, NWDAF(network data analytics function)인, 방법."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "통신 시스템에서, 제1 네트워크 엔티티에 있어서,통신 인터페이스; 및 상기 통신 인터페이스를 통해, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반인공지능 응용에 대한 지원 서비스를 요청받고, 상기 제1 네트워크 엔티티 및 상기 제2 네트워크 엔티티와 구별공개특허 10-2025-0029578-5-되는 제3 네트워크 엔티티로부터 네트워크 분석정보를 수신하고, 상기 네트워크 분석정보를 기초로 상기 분할연산 기반 인공지능 응용에 대한 지원 서비스를 제공하는 프로세서를 포함하는, 제1 네트워크 엔티티."}
{"patent_id": "10-2023-0110677", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "통신 시스템에서 분할연산을 분할연산 기반 인공지능 응용 서비스를 제공하기 위한 방법으로서, 제1 네트워크 엔티티가, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티에게, 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청하는 과정; 및 상기 제1 및 제2 네트워크 엔티티와 구별되는 제3 네트워크 엔티티에 의해 분석된 네트워크 분석정보를 기초로,상기 제1 네트워크 엔티티가 상기 제2 네트워크 엔티티로부터 상기 분할연산 기반 인공지능 응용에 대한 지원서비스를 제공받는 과정을 포함하는 것을 특징으로 하는, 방법."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "분할연산 기반 응용 서비스를 지원하기 위한 방법 및 장치를 개시한다. 본 개시의 일 측면에 의하면, 분할연산을 지원하는 통신 시스템에서, 제1 네트워크 엔티티에 의해 수행되는 방법 으로서, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청받는 과정; 상기 제1 네트워크 엔티티 및 상기 제2 네트워크 엔티티와 구별되는 제3 네트워크 엔티티로부터 네트워크 분석정보를 수신하는 과정; 및 상기 네트워크 분석정보를 기초로, 상기 분할연산 기반 인 공지능 응용에 대한 지원 서비스를 제공하는 과정을 포함하는 것을 특징으로 하는, 방법을 제공한다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 분할연산 기반 응용 서비스를 지원하기 위한 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. 인공지능의 확산과 함께 스마트폰 및 자율주행 차량과 같은 모바일 장치에서 이미지 분류, 음성인식과 같이 복 잡한 추론 작업을 실행하기 위한 기술도 동시에 연구되고 있다. 에지 컴퓨팅(Multi-access Edge Computing, MEC)은 중앙 클라우드에 비해 네트워크 관점에서 가까운 위치에 설치된 소규모 클라우드를 활용하는 방법으로, 클라우드에 비해 낮은 지연시간에 단말에게 연산서비스를 제공할 수 있는 장점이 있다. 최근에는 분할연산(split computing)의 개념이 제안되어 지속적으로 연구되고 있다. 분할연산에서는, 학습된 심 층신경망을 레이어 단위로 분리하고, 추론 시 연산의 일부 혹은 전부를 MEC 환경으로 오프로드(off-load) 하여, 단말의 배터리 사용을 줄임과 동시에 높은 복잡도의 연산 프로세스를 빠르게 처리할 수 있다. 이때, 오프로딩 정도(즉, 총 연산량 중 모바일 연산 또는 에지 연산의 비율)를 최적화하기 위해서는, 모바일 장치의 연산능력 및 배터리상태, 모바일 장치와 에지 서버 간의 자원상황(예: 네트워크, CPU, 스토리지 등)등이 주요 파라미터로 고려되어야 한다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 인공지능 응용서비스인 분할연산 및 추론을 지원하기 방법 및 장치를 제공하는 데 일 목적이 있다. 본 개시는, 네트워크 분석 정보를 활용하여 분할된 모델을 설계(또는 선택)할 수 있는 방법 및 장치를 제공하는 데 일 목적이 있다. 본 개시는, 네트워크 분석을 기반으로 다수의 노드에 배포된 분할추론 서비스의 엔드포인트 를 라우팅할 수 있는 방법 및 장치를 제공하는 데 일 목적이 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 분할연산을 지원하는 통신 시스템에서, 제1 네트워크 엔티티에 의해 수행되는 방 법으로서, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반 인공지능 응용에 대 한 지원 서비스를 요청받는 과정; 상기 제1 네트워크 엔티티 및 상기 제2 네트워크 엔티티와 구별되는 제3 네트 워크 엔티티로부터 네트워크 분석정보를 수신하는 과정; 및 상기 네트워크 분석정보를 기초로, 상기 분할연산 기반 인공지능 응용에 대한 지원 서비스를 제공하는 과정을 포함하는 것을 특징으로 하는, 방법을 제공한다. 본 개시의 다른 측면에 의하면, 통신 시스템에서, 제1 네트워크 엔티티에 있어서, 통신 인터페이스; 및 상기 통 신 인터페이스를 통해, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청받고, 상기 제1 네트워크 엔티티 및 상기 제2 네트워크 엔티티와 구별되는 제3 네트워크 엔티티로부터 네트워크 분석정보를 수신하고, 상기 네트워크 분석정보를 기초로 상기 분할연산 기 반 인공지능 응용에 대한 지원 서비스를 제공하는 프로세서를 포함하는, 제1 네트워크 엔티티를 제공한다. 본 개시의 또 다른 측면에 의하면, 통신 시스템에서 분할연산을 분할연산 기반 인공지능 응용 서비스를 제공하 기 위한 방법으로서, 제1 네트워크 엔티티가, 상기 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티에게, 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청하는 과정; 및 상기 제1 및 제2 네트워크 엔티티와 구 별되는 제3 네트워크 엔티티에 의해 분석된 네트워크 분석정보를 기초로, 상기 제1 네트워크 엔티티가 상기 제2 네트워크 엔티티로부터 상기 분할연산 기반 인공지능 응용에 대한 지원 서비스를 제공받는 과정을 포함하는 것 을 특징으로 하는, 방법을 제공한다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 의하면, 기존 5GS에서 정의되지 않은 인공지능 응용서비스를 지원하는 신규 네트워크 기능 을 활용함으로써, 인공지능 응용서비스, 특히 단일 모델이 사용자 단말과 에지(또는 클라우드)로 분할된 형태로 수행되는 추론 서비스의 추론 성능의 향상효과를 기대할 수 있다. 기존 기술들에서는 네트워크의 자원상태를 인지하기 위해 응용서비스 레벨에서 종단간(End-to-End) 성능을 측정 하거나, 현재 네트워크 품질 수준을 바탕으로 추론을 수행하는 등 추상화된 형태로 수집된 정보를 바탕으로 분 할 지점을 결정하고 서비스를 수행하였다. 반면, 본 개시의 실시예에 의하면, 모바일 네트워크에서 정밀하게 측 정된 분석정보를 바탕으로, 분할모델 설계, 분할모델 선택 및/또는 엔드포인트 라우팅을 수행할 수 있다. 본 개시의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 도 1은 본 개시의 일 실시예에 따른 분할연산의 기본적인 구조를 보여주는 개념도이다. 도 1에서 볼 수 있듯이, 분할연산의 기본적인 구조는 원본 모델을 분할하여, 입력층에 가까운 헤드모델은 사용 자 단말(User Equipment, UE)에서 연산을 수행하고, 나머지 테일모델은 네트워크를 통해 연결된 서버에서 연산 을 수행하는 것을 의미한다. 이때, 서버는 요구되는 시간 내에 연산을 수행하여야 하며, 네트워크 자원 상황과 UE의 자원상황(예컨대, 컴퓨팅, 배터리 등)을 동시에 고려하여 요구되는 데드라인을 충족하는 범위 내에서 자원 사용률을 최소화할 수 있어야 한다. 이는 사용하는 모델의 구조와도 밀접한 연관이 있다. 분할의 최소단위는 레이어이며, 모델의 레이어 구성 및 레 이어의 특성에 따라 연산량과 출력 데이터 사이즈가 달라진다. 따라서 분할지점(spilt point)의 출력 데이터의 사이즈가 곧 UE와 서버 간 전달해야 할 데이터의 양이 된다. 도 2는 AlexNet 모델의 레이어별 출력 데이터의 연산 레이턴시(layer latency)와 출력 데이터의 사이즈를 보여 주는 예시도이다. 도 3은 VGG-16 모델의 레이어별 출력 데이터의 연산 레이턴시와 출력 데이터의 사이즈를 보여 주는 예시도이다. 도 2 및 도 3에 도시되듯이, 레이어별로 출력 데이터 크기의 편차가 상당함을 알 수 있다. 따라서, 미리 적절한 (예컨대, 출력 데이터 사이즈가 비교적 작은) 후보 분할지점들(candidate split points)을 선정한 후, 실시간 자원 상황에 따라 후보 분할지점들 중에서 최적의 분할지점을 선택할 수 있다. 표 1 및 표 2는 각각 AlexNet 모델과 VGG-16 모델에서, 후보 분할지점별 출력 데이터 크기와 업링크 대역폭 요 구량을 예시한다. 표 1 및 표 2은, 입력 이미지의 해상도가 227×227 픽셀이고, 초당 30프레임의 이미지가 처리 되어야 한다고 가정할 때의 요구되는 업링크 대역폭을 보여준다. 표 1 Split point Approximate output data size (MByte)Required UL data rate (Mbps) Candidate split point 0 (Cloud-based inference)0.15 36 Candidate split point 1 (after pool1 layer) 0.27 65 Candidate split point 2 (after pool2 layer)0.17 41 Candidate split point 3 (after pool5 layer)0.02 4.8 Candidate split point 4 (Device-based inference) N/A N/A 표 2 Split point Approximate output data size (MByte)Required UL data rate (Mbps) Candidate split point 0 (Cloud-based inference)0.6 145Candidate split point 1 (after pool1 layer) 3 720 Candidate split point 2 (after pool2 layer)1.5 360 Candidate split point 3 (after pool3 layer)0.8 192 Candidate split point 4 (after pool4 layer) 0.5 120 Candidate split point 5 (after pool5 layer) 0.1 24 Candidate split point 6 (Device-based inference)N/A N/A 표 1을 참조하면, AlexNet 모델에서는 분할지점에 따라 4.8MBps 내지 65Mbps의 대역폭이 요구됨을 알 수 있다. 반면, VGG-16 모델은 AlexNet 모델 대비 레이어의 수가 2배로 증가된 모델로, 표 2에 나타나듯이, 모델의 규모 가 커짐에 따라 요구되는 업링크 대역폭도 24Mbps 내지 720Mbps로 증가된 것을 확인할 수 있다. 그러나 위의 예시들은 점점 거대규모의 모델로 진화되고 있는 최신 인공지능 동향을 반영하지 않고 있다. 최근 에는 컴퓨터 비전(Computer Vision)분야에서 연구되는 BASIC-L 모델의 규모는, VGG-16 모델의 파라미터 수 (1.38억 개) 대비 17배 이상(24.4억개)으로 증가되었다. 표 3은 컴퓨터 비전분야에서 주로 사용되는 데이터셋의 종류 및 그 특징을 예시한다. 표 3 Year Dataset Description ResolutionSize (GB) Usage 1999 MNIST Modified National Institute of Standards and Technology dataset28×28 pixels~ 0.05Image classification 2005Pascal VOCPascal Visual Object Classes dataset Various resolutions up to 500×500 pixels~ 2Object recognition and detection 2009 CIFARCanadian Institute for Advance Research dataset32×32 pixels~ 0.17Image classification 2009ImageNetLarge-scale dataset of images organized into hierarchical categories256×256 or 224×224 pixels~ 155Object recognition and clasification 2014 COCO Common Objects in Context datasetVarious resolutions up to 1024×1024 pixels~ 25Object detection, segmentation, and captioning 2016Open ImagesLarge-scale dataset of images with annotate labelsVarious resolutions up to 1024×1024 pixels~ 500Object recognition, detection, and segmentation 표 3에 나타나듯이, 최신 인공지능 동향에 따르면, 해결하고자 하는 데이터셋의 크기 및 개별 데이터 인스턴스 의 해상도(coco dataset; up to 1024x1024)도 증가하고 있는 것을 알 수 있다. 이러한 추세에 따른 요구사항의 변화를 적절히 지원하기 위해 분할추론의 개선도 요구된다. 도 4는 본 개시의 일 실시예에 따른 인공 병목(artificial bottleneck) 개념이 적용된 분할연산을 보여주는 개 념도이다. 모델의 크기증가에 따른 네트워크 대역폭 사용량 완화를 위해 모델의 구조적 변화가 도입될 수 있다. 도 2에서 예시한 AlexNet과 같은 모델의 경우 비교적 규모가 작고 모델 구조적으로 중간 레이어 결과물의 크기가 작은 지점인 자연적인 병목 지점(natural bottleneck point)를 포함하고 있었다. 그러나 최근 활용되는 모델의 경우 자 연적인 병목 지점을 포함하지 않는 경우가 대부분이고, 포함하더라도 출력 레이어에 인접하게 위치하여 분할의 의미가 없는 경우가 대부분이다. 이를 극복하기 위해, 도 4에 도시된 것과 같이, 오리지널 모델의 중간에 인코더(encoder) 및 디코더(decoder) 레이어를 삽입하여 병목 지점을 추가하는 인공 병목의 개념이 적용될 수 있다. 이 경우, 모델 구조의 변경으로 인해 재학습이 요구되는 대신, 학습된 이후에는 비교적 입력층과 가까운 초기 레이어에서 모델을 분할할 수 있 다는 장점이 있다. 최근 3GPP SA2에서는 인공지능 기반 서비스를 지원하기 위한 시스템(AIMLsys)에 대한 페이즈 1(phase 1) 작업이 2020년 8월부터 2021년 12월까지 진행됐으며, 현재 이를 바탕으로 규격화(normative work) 절차가 진행중이다. 페이즈 1에서는 관련하여, 분할된 모델이 포함된 UE 및 서버를 엔드포인트(endpoint)라 정의하고, 엔드포인트 간 동작 및 절차에 대한 내용이 논의되었으며, 유즈 케이스로는 객체 인식, 비디오 스트리밍 화질 개선, 로보틱 스 제어, 분할 추론에서의 모델 전송 등이 논의되었다. 그러나 상기한 모델 규모 및 데이터 크기 증가와 인공 병목의 도입으로 인해 기존까지 3GPP SA AI/MLsys에서 논의된 모델 분할 및 상황에 따른 분할지점 최적화의 경 우 그 전제가 변경되었다. 다수의 후보 분할지점을 보유하고 있던 이전 예제(최적화 변수로서의 분할지점)와는 다르게, 분할지점은 인공 병목 단일 지점으로 고정되었다. 또한, 고정된 구조를 기반으로 모델을 학습시키므로, 학습된 모델을 기반으로 추론 시 분할지점을 변경할 수 없다. 따라서 고정된 분할지점 기반 자원최적화로 재정 의가 필요하다. 이에 따른 네트워크 관점에서의 응용지원 요구사항 및 기능블록, 인터페이스들도 새로이 논의될 필요가 있다. 표 4 및 표 5는 TR22.876(AI/MLsys phase2)에서 제시된 모델별 중간 연산 결과의 데이터사이즈 및 요구대역폭의 범위를 보여준다. 표 4 Model Name Model type Intermediate data size (MB) 8 bits data format 32 bits data format Min MAx Min MAx AlexNet Image recognition 0.02 0.06 0.08 0.27 ResNet50 Image recognition 0.002 1.6 0.008 0.27 SoundNet Sound recognition0.0017 0.22 0.0068 6.4 PointNet Point Cloud 0.262 1.04 0.0068 4.19 VGGFace Face recognition0.000016 0.8 0.000064 3.2 Inception resnetFace recognition 0.0017 0.37 0.0068 1.51 표 5 Model Name Offloading targetIntermediate data size (MB)Transfer time (ms)Data rate (Gb/s) AlexNet Proximity robot or Service Hosting Environment[0.000016 - 1.6] (8 bits data format)10 [0.128 - 1.28] ResNet50 SoundNet PointNet [0.000064 - 6.4] (32 bits data format)10 [0.512 - 5.12] VGGFace Inception resnet 표 5를 참조하면, 예시적인 일부 모델들의 요구대역폭의 범위가 500Mbps 내지 5.12Gbps인 것을 알 수 있다. 그 러나 현재 표준 규격에서는 해당 범위의 대역을 제공해줄 수 있는 QFI(QoS flow ID) 모델이 존재하지 않는다. 5G 코어에서는 MFBR(Maximum Flow bitrate), non GBR(Guaranteed Bit Rate)인 경우 AMBR(Aggregated MBR) 형 태로 PDU 세션의 QoS가 관리되는데, 해당 UPF가 PDU 세션에 할당한 GFBR(Guaranteed Flow Bitrate), MFBR의 범 위가 5.12Gbps 이상이면서 매우 높은 우선순위(priority)를 제공해야 한다. 따라서 고대역이면서 넓은 범위의 대역폭의 최댓값 기준으로 정적으로(static) GFBR 타입의 QoS를 제공하는 것은 현재 5G 코어의 QoS 모델에서는 지원되기 어렵다.도 5는 본 개시의 일 실시 예에 따른 통신 시스템의 아키텍처를 예시한 도면이다. 이하 편의를 위하여, 본 개시 는 5세대 이동통신 시스템(5GS: 5G System) 규격에서 정의하는 용어와 명칭들을 사용하지만 상기 용어 및 명칭 들에 의해 한정되는 것은 아니며, 다른 규격에 따르는 시스템에도 동일하게 적용될 수 있다. 통신 시스템은 사용자 UE, (무선) 액세스 네트워크((Radio) Access Network), 코어 네트워크, 데이 터 네트워크(data network, DN)(560 및 580) 등으로 구성될 수 있다. 코어 네트워크는 액세스 네트워크에 의해 상호접속되는 고객들에게 많은 통신 서비스들을 제공할 수 있다. 코어 네트워크는 네트워크 기능(Network Function)을 수행하는 다수의 엔티티들을 포함할 수 있다. 본 개시에서 '네트워크 엔티티' 또는 '네트워크 기능'이라는 용어는 코어 네트워크의 하나 이상의 기능을 수행하는 임의의 엔티티를 지칭할 수 있다. 네트워크 엔티티들은, 무선 및/또는 네트워크 통신들을 위해 구성된 장치 또는 컴퓨 터 시스템의 메모리에 저장되고 그의 프로세서 상에서 실행되는 컴퓨터 실행가능 명령어들(소프트웨어)의 형태 로 구현되는 논리적 엔티티들일 수 있다. 코어 네트워크는 다양한 네트워크 엔티티들을 포함할 수 있다. 일 예로, 코어 네트워크는 액세스 및 이동성 관 리 기능(access and mobility management function, AMF), 세션 관리 기능(session management function, SMF), 정책 제어 기능(policy control function, PCF), 사용자 평면 기능(user plane function, UPF)(544a 내지 544c), 어플리케이션 기능(application function, AF), 통합된 데이터 관리 (unified data management, UDM), 네트워크 노출 기능(network exposure function, NEF), 네트워크 저장 기능(Network Repository Function, NRF) 및 네트워크 데이터 분석 기능(network data analytics function, NWDAF)을 포함할 수 있다. 도 5에 도시된 것과 같이 코어 네트워크는 서로 다른 복수개의 UPF들 (544a 내지 544c)을 포함할 수 있다. AN과 직접 연결되는 UPF(554a)는 업링크 분류기(uplink classifier, UL-CL) 및/또는 분기점(branching point, BP)으로 동작할 수 있다. UPF(554a)는 UPF(554b) 및 UPF(554c)와 각 각 연결될 수 있다. UPF(554b)는 중앙 데이터 네트워크(central DN)와 연결되는, 중앙 PDU 세션 앵커 (Central PDU Session Anchor, C-PSA)로 동작할 수 있다. 또한 UPF(554c)는 DN의 로컬 부분(local part of DN)과 연결되는 로컬 PSA(local PSA, L-PSA)로 동작할 수 있다. 각 네트워크 엔티티에 의해 지원되는 기능 및 네트워크 엔티티들 사이의 인터페이스들은 해당 분야에서 알려진 사항이므로, 본 개시에서는 이에 대한 자세 한 설명은 생략한다. 도 1 내지 도 4에서 전술한 문제들을 해결하기 위해 본 개시의 일 실시예에 따른 코어 네트워크는, 신규 기능으 로 AI 응용 보조 기능(AI Application Assist Function, AAAF)을 더 포함할 수 있다. AAAF는 NEF와는 다르게 AI 응용 서비스를 위한 특화된 기능을 제공할 수 있다. 따라서 AAAF는 AI 응용 서비 스의 특성 및 일반 응용서비스와는 구별되는 요구사항을 충족시켜줄 수 있다. 본 개시에서는 AAAF의 분할 연산 서비스와 관련된 기능들을 제안한다. 예컨대, AI 응용 관점에서 AAAF는 모델 학습 및 설계, 추론 시 UE에서 활용할 모델 배포 및/또는 추론 시 테일모델이 위치한 최적 엔드포인트 선택 및 라우팅 등의 기능 을 제공할 수 있다. 표 6은 AAAF가 제공할 수 있는 서비스 인터페이스를 예시한다. 표 6 Service Name Description Service OperationOperation semanticExample consumer Naaaf_Endpoint Selection다수의 Endpoint 리스트를 기반으로 가장 적 절한 Endpoint를 선정해주는 서비스Create UpdateRequest/ responseAF Naaaf_Endpoint Routing다수의 Endpoint 리스트를 기반으로 가장 적 절한 Endpoint를 선정하고, 대상 UE에서 구 동 중인 Application 트래픽을 라우팅해주는 서비스CreateRequest/ responseAF Subscribe/ NotifySubscribe/ NotifyAF UpdateRequest/ responseAF Naaaf_ModelSel ectionUE에서 사용 가능한 다수의 모델이 있는 경 우 네트워크 상태를 기반으로 가장 적절한 모델을 선정해주는 서비스Create UpdateRequest/ responseAF 표 6을 참조하면, AAAF의 주요 소비자(consumer)는 인공지능 서비스를 제공하는 AF이다. 한편, 본 개 시에서는 인공지능 서비스로서 분할연산기반 인공지능 서비스를 가정한다. 하지만, 다른 예에서, 다양한 인공지능 시나리오(예컨대, 연합학습, 전이학습, 모사학습, 경량화, 자율주행 시나리오 등)를 위한 기능이 추가 확장 될 수 있음에 유의하여야 한다. 도 6은 본 개시의 일 실시예에 따른 헤드모델의 다양한 예를 보여주는 예시도이다. AI 응용 모델 제공자는 원본 모델에 병목을 삽입(bottleneck injection)하기 위해 분할지점(즉, 원본 모델 에서 병목을 추가할 레이어)을 선정하고, 입력 레이어부터 분할지점까지의 영역에서 헤드모델을 설계 하며, 분할지점부터 출력 레이어까지의 영역에서 테일모델을 설계한다. 헤드모델의 연산은 UE에 서 수행되며, 테일모델의 연산은 에지 서버(edge application server, EAS) 또는 클라우드(예컨대, 도 5의 중앙 DN과 연결된 AF)에서 수행될 수 있다. 일반적으로 헤드모델은 UE의 한정된 자 원(예컨대, 컴퓨팅, 배터리)으로 인해, 그 헤드모델 크기 및 헤드모델의 연산결과(즉, 테일모델로 전달되어야 할 중간 결과물)의 크기가 최소화될수록 연산지연 및 통신지연을 줄일 수 있다. 그러나 이는 추론 결과의 정확 도와 트레이드-오프 관계에 있으므로 모델 설계 시 최적화가 요구된다. 예를 들어, 도 6에 도시된 것과 같이 동 일한 분할지점에서 원본 모델을 분할하더라도, 헤드모델의 크기 및/또는 압축률에 따라 다양한 헤드모델들 (620 내지 624)이 고려될 수 있으며, 연산지연, 통신지연, 및 추론결과의 정확도를 고려하여 적절한 헤드모델 (620 내지 624)이 선정되어야 한다. UE의 연산성능은 하드웨어 규격에 따라 고정되지만 네트워크 성능의 불확실성으로 인해, 기존 기술들에서 는 중간 결과물의 크기를 과도하게 최소화하는 경향이 있었다. 본 개시에서는 코어 네트워크에서 제공되는 네트 워크 분석(NWDAF analytics IDs)을 기반으로 통신 QoS의 불확실성을 낮춤으로써, 헤드모델의 크기와 압축 률을 최적화할 수 있는 헤드모델 설계 절차를 제안한다. 도 7은 본 개시의 일 실시예에 따른 헤드모델의 설계 절차를 보여주는 흐름도이다. 도 8은 본 개시의 일 실시예 에 따른 헤드모델 및 테일모델의 배포 형태의 일 예를 보여주는 예시도이다. 응용 제공자는 원본 모델을 분리함과 동시에 헤드모델의 구조를 설계해야 한다. 이를 위해 AF는 코어 네트워크에 위치한 AAAF가 제공하는 Naaaf 인터페이스로 요구되는 정보를 요청할 수 있다. 구체적으 로, AF는 AAAF에게 Naaaf_EventExposure 메시지를 전송할 수 있다(S700). Naaaf_EventExposure는 필 터(filter)를 포함할 수 있으며, 필터는 가입 영구 식별자(Subscription Permanent Identifier, SUPI)으로 설 정될 수 있다. 과정 S700에서, AF는 테일모델의 엔드포인트에 도달하기 위해 경유해야 하는 네트워크 구간들의 정보 를 모두 요구할 수 있다. 예컨대, 도 8에 도시된 것과 같이, 테일모델이 중앙 클라우드(도 5의 AF)에 배포된 경우, AF는 N3, N9 및 N6 인터페이스에 관한 정보를 모두 요구할 수 있다. 부가적으로 또는 대안적 으로, AF는 네트워크 구간들 중 병목 지점, 계약에 의한 QoS 정보 외에 실제로 UE과 엔드포인트 간 보장받을 수 있는 QoS 정보, 및/또는 지속가능성 등을 요구할 수도 있다. 요청을 받은 AAAF는 NWDAF가 제공하는 Nnwdaf 서비스 기반 인터페이스(service based interface, SBI)를 통해 필요한 정보를 수집할 수 있다. 구체적으로, AAAF는 NWDAF에게 네트워크 데이터 분석 서 비스에 대한 구독요청 메시지(예를 들면, Nnwdaf_AnalyticsSubscription_Subscribe 메시지)를 전송할 수 있다 (S710 내지 S730). 과정 S710 내지 S730에서, Nnwdaf_AnalyticsSubscription_Subscribe 메시지는, 데이터 분석을 요청하는 분석 정보를 지정하는 식별자인 Analytics ID를 포함할 수 있다. 예를 들어, Analytics ID는 \"QoS sustainability\", \"WLAN performance\", \"OSE\" 및/또는 \"DN performance\"로 설정될 수 있다. 구독요청 메시지는, 필터를 더 포함 할 수 있다. 필터는 SUPI로 설정될 수 있다. NWDAF는 AAAF가 구독을 통해 요청한 분석 결과를 전달하는 분석 결과 보고 메시지(예를 들면, Nnwdaf_AnalyticsSubscription_Notify 메시지)를 AAAF에게 전송할 수 있다(S740). AAAF는 UE와 엔드포인트 간 병목 지점의 정보, UE와 관련된 네트워크 QoS 정책 정보(예컨대, UPF의 경우, MFBR, GFBR 등) 및/또는 analytics ID들 등을 종합으로 수집하여 가공할 수 있다. AAAF는 가 공된 분석 결과를 AF에게 제공할 수 있다. 구체적으로, AAAF는 Naaaf_EventExposure_Notify를 통해 분석 결과를 AF에게 제공할 수 있다(S750). AF는 AAAF로부터 수신한 분석 정보, UE의 연산성능 및/또는 배터리상태 등을 종합적으로 고려하 여, 헤드모델의 연산복잡도 및/또는 압축률 등을 결정함으로써, 특정 헤드모델(620-n)을 설계할 수 있다(S760).AF는 병목 삽입된-모델(즉, 헤드모델(620-n)과 테일모델의 셋)을 재학습시킬 수 있다(S770). 표 7은 네트워크 구간별 활용 가능한 NWDAF의 analytics ID를 예시한다. 표 7 구간 analytics ID AN - Central DN Observed Service Experience (OSE) - Service experience for an Application applying RSC(e.g. S-NSSAI, DNN, PDU Session type, SSC Mode, Access Type) or combination of RSC or URSP rule - Service experience for network Slice AM - UPF(UL CL/ BP) Network performance WLAN performance QoS sustainability Observed Service Experience (OSE) - APP over RAT type and/or frequency UPF(C-PSA) - Central DNDN performance User Data congestion Observed Service Experience (OSE) - Service experience for an Application UPF(L-PSA) - EAS Observed Service Experience (OSE) - Service experience for an Edge Application over UP path 응용제공자는 \"Observed service experience (OSE)\"를 통해 직접적으로 UE와 AF 간 종단간(end-to- end) 네트워크 성능 및/또는 서비스 QoE(Quality of Experience)를 알 수 있다. OSE analytics ID의 경우 AN으로부터 중앙 DN을 모두 경유하는 형태로 데이터 경로가 형성되기 때문에, 필요한 경우 AN, 중앙 DN, L-PSA-to-EAS의 구간별 정보를 제공하는 analytics ID를 활용할 수 있다. 예컨대, 성능부족으로 인한 병목구간 구체화가 필요한 경우, 구간별 정보를 제공하는analytics ID를 활용할 수 있다. 응용제공자는 AN의 네트워크 정보를 수집하기 위해 \"network performance\", \"WLAN performance\", \"QoS sustainability\", \"Observed service experience (application over radio access technology and (or) frequency)\" 등의 analytics ID를 활용할 수 있다. 응용제공자는 중앙 DN의 성능정보를 위해 \"DN performance\", \"user data congestion\", \"OSE(service experience for an application)\" 등의 analytics ID를 활용할 수 있다. AN과 중앙 DN, 전달망(예컨대, N3, N9 및 N6 인터페이스)를 포괄하는 정보를 얻기 위해서는 OSE(service experience for an Application applying RSC or combination of RSC or URSP rule)을 활용하거 나, 슬라이스 정보를 바탕으로 한 \"service experience for a Network slice\"를 활용할 수 있다. 도 9는 본 개시의 일 실시예에 따른 헤드모델 및 테일모델의 배포 형태의 다른 예를 보여주는 예시도이다. 테일모델이 단일 엔드포인트(예컨대, AF 또는 EAS)에서 제공되는 케이스에서, 병목 삽입된-모델 의 설계시 단일 헤드모델이 아닌 다수의 헤드모델(620-1, 620-2 및 620-3)이 모두 선택되어 재학습될 수도 있다. 이 경우, UE는 추론 시점에서 다양한 헤드모델들(620-1, 620-2 및 620-3) 중에서 하나의 헤드모델 (620-n)을 선택하여 사용할 수 있다. 헤드모델들(620-1, 620-2 및 620-3)의 연산 및/또는 압축률 특성이 상이한 경우, 네트워크 분석 정보를 바탕으로 자원상황에 적절한 헤드모델(620-n)이 선택될 수 있다. 도 9의 예시에서, 헤드모델(620-n)의 선택에 활용 가능한 분석 대상 영역은, AN, N3 인터페이스, UL/CL UPF(544a)와 L-PSA UPF(544b) 간 N9 인터페이스, 및/또는 EAS와 연결된 N6 인터페이스 등을 포함할 수 있다. 헤드모델(620- n)은 응용 제공자가 제공한 AF 내 알고리즘에 의해 결정되거나(application layer level negotiation), AAAF가 결정할 수 있다. AAAF가 결정하는 경우에는 UE의 연산성능 및 상태정보도 AF를 통해 수집될 수 있다. 도 10은 본 개시의 일 실시예에 따른 헤드모델의 선택 절차를 보여주는 흐름도이다. UE는 AF에게 추론서비스를 요청할 수 있다(S1000). AF는 테일모델의 엔드포인트가 될 EAS를 배포할 수 있다(S1010). AF는 UE의 연산능력, 엔드포인트 간의 네트워킹 성능 상황 등을 고려하여 적절한 헤드모델(620-n)을 선정하기 위해, Naaaf_EventExposure 인터페이스를 호출할 수 있다(S1020). Naaaf_EventExposure 인터페이스의 필터로는, SUPI 및 EAS 정보가 설정될 수 있다. AAAF는 UE와 관련된 네트워크 분석 서비스를 구독할 수 있다. 이를 위해, AAAF는 NWDAF에 게 Nnwdaf_AnalyticsSubscription_Subscribe 메시지를 전송할 수 있다(S1030). 여기서, analytics ID는 \"OSE\" 또는 \"service experience\"로 설정될 수 있다. AAAF는 UE의 무선구간 성능과 연관된 네트워크 분석 서비스를 구독할 수 있다. 이를 위해, AAAF(55 0)는 NWDAF에게 Nnwdaf_AnalyticsSubscription_Subscribe 메시지를 전송할 수 있다(S1040). 여기서, analytics ID는 \"WLAN performance\"로 설정될 수 있다. AAAF는 UE 네트워크 성능의 지속(혹은 변동)가능성과 관련된 네트워크 분석 서비스를 구독할 수 있다. 이 를 위해, AAAF는 NWDAF에게 Nnwdaf_AnalyticsSubscription_Subscribe 메시지를 전송할 수 있다 (S1050). 여기서, analytics ID는 \"sustainability\"로 설정될 수 있다. AAAF는 구독한 분석정보에 대한 분석결과를 받을 수 있다. 예를 들어, AAAF는 Nnwdaf_AnalyticsSubscription_Notify 메시지를 통해 NWDAF로부터 분석결과를 전달받을 수 있다(S1060). 일부 실시예들에서, AAAF는 분석결과를 기초로 UE에서 연산을 수행할 헤드모델(620-n)을 선택할 수 있다(S1070). 이를 위해, AAAF는 AF를 통해 UE의 연산성능 및/또는 상태정보를 더 수집할 수도 있다. AAAF는 분석결과를 AF에게 전달할 수 있다. 예를 들어, AAAF는 Naaaf_EventExposure_Notify를 통해 분석 결과를 AF에게 전달할 수 있다(S1080). 분석결과를 전달하는 과정에서, AAAF는 데이터 처 리 및 가공을 선택적으로 더 수행할 수도 있다. 일부 실시예들에서, AF는 전달받은 분석결과를 기초로 UE에서 연산을 수행할 헤드모델(620-n)을 선택할 수 있다(S1090). 도 11은 본 개시의 일 실시예에 따른 헤드모델 및 테일모델의 배포 형태의 또 다른 예를 보여주는 예시도이다. 실시예들에 따라, 테일모델(640-1 내지 640-2)이 다수의 에지 및 센트럴 클라우드에 위치한 노드들에 분산 배포 된 형태로 UE에게 분할 추론서비스가 제공될 수도 있다. 이 경우, AF는 서비스 요구사항을 충족시키 기 위해 UE가 어떤 테일모델(640-1 내지 640-2)(즉, 어떤 엔드포인트)와 분할추론을 수행할지 결정할 수 있다. 일 예로, AF는 일정 시간마다 적절한 엔드포인트를 선택할 수 있다. 다른 예로, AF는 최초 추 론서비스 개시절차에서, AAAF가 제공한 네트워크 정보를 기초로 엔드포인트를 선택할 수도 있다. 이를 위 해, AAAF는 네트워크 분석정보를 네트워크 분석 정보를 바탕으로 엔드포인트 선택을 위해 참조할 정보를 가공하여, AF에게 제공할 수 있다. 또한, AAAF는 엔드포인트 선택절차에 보다 적극적으로 개입하여, 테일모델(640-1 내지 640-2)의 엔드포인트를 직접 설정하고, UE 및 엔드포인트 간 트래픽 경로를 설정하도록 PCF에게 정책을 요청할 수도 있다. 도 12a 및 도 12b는 본 개시의 일 실시예에 따른 엔드포인트 선택 및 라우팅 절차를 보여주는 흐름도이다. UE는 AF에게 응용레벨에서의 추론서비스 세션 요청 메시지를 전달할 수 있다(S1200). AF는 테일 모델이 포함된 EAS를 배포할 수 있다(S1202). 이미 배포된 EAS를 이용 가능한 경우, 과정 S1202가 생 략될 수도 있다. AF는 Nnef_EASDeployment_Create 인터페이스를 통해 코어 네트워크(예컨대, NEF)로 EAS 배포정보를 전달할 수 있다(S1204). AF는 Naaaf_EndpointRouting_Create request 인터페이스를 호출할 수 있다(S1210). 이때, 메시지의 파라 미터로는 UE ID, APP ID, Model 정보 및/또는 QoS요구사항을 포함할 수 있다. Model 정보는 분할 정보(예컨대, 분할 지점 등) 및/또는 입출력 사이즈 등이 포함될 수 있다. QoS 요구사항은, 비트율(biterate)과 총 지연시간(total latency)을 포함할 수 있다. 총 지연시간은, UE의 연산에 소요되는 시간, 중간 결과물의 전송에 소 요되는 시간, 엔드포인트의 연산에 소요되는 시간 및 연산결과의 반환에 소요되는 시간의 합을 포함할 수 있다. AAAF는 EAS 배포정보를 얻기 위해 Nnef_EASDeployment_Subscribe를 요청하고(S1220), NEF로부터 요 청한 EAS 배포정보를 통지(notify)받을 수 있다(S1222). AAAF는 NWDAF를 통해 네트워크 분석정보를 수집할 수 있다(S1230). 예를 들어, AAAF는 Nnwdaf_AnalyticsSubscription_Subscribe 및/또는 Nnwdaf_AnalyticsSubscription_Notify를 통해, NWDAF 에게 네트워크 분석 서비스를 구독하고 NWDAF로부터 분석결과를 통지받을 수 있다. 여기서, analytics ID 로는 UE-EAS 간 네트워크 구간(Access, transport 및 data network)과 관련된 분석을 제공하는 network performance, WLAN performance, QoS sustainability, service experience, DN performance 및/또는 user data congestion 등이 활용될 수 있다. AAAF는 획득한 분석 정보를 기반으로 가장 적절한 테일모델의 엔드포인트를 선정할 수 있다(S1240). 선정된 엔드포인트로 UE의 서비스 트래픽을 라우팅하기 위해, traffic influence 절차(S1250)가 활용될 수 있다. AAAF는 Nnef_TrafficInfluence_Create request를 통해 라우팅을 요청할 수 있다(S1252). Nnef_TrafficInfluence_Create requests는 AF 서비스 식별자와, 매핑될 슬라이스 정보(SNSSAI) 및 EAS와 연결 성을 제공할 DNAI 정보 등을 포함할 수 있다. 3GPP TS23.502 4.3.6절에 따라 후속 절차가 수행되어, UE의 라우팅 경로가 업데이트될 수 있다(S1254). 과정 S1250의 대안으로, URSP(UE Route Selection Policy)가 활용될 수도 있다(S1260). AAAF는 URSP를 활 용하여 라우팅을 제어하도록Nnef_ServiceParameterCreate Request 인터페이스를 호출할 수 있다(S1262). 후속 절차로는 표준규격 TS23.502 4.15절에 정의된 Service specific parameter provisioning 및 Application guidance for URSP determination 절차가 수행될 수 있다(S1264). 결과적으로 UE의 추론서비스 관련 트래 픽을 구분하는 식별자를 기반으로 AAAF가 결정한 EAS로 라우팅 되도록 URSP 관련 파라메터의 값이 적용될 수 있다. 이를 위해, Nnef_ServiceParameterCreate Request는, UE ID와 App ID를 기반으로한 URSP의 트래픽 기 술자(Traffic Descriptor) 및 경로 선택 요소(route selection)(예컨대, S-NSSAI, DNN, PDU 세션 타입 및 PDU 세션 pair ID 등) 중 하나 이상을 포함할 수 있다. 라우팅을 설정하고 나면 최종적으로 AAAF는 AF에게 Naaaf_EndpointRouting_create Request에 대해 응답한다. AAAF는 Naaaf_EndpointRouting_create Reply를 통해 요청에 대해 응답할 수 있다(S1270). 응답 의 내용으로는, 선택된 엔드포인트(예컨대, EAS)에 대한 정보 및/또는 UE와 엔드포인트 간에 적용된 QoS정보가 포함될 수 있다. 과정 S1250 및 S1260은, 2022.12월 정의된 릴리스 18을 기반으로 한 것으로, AAAF 및 5G 시스템의 표준규격 개 발에 따라 변동될 수도 있다. 특히 과정 S1250 및 S1260에서는 라우팅 정보가 NEF 인터페이스에 의해 간접적으 로 PCF를 통해 SMF-UPF로 전달되는 절차지만, 구현예에 따라 중간 기능블록을 생략하고 직접적으로 UPF, SMF 및 /또는 UE(URSP의 경우)로 전달될 수도 있다. 한편, 이상에서는, AAAF가 다른 네트워크 엔티티들과 별도로 구분되는, 독립적인 네트워크 엔티티인 것으 로 가정하였으나, 다른 예에서, AAAF의 기능 및/또는 서비스들이 통신 시스템의 다른 네트워크 엔티티에 의해 수행될 수도 있다. 예컨대, 2023.08. 기준 3GPP Release-18 표준규격에서는 인공지능을 포함한 응용서비스를 지원하는 기능들은 모 두 NEF에 추가되고 있다. 이와 같이, NEF 내 신규 응용지원 기능이 확장되고 있는 현재표준의 규격화 동향을 고려하면, AAAF의 기능 및/또는 서비스들은 NEF에 포함되어 구현될 수도 있다. 이 경우, 전술 한 예시들에서 활용된 기능들은 표 8과 같이 NEF 기능과 매핑될 수 있다. 표 8 AAAF NEF 비고 Naaaf_EventExposure Nnef_EventExposure 기존 NEF 기능 확장 또는 Nnef_AnalyticsExposure기존 NEF 기능 확장 Naaaf_EndpointSelection Nnef_EndpointSelection 신규 Naaaf_EndpointRouting Nnef_EndpointRouting 신규 Naaaf_ModelSelection Nnef_ModelSelection 신규표 8을 참조하면, 5G 제어 플레인 기능들이 공통적으로 제공하는 서비스인 AAAF의 EventExposure 기능은 NEF의 EventExposure로 통합 구현될 수 있고, 또는 분석정보를 제공하는 기능의 경우 현재 NEF의 AnalyticsExposure 기능으로 대체되어 활용될 수 있다. 반면, 엔드포인트 선택, 라우팅 및/또는 헤드모델 선택 기능의 경우 현재 NEF에서 유사한 기능을 제공하지 않고 있으므로, 신규 기능으로 도입될 수 있다. 예컨대, 도 7 및/또는 도 10에서 전술한 절차들에서 AAAF 는 모두 NEF로 대체되어 수행될 수 있다. 도 12a 내지 도 12b의 엔드포인트 선택 및/또는 라우팅 절차에서, AAAF와 NEF와 상호작용은 후술할 도 13과 같이 NEF 내에서 처리되는 형태로 구현될 수 있다. 도 13은 본 개시의 다른 실시예에 따른 엔드포인트 선택 및 라우팅 절차를 보여주는 흐름도이다. UE는 AF에게 응용레벨에서의 추론서비스 세션 요청 메시지를 전달할 수 있다(S1300). AF는 테일 모델이 포함된 EAS를 배포할 수 있다(S1302). 이미 배포된 EAS를 이용 가능한 경우, 과정 S1302가 생 략될 수도 있다. AF는 Nnef_EASDeployment_Create 인터페이스를 통해 NEF로 EAS 배포정보를 전달할 수 있다(S1304). AF는 Nnef_EndpointRouting_Create Request 인터페이스를 호출할 수 있다(S1310). 이때, 메시지의 파라미 터로는 UE ID, APP ID, Model 정보 및/또는 QoS요구사항을 포함할 수 있다. Model 정보는 분할 정보(예컨대, 분 할 지점 등) 및/또는 입출력 사이즈 등이 포함될 수 있다. QoS 요구사항은, 비트율(biterate)과 총 지연시간 (total latency)을 포함할 수 있다. 총 지연시간은, UE의 연산에 소요되는 시간, 중간 결과물의 전송에 소 요되는 시간, 엔드포인트의 연산에 소요되는 시간 및 연산결과의 반환에 소요되는 시간의 합을 포함할 수 있다. NEF는 NWDAF를 통해 네트워크 분석정보를 수집할 수 있다(S1330). 예를 들어, NEF는 Nnwdaf_AnalyticsSubscription_Subscribe 및/또는 Nnwdaf_AnalyticsSubscription_Notify를 통해, NWDAF 에게 네트워크 분석 서비스를 구독하고 NWDAF로부터 분석결과를 통지받을 수 있다. 여기서, analytics ID 로는 UE-EAS 간 네트워크 구간(Access, transport 및 data network)과 관련된 분석을 제공하는 network performance, WLAN performance, QoS sustainability, service experience, DN performance 및/또는 user data congestion 등이 활용될 수 있다. NEF는 획득한 분석 정보를 기반으로 가장 적절한 테일모델의 엔드포인트를 선정할 수 있다(S1340). 선정된 엔드포인트로 UE의 서비스 트래픽을 라우팅하기 위해, traffic influence 절차가 활용되거나 (S1350), URSP(UE Route Selection Policy)가 활용될 수도 있다(S1360). 과정 S1350 또는 과정 S1360은 도 12b 에 도시된 과정 S1254 또는 과정 S1264와 동일하거나 상응하므로, 이에 대한 중복되는 내용은 생략한다. 라우팅을 설정하고 나면 최종적으로 NEF는 AF에게 Nnef_EndpointRouting_create Request에 대해 응 답한다. NEF는 Nnef_EndpointRouting_create Response를 통해 요청에 대해 응답할 수 있다(S1370). 응답 의 내용으로는, 선택된 엔드포인트(예컨대, EAS)에 대한 정보 및/또는 UE와 엔드포인트 간에 적용된 QoS정보가 포함될 수 있다. 도 14는 본 개시의 일 실시예에 따른 코어 네트워크의 기능 엔티티가 인공지능 응용 서비스를 지원하는 방법을 나타내는 순서도이다. 도 14에 도시된 방법은, 전술한 통신 시스템의 하나 이상의 네트워크 기능이 적어도 하나 의 전자장치에 의해 수행됨으로써 구현될 수 있다. 따라서 이하의 설명은 전자장치가 수행하는 동작 측면으로도 이해될 수 있다. 이하에서는, 코어 네트워크의 제1 기능 엔티티가 도 14에 도시된 방법을 수행하는 것으로 가정 하여 설명한다. 과정 S1400에서, 제1 네트워크 엔티티는, 제1 네트워크 엔티티와 구별되는 제2 네트워크 엔티티로부터 분할연산 기반 인공지능 응용에 대한 지원 서비스를 요청받을 수 있다. 여기서, 제2 네트워크 엔티티는, AF일 수 있 다. 분할연산 기반 인공지능 응용은, 헤드모델 및 테일모델을 포함하는 병목-삽입된 모델(Bottleneck-injected model)을 이용하여 추론을 수행하는 응용일 수 있다. 분할연산 기반 인공지능 응용에 대해 지원 서비스는, 헤드모델의 구조 설계 보조, 헤드모델의 선택 보조, 헤드 모델의 선택, 테일모델의 엔드포인트 선택 보조, 테일모델의 엔드포인트 선택 및 사용자 단말과 엔드포인트 간 라우팅 중 하나 이상을 포함할 수 있다. 일 예로, 제1 네트워크 엔티티는 제2 네트워크 엔티티로부터 헤드모델의 구조 설계 보조를 요청받을 수 있다. 다른 예로, 제1 네트워크 엔티티는, 제2 네트워크 엔티티로부터 헤드모 델의 선택 보조 또는 헤드모델의 선택을 요청받을 수 있다. 여기서, 헤드모델의 선택은 복수개의 기훈련된 헤드 모델들 중에서 사용자 단말이 연산을 수행할 헤드모델을 선택하는 것을 의미할 수 있다. 또 다른 예로, 제1 네 트워크 엔티티는, 제2 네트워크 엔티티로부터 테일모델에 대한 연산을 수행할 엔드포인트의 선택 보조 또는 엔 드포인트의 선택을 요청받을 수 있다. 여기서, 제1 네트워크 엔티티는 사용자 단말에 의한 추론이 개시되기 이 전, 또는 사용자 단말에 의한 추론이 수행되는 도중에 미리 설정된 시간 간격으로 제2 네트워크 엔티티로부터, 선택 보조 또는 선택을 요청받을 수 있다. 여기서, 엔드포인트의 선택은, 배포된 복수개의 후보 엔드포인트들 중에서 특정한 사용자 단말과 연동하여 분할추론을 수행할 엔드포인트를 선택하는 것을 의미할 수 있다. 과정 S1420에서, 제1 네트워크 엔티티는, 제1 및 제2 네트워크 엔티티와 구별되는 제3 네트워크 엔티티로부터 네트워크 분석정보를 수신할 수 있다. 여기서, 제3 네트워크 엔티티는, NWDAF일 수 있다. 제1 네트워크 엔 티티는, 제3 네트워크 엔티티가 제공하는 인터페이스를 호출하여 네트워크 분석 서비스를 구독할 수 있다. 제1 네트워크 엔티티는 제3 네트워크로부터 구독한 네트워크 분석 서비스에 대한 분석결과를 보고받을 수 있다. 일 예로, 과정 S1400에서 제2 네트워크 엔티티로부터 헤드모델의 구조 설계 보조를 요청받은 경우, 과정 S142에 서 수신하는 네트워크 분석정보에는, 헤드모델에 대한 연산을 수행할 사용자 단말이 테일모델에 대한 연산을 수 행할 엔드포인트에 도달하기 위해 경유해야 하는 네트워크 구간들의 전부 또는 일부, 네트워크 구간들 중 병목 지점, 계약에 의한 QoS, 사용자 단말과 엔드포인트 간 보장받을 수 있는 QoS, 및 네트워크 성능의 지속 가능성 중 하나 이상에 관한 분석결과가 포함될 수 있다. 다른 예로, 과정 S1400에서 제2 네트워크 엔티티로부터 헤드 모델의 선택 보조 또는 헤드모델의 선택을 요청받은 경우, 과정 S142에서 수신하는 네트워크 분석정보에는, 헤 드모델에 대한 연산을 수행할 사용자 단말에 대한 관측된 서비스 경험(observed service experience, OSE), 사 용자 단말과 기지국 사이의 무선구간 성능 및 사용자 단말에 대한 네트워크 성능의 지속 가능성 중 하나 이상에 대한 분석결과가 포함될 수 있다. 또 다른 예로, 과정 S1400에서 제2 네트워크 엔티티로부터 엔드포인트의 선택 보조 또는 엔드포인트의 선택을 요청받은 경우, 과정 S142에서 수신하는 네트워크 분석정보에는, 테일모델이 배 포된 하나 이상의 후보 엔드포인트들과 헤드모델에 대한 연산을 수행할 사용자 단말 사이의 네트워크 구간들의 전부 또는 일부에 대한 분석 결과가 포함될 수 있다. 과정 S1440에서, 제1 네트워크 엔티티는, 네트워크 분석정보를 기초로, 분할연산 기반 인공지능 응용에 대한 지 원 서비스를 제공할 수 있다(S1440). 일 실시예에서, 제1 네트워크 엔티티는, 제3 네트워크 엔티티로부터 수신한 분석정보를 제2 네트워크 엔티티에 게 전달할 수 있다. 다른 실시예에서, 제1 네트워크 엔티티는, 수신한 분석정보를 가공하여 제2 네트워크 엔티티에게 제공할 수 있 다. 일 예로, 제1 네트워크 엔티티는 네트워크 분석정보를 기초로, 헤드모델의 구조 설계에 필요한 정보를 가공 하고, 가공된 정보를 제2 네트워크 엔티티에게 전송할 수 있다. 다른 예로, 제1 네트워크 엔티티는 네트워크 분 석정보를 기초로, 헤드모델의 선택에 필요한 정보를 가공하고, 가공된 정보를 제2 네트워크 엔티티에게 전송할 수 있다. 또 다른 예로, 제1 네트워크 엔티티는 네트워크 분석정보를 기초로, 엔드포인트의 선택에 필요한 정보 를 가공하고, 가공된 정보를 제2 네트워크 엔티티에게 전송할 수 있다. 또 다른 실시예에서, 제1 네트워크 엔티티는, 수신한 분석정보를 기초로 헤드모델 또는 엔드포인트를 직접 선택 할 수도 있다. 일 예로, 제1 네트워크 엔티티는, 네트워크 분석정보를 기초로 서로 다른 구조를 갖는 복수개의 기훈련된 헤드모델들 중에서 사용자 단말이 연산을 수행할 헤드모델을 선택하고, 선택된 헤드모델에 관한 정보 를 제2 네트워크 엔티티에게 전송할 수 있다. 이를 위해, 제1 네트워크 엔티티는 제2 네트워크 엔티티로부터 사 용자 단말의 연산성능 및 상태 정보 중 하나 이상을 수집할 수 있다. 다른 예로, 제1 네트워크 엔티티는, 네트 워크 분석정보를 기초로, 하나 이상의 후보 엔드포인트들 중에서 사용자 단말과 분할 추론을 수행할 엔드포인트 를 선택할 수 있다. 제1 네트워크 엔티티는, 제1 내지 제3 네트워크 엔티티와 구별되는 제4 네트워크 엔티티에 게 상과 사용자 단말과 선택된 엔드포인트 사이의 서비스 트래픽 라우팅을 요청할 수 있다. 여기서, 제4 네트워 크 엔티티는, NEF일 수 있다. 제1 네트워크 엔티티는, 선택된 엔드포인트 및 사용자 단말과 엔드포인트 간 적용된 QoS 중 하나 이상에 대한 정보를 포함하는, 응답 메시지를 제2 네트워크 엔티티로 전송할 수 있다. 한편, 과정 S1400에서 제2 네트워크 엔티티로부터 엔드포인트의 선택 보조 또는 엔드포인트의 선택을 요청받은 경우, 제1 네트워크 엔티티는, 과정 S1420을 수행하기에 앞서, 제1 내지 제3 네트워크 엔티티와 구별되는 제4 네트워크 엔티티로부터, 하나 이상의 후보 엔드포인트들에 대한 배포 정보를 수신할 수도 있다. 다른 예에서, 제1 네트워크 엔티티는 과정 S1400을 수행하기 이전에, 제2 네트워크로부터 하나 이상의 후보 엔드포인트들에대한 배포 정보를 수신할 수도 있다. 도 15는 본 개시의 일 실시예에 따른 인공지능 응용 서비스를 지원하기 위한 기능을 포함하는 전자장치를 개략 적으로 나타낸 블록구성도이다. 전자장치는 통신 인터페이스, 메모리 및 프로세서를 전부 또는 일부 포함한다. 통신 인터페이스는 프로세서와 연결되어, 다른 전자장치와 각종 무선신호를 송수신할 수 있다. 통 신 인터페이스는 예컨대, 송신 필터, 수신 필터, 증폭기, 믹서(mixer), 오실레이터(oscillator), DAC(Digital to Analog Convertor) 및 ADC(Analog to Digital Convertor) 등을 전부 또는 일부 포함할 수 있다. 메모리는 프로세서와 연결되어, 프로세서를 구동하기 위한 다양한 정보를 저장할 수 있다. 메모리는 프로세서를 통해 실행가능한 적어도 하나의 명령어를 저장할 수 있다. 메모리는 휘 발성 메모리 및 비휘발성 메모리 중 적어도 하나를 포함할 수 있다. 휘발성 메모리는 SRAM(Static Random Access Memory) 또는 DRAM(Dynamic Random Access Memory) 등 중 어느 하나를 포함할 수 있고, 비휘발성 메모 리는 플래시 메모리(flash memory) 등 중 어느 하나를 포함할 수 있다. 프로세서는 통신 인터페이스가 각종 무선신호를 송수신하도록 제어할 수 있으며, 전송할 또는 수신 한 무선신호 등을 메모리에 저장하도록 제어할 수도 있다. 프로세서는 본 개시에서 제안된 기능, 과정 및/또는 방법을 구현하도록 구성될 수 있다. 전술한 예시들에 서 하나 이상의 기능 엔티티의 동작이 프로세서에 의해 구현될 수 있다. 일 예로, 프로세서는 통신 인터페이스 및/또는 메모리와 연동하여 도 13에 도시된 방법을 수행할 수 있다. 본 개시의 예시적인 실시예들에 기술된 적어도 일부의 구성요소들은 DSP(Digital Signal Processor), 프로세서, 컨트롤러, ASIC(Application-Specific IC), 프로그래머블 로직소자(FPGA 등), 기타 전자소자 중의 적어도 하나 또는 이들의 조합이 포함되는 하드웨어 요소로써 구현될 수 있다. 또한, 예시적인 실시예들에서 기술된 적어도 일부의 기능(function)들 또는 처리과정(process)들은 소프트웨어로 구현될 수 있으며, 소프트웨어는 기록매체 에 저장될 수 있다. 본 개시의 예시적인 실시예들에 기술된 적어도 일부의 구성요소들, 기능들, 그리고 처리과 정들은 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 본 개시의 예시적인 실시예들에 따른 방법은 컴퓨터에서 실행될 수 있는 프로그램으로 작성될 수 있고, 마그네 틱 저장매체, 광학적 판독매체, 디지털 저장매체 등 다양한 기록 매체로도 구현될 수 있다. 본 명세서에 설명된 각종 기술들의 구현들은 디지털 전자 회로조직으로, 또는 컴퓨터 하드웨어, 펌웨어, 소프트 웨어로, 또는 그들의 조합들로 구현될 수 있다. 구현들은 데이터 처리 장치, 예를 들어 프로그램가능 프로세서, 컴퓨터, 또는 다수의 컴퓨터들의 동작에 의한 처리를 위해, 또는 이 동작을 제어하기 위해, 컴퓨터 프로그램 제 품, 즉 정보 캐리어, 예를 들어 기계 판독가능 저장 장치(컴퓨터 판독가능 매체) 또는 전파 신호에서 유형적으 로 구체화된 컴퓨터 프로그램으로서 구현될 수 있다. 상술한 컴퓨터 프로그램(들)과 같은 컴퓨터 프로그램은 컴 파일된 또는 인터프리트된 언어들을 포함하는 임의의 형태의 프로그래밍 언어로 기록될 수 있고, 독립형 프로그 램으로서 또는 모듈, 구성요소, 서브루틴, 또는 컴퓨팅 환경에서의 사용에 적절한 다른 유닛으로서 포함하는 임 의의 형태로 전개될 수 있다. 컴퓨터 프로그램은 하나의 사이트에서 하나의 컴퓨터 또는 다수의 컴퓨터들 상에 서 처리되도록 또는 다수의 사이트들에 걸쳐 분배되고 통신 네트워크에 의해 상호 연결되도록 전개될 수 있다. 컴퓨터 프로그램의 처리에 적절한 프로세서들은 예로서, 범용 및 특수 목적 마이크로프로세서들 둘 다, 및 임의 의 종류의 디지털 컴퓨터의 임의의 하나 이상의 프로세서들을 포함한다. 일반적으로, 프로세서는 판독 전용 메 모리 또는 랜덤 액세스 메모리 또는 둘 다로부터 명령어들 및 데이터를 수신할 것이다. 컴퓨터의 요소들은 명령 어들을 실행하는 적어도 하나의 프로세서 및 명령어들 및 데이터를 저장하는 하나 이상의 메모리 장치들을 포함 할 수 있다. 일반적으로, 컴퓨터는 데이터를 저장하는 하나 이상의 대량 저장 장치들, 예를 들어 자기, 자기-광 디스크들, 또는 광 디스크들을 포함할 수 있거나, 이것들로부터 데이터를 수신하거나 이것들에 데이터를 송신하 거나 또는 양쪽으로 되도록 결합될 수도 있다. 컴퓨터 프로그램 명령어들 및 데이터를 구체화하는데 적절한 정 보 캐리어들은 예로서 반도체 메모리 장치들, 예를 들어, 하드 디스크, 플로피 디스크 및 자기 테이프와 같은 자기 매체(Magnetic Media), CD-ROM(Compact Disk Read Only Memory), DVD(Digital Video Disk)와 같은 광 기 록 매체(Optical Media), 플롭티컬 디스크(Floptical Disk)와 같은 자기-광 매체(Magneto-Optical Media), 롬 (ROM, Read Only Memory), 램(RAM, Random Access Memory), 플래시 메모리, EPROM(Erasable Programmable ROM), EEPROM(Electrically Erasable Programmable ROM) 등을 포함한다. 프로세서 및 메모리는 특수 목적 논리회로조직에 의해 보충되거나, 이에 포함될 수 있다. 프로세서는 운영 체제(Operating System) 및 상기 운영 체제 상에서 수행되는 소프트웨어 애플리케이션을 수행 할 수 있다. 또한, 프로세서 디바이스는 소프트웨어의 실행에 응답하여, 데이터를 접근, 저장, 조작, 처리 및 생성할 수도 있다. 이해의 편의를 위하여, 프로세서 디바이스는 하나가 사용되는 것으로 설명된 경우도 있지만,"}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "해당 기술분야에서 통상의 지식을 가진 자는, 프로세서 디바이스가 복수 개의 처리 요소(processing element) 및/또는 복수 유형의 처리 요소를 포함할 수 있음을 알 수 있다. 예를 들어, 프로세서 디바이스는 복수 개의 프 로세서 또는 하나의 프로세서 및 하나의 컨트롤러를 포함할 수 있다. 또한, 병렬 프로세서(parallel processo r)와 같은, 다른 처리 구성(processing configuration)도 가능하다. 또한, 비-일시적 컴퓨터 판독가능 매체(non-transitory computer-readable media)는 컴퓨터에 의해 액세스될 수 있는 임의의 가용매체일 수 있고, 컴퓨터 저장매체 및 전송매체를 모두 포함할 수 있다. 본 명세서는 다수의 특정한 구현물의 세부사항들을 포함하지만, 이들은 어떠한 발명이나 청구 가능한 것의 범위 에 대해서도 제한적인 것으로서 이해되어서는 안되며, 오히려 특정한 발명의 특정한 실시형태에 특유할 수 있는 특징들에 대한 설명으로서 이해되어야 한다. 개별적인 실시형태의 문맥에서 본 명세서에 기술된 특정한 특징들 은 단일 실시형태에서 조합하여 구현될 수도 있다. 반대로, 단일 실시형태의 문맥에서 기술한 다양한 특징들 역 시 개별적으로 혹은 어떠한 적절한 하위 조합으로도 복수의 실시형태에서 구현 가능하다. 나아가, 특징들이 특 정한 조합으로 동작하고 초기에 그와 같이 청구된 바와 같이 묘사될 수 있지만, 청구된 조합으로부터의 하나 이 상의 특징들은 일부 경우에 그 조합으로부터 배제될 수 있으며, 그 청구된 조합은 하위 조합이나 하위 조합의 변형물로 변경될 수 있다. 마찬가지로, 특정한 순서로 도면에서 동작들을 묘사하고 있지만, 이는 바람직한 결과를 얻기 위하여 도시된 그 특정한 순서나 순차적인 순서대로 그러한 동작들을 수행하여야 한다거나 모든 도시된 동작들이 수행되어야 하는 것으로 이해되어서는 안 된다. 특정한 경우, 멀티태스킹과 병렬 프로세싱이 유리할 수 있다. 또한, 상술한 실시 형태의 다양한 장치 컴포넌트의 분리는 그러한 분리를 모든 실시형태에서 요구하는 것으로 이해되어서는 안되며, 설명한 프로그램 컴포넌트와 장치들은 일반적으로 단일의 소프트웨어 제품으로 함께 통합되거나 다중 소프트웨어 제품에 패키징 될 수 있다는 점을 이해하여야 한다. 한편, 본 명세서와 도면에 개시된 본 발명의 실시 예들은 이해를 돕기 위해 특정 예를 제시한 것에 지나지 않으 며, 본 발명의 범위를 한정하고자 하는 것은 아니다. 여기에 개시된 실시 예들 이외에도 본 발명의 기술적 사상"}
{"patent_id": "10-2023-0110677", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "에 바탕을 둔 다른 변형 예들이 실시 가능하다는 것은, 본 발명이 속하는 기술분야에서 통상의 지식을 가진 자 에게 자명한 것이다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4 도면5 도면6 도면7 도면8 도면9 도면10 도면11 도면12a 도면12b 도면13 도면14 도면15"}
{"patent_id": "10-2023-0110677", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시예에 따른 분할연산의 기본적인 구조를 보여주는 개념도이다 도 2는 AlexNet 모델의 레이어별 출력 데이터의 연산 레이턴시와 출력 데이터의 사이즈를 보여주는 예시도이다. 도 3은 VGG-16 모델의 레이어별 출력 데이터의 연산 레이턴시와 출력 데이터의 사이즈를 보여주는 예시도이다. 도 4는 본 개시의 일 실시예에 따른 인공 병목 개념이 적용된 분할연산을 보여주는 개념도이다. 도 5는 본 개시의 일 실시 예에 따른 통신 시스템의 아키텍처를 예시한 도면이다. 도 6은 본 개시의 일 실시예에 따른 헤드모델의 다양한 예를 보여주는 예시도이다. 도 7은 본 개시의 일 실시예에 따른 헤드모델의 설계 절차를 보여주는 흐름도이다. 도 8은 본 개시의 일 실시예에 따른 헤드모델 및 테일모델의 배포 형태의 일 예를 보여주는 예시도이다. 도 9는 본 개시의 일 실시예에 따른 헤드모델 및 테일모델의 배포 형태의 다른 예를 보여주는 예시도이다. 도 10은 본 개시의 일 실시예에 따른 헤드모델의 선택 절차를 보여주는 흐름도이다. 도 11은 본 개시의 일 실시예에 따른 헤드모델 및 테일모델의 배포 형태의 또 다른 예를 보여주는 예시도이다. 도 12a 및 도 12b는 본 개시의 일 실시예에 따른 엔드포인트 선택 및 라우팅 절차를 보여주는 흐름도이다. 도 13은 본 개시의 다른 실시예에 따른 엔드포인트 선택 및 라우팅 절차를 보여주는 흐름도이다. 도 14는 본 개시의 일 실시예에 따른 코어 네트워크의 기능 엔티티가 인공지능 응용 서비스를 지원하는 방법을 나타내는 순서도이다. 도 15는 본 개시의 일 실시예에 따른 인공지능 응용 서비스를 지원하기 위한 기능을 포함하는 전자장치를 개략 적으로 나타낸 블록구성도이다."}
