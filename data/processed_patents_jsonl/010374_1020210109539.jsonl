{"patent_id": "10-2021-0109539", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2021-0110780", "출원번호": "10-2021-0109539", "발명의 명칭": "멀티태스크 모델의 파라미터 업데이트 방법, 장치 및 전자 기기", "출원인": "베이징 바이두 넷컴 사이언스 앤 테크놀로지 코.,", "발명자": "쟝, 원후이"}}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "멀티태스크 모델의 파라미터 업데이트 방법에 있어서,트레이닝 샘플 집합을 획득하는 단계 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함함;상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 단계;임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의샘플을 사용하여 상기 멀티태스크 모델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는모델 파라미터 업데이트 경도를 생성하는 단계;상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의파라미터에 대해 업데이트하는 단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 단계는,상기 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정하는 단계; 및각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로넣는 단계는,임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 상기 복수의 태스크 태그 중의 각 태스크 태그에 대응되는 샘플 대열에 상기 임의의 샘플이 모두 포함되는 것을 결정하는 단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의파라미터에 대해 업데이트하는 단계는,상기 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하는 단계;상기 생성된 모델 파라미터 업데이트 경도를 상기 타겟 파라미터 서버에 송신하여, 상기 타겟 파라미터 서버로하여금, 상기 업데이트 경도에 따라 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에대해 업데이트하도록 하는 단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법.공개특허 10-2021-0110780-3-청구항 5 제1항에 있어서,상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의파라미터에 대해 업데이트하는 단계는,상기 임의의 샘플 대열에 관련된 태스크의 가중치를 결정하는 단계;상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서,상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계는,상기 업데이트 경도에 따라 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계;상기 가중치 및 상기 업데이트 경도에 따라 상기 공유 네트워크 계층의 업데이트 경도를 결정하는 단계; 및상기 공유 네트워크 계층의 업데이트 경도에 따라 상기 공유 네트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "멀티태스크 모델의 파라미터 업데이트 장치에 있어서,트레이닝 샘플 집합을 획득하는 획득 모듈 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함함;상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 분류 모듈;임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의샘플을 사용하여 상기 멀티태스크 모델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는모델 파라미터 업데이트 경도를 생성하는 트레이닝 모듈; 및상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의파라미터에 대해 업데이트하는 업데이트 모듈을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 분류 모듈은,상기 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정하는 제1 결정 유닛; 및각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는분류 유닛을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,공개특허 10-2021-0110780-4-상기 분류 유닛은,임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 상기 복수의 태스크 태그 중의 각 태스크 태그에 대응되는 샘플 대열에 상기 임의의 샘플이 모두 포함되는 것을 결정하는 제1 결정 서브 유닛을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 업데이트 모듈은,상기 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하는 제2 결정 유닛; 및상기 생성된 모델 파라미터 업데이트 경도를 상기 타겟 파라미터 서버에 송신하여, 상기 타겟 파라미터 서버로하여금, 상기 업데이트 경도에 따라 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에대해 업데이트하도록 하는 송신 유닛을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제7항에 있어서,상기 업데이트 모듈은,상기 임의의 샘플 대열에 관련된 태스크의 가중치를 결정하는 제3 결정 유닛; 및상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 업데이트 유닛을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 업데이트 유닛은,상기 업데이트 경도에 따라 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 제1 업데이트 서브유닛;상기 가중치 및 상기 업데이트 경도에 따라 상기 공유 네트워크 계층의 업데이트 경도를 결정하는 제2 결정 서브 유닛; 및상기 공유 네트워크 계층의 업데이트 경도에 따라 상기 공유 네트워크 계층의 파라미터에 대해 업데이트하는 제2 업데이트 서브 유닛을 포함하는 것,을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "전자 기기에 있어서,적어도 하나의 프로세서; 및상기 적어도 하나의 프로세서와 통신 가능하게 연결되는 메모리를 포함하고; 여기서,상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 것,을 특징으로 하는 전자 기기.공개특허 10-2021-0110780-5-청구항 14 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체에 있어서,상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 것,을 특징으로 하는 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "컴퓨터 프로그램 제품에 있어서,컴퓨터 프로그램을 포함하고, 상기 컴퓨터 프로그램이 프로세서에 의해 수행되는 경우 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 것,을 특징으로 하는 컴퓨터 프로그램 제품."}
{"patent_id": "10-2021-0109539", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램에 있어서,상기 컴퓨터 프로그램중의 명령이 실행될 경우, 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법이 실행되는,것을 특징으로 하는 컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램."}
{"patent_id": "10-2021-0109539", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 출원은 멀티태스크 모델의 파라미터 업데이트 방법, 장치 및 전자 기기를 개시하고, 컴퓨터"}
{"patent_id": "10-2021-0109539", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "에 관한 것으로, 특히 딥러닝 및 인공지능 기술 분야에 관한 것이다. 구체적인 구현 방안은 트레이닝 샘플 집합을 획득하 고; 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고; 임의의 샘플 대열 중의 (뒷면에 계속) 대 표 도 - 도1"}
{"patent_id": "10-2021-0109539", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 2, "content": "공개특허10-2021-0110780 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 임의의 샘플 대열 중의 데이터를 사용하여, 멀티태스크 모델 중의 공유 네트워크 계층 및 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대 해 트레이닝하여, 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고; 업데 이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데 이트하는 것을 포함한다. 이리하여, 이러한 멀티태스크 모델의 파라미터 업데이트 방법을 통해, 멀티태스크 모델 에 대해 분산형 트레이닝할 경우, 국부 파라미터 업데이트를 구현하여, 멀티태스크 모델의 정밀도를 개선한다. CPC특허분류 G06N 3/08 (2013.01) G06N 5/02 (2019.01)명 세 서 청구범위 청구항 1 멀티태스크 모델의 파라미터 업데이트 방법에 있어서, 트레이닝 샘플 집합을 획득하는 단계 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함함; 상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 단계; 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의 샘플을 사용하여 상기 멀티태스크 모델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하는 단계; 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법. 청구항 2 제1항에 있어서, 상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 단계는, 상기 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정하는 단계; 및 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는 단계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법. 청구항 3 제2항에 있어서, 상기 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는 단계는, 임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 상기 복수의 태스크 태그 중의 각 태스크 태그에 대 응되는 샘플 대열에 상기 임의의 샘플이 모두 포함되는 것을 결정하는 단계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법. 청구항 4 제1항에 있어서, 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계는, 상기 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하는 단계; 상기 생성된 모델 파라미터 업데이트 경도를 상기 타겟 파라미터 서버에 송신하여, 상기 타겟 파라미터 서버로 하여금, 상기 업데이트 경도에 따라 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하도록 하는 단계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법.청구항 5 제1항에 있어서, 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계는, 상기 임의의 샘플 대열에 관련된 태스크의 가중치를 결정하는 단계; 상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네 트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법. 청구항 6 제5항에 있어서, 상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네 트워크 계층의 파라미터에 대해 업데이트하는 단계는, 상기 업데이트 경도에 따라 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계; 상기 가중치 및 상기 업데이트 경도에 따라 상기 공유 네트워크 계층의 업데이트 경도를 결정하는 단계; 및 상기 공유 네트워크 계층의 업데이트 경도에 따라 상기 공유 네트워크 계층의 파라미터에 대해 업데이트하는 단 계를 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 방법. 청구항 7 멀티태스크 모델의 파라미터 업데이트 장치에 있어서, 트레이닝 샘플 집합을 획득하는 획득 모듈 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포 함함; 상기 각 샘플이 소속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 분류 모듈; 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의 샘플을 사용하여 상기 멀티태스크 모델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하는 트레이닝 모듈; 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 업데이트 모듈을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 8 제7항에 있어서, 상기 분류 모듈은, 상기 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정하는 제1 결정 유닛; 및 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 상기 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는 분류 유닛을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 9 제8항에 있어서,상기 분류 유닛은, 임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 상기 복수의 태스크 태그 중의 각 태스크 태그에 대 응되는 샘플 대열에 상기 임의의 샘플이 모두 포함되는 것을 결정하는 제1 결정 서브 유닛을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 10 제7항에 있어서, 상기 업데이트 모듈은, 상기 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하는 제2 결정 유닛; 및 상기 생성된 모델 파라미터 업데이트 경도를 상기 타겟 파라미터 서버에 송신하여, 상기 타겟 파라미터 서버로 하여금, 상기 업데이트 경도에 따라 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하도록 하는 송신 유닛을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 11 제7항에 있어서, 상기 업데이트 모듈은, 상기 임의의 샘플 대열에 관련된 태스크의 가중치를 결정하는 제3 결정 유닛; 및 상기 가중치 및 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네 트워크 계층의 파라미터에 대해 업데이트하는 업데이트 유닛을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 12 제11항에 있어서, 상기 업데이트 유닛은, 상기 업데이트 경도에 따라 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 제1 업데이트 서브 유닛; 상기 가중치 및 상기 업데이트 경도에 따라 상기 공유 네트워크 계층의 업데이트 경도를 결정하는 제2 결정 서 브 유닛; 및 상기 공유 네트워크 계층의 업데이트 경도에 따라 상기 공유 네트워크 계층의 파라미터에 대해 업데이트하는 제 2 업데이트 서브 유닛을 포함하는 것, 을 특징으로 하는 멀티태스크 모델의 파라미터 업데이트 장치. 청구항 13 전자 기기에 있어서, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세서와 통신 가능하게 연결되는 메모리를 포함하고; 여기서, 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령은 상기 적 어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 제1항 내지 제6항 중 어느 한 항에 따 른 상기 방법을 수행하도록 하는 것, 을 특징으로 하는 전자 기기.청구항 14 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법을 수행하도록 하는 것, 을 특징으로 하는 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체. 청구항 15 컴퓨터 프로그램 제품에 있어서, 컴퓨터 프로그램을 포함하고, 상기 컴퓨터 프로그램이 프로세서에 의해 수행되는 경우 제1항 내지 제6항 중 어 느 한 항에 따른 상기 방법을 수행하도록 하는 것, 을 특징으로 하는 컴퓨터 프로그램 제품. 청구항 16 컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램에 있어서, 상기 컴퓨터 프로그램중의 명령이 실행될 경우, 제1항 내지 제6항 중 어느 한 항에 따른 상기 방법이 실행되는, 것을 특징으로 하는 컴퓨터 판독 가능 저장 매체에 저장되어 있는 컴퓨터 프로그램. 발명의 설명 기 술 분 야 본 출원은 컴퓨터 기술 분야에 관한 것으로, 특히 딥러닝 및 인공지능 기술 분야에 관한 것으로, 멀티태스크 모 델의 파라미터 업데이트 방법, 장치 및 전자 기기를 제공한다."}
{"patent_id": "10-2021-0109539", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "멀티태스크 러닝은 복수의 타겟 함수의 트레이닝 태스크를 최적화하는 것이며, 모니터링 러닝의 일종 형태이다. 멀티태스크 러닝의 핵심 문제와 난점은 복수의 서브 태스크의 트레이닝 데이터를 처리하고, 복수의 서브 태스크 의 최적화 러닝을 균형하게 하는 것이다. 관련 기술에서, 분산형 트레이닝하는 방식으로 멀티태스크 모델의 트레이닝을 구현할 수 있는 바, 즉 복수의 기 계를 통해 연합 트레이닝하여 기계의 데이터 용량 제한과 딥러닝의 성능 요구를 만족하도록 한다. 분산형 트레 이닝에서 가장 자주 쓰는 패턴이 파라미터 서버 패턴이다. 집합군 중의 노드가 컴퓨팅 노드 및 파라미터 서비스 노드 두가지로 나눌 수 있다. 컴퓨팅 노드는 로컬에 분배된 트레이닝 데이터(블록)에 대한 컴퓨팅 러닝을 책임 지고, 대응되는 파라미터를 업데이트하며; 파라미터 서비스 노드는 분산형 저장하는 방식을 채택하여 각자 전역 파라미터의 일 부분을 저장하여 서비스 쪽으로서 컴퓨팅 노드의 파라미터 조회 및 업데이트 청구를 접수한다. 본 출원은 멀티태스크 모델에 사용한 파라미터 업데이트 방법, 장치, 전자 기기, 저장 매체 및 컴퓨터 프로그램 제품을 제공한다. 본 출원의 일 측면에 따라 제공되는 멀티태스크 모델의 파라미터 업데이트 방법은, 트레이닝 샘플 집합을 획득 하는 단계 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함함; 상기 각 샘플이 소속된 태 스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 단계; 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의 샘플을 사용하여 상기 멀티태스크 모 델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하 는 단계; 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함한다. 본 출원의 다른 측면에 따라 제공되는 멀티태스크 모델의 파라미터 업데이트 장치는, 트레이닝 샘플 집합을 획 득하는 획득 모듈 - 상기 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함함; 상기 각 샘플이 소 속된 태스크에 따라 상기 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 분류 모듈; 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 상기 임의의 샘플 대열 중의 샘플을 사용하여 상기 멀 티태스크 모델 중의 공유 네트워크 계층 및 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트 워크 계층에 대해 트레이닝하여, 상기 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하는 트레이닝 모듈; 상기 업데이트 경도에 따라 파라미터 서버 중의 상기 공유 네트워크 계층 및 상기 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 업데이트 모듈을 포함한다. 본 출원의 또 다른 측면에 따라 제공되는 전자 기기는, 적어도 하나의 프로세서; 및 상기 적어도 하나의 프로세 서와 통신 가능하게 연결되는 메모리를 포함하고; 상기 메모리에는 상기 적어도 하나의 프로세서에 의해 수행 가능한 명령이 저장되어 있고, 상기 명령은 상기 적어도 하나의 프로세서에 의해 수행되어, 상기 적어도 하나의 프로세서가 상기 멀티태스크 모델의 파라미터 업데이트 방법을 수행하도록 한다. 본 출원의 또 다른 측면에 따라 제공되는 컴퓨터 명령이 저장되어 있는 비일시적 컴퓨터 판독 가능 저장 매체에 있어서, 상기 컴퓨터 명령은 상기 컴퓨터가 상기 멀티태스크 모델의 파라미터 업데이트 방법을 수행하도록 사용 된다. 본 출원의 또 다른 측면에 따라 제공되는 컴퓨터 프로그램 제품은 컴퓨터 프로그램을 포함하고, 상기 컴퓨터 프 로그램이 프로세서에 의해 수행되는 경우 상기 멀티태스크 모델의 파라미터 업데이트 방법을 수행하도록 한다. 본 출원의 또 다른 측면의 실시예에서 제공하는 컴퓨터 판독 가능 매체에 저장되어 있는 컴퓨터 프로그램은, 상 기 컴퓨터 프로그램중의 명령이 실행될 경우, 상기 멀티태스크 모델의 파라미터 업데이트 방법이 실행된다. 본 출원의 기술 방안에 따르면, 관련 기술에서 멀티태스크 모델의 분산형 트레이닝에서 하나의 태스크에 대해 트레이닝 한 번 수행한 다음 멀티태스크 모델 중의 모든 태스크의 모델 파라미터를 업데이트해야 하고 멀티태스 크 모델의 정밀도 및 성능을 저하시키는 문제를 해결하였다. 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크 에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이 터 요구에 도달한 상황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계 층 및 당해 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 업데이트 경 도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트한 다. 이리하여, 멀티태스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우 당 해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 기타 태스크에 대응되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없으므로, 멀티태스크 모델에 대해 분산형 트레이닝할 경우 국부 파라미터 업데이트를 구현 하여 멀티태스크 모델의 정밀도를 개선한다. 이해 가능한 바로는 본 부분에서 설명된 내용은 본 출원의 실시예의 핵심 또는 중요한 특징을 식별하기 위한 것 이 아니며, 본 출원의 범위를 한정하지도 않는다. 본 출원의 기타 특징들은 하기의 명세서에 의해 쉽게 이해될 것이다."}
{"patent_id": "10-2021-0109539", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 첨부된 도면을 결합하여 본 출원의 예시적인 실시예에 대해 설명하며, 여기에는 이해를 돕기 위해 본 출 원의 실시예의 다양한 세부 사항을 포함하므로, 이는 단지 예시적인 것으로 이해해야 한다. 따라서, 당업자는 본 출원의 범위 및 사상을 벗어나지 않는 한 여기에 설명된 실시예에 대해 다양한 변경 및 수정이 이루어질 수 있음을 인식해야 한다. 마찬가지로, 명확성과 간결성을 위해, 하기의 설명에서는 공지된 기능 및 구조에 대한 설명을 생략한다. 이하, 본 출원의 방안에 언급된 기술 분야에 대해 간단히 설명한다. 인공지능은 인간의 특정 사유 과정 및 지능 행위(예컨대, 러닝, 추리, 사고, 계획 등)를 컴퓨터로 시뮬레이션하 기 위해 연구하는 학과이며, 하드웨어 층면의 기술 뿐만 아니라 소프트웨어 층면의 기술도 포함한다. 인공지능 하드웨어 기술은 일반적으로 컴퓨터 시각 기술, 음성 인식 기술, 자연 언어 처리 기술 및 기계 러닝/딥러닝, 빅 데이터 처리 기술, 지식 그래프 기술 등 몇 가지 주요 방향을 포함한다. 딥러닝은 기계 러닝 분야의 새로운 연구 방향이며, 기계 러닝에 인입되어 기계 러닝을 최초의 타겟 - 인공지능 에 더 가까워지도록 한다. 딥러닝은 샘플 데이터의 내재적 법칙 및 표현 계층을 러닝하는 것이며, 이러한 러닝 과정에서 획득한 정보는 문자, 이미지 및 소리 등과 같은 데이터의 해석에 대해 큰 도움이 된다. 딥러닝의 최종 타겟은 기계로 하여금 인간과 같은 분석 러닝 능력을 구비할 수 있고, 문자, 이미지 및 소리 등과 같은 데이터 를 인식할 수 있도록 한다. 딥러닝은 검색 기술, 데이터 마이닝, 기계 러닝, 기계 번역, 자연 언어 처리, 멀티 미디어 러닝, 음성, 추천 및 개성화 기술, 및 기타 관련 분야에서 많은 성과를 획득하였다. 본 출원의 실시예는 관련 기술에서 멀티태스크 모델의 분산형 트레이닝에서, 하나의 태스크에 대해 트레이닝 한 번 수행한 다음 멀티태스크 모델 중의 모든 태스크의 모델 파라미터를 업데이트해야 하고, 멀티태스크 모델 의 정밀도 및 성능을 저하시키는 문제에 대해, 멀티태스크 모델의 파라미터 업데이트 방법을 제출하였다. 이하, 첨부된 도면을 참조하여 본 출원에서 제공되는 멀티태스크 모델의 파라미터 업데이트 방법, 장치, 전자 기기, 저장 매체 및 컴퓨터 프로그램 제품에 대해 상세히 설명한다. 도1은 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이다. 도1에 도시된 바와 같이, 당해 멀티태스크 모델의 파라미터 업데이트 방법은 하기의 단계를 포함한다. 단계101, 트레이닝 샘플 집합을 획득하고, 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함한다. 설명해야 하는 바로는, 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 방법은 본 출원의 실시 예에 따른 멀티태스크 모델의 파라미터 업데이트 장치에 의해 수행될 수 있으며, 본 출원의 실시예에 따른 멀티 태스크 모델의 파라미터 업데이트 장치는 임의의 전자 기기에 구성될 수 있고, 본 출원의 실시예에 따른 멀티태 스크 모델의 파라미터 업데이트 방법을 수행하도록 한다. 본 출원의 실시예에서, 본 출원의 실시예에 따른 멀티태스크 모델 트레이닝 방법은 복수의 기능을 동시에 구현 하기 위한 임의의 모델 트레이닝 장면에 응용될 수 있으며, 예를 들어, 정보 스트림 추천, 광고 추천, 검색 추 천 등 응용 장면에 응용될 수 있다. 하나의 예시로서, 본 출원의 실시예에 따른 멀티태스크 모델 트레이닝 방법은 광고 추천 장면에 응용될 수 있으 므로, 복수의 서브 태스크를 구비한 광고 추천 모델을 트레이닝하도록 한다. 예를 들어, 광고 추천 모델은 광고 의 조회수 및 전환율을 예측하도록 사용될 수 있으며, 즉 광고 추천 모델은 두개의 서브 태스크 - 광고의 조회 수 및 전환율- 을 포함한다. 트레이닝 샘플 집합은 멀티태스크 모델의 실제 응용 장면 중의 과거 사용 데이터에 따라 생성될 수 있다. 설명해야 하는 바로는, 지도형 트레이닝 장면에서 각 샘플은 트레이닝 데이터 및 트레이닝 데이터에 대한 라벨 데이터를 포함할 수 있고; 비지도형 트레이닝 장면에서 각 샘플은 트레이닝 데이터 만을 포함할 수 있다. 예를 들어 설명하면, 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 방법이 광고 추천 장면에 응용될 경우, 즉 멀티태스크 모델은 광고 추천 모델일 수 있고, 광고 추천 모델은 광고 추천 기능을 구비한 응 용 장면에 응용될 수 있으므로, 광고 추천 모델이 응용한 응용 프로그램의 과거 사용 데이터로부터 트레이닝 샘 플 집합을 획득할 수 있다. 예를 들어, 비지도형 트레이닝 장면에서, 응용 프로그램 중 하나의 광고의 광고 유 형, 시간 길이, 태그 등 특징 데이터를 하나의 샘플로 사용할 수 있고; 지도형 트레이닝 장면에서, 응용 프로그 램 중 하나의 광고의 광고 유형, 시간 길이, 태그 등 특징 데이터를 하나의 트레이닝 데이터로 사용할 수 있고, 또한 사용자의 당해 광고에 대한 조회수, 관람 시간 길이, 좋아요수, 배포수, 공유수, 전환율 등 사용자 조작 데이터를 당해 트레이닝 데이터에 대응되는 라벨 데이터로 사용할 수 있고, 더 나아가 당해 트레이닝 데이터 및당해 트레이닝 데이터에 대응되는 라벨 데이터를 하나의 샘플로 사용할 수 있다. 더 나아가 상기 방식에 따라 응용 프로그램 중의 대량 광고의 과거 사용 데이터에 따라 대량 트레이닝 샘플을 포함하는 트레이닝 샘플 집합 을 생성한다. 본 출원의 실시예에서, 트레이닝 샘플 집합을 획득할 때, 또한 각 샘플이 멀티태스크 모델 중의 어느 태스크를 트레이닝하는지를 라벨할 수 있다. 즉, 트레이닝 샘플 집합에는 또한 각 샘플이 소속된 태스크를 포함할 수 있 다. 단계102, 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는다. 본 출원의 실시예에서, 멀티태스크 모델 중의 복수의 태스크에 대한 트레이닝 타겟이 다르기 때문에, 각 태스크 를 트레이닝하기 위해 사용된 샘플이 또한 다를 수 있으며, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크 에 따라 트레이닝 샘플 집합을 분류할 수 있으므로, 동일한 태스크에 소속된 샘플을 같은 샘플 대열에 넣는다. 따라서, 하나의 샘플 대열 중의 샘플을 사용하여 멀티태스크 모델 중의 하나의 태스크에 대해 단독으로 트레이 닝할 수 있다. 가능한 구현 방식으로서, 또한, 멀티태스크 모델 트레이닝 과정에서 트레이닝 샘플 집합을 실시간으로 획득하여, 각 샘플을 획득할 때 마다 당해 샘플이 소속된 태스크에 따라 당해 샘플을 해당되는 샘플 대열에 넣 는다. 단계103, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 임의의 샘플 대열 중 의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 임의의 샘플 대열에 관련된 태스크에 대응 되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라 미터 업데이트 경도를 생성한다. 공유 네트워크 계층은 멀티태스크 모델의 각 태스크에 모두 사용되는 네트워크 계층을 의미한다. 서브 네트워크 계층은 멀티태스크 모델 중의 임의의 태스크에 단독으로 사용되는 네트워크 계층을 의미한다. 타겟 서브 네트워 크 계층은 현재 트레이닝할 태스크에 단독으로 사용되는 서브 네트워크 계층을 의미한다. 본 출원의 실시예에서, 샘플 대열 중의 샘플 수량이 일정한 수량에 도달해야 태스크에 대한 트레이닝 효과를 확 보할 수 있으므로, 배치 데이터의 수량(즉, 트레이닝 데이터 요구)을 미리 설정할 수 있다. 따라서, 트레이닝 데이터 집합 중의 샘플을 해당되는 샘플 대열에 넣는 과정에서 각 샘플 대열 중의 샘플 수량을 실시간으로 결정 할 수 있고, 임의의 샘플 대열 중의 샘플 수량이 미리 설정된 배치 데이터의 수량에 도달할 경우 당해 샘플 대 열 중의 각 샘플을 멀티태스크 모델에 순차적으로 입력하므로, 당해 샘플 대열 중의 샘플을 사용하여 멀티태스 크 모델 중의 공유 네트워크 계층에 대해 트레이닝하고, 그리고 당해 샘플 대열에 관련된 태스크에 대응되는 타 겟 서브 네트워크 계층에 대해 트레이닝하여 당해 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이 트 경도를 결정한다. 단계104, 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미 터에 대해 업데이트한다. 파라미터 서버는 멀티태스크 모델의 모델 파라미터를 저장하는 서버를 의미하고, 저장된 모델 파라미터에 대해 업데이트할 수 있도록 컴퓨팅 노드의 파라미터 조회 및 업데이트 청구를 접수할 수 있다. 본 출원의 실시예에서, 태스크와 관련된 샘플 대열 중의 샘플을 사용하여 멀티태스크 모델 중의 당해 태스크에 대해 트레이닝 한 라운드 수행한 다음, 생성된 모델 파라미터 업데이트 경도를 파라미터 서버에 송신할 수 있으 므로, 파라미터 서버로 하여금 생성된 모델 파라미터 업데이트 경도에 포함된 타겟 서브 네트워크 계층에 대한 업데이트 경도에 따라 타겟 서브 네트워크 계층의 모델 파라미터에 대해 업데이트하고, 그리고 생성된 모델 파 라미터 업데이트 경도에 포함된 공유 네트워크 계층에 대한 업데이트 경도에 따라 공유 네트워크 계층의 모델 파라미터에 대해 업데이트할 수 있다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상 황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대 열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘플 대열에 관 련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트한다. 이리하여, 멀티태스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우 당해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 기타 태스크에 대응되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없 으므로, 멀티태스크 모델에 대해 분산형 트레이닝할 경우 국부 파라미터 업데이트를 구현하여 멀티태스크 모델 의 정밀도를 개선한다. 본 출원의 가능한 구현 형태에서, 태그 기술을 통해 각 샘플이 소속된 태스크에 대해 라벨링할 수 있으므로, 트 레이닝 샘플 집합에 대해 분류할 수 있고 멀티태스크 모델의 트레이닝 효과를 더 개선할 수 있다. 이하, 도2를 결합하여 본 출원의 실시예에서 제공되는 멀티태스크 모델의 파라미터 업데이트 방법에 대해 진일 보 설명한다. 도2는 본 출원의 실시예에 따른 다른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이다. 도2에 도시된 바와 같이, 당해 멀티태스크 모델의 파라미터 업데이트 방법은 하기의 단계를 포함한다. 단계201, 트레이닝 샘플 집합을 획득하고, 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함한다. 상기 단계201의 구체적인 구현 과정 및 원리는 상기 실시예의 상세 설명을 참조할 수 있으므로, 여기서는 더 이 상 설명하지 않는다. 단계202, 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정한다. 가능한 구현 방식으로서, 각 샘플에 하나의 속성 도메인을 첨가할 수 있고, 그 다음 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정할 수 있고, 더 나아가 각 샘플에 대응되는 태스크 태그에 따라 각 샘플의 속성 도메인의 값을 결정하여, 속성 도메인의 값을 통해 샘플의 태스크 태그를 표시할 수 있다. 예를 들어 설명하면, 멀티태스크 모델은 광고 추천 모델이고, 광고 추천 모델은 두개의 태스크를 포함하는 바, 태스크 1은 예측 광고 조회수이고, 태스크 2는 예측 광고 전환율이다. 샘플 1이 소속된 태스크가 태스크 1인 경 우, 샘플 1에 대응되는 태스크 태그를 1로 결정할 수 있고, 샘플 1의 속성 도메인의 값을 [1]로 결정하며; 샘플 2가 소속된 태스크가 태스크 2인 경우, 샘플 2에 대응되는 태스크 태그를 2로 결정할 수 있고, 샘플 2의 속성 도메인의 값을 [2]로 결정하며; 샘플 3이 소속된 태스크가 태스크 1과 태스크 2인 경우, 샘플 3에 대응되는 태 스크 태그를 1과 2로 결정할 수 있고, 샘플 3의 속성 도메인의 값을 [1, 2]로 결정하며; 이리하여 트레이닝 샘 플 집합 중의 모든 샘플의 태스크 태그를 결정한다. 단계203, 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는다. 본 출원의 실시예에서, 각 샘플에 대응되는 태스크 태그를 결정한 다음, 같은 태스크 태그를 구비한 샘플을 동 일한 샘플 대열에 분류할 수 있다. 하나의 예시로서, 샘플에 대응되는 태스크 태그는 하나 또는 복수일 수 있으며, 샘플에 대응되는 태스크 태그의 수량에 따라 트레이닝 샘플 집합에 대해 분류할 수 있으므로 각 샘플에 대응되는 샘플 대열을 결정한다. 상황 1 임의의 샘플이 하나의 태스크 태그에 대응되는 상황하에서, 당해 태스크 태그에 대응되는 샘플 대열에 임의의 샘플이 모두 포함되는 것을 결정한다. 예를 들어, 샘플 1에 대응되는 태스크 태그가 1인 경우, 샘플 1을 태스크 태그 1에 대응되는 샘플 대열에 넣을 수 있고, 태스크 태그 1에 대응되는 샘플 대열에는 태스크 1을 트레이닝하 기 위한 모든 샘플을 포함한다. 상황 2 임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 복수의 태스크 태그 중의 각 태스크 태그에 대응되 는 샘플 대열에 임의의 샘플이 모두 포함되는 것을 결정한다. 예를 들어 설명하면, 샘플 2에 대응되는 태스크 태그가 1과 2인 경우, 샘플 2를 태스크 태그 1에 대응되는 샘플 대열에 넣을 수도 있고, 샘플 2를 태스크 태그 2에 대응되는 샘플 대열에 넣을 수도 있으므로, 샘플 2는 태스크 1을 트레이닝할 수도 있고, 태스크 2를 트레이닝할 수도 있다. 단계204, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 임의의 샘플 대열 중 의 샘플을 사용하여, 멀티태스크 모델 중의 공유 네트워크 계층 및 임의의 샘플 대열에 관련된 태스크에 대응되 는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미 터 업데이트 경도를 생성한다. 상기 단계204의 구체적인 구현 과정 및 원리는 상기 실시예의 상세 설명을 참조할 수 있으므로, 여기서는 더 이 상 설명하지 않는다. 단계205, 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정한다. 본 출원의 실시예에서, 서로 다른 태스크에 대응되는 모델 파라미터는 서로 다른 파라미터 서버를 사용하여 저 장할 수 있으므로, 멀티태스크 모델의 국부 파라미터 업데이트에 편리하다. 따라서, 각 파라미터 서버에 저장된 모델 파라미터에 관련된 태스크에 따라 각 파라미터 서버에 대응되는 태스크 태그를 결정할 수 있으므로, 태스 크 태그와 파라미터 서버의 대응관계를 구축하고, 각 파라미터 서버에 저장된 모델 파라미터가 소속된 태스크에 대해 표시한다. 따라서, 샘플 대열 중의 샘플을 사용하여 멀티태스크 모델 중의 임의의 태스크에 대해 트레이닝 한 라운드 수행하여 모델 파라미터 업데이트 경도를 생성한 다음, 샘플 대열에 대응되는 태스크 태그 및 태스크 태그와 파라미터 서버의 대응관계에 따라 당해 태스크 태그에 대응되는 타겟 파라미터 서버를 결정할 수 있다. 단계206, 생성된 모델 파라미터 업데이트 경도를 타겟 파라미터 서버에 송신하여, 타겟 파라미터 서버로 하여금, 업데이트 경도에 따라 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하 도록 한다. 본 출원의 실시예에서, 현재 트레이닝하는 태스크에 대응되는 타겟 파라미터를 저장하는 서버를 결정한 다음, 당해 태스크에 대응되는 모델 파라미터 업데이트 경도를 타겟 파라미터 서버에 송신할 수 있으므로, 타겟 파라 미터 서버로 하여금 공유 네트워크 계층에 대한 업데이트 경도에 따라 저장된 공유 네트워크 계층의 모델 파라 미터에 대해 업데이트하고, 그리고 타겟 서브 네트워크 계층에 대한 업데이트 경도에 따라 저장된 타겟 서브 네 트워크 계층의 모델 파라미터에 대해 업데이트한다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정할 수 있고, 그리고 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 태스크 태 그에 대응되는 샘플 대열에 순차적으로 넣고, 그 다음, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요 구에 도달한 상황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘 플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하고, 생성된 모델 파라미터 업데이트 경도를 타겟 파라 미터 서버에 송신하여, 타겟 파라미터 서버로 하여금 업데이트 경도에 따라 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하도록 한다. 이리하여, 태그 기술에서 각 샘플이 소속된 태스크에 대해 라벨링함을 통해, 트레이닝 샘플 집합에 대해 분류하기 편리하고, 서로 다른 파라미터 서버를 통해 서로 다른 태스크에 대응되는 모델 파라미터를 저장하고, 더 나아가 멀티태스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우, 당해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하여, 기타 태스크에 대응 되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없으므로, 멀티태스크 모델에 대해 분산형 트레이닝 할 경우 국부 파라미터 업데이트를 구현하여 멀티태스크 모델의 정밀도를 개선할 뿐만 아니라, 국부 파라미터 업데이트의 간편성도 진일보 개선한다. 본 출원의 가능한 구현 형태에서, 또한 멀티태스크 모델 중의 각 태스크의 가중치에 따라 공유 네트워크 계층의 모델 파라미터에 대한 각 태스크의 업데이트 폭을 제약할 수 있으므로, 멀티태스크의 태스크 편향성을 낮추고, 멀티태스크 모델의 정밀도를 진일보 개선한다. 이하, 도3을 결합하여 본 출원의 실시예에서 제공되는 멀티태스크 모델의 파라미터 업데이트 방법에 대해 진일 보 설명한다. 도3은 본 출원의 실시예에 따른 또 다른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이다. 도3에 도시된 바와 같이, 당해 멀티태스크 모델의 파라미터 업데이트 방법은 하기의 단계를 포함한다. 단계301, 트레이닝 샘플 집합을 획득하고, 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포함한다. 단계302, 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는다. 단계303, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 임의의 샘플 대열 중 의 샘플을 사용하여, 멀티태스크 모델 중의 공유 네트워크 계층 및 임의의 샘플 대열에 관련된 태스크에 대응되 는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미 터 업데이트 경도를 생성한다. 상기 단계301-303의 구체적인 구현 과정 및 원리는 상기 실시예의 상세 설명을 참조할 수 있으므로, 여기서는 더 이상 설명하지 않는다. 단계304, 임의의 샘플 대열에 관련된 태스크의 가중치를 결정한다. 본 출원의 실시예에서, 태스크에 대해 트레이닝하는 횟수가 많을수록 당해 태스크에 대응되는 샘플이 멀티태스 크 모델에 대한 최적화 효과가 좋고, 따라서 태스크에 대한 트레이닝하는 횟수, 즉 태스크와 관련된 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 횟수에 따라 각 태스크의 가중치를 결정할 수 있다. 가능한 구현 방식으로서, 샘플 대열 중의 샘플을 사용하여 멀티태스크 모델 중의 공유 네트워크 계층에 대해 트 레이닝하고, 당해 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하여, 당해 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성한 다음, 당해 태스크에 관련된 샘플 대열 중의 샘플 수 량이 트레이닝 데이터 요구에 도달한 횟수를 결정할 수 있고, 또한 당해 횟수를 당해 태스크의 가중치로 결정한 다. 예를 들어 설명하면, 태스크 1과 관련된 샘플 대열 중의 샘플 수량이 처음으로 트레이닝 데이터 요구에 도달할 경우, 태스크 1의 가중치를 1로 결정할 수 있고; 태스크 1과 관련된 샘플 대열 중의 샘플 수량이 두 번째로 트 레이닝 데이터 요구에 도달할 경우, 태스크 1의 가중치를 2로 결정할 수 있다. 단계305, 가중치 및 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층 의 파라미터에 대해 업데이트한다. 본 출원의 실시예에서, 현재 트레이닝하는 태스크의 가중치를 결정한 다음, 당해 태스크의 가중치와 생성된 당 해 태스크에 대응되는 모델 파라미터 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층의 모델 파 라미터에 대해 업데이트할 수 있고, 또한 태스크에 대응되는 타겟 서브 네트워크 계층의 모델 파라미터에 대해 업데이트할 수 있다. 가능한 구현 방식으로서, 멀티태스크 모델 중의 공유 네트워크 계층이 모든 태스크에 대응되는 샘플 대열 중의 샘플에 대해 처리할 수 있으므로, 멀티태스크 모델 중의 모든 태스크의 성능에 영향을 미칠 수 있고, 하지만 각 태스크에 대응되는 서브 네트워크 계층이 당해 태스크에 대응되는 샘플 대열 중의 샘플 만에 대해 처리하므로 멀티태스크 모델 중의 기타 태스크의 샘플 및 성능과는 무관하다. 따라서, 공유 네트워크 계층의 파라미터를 업 데이트할 때만 태스크의 가중치를 인입할 수 있다. 즉, 본 출원의 실시예의 가능한 구현 방식에서, 상기 단계 305는, 업데이트 경도에 따라 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 단계; 가중치 및 업데이트 경도에 따라 공유 네트워크 계층의 업데이트 경도를 결정하는 단계; 공유 네트워크 계층의 업데이트 경도에 따라 공유 네트워크 계층의 파라미터에 대해 업데이트하는 단계를 포함 할 수 있다. 본 출원의 실시예에서, 각 태스크에 대응되는 서브 네트워크 계층이 당해 태스크에 대응되는 샘플 대열 중의 샘 플 만에 대해 처리하므로 멀티태스크 모델 중의 기타 태스크의 샘플 및 성능과는 무관하고, 따라서, 샘플 대열 을 통해 생성된 모델 파라미터 업데이트 경도에 따라 해당 태스크에 대응되는 서브 네트워크 계층 중의 파라미 터에 대해 직접 업데이트할 수 있으므로, 타겟 서브 네트워크 계층의 파라미터 업데이트 과정을 해당 태스크에 대응되는 샘플 대열 만과 관련하여, 기타 태스크에 대응되는 샘플에 의존하지 않도록 하고, 멀티태스크 모델 트 레이닝의 태스크 편향성을 낮추고, 각 태스크의 트레이닝 정밀도를 개선한다. 본 출원의 실시예에서, 멀티태스크 모델 중의 공유 네트워크 계층이 모든 태스크에 대응되는 샘플 대열 중의 샘 플에 대해 처리할 수 있으므로, 멀티태스크 모델 중의 모든 태스크의 성능에 영향을 미칠 수 있다. 따라서, 각 태스크에 대응되는 샘플 대열을 통해 생성된 모델 파라미터 업데이트 경도에 따라 멀티태스크 모델 중의 공유 네트워크 계층의 모델 파라미터에 대해 공동으로 업데이트할 수 있으므로, 공유 네트워크 계층이 각 태스크에서상대적으로 우수한 처리 성능을 구비할 수 있도록 한다. 본 출원의 실시예의 가능한 구현 방식에서, 태스크의 가중치는 당해 태스크에 대응되는 샘플 대열을 통해 생성 된 경도가 멀티태스크 모델 트레이닝 과정에서 모델 파라미터 업데이트 수행할 때의 중요성을 반영할 수 있으므 로, 모델 파라미터 업데이트 경도 및 각 태스크의 가중치에 따라 공유 네트워크 계층의 업데이트 경도를 결정할 수 있고, 공유 네트워크 계층의 업데이트 경도에 따라 공유 네트워크 계층 중의 모델 파라미터를 업데이트할 때 태스크의 가중치를 통해 각 태스크의 샘플 대열이 공유 네트워크 계층에 대한 최적화 효과를 균형있게 하고, 공 유 네트워크 계층이 각 태스크에 대해 처리를 수행할 때 모두 상대적으로 높은 정밀도를 구비할 수 있도록 확보 한다. 선택적으로, 모델 파라미터에 대한 태스크의 업데이트 경도와 태스크의 가중치의 승적을 공유 네트워크 계층의 업데이트 경도로 결정할 수 있고, 그리고 공유 네트워크 계층의 업데이트 경도를 파라미터 서버에 송신하여, 파 라미터 서버로 하여금 공유 네트워크 계층의 업데이트 경도에 따라 공유 네트워크 계층의 파라미터에 대해 업데 이트할 수 있도록 한다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상 황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대 열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘플 대열에 관 련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 태스크의 가중치 및 업데이트 경 도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트한 다. 이리하여, 멀티태스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우 당 해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 기타 태스크에 대응되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없으므로, 멀티태스크 모델 중의 각 태스크의 가중치에 따라 공유 네트워크 계층의 모델 파 라미터에 대한 각 태스크의 업데이트 폭을 제약하여, 멀티태스크 모델에 대해 분산형 트레이닝할 경우 국부 파 라미터 업데이트를 구현할 뿐만 아니라, 멀티태스크의 태스크 편향성을 낮추고, 멀티태스크 모델의 정밀도를 진 일보 개선한다. 상기 실시예를 구현하기 위해, 본 출원은 또한 멀티태스크 모델의 파라미터 업데이트 장치를 제공한다. 도4는 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 장치의 구조 개략도이다. 도4에 도시된 바와 같이, 당해 멀티태스크 모델의 파라미터 업데이트 장치는, 트레이닝 샘플 집합을 획득하는 획득 모듈 - 샘플 집합에는 복수의 샘플 및 각 샘플이 소속된 태스크를 포 함함; 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣는 분류 모듈; 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상황하에서, 임의의 샘플 대열 중의 샘플을 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 임의의 샘플 대열에 관련된 태스크에 대응되는 타겟 서 브 네트워크 계층에 대해 트레이닝하여, 임의의 샘플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하는 트레이닝 모듈; 및 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 업데이트 모듈을 포함한다. 실제 사용시, 본 출원의 실시예에서 제공되는 멀티태스크 모델의 파라미터 업데이트 장치는 전술한 멀티태스크 모델의 파라미터 업데이트 방법을 수행하도록 임의의 전자 기기에 구성될 수 있다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상 황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대 열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘플 대열에 관 련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트한다. 이리하여, 멀티태 스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우 당해 태스크에 대응되는샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 기타 태스크에 대응되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없 으므로, 멀티태스크 모델에 대해 분산형 트레이닝할 경우 국부 파라미터 업데이트를 구현하여 멀티태스크 모델 의 정밀도를 개선한다. 본 출원의 가능한 구현 형태에서, 상기 분류 모듈은, 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정하는 제1 결정 유닛; 및 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 태스크 태그에 대응되는 샘플 대열에 순차적으로 넣는 분류 유닛을 포함한다. 더 나아가, 본 출원의 다른 가능한 구현 형태에서, 상기 분류 유닛은, 임의의 샘플이 복수의 태스크 태그에 대응되는 상황하에서, 복수의 태스크 태그 중의 각 태스크 태그에 대응되 는 샘플 대열에 임의의 샘플이 모두 포함되는 것을 결정하는 제1 결정 서브 유닛을 포함한다. 더 나아가, 본 출원의 또 다른 가능한 구현 형태에서, 상기 업데이트 모듈은, 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하는 제2 결정 유닛; 및 생성된 모델 파라미터 업데이트 경도를 타겟 파라미터 서버에 송신하여, 타겟 파라미터 서버로 하여금, 업데이 트 경도에 따라 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 송신 유닛을 포함한다. 더 나아가, 본 출원의 또 다른 가능한 구현 형태에서, 상기 업데이트 모듈은, 임의의 샘플 대열에 관련된 태스크의 가중치를 결정하는 제3 결정 유닛; 및 가중치 및 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미 터에 대해 업데이트하는 업데이트 유닛을 포함한다. 더 나아가, 본 출원의 또 다른 가능한 구현 형태에서, 상기 업데이트 유닛은, 업데이트 경도에 따라 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하는 제1 업데이트 서브 유닛; 가중치 및 업데이트 경도에 따라 공유 네트워크 계층의 업데이트 경도를 결정하는 제2 결정 서브 유닛; 및 공유 네트워크 계층의 업데이트 경도에 따라 공유 네트워크 계층의 파라미터에 대해 업데이트하는 제2 업데이트 서브 유닛을 포함한다. 설명해야 하는 바로는, 상기 도1, 도2, 도3에 도시된 멀티태스크 모델의 파라미터 업데이트 방법 실시예에 대한 해석과 설명은 당해 실시예의 멀티태스크 모델의 파라미터 업데이트 장치에도 적용되어, 여기서는 더 이상 설명하지 않는다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플에 대응되는 태스크 태그를 결정할 수 있고, 그리고 각 샘플에 대응되는 태스크 태그에 따라 각 샘플을 태스크 태 그에 대응되는 샘플 대열에 순차적으로 넣고, 그 다음, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요 구에 도달한 상황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘 플 대열에 관련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 임의의 샘플 대열에 관련된 태스크 태그에 따라 타겟 파라미터 서버를 결정하고, 생성된 모델 파라미터 업데이트 경도를 타겟 파라 미터 서버에 송신하여, 타겟 파라미터 서버로 하여금 업데이트 경도에 따라 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트하도록 한다. 이리하여, 태그 기술에서 각 샘플이 소속된 태스크에 대해 라벨링함을 통해, 트레이닝 샘플 집합에 대해 분류하기 편리하고, 서로 다른 파라미터 서버를 통해 서로 다른 태스크에 대응되는 모델 파라미터를 저장하고, 더 나아가 멀티태스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우, 당해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하여, 기타 태스크에 대응 되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없으므로, 멀티태스크 모델에 대해 분산형 트레이닝 할 경우 국부 파라미터 업데이트를 구현하여 멀티태스크 모델의 정밀도를 개선할 뿐만 아니라, 국부 파라미터업데이트의 간편성도 진일보 개선한다. 본 출원의 실시예에 따라 본 출원은 또한 전자 기기, 판독 가능 저장 매체 및 컴퓨터 프로그램 제품을 제공한다. 본 출원의 실시예에 따르면, 컴퓨터 판독 가능 매체에 저장되어 있는 컴퓨터 프로그램을 더 제공한다. 당해 컴퓨터 프로그램중의 명령이 실행될 경우, 상기 멀티태스크 모델의 파라미터 업데이트 방법이 실행된다. 도5는 본 출원의 실시예를 실시하기 위한 예시적인 전자 기기의 개략적 블록도이다. 전자 기기는 랩톱 컴 퓨터, 데스크톱 컴퓨터, 워크 스테이션, 개인용 디지털 비서, 서버, 블레이드 서버, 메인 프레임 컴퓨터 및 기 타 적합한 컴퓨터와 같은 다양한 형태의 디지털 컴퓨터를 나타내기 위한 것이다. 전자 기기는 또한 개인용 디지 털 처리, 셀룰러 폰, 스마트 폰, 웨어러블 기기 및 기타 유사한 컴퓨팅 장치와 같은 다양한 형태의 모바일 장치 를 나타낼 수도 있다. 본 명세서에서 제시된 구성 요소, 이들의 연결 및 관계, 또한 이들의 기능은 단지 예일 뿐이며 본문에서 설명되거나 및/또는 요구되는 본 출원의 구현을 제한하려는 의도가 아니다. 도5에 도시된 바와 같이, 기기는 컴퓨팅 유닛을 포함하며, 읽기 전용 메모리(ROM)에 저장된 컴 퓨터 프로그램에 의해 또는 저장 유닛로부터 랜덤 액세스 메모리(RAM)에 로딩된 컴퓨터 프로그램에 의해 수행되어 각종 적절한 동작 및 처리를 수행할 수 있다. RAM에, 또한 기기가 오퍼레이션을 수행 하기 위해 필요한 각종 프로그램 및 데이터가 저장되어 있다. 컴퓨팅 유닛, ROM 및 RAM는 버스 를 통해 서로 연결되어 있다. 입력/출력(I/O) 인터페이스도 버스에 연결되어 있다. 키보드, 마우스 등과 같은 입력 유닛; 각종 유형의 모니터, 스피커 등과 같은 출력 유닛; 자기 디스 크, 광 디스크 등과 같은 저장 유닛; 및 네트워크 카드, 모뎀, 무선 통신 트랜시버 등과 같은 통신 유닛 을 포함하는 기기 중의 복수의 부품이 I/O 인터페이스에 연결된다. 통신 유닛은 장치(50 0)가 인터넷과 같은 컴퓨터 네트워크 및/또는 다양한 통신 네트워크를 통해 다른 기기와 정보/데이터를 교환하 도록 허락한다. 컴퓨팅 유닛은 프로세싱 및 컴퓨팅 능력을 구비한 다양한 범용 및/또는 전용 프로세싱 컴포넌트일 수 있다. 컴퓨팅 유닛의 일부 예시는 중앙 처리 유닛(CPU), 그래픽 처리 유닛(GPU), 다양한 전용 인공 지능 (AI) 컴퓨팅 칩, 기계 러닝 모델 알고리즘을 수행하는 다양한 컴퓨팅 유닛, 디지털 신호 처리기(DSP), 및 임의 의 적절한 프로세서, 컨트롤러, 마이크로 컨트롤러 등을 포함하지만, 이에 제한되지 않는다. 컴퓨팅 유닛 은 예를 들어 멀티태스크 모델의 파라미터 업데이트 방법과 같은 윗글에서 설명한 각각의 방법 및 처리를 수행 한다. 예를 들어, 일부 실시예에서, 멀티태스크 모델의 파라미터 업데이트 방법은 저장 유닛과 같은 기계 판독 가능 매체에 유형적으로 포함되어 있는 컴퓨터 소프트웨어 프로그램으로 구현될 수 있다. 일부 실시예에서, 컴퓨터 프로그램의 일부 또는 전부는 ROM 및/또는 통신 유닛을 통해 기기에 로드 및/또는 설치될 수 있다. 컴퓨터 프로그램이 RAM에 로딩되고 컴퓨팅 유닛에 의해 수행되는 경우, 전 술한 멀티태스크 모델의 파라미터 업데이트 방법의 하나 또는 하나 이상의 단계를 수행할 수 있다. 대안적으로, 다른 실시예에서, 컴퓨팅 유닛은 임의의 다른 적절한 방식(예를 들어, 펌웨어에 의해)을 통해 구성되어 멀 티태스크 모델의 파라미터 업데이트 방법을 수행하도록 한다. 여기서 설명되는 시스템 및 기술의 다양한 실시 방식은 디지털 전자 회로 시스템, 집적 회로 시스템, 필드 프로 그래머블 게이트 어레이(FPGA), 주문형 집적 회로(ASIC), 특정 용도 표준 제품(ASSP), 시스템온칩(SOC), 복합 프로그래머블 논리 소자(CPLD), 컴퓨터 하드웨어, 펌웨어, 소프트웨어 및 이들의 조합 중의 적어도 하나로 구현 될 수 있다. 이러한 다양한 실시 방식은 하나 또는 하나 이상의 컴퓨터 프로그램에서의 구현을 포함할 수 있으 며, 당해 하나 또는 하나 이상의 컴퓨터 프로그램은 적어도 하나의 프로그램 가능 프로세서를 포함하는 프로그 램 가능 시스템에서 수행 및/또는 해석될 수있고, 당해 프로그램 가능 프로세서는 전용 또는 일반용일 수 있고, 저장 시스템, 적어도 하나의 입력 장치 및 적어도 하나의 출력 장치로부터 데이터 및 명령을 수신하고 또한 데 이터 및 명령을 당해 저장 시스템, 당해 적어도 하나의 입력 장치 및 당해 적어도 하나의 출력 장치에 전송할 수 있다. 본 출원의 방법을 구현하기 위해 사용되는 프로그램 코드는 하나 또는 하나 이상의 프로그래밍 언어의 임의의 조합으로 작성될 수 있다. 이러한 프로그램 코드는 범용 컴퓨터, 전용 컴퓨터 또는 기타 프로그래머블 데이터 처리 장치의 프로세서 또는 컨트롤러에 제공될 수 있으므로, 프로그램 코드가 프로세서 또는 컨트롤러에 의해 수행되는 경우, 흐름도 및/또는 블록도에서 규정한 기능/조작을 구현하도록 한다. 프로그램 코드는 전체적으로 기계에서 수행되거나, 부분적으로 기계에서 수행되거나, 독립 소프트웨어 패키지로서 부분적으로 기계에서 수행 되고 부분적으로 원격 기계에서 수행되거나 또는 전체적으로 원격 기계 또는 서버에서 수행될 수 있다.본 출원의 문맥에서, 기계 판독 가능 매체는 명령 수행 시스템, 장치 또는 기기에 의해 사용되거나 명령 수행 시스템, 장치 또는 기기와 결합하여 사용되는 프로그램을 포함하거나 저장할 수 있는 유형의 매체일 수 있다. 기계 판독 가능 매체는 기계 판독 가능 신호 매체 또는 기계 판독 가능 저장 매체일 수 있다. 기계 판독 가능 매체는 전자, 자기, 광학, 전자기, 적외선 또는 반도체 시스템, 장치 또는 기기, 또는 상기 내용의 임의의 적절 한 조합을 포함할 수 있지만 이에 제한되지 않는다. 기계 판독 가능 저장 매체의 더 구체적인 예시는 하나 또는 하나 이상의 전선을 기반한 전기 연결, 휴대용 컴퓨터 디스크, 하드 디스크, 랜덤 액세스 메모리(RAM), 읽기 전 용 메모리(ROM), 지울 수 있는 프로그래머블 읽기 전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 휴대용 컴 팩트 디스크 읽기 전용 메모리(CD-ROM), 광학 저장 기기, 자기 저장 기기 또는 상기 내용의 임의의 적절한 조합 을 포함할 수 있지만 이에 제한되지 않는다. 사용자와의 인터랙션을 제공하기 위해 여기에 설명된 시스템 및 기술은 컴퓨터에서 실시될 수 있다. 당해 컴퓨 터는 사용자에게 정보를 디스플레이하기 위한 디스플레이 장치(예를 들어, CRT(음극선관) 또는 LCD(액정 디스플 레이) 모니터); 및 키보드 및 포인팅 장치(예를 들어, 마우스 또는 트랙볼)를 구비하며, 사용자는 당해 키보드 및 당해 포인팅 장치를 통해 컴퓨터에 입력을 제공할 수 있다. 다른 유형의 장치를 사용하여 사용자와의 인터랙 션을 제공할 수도 있으며, 예를 들어, 사용자에게 제공되는 피드백은 임의의 형태의 감지 피드백(예를 들어, 시 각적 피드백, 청각적 피드백 또는 촉각적 피드백)일 수 있고; 임의의 형태(소리 입력, 음성 입력 또는 촉각 입 력을 포함)로 사용자로부터의 입력을 수신할 수 있다. 여기서 설명된 시스템 및 기술은 백엔드 부품을 포함하는 컴퓨팅 시스템(예를 들어, 데이터 서버로서), 또는 미 들웨어 부품을 포함하는 컴퓨팅 시스템(예를 들어, 응용 서버), 또는 프런트 엔드 부품을 포함하는 컴퓨팅 시스 템(예를 들어, 그래픽 사용자 인터페이스 또는 네트워크 브라우저를 구비하는 사용자 컴퓨터인 바, 사용자는 당 해 그래픽 사용자 인터페이스 또는 네트워크 브라우저를 통해 여기서 설명된 시스템 및 기술의 실시 방식과 인 터랙션할 수 있음), 또는 이러한 백엔드 부품, 미들웨어 부품 또는 프런트 엔드 부품의 임의의 조합을 포한하는 컴퓨팅 시스템에서 실시될 수 있다. 시스템의 부품은 임의의 형태 또는 매체의 디지털 데이터 통신(예를 들어, 통신 네트워크)을 통해 서로 연결될 수 있다. 통신 네트워크의 예시는 근거리 통신망(LAN), 광역 통신망(WAN), 인터넷 및 블록체인 네트워크를 포함한다. 컴퓨터 시스템은 클라이언트 및 서버를 포함할 수 있다. 클라이언트 및 서버는 일반적으로 서로 멀리 떨어져 있 고, 통신 네트워크를 통해 인터랙션한다. 서로 클라이언트-서버 관계를 가지는 컴퓨터 프로그램을 대응되는 컴 퓨터에서 수행하여 클라이언트와 서버 간의 관계를 생성한다. 서버는 클라우드 컴퓨팅 서버 또는 클라우드 호스 트라고도 하는 클라우드 서버일 수 있고, 클라우드 컴퓨팅 서비스 시스템 중의 일종의 호스트 제품이고, 기존의 물리적 호스트 및 VPS(Virtual Private Server, 가상 사설 서버) 서비스에 존재하고 있는 관리가 어렵고 비즈니 스 확장이 약한 결점을 해결하기 위한 것이다. 본 출원의 실시예의 기술 방안에 따르면, 트레이닝 샘플 집합 중의 각 샘플이 소속된 태스크에 따라 각 샘플을 대응되는 샘플 대열에 순차적으로 넣고, 임의의 샘플 대열 중의 샘플 수량이 트레이닝 데이터 요구에 도달한 상 황하에서, 당해 샘플 대열 중의 데이터를 사용하여 멀티태스크 모델 중의 공유 네트워크 계층 및 당해 샘플 대 열에 관련된 태스크에 대응되는 타겟 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 당해 샘플 대열에 관 련된 태스크에 대응되는 모델 파라미터 업데이트 경도를 생성하고, 더 나아가 업데이트 경도에 따라 파라미터 서버 중의 공유 네트워크 계층 및 타겟 서브 네트워크 계층의 파라미터에 대해 업데이트한다. 이리하여, 멀티태 스크 모델 중의 특정 태스크에 대응되는 샘플 수량이 배치 데이터 요구에 도달할 경우, 당해 태스크에 대응되는 샘플 만을 사용하여 멀티태스크 모델의 공유 네트워크 계층 및 당해 태스크에 대응되는 서브 네트워크 계층에 대해 트레이닝하는 것을 통해, 기타 태스크에 대응되는 서브 네트워크 계층의 파라미터를 업데이트할 필요가 없 으므로, 멀티태스크 모델에 대해 분산형 트레이닝할 경우 국부 파라미터 업데이트를 구현하여 멀티태스크 모델 의 정밀도를 개선한다. 이해 가능한 바로는, 전술한 다양한 형식의 프로세스에 있어서 단계 재정렬, 추가 또는 삭제를 할 수 있다. 예 를 들어, 본 출원에 개시된 기술 솔루션이 이루고자 하는 결과를 구현할 수 있는 한, 본 출원에 기재된 각 단계 들은 병렬로, 순차적으로 또는 다른 순서로 수행될 수 있으나, 본 명세서에서 이에 대해 한정하지 않는다. 전술한 구체적인 실시 방식들은 본 출원의 보호 범위에 대한 한정을 구성하지 않는다. 당업자라면 본 출원의 설 계 요건 및 기타 요인에 따라 다양한 수정, 조합, 서브 조합 및 대체가 이루어질 수 있음을 이해해야 한다. 본 출원의 정신과 원칙 내에서 이루어진 모든 수정, 동등한 대체 및 개선은 본 출원의 보호 범위에 포함된다.도면 도면1 도면2 도면3 도면4 도면5"}
{"patent_id": "10-2021-0109539", "section": "도면", "subsection": "도면설명", "item": 1, "content": "첨부된 도면은 본 방안을 더 잘 이해하기 위한 것으로, 본 출원에 대한 한정이 구성되지 않는다. 여기서, 도1은 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이고; 도2는 본 출원의 실시예에 따른 다른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이고; 도3은 본 출원의 실시예에 따른 또 다른 멀티태스크 모델의 파라미터 업데이트 방법의 흐름도이고; 도4는 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 장치의 구조 개략도이고; 도5는 본 출원의 실시예에 따른 멀티태스크 모델의 파라미터 업데이트 방법을 구현하기 위한 전자 기기의 블록 도이다."}
