{"patent_id": "10-2022-0093959", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0015991", "출원번호": "10-2022-0093959", "발명의 명칭": "표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호화/복호화 방법 및 장치", "출원인": "한국전자기술연구원", "발명자": "김성제"}}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호화 방법에 있어서, 잔차 파라미터를 양자화 하는 단계, 및 상기 양자화된 잔차 파라미터를 엔트로피 부호화 하는 단계를 포함하되, 상기 잔차 파라미터를 양자화 하는 단계는 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 상기 잔차 파라미터의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 잔차 파라미터를 양자화 하는 단계는, 상기 잔차 파라미터의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그를 결정하는 단계,상기 잔차 파라미터를 표준 정규분포로 변환하는 단계, 및상기 표준 정규분포로부터 상기 잔차 파라미터의 중요도를 결정하고, 상기 결정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 파라미터를 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 또는 누적 지수양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계를 포함하는, 딥러닝 네트워크 부호화방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 잔차 파라미터는, 연합 학습에서 발생하는 딥러닝 네트워크의 잔차 가중치인, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2항에 있어서, 상기 잔차 파라미터를 양자화 하는 단계는, 양자화 대상 잔차 파라미터의 차원을 낮추는 단계를 더 포함하되, 변경된 잔차 파라미터의 차원은 1차원으로 유도하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 제1 임계값 보다 작은 경우는, 상기 제거 양자화 기법을 선택하는, 딥러닝 네트워크 부호화 방법.공개특허 10-2024-0015991-3-청구항 6 제5항에 있어서,상기 기설정된 제1 임계값은 1인, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 제거 양자화 기법은, 제거 양자화 기법이 적용되는 잔차 파라미터를 모두 0 으로 대체하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제2항에 있어서,상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 특정 구간내에 해당하는 경우는, 상기 이항 양자화 기법 또는 삼항 양자화 기법 중 어느 하나를 선택하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제8항에 있어서,상기 기설정된 특정 구간은, 1 과 2 사이 구간인, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제8항에 있어서, 상기 잔차 파라미터를 양자화 하는 단계는, 딥러닝 네트워크의 레이어별 확률밀도 함수를 계산하는 단계를 더 포함하되, 상기 확률밀도 함수의 정규분포로부터 평균과 표준편차를 구하여, 상기 이항 양자화 기법 또는 삼항 양자화 기법 중 어느 하나를 선택하기 위한상기 바이너리 플래그를 결정하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제10항에 있어서, 상기 잔차 파라미터 평균과 표준편차의 차의 절대값이 0보다 크면, 상기 바이너리 플래그를 참(True)으로 설정하고 상기 이항 양자화 기법을 선택하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제10항에 있어서, 상기 잔차 파라미터 평균과 표준편차의 차의 절대값이 0보다 작으면, 상기 바이너리 플래그를 거짓(False)으로설정하고 상기 삼항 양자화 기법을 선택하는, 딥러닝 네트워크 부호화 방법. 공개특허 10-2024-0015991-4-청구항 13 제2항에 있어서,상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 제2 임계값보다 큰 경우는, 상기 누적 지수 양자화 기법을 선택하는, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 기설정된 제2 임계값은 2인, 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "표준 정규분포 기반 양자화 기법으로 부호화된 딥러닝 네트워크 복호화 방법에 있어서, 역양자화 대상 잔차 파라미터 및 양자화 정보를 획득하는 엔트로피 복호화 단계, 및상기 잔차 파라미터를 역양자화 하는 역양자화 단계를 포함하되,상기 역양자화 단계는, 상기 획득된 양자화 정보로부터 부호화된 잔차 파라미터가 복수의 양자화 기법들 중 어느 양자화 기법을 적용하였는 지를 유도하는 단계, 및 상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법을 적용하여 복원된 잔차 파라미터를 유도하는 단계를 포함하는, 딥러닝 네트워크 복호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제15항에 있어서,상기 복수의 양자화 기법들은, 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 및 누적 지수 양자화 기법을 포함하는, 딥러닝 네트워크 복호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "복수의 클라이언트를 통해 연합 학습을 수행하는 딥러닝 네트워크 부호화 방법에 있어서, 상기 클라이언트에 의해 추가 학습된 업데이트 모델로부터 기준 모델의 차이값인 잔차 정보를 생성하는 단계,및상기 잔차 정보를 양자화 하는 단계를 포함하되, 상기 잔차 정보를 양자화 하는 단계는 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 잔차 정보의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용하는, 연합 학습을 수행하는 딥러닝 네트워크 부호화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "연합 학습을 수행하는 딥러닝 네트워크 시스템에 있어서, 추가 학습된 업데이트 모델로부터 기준 모델의 차이값인 잔차 정보를 생성하는 복수의 클라이언트, 및상기 복수 클라이언트로부터 생성된 잔차 정보를 수신하여, 보완된 잔차 정보를 생성하여 상기 복수 클라이언트에 전송하는 중앙 서버를 포함하되,상기 복수 클라이언트에 의해 생성된 상기 잔차 정보 또는 상기 중앙 서버에 의해 생성된 상기 보완된 잔차 정공개특허 10-2024-0015991-5-보는, 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 잔차 정보의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용하여 양자화되는 것을 특징으로 하는, 연합 학습을 수행하는 딥러닝 네트워크 시스템."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "딥러닝 네트워크 부호화를 위한, 양자화 방법에 있어서,양자화 대상인 잔차 정보의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그를 결정하는단계,상기 잔차 정보를 표준 정규분포로 변환하는 단계, 및상기 표준 정규분포 로부터 상기 잔차 정보의 중요도를 결정하고, 상기 결정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 정보를 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 또는 누적 지수 양자화기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계를 포함하는, 양자화 방법."}
{"patent_id": "10-2022-0093959", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "딥러닝 네트워크 복호화를 위한, 역양자화 방법에 있어서, 부호화된 잔차 정보에 대한 양자화 정보를 획득하는 단계,상기 양자화 정보로부터, 부호화된 잔차 정보가 이항 양자화 기법, 삼항 양자화 기법, 또는 누적 지수 양자화기법 중 어느 양자화 기법을 적용하였는 지를 유도하는 단계, 및상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법을 적용하여 복원된 잔차 정보를 유도하는단계를 포함하는, 역양자화 방법."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 딥러닝 네트워크((Deep Learning Network)의 부호화 또는 복호화 방법 및 장치에 관한 것이다. 특히, 잔차 정보 또는 잔차 파라미터의 부호화 또는 복호화를 위해 표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호화 또는 복호화 방법 및 장치에 관한 것이다. 본 개시의 일 실시예에 의한, 표준 정규분포 기반 양 자화 기법을 이용한 딥러닝 네트워크 부호화 방법은 잔차 파라미터를 양자화 하는 단계, 및 상기 양자화된 잔차 파라미터를 엔트로피 부호화 하는 단계를 포함한다. 여기서, 상기 잔차 파라미터를 양자화 하는 단계는 표준 정 규분포를 활용하여 기정의된 임계값을 기준으로 상기 잔차 파라미터의 중요도를 결정한 후, 상기 결정된 중요도 를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용한다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 딥러닝 네트워크((Deep Learning Network)의 부호화 또는 복호화 방법 및 장치에 관한 것으로서, 특 히, 잔차 정보 또는 잔차 파라미터의 부호화 또는 복호화를 위해 표준 정규분포 기반 양자화 기법을 이용한 딥 러닝 네트워크 부호화 또는 복호화 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "최근 저장 장치와 계산 장치의 발전으로 인공지능(AI) 시스템이 빠르게 발전하고 있다. 특히, 이러한 인공지능 을 구현하는 기술인 딥러닝(Deep Learning)에 관한 연구가 활발히 이루어지고 있다. 특히, 저장 및 계산 장치의 발전으로 딥러닝 네트워크 또한 용량이 비약적으로 증대함에 따라 최근에는 데이터 전송 시에 많은 어려움을 야 기하고 있다. 이에 ISO/IEC 산하 JTC1/SC29/WG4의 표준인 NNC(Compression of Neural Network for Multimedia Content Description and Analysis)는 네트워크 경량화를 위해 파라미터 제거(Parameter Reduction)를 진행한 후, 파라 미터 양자화(Parameter Quantization) 및 엔트로피 부호화(Entropy Coding)를 활용하여 딥러닝 네트워크를 압 축하는 방안을 제시하였다. 상기 NNC 표준은 이러한 사전 학습된 딥러닝 네트워크의 압축에 관한 표준화에 관한 것으로, 현재는 연합 학습(Federated Learning)시 발생하는 잔차 정보(Residual Information)의 압축을 논의하 고 있다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "발명의 내용"}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 딥러닝 네트워크 부호화/복호화 하는 방법 및 장치를 제공하는 것을 목적으로 한다. 또한, 본 개시는 딥러닝 네트워크의 연합 학습에서 발생하는 잔차 정보를 효율적으로 양자화 하는 방법 및 장치 를 제공하고자 한다. 또한, 본 개시는 딥러닝 네트워크를 활용하는 인공지능 시스템 및 그 응용에 있어서, 사전 학습된 딥러닝 네트 워크의 가중치 등 파라미터 압축을 통한 딥러닝 네트워크의 경량화 방법 및 장치를 제공하고자 한다. 본 개시의 다른 목적 및 장점들은 하기의 설명에 의해서 이해될 수 있으며, 본 개시의 실시예에 의해 보다 분명 하게 알게 될 것이다. 또한, 본 개시의 목적 및 장점들은 특허청구범위에 나타낸 수단 및 그 조합에 의해 실현 될 수 있음을 쉽게 알 수 있을 것이다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호화 방법은, 잔차 파라미터를 양자화 하는 단계, 및 상기 양자화된 잔차 파라미터를 엔트로피 부호화 하는 단계를 포함하되, 상기 잔차 파라미터를 양자화 하는 단계는 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 상기 잔차 파라미 터의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으 로 적용한다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터를 양자화 하는 단계는, 상기 잔차 파라미터의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그를 결정하는 단계, 상기 잔차 파라미터를 표준 정규분포로 변환하는 단계, 및 상기 표준 정규분포로부터 상기 잔차 파라미터의 중요도를 결정하고, 상기 결정 된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 파라미터를 제거 양자화 기법, 이항 양자화 기법, 삼 항 양자화 기법 또는 누적 지수 양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계를 포 함한다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터는, 연합 학습에서 발생하는 딥러닝 네트워크의 잔차 가중치일 수 있다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터를 양자화 하는 단계는, 양자화 대상 잔차 파라미터의 차원을 낮추는 단계를 더 포함하되, 변경된 잔차 파라미터의 차원은 1차원으로 유도한다. 또한, 본 개시의 일 실시예에 의하면, 상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 제1 임 계값 보다 작은 경우는, 상기 제거 양자화 기법을 선택한다. 여기서, 상기 기설정된 제1 임계값은 1일 수 있다. 또한, 본 개시의 일 실시예에 의하면, 상기 제거 양자화 기법은, 제거 양자화 기법이 적용되는 잔차 파라미터를 모두 0 으로 대체한다. 또한, 본 개시의 일 실시예에 의하면, 상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 특정 구 간내에 해당하는 경우는, 상기 이항 양자화 기법 또는 삼항 양자화 기법 중 어느 하나를 선택한다. 여기서, 상 기 기설정된 특정 구간은, 1 과 2 사이 구간일 수 있다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터를 양자화 하는 단계는, 딥러닝 네트워크의 레이어별 확률밀도 함수를 계산하는 단계를 더 포함하되, 상기 확률밀도 함수의 정규분포로부터 평균과 표준편차를 구하 여, 상기 이항 양자화 기법 또는 삼항 양자화 기법 중 어느 하나를 선택하기 위한 상기 바이너리 플래그를 결정 한다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터 평균과 표준편차의 차의 절대값이 0보다 크면, 상기 바이너리 플래그를 참(True)으로 설정하고 상기 이항 양자화 기법을 선택한다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터 평균과 표준편차의 차의 절대값이 0보다 작으면, 상 기 바이너리 플래그를 거짓(False)으로 설정하고 상기 삼항 양자화 기법을 선택한다. 또한, 본 개시의 일 실시예에 의하면, 상기 표준 정규분포에 의해 정규화된 값(z)의 절대값이 기설정된 제2 임 계값보다 큰 경우는, 상기 누적 지수 양자화 기법을 선택한다. 여기서, 상기 기설정된 제2 임계값은 2일 수 있다. 또한, 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법으로 부호화된 딥러닝 네트워크 복호화 방 법은, 역양자화 대상 잔차 파라미터 및 양자화 정보를 획득하는 엔트로피 복호화 단계, 및 상기 잔차 파라미터 를 역양자화 하는 역양자화 단계를 포함하되, 상기 역양자화 단계는, 상기 획득된 양자화 정보로부터 부호화된 잔차 파라미터가 복수의 양자화 기법들 중 어느 양자화 기법을 적용하였는 지를 유도하는 단계, 및 상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법을 적용하여 복원된 잔차 파라미터를 유도하는 단계를 포함한다. 또한, 본 개시의 일 실시예에 의하면, 상기 복수의 양자화 기법들은, 제거 양자화 기법, 이항 양자화 기법, 삼 항 양자화 기법 및 누적 지수 양자화 기법을 포함한다. 또한, 본 개시의 일 실시예에 따른, 복수의 클라이언트를 통해 연합 학습을 수행하는 딥러닝 네트워크 부호화 방법은, 각 클라이언트에 의해 추가 학습된 업데이트 모델로부터 기준 모델의 차이값인 잔차 정보를 생성하는 단계, 및 상기 잔차 정보를 양자화 하는 단계를 포함하되, 상기 잔차 정보를 양자화 하는 단계는 표준 정규분포 를 활용하여 기정의된 임계값을 기준으로 잔차 정보의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복 수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용한다. 또한, 본 개시의 일 실시예에 따른, 연합 학습을 수행하는 딥러닝 네트워크 시스템은, 추가 학습된 업데이트 모 델로부터 기준 모델의 차이값인 잔차 정보를 생성하는 복수의 클라이언트, 및 상기 복수 클라이언트로부터 생성 된 잔차 정보를 수신하여, 보완된 잔차 정보를 생성하여 상기 복수 클라이언트에 전송하는 중앙 서버를 포함하 되, 상기 복수 클라이언트에 의해 생성된 잔차 정보 또는 상기 중앙 서버에 의해 생성된 보완된 잔차 정보는, 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 잔차 정보의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용하여 양자화되는 것을 특징으로 한다. 또한, 본 개시의 일 실시예에 따른, 딥러닝 네트워크 부호화를 위한, 양자화 방법은, 양자화 대상인 잔차 정보 의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그를 결정하는 단계, 상기 잔차 정보를 표준 정규분포로 변환하는 단계, 및 상기 표준 정규분포 로부터 상기 잔차 정보의 중요도를 결정하고, 상기 결 정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 정보를 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 또는 누적 지수 양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계를 포함 한다. 또한, 본 개시의 일 실시예에 따른, 딥러닝 네트워크 복호화를 위한, 역양자화 방법은, 부호화된 잔차 정보에 대한 양자화 정보를 획득하는 단계, 상기 양자화 정보로부터, 부호화된 잔차 정보가 이항 양자화 기법, 삼항 양 자화 기법, 또는 누적 지수 양자화 기법 중 어느 양자화 기법을 적용하였는 지를 유도하는 단계, 및 상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법을 적용하여 복원된 잔차 정보를 유도하는 단계를 포함한다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 다양한 실시예에 의하면, 딥러닝 네트워크 부호화 및 복호화에 있어서, 연합 학습에서 발생하는 딥러 닝 네트워크의 잔차 정보에 대하여 표준 정규분포를 기반으로 하는 양자화 기법을 적용함으로써 잔차 정보를 효 율적으로 압축할 수 있게 된다. 구체적으로, 표준 정규분포에서 효율적인 임계치(threshold)를 설정해줌으로써 중요도가 높다고 판단되는 가중치는 최대한 유지하여 데이터 손실을 최소화할 수 있게 된다. 구체적으로, 본 개 시의 다양한 실시예에 의하면, 데이터 집합에서 0을 제외한 양수와 음수 데이터에 대하여 평균값으로 양자화 함 으로서 큰 데이터 손실을 유발하는 종래 기술의 문제점을 해결할 수 있게 된다. 또한, 본 개시의 다양한 실시예 에 의하면, 연합 학습에서 발생하는 잔차 정보를 중앙 서버(central server)에서 다양한 장치로 효율적으로 전 송해야 하는 경우, 높은 압축률과 낮은 성능 저하가 보장되는 양자화 기법으로써 활용될 수 있다."}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시는 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세한 설명에 상세하게 설명하고자 한다. 그러나, 이는 본 개시를 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 개시의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어 야 한다. 도면에서 유사한 참조부호는 여러 측면에 걸쳐서 동일하거나 유사한 기능을 지칭한다. 도면에서의 요 소들의 형상 및 크기 등은 보다 명확한 설명을 위해 과장될 수 있다. 후술하는 예시적 실시예들에 대한 상세한 설명은, 특정 실시예를 예시로서 도시하는 첨부 도면을 참조한다. 이들 실시예는 당업자가 실시예를 실시할 수 있기에 충분하도록 상세히 설명된다. 다양한 실시예들은 서로 다르지만 상호 배타적일 필요는 없음이 이해되어 야 한다. 예를 들어, 여기에 기재되어 있는 특정 형상, 구조 및 특성은 일 실시예에 관련하여 본 개시의 정신 및 범위를 벗어나지 않으면서 다른 실시예로 구현될 수 있다. 또한, 각각의 개시된 실시예 내의 개별 구성요소의 위치 또는 배치는 실시예의 정신 및 범위를 벗어나지 않으면서 변경될 수 있음이 이해되어야 한다. 따라서, 후술하는 상세한 설명은 한정적인 의미로서 취하려는 것이 아니며, 예시적 실시예들의 범위는, 적절하게 설명된 다면, 그 청구항들이 주장하는 것과 균등한 모든 범위와 더불어 첨부된 청구항에 의해서만 한정된다. 본 개시에서 제1, 제2 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으 로만 사용된다. 예를 들어, 본 개시의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. 및/또는 이라는 용어는 복수의 관련된 기재 된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 본 개시의 어떤 구성 요소가 다른 구성 요소에 “연결되어” 있다거나 “접속되어” 있다고 언급된 때에는, 그 다른 구성 요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있으나, 중간에 다른 구성 요소가 존재 할 수도 있다고 이해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \" 직접 접속되어\"있다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 개시의 실시예에 나타나는 구성부들은 서로 다른 특징적인 기능들을 나타내기 위해 독립적으로 도시되는 것으로, 각 구성부들이 분리된 하드웨어나 하나의 소프트웨어 구성단위로 이루어짐을 의미하지 않는다. 즉, 각 구 성부는 설명의 편의상 각각의 구성부로 나열하여 포함한 것으로 각 구성부 중 적어도 두개의 구성부가 합쳐져 하나의 구성부로 이루어지거나, 하나의 구성부가 복수 개의 구성부로 나뉘어져 기능을 수행할 수 있고 이러한 각 구성부의 통합된 실시예 및 분리된 실시예도 본 개시의 본질에서 벗어나지 않는 한 본 개시의 권리범위에 포 함된다. 본 개시에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 개시를 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 개시에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 즉, 본 개시에서 특정 구성을 “포함”한다고 기술하는 내용은 해당 구성 이외의 구성을 배제하는 것이 아니며, 추가적 인 구성이 본 개시의 실시 또는 본 개시의 기술적 사상의 범위에 포함될 수 있음을 의미한다. 본 개시의 일부의 구성 요소는 본 개시에서 본질적인 기능을 수행하는 필수적인 구성 요소는 아니고 단지 성능 을 향상시키기 위한 선택적 구성 요소일 수 있다. 본 개시는 단지 성능 향상을 위해 사용되는 구성 요소를 제외 한 본 개시의 본질을 구현하는데 필수적인 구성부만을 포함하여 구현될 수 있고, 단지 성능 향상을 위해 사용되 는 선택적 구성 요소를 제외한 필수 구성 요소만을 포함한 구조도 본 개시의 권리범위에 포함된다. 이하, 첨부한 도면을 참고로 하여 본 개시의 실시 예에 대하여 본 개시이 속하는 기술 분야에서 통상의 지식을 가진 자가 용이하게 실시할 수 있도록 상세히 설명한다. 또한 본 명세서의 실시예를 설명함에 있어, 관련된 공 지 구성 또는 기능에 대한 구체적인 설명이 본 명세서의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략하고, 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 관련하여, 본 개시에서 설명하는 발명의 일 실시예는, 연합 학습에서 생성된 잔차 정보에 새로운 양자화 기법을 적용하여 압축하는 것을 특징으로 하고 있으며, 이하에서는 이를 구현하기 위한 기술에 대해 상세히 설명한다. 우선, 본 개시에서 언급하는 '연합 학습'이란, 중앙 서버(central server)에서 기준 모델(baseline model)을 여러 장치로 배포하여 각 장치에서 기준 모델을 추가 학습하게 한 뒤, 업데이트된 모델(updated model)과 기준 모델의 잔차 정보를 수집하여 더욱 강화된 모델을 만들어 재배포하는 방식의 의미할 수 있다. 이에 대해서는 도 2를 참고하여 상세히 후술할 예정이며, 본 개시는 상기 연합 학습 과정에서 발생하는 잔차 정보를 압축하기 위 해 NNC 표준이 제시한 기본적인 모듈의 개념은 유지하되, 상기 연합 학습시 잔차 정보의 특성을 고려하여 상기 NNC 표준 모듈의 변형 및 수정을 제안하고자 한다. 또한, 본 개시에서 언급하는 '잔차 정보(residual information)'는 전술한 잔차 파라미터(residual paramete r)를 포함하여 잔차와 관련된 모든 정보를 의미한다. 여기서, 상기 잔차 파라미터는 잔차 가중치를 포함할 수 있다. 이하 본 개시의 설명에서는 보다 정확한 이해를 위해, 잔차 정보, 잔차 파라미터, 잔차 가중치 용어를 혼 용하여 사용한다. 도1은 본 개시의 일 실시예에 따른, 딥러닝 네트워크의 완전 연결 계층(fully connected layer)을 나타낸 하나 의 예시로, 딥러닝 네트워크에서 발생하는 파라미터를 설명하기 위한 도면이다. 일반적으로, 딥러닝 네트워크는 수많은 레이어(layer)로 구성되며, 각 레이어는 도1에서 도시된 바와 같이 x1~x3와 y1~y2에 해당하는 뉴런(neurons), W11~W32에 해당하는 가중치(weight), 그리고 b에 해당하는 편향 (bias)으로 구성되어 있다. 반면, 도1은 설명의 편의를 위해 간략화한 개념도로 제시한 것일뿐, 실제 일반적으 로 사용되는 딥러닝 모델의 레이어는 수많은 뉴런 및 가중치로 구성되었기 때문에, 막대한 양의 데이터를 포함 하고 있다. 이 중 뉴런은 가중치에 의해 계산이 되는 값이고, 일반적으로 사전 학습된 모델(pre-trained mode l)로 명명하는 것은 학습을 통해 정해진 가중치 및 편향을 의미할 수 있다. 특히, 상기 가중치 및 편향 정보는 딥러닝 프레임워크(framework)에 따라 저장 방식이 달라지는 바, 예를 들어 범용적으로 사용되는 프레임워크는 파이토치(PyTorch) 혹은 텐서플로우(TensorFlow) 등이 있다. 도2는 본 개시의 일 실시예에 따른, 연합 학습의 서비스 모델의 한 예를 설명하는 도면이다. 도2에서 제시하는 서비스 모델은 일반적인 연합 학습의 시나리오를 설명의 편의를 위해 개념적으로 도시한 것이다. 예를 들어, 중 앙 서버에서 기준 모델(baseline model)을 각 클라이언트(client, 221, 223, 225)에 배포하면, 각 클라이 언트에서는 각자의 고유 데이터를 활용하여 기준 모델을 학습시킨다. 이후, 상기 각 클라이언트(221, 223, 22 5)는 학습된 업데이트 모델과 기준 모델의 차이값인 잔차 정보를 중앙 서버로 다시 보낸다. 여기서 잔차 정보에 해당하는 것은 도2의 ~ 이다. 이 후, 상기 중앙 서버에서는 수집한 잔차 정보를 기반으로 기준 모델을 보완하며, 보완된 모델과 기존의 기준 모델의 차이값인 보완된 잔차 정보를 다시 각 클라이언트에 배포하게 된다. 이때 보완된 잔차 정보는 도2의 에 해당한다. 상기 과정을 매 단계(epoch)마다 시행하게 된 다. 관련하여, 상기 연합 학습 방식은 클라이언트(221, 223, 225)의 데이터를 직접적으로 중앙 서버로 보 내지 않기에 개인 정보의 유출을 막을 수 있다는 장점을 가진다. 도3은 본 개시의 일 실시예에 따른, NNC 표준에서 논의하고 있는 연합 학습에서의 잔차 정보 생성 과정을 설명 하기 위한 도면이다. 도3에서 도시한 바와 같이, 잔차 정보란 추가 학습된 모델의 파라미터와 기준 모델 파라미 터의 차이 값을 의미한다. 즉, 도3의 로 표현된 잔차 정보는 에서 을 뺀 값이 다. 편향 또한 차이가 있으면 그에 해당하는 차이를 저장하지만, 일반적으로 편향의 잔차 정보는 0이 된다. 즉, 잔차 정보를 발생시키는 두 개의 모델은 동일한 형태를 갖추어야 하며, 잔차 정보는 두 개의 모델과 같은 차원 을 가진다. 일반적으로 연합 학습에서는 각 단계별 재학습이 모델의 가중치에 큰 영향을 끼치지 않으며, 따라서 각 단계별 보완된 모델끼리 유사한 파라미터를 갖게 된다. 따라서, 각 단계별 모델의 차이값인 잔차 정보는 0의 비율이 높을 수밖에 없으며, 0이 아닌 값 또한 0에 매우 가까운 값을 갖게 된다. 결국 이러한 잔차 정보를 압축 하기 위한 방안은 기존의 학습된 모델의 가중치를 압축하는 기법과는 차이가 있어야 함을 의미한다. 도4는 본 개시의 일 실시예에 따른, 연합 학습에서 발생하는 잔차 정보의 일반적인 경향성을 설명하기 위한 도 면이다. 전술한 바와 같이, 잔차 정보는 대부분 0 혹은 0에 가까운 값으로 구성된다. 도4는 이를 시각적으로 도 시한 히스토그램(Histogram)이며, 전술한 도2에 해당하는 연합 학습 시나리오에 대한 실제 데이터 값을 통해 도 출해낸 결과이다. 도4의 히스토그램을 참고하면, 0의 비율이 가장 높음을 알 수 있고, 또한 순차적으로 0에서 멀어질수록 빈도수가 떨어짐을 알 수 있다. 즉, 도4로부터 연합 학습에서 발생하는 잔차 정보 데이터가 정규분 포의 형태를 띄고 있음을 알 수 있다. 도5는 본 개시에 따른, NNC 표준에서의 압축 과정을 설명하기 위한 NNC 부호화 및 복호화 장치의 구성을 도시한 것이다. 도5에서 도시한 바와 같이, NNC 부호화 장치는 세 가지 기본 모듈을 포함하여 구성될 수 있다. 즉, NNC 부호화 장치는 매개변수 제거(parameter reduction) 모듈 (이는 '파라미터 제거 모듈' 라고 도 한다), 매개변수 양자화(parameter quantization) 모듈 (이는 '파라미터 양자화 모듈' 라고도 한다), 및 엔트로피 부호화(entropy coding) 모듈을 포함한다. 구체적으로, 상기 파라미터 제거 모듈은 네트워크 경량화를 위한 단계이며, 대표적으로 희소화 (sparsification) 과정 및 프루닝(pruning) 과정을 포함할 수 있다. 예를 들어, 사전 학습된 모델에 대한 희소 화는 모델 가중치를 0에 가까운 값의 비율을 높이는 방향으로 재학습을 진행하는 것을 의미하며, 상기 파라미터 제거를 통해 가중치의 중요도를 따져 중요도가 낮은 값을 0으로 만드는 것을 의미한다. 여기서 중요도는 파라미터가 딥러닝 네트워크에 영향을 끼치는 정도를 의미할 수 있으며, 0에 가까울수록 중요도가 낮음을 의미한다. 하지만 학습된 모델의 가중치가 아닌 연합 학습에서의 잔차 정보에 대해서는 파라미터 제거를 일반적으로 사용 하지는 않는데, 이는 잔차 정보에 대한 재학습을 진행하기 어렵기 때문이다. 상기 파라미터 양자화 모듈은, 상기 파라미터 제거를 거친 정보 (예, 잔차 정보)에 대해서 파라미터 양자 화를 수행한다. 예를 들어, 학습된 모델의 가중치를 양자화 하는 방법에는 대표적으로 스칼라 양자화(scalar quantization), 코드북 양자화(codebook quantization) 및 확률적 이항-삼항 양자화(SBT 양자화, Stochastic Binary-Ternary quantization)가 있을 수 있다. 특히, 전술한 잔차 정보를 양자화 하는 경우 주로 '확률적 이항 -삼항 양자화 (SBT 양자화)'가 사용된다. 여기서, 상기 파라미터 양자화 모듈의 SBT 양자화는 확률에 기반하여 무작위로 이항 또는 삼항 양자화를 선택하 는 방식이다. 상기 SBT 양자화는 데이터 집합에서 0을 제외한 양수와 음수 데이터에 대하여 평균값으로 양자화 함으로써 압축 효율을 높이고자 한다. 그러나 0을 제외한 데이터 값들이 평균값으로 대체하기에 큰 데이터 손실 을 일으키므로 해당 압축을 적용한 딥러닝 네트워크는 성능 하락이 발생할 수 있다는 제한 사항이 있다. 예를 들어, 일반적으로 연합 학습에서 발생하는 잔차 정보는 대부분이 0 혹은 0과 가까운 값이라는 특징을 갖고 있다. 상기 특징을 가지는 정보에 대해 SBT 양자화를 활용하는 것은 데이터의 특성을 고려한 방법이며, 이러한 양자화 기법을 활용함으로써 데이터의 중복성(redundancy)을 높이는 것은 압축률을 높일 수 있는 결과를 얻을 수 있게 된다. 하지만 중복성이 높다는 것은 큰 데이터 손실을 의미하며, 이는 곧 성능 저하로 연결되어진다. 이에 본 개시는 전술한 NNC 표준을 지원하는 요소 기술로써, 연합 학습시 발생하는 잔차 정보에 대한 성능 저하 를 최소화한 새로운 파라미터 양자화 기법을 제안하고자 한다. 구체적으로, 본 개시는 연합 학습에서 발생하는 잔차 정보를 압축할 시 딥러닝 성능 저하를 막기 위해 표준 정규분포를 활용하여 사전 정의된 임계값을 기준으 로 각 파라미터의 중요도를 설정한 후, 각 중요도에 따라 서로 다른 양자화 기법을 활용하는 방법을 제안한다. 이에 대해서는 도6 내지 도15를 참고하여 상세히 후술한다. 또한, 상기 엔트로피 부호화 모듈은, 유사한 값들을 하나의 통일된 값으로 대체하는 엔트로피 부호화 방식 을 통해 압축 효율을 높일 수 있는 기법이다. 예를 들어, 상기 엔트로피 부호화 기법은 값의 출현 확률에 따라 할당하는 비트(bit)수를 다르게 하여 같은 정보를 더 적은 비트수로 표현할 수 있게 하는 압축 기법으로써, 예 를 들어, DeepCABAC(Context Adaptive Binary Arithmetic Coding for Deep neural network compression)을 적 용할 수 있다. 따라서, 상기 양자화를 거친 정보 (예, 잔차 정보)는 최종적으로 상기 엔트로피 부호화 모듈을 거쳐 비트스트림으로 압축되어 전송되어 진다. 또한, NNC 복호화 장치는 두 가지 기본 모듈을 포함하여 구성될 수 있다. 즉, NNC 복호화 장치는 엔 트로피 복호화(entropy decoding) 모듈 및 매개변수 역양자화(parameter inverse-quantization) 모 듈 (이는 '파라미터 역양자화 모듈' 라고도 한다)을 포함한다. 여기서, 상기 엔트로피 복호화 모듈은 전술한 엔트로피 부호화 모듈에 의해 부호화된 비트스트림을 복호화 하는 과정을 의미한다. 또한, 상기 매개변수 역양자화 모듈은, 전술한 상기 파라미터 양자화 모듈 에 의해 양자화된 정보 (예, 잔차 정보)를 복원하여 복원된 잔차 정보를 생성하는 역양자화 과정을 의미한 다. 도6은 본 개시의 일 실시예에 따른, 연합 학습에서의 잔차 가중치를 위한 표준 정규분포 기반 양자화 기법의 과 정을 설명하기 위한 도면이다. 도6의 과정은, 예를 들어 전술한 도5의 상기 파라미터 양자화 모듈에 의해 수행될 수 있다. 도 6을 참고하면, 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부 호화 방법은, 상기 파라미터 양자화 모듈을 통해 잔차 파라미터를 양자화 하는 단계, 및 상기 엔트로피 부 호화 모듈을 통해 상기 양자화된 잔차 파라미터를 엔트로피 부호화 하는 단계를 포함한다. 여기서, 상기 잔차 파라미터를 양자화 하는 단계는 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 상기 잔차 파라미 터의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으 로 적용한다. 또한, 본 개시의 일 실시예에 의하면, 상기 잔차 파라미터를 양자화 하는 단계는, 상기 잔차 파라미터의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그를 결정하는 단계, 상기 잔차 파라미터를 표준 정규분포로 변환하는 단계, 및 상기 표준 정규분포로부터 상기 잔차 파라미터의 중요도를 결정하고, 상기 결정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 파라미터를 제거 양자화 기법, 이항 양자화 기법, 삼 항 양자화 기법 또는 누적 지수 양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계를 포 함한다. 예를 들어, 도6의 과정을 부언하여 설명하면 다음과 같다. 먼저, 연합 학습에서 발생하는 잔차 정보를 입력으로 받는다(S1110). 이때 잔차 정보는 잔차 가중치일 수 있고, 잔차 파라미터로도 명명될 수 있다. 여기서, 입력된 잔차 정보는 다차원 텐서(Tensor)의 형태를 가지므로, 이를 각 레이어별 차원 낮춤(flattening)을 통해 1차원으 로 차원을 낮춘다 (S1120). 또한, 이후 각 레이어별 데이터에 대해서 확률밀도 함수를 구하여 정규분포를 구하 는 레이어별 확률밀도 함수 계산 단계 (S1130)를 통해, 상기 정규분포를 기반으로 바이너리 플래그 (binary_flag)의 참/거짓 여부를 결정한다 (S1140). 이후, 상기 정규분포는 표준 정규분포로 변환시키고 (S1150), 해당 표준 정규분포에서의 위치를 기반으로 세 종류의 양자화 기법 중 적어도 하나 이상 또는 이들의 조합으로 활용한다. 관련하여, 상기 본 개시에서 제안하는 양자화 기법들은, 제거 양자화(Pruning Quantization)(S1160), 이항-삼항 양자화(Binary-Ternary Quantization)(S1170) 및 누적 지수 양자화(Additive Exponent Quantization)(S1180)를 포함한다, 여기서, 상기 이항-삼항 양자화(Binary-Ternary Quantization)(S1170)는 이항 양자화 기법 (S1171) 및 삼항 양자화 기법 (S1172)를 각각 포함한다. 이하 상기 도6의 각 과정을 도7 내지 도15를 참고하여 상세히 설명한다. 도7은 본 개시의 일 실시예에 따른, 차원 낮춤 과정(flattening process, S1120)을 설명하기 위한 도면이다. 여 기서, 차원 낮춤 과정은, 데이터를 더욱 손쉽게 다루기 위해서 다차원 텐서(tensor)를 1차원 텐서로 바꾸는 과 정을 의미한다. 도7의 예시와 같이, 하나의 레이어에 대해 확률밀도 함수를 구하고자 할 때, 2차원 텐서로 구성 된 데이터보다는 1차원 데이터를 활용하는 것이 데이터 처리 면에서 더욱 간편할 수 있다. 도8은 본 개시의 일 실시예에 따른, 레이어별 확률밀도 함수 계산(S1130)을 이용한 정규분포를 설명하기 위한 도면이다. 정규분포의 확률밀도 함수의 공식은 수학식1과 같으며, 도8은 정규분포의 확률밀도 함수를 나타낸 것 이다. [수학식1]"}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "이때, 는 해당 레이어의 가중치 평균을 의미하며, 는 가중치 분산, 은 가중치 표준편차를 의미한다. 전술 한 도4를 다시 참고하면, 잔차 정보의 분포도는 정규분포의 형태를 띄며, 표본 수 또한 많기에 정규분포를 가정 하여 수학식1로 나타낼 수 있다. 예를 들어, 실험적으로 제1 가중치 표준편차 (- ~ ) 구간 사이의 확률은 68.27%, 제2 가중치 표준편차 (-2 ~2 ) 구간 사이의 확률은 95.45%, 제3 가중치 표준편차 (-3 ~3 ) 구간 사이 의 확률은 99.73% 에 도달함을 알 수 있다. 이는 후술할 표준 정규분포로 변환할 때 사용되며, 정규분포의 확률 밀도 함수를 구하기 위해 도출한 가중치 평균과 표준편차는 다음 모듈의 바이너리 플래그(binary_flag)의 참/거 짓(True/False) 판단 여부에 활용될 수 있다. 도9는 본 개시의 일 실시예에 따른, 레이어별 바이너리 플래그 설정(S1140)을 설명하기 위한 도면으로, 상기 바 이너리 플래그의 참/거짓을 결정하는 기준을 시각적으로 예시한 것이다. 상기 바이너리 플래그는 전술한 양자화 기법들 중, 이항-삼항 양자화(Binary-Ternary Quantization) 과정(S1170)에서 이항 양자화(Binary Quantization)(S1171) 혹은 삼항 양자화(Ternary Quantization)(S1172)를 선택하는 기준이 된다. 일반적으로 잔차 정보는 0을 기준으로 정규분포로 도시될 수 있다. 즉, 예외적으로 평균이 0이 아닌 특정 양의 값, 혹은 음 의 값인 경우도 발생할 수 있다. 이러한 경우 도9에서 도시하는 바와 같이 평균과 표준편차의 차의 절대값이 0 보다 크다면, 바이너리 플래그를 참(True)으로 설정한다. 이는 평균의 부호와 반대되는 값들은 일반적으로 중요 도가 떨어진다고 판단되어, 이후 이항 양자화에서 한가지 부호만 사용하게끔 유도하기 위한 것이다. 도10은 본 개시의 일 실시예에 따른, 상기 바이너리 플래그의 참/거짓이 결정되는 과정을 설명하기 위한 도면 이다. 예를 들어, 도10은 전술한 도9에서 설명하는 바를 의사코드(pseudocode)로 표현한 것이다. 예를 들어, ~ 는 잔차 정보의 한 레이어의 가중치 값들이며, 차원을 낮추어 1차원 텐서인 상태로 저장 되어 있다 . 이후, ~ 의 평균과 표준편차는 전술한 정규분포의 확률밀도 함수를 구 하는 과정에서 도출된다. 예를 들어, 평균이 양수일 경우, 평균과 표준편차의 차 값이 양수라면 바이너리 플래 그는 참이며, 음수라면 거짓이다. 반대로 평균이 음수일 경우, 평균과 표준편차의 합이 0보다 작다 면 바이너리 플래그가 참이고, 0보다 더 크다면 거짓이다. 도11은 본 개시의 일 실시예에 따른, 표준 정규분포 변환 단계 (S1150)를 설명하기 위한 도면이다. 표준정규 분 포는 정규분포를 표준화한 것으로써, 평균이 0이며 표준편차가 1인 정규분포를 의미한다. 정규분포를 표준 정규 분포로 표준화하는 이유는, 계산의 편의 때문이다. 예를 들어, 정규분포의 확률밀도 함수에서는, 특정 값에 대 한 출현 확률을 구하기 위하여 평균과 분산을 활용하는 반면, 표준 정규분포의 확률밀도 함수에서는, 도11에서 와 같이 z를 통해 바로 출현 확률을 구할 수 있게 된다. 전술한 표준 정규분포는 잔차 정보의 가중치 값들의 크기 및 출현 빈도를 동시에 고려하여 가중치의 중요도를 산정할 수 있으며, 이후 양자화에서의 임계치를 보다 용이하게 설정할 수 있다는 장점이 있다. 예를 들어, 표준 정규분포의 확률밀도 함수를 구하는 공식은 수학식2와 같으며, 이때 x의 입력 데이터는 수학식 3을 통해서 z로 정규화된다. [수학식2]"}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "[수학식3]"}
{"patent_id": "10-2022-0093959", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "예를 들어, 전술한 도8의 경우를 참고하면, 제1 표준 정규분포 (-1~1) 구간 사이의 확률은 68.27%, 제2 표준 정 규분포 (-2~2) 구간 사이의 확률은 95.457%, 제3 표준 정규분포 (-3~3) 구간 사이의 확률은 99.73% 에 도달함을 알 수 있다. 관련하여, 상기 바이너리 플래그 및 표준 정규분포를 이용한, 본 개시에서 제안하는 양자화 기법들은, 제거 양 자화(Pruning Quantization)(S1160), 이항-삼항 양자화(Binary-Ternary Quantization)(S1170) 및 누적 지수 양 자화(Additive Exponent Quantization)(S1180)를 포함한다, 즉, 중요도가 가장 떨어지는 데이터에 대해서는 상 기 제거 양자화(S1160)를 수행할 수 있다. 반면. 중요도가 더 높다고 판단되는 데이터는 상기 이항-삼항 양자화 (S1170)을 통해 양자화되며, 중요도가 가장 높은 값들에 대해서는 상기 누적 지수 양자화(S1180)를 통해 원본과 최대한 유사한 값을 유지하면서 양자화를 수행할 수 있다. 따라서, 중요도가 높은 파라미터에 대해 원본과 유사 한 값을 유지함으로써 연합 학습에서 발생하는 잔차 정보의 압축에 대해 성능 저하를 최소화할 수 있게 된다. 이하 상기 양자화 방식들에 대해 상세히 설명한다. 우선, 도12는 본 개시의 일 실시예에 따른, 상기 제거 양자화(S1160)를 설명하기 위한 도면이다. 상기 제거 양 자화 방식은 본 개시의 일 실시예에 따라 중요도가 가장 낮은 잔차 정보에 대해 수행하는 양자화 기법으로 활용 될 수 있다. 예를 들어, 전술한 S1150 단계의 표준 정규분포에서 z의 절대값이 특정 임계값 (예, '1', '0.1' 등)보다 작은 값을 전부 0으로 대체하는 방식이다. 이는 z의 절대값이 상기 임계값 보다 작은 값들은 출현 빈도 수는 높으나 실질적으로 모델의 성능에 큰 영향을 끼치지 않는 중요도가 떨어지는 값으로 정의할 수 있기 때문 이다. 상기 제거 양자화 방식을 통해 빈도수가 높은 값을 0으로 대체함으로써 데이터 손실이 일부 발생할 수 있 지만, 그만큼 중복성(redundancy)이 높아지므로 엔트로피 부호화 과정에서 높은 압축률을 달성할 수 있게 된다. 예를 들어, 도12의 예시를 참고하면, 상기 제거 양자화 이전 잔차 가중치가 각각, 0.21, 0.09, 0.05 를 포함할 때, 상기 임계값을 '0.1'로 설정하여 상기 제거 양자화 방식을 적용하면, 양자화된 잔차 가중치는 0.21, 0, 만을 가지게 된다. 도13은 본 개시의 일 실시예에 따른, 상기 이항-삼항 양자화(S1170)를 설명하기 위한 도면이다. 본 개시의 일 실시예에 의하면, 상기 이항-삼항 양자화 방식은, 상기 표준 정규분포에서 z의 절대값이 특정 구간 (예, 1과 2 사이)일 경우에 사용하는 양자화 기법으로 활용할 수 있다. 이는 전술한 종래 NNC 표준의 양자화 기법 중 하나 인 확률적 이항-삼항 양자화 (SBT 양자화) 기법과 개념은 같으나, 이항 양자화와 삼항 양자화를 선택하는 기준 이 종래 베르누이 확률분포에 기반한 무작위성이 아닌, 전술한 S1140단계에서 설정된 상기 바이너리 플래그에 의해 결정된다는 차이점이 있다. 즉, 본 개시의 일 실시예에 따르면, 상기 바이너리 플래그가 참일 경우 상기 이항 양자화 (S1171)를 선택하며, 거짓일 경우 상기 삼항 양자화(S1172)를 선택할 수 있다. 예를 들어, 도 13의 (a)는 이항 양자화 기법을, 도13의 (b)는 삼항 양자화 기법에 대한 간단한 예시를 도시하였 다. 관련하여, 상기 이항 양자화 (S1171) 기법은 양수 값의 평균과 음수 값의 평균을 비교하여, 영향을 더 많이 주 는 부호에 대한 특성만을 유지하는 양자화 기법이다. 예를 들어, 평균이 큰 부호가 영향을 더 많이 주는 것이며, 반대 부호는 전부 0이 된다. 즉, 평균이 큰 부호의 값들은, 해당 부호의 평균으로 대체되며, 이를 통해 중복성을 높일 수 있게 된다. 예를 들어, 도13의 (a) 예시에 따라 잔차 정보가 {0,0,1,3,2,0,0,0,-2,-1} 이라고 가정하면, 양수의 평균은 2이며 음수의 평균은 -1.5 이다. 따라서, 2는 -1.5의 절대값인 1.5보다 크므로, 양수 가 음수보다 영향력이 큰 것으로 해석하고, 이에 따라 음수 값은 전부 0으로 설정한다. 즉, 양수 값인 1과 3은 평균인 2로 대체되어, 결과적으로 -2, -1, 0, 1, 2, 3의 값들로 이루어졌던 텐서는 0과 2로만 표현된 텐서로, {0,0,2,2,2,0,0,0,2,2} 로 양자화되며, 0과 2의 중복성이 높아지게 된다. 상기 삼항 양자화 (S1172) 기법의 경우, 도13의 (b) 예시에 따라 잔차 정보가 {0,0,1,3,2,0,0,0,-2,-1} 이라고 가정하면, 각 부호(양 및 음)의 평균을 구한 후 양수인 가중치는 양의 평균으로 대체하며 음수인 가중치는 음의 평균으로 대체한다. 즉, 전술한 이항 양자화 기법은 하나의 부호만 유지한 반면, 상기 삼항 양자화 기법은 양쪽 부호를 모두 유지함으로써 데이터 손실을 줄일 수 있게 된다. 예를 들어, 도13의 (b) 예시와 같이, 양수의 값은 양의 평균인 2로, 음수는 음의 평균인 -1.5로 되어, 결국 {0,0,2,2,2,0,0,0,-1.5, -1.5} 로 양자화될 수 있다. 결국, 전술한 이항 양자화 기법보다 더 많은 값(예, '-1.5')을 활용하여 더 많은 특성을 유지할 수 있게 된다. 전술한 이항 양자화 (S1171) 및 삼항 양자화 (S1172)는, 전술한 도 12의 제거 양자화 기법만큼 데이터 손실을 발생시키지는 않지만, 다양한 값들을 하나의 평균값으로 대체한다는 점에서 일정 부분 데이터 손실이 불가피하 다. 하지만, 본 개시의 일 실시예에서는, 양자화를 진행하는 레이어가 가지는 최소한의 특성을 유지할 수 있다 는 장점으로 인해, 표준 정규분포에서 Z의 절대값이 특정 구간 (예, 1과 2 사이) 인 값들에 대해서 상기 이항 양자화 (S1171) 기법 또는 삼항 양자화 (S1172) 기법을 선택적으로 적용한다. 즉, 중요도가 상대적으로 낮은 것 도, 높은 것도 아닌 것으로 판단되는 이러한 가중치에 대해서는 최소한의 특성만 남겨도 된다는 가정으로, 전술 한 이항 양자화 (S11710 기법 및/또는 삼항 양자화 (S1172) 기법을 선택적으로 수행하게 된다. 도14는 본 개시의 일 실시예에 따른, 상기 누적 지수 양자화 (S1180) 방식을 설명하기 위한 도면이다. 예를 들 어, 본 개시의 일 실시예에 의하면, 전술한 S1150 단계의 표준 정규분포에서 z의 절대값이 특정 임계값 (예, '2') 보다 큰 값들에 대해 적용하는 양자화 기법이 될 수 있다. 즉, 상기 누적 지수 양자화 (S1180)는, 적은 비 트(bit)수를 활용하며 최대한 원본과 유사한 값을 유지할 수 있는 양자화 기법이다. 예를 들어, 도14는 상기 누적 지수 양자화 (S1180) 과정을 의사코드(pseudocode)로 표현한 것이다. 먼저, 원소 가 N개 (예, 4개) 인 배열을 초기화한다. 이 후 i=0부터 시작하여, 2의 i 승을 구한다 . 다음, 양 자화 하려는 값보다 작아질 때까지 i를 감소시킨다 . 예를 들어, 양자화 하려는 파라미터보다 작아지면 i의 감소를 멈추고, 현재 2의 i승을 기준점으로 잡는다 . 상기 기준점으로부터 다시 위와 같은 과정을 진행하며, 이를 총 N번 (예, 4번) 진행하여 각 루프(loop) 문에서 결정되는 i를 미리 초기화해둔 배열의 원소로 입력한다. 상기 과정을 예를 들어 도15를 참고하여 설명하면 다음과 같다. 도15는 본 개시의 일 실시예에 따른, 상기 누적 지수 양자화 (S1180)에 대한 구체적 예시를 도시화한 것이다. 예를 들어, 도15는 '0.65' 라는 값을 상기 누적 지수 양자화 (S1180) 기법을 통해 양자화 하는 과정을 도시화한 예시이다. 도15를 참고하면, 먼저, i=0 이라면, 는 1이다. 1은 0.65 보다 크므로 i를 -1로 감소시킨다. 은 0.5 이며, 0.5 는 0.65 보다 작으므로 0.5가 기준점이 되며, 배열의 첫 번째 원소로 -1을 삽입한다 . 다음으로, 0.5에 2의 i승을 더하여 0.65 보다 작아질 때까지 i를 감소시킨다. i가 -2이면 0.5 + 은 0.5+0.25인 0.75 이다. i를 다시 감소시키면 0.5 + 이며, 이는 0.5+0.125인 0.625 로 계산된다. 이때, 0.625는 0.65보다 작 으므로 0.625 가 기준점이 되며, 배열의 두 번째 원소로 -3을 삽입한다 . 상기 과정을 두 번 더 하면 최 종적으로 도15에 도시된 바와 같이 [-1,-3,-6,-7]이라는 1차원 배열을 얻을 수 있다 . 도15의 예시로부터, 결과적으로 0.65는 매우 유사한 0.6484375 라는 값으로 근사 시킬 수 있게 된다. 상기 근사 된 결과 값 (0.6484375)만 보면, 0.65를 표현하는 데에 필요한 비트수가 더 적어 보일 수도 있다. 그러나 전체 파라미터를 고려하면, 상기 누적 지수 양자화 (S1180) 방식에 따른 정수의 배열로써 표현하는 방법이 중복성을 높이고 결국 엔트로피 부호화에서 높은 압축률을 달성할 수 있게 된다. 또한 상기 누적 지수 양자화 (S1180) 기 법은 원본 파라미터와 매우 유사한 값을 복원할 수 있기 때문에, 중요도가 높은 가중치에 적용하기 적합한 양자 화 기법으로 활용할 수 있다. 따라서, 중요도가 높다고 판단되는, 예를 들어, 전술한 S1150 단계의 상기 표준 정규분포에서 z의 절대값이 2 이상인 파라미터에 적용하면 보다 효율적일 수 있다. 전술한 도 6 내지 도 15의 과정을 정리하면, 본 개시의 일 실시예에 따르면, 입력 받은 잔차 정보의 파라미터를 상기 표준 정규분포로 변환한 후, z의 범위에 따라 서로 다른 양자화 기법을 활용한다. 예를 들어, z의 절대값 이 1보다 작다면 중요도가 떨어지는 파라미터라고 판단하여 상기 제거 양자화(S1160)를 통해 0으로 대체한다. 반면, z의 값이 상기 표준 정규분포에서 1과 2 사이인 파라미터에 대해서는, 최소한의 특성을 살릴 수 있는 상 기 이항 양자화(S1171) 또는 상기 삼항 양자화(S1172)를 수행한다. 끝으로, 중요도가 가장 높다고 판단되는, z 의 값이 2 이상인 파라미터는, 상기 누적 지수 양자화(S1180)를 활용하여 양자화 한다. 본 개시에서 제안하는 적어도 세 종류의 양자화 기법들을 모두 사용하거나 또는 일부 양자화 기법들을 조합하여 사용함에 의해, 높은 압축률과 낮은 성능 저하를 보장할 수 있게 된다. 도16은 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호화 방법을 도시한 것이다. 상기 딥러닝 네트워크 부호화 방법은, 표준 정규분포를 활용하여 기정의된 임계값을 기준으로 상기 잔차 파라미터의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용하는 잔차 파라미터를 양자화 하는 단계 (S1610), 및 상기 양자화된 잔차 파라미터를 엔 트로피 부호화 하는 단계(S1620)를 포함하여 구성될 수 있다. 도17은 전술한 도16의 잔차 파라미터를 양자화 하는 단계 (S1610)의 세부 단계를 도시한 것이다. 즉, 상기 잔차 파라미터를 양자화 하는 단계 (S1610)는, 상기 잔차 파라미터의 평균과 표준편차를 활용하여 양자화 기법 선택 을 위한 바이너리 플래그를 결정하는 단계(S1611), 상기 잔차 파라미터를 표준 정규분포로 변환하는 단계 (S1612), 및 상기 표준 정규분포로부터 상기 잔차 파라미터의 중요도를 결정하고, 상기 결정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 파라미터를 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 또 는 누적 지수 양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계(S1613)를 포함한다. 도18은 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법으로 부호화된 딥러닝 네트워크 복호화 방 법을 도시한 것이다. 상기 딥러닝 네트워크 복호화 방법은, 역양자화 대상 잔차 파라미터 및 양자화 정보를 획 득하는 엔트로피 복호화 단계(S1810), 및 상기 잔차 파라미터를 역양자화 하는 역양자화 단계(S1820)를 포함한 다. 도19는 전술한 도18의 역양자화 단계 (S1820)의 세부 단계를 도시한 것이다. 상기 역양자화 단계(S1820)는, 상 기 획득된 양자화 정보로부터 부호화된 잔차 파라미터가 복수의 양자화 기법들 중 어느 양자화 기법을 적용하였 는 지를 유도하는 단계(S1821), 및 상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법을 적용 하여 복원된 잔차 파라미터를 유도하는 단계(S1822)를 포함한다. 여기서, 본 개시의 일 실시예에 의하면, 상기 복수의 양자화 기법들은, 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 및 누적 지수 양자화 기법을 포함한다. 도20은 본 개시의 일 실시예에 따른, 복수의 클라이언트를 통해 연합 학습을 수행하는 딥러닝 네트워크 부호화 방법을 도시한 것이다. 상기 딥러닝 네트워크 부호화 방법은, 클라이언트에 의해 추가 학습된 업데이트 모델로 부터 기준 모델의 차이값인 잔차 정보를 생성하는 단계(S2010), 및 상기 잔차 정보를 양자화 하는 단계(S2020) 를 포함하되, 상기 잔차 정보를 양자화 하는 단계(S2020)는 표준 정규분포를 활용하여 기정의된 임계값을 기준 으로 잔차 정보의 중요도를 결정한 후, 상기 결정된 중요도를 기반으로 복수의 양자화 기법들 중 어느 하나 이상을 선택적으로 적용한다. 도21은 본 개시의 일 실시예에 따른, 딥러닝 네트워크 부호화를 위한, 양자화 방법을 도시한 것이다. 상기 양자 화 방법은, 양자화 대상인 잔차 정보의 평균과 표준편차를 활용하여 양자화 기법 선택을 위한 바이너리 플래그 를 결정하는 단계(S2110), 상기 잔차 정보를 표준 정규분포로 변환하는 단계(S2120), 및 상기 표준 정규분포 로 부터 상기 잔차 정보의 중요도를 결정하고, 상기 결정된 중요도와 상기 바이너리 플래그를 활용하여, 상기 잔차 정보를 제거 양자화 기법, 이항 양자화 기법, 삼항 양자화 기법 또는 누적 지수 양자화 기법 중 어느 하나 또는 이들의 조합을 적용하여 양자화 하는 단계(S2220)를 포함한다. 도22는 본 개시의 일 실시예에 따른, 딥러닝 네트워크 복호화를 위한, 역양자화 방법을 도시한 것이다. 상기 역 양자화 방법은, 부호화된 잔차 정보에 대한 양자화 정보를 획득하는 단계(S2210), 상기 양자화 정보로부터, 부 호화된 잔차 정보가 이항 양자화 기법, 삼항 양자화 기법, 또는 누적 지수 양자화 기법 중 어느 양자화 기법을 적용하였는 지를 유도하는 단계(S2220, 및 상기 확인 결과에 따라 해당 양자화 기법에 대응하는 역양자화 기법 을 적용하여 복원된 잔차 정보를 유도하는 단계(S2230)를 포함한다. 전술한 본 개시의 예시적인 방법들은 설명의 명확성을 위해서 동작의 시리즈로 표현되어 있지만, 이는 단계가 수행되는 순서를 제한하기 위한 것은 아니며, 필요한 경우에는 각각의 단계가 동시에 또는 상이한 순서로 수행 될 수도 있다. 본 개시에 따른 방법을 구현하기 위해서, 예시하는 단계에 추가적으로 다른 단계를 포함하거나, 일부의 단계를 제외하고 나머지 단계를 포함하거나, 또는 일부의 단계를 제외하고 추가적인 다른 단계를 포함할 수도 있다. 본 개시에 있어서, 소정의 동작(단계)을 수행하는 부호화 장치 또는 복호화 장치는 해당 동작(단계)의 수행 조 건이나 상황을 확인하는 동작(단계)을 수행할 수 있다. 예를 들어, 소정의 조건이 만족되는 경우 소정의 동작을 수행한다고 기재된 경우, 부호화 장치 또는 복호화 장치는 상기 소정의 조건이 만족되는지 여부를 확인하는 동 작을 수행한 후, 상기 소정의 동작을 수행할 수 있다. 본 개시의 다양한 실시예는 모든 가능한 조합을 나열한 것이 아니고 본 개시의 대표적인 양상을 설명하기 위한 것이며, 다양한 실시예에서 설명하는 사항들은 독립적으로 적용되거나 또는 둘 이상의 조합으로 적용될 수도 있 다. 또한, 본 개시의 다양한 실시예는 하드웨어, 펌웨어(firmware), 소프트웨어, 또는 그들의 결합 등에 의해 구현 될 수 있다. 하드웨어에 의한 구현의 경우, 하나 또는 그 이상의 ASICs(Application Specific Integrated Circuits), DSPs(Digital Signal Processors), DSPDs(Digital Signal Processing Devices), PLDs(Programmable Logic Devices), FPGAs(Field Programmable Gate Arrays), 범용 프로세서(general processor), 컨트롤러, 마이크로 컨트롤러, 마이크로 프로세서 등에 의해 구현될 수 있다. 또한, 본 개시의 실시예가 적용된 복호화 장치 및 부호화 장치는 멀티미디어 방송 송수신 장치, 모바일 통신 단 말, 홈 시네마 비디오 장치, 디지털 시네마 비디오 장치, 감시용 카메라, 비디오 대화 장치, 비디오 통신과 같 은 실시간 통신 장치, 모바일 스트리밍 장치, 저장 매체, 캠코더, 주문형 비디오(VoD) 서비스 제공 장치, OTT 비디오(Over the top video) 장치, 인터넷 스트리밍 서비스 제공 장치, 3차원(3D) 비디오 장치, 화상 전화 비디 오 장치, 및 의료용 비디오 장치 등에 포함될 수 있으며, 비디오 신호 또는 데이터 신호를 처리하기 위해 사용 될 수 있다. 예를 들어, OTT 비디오(Over the top video) 장치로는 게임 콘솔, 블루레이 플레이어, 인터넷 접속 TV, 홈시어터 시스템, 스마트폰, 태블릿 PC, DVR(Digital Video Recoder) 등을 포함할 수 있다. 도23은 본 개시에 따른 실시예가 적용될 수 있는 컨텐츠 스트리밍 시스템을 예시적으로 나타내는 도면이다. 도 23에 도시된 바와 같이, 본 개시의 실시예가 적용된 컨텐츠 스트리밍 시스템은 크게 인코딩 서버, 스트리밍 서 버, 웹 서버, 미디어 저장소, 사용자 장치 및 멀티미디어 입력 장치를 포함할 수 있다. 상기 인코딩 서버는 스마트폰, 카메라, CCTV 등과 같은 멀티미디어 입력 장치들로부터 입력된 컨텐츠를 디지털 데이터로 압축하여 비트스트림을 생성하고 이를 상기 스트리밍 서버로 전송하는 역할을 한다. 다른 예로, 스마 트폰, 카메라, CCTV 등과 같은 멀티미디어 입력 장치들이 비트스트림을 직접 생성하는 경우, 상기 인코딩 서버 는 생략될 수도 있다. 상기 비트스트림은 본 개시의 실시예가 적용된 부호화 방법 및/또는 부호화 장치에 의해 생성될 수 있고, 상기 스트리밍 서버는 상기 비트스트림을 전송 또는 수신하는 과정에서 일시적으로 상기 비트스트림을 저장할 수 있다. 상기 스트리밍 서버는 웹 서버를 통한 사용자 요청에 기반하여 멀티미디어 데이터를 사용자 장치에 전송하고, 상기 웹 서버는 사용자에게 어떠한 서비스가 있는지를 알려주는 매개체 역할을 할 수 있다. 사용자가 상기 웹 서버에 원하는 서비스를 요청하면, 상기 웹 서버는 이를 스트리밍 서버에 전달하고, 상기 스트리밍 서버는 사용 자에게 멀티미디어 데이터를 전송할 수 있다. 이때, 상기 컨텐츠 스트리밍 시스템은 별도의 제어 서버를 포함할 수 있고, 이 경우 상기 제어 서버는 상기 컨텐츠 스트리밍 시스템 내 각 장치 간 명령/응답을 제어하는 역할을 수행할 수 있다. 상기 스트리밍 서버는 미디어 저장소 및/또는 인코딩 서버로부터 컨텐츠를 수신할 수 있다. 예를 들어, 상기 인 코딩 서버로부터 컨텐츠를 수신하는 경우, 상기 컨텐츠를 실시간으로 수신할 수 있다. 이 경우, 원활한 스트리 밍 서비스를 제공하기 위하여 상기 스트리밍 서버는 상기 비트스트림을 일정 시간동안 저장할 수 있다. 상기 사용자 장치의 예로는, 휴대폰, 스마트 폰(smart phone), 노트북 컴퓨터(laptop computer), 디지털방송용 단말기, PDA(personal digital assistants), PMP(portable multimedia player), 네비게이션, 슬레이트 PC(slate PC), 태블릿 PC(tablet PC), 울트라북(ultrabook), 웨어러블 디바이스(wearable device, 예를 들어, 워치형 단말기 (smartwatch), 글래스형 단말기 (smart glass), HMD(head mounted display)), 디지털 TV, 데스 크탑 컴퓨터, 디지털 사이니지 등이 있을 수 있다. 상기 컨텐츠 스트리밍 시스템 내 각 서버들은 분산 서버로 운영될 수 있으며, 이 경우 각 서버에서 수신하는 데 이터는 분산 처리될 수 있다. 본 개시의 범위는 다양한 실시예의 방법에 따른 동작이 장치 또는 컴퓨터 상에서 실행되도록 하는 소프트웨어 또는 머신-실행가능한 명령들(예를 들어, 운영체제, 애플리케이션, 펌웨어(firmware), 프로그램 등), 및 이러한 소프트웨어 또는 명령 등이 저장되어 장치 또는 컴퓨터 상에서 실행 가능한 비-일시적 컴퓨터-판독가능 매체 (non-transitory computer-readable medium)를 포함한다."}
{"patent_id": "10-2022-0093959", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도1은 본 개시의 일 실시예에 따른, 딥러닝 네트워크의 완전 연결 계층(fully connected layer)을 나타낸 하나 의 예시로, 딥러닝 네트워크에서 발생하는 파라미터를 설명하기 위한 도면이다. 도2는 본 개시의 일 실시예에 따른, 연합 학습의 서비스 모델의 한 예를 설명하는 도면이다. 도3은 본 개시의 일 실시예에 따른, NNC(Compression of Neural Network for Multimedia Content Description and Analysis) 표준에서 다루고 있는 연합 학습에서의 잔차 정보 생성 과정을 설명하기 위한 도면이다. 도4는 본 개시의 일 실시예에 따른, 연합 학습에서 발생하는 잔차 정보의 일반적인 경향성을 설명하기 위한 도 면이다. 도5는 본 개시의 일 실시예에 따른, NNC 표준에서의 압축 과정을 설명하기 위한 NNC 부호화 및 복호화 장치를 도시한 것이다. 도6은 본 개시의 일 실시예에 따른, 연합 학습에서의 잔차 가중치를 위한 표준 정규분포 기반 양자화 기법의 과 정을 설명하기 위한 도면이다. 도7은 본 개시의 일 실시예에 따른, 차원 낮춤 과정(flattening process)을 설명하기 위한 도면이다. 도8은 본 개시의 일 실시예에 따른, 정규분포를 설명하기 위한 도면이다. 도9는 본 개시의 일 실시예에 따른, 바이너리 플래그(binary_flag)의 참/거짓(True/False)을 결정하는 기준을 시각적으로 보이기 위한 도면이다. 도10은 본 개시의 일 실시예에 따른, 바이너리 플래그의 참/거짓이 결정되는 과정을 설명하기 위한 도면이다. 도11은 본 개시의 일 실시예에 따른, 표준 정규분포를 설명하기 위한 도면이다. 도12는 본 개시의 일 실시예에 따른, 제거 양자화(pruning quantization)를 설명하기 위한 도면이다. 도13은 본 개시의 일 실시예에 따른, 이항-삼항 양자화(binary-ternary quantization)를 설명하기 위한 도면이 다. 도14는 본 개시의 일 실시예에 따른, 누적 지수 양자화(additive exponent quantization)를 설명하기 위한 도 면이다. 도15는 본 개시의 일 실시예에 따른, 누적 지수 양자화에 대한 구체적 예시를 도시화한 도면이다. 도16 및 도17은 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법을 이용한 딥러닝 네트워크 부호 화 방법을 도시한 것이다. 도18 및 도19는 본 개시의 일 실시예에 따른, 표준 정규분포 기반 양자화 기법으로 부호화된 딥러닝 네트워크 복호화 방법을 도시한 것이다 도20은 본 개시의 일 실시예에 따른, 복수의 클라이언트를 통해 연합 학습을 수행하는 딥러닝 네트워크 부호화 방법을 도시한 것이다. 도21은 본 개시의 일 실시예에 따른, 딥러닝 네트워크 부호화를 위한, 양자화 방법을 도시한 것이다. 도22는 본 개시의 일 실시예에 따른, 딥러닝 네트워크 복호화를 위한, 역양자화 방법을 도시한 것이다. 도23은 본 개시에 따른 실시예가 적용될 수 있는 컨텐츠 스트리밍 시스템을 예시적으로 도시한 것이다."}
