{"patent_id": "10-2024-0037375", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0012504", "출원번호": "10-2024-0037375", "발명의 명칭": "인코더 학습 방법 및 장치", "출원인": "현대자동차주식회사", "발명자": "황성웅"}}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "텍스트 및 음성을 포함하는 학습 데이터를 이용하여 음성변환 모델 - 상기 음성변환 모델은, 제1 인코더, 제2인코더, 제3 인코더, 하나 이상의 어텐션 모듈 및 음소길이 예측부를 포함함 - 을 학습시키기 위한 컴퓨터 구현방법에 있어서, 상기 제1 인코더를 이용하여 상기 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제1 출력데이터를 생성하는 과정;상기 제2 인코더를 이용하여 상기 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제2 출력데이터를 생성하는 과정;상기 제3 인코더를 이용하여 상기 학습 데이터의 음성으로부터 상기 음성의 특징을 나타내는 제3 출력데이터를생성하는 과정; 상기 하나 이상의 어텐션 모듈을 이용하여 상기 제1 출력데이터 및 상기 제3 출력데이터로부터 제1 유사도를 생성하고, 상기 제2 출력데이터 및 상기 제3 출력데이터로부터 제2 유사도를 생성하되, 상기 제1 유사도 및 상기제2 유사도는 상기 텍스트의 특징 및 상기 음성의 특징 간의 유사도인 과정;상기 음소길이 예측부를 이용하여 상기 제1 유사도, 상기 제2 유사도, 상기 제1 출력데이터 및 상기 제2 출력데이터로부터 상기 텍스트의 예측 음소 길이를 생성하는 과정; 및상기 제1 내지 제3 출력데이터 및 상기 예측 음소 길이 중 적어도 하나를 손실 함수에 적용하여, 상기 음성변환모델의 파라미터들 중 적어도 일부를 갱신하는 과정을 포함하는 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 음성변환 모델의 학습에 이용되지 않은 다른 언어의 데이터셋을 이용하여 상기 음성변환 모델을 미세조정하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 제1 인코더는 텍스트 음성 변환을 위한 트랜스포머 모델의 인코더이고, 상기 제2 인코더는 텍스트 음성 변환을 위한 tacotron 2 모델의 인코더인, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 제1 인코더 및 상기 제2 인코더는, SALN(Style-Adaptive Layer Normalization)을 이용하여 제1 출력데이터 및 제2 출력데이터를 생성하는 인코더인, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서,상기 유사도는, 공개특허 10-2025-0012504-3-상기 텍스트 및 상기 음성을 정렬(alignment)하기 위한 정보인, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 제1 출력데이터를 생성하는 과정, 및 상기 제2 출력데이터를 생성하는 과정은,어떤 언어를 수신하여 생성된 출력 데이터인지를 의미하는 언어 임베딩을 수행하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제1항에 있어서,상기 예측 음소 길이를 생성하는 과정은,상기 제1 유사도 및 상기 제2 유사도를 기초로, 상기 제1 출력데이터 및 상기 제2 출력데이터의 가중합을 계산하는 과정; 및상기 가중합에 기초하여 상기 예측 음소 길이를 생성하는 과정을 포함하는, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서,상기 학습 데이터는,데이터셋에 원-핫 인코딩, 문자 임베딩 및 위치 인코딩 중 선택된 적어도 하나 이상의 전처리 방법이 수행된 데이터인 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 음성변환 모델은, 상기 제3 출력데이터로부터 음성을 생성하는 디코더를 더 포함하고,상기 갱신하는 과정은, 상기 학습 데이터의 음성 및 상기 생성된 음성 사이의 손실에 더 기초하여 상기 음성변환 모델의 파라미터를 갱신하는, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제4항에 있어서,복수개의 언어들로 사전-학습된 상기 제1 인코더의 상기 SALN 및 상기 제2 인코더의 상기 SALN을 각각 레이어정규화(layer normalization)로 교체하는 과정; 및상기 복수개의 언어들과 구별되는 다른 언어를 이용하여 상기 음성변환 모델을 미세조정하는 과정을 더 포함하는, 방법."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "음성변환 모델 - 상기 음성변환 모델은, 제1 인코더, 제2 인코더, 제3 인코더, 하나 이상의 어텐션 모듈 및 음소길이 예측부를 포함함 - 을 학습시키기 위한 장치에 있어서명령어들을 저장하는 메모리; 및 적어도 하나 이상의 프로세서를 포함하되,상기 프로세서는 상기 명령어들을 실행함으로써,상기 제1 인코더를 이용하여 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제1 출력데이터를 생성하고,상기 제2 인코더를 이용하여 상기 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제2 출력데이터공개특허 10-2025-0012504-4-를 생성하고,상기 제3 인코더를 이용하여 상기 학습 데이터의 음성으로부터 상기 음성의 특징을 나타내는 제3 출력데이터를생성하고, 상기 하나 이상의 어텐션 모듈을 이용하여 상기 제1 출력데이터 및 상기 제3 출력데이터로부터 제1 유사도를 생성하고, 상기 제2 출력데이터 및 상기 제3 출력데이터로부터 제2 유사도를 생성하되, 상기 제1 유사도 및 상기제2 유사도는 상기 텍스트의 특징 및 상기 음성의 특징 간의 유사도이고,상기 음소길이 예측부를 이용하여 상기 제1 유사도, 상기 제2 유사도, 상기 제1 출력데이터 및 상기 제2 출력데이터로부터 상기 텍스트의 예측 음소 길이를 생성하며,상기 제1 내지 제3 출력데이터 및 상기 예측 음소 길이 중 적어도 하나를 손실 함수에 적용하여, 상기 음성변환모델의 파라미터들 중 적어도 일부를 갱신하는, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서,상기 프로세서는,상기 음성변환 모델의 학습에 이용되지 않은 다른 언어의 데이터셋을 이용하여 상기 음성변환 모델을 미세조정하는, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제11항에 있어서,상기 제1 인코더는 텍스트 음성 변환을 위한 트랜스포머 모델의 인코더이고, 상기 제2 인코더는 텍스트 음성 변환을 위한 tacotron 2 모델의 인코더인 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제11항에 있어서,상기 제1 인코더 및 상기 제2 인코더는, SALN을 이용하여 상기 제1 출력데이터 및 상기 제2 출력데이터를 생성하는 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제11항에 있어서,상기 유사도는, 상기 텍스트 및 상기 음성을 정렬(alignment)하기 위한 정보인, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제11항에 있어서,상기 제1 출력데이터 및 상기 제2 출력데이터는, 어떤 언어를 수신하여 생성된 출력데이터인지를 의미하는 언어 임베딩을 포함하는, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제11항에 있어서,상기 음소길이 예측부는,상기 제1 유사도 및 상기 제2 유사도를 기초로 상기 제1 출력데이터 및 상기 제2 출력데이터의 가중합을 계산하고, 상기 계산된 가중합을 이용하여, 상기 예측 음소 길이를 생성하는, 장치.공개특허 10-2025-0012504-5-청구항 18 제11항에 있어서, 상기 학습 데이터는, 데이터셋에 원-핫 인코딩, 문자 임베딩 및 위치 인코딩 중 선택된 적어도 하나 이상의 전처리 방법이 수행된 데이터인, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제11항에 있어서,상기 음성변환 모델은, 상기 제3 출력데이터로부터 음성을 생성하는 디코더를 더 포함하고,상기 프로세서는, 상기 학습 데이터의 음성 및 상기 생성된 음성 사이의 손실에 더 기초하여 상기 음성변환 모델의 파라미터를 갱신하는, 장치."}
{"patent_id": "10-2024-0037375", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제14항에 있어서,상기 프로세서는,복수개의 언어들로 사전-학습된 상기 제1 인코더의 상기 SALN 및 상기 제2 인코더의 상기 SALN을 각각 레이어정규화(layer normalization)로 교체하고;상기 음성변환 모델의 학습에 이용되지 않은 다른 언어의 데이터셋을 이용하여 상기 음성변환 모델을 미세조정하는, 장치."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 음성변환 모델을 학습시키기 위한 컴퓨터 구현방법이 제안된다. 제안된 방법은 제1 인코더(예컨대, flow 기반 모델의 인코더) 및 제2 인코더(예컨대, tacotron 2 모델의 인코더)를 병렬로 학습시킴 으로 문맥에 맞는 자연스러운 음성 생성이 가능하면서도 발음 정확도를 높일 수 있다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 인코더 학습 방법 및 장치에 관한 것이다. 더욱 상세하게는, 음성 합성 시스템의 발음 개선을 위한 인코더 학습 방법 및 장치에 관한 것이다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이하에 기술되는 내용은 단순히 본 실시예와 관련되는 배경 정보만을 제공할 뿐 종래기술을 구성하는 것이 아니 다. TTS(text-to-speech)는 컴퓨터를 이용하여, 텍스트를 음성으로 변환하는 기술을 말한다. TTS는 텍스트를 이용하 여 정보를 전달하기 어려운 상황에서 사용자 접근성(accessibility)을 향상시키기 위해 개발됐다. 예컨대, 차량 의 운전자는 전방을 주시해야 하기 때문에 텍스트로 제공되는 정보를 용이하게 전달받기 어렵다. 따라서, 운전 자에게 TTS를 이용하여 정보를 제공할 필요가 있다. 인공지능(artificial intelligence)의 발달에 따라, 인공지능, 또는 인공신경망(artificial neural network)을 이용한 TTS 모델이 개발되고 있다. 인공신경망을 이용한 TTS 모델은 인코더, 어텐션 및 디코더를 포함하여 구성 된다. 인코더 및 디코더의 구조에 따라 TTS 모델이 구별된다. 도 1은 종래의 flow 기반 모델(flow-based model)의 인코더를 도시한 것이다. 도 1을 참조하면, 종래의 flow 기반 모델의 인코더는 멀티 헤드 셀프 어텐션(multi-head self-attention, 100), 복수의 잔차 연결 및 레이어 정규화들(residual connections and layer normalizations, 110), 및 FFNN(feed-forward neural network, 120)를 포함한다. 종래의 flow 기반 모델의 예로는 Glow-TTS 모델이 있다. Glow-TTS 모델의 인코더는 트랜스포머(transformer) 모델의 인코더와 구조가 유사하다. flow 기반 모델은 문장 내의 음소 간 유사도(similarity between phoneme s)를 찾도록 학습한다. 따라서, 문맥에 맞는 자연스러운 음성 생성이 가능한 장점이 있다. 그러나, flow 기반 모델은 발음 정확도(accuracy of pronunciation)가 떨어지는 문제점이 있다.도 2는 종래의 tacotron 2 모델의 인코더를 도시한 것이다. 도 2를 참조하면, tacotron 2 모델의 인코더는 임베딩 행렬(embedding matrix, 200), 복수의 CNN 및 배치 정규 화들(convolution neural networks and batch normalizations, 210) 및 LSTM을 포함한다. Tacotron 2 모델을 이용한 TTS는 flow 기반 모델보다 정확한 발음을 가지는 음성을 생성하는 것으로 알려져 있 다. 그러나, tacotron 2 모델은 생성된 음성이 자연스럽지 못하다는 단점이 있다. 이에 flow 기반 모델 및 tacotron 2 모델의 장점들을 유지하면서도 단점들을 보완하기 위한 인코더 학습 방법이 필요하다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는, 문맥에 맞는 자연스러운 음성 생성이 가능하면서도 발음 정확도를 높인 TTS를 위한 인코더 학습 방 법을 제공하는 데 주된 목적이 있다. 본 개시는, 한국어 음소에 강인한 인코더 학습 방법을 제공하는 데 다른 목적이 있다. 본 개시는, 다양한 언어로 확장 가능한 다국어 음소 임베딩을 기반으로 하는 학습 방법을 제공하는 데 다른 목 적이 있다. 본 발명이 해결하고자 하는 과제들은 이상에서 언급한 과제들로 제한되지 않으며, 언급되지 않은 또 다른 과제 들은 아래의 기재로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 일 측면에 의하면, 텍스트 및 음성을 포함하는 학습 데이터를 이용하여 음성변환 모델 - 상기 음성변 환 모델은, 제1 인코더, 제2 인코더, 제3 인코더, 하나 이상의 어텐션 모듈 및 음소길이 예측부를 포함함 - 을 학습시키기 위한 컴퓨터 구현방법에 있어서, 상기 제1 인코더를 이용하여 상기 학습 데이터의 텍스트로부터 상 기 텍스트의 특징을 나타내는 제1 출력데이터를 생성하는 과정; 상기 제2 인코더를 이용하여 상기 학습 데이터 의 텍스트로부터 상기 텍스트의 특징을 나타내는 제2 출력데이터를 생성하는 과정; 상기 제3 인코더를 이용하여 상기 학습 데이터의 음성으로부터 상기 음성의 특징을 나타내는 제3 출력데이터를 생성하는 과정; 상기 하나 이 상의 어텐션 모듈을 이용하여 상기 제1 출력데이터 및 상기 제3 출력데이터로부터 제1 유사도를 생성하고, 상기 제2 출력데이터 및 상기 제3 출력데이터로부터 제2 유사도를 생성하되, 상기 제1 유사도 및 상기 제2 유사도는 상기 텍스트의 특징 및 상기 음성의 특징 간의 유사도인 과정; 상기 음소길이 예측부를 이용하여 상기 제1 유사 도, 상기 제2 유사도, 상기 제1 출력데이터 및 상기 제2 출력데이터로부터 상기 텍스트의 예측 음소 길이를 생 성하는 과정; 및 상기 제1 내지 제3 출력데이터 및 상기 예측 음소 길이 중 적어도 하나를 손실 함수에 적용하 여, 상기 음성변환 모델의 파라미터들 중 적어도 일부를 갱신하는 과정을 포함하는 방법을 제공한다. 본 개시의 다른 측면에 의하면, 음성변환 모델 - 상기 음성변환 모델은, 제1 인코더, 제2 인코더, 제3 인코더, 하나 이상의 어텐션 모듈 및 음소길이 예측부를 포함함 - 을 학습시키기 위한 장치에 있어서 명령어들을 저장하 는 메모리; 및 적어도 하나 이상의 프로세서를 포함하되, 상기 프로세서는 상기 명령어들을 실행함으로써, 상기 제1 인코더를 이용하여 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제1 출력데이터를 생성하고, 상기 제2 인코더를 이용하여 상기 학습 데이터의 텍스트로부터 상기 텍스트의 특징을 나타내는 제2 출력데이터를 생성하고, 상기 제3 인코더를 이용하여 상기 학습 데이터의 음성으로부터 상기 음성의 특징을 나 타내는 제3 출력데이터를 생성하고, 상기 하나 이상의 어텐션 모듈을 이용하여 상기 제1 출력데이터 및 상기 제 3 출력데이터로부터 제1 유사도를 생성하고, 상기 제2 출력데이터 및 상기 제3 출력데이터로부터 제2 유사도를 생성하되, 상기 제1 유사도 및 상기 제2 유사도는 상기 텍스트의 특징 및 상기 음성의 특징 간의 유사도이고, 상기 음소길이 예측부를 이용하여 상기 제1 유사도, 상기 제2 유사도, 상기 제1 출력데이터 및 상기 제2 출력데 이터로부터 상기 텍스트의 예측 음소 길이를 생성하며, 상기 제1 내지 제3 출력데이터 및 상기 예측 음소 길이 중 적어도 하나를 손실 함수에 적용하여, 상기 음성변환 모델의 파라미터들 중 적어도 일부를 갱신하는, 장치를 제공한다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 개시의 실시예에 의하면, 제1 인코더(예컨대, flow 기반 모델의 인코더) 및 제2 인코더(예컨대, tacotron 2 모델의 인코더)를 병렬로 학습시킴으로 문맥에 맞는 자연스러운 음성 생성이 가능하면서도 발음 정확도를 높인TTS를 위한 인코더 학습 방법을 제공할 수 있다는 효과가 있다. 본 개시의 실시예에 의하면, 한국어 데이터셋을 이용한 미세조정을 수행함으로써 한국어 음소에 강인한 인코더 학습 방법을 제공할 수 있다는 효과가 있다. 본 개시의 실시예에 의하면, 언어 임베딩 및 다양한 언어의 데이터셋으로 인코더를 학습시킴으로써 다양한 언어 로 확장 가능한 다국어 음소 임베딩을 기반으로 하는 학습 방법을 제공할 수 있다는 효과가 있다. 본 개시의 효과들은 이상에서 언급한 효과들로 제한되지 않으며, 언급되지 않은 또 다른 효과들은 아래의 기재 로부터 통상의 기술자에게 명확하게 이해될 수 있을 것이다."}
{"patent_id": "10-2024-0037375", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하, 본 개시의 일부 실시예들을 예시적인 도면을 이용해 상세하게 설명한다. 각 도면의 구성 요소들에 참조 부호를 부가함에 있어서, 동일한 구성 요소들에 대해서는 비록 다른 도면 상에 표시되더라도 가능한 한 동일한 부호를 가지도록 하고 있음에 유의해야 한다. 또한, 본 개시를 설명함에 있어, 관련된 공지 구성 또는 기능에 대한 구체적인 설명이 본 개시의 요지를 흐릴 수 있다고 판단되는 경우에는 그 상세한 설명은 생략한다. 본 개시에 따른 실시예의 구성요소를 설명하는 데 있어서, 제1, 제2, i), ii), a), b) 등의 부호를 사용할 수 있다. 이러한 부호는 그 구성요소를 다른 구성 요소와 구별하기 위한 것일 뿐, 그 부호에 의해 해당 구성요소의 본질 또는 차례나 순서 등이 한정되지 않는다. 명세서에서 어떤 부분이 어떤 구성요소를 '포함' 또는 '구비'한 다고 할 때, 이는 명시적으로 반대되는 기재가 없는 한 다른 구성요소를 제외하는 것이 아니라 다른 구성요소를 더 포함할 수 있는 것을 의미한다. 첨부된 도면과 함께 이하에 개시될 상세한 설명은 본 개시의 예시적인 실시형태를 설명하고자 하는 것이며, 본 개시가 실시될 수 있는 유일한 실시형태를 나타내고자 하는 것이 아니다. 이하에 사용된 단수형의 용어는 달리 명시되지 않는 한 복수형을 포함할 수 있다. 도 3은 본 개시의 일 실시예에 따른 음성변환 모델을 개략적으로 도시한 블록구성도이다. 도 3을 참조하면, 인코더 학습장치(encoder training apparatus)는 음성변환 모델(speech transformation model, 30), 학습부(training unit, 370) 및 튜닝부(tuning unit, 380)를 전부 또는 일부 포함한다. 음성변환 모델은 전처리부(preprocessing unit, 310), 제1 인코더(first encoder, 320), 제2 인코더(second encoder, 330), 제3 인코더(third encoder, 340), 어텐션 모듈(attention module, 350), 및 음소길이 예측부 (phenome duration prediction unit, 360)를 전부 또는 일부 포함할 수 있다. 도 3에 도시된 구성요소들은 기 능적으로 구분되는 요소들을 나타낸 것으로서, 적어도 하나의 구성요소가 실제 물리적 환경에서는 서로 통합되 는 형태로 구현될 수도 있다. 도 3에 도시된 구성요소들 각각은 인공신경망(artificial neural network)을 포함할 수 있다. 인공신경망은 예 컨대, RNN(recurrent neural network), FFNN(feed-forward neural network) 또는 CNN(convolution neural network)을 포함할 수 있지만 반드시 이에 한하지 않는다. 음성변환 모델은 데이터셋(dataset)을 수신한다. 여기서, 데이터셋은 TTS(text-to-speech) 모델을 학습시키 기 위한, 음성 및 음성에 대응하는 텍스트를 포함하는 데이터의 그룹을 말한다. 데이터 셋 내의 음성 및 텍스트 는 다양한 언어로 표현된 것 일 수 있다. 예컨대, 음성변환 모델은 한국어(Korean), 영어(English), 일본어 (Japanese), 프랑스어(French), 중국어(Chinese), 독일어(German) 등의 언어들로 표현된 다국어 데이터셋을 수 신할 수 있다. 데이터셋이 지원하는 언어들은 설명을 위한 예시로서, 이에 한하지 않고 다양한 언어들을 더 포 함할 수 있다. 데이터셋에 포함된 음성은, 음성 신호, 멜-스펙트로그램 및/또는 선형 스케일 스펙트로그램의 형 태일 수 있다. 멜 스펙트로그램, 또는 선형 스케일 스펙트로그램은 음성 신호를 시간에 대한 주파수(frequency)및 강도(amplitude)로서 표현한 것이다. 일부 실시예들에서, 음성 신호는 음성 파형으로 지칭될 수도 있다. 전처리부는 수신한 데이터셋에 포함된 텍스트를 전처리(preprocessing)하여 학습 데이터를 생성한다. 전처 리부는 텍스트를 음소(phoneme) 단위로 분해한다. 음소는 언어 사용자가 인식하는 소리의 최소 단위를 말 한다. 전처리부는 분해한 음소를 원-핫 인코딩(one-hot encoding)한다. 원-핫 인코딩은 어떤 행렬 내에서 숫자 0 과 1의 위치를 이용하여 데이터를 나타내는 인코딩 방법이다. 즉, 원-핫 인코딩을 이용하면 음소 데이터를 숫자 데이터로 변환할 수 있다. 원-핫 인코딩은 이미 공지된 방법이므로, 이에 대한 자세한 설명은 생략한다. 전처리부는 원-핫 인코딩한 음소에 대해 문자 임베딩(character embedding)을 수행 한다. 문자 임베딩은 각각의 문자를 이에 대응하는 벡터로 변환하는 것이다. 원-핫 인코딩된 음소는 문자 임베딩을 이용하여 학습에 적합한 차원(dimension)의 벡터로 변환된다. 문자 임베딩을 수행하는 과정은 위치 인코딩(position encoding)을 수행하는 과정을 포함할 수 있다. 위치 인코 딩은 입력된 텍스트에 포함된 특정 단어의 위치정보를 문자 임베딩을 수행한 벡터에 추가하는 방법이다. 문자 임베딩 및 위치 인코딩한 벡터는 각 단어가 입력된 문장의 어떤 위치에 있었는지에 대한 정보를 포함한다. 본 개시의 일 실시예에 따르면, 전처리부는 절대적 위치 인코딩 또는 상대적 위치 표현을 수행할 수 있다. 전처리부는 학습 데이터(learning data)를 출력한다. 여기서, 학습 데이터는 전처리부가 텍스트에 원 -핫 인코딩, 문자 임베딩, 및 위치 인코딩 중 선택된 전부 또는 일부를 수행한 결과 데이터를 의미한다. 제1 인코더 및 제2 인코더는 학습 데이터를 수신하고 학습 데이터로부터 텍스트의 특징을 추출하여, 특징 벡터(feature vector)를 포함하는 데이터를 각각 출력한다. 여기서, 제1 인코더가 출력한 데이터를 제1 출력데이터, 제2 인코더가 출력한 데이터를 제2 출력데이터라 한다. 즉, 제1 출력데이터 및 제2 출력 데이터는 텍스트로부터 추출된 특징을 포함한다. 제1 인코더는 도 1에 도시된 것과 같은 플로우 기반 모델에 포함된 인코더를 포함할 수 있다. 제1 인코더 는 예컨대, 트랜스포머(transformer) 모델에 포함된 인코더일 수 있다. 제2 인코더는 도 2에 도시된 것과 같은 Tacotron 2 모델에 포함된 인코더일 수 있다. 제1 인코더는 레이어 정규화(layer normalization, 110) 대신 Style-Adaptive Layer Normalization(이하, “SALN”)을 포함할 수 있다. SALN는 스타일 벡터에 기초한 정규화된 입력 특징들의 gain 또는 bias를 적응적으 로(adaptively) 쉬프팅(shifting) 또는 스케일링(scaling)하기 위한 정규화 방법이다. SALN은 이미 공지된 것 이므로, 이에 대한 자세한 설명은 생략한다. 본 개시에 따른 SALN은 언어 특징에 의해(according to language feature) gain 또는 bias가 변화한다. SALN에 비해, 제1 인코더에 포함된 레이어 정규화의 gain 또는 bias는 고정(fix)되어 있다. 다시 말해, 레이어 정규화의 gain 또는 bias는 학습에 적합한 특정 값을 가진다. 제2 인코더는 배치 정규화(batch normalization, 210) 대신 SALN을 포함할 수 있다. SALN에 비해, 배치 정규화의 gain 또는 bias는 고정되어 있다. 다시 말해, 배치 정규화의 gain 또는 bias는 학습에 적합한 특정 값 을 가진다. SALN은 레이어 정규화 및 배치 정규화에 비해 다양한 언어에 강인한(robust) 장점을 가진다. 제1 인코더 및 제2 인코더는 언어 임베딩(language embedding)을 수행한다. 언어 임베딩은 데이터셋 에 포함된 언어에 관한 정보를 벡터로 표현하는 것이다. 언어 임베딩을 수행함으로써, 제1 출력데이터 및 제2 출력데이터는 입력된 데이터가 어떤 언어를 포함하는지에 대한 정보를 포함할 수 있다. 또한, 언어 임베딩을 수 행함으로써, 제1 출력데이터 및 제2 출력데이터에 포함된 각 값이 어떤 언어를 이용하여 출력된 것인지에 대한 정보가 제1 출력데이터 및 제2 출력데이터에 포함될 수 있다. 제3 인코더는 데이터셋에 포함된 음성을 수신하고, 음성의 특징 데이터를 추출한다. 음성의 특징 데이터는 잠재 변수들 및 잠재 변수들의 사전 분포를 포함한다. 본 개시의 일 실시예에 따른 제3 인코더는 예컨대, non-casual WaveNet residual block을 포함할 수 있다. 제3 인코더는 음성의 특징 데이터에 포함된 사전 분포를 변환한다. 제3 인코더는 Normalizing flow 함수를 이용하여 음성의 특징 데이터에 포함된 사전 분포를 변환할 수 있다. 제3 인코더는 음성의 특징 데 이터를 Normalizing flow 함수에 적용한 결과를 제3 출력데이터로 생성한다. 제3 인코더는 가역성 (reversibility)을 갖는 플로우-기반(flow-based) 모델일 수 있다. 예컨대, 제3 인코더는 학습 과정에서는 데이터셋에 포함된 음성(즉, 정답 음성)을 잠재 변수(들)(latent variable(s))로 변환하는 반면, 추론 과정 에서 제3 인코더는 잠재 변수들을 음성으로 변환할 수 있다. 다른 예에서, 잠재 변수(들)를 음성으로 변환 하도록 구성된 디코더(미도시)가 구비될 수도 있다. 디코더는 훈련과정에서는 제3 인코더의 출력(즉, 제3 출력 데이터)을 음성으로 변환하는 반면, 추론과정에서는 음성변환 모델의 출력을 음성으로 변환할 수 있다. 본 개시의 일 실시예에 따른 음성변환 모델은 적어도 하나 이상의 어텐션 모듈을 포함한다. 하나의 어 텐션 모듈은 제1 출력데이터, 제2 출력데이터 및 제3 출력데이터 중 적어도 하나 이상을 수신한다. 본 개 시의 일 실시예에 따르면 제1 어텐션 모듈은 제1 출력데이터 및 제3 출력데이터를 수신하고, 제2 어텐션 모듈은 제2 출력데이터 및 제3 출력데이터를 수신한다. 어텐션 모듈은 출력데이터들을 이용하여, 음성과 텍스트 간의 유사도(similarity)를 계산하여 생성한다. 여기서 유사도는 텍스트의 어떤 부분이 음성의 어떤 부분과 유사한지에 관한 값이다. 복수 개의 어텐션 모듈 각각은 제1 인코더 및 제2 인코더 각각의 특성을 반영하여, 음성 데이터와 텍스트의 유사도를 각각 생성한다. 제1 어텐션 모듈이 생성한 유사도를 제1 유사도라 하고, 제2 어텐션 모듈이 생성한 유사도를 제 2 유사도라 한다. 전처리부에서의 위치 인코딩에 의해, 제1 출력데이터 및 제2 출력데이터는 각 음소의 위치에 관한 정보를 포함할 수 있다. 어텐션 모듈은 텍스트에 포함된 각 음소의 위치에 기초하여, 텍스트에 포함된 각 음소와 음성에 포함된 각 음소의 유사도를 계산할 수 있다. 즉, 텍스트와 음성을 정렬(alignment)할 수 있다. 어텐션 모듈은 유사도를 벡터 또는 행렬의 형식으로 출력할 수 있다. 일 실시예에서, 유사도는 확률로서 표현될 수 있다. 이와 달리, 유사도는 어텐션 스코어(attention score)로서 표현될 수 있다. 유사도는 텍스트 및 음성에 포함된 각 음소의 길이에 대한 정보를 포함할 수 있다. 본 개시에 따른 어텐션 모듈은 다양한 종류의 어텐션 네트워크를 포함할 수 있다. 음소길이 예측부는 제1 출력데이터 및 제2 출력데이터에 가중치를 각각 적용할 수 있다. 가중치는 유사도 를 이용하여 계산될 수 있다. 다시 말해, 음소길이 예측부는 제1 유사도에 따른 가중치가 적용된 제1 출력 데이터와 제2 유사도에 따른 가중치가 적용된 제2 출력데이터를 더한 가중합(weighted sum)을 계산할 수 있다. 음소길이 예측부는 가중합에 기초하여 텍스트에 포함된 음소의 길이를 예측한다. 즉, 음소길이 예측부 는 가중합에 기초하여 텍스트의 예측 음소 길이를 생성하고, 출력한다. 학습부는 음소길이 예측부가 예측한 음소 길이, 제1 출력데이터, 제2 출력데이터 및 제3 출력데이터 중 적어도 하나 이상을 이용하여 음성변환 모델을 훈련시킨다. 더욱 상세하게는, 학습부는 예측 음소 길이, 제1 출력데이터, 제2 출력데이터 및 제3 출력데이터 중 적어도 하나 이상을 손실 함수에 적용하고, 손실 함수가 감소하는 방향으로 제1 인코더, 제2 인코더, 제3 인코더 및 음소길이 예측부 중 적 어도 하나 이상의 파라미터를 적어도 하나 이상 업데이트할 수 있다. 일 예로, 학습부는 정답 음성으로부 터 변환된 잠재변수들(또는 이들의 분포)과 텍스트로부터 변환된 잠재변수들(또는 이들의 분포) 간의 차이가 최 소화되도록 음성변환 모델의 파라미터를 업데이트할 수 있다. 정답 음성으로부터 변환된 잠재변수들의 분포 는, 예컨대, 제3 출력 데이터를 기초로 결정될 수 있다. 텍스트로부터 변환된 잠재변수들은, 예컨대, 제1 출력 데이터, 제2 출력 데이터 및/또는 이들의 가중합을 기초로 결정될 수 있다. 다른 예로, 학습부는 정답 음 성으로부터 계산된 음소 길이와 음소길이 예측부에 의해 예측된 음소 길이 간의 차이가 최소화되도록 음성 변환 모델의 파라미터를 업데이트 할 수 있다. 잠재변수들 간의 정렬(alignment) 및/또는 정답 음성에 대한 음소 길이의 계산에는 단조 정렬 검색(Monotonic Alignment Search, MAS)이 이용될 수 있으나 이러한 예시에 한 정되는 것은 아니다. 다른 예에서, 정답 음성의 각 음소의 길이에 대한 정보가 학습 데이터셋에 미리 포함되어 있을 수 있다. 한편, 음성변환 모델이 별도의 디코더를 더 포함하는 경우, 학습부는 정답 음성 및 디 코더에 의해 생성된 음성과 사이의 손실에 더 기초하여 음성변환 모델의 파라미터를 갱신할 수 있다. 튜닝부 는 음성변환 모델을 미세조정(fine-tuning)한다. 튜닝부는 음성변환 모델의 학습에 이용되지 않은 언어의 데이터셋을 이용하여 미세조정을 수행할 수 있다. 예컨대, 한국어에 대한 데이터(즉, 텍스트-정답 음성의 쌍)가 한정되어 있는 경우, 다량의 데이터가 구비되어 있는 다른 언어들(예컨대, 영어, 일본어, 중국어, 프랑스어 및 독일어)로 학습을 먼저 진행한 후, 한국어에 대해 미세조정을 수행할 수 있다. 여기서 미세조정은 기 학습된(pre-training) 모델의 파라미터(parameter) 중 일부만을 변화시키는 학습 과정을 이용하여 기 학습된 모델을 조정(tuning)하는 기법(technique)이다. 기 학습된 모델로부터 일부 파라미터만을 조정하기 때문에 미세 조정에 시간이 적게 소요된다. 미세조정을 수행함으로써 모델의 성능을 향상시킬 수 있다. 튜닝부는 레이어 정규화를 이용하여 미세조정을 수행할 수 있다. 예컨대, 튜닝부는 기 학습된 제1 인코더 및 제2 인코더의 SALN를 레이어 정규화로 교체한 이후에, 미세조정을 수행할 수 있다. 튜닝부는 제1 인코더, 제2 인코더, 및 제3 인코더에 한국어 데이터셋을 입력하여 미세조정 을 수행할 수 있다. 도 3에는 제1 인코더, 제2 인코더, 제3 인코더 및 어텐션 모듈이 하나씩 도시되었지만, 본 개시에 따른 음성변환 모델은 복수의 제1 인코더, 제2 인코더, 제3 인코더 및 어텐션 모듈 을 각각 포함할 수 있다. 도 4는 본 개시의 일 실시예에 따른 음성변환 모델을 학습시키는 과정을 도시한 순서도이다. 도 4를 참조하면, 음성변환 모델은 데이터셋을 수신한다(S400). 데이터셋은 다양한 언어를 포함할 수 있다. 예컨대, 데이터셋은 한국어, 영어, 일본어, 중국어, 프랑스어 및 독일어를 전부 또는 일부 포함할 수 있다. 전처리부는 수신한 데이터셋을 전처리하여 학습 데이터를 생성한다(S410). 전처리부는 데이터셋을 음 소 단위로 분해한다. 전처리부는 음소 단위로 분해한 데이터셋 중 텍스트를 원-핫 인코딩한다. 전처리부 는 원-핫 인코딩한 데이터셋을 문자 임베딩한다. 전처리부는 문자 임베딩한 데이터셋을 위치 인코딩 할 수 있다. 전처리 과정을 거친 데이터셋은 학습에 적합한 차원의 벡터로 변환된다. 전처리부는 학습 데 이터를 출력한다. 학습 데이터는 벡터의 형태로 표현될 수 있다. 제1 인코더 및 제2 인코더는 학습 데이터를 수신한다. 제1 인코더 및 제2 인코더는 SALN을 포함하는 인코더일 수 있다. 제1 인코더 및 제2 인코더는 텍스트의 특징 벡터를 포함하는 제1 출력데 이터 및 제2 출력데이터를 각각 출력한다(S420). 제1 인코더 및 제2 인코더는 출력한 데이터에 언어 임베딩을 수행할 수 있다. 다시 말해, 제1 인코더 및 제2 인코더는 학습 데이터를 수신하고 텍스트의 특징을 추출한 뒤 추출한 특징에 언어 임베딩을 수행하여, 제1 출력데이터 및 제2 출력데이터로 최종 출력한다. 제3 인코더는 학습 데이터를 수신한다. 제3 인코더는 수신한 데이터셋 중 음성 데이터로부터 특징을 추출하고, 추출한 특징의 사전 분포를 변환하여 제3 출력데이터를 출력한다(S430). 제3 출력데이터는 사전 분포 가 변환된 잠재 변수들을 포함한다. 제3 출력데이터는 Normalizing flow에 의해 사전 분포가 변환된 데이터일 수 있다. 어텐션 모듈은 제1 출력데이터, 또는 제2 출력데이터, 및 제3 출력데이터를 수신한다. 즉, 어텐션 모듈은 제1 출력데이터 및 제3 출력데이터, 및 제2 출력데이터 및 제3 출력데이터를 각각 수신한다. 어텐션 모듈 은 수신한 데이터를 이용하여 유사도를 계산하고 출력한다(S440). 어텐션 모듈은 유사도를 벡터 또는 행렬 의 형태로서 출력할 수 있다. 음소길이 예측부는 유사도를 이용하여 가중치를 계산한다. 음소길이 예측부는 제1 출력데이터 및 제2 출력데이터에 가중치를 각각 곱하여 가중합을 계산한다. 음소길이 예측부는 가중합을 이용하여 텍스트의 음소의 길이를 예측한다(S450). 다시 말해, 음소길이 예측부는 제1 출력데이터 및 제2 출력데이터에 가중 치를 각각 더한 데이터를 입력받아 음소의 길이를 예측한다. 학습부는 제1 인코더, 제2 인코더, 제3 인코더 및 음소길이 예측부의 출력을 이용하 여, 제1 인코더, 제2 인코더, 제3 인코더 및 음소길이 예측부 중 적어도 하나 이상을 학습 시킨다(S460). 튜닝부는 학습이 완료된 제1 인코더, 제2 인코더, 제3 인코더 및 음소길이 예측부 중 적어도 하나 이상을 미세조정한다(S470). 튜닝부는 레이어 정규화를 이용하여 제1 인코더, 제2 인코 더, 제3 인코더 및 음소길이 예측부를 미세조정할 수 있다. 도 4의 순서도에는 S400 내지 S470 과정이 순차적으로 진행되는 것으로 도시되어 있지만, 반드시 도 4에 기재된 순서대로 인코더의 학습이 진행되어야만 하는 것은 아니다. 예를 들어, 도 4에는 S420 과정 이후 S430 과정이 진행되도록 도시되어 있지만, S420 과정 및 S430 과정의 순서는 서로 바뀌거나, 동시에 진행될 수 있다. 본 개시의 일 실시예에 따른 제1 인코더 및 제2 인코더는 SALN을 이용하여 학습된다. 따라서, 다양한 언어에 강인한 특성을 갖는다. 한국어 데이터셋을 이용하여 미세조정된 제1 인코더, 제2 인코더 및 음소길이 예측부는 한국어에 강인한 특성을 갖는다. 본 개시에 따른 학습된 제1 인코더, 제2 인코더 및 음소길이 예측부는 TTS 장치 또는 TTS 방법에 이용될 수 있다. 예컨대, 학습된 제1 인코더,제2 인코더 및 음소길이 예측부를 flow 기반 TTS 모델의 텍스트 인코더 및 음소길이 예측부 대신 이 용하여, 텍스트로부터 음성을 생성할 수 있다. 본 발명에 따른 장치 또는 방법의 각 구성요소는 하드웨어 또는 소프트웨어로 구현되거나, 하드웨어 및 소프트 웨어의 결합으로 구현될 수 있다. 또한, 각 구성요소의 기능이 소프트웨어로 구현되고 마이크로프로세서가 각 구성요소에 대응하는 소프트웨어의 기능을 실행하도록 구현될 수도 있다. 본 명세서에 설명되는 시스템들 및 기법들의 다양한 구현예들은, 디지털 전자 회로, 집적회로, FPGA(field programmable gate array), ASIC(application specific integrated circuit), 컴퓨터 하드웨어, 펌웨어, 소프 트웨어, 및/또는 이들의 조합으로 실현될 수 있다. 이러한 다양한 구현예들은 프로그래밍가능 시스템 상에서 실 행 가능한 하나 이상의 컴퓨터 프로그램들로 구현되는 것을 포함할 수 있다. 프로그래밍가능 시스템은, 저장 시 스템, 적어도 하나의 입력 디바이스, 그리고 적어도 하나의 출력 디바이스로부터 데이터 및 명령들을 수신하고 이들에게 데이터 및 명령들을 전송하도록 결합되는 적어도 하나의 프로그래밍가능 프로세서(이것은 특수 목적 프로세서일 수 있거나 혹은 범용 프로세서일 수 있음)를 포함한다. 컴퓨터 프로그램들(이것은 또한 프로그램들, 소프트웨어, 소프트웨어 애플리케이션들 혹은 코드로서 알려져 있음)은 프로그래밍가능 프로세서에 대한 명령어 들을 포함하며 \"컴퓨터가 읽을 수 있는 기록매체\"에 저장된다. 컴퓨터가 읽을 수 있는 기록매체는, 컴퓨터 시스템에 의하여 읽혀질 수 있는 데이터가 저장되는 모든 종류의 기 록장치를 포함한다. 이러한 컴퓨터가 읽을 수 있는 기록매체는 ROM, CD-ROM, 자기 테이프, 플로피디스크, 메모 리 카드, 하드 디스크, 광자기 디스크, 스토리지 디바이스 등의 비휘발성(non-volatile) 또는 비일시적인(non- transitory) 매체일 수 있으며, 또한 데이터 전송 매체(data transmission medium)와 같은 일시적인 (transitory) 매체를 더 포함할 수도 있다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어, 분산방식으로 컴퓨터가 읽을 수 있는 코드가 저장되고 실행될 수도 있다. 본 명세서의 흐름도/타이밍도에서는 각 과정들을 순차적으로 실행하는 것으로 기재하고 있으나, 이는 본 개시의 일 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것이다. 다시 말해, 본 개시의 일 실시예가 속하는 기 술 분야에서 통상의 지식을 가진 자라면 본 개시의 일 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 흐 름도/타이밍도에 기재된 순서를 변경하여 실행하거나 각 과정들 중 하나 이상의 과정을 병렬적으로 실행하는 것으로 다양하게 수정 및 변형하여 적용 가능할 것이므로, 흐름도/타이밍도는 시계열적인 순서로 한정되는 것은 아니다. 이상의 설명은 본 실시예의 기술 사상을 예시적으로 설명한 것에 불과한 것으로서, 본 실시예가 속하는 기술 분 야에서 통상의 지식을 가진 자라면 본 실시예의 본질적인 특성에서 벗어나지 않는 범위에서 다양한 수정 및 변 형이 가능할 것이다. 따라서, 본 실시예들은 본 실시예의 기술 사상을 한정하기 위한 것이 아니라 설명하기 위 한 것이고, 이러한 실시예에 의하여 본 실시예의 기술 사상의 범위가 한정되는 것은 아니다. 본 실시예의 보호 범위는 아래의 청구범위에 의하여 해석되어야 하며, 그와 동등한 범위 내에 있는 모든 기술 사상은 본 실시예의 권리범위에 포함되는 것으로 해석되어야 할 것이다.도면 도면1 도면2 도면3 도면4"}
{"patent_id": "10-2024-0037375", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 종래의 flow 기반 모델의 인코더를 도시한 것이다. 도 2는 종래의 tacotron 2 모델의 인코더를 도시한 것이다. 도 3은 본 개시의 일 실시예에 따른 음성변환 모델을 개략적으로 도시한 블록구성도이다. 도 4는 본 개시의 일 실시예에 따른 음성변환 모델이 학습하는 과정을 도시한 순서도이다."}
