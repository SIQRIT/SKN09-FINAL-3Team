{"patent_id": "10-2022-0174206", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2024-0088457", "출원번호": "10-2022-0174206", "발명의 명칭": "합성 음성을 식별하는 전자 장치 및 그 제어 방법", "출원인": "삼성전자주식회사", "발명자": "소콜 올렉산드라"}}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "마이크; 및상기 마이크를 통해 음성 데이터가 수신되면,상기 음성 데이터를 비의미적 특징 추출(Non-semantic feature extract) 모델에 입력하여 상기 음성 데이터에포함된 비의미적 특징을 획득하고,상기 비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 상기 음성 데이터를 합성 음성 또는 사용자음성 중 어느 하나로 분류하며,상기 분류 결과를 제공하는 하나 이상의 프로세서;를 포함하며,상기 합성 음성 분류 모델은,상기 비의미적 특징 추출 모델에 기초하여 전이 학습된(Transfer learned) 모델인, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 비의미적 특징은,상기 음성 데이터에 대응되는 특징 벡터를 포함하며,상기 하나 이상의 프로세서는,복수의 샘플 사용자 음성 중 제1 샘플 사용자 음성 및 제2 샘플 사용자 음성을 획득하고,상기 제1 샘플 사용자 음성으로부터 제1 부분 음성 및 제2 부분 음성을 획득하며,상기 제2 샘플 사용자 음성으로부터 제3 부분 음성을 획득하며,상기 제1 내지 제3 부분 음성 각각을 상기 비의미적 특징 추출 모델에 입력하여 상기 제1 내지 제3 부분 음성각각에 대응되는 제1 내지 제3 특징 벡터를 획득하며,상기 제1 내지 제3 특징 벡터에 기초하여 감정 분류 손실(emotion classification loss) 및 유사도 손실(similarity loss)을 획득하며,상기 감정 분류 손실 및 상기 유사도 손실에 기초하여 상기 비의미적 특징 추출 모델을 업데이트하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2항에 있어서,상기 하나 이상의 프로세서는,상기 제1 특징 벡터 및 상기 제2 특징 벡터를 감정 분류기(emotion classifier)에 입력하여 상기 제1 특징 벡터에 대응되는 제1 예측 감정(predicted emotion) 및 상기 제2 특징 벡터에 대응되는 제2 예측 감정을 획득하며,상기 제1 예측 감정 및 상기 제2 예측 감정과 상기 제1 샘플 사용자 음성에 대응되는 제1 실제 감정(trueemotion)에 기초하여 상기 감정 분류 손실을 획득하며,상기 감정 분류 손실에 대응되는 가중치에 기초하여 상기 감정 분류기를 업데이트하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "공개특허 10-2024-0088457-3-제3항에 있어서,상기 제1 예측 감정과 상기 제2 예측 감정은 동일한, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제2항에 있어서,상기 하나 이상의 프로세서는,상기 제1 내지 제3 특징 벡터 간의 거리 정보에 기초하여 상기 유사도 손실을 획득하며,상기 감정 분류 손실 및 상기 유사도 손실의 집합(Aggregation)에 대응되는 가중치에 기초하여 상기 비의미적특징 추출 모델을 업데이트하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제1항에 있어서,상기 하나 이상의 프로세서는,상기 비의미적 특징 추출 모델 및 손실 함수(loss function)에 기초하여 상기 합성 음성 분류 모델을 전이 학습시키는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서,상기 하나 이상의 프로세서는,복수의 샘플 음성 데이터를 상기 비의미적 특징 추출 모델에 입력하여 상기 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 획득하며,상기 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 상기 합성 음성 분류 모델에 입력하여 상기 복수의 샘플 음성 데이터 각각을 상기 합성 음성 또는 상기 사용자 음성 중 어느 하나로 분류한 예측 결과를 획득하며,상기 손실 함수에 기초하여 상기 예측 결과와 실제 결과에 대응되는 크로스 엔트로피 손실(Cross entropy loss)을 획득하며,상기 크로스 엔트로피 손실에 기초하여 상기 합성 음성 분류 모델을 업데이트하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 복수의 샘플 음성 데이터는,복수의 샘플 사용자 음성 및 복수의 샘플 합성 음성을 포함하며,상기 실제 결과는,상기 복수의 샘플 음성 데이터 각각에 대응되는 실제 라벨(True label)에 기초하여 상기 복수의 샘플 음성 데이터 각각을 상기 합성 음성 또는 상기 사용자 음성으로 분류한 결과인, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제1항에 있어서,상기 합성 음성 분류 모델은,상기 음성 데이터가 상기 합성 음성에 포함될 확률을 출력하며,상기 하나 이상의 프로세서는,상기 확률이 임계 확률을 초과하면, 상기 음성 데이터를 상기 합성 음성으로 분류하는, 전자 장치.공개특허 10-2024-0088457-4-청구항 10 제9항에 있어서,상기 하나 이상의 프로세서는,상기 전자 장치에서 실행 중인 어플리케이션에 대응되는 보안 레벨에 기초하여 상기 임계 확률을 조정하며,상기 음성 데이터가 상기 합성 음성으로 분류되면, 노티를 제공하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제1항에 있어서,상기 비의미적 특징은,상기 음성 데이터에 대응되는 특징 벡터를 포함하며,상기 하나 이상의 프로세서는,상기 특징 벡터를 감정 분류기(emotion classifier)에 입력하여 상기 특징 벡터에 대응되는 예측 감정(predicted emotion)을 획득하며,상기 예측 감정에 대응되는 피드백을 제공하는, 전자 장치."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "전자 장치의 제어 방법에 있어서,음성 데이터를 비의미적 특징 추출(Non-semantic feature extract) 모델에 입력하여 상기 음성 데이터에 포함된비의미적 특징을 획득하는 단계;비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 상기 음성 데이터를 합성 음성 또는 사용자 음성중 어느 하나로 분류하는 단계; 및상기 분류 결과를 제공하는 단계;를 포함하는 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제12항에 있어서,상기 비의미적 특징은,상기 음성 데이터에 대응되는 특징 벡터를 포함하며,복수의 샘플 사용자 음성 중 제1 샘플 사용자 음성 및 제2 샘플 사용자 음성을 획득하는 단계;상기 제1 샘플 사용자 음성으로부터 제1 부분 음성 및 제2 부분 음성을 획득하는 단계;상기 제2 샘플 사용자 음성으로부터 제3 부분 음성을 획득하는 단계;상기 제1 내지 제3 부분 음성 각각을 상기 비의미적 특징 추출 모델에 입력하여 상기 제1 내지 제3 부분 음성각각에 대응되는 제1 내지 제3 특징 벡터를 획득하는 단계;상기 제1 내지 제3 특징 벡터에 기초하여 감정 분류 손실(emotion classification loss) 및 유사도 손실(similarity loss)을 획득하는 단계; 및상기 감정 분류 손실 및 상기 유사도 손실에 기초하여 상기 비의미적 특징 추출 모델을 업데이트하는 단계;를더 포함하는, 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서,상기 감정 분류 손실 및 상기 유사도 손실을 획득하는 단계는,상기 제1 특징 벡터 및 상기 제2 특징 벡터를 감정 분류기(emotion classifier)에 입력하여 상기 제1 특징 벡터공개특허 10-2024-0088457-5-에 대응되는 제1 예측 감정(predicted emotion) 및 상기 제2 특징 벡터에 대응되는 제2 예측 감정을 획득하는단계;상기 제1 예측 감정 및 상기 제2 예측 감정과 상기 제1 샘플 사용자 음성에 대응되는 제1 실제 감정(trueemotion)에 기초하여 상기 감정 분류 손실을 획득하는 단계; 및상기 감정 분류 손실에 대응되는 가중치에 기초하여 상기 감정 분류기를 업데이트하는 단계;를 더 포함하는, 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서,상기 제1 예측 감정과 상기 제2 예측 감정은 동일한, 제어 방법.."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13항에 있어서,상기 감정 분류 손실 및 상기 유사도 손실을 획득하는 단계는,상기 제1 내지 제3 특징 벡터 간의 거리 정보에 기초하여 상기 유사도 손실을 획득하는 단계;를 포함하며,상기 업데이트하는 단계는,상기 감정 분류 손실 및 상기 유사도 손실의 집합(Aggregation)에 대응되는 가중치에 기초하여 상기 비의미적특징 추출 모델을 업데이트하는 단계;를 포함하는, 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제12항에 있어서,상기 비의미적 특징 추출 모델 및 손실 함수(loss function)에 기초하여 상기 합성 음성 분류 모델을 전이 학습시키는 단계;를 더 포함하는, 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "제17항에 있어서,상기 학습시키는 단계는,복수의 샘플 음성 데이터를 상기 비의미적 특징 추출 모델에 입력하여 상기 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 획득하는 단계;상기 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 상기 합성 음성 분류 모델에 입력하여 상기 복수의 샘플 음성 데이터 각각을 상기 합성 음성 또는 상기 사용자 음성 중 어느 하나로 분류한 예측 결과를 획득하는 단계;상기 손실 함수에 기초하여 상기 예측 결과와 실제 결과에 대응되는 크로스 엔트로피 손실(Cross entropy loss)을 획득하는 단계; 및상기 크로스 엔트로피 손실에 기초하여 상기 합성 음성 분류 모델을 업데이트하는 단계;를 포함하는, 제어방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제18항에 있어서,상기 복수의 샘플 음성 데이터는,복수의 샘플 사용자 음성 및 복수의 샘플 합성 음성을 포함하며,상기 실제 결과는,상기 복수의 샘플 음성 데이터 각각에 대응되는 실제 라벨(True label)에 기초하여 상기 복수의 샘플 음성 데이공개특허 10-2024-0088457-6-터 각각을 상기 합성 음성 또는 상기 사용자 음성으로 분류한 결과인, 제어 방법."}
{"patent_id": "10-2022-0174206", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_20", "content": "제12항에 있어서,상기 합성 음성 분류 모델은,상기 음성 데이터가 상기 합성 음성에 포함될 확률을 출력하며,상기 분류하는 단계는,상기 확률이 임계 확률을 초과하면, 상기 음성 데이터를 상기 합성 음성으로 분류하는 단계;를 포함하는, 제어방법."}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 전자 장치는, 마이크 및 마이크를 통해 음성 데이터가 수신되면, 음성 데이터를 비의미적 특징 추출(Non-semantic feature extract) 모델에 입력하여 음성 데이터에 포함된 비의미적 특징을 획득하고, 비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 음성 데이터를 합성 음성 또는 사용자 음성 중 어 느 하나로 분류하며, 분류 결과를 제공하는 하나 이상의 프로세서를 포함하며, 합성 음성 분류 모델은, 비의미적 특징 추출 모델에 기초하여 전이 학습된(Transfer learned) 모델이다."}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 전자 장치 및 그 제어 방법에 관한 것으로, 더욱 상세하게는, 음성 데이터를 합성 음성 또는 사용자 음성으로 분류하는 전자 장치 및 그 제어 방법에 관한 것이다."}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공 지능 모델에 대한 개발이 활발히 진행되면서, 다양한 목적을 가지는 인공 지능 모델이 배포되고 있다. 인공 지능 모델과 관련된 기술이 발달함에 따라 진짜 같은 가짜 음성(예를 들어, 모방 음성), 가짜 영상을 만들 수 있게 되면서 예전에는 없었던 새로운 법적인 문제들이 생겨나고 있다. 인공 지능 모델이 생성한 가짜 음성, 가짜 영상을 가리키는 딥페이크(deepfake)라는 단어는 딥러닝(deep learning)과 가짜(fake)의 합성어로서, 진짜처럼 보이기 위해 조작된 음성, 영상 등을 지칭한다. 이러한 딥페이 크 음성이나 딥페이크 영상이 악의적으로 타인을 속이는데 이용되거나, 보안과 관련된 인증 과정에서 이용될 우 려가 있으므로, 역으로 음성이나 영상이 가짜 즉, 인공 지능 모델이 생성한 딥페이크 음성, 영상인지 식별하는 장치 및 방법에 대한 요구가 있어왔다. 최근 지속적으로 증가하는 보이스 피싱(Voice phishing, vishing) 관련 피해 사례를 보면, 인공 지능 모델이 생 성한 딥페이크 음성을 이용한 사례가 전체 사례에서 차지하는 비율이 점차 증가하고 있으므로, 딥페이크 음성을 식별하여 피해가 발생하기 전에 예방하는 기술적 방법에 대한 요구가 증가하는 실정이다."}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 상술한 목적을 달성하기 위한 일 실시 예에 따른 전자 장치는, 마이크 및 상기 마이크를 통해 음성 데이터가 수신되면, 상기 음성 데이터를 비의미적 특징 추출(Non-semantic feature extract) 모델에 입력하여 상기 음성 데이터에 포함된 비의미적 특징을 획득하고, 상기 비의미적 특징을 합성(Synthetic) 음성 분류 모델 에 입력하여 상기 음성 데이터를 합성 음성 또는 사용자 음성 중 어느 하나로 분류하며, 상기 분류 결과를 제공 하는 하나 이상의 프로세서를 포함하며, 상기 합성 음성 분류 모델은, 상기 비의미적 특징 추출 모델에 기초하 여 전이 학습된(Transfer learned) 모델일 수 있다. 본 개시의 상술한 목적을 달성하기 위한 일 실시 예에 따른 전자 장치의 제어 방법은, 음성 데이터를 비의미적 특징 추출(Non-semantic feature extract) 모델에 입력하여 상기 음성 데이터에 포함된 비의미적 특징을 획득하 는 단계, 비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 상기 음성 데이터를 합성 음성 또는 사 용자 음성 중 어느 하나로 분류하는 단계 및 상기 분류 결과를 제공하는 단계를 포함할 수 있다. 본 개시의 상술한 목적을 달성하기 위한 일 실시 예에 따른 전자 장치의 제어 방법을 실행하는 프로그램을 포함 하는 컴퓨터 판독 가능 기록매체에 있어서, 상기 전자 장치의 제어 방법은, 음성 데이터를 비의미적 특징 추출 (Non-semantic feature extract) 모델에 입력하여 상기 음성 데이터에 포함된 비의미적 특징을 획득하는 단계,비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 상기 음성 데이터를 합성 음성 또는 사용자 음성 중 어느 하나로 분류하는 단계 및 상기 분류 결과를 제공하는 단계를 포함할 수 있다."}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "이하에서는 첨부 도면을 참조하여 본 개시를 상세히 설명한다. 본 개시의 실시 예에서 사용되는 용어는 본 개시에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어들을 선택하였으나, 이는 당 분야에 종사하는 기술자의 의도 또는 판례, 새로운 기술의 출현 등에 따라 달 라질 수 있다. 또한, 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 개시의 설명 부 분에서 상세히 그 의미를 기재할 것이다. 따라서 본 개시에서 사용되는 용어는 단순한 용어의 명칭이 아닌, 그 용어가 가지는 의미와 본 개시의 전반에 걸친 내용을 토대로 정의되어야 한다. 본 명세서에서, \"가진다,\" \"가질 수 있다,\" \"포함한다,\" 또는 \"포함할 수 있다\" 등의 표현은 해당 특징(예: 수 치, 기능, 동작, 또는 부품 등의 구성요소)의 존재를 가리키며, 추가적인 특징의 존재를 배제하지 않는다. A 또는/및 B 중 적어도 하나라는 표현은 \"A\" 또는 \"B\" 또는 \"A 및 B\" 중 어느 하나를 나타내는 것으로 이해되어 야 한다. 본 명세서에서 사용된 \"제1,\" \"제2,\" \"첫째,\" 또는 \"둘째,\"등의 표현들은 다양한 구성요소들을, 순서 및/또는 중요도에 상관없이 수식할 수 있고, 한 구성요소를 다른 구성요소와 구분하기 위해 사용될 뿐 해당 구성요소들 을 한정하지 않는다. 어떤 구성요소(예: 제1 구성요소)가 다른 구성요소(예: 제2 구성요소)에 \"(기능적으로 또는 통신적으로) 연결되 어((operatively or communicatively) coupled with/to)\" 있다거나 \"접속되어(connected to)\" 있다고 언급된 때에는, 어떤 구성요소가 다른 구성요소에 직접적으로 연결되거나, 다른 구성요소(예: 제3 구성요소)를 통하여 연결될 수 있다고 이해되어야 할 것이다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함하다\" 또 는 \"구성되다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것 이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 본 개시에서 \"모듈\" 혹은 \"부\"는 적어도 하나의 기능이나 동작을 수행하며, 하드웨어 또는 소프트웨어로 구현되 거나 하드웨어와 소프트웨어의 결합으로 구현될 수 있다. 또한, 복수의 \"모듈\" 혹은 복수의 \"부\"는 특정한 하드 웨어로 구현될 필요가 있는 \"모듈\" 혹은 \"부\"를 제외하고는 적어도 하나의 모듈로 일체화되어 적어도 하나의 프 로세서(미도시)로 구현될 수 있다. 본 명세서에서, 사용자라는 용어는 전자 장치를 사용하는 사람 또는 전자 장치를 사용하는 장치(예: 인공지능 전자 장치)를 지칭할 수 있다. 이하 첨부된 도면들을 참조하여 본 개시의 일 실시 예를 보다 상세하게 설명한다. 도 1은 본 개시의 일 실시 예에 따른 합성 음성을 생성하는 전자 장치를 설명하기 위한 도면이다. 본 개시의 일 예에 따른 전자 장치는 합성(Synthetic) 음성을 생성할 수 있고, 음성 데이터가 합성 음성인 지 여부를 식별할 수도 있다. 여기서, 합성 음성은, 전자 장치가 생성한 음성 예를 들어, 전자 장치가 신경망 모델을 이용하여 생 성한 음성을 포함한다. 여기서, 신경망 모델은, 원본 음성(예를 들어, 전자 장치의 사용자(도 1의 남자 사 용자 Isaac)가 발화한 음성)(이하, 사용자 음성) 또는 텍스트가 입력되면, 사용자 음성 또는 텍스트를 합성하려 는(또는, 변조하려는) 사용자(예를 들어, 도 1의 여자 사용자 Heidi)(이하, 합성 대상 사용자)의 음성 특성 정 보에 기초하여 합성한 후, 합성 음성을 출력하도록 학습된 모델일 수 있다. 즉, 합성 음성은, 합성 대상 사용자의 음성(또는, 목소리)을 흉내 내는 음성이며, 딥페이크(Deepfake) 음성, 변 조 음성, 모방 음성 등으로 불릴 수 있으나, 이하에서는 설명의 편의를 위해 합성 음성으로 통칭하도록 한다. 여기서, 음성 특성 정보는, 합성 대상 사용자가 발화한 음성의 일부, 합성 대상 사용자의 발화 주파수(또는, 파 형), 성별, 나이 대(예를 들어, 청년, 중년, 노년 등), 사용 언어 등을 포함하며, 신경망 모델은, 음성 특성 정 보에 기초하여 합성 대상 사용자가 실제로 발화한 음성과 유사한 합성 음성을 출력하도록 학습된 모델일 수 있 다. 한편, 사람의 음성 즉, 성문은 사람 마다 상이한 고유한 특징이므로, 음성을 이용하여 사람을 특정할 수 있고, 보안과 관련된 인증 과정에서 음성이 이용되므로, 신경망 모델이 출력하는 합성 음성이 악의적으로 사용될 가능 성이 있다. 예를 들어, 신경망 모델을 통해 합성 대상 사용자의 음성(또는, 목소리)을 흉내 내어 인증 과정 또 는 타인을 속이는 사기(Fraud)(예를 들어, Voice phishing, 이하, Vishing) 과정에 악용될 여지가 있다. 이하에서는, 본 개시의 다양한 예에 따라 전자 장치가 음성 데이터를 획득하며, 획득된 음성 데이터가 합 성 음성인지 또는 사용자 음성인지 식별하는 방법을 설명하도록 한다. 도 2는 본 개시의 일 실시 예에 따른 합성 음성과 사용자 음성을 설명하기 위한 도면이다. 도 2를 참조하면, 전자 장치의 사용자가 발화한 음성 즉, 사용자 음성 대비 합성 음성은 시간에 따른 감정 의 변동폭(Variability)이 작다. 예를 들어, 음성 데이터는 노이즈(가청 노이즈 및 비가청 노이즈)와 의미적 특징(Semantic features) 및 비 의미적 특징(Non-semantic features)을 포함할 수 있다. 여기서, 의미적 특징은 의미를 전달하기 위해 음성에 포함된 음성의(phonetic) 특징 및 어휘의(lexical) 특징을 포함할 수 있다. 일 예에 따라 비의미적 특징은 음성에서 의미적 특징을 제외한 나머지, 예를 들어, 의미를 전달하지는 않으나, 음성에 포함된 화자 정체성(speaker identity), 언어(language), 감정(emotion) 특징, 운율(prosody) 특징 등 을 포함할 수 있다. 예를 들어, 비의미적 특징은 화자의 감정, 운율과 상관관계(correlation)가 있으며, 비의미 적 특징에 기초하여 화자의 감정을 예측할 수 있다. 이에 대한 구체적인 설명은 후술하도록 한다. 일 예에 따라 시간의 흐름에 따라 비의미적 특징은 의미적 특징 대비 급격히 변동하지 않는다(예를 들어, 비의 미적 특징은 의미적 특징 대비 느리게 변동한다(change more slowly)). 본 개시의 일 실시 예에 따른 전자 장치는 음성 데이터에서 비의미적 특징을 획득하며, 비의미적 특징 에 기초하여 음성 데이터를 합성 음성 또는 사용자 음성(즉, 비합성 음성)으로 분류할 수 있다. 이에 대한 구체적인 설명은 후술하도록 한다.도 3은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 3을 참조하면, 전자 장치는 마이크 및 하나 이상의 프로세서를 포함한다. 본 개시의 다양한 예에서는 설명의 편의를 위해 전자 장치를 사용자 단말 장치로 상정하여 도시하였으나, 이는 일 예시이며 전자 장치는 TV, 사용자 단말 장치, 태블릿 PC, 이동 전화기, 영상 전화기, 전자책 리더 기, 데스크탑 PC, 랩탑 PC, 넷북 컴퓨터, 워크스테이션, 서버, PDA, PMP(portable multimedia player), MP3 플 레이어, 의료기기, 카메라, 가상 현실(virtual reality(VR)) 구현 장치 또는 웨어러블 장치 중 적어도 하나를 포함할 수 있다. 여기서, 웨어러블 장치는 액세서리형(예: 시계, 반지, 팔찌, 발찌, 목걸이, 안경, 콘택트 렌즈, 또는 머리 착용형 장치(head-mounted-device(HMD)), 직물 또는 의류 일체형(예: 전자 의복), 신체 부착형 (예: 스킨 패드 또는 문신), 또는 생체 이식형 회로 중 적어도 하나를 포함할 수 있다. 어떤 실시 예에서, 전자 장치는 텔레비전, DVD(digital video disk) 플레이어, 오디오, 냉장고, 에어컨, 청소기, 오븐, 전자레인지, 세탁기, 공기 청정기, 소스 장치(Source Device(예를 들어, 셋톱 박스(Set-top box), 클라우드 (cloud) 서버, OTT 서비스(Over-the-top media service) 서버 등), 홈 오토매이션 컨트롤 패널, 보안 컨트롤 패널, 미디어 박스(예: 애플TVTM, 또는 구글 TVTM), LED S-Box, 게임 콘솔(예: XboxTM, PlayStationTM, Nintendo SwitchTM), 전자 사전, 전자 키, 캠코더, 또는 전자 액자 중 적어도 하나를 포함할 수 있다. 다른 실시 예에서, 전자 장치는 각종 의료기기(예: 각종 휴대용 의료측정기기(혈당 측정기, 심박 측정기, 혈압 측정기, 또는 체온 측정기 등), MRA(magnetic resonance angiography), MRI(magnetic resonance imaging), CT(computed tomography), 촬영기, 또는 초음파기 등), 네비게이션 장치, 위성 항법 시스템 (GNSS(global navigation satellite system)), EDR(event data recorder), FDR(flight data recorder), 자동 차 인포테인먼트 장치, 선박용 전자 장비(예: 선박용 항법 장치, 자이로 콤파스 등), 항공 전자기기(avionics), 보안 기기, 차량용 헤드 유닛(head unit), 산업용 또는 가정용 로봇, 드론(drone), 금융 기관의 ATM, 상점의 POS(point of sales), 또는 사물 인터넷 장치 (예: 전구, 각종 센서, 스프링클러 장치, 화재 경보기, 온도조절 기, 가로등, 토스터, 운동기구, 온수탱크, 히터, 보일러 등) 중 적어도 하나를 포함할 수 있다. 일 예에 따른 마이크는 전자 장치의 주변에서 발생하는 음성 및 노이즈를 포함하는 음성 데이터 를 수신하여 전기적 신호로 변환하며, 하나 이상의 프로세서로 전송할 수 있다. 한편, 본 개시의 다양한 실시 예에서는 마이크를 통해 음성 데이터를 수신하는 경우를 상정하였으나, 이는 일 예시이며 이에 한정되지 않음은 물론이다. 예를 들어, 전자 장치는 통신 인터페이스(미도시)를 통해 타 전자 장치로부터 음성 데이터를 수신할 수도 있음은 물론이다. 본 개시의 일 실시 예에 따른 하나 이상의 프로세서는 전자 장치의 전반적인 동작을 제어한다. 본 개시의 일 실시 예에 따라, 하나 이상의 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서 (digital signal processor(DSP), 마이크로 프로세서(microprocessor), TCON(Timing controller)으로 구현될 수 있다. 다만, 이에 한정되는 것은 아니며, 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(micro processing unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서, AI(Artificial Intelligence) 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 하나 이상의 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 하나 이상의 프로세서는 메 모리에 저장된 컴퓨터 실행가능 명령어(computer executable instructions)를 실행함으로써 다양한 기능을 수행 할 수 있다. 하나 이상의 프로세서는 CPU (Central Processing Unit), GPU (Graphics Processing Unit), APU (Accelerated Processing Unit), MIC (Many Integrated Core), DSP (Digital Signal Processor), NPU (Neural Processing Unit), 하드웨어 가속기 또는 머신 러닝 가속기 중 하나 이상을 포함할 수 있다. 하나 이상의 프로 세서는 전자 장치의 다른 구성요소 중 하나 또는 임의의 조합을 제어할 수 있으며, 통신에 관한 동작 또는 데이터 처리를 수행할 수 있다. 하나 이상의 프로세서는 메모리에 저장된 하나 이상의 프로그램 또는 명령 어(instruction)을 실행할 수 있다. 예를 들어, 하나 이상의 프로세서는 메모리에 저장된 하나 이상의 명 령어를 실행함으로써, 본 개시의 일 실시 예에 따른 방법을 수행할 수 있다. 본 개시의 일 실시 예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 하나의 프로세서에 의해 수 행될 수도 있고, 복수의 프로세서에 의해 수행될 수도 있다. 예를 들어, 일 실시 예에 따른 방법에 의해 제 1동작, 제 2 동작, 제 3 동작이 수행될 때, 제 1 동작, 제 2 동작, 및 제 3 동작 모두 제 1 프로세서에 의해 수 행될 수도 있고, 제 1 동작 및 제 2 동작은 제 1 프로세서(예를 들어, 범용 프로세서)에 의해 수행되고 제 3 동 작은 제 2 프로세서(예를 들어, 인공지능 전용 프로세서)에 의해 수행될 수도 있다. 하나 이상의 프로세서는 하나의 코어를 포함하는 단일 코어 프로세서(single core processor)로 구현될 수 도 있고, 복수의 코어(예를 들어, 동종 멀티 코어 또는 이종 멀티 코어)를 포함하는 하나 이상의 멀티 코어 프 로세서(multicore processor)로 구현될 수도 있다. 하나 이상의 프로세서가 멀티 코어 프로세서로 구현되 는 경우, 멀티 코어 프로세서에 포함된 복수의 코어 각각은 캐시 메모리, 온 칩(On-chip) 메모리와 같은 프로세 서 내부 메모리를 포함할 수 있으며, 복수의 코어에 의해 공유되는 공통 캐시가 멀티 코어 프로세서에 포함될 수 있다. 또한, 멀티 코어 프로세서에 포함된 복수의 코어 각각(또는 복수의 코어 중 일부)은 독립적으로 본 개 시의 일 실시 예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수행할 수도 있고, 복수의 코어 전체 (또는 일부)가 연계되어 본 개시의 일 실시 예에 따른 방법을 구현하기 위한 프로그램 명령을 판독하여 수행할 수도 있다. 본 개시의 일 실시 예에 따른 방법이 복수의 동작을 포함하는 경우, 복수의 동작은 멀티 코어 프로세서에 포함 된 복수의 코어 중 하나의 코어에 의해 수행될 수도 있고, 복수의 코어에 의해 수행될 수도 있다. 예를 들어, 일 실시 예에 따른 방법에 의해 제 1 동작, 제 2 동작, 및 제 3 동작이 수행될 때, 제 1 동작, 제2 동작, 및 제 3 동작 모두 멀티 코어 프로세서에 포함된 제 1 코어에 의해 수행될 수도 있고, 제 1 동작 및 제 2 동작은 멀티 코어 프로세서에 포함된 제 1 코어에 의해 수행되고 제 3 동작은 멀티 코어 프로세서에 포함된 제 2 코어에 의 해 수행될 수도 있다. 본 개시의 실시 예들에서, 프로세서는 하나 이상의 프로세서 및 기타 전자 부품들이 집적된 시스템 온 칩(SoC), 단일 코어 프로세서, 멀티 코어 프로세서, 또는 단일 코어 프로세서 또는 멀티 코어 프로세서에 포함된 코어를 의미할 수 있으며, 여기서 코어는 CPU, GPU, APU, MIC, DSP, NPU, 하드웨어 가속기 또는 기계 학습 가속기 등으 로 구현될 수 있으나, 본 개시의 실시 예들이 이에 한정되는 것은 아니다. 도 4는 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델(Non-Semantic feature extractor model) 및 합 성 음성 분류 모델을 설명하기 위한 도면이다. 우선, 하나 이상의 프로세서는 음성 데이터를 비의미적 특징 추출 모델에 입력하여 음성 데이터 에 포함된 비의미적 특징을 획득할 수 있다. 여기서, 비의미적 특징 추출 모델은, CNN(Convolution Neural Network), 트랜스포머(Transformer), CNN + RNN(Recurrent Neural Network, CNN + 트랜스포머 등으로 구현될 수 있다. 여기서, CNN은 ResNet(예를 들어, ResNet-50), MobileNet(예를 들어, MobileNet-V3), EfficientNet, Inception 등으로 구현될 수 있다. 이어서, 하나 이상의 프로세서는 비의미적 특징을 합성(Synthetic) 음성 분류 모델에 입력하여 음성 데 이터를 합성 음성 또는 사용자 음성 중 어느 하나로 분류할 수 있다. 이어서, 하나 이상의 프로세서는 분 류 결과를 제공할 수 있다. 여기서, 합성 음성 분류 모델은, 비의미적 특징 추출 모델에 기초하여 전이 학습된(Transfer learned) 모 델일 수 있다. 합성 음성 분류 모델은 다양한 종류의 이진 분류 모델(binary classification model)로 구현될 수 있다. 예를 들어, 합성 음성 분류 모델은 로지스틱 회귀(Logistic regression), 서포트 벡터 머신 분류기(Support Vector Machine classifier), 트리 모델(Tree models), 부스팅 모델(Boosting models), 다층 퍼셉트론 (Multilayer perceptron, (MLP)), 완전 연결 신경망(Fully connected neural network, (FCNN)) 등으로 구현될 수 있다. 예를 들어, 본 개시의 일 예에 따른 합성 음성 분류 모델은 Adam과 교차 엔트로피 손실(cross entropy loss)을 이용하여 학습된 완전 연결 신경망과 ReLU 활성화 함수로 구현될 수 있다. 여기서, Adam은 최적화 알고리즘의 일 예이며, 이에 한정되지 않음은 물론이다. 예를 들어, 비의미적 특징 추출 모델 및 합성 음성 분류 모델 각각은, 다양한 종류의 경사 하강법(Gradient descent) 기반 최적화 알고리 즘(예를 들어, Adam, AdaGrad, NAG(Nesterov's Accelerated Gradient and Momentum)을 이용하여 학습될 수 있 다. 도 5는 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델의 학습을 설명하기 위한 도면이다. 도 5를 참조하면, 하나 이상의 프로세서는 복수의 샘플 사용자 음성 각각을 전처리(pre- processing)(또는, 정규화(regularization))할 수 있다. 예를 들어, 하나 이상의 프로세서는 복수의 샘플 사용자 음성 각각에 노이즈 추가(noise Injection) (또는, 노이즈 증강(augmentation)), 압축 증강(compression augmentation)을 수행하여 복수의 샘플 사용자 음 성에 대한 데이터 증강(Data Augmentation)을 수행할 수 있다. 이어서, 하나 이상의 프로세서는 전처리된 복수의 샘플 사용자 음성, 유사도 손실(similarity loss)(A) 및 감정 분류 손실(emotion classification loss)(B)에 기초하여 비의미적 특징 추출 모델을 학습 시킬 수 있다. 비의미적 특징 추출 모델을 학습시키는 방법에 대해 도 6을 참조하여 보다 구체적으로 설명하 도록 한다. 도 6은 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델의 학습을 상세히 설명하기 위한 도면이다. 일 예에 따른 하나 이상의 프로세서는 복수의 샘플 사용자 음성 중 제1 샘플 사용자 음성 및 제2 샘플 사용자 음성을 획득할 수 있다. 이어서, 하나 이상의 프로세서는 제1 샘플 사용자 음성 및 제2 샘플 사용자 음성 각각을 증강 (Augmentation) 및 전처리할 수 있다(S610). 이어서, 하나 이상의 프로세서는 전처리된 제1 샘플 사용자 음성에서 제1 부분(Segmentation) 음성 (41-1) 및 제2 부분 음성(41-2)을 획득할 수 있다. 또한, 하나 이상의 프로세서는 전처리된 제2 샘플 사용 자 음성에서 제3 부분 음성(42-1)을 획득할 수 있다. 일 예에 따른 하나 이상의 프로세서는 제1 부분 음성(41-1), 제2 부분 음성(41-2) 및 제3 부분 음성(42- 1)을 비의미적 특징 추출 모델에 입력하며, 비의미적 특징 추출 모델은 제1 부분 음성(41-1), 제2 부분 음성(41-2) 및 제3 부분 음성(42-1) 각각에 대응되는 제1 특징 벡터(Feature vector)(51-1), 제2 특징 벡터 (Feature vector)(51-2), 제3 특징 벡터(Feature vector)(52-1)를 출력할 수 있다. 일 예로, 비의미적 특징 추출 모델은 입력된 음성(또는, 입력된 부분 음성)에서 화자 정체성, 언어, 감정 특 징, 운율 특징 등을 포함하는 비의미적 특징을 수치화해 벡터 공간에 표현하는 임베딩(embedding) 과정을 수행 하여 입력된 음성에 대응되는 특징 벡터를 출력할 수 있다. 이어서, 하나 이상의 프로세서는 제1 특징 벡터(Feature vector)(51-1) 및 제2 특징 벡터(Feature vector)(51-2)를 감정 분류기(Emotion Classifier)에 입력할 수 있다. 일 예에 따른 감정 분류기는 제1 특징 벡터(Feature vector)(51-1)에 대응되는 제1 예측 감정(Predicted emotion) 및 제2 특징 벡터(Feature vector)(51-2)에 대응되는 제2 예측 감정을 출력할 수 있다. 일 예에 따른 감정 분류기는 특징 벡터가 입력되면, Paul Ekman이 정의한 6 가지의 기본 감정(분노(anger), 놀라움(surprise), 혐오(disgust), 즐거움(enjoyment), 두려움(fear) 및 슬픔(sadness)) 중 어느 하나를 입력 된 특징 벡터에 대응되는 예측 감정으로 출력할 수 있다. 다만, 이는 일 예시이며, 8 가지 감정(분노(anger), 놀라움(surprise), 혐오(disgust), 즐거움(enjoyment), 두려움(fear), 슬픔(sadness), 경멸(contempt) 및 중립 (neutral)) 중 어느 하나를 예측 감정으로 출력할 수도 있음은 물론이다. 한편, 상술한 바와 같이 비의미적 특징은 느리게 변동하므로, 동일한 샘플 사용자 음성(예를 들어, 제1 샘플 사 용자 음성)으로부터 획득된 제1 특징 벡터(51-1)에 대응되는 제1 예측 감정과 제2 특징 벡터(51-2)에 대응 되는 제2 예측 감정은 동일할 수 있다. 이어서, 하나 이상의 프로세서는 제1 샘플 사용자 음성에 대응되는 제1 실제 감정(True emotion)과 제1 예 측 감정 및 제2 예측 감정에 기초하여 감정 분류 손실(B)을 획득할 수 있다. 여기서, 복수의 샘플 사용자 음성 은 복수의 샘플 사용자 음성 각각에 대응되는 실제 감정 즉, 정답(True lable)을 포함할 수 있다. 일 예에 따른, 하나 이상의 프로세서는 크로스 엔트로피 손실(Cross entropy loss), Focal Loss를 이용하여 감정 분류 손실(B)을 획득할 수 있다. 일 예에 따른 하나 이상의 프로세서는 감정 분류 손실(B)에 대응되는 가중치(weight)에 기초하여 감정 분 류기를 학습 또는 업데이트시킬 수 있다(S620). 일 예에 따른 하나 이상의 프로세서는 제1 특징 벡터(Feature vector)(51-1), 제2 특징 벡터(Feature vector)(51-2) 및 제3 특징 벡터(Feature vector)(52-1) 간의 거리 정보에 기초하여 유사도 손실(similarity loss)(A)를 획득할 수 있다. 일 예에 따라 제1 특징 벡터(51-1)와 제2 특징 벡터(51-2)는 제1 샘플 사용자 음성으로부터 획득된 특징 벡 터이므로, 벡터 공간 상에서 제1 특징 벡터(51-1)와 제2 특징 벡터(51-2) 사이의 거리는 가까우며, 제3 특징 벡 터(Feature vector)(52-1)는 제2 샘플 사용자 음성으로부터 획득된 특징 벡터이므로, 벡터 공간 상에서 제1 특징 벡터(51-1)와 제3 특징 벡터(Feature vector)(52-1) 사이의 거리 및 제2 특징 벡터(51-2)와 제3 특징 벡 터(Feature vector)(52-1) 사이의 거리 각각은 멀 수 있다. 일 예에 따른 하나 이상의 프로세서는 Triple loss, Margin loss, MSE 등을 이용하여 유사도 손실 (similarity loss)(A)을 획득할 수 있다. 이어서, 하나 이상의 프로세서는 유사도 손실(similarity loss)(A)과 감정 분류 손실(B)의 집합 (Aggregation)(C)에 대응되는 가중치를 획득하며, 획득된 가중치에 기초하여 비의미적 특징 추출 모델을 학 습 또는 업데이트시킬 수 있다(S630). 일 예에 따라 하나 이상의 프로세서는 감정 분류 손실(B)을 이용하여 비의미적 특징으로부터 예측 감정을 획득할 수 있으나, 감정 분류 손실(B)만을 이용한다면, 동일한 감정(예를 들어, 즐거움)을 가진 모든 음성이 서 로 거의 동일한 비의미적 특징을 포함하는 것으로 왜곡될 여지가 있다. 일 예로, 비의미적 특징 추출 모델을 학습시키는 과정에서 음성의 비의미적 특징에 포함된 감정 특징 외에 화자 정체성, 언어, 운율 특징 등이 제외 되는 문제가 있다. 따라서, 일 예에 따라 감정 분류 손실(B) 외에 유사도 손실(A)도 이용하므로 비의미적 특징 추출 모델을 학습시키는 과정에서 비의미적 특징에 포함된 화자 정체성, 언어, 운율 특징 등도 제외되지 않 을 수 있다. 도 7은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델의 학습을 설명하기 위한 도면이다. 일 예에 따른 합성 음성 분류 모델은 비의미적 특징 추출 모델에 기초하여 전이 학습된 모델일 수 있다. 예를 들어, 하나 이상의 프로세서는 비의미적 특징 추출 모델 및 손실 함수(loss function)에 기초하여 합 성 음성 분류 모델을 전이 학습시킬 수 있다. 우선, 하나 이상의 프로세서는 복수의 샘플 음성 데이터 각각을 증강(Augmentation)(S710-1) 및 전 처리(S710-2)할 수 있다(S610). 일 예로, 증강하는 S710-1 단계는, 복수의 샘플 음성 데이터 각각에 노이즈 추가, 손실 압축(Lossy compression)을 수행하며, 전처리하는 S710-2 단계는, 증강된 복수의 샘플 음성 데이터 각각에 리샘플링 (resampling), 노이즈 제거(denoising), 볼륨 레벨 조정(Adjust volume level)을 수행할 수 있다. 이어서, 하나 이상의 프로세서는 전처리된 복수의 샘플 음성 데이터 각각을 비의미적 특징 추출 모델에 입력할 수 있다. 이어서, 하나 이상의 프로세서는 비의미적 특징 추출 모델이 출력한 비의미적 특징(예를 들어, 전처리 된 복수의 샘플 음성 데이터 각각에 대응되는 특징 벡터)을 합성 음성 분류 모델(Binary deepfake classifier model)에 입력할 수 있다. 일 예에 따른 합성 음성 분류 모델은, 복수의 샘플 음성 데이터 각각을 합성 음성 또는 사용자 음성으로 분류할 수 있다. 한편, 하나 이상의 프로세서는 합성 음성 분류 모델이 복수의 샘플 음성 데이터 각각을 합성 음성 또는 사용자 음성으로 분류한 예측 결과와 실제 결과에 기초하여 분류 손실(Classification loss)를 획득하고, 획득 된 분류 손실에 기초하여 합성 음성 분류 모델을 학습 또는 업데이트시킬 수 있다. 이에 대한 구체적인 설명은 도 8을 참조하여 하도록 한다. 도 8은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델의 학습을 상세히 설명하기 위한 도면이다. 도 8을 참조하면, 복수의 샘플 음성 데이터는 복수의 샘플 사용자 음성 및 복수의 샘플 합성 음성을 포함 할 수 있다. 또한, 복수의 샘플 음성 데이터은 복수의 샘플 사용자 음성 각각이 사용자 음성에 해당함을 나타내는 실 제 라벨(True label) 및 복수의 샘플 합성 음성 각각이 합성 음성에 해당함을 나타내는 실제 라벨을 포함할 수있다. 일 예에 따른 하나 이상의 프로세서는 복수의 샘플 음성 데이터 각각을 증강(Augmentation) 및 전처리할 수 있다(S710). 이어서, 하나 이상의 프로세서는 전처리된 복수의 샘플 음성 데이터 각각을 비의미적 특징 추출 모델에 입력할 수 있다. 이어서, 하나 이상의 프로세서는 전처리된 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징(예를 들어, 특징 벡터)를 획득하며, 전처리된 복수의 샘플 음성 데이터 각각에 대응되는 특징 벡터를 합성 음성 분류 모델에 입력할 수 있다. 이어서, 합성 음성 분류 모델은 복수의 샘플 음성 데이터 각각에 대응되는 특징 벡터에 기초하여 복수의 샘 플 음성 데이터 각각을 합성 음성 또는 사용자 음성 중 어느 하나로 분류한 예측 결과를 출력할 수 있다. 일 예에 따른 하나 이상의 프로세서는 손실 함수에 기초하여 예측 결과와 실제 결과에 대응되는 크로스 엔트로피 손실을 획득하며, 크로스 엔트로피 손실에 기초하여 합성 음성 분류 모델을 학습 또는 업데이트시 킬 수 있다. 여기서, 실제 결과는, 복수의 샘플 음성 데이터 각각에 대응되는 실제 라벨에 기초하여 복수의 샘플 음성 데이 터 각각을 합성 음성 또는 사용자 음성으로 분류한 결과를 포함할 수 있다. 도 9는 본 개시의 일 실시 예에 따른 합성 음성 분류 모델이 음성의 진위 여부를 확인하는 방법을 설명하기 위 한 도면이다. 도 9를 참조하면, 종래의 합성 음성 생성기 또는 음성 모방기(Voice imitator)는 User A가 실제로 발화한 사용 자 음성(즉, 실제 음성)에 대응되는 합성 음성을 출력할 수 있다. 합성 음성 즉, 음성 데이터는 보안과 관련된 인증 과정에서 이용되며, 일 예에 따른 전자 장치에서 실 행 중인 어플리케이션, 프로그램 등은 합성 음성 탐지기(Synthetic voice detector)를 이용하여 음성 데이터 를 합성 음성 또는 사용자 음성 중 어느 하나로 분류할 수 있다. 여기서, 합성 음성 탐지기는, 비의미적 특 징 추출 모델과 합성 음성 분류 모델을 포함하는 모델일 수 있다. 일 예에 따른 전자 장치는 음성 데이터가 합성 음성으로 분류되면, 음성 데이터를 이용하여 인증 과정을 수행하지 않을 수 있다. 다른 예로, 전자 장치는 음성 데이터가 사용자 음성으로 분류되면, 음 성 데이터를 이용하여 인증 과정을 수행할 수 있다. 한편, 합성 음성 분류 모델은, 음성 데이터가 합성 음성에 해당할 확률을 출력할 수도 있다. 일 예에 따라 하나 이상의 프로세서는 합성 음성 분류 모델이 출력한 확률이 임계 확률(예를 들어, 0.7)을 초과하면, 음성 데이터를 합성 음성으로 분류할 수 있다. 일 예에 따른 하나 이상의 프로세서는 전자 장치에서 실행 중인 어플리케이션에 대응되는 보안 레벨 에 기초하여 임계 확률을 조정할 수도 있다. 예를 들어, 어플리케이션이 은행/금융 어플리케이션이고, 은행/금 융 어플리케이션에 대응되는 보안 레벨이 최고 레벨이면, 임계 확률을 낮출 수 있다(예를 들어, 0.7->0.5). 이 어서, 하나 이상의 프로세서는 음성 데이터가 합성 음성으로 분류되면, 음성 데이터를 이용하여 인증 과정을 수행하지 않을 수 있다. 여기서, 구체적인 숫자는 설명의 편의를 위한 일 예시이며 이에 한정되지 않음은 물론이다. 일 예에 따른 하나 이상의 프로세서는 음성 데이터가 합성 음성으로 분류되면, 노티(notification)를 제공할 수도 있다. 도 10은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델이 음성의 진위 여부를 확인하는 방법을 설명하기 위 한 도면이다. 도 10을 참조하면, 종래의 합성 음성 생성기 또는 음성 모방기(Voice imitator)는 User A가 실제로 발화한 사용 자 음성(즉, 실제 음성)에 대응되는 합성 음성을 출력할 수 있다. 일 예에 따라 전자 장치가 합성 음성 즉, 음성 데이터를 수신하며, 전자 장치는 합성 음성 탐지 기(Synthetic voice detector)를 이용하여 음성 데이터를 합성 음성 또는 사용자 음성 중 어느 하나로 분류 할 수 있다. 일 예에 따라, 전자 장치는 수신된 음성 데이터가 합성 음성을 분류되면, 분류 결과를 제공할 수 있다. 일 예에 따라, 전자 장치는 통화 어플리케이션에 따른 상대방(예를 들어, 발신자(User A))가 합성 음성 생 성기를 이용하여 합성 음성을 전송하면, 합성 음성 탐지기(Synthetic voice detector)를 이용하여 합성 음성을 탐지(즉, 수신된 음성 데이터를 합성 음성으로 분류)하며, 탐지 결과를 제공할 수 있다. 도 11은 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델과 합성 음성 분류 모델을 이용에 따른 효과를 설명하기 위한 도면이다. 도 11을 참조하면, 하나 이상의 프로세서는 음성 데이터를 비의미적 특징 추출 모델과 합성 음성 분류 모델를 이용하여 합성 음성 또는 사용자 음성 중 어느 하나로 분류하며, 음성 데이터가 합성 음성 으로 분류되면, 합성 음성으로 식별 가능한 특징(예를 들어, 성문)을 이용하여 사람을 특정하지 않으므로 사기 발생을 방지할 수 있다(예를 들어, 비싱(vishing) 방지). 또한, 하나 이상의 프로세서는 비의미적 특징 추출 모델이 출력한 특징 벡터를 감정 분류기에 입력 하여 음성 데이터가 사용자 음성일 때 사용자 음성을 발화한 사용자의 감정을 예측할 수 있다(즉, 특징 벡 터에 대응되는 예측 감정을 획득할 수 있다). 일 예에 따른 하나 이상의 프로세서는 예측 감정에 대응되는 피드백을 제공할 수 있다. 예를 들어, 하나 이상의 프로세서는 예측 감정이 분노(anger)이면, 뮤직 어플리케이션을 실행하여 노래를 제공할 수 있고, 쇼핑 어플리케이션이 실행 중이면 충동 구매를 방지하기 위한 문구를 제공할 수도 있다. 또한, 하나 이상의 프로세서는 예측 감정이 즐거움 또는 놀라움이면, 사진 어플리케이션을 실행할 수도 있 다. 이는 일 예시이며 이에 한정되지 않음은 물론이다. 예를 들어, 하나 이상의 프로세서는 예측 감정이 혐오이면, SNS 어플리케이션의 실행을 제한할 수도 있다. 도 3으로 돌아와서, 본 개시에 따른 신경망 모델, 비의미적 특징 추출 모델, 합성 음성 분류 모델 및 감 정 분류기를 포함하는 인공지능 모델과 관련된 기능은 전자 장치의 하나 이상의 프로세서와 메모 리를 통해 동작된다. 프로세서는 하나 또는 복수의 프로세서로 구성될 수 있다. 이때, 하나 또는 복수의 프로세서는 CPU(Central Processing Unit), GPU(Graphic Processing Unit), NPU(Neural Processing Unit) 중 적어도 하나를 포함할 수 있으나 전술한 프로세서의 예시에 한정되지 않는다. CPU는 일반 연산뿐만 아니라 인공지능 연산을 수행할 수 있는 범용 프로세서로서, 다계층 캐시(Cache) 구조를 통해 복잡한 프로그램을 효율적으로 실행할 수 있다. CPU는 순차적인 계산을 통해 이전 계산 결과와 다음 계산 결과의 유기적인 연계가 가능하도록 하는 직렬 처리 방식에 유리하다. 범용 프로세서는 전술한 CPU로 명시한 경 우를 제외하고 전술한 예에 한정되지 않는다. GPU는 그래픽 처리에 이용되는 부동 소수점 연산 등과 같은 대량 연산을 위한 프로세서로서, 코어를 대량으로 집적하여 대규모 연산을 병렬로 수행할 수 있다. 특히, GPU는 CPU에 비해 컨볼루션(Convolution) 연산 등과 같 은 병렬 처리 방식에 유리할 수 있다. 또한, GPU는 CPU의 기능을 보완하기 위한 보조 프로세서(co-processor)로 이용될 수 있다. 대량 연산을 위한 프로세서는 전술한 GPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않 는다. NPU는 인공 신경망을 이용한 인공지능 연산에 특화된 프로세서로서, 인공 신경망을 구성하는 각 레이어를 하드 웨어(예로, 실리콘)로 구현할 수 있다. 이때, NPU는 업체의 요구 사양에 따라 특화되어 설계되므로, CPU나 GPU 에 비해 자유도가 낮으나, 업체가 요구하기 위한 인공지능 연산을 효율적으로 처리할 수 있다. 한편, 인공지능 연산에 특화된 프로세서로, NPU 는 TPU(Tensor Processing Unit), IPU(Intelligence Processing Unit), VPU(Vision processing unit) 등과 같은 다양한 형태로 구현 될 수 있다. 인공 지능 프로세서는 전술한 NPU로 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 또한, 하나 또는 복수의 프로세서는 SoC(System on Chip)으로 구현될 수 있다. 이때, SoC에는 하나 또는 복수 의 프로세서 이외에 메모리, 및 프로세서와 메모리 사이의 데이터 통신을 위한 버스(Bus)등과 같은 네트워크 인 터페이스를 더 포함할 수 있다. 전자 장치에 포함된 SoC(System on Chip)에 복수의 프로세서가 포함된 경우, 전자 장치는 복수의 프 로세서 중 일부 프로세서를 이용하여 인공지능과 관련된 연산(예를 들어, 인공지능 모델의 학습(learning)이나 추론(inference)에 관련된 연산)을 수행할 수 있다. 예를 들어, 전자 장치는 복수의 프로세서 중 컨볼루션 연산, 행렬 곱 연산 등과 같은 인공지능 연산에 특화된 GPU, NPU, VPU, TPU, 하드웨어 가속기 중 적어도 하나를 이용하여 인공지능과 관련된 연산을 수행할 수 있다. 다만, 이는 일 실시예에 불과할 뿐, CPU 등과 범용 프로세 서를 이용하여 인공지능과 관련된 연산을 처리할 수 있음은 물론이다. 또한, 전자 장치는 하나의 프로세서에 포함된 멀티 코어(예를 들어, 듀얼 코어, 쿼드 코어 등)를 이용하여 인공지능과 관련된 기능에 대한 연산을 수행할 수 있다. 특히, 전자 장치는 프로세서에 포함된 멀티 코어 를 이용하여 병렬적으로 컨볼루션 연산, 행렬 곱 연산 등과 같은 인공 지능 연산을 수행할 수 있다. 하나 또는 복수의 프로세서는, 메모리에 저장된 기정의된 동작 규칙 또는 인공지능 모델에 따라, 입력 데이터를 처리하도록 제어한다. 기정의된 동작 규칙 또는 인공지능 모델은 학습을 통해 만들어진 것을 특징으로 한다. 여기서, 학습을 통해 만들어진다는 것은, 다수의 학습 데이터들에 학습 알고리즘을 적용함으로써, 원하는 특성 의 기정의된 동작 규칙 또는 인공지능 모델이 만들어짐을 의미한다. 이러한 학습은 본 개시에 따른 인공지능이 수행되는 기기 자체에서 이루어질 수도 있고, 별도의 서버/시스템을 통해 이루어 질 수도 있다. 인공지능 모델은, 복수의 신경망 레이어들로 구성될 수 있다. 적어도 하나의 레이어는 적어도 하나의 가중치 (weight values)을 갖고 있으며, 이전(previous) 레이어의 연산 결과와 적어도 하나의 정의된 연산을 통해 레이 어의 연산을 수행한다. 신경망의 예로는, CNN (Convolutional Neural Network), DNN (Deep Neural Network), RNN (Recurrent Neural Network), RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), BRDNN(Bidirectional Recurrent Deep Neural Network) 및 심층 Q-네트워크 (Deep Q-Networks), Transformer가 있으며, 본 개시에서의 신경망은 명시한 경우를 제외하고 전술한 예에 한정되지 않는다. 학습 알고리즘은, 다수의 학습 데이터들을 이용하여 소정의 대상 기기(예컨대, 로봇)을 훈련시켜 소정의 대상 기기 스스로 결정을 내리거나 예측을 할 수 있도록 하는 방법이다. 학습 알고리즘의 예로는, 지도형 학습 (supervised learning), 비지도형 학습(unsupervised learning), 준지도형 학습(semi-supervised learning) 또 는 강화 학습(reinforcement learning)이 있으며, 본 개시에서의 학습 알고리즘은 명시한 경우를 제외하고 전술 한 예에 한정되지 않는다. 도 12는 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다. 본 개시의 일 예에 따른 전자 장치의 제어 방법은, 우선, 음성 데이터를 비의미적 특징 추출 모델에 입력하여 음성 데이터에 포함된 비의미적 특징을 획득한다(S1210). 이어서, 비의미적 특징을 합성 음성 분류 모델에 입력하여 음성 데이터를 합성 음성 또는 사용자 음성 중 어느 하나로 분류한다(S1220). 이어서, 분류 결과를 제공할 수 있다(S1230). 여기서, 비의미적 특징은, 음성 데이터에 대응되는 특징 벡터를 포함하며, 일 예에 따른 제어 방법은, 복수의 샘플 사용자 음성 중 제1 샘플 사용자 음성 및 제2 샘플 사용자 음성을 획득하는 단계, 제1 샘플 사용자 음성으 로부터 제1 부분 음성 및 제2 부분 음성을 획득하는 단계, 제2 샘플 사용자 음성으로부터 제3 부분 음성을 획득 하는 단계, 제1 내지 제3 부분 음성 각각을 비의미적 특징 추출 모델에 입력하여 제1 내지 제3 부분 음성 각각 에 대응되는 제1 내지 제3 특징 벡터를 획득하는 단계, 제1 내지 제3 특징 벡터에 기초하여 감정 분류 손실 (emotion classification loss) 및 유사도 손실(similarity loss)을 획득하는 단계 및 감정 분류 손실 및 유사 도 손실에 기초하여 비의미적 특징 추출 모델을 업데이트하는 단계를 더 포함할 수 있다. 여기서, 감정 분류 손실 및 유사도 손실을 획득하는 단계는, 제1 특징 벡터 및 제2 특징 벡터를 감정 분류기 (emotion classifier)에 입력하여 제1 특징 벡터에 대응되는 제1 예측 감정(predicted emotion) 및 제2 특징 벡터에 대응되는 제2 예측 감정을 획득하는 단계, 제1 예측 감정 및 제2 예측 감정과 제1 샘플 사용자 음성에 대응되는 제1 실제 감정(true emotion)에 기초하여 감정 분류 손실을 획득하는 단계 및 감정 분류 손실에 대응 되는 가중치에 기초하여 감정 분류기를 업데이트하는 단계를 더 포함할 수 있다. 여기서, 제1 예측 감정과 제2 예측 감정은 동일할 수 있다. 본 개시의 일 예에 따른 감정 분류 손실 및 유사도 손실을 획득하는 단계는, 제1 내지 제3 특징 벡터 간의 거리 정보에 기초하여 유사도 손실을 획득하는 단계를 포함하며, 업데이트하는 단계는, 감정 분류 손실 및 유사도 손실의 집합(Aggregation)에 대응되는 가중치에 기초하여 비의미적 특징 추출 모델을 업데이트하는 단계를 포함할 수 있다. 본 개시의 일 예에 따른 제어 방법은, 비의미적 특징 추출 모델 및 손실 함수(loss function)에 기초하여 합성 음성 분류 모델을 전이 학습시키는 단계를 더 포함할 수 있다. 여기서, 학습시키는 단계는, 복수의 샘플 음성 데이터를 비의미적 특징 추출 모델에 입력하여 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 획득하는 단계, 복수의 샘플 음성 데이터 각각에 대응되는 비의미적 특징을 합성 음성 분류 모델에 입력하여 복수의 샘플 음성 데이터 각각을 합성 음성 또는 사용자 음성 중 어느 하나로 분류한 예측 결과를 획득하는 단계, 손실 함수에 기초하여 예측 결과와 실제 결과에 대응되는 크로스 엔 트로피 손실(Cross entropy loss)을 획득하는 단계 및 크로스 엔트로피 손실에 기초하여 합성 음성 분류 모델을 업데이트하는 단계를 포함할 수 있다. 본 개시의 일 예에 따른 복수의 샘플 음성 데이터는, 복수의 샘플 사용자 음성 및 복수의 샘플 합성 음성을 포 함하며, 실제 결과는, 복수의 샘플 음성 데이터 각각에 대응되는 실제 라벨(True label)에 기초하여 복수의 샘 플 음성 데이터 각각을 합성 음성 또는 사용자 음성으로 분류한 결과일 수 있다. 본 개시의 일 예에 따른 합성 음성 분류 모델은, 음성 데이터가 합성 음성에 포함될 확률을 출력하며, 분류하는 S1220 단계는, 확률이 임계 확률을 초과하면, 음성 데이터를 합성 음성으로 분류하는 단계를 포함할 수 있다. 다만, 본 개시의 다양한 실시 예들은 전자 장치 뿐 아니라, 음성 데이터를 수신 가능한 모든 유형의 전자 장치 에 적용될 수 있음은 물론이다. 한편, 이상에서 설명된 다양한 실시 예들은 소프트웨어(software), 하드웨어(hardware) 또는 이들의 조합을 이 용하여 컴퓨터(computer) 또는 이와 유사한 장치로 읽을 수 있는 기록 매체 내에서 구현될 수 있다. 일부 경우 에 있어 본 명세서에서 설명되는 실시 예들이 프로세서 자체로 구현될 수 있다. 소프트웨어적인 구현에 의하면, 본 명세서에서 설명되는 절차 및 기능과 같은 실시 예들은 별도의 소프트웨어 모듈들로 구현될 수 있다. 소프트 웨어 모듈들 각각은 본 명세서에서 설명되는 하나 이상의 기능 및 동작을 수행할 수 있다. 한편, 상술한 본 개시의 다양한 실시 예들에 따른 전자 장치의 프로세싱 동작을 수행하기 위한 컴퓨터 명령어 (computer instructions)는 비일시적 컴퓨터 판독 가능 매체(non-transitory computer-readable medium) 에 저 장될 수 있다. 이러한 비일시적 컴퓨터 판독 가능 매체에 저장된 컴퓨터 명령어는 특정 기기의 프로세서에 의해 실행되었을 때 상술한 다양한 실시 예에 따른 전자 장치에서의 처리 동작을 특정 기기가 수행하도록 한다. 비일시적 컴퓨터 판독 가능 매체란 레지스터, 캐쉬, 메모리 등과 같이 짧은 순간 동안 데이터를 저장하는 매체 가 아니라 반영구적으로 데이터를 저장하며, 기기에 의해 판독(reading)이 가능한 매체를 의미한다. 비일시적 컴퓨터 판독 가능 매체의 구체적인 예로는, CD, DVD, 하드 디스크, 블루레이 디스크, USB, 메모리카드, ROM 등 이 있을 수 있다. 이상에서는 본 개시의 바람직한 실시 예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시 예에"}
{"patent_id": "10-2022-0174206", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "한정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통상의 지식을 가진자에 의해 다양한 변형실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술적 사상이나 전망으로부터 개별적으로 이해되어져서는 안될 것이다."}
{"patent_id": "10-2022-0174206", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 일 실시 예에 따른 합성 음성을 생성하는 전자 장치를 설명하기 위한 도면이다. 도 2는 본 개시의 일 실시 예에 따른 합성 음성과 사용자 음성을 설명하기 위한 도면이다. 도 3은 본 개시의 일 실시 예에 따른 전자 장치의 구성을 설명하기 위한 블록도이다. 도 4는 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델(Non-Semantic feature extractor model) 및 합 성 음성 분류 모델을 설명하기 위한 도면이다. 도 5는 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델의 학습을 설명하기 위한 도면이다. 도 6은 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델의 학습을 상세히 설명하기 위한 도면이다. 도 7은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델의 학습을 설명하기 위한 도면이다. 도 8은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델의 학습을 상세히 설명하기 위한 도면이다. 도 9는 본 개시의 일 실시 예에 따른 합성 음성 분류 모델이 음성의 진위 여부를 확인하는 방법을 설명하기 위 한 도면이다. 도 10은 본 개시의 일 실시 예에 따른 합성 음성 분류 모델이 음성의 진위 여부를 확인하는 방법을 설명하기 위 한 도면이다. 도 11은 본 개시의 일 실시 예에 따른 비의미적 특징 추출 모델과 합성 음성 분류 모델을 이용에 따른 효과를 설명하기 위한 도면이다. 도 12는 본 개시의 일 실시 예에 따른 전자 장치의 제어 방법을 설명하기 위한 흐름도이다."}
