{"patent_id": "10-2021-0153377", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2022-0080696", "출원번호": "10-2021-0153377", "발명의 명칭": "깊이 추정 방법, 디바이스, 전자 장비 및 컴퓨터 판독가능 저장 매체", "출원인": "삼성전자주식회사", "발명자": "레이 주안"}}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "깊이 추정 방법으로서,기설정된 이미징 평면에 이미지를 매핑하고, 상기 기설정된 평면 상에서 상기 이미지에서의 화소들의 제1 위치정보를 취득하는 단계; 및상기 제1 위치 정보에 기초하여 상기 이미지에 대한 깊이 추정을 수행하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1 항에 있어서, 상기 기설정된 이미징 평면 상에서 상기 이미지에서의 화소들의 제1 위치 정보를 취득하는 단계는,현재 이미징 평면에 대응하는 제2 카메라 파라미터에 기초하여, 상기 현재 이미징 평면 상에서 상기 이미지에서의 화소들의 제2 위치 정보를 취득하는 단계; 및상기 제2 위치 정보, 상기 기설정된 이미징 평면에 대응하는 제1 카메라 파라미터, 및 상기 제2 카메라 파라미터에 기초하여 상기 제1 위치 정보를 취득하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제2 항에 있어서, 상기 카메라 파라미터는 카메라의 초점 거리, 주점의 위치, 및 센서의 사이즈 중 적어도 하나를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제2 항3항에 있어서, 상기 제2 위치 정보, 상기 기설정된 이미징 평면에 대응하는 제1 카메라 파라미터, 및 상기 제2 카메라 파라미터에 기초하여 상기 제1 위치 정보를 취득하는 단계는,상기 제1 카메라 파라미터 및 상기 제2 카메라 파라미터에 기초하여 상기 제1 위치 정보와 상기 제2 위치 정보사이의 매핑 관계를 취득하는 단계; 및상기 제2 위치 정보 및 상기 매핑 관계에 기초하여 상기 제1 위치 정보를 취득하는 단계를 포함하는, 깊이 추정방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1 항에 있어서, 상기 제1 위치 정보에 기초하여 상기 이미지에 대한 깊이 추정을 수행하는 단계는,인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 상기 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하는 단계; 및제1 디코딩 네트워크를 통해, 깊이 추정 결과를 획득하기 위해 상기 특징 맵 및 상기 제1 위치 정보에 기초하여적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5 항에 있어서, 제1 특징 다운샘플링 모듈을 통해, 상기 이미지 또는 상기 특징 맵에 대해 특징 다운샘플링을 수행하여 제1 특징 맵을 획득하는 단계;제2 특징 다운샘플링 모듈을 통해, 상기 이미지 또는 상기 특징 맵에 대해 특징 다운샘플링을 수행하여 제2 특징 맵을 획득하는 단계; 및공개특허 10-2022-0080696-3-상기 제1 특징 맵 및 상기 제2 특징 맵을 융합하여 출력하는 단계를 더 포함하고,상기 인코딩 네트워크는 적어도 하나의 특징 다운샘플링 유닛을 포함하고, 상기 적어도 하나의 특징 다운샘플링유닛은 상기 제1 특징 다운샘플링 모듈과 상기 제2 특징 다운샘플링 모듈을 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6 항에 있어서,상기 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널이 제1 차원의 제1 컨볼루션 커널과 제2 차원의제2 컨볼루션 커널을 포함하고,상기 제2 컨볼루션 커널의 값은 0이고,상기 제1 차원은 이전의 특징 다운샘플링 유닛에서 상기 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션커널들의 수에 기초하여 결정되고,상기 제2 차원은 상기 이전의 특징 다운샘플링 유닛에서 상기 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수에 기초하여 결정되는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제6 항에 있어서,상기 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 상기 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고,상기 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 상기 이전의 특징 다운샘플링 유닛에서 상기 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일한, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제6 항에 있어서,제2 디코딩 네트워크를 통해, 프로세싱 결과를 획득하기 위해 상기 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 더 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9 항에 있어서, 상기 제2 디코딩 네트워크에 의해 획득되는 상기 프로세싱 결과는 상기 이미지의 의미론적 파싱 결과인, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제5 항에 있어서, 상기 깊이 추정 결과를 획득하기 위해 상기 특징 맵 및 상기 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는,적어도 하나의 제1 융합된 특징 맵을 획득하기 위해 상기 제1 위치 정보 및 상기 특징 맵에 대해 특징 융합을수행하는 단계; 및상기 적어도 하나의 제1 융합된 특징 맵에 기초하여 상기 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수행하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11 항에 있어서, 상기 적어도 하나의 제1 융합된 특징 맵에 기초하여 상기 적어도 한 번의 특징 다운샘플링에대응하는 특징 업샘플링을 수행하는 단계는,제2 융합된 특징 맵을 획득하기 위해 상기 특징 업샘플링의 입력 특징 맵 및 상기 제1 융합된 특징 맵에 대해특징 융합을 수행하는 단계; 및상기 제2 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함하는,깊이 추정 방법.공개특허 10-2022-0080696-4-청구항 13 제5항에 있어서,상기 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하는 단계; 및상기 연속적인 적어도 두 개의 이미지들에 기초하여 상기 이미지에 대응하는 제1 디스패리티 정보를 취득하는단계를 더 포함하며,깊이 추정 결과를 획득하기 위해 상기 특징 맵 및 상기 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는,상기 깊이 추정 결과를 획득하기 위해 상기 특징 맵, 상기 제1 위치 정보, 및 상기 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13 항에 있어서, 상기 연속적인 적어도 두 개의 이미지들에 기초하여 상기 이미지에 대응하는 제1 디스패리티정보를 취득하는 단계는,상기 연속적인 적어도 두 개의 이미지들에서의 인접한 두 개의 이미지들 사이의 제2 디스패리티 정보를 취득하는 단계; 및상기 제2 디스패리티 정보에 기초하여 상기 제1 디스패리티 정보를 취득하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14 항에 있어서, 상기 제2 디스패리티 정보에 기초하여 상기 제1 디스패리티 정보를 취득하는 단계는,상기 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를취득하고, 상기 평균 디스패리티 정보 또는 상기 누적된 디스패리티 정보를 상기 제1 디스패리티 정보로서 사용하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제13 항에 있어서, 상기 깊이 추정 결과를 획득하기 위해 상기 특징 맵, 상기 제1 위치 정보, 및 상기 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는,적어도 하나의 제3 융합된 특징 맵을 획득하기 위해 상기 제1 위치 정보, 상기 제1 디스패리티 정보, 및 적어도한 번의 특징 다운샘플링에 의해 출력된 상기 특징 맵에 대해 특징 융합을 수행하는 단계; 및상기 적어도 하나의 제3 융합된 특징 맵에 기초하여 상기 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수행하는 단계를 포함하는, 깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_17", "content": "제16 항에 있어서, 상기 적어도 하나의 제3 융합된 특징 맵에 기초하여 상기 적어도 한 번의 특징 다운샘플링에대응하는 특징 업샘플링을 수행하는 단계는,해당하는 제4 융합된 특징 맵을 획득하기 위해 상기 적어도 하나의 제3 융합된 특징 맵 및 상기 특징 업샘플링의 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및상기 제4 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함하는,깊이 추정 방법."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_18", "content": "적어도 하나의 인스트럭션을 저장하는 메모리; 및적어도 하나의 프로세서를 포함하고,상기 적어도 하나의 프로세서는 상기 적어도 하나의 인스트럭션을 실행하여,공개특허 10-2022-0080696-5-기설정된 이미징 평면에 이미지를 매핑하고, 상기 기설정된 평면 상에서 상기 이미지에서의 화소들의 제1 위치정보를 취득하고,상기 제1 위치 정보에 기초하여 상기 이미지에 대한 깊이 추정을 수행하는, 전자 장치."}
{"patent_id": "10-2021-0153377", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_19", "content": "제1 항 내지 제17 항 중에서 어느 한 항의 방법을 컴퓨터에서 실행시키기 위한 프로그램을 기록한 컴퓨터로 읽을 수 있는 기록매체."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 개시는 깊이 추정 방법, 디바이스, 전자 장비, 및 컴퓨터 판독가능 저장 매체를 제공한다. 이 해법의 깊이 추 정의 관련 단계들은 인공지능 모듈에 의해 프로세싱될 수 있다. 그 해법은 프로세싱될 이미지를 기설정된 평면에 매핑하여 기설정된 이미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 위치 정보를 획득하고, 깊이 추정 프 로세스에서 기설정된 이미징 평면의 프로세싱될 이미지에서의 각각의 화소의 위치 정보를 사용하여 깊이 추정 범 위에 대한 카메라 파라미터들의 영향을 제거하여서, 동일한 네트워크 모델은 상이한 카메라 파라미터들에 대응하 는 프로세싱될 이미지의 깊이를 추정할 수 있다. 광범위한 깊이 추정을 보장하면서, 그 해법은 컴퓨팅 자원들 및 저장 공간을 절약한다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 개시는 컴퓨터 기술 분야에 관한 것이다. 구체적으로는, 본 개시는 깊이 추정 방법, 디바이스, 전자 장비, 및 컴퓨터 판독가능 저장 매체에 관한 것이다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "자동 초점(autofocus)은 많은 스마트 디바이스들에서 이미지들 및 비디오들을 촬영(shooting)할 때 핵심적인 기 능이다. 촬영 디바이스에서부터 물체의 거리에 상관없이, 사용자들은 촬영될 관심 물체가 선명하기를 바라고, 깊이 추정은 빠른 자동 초점을 성취하기 위한 기초가 된다. 그러나, 현존 깊이 추정 방법에 의해 추정될 수 있 는 깊이 범위(depth ragne)는 작고, 현존 깊이 추정 방법은 근거리 또는 장거리 타겟들에 대한 스마트 디바이스 들의 자동 포커싱 요건들을 충족시킬 수 없다. 그러므로, 현존하는 깊이 추정 방법을 개선하는 것이 필요하다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 개시는 깊이 추정 방법, 디바이스, 전자 장비, 및 컴퓨터 판독가능 저장 매체를 제공한다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 목적은 위의 기술적 결함들 중 적어도 하나를 해결하고자 하는 것이다. 본 개시의 실시예들에 의해 제공되는 기술적인 해법들은 다음과 같다: 제1 양태에서, 본 개시의 일 실시예가 깊이 추정 방법을 제공하며, 그 방법은, 기설정된 평면에 프로세싱될 이 미지를 매핑하고, 기설정된 평면 상에서 프로세싱될 이미지에서의 화소들의 제1 위치 정보를 취득하는 단계; 및 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 기설정된 평면 상에서 프로세싱될 이미지에서의 화소들의 제1 위치 정보를 획 득하는 단계는, 현재 이미징 평면에 대응하는 제2 카메라 파라미터에 기초하여 현재 이미징 평면 상에서 프로세 싱될 이미지에서의 화소들의 제2 위치 정보를 취득하는 단계; 및 제2 위치 정보, 기설정된 평면에 대응하는 제1 카메라 파라미터, 및 제2 카메라 파라미터에 기초하여 제1 위치 정보를 취득하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 카메라 파라미터는 카메라의 초점 거리, 주점(principal point)의 위치, 및 센 서의 사이즈 중 적어도 하나를 포함한다. 본 개시의 선택적인 실시예에서, 제2 위치 정보, 기설정된 평면에 대응하는 제1 카메라 파라미터, 및 제2 카메 라 파라미터에 기초하여 상기 제1 위치 정보를 취득하는 단계는, 제1 카메라 파라미터 및 제2 카메라 파라미터 에 기초하여 제1 위치 정보와 제2 위치 정보 사이의 매핑 관계를 취득하는 단계; 및 제2 위치 정보 및 매핑 관 계에 기초하여 제1 위치 정보를 취득하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단 계는, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하는 단계; 및 제1 디코딩 네트워크를 통해, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 제1 특징 다운샘플 링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하여 제1 특징 맵을 획득하고, 제2 특징 다운샘플링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하여 제2 특 징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합 결과를 출력한다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널이 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함하며, 제2 컨볼루션 커널의 값은 0이며, 제1 차원은 이전 의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수에 기초하여 결 정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커 널들의 수에 기초하여 결정된다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널들은 표준 컨볼루션 또는 점 방식(point-wise) 컨볼루션이다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특 징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특 징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운 샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널들은 점 방식 컨볼루션이다. 본 개시의 선택적인 실시예에서는, 제2 디코딩 네트워크를 통해, 해당하는 프로세싱 결과를 획득하기 위해 인코 딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력된 제1 특징 맵에 기초하여 적어도 한 번의 특 징 업샘플링을 수행하는 단계를 더 포함한다. 본 개시의 선택적인 실시예에서, 제2 디코딩 네트워크에 의해 획득되는 해당하는 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결과이다. 본 개시의 선택적인 실시예에서, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제1 융합된 특징 맵을 획득하기 위해 제1 위치 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하는 단계; 및 적어도 하나 의 제1 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수행하는 단계 를 포함한다. 본 개시의 선택적인 실시예에서, 적어도 하나의 제1 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플 링에 대응하는 특징 업샘플링을 수행하는 단계는, 해당하는 제2 융합된 특징 맵을 획득하기 위해 특징 업샘플링 에 대응하는 제1 융합된 특징 맵 및 그것의 대응하는 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제2 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 그 방법은, 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들 을 취득하는 단계; 및 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패 리티 정보를 취득하는 단계를 포함하며, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 깊이 추정 결과를 획득하기 위해 특징 맵, 제1 위치 정보, 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계는, 연속적인 적어도 두 개의 이미지들에서의 인접한 두 개의 이미지들 사 이의 제2 디스패리티 정보를 취득하는 단계; 및 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하 는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계는, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평균 디 스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 깊이 추정 결과를 획득하기 위해 특징 맵, 제1 위치 정보, 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제3 융합된 특징 맵을 획득 하기 위해 제1 위치 정보, 제1 디스패리티 정보, 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에대해 특징 융합을 수행하는 단계; 및 적어도 하나의 제3 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운 샘플링에 대응하는 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 적어도 하나의 제3 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플 링에 대응하는 특징 업샘플링을 수행하는 단계는, 해당하는 제4 융합된 특징 맵을 획득하기 위해 특징 업샘플링 에 대응하는 제3 융합된 특징 맵 및 그것의 대응하는 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제4 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 제2 양태에서, 본 개시의 일 실시예가 이미지 프로세싱 방법을 제공하며, 그 방법은, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하는 단계 로서, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘 플링 모듈은 각각 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하여 제1 특징 맵 및 제 2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 결과를 출력하는, 상기 적어도 한 번의 특징 다운 샘플링을 수행하는 단계; 제1 디코딩 네트워크를 통해, 제1 프로세싱 결과를 획득하기 위해 인코딩 네트워크에 의해 출력된 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계; 및 제2 디코딩 네트워크를 통 해, 제2 프로세싱 결과를 획득하기 위해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력 된 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널이 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함하며, 제2 컨볼루션 커널의 값은 0이며, 제1 차원은 이전 의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수에 기초하여 결 정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커 널들의 수에 기초하여 결정된다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특 징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특 징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운 샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 본 개시의 선택적인 실시예에서, 제1 프로세싱 결과는 프로세싱될 이미지의 깊이 추정 결과이고, 제2 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결과이다. 제3 양태에서, 본 개시의 일 실시예가 깊이 추정 방법을 제공하며, 그 방법은, 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하는 단계; 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계; 및 제1 디스패리티 정보에 기초하여 프로세싱될 이미 지에 대한 깊이 추정을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계는, 연속적인 적어도 두 개의 이미지들에서의 인접한 두 개의 이미지들 사 이의 제2 디스패리티 정보를 취득하는 단계; 및 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하 는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계는, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평균 디 스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제1 디스패리티 정보에 기초하여 깊이 추정을 수행하는 단계는, 인코딩 네트워 크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수 행하는 단계; 및 제1 디코딩 네트워크를 통해, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 디스패리티 정 보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 특징 맵 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제5 융합된 특징 맵을 획득하기 위해 제1 디스패리티 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하는 단계; 및 적어도 하나의 제5 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수행하는 단계를 포함한다.본 개시의 선택적인 실시예에서, 적어도 하나의 제5 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플 링에 대응하는 특징 업샘플링을 수행하는 단계는, 해당하는 제6 융합된 특징 맵을 획득하기 위해 특징 업샘플링 에 대응하는 제5 융합된 특징 맵 및 그것의 대응하는 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제6 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 제4 양태에서, 본 개시의 일 실시예가 깊이 추정 디바이스를 제공하며, 그 디바이스는, 프로세싱될 이미지를 기 설정된 평면에 매핑하고, 기설정된 평면 상에서 프로세싱될 이미지에서의 화소들의 제1 위치 정보를 취득하도록 구성되는 위치 정보 취득 모듈; 및 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하도 록 구성되는 깊이 추정 모듈을 포함한다. 제5 양태에서, 본 개시의 일 실시예가 이미지 프로세싱 디바이스를 제공하며, 그 디바이스는, 인코딩 네트워크 를 통해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하여 해당하는 특징 맵을 획득하도록 구성되는 인코딩 모듈로서, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 제1 특징 다운샘플링 모 듈 및 제2 특징 다운샘플링 모듈은 각각 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행 하여 제1 특징 맵 및 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 특징 맵을 출력하는, 상기 인코딩 모듈; 제1 디코딩 네트워크를 통해 인코딩 네트워크에 의해 출력되는 특징 맵에 기초하여 적어도 한 번 의 특징 업샘플링을 수행하여 제1 프로세싱 결과를 획득하도록 구성되는 제1 디코딩 모듈; 및 제2 디코딩 네트 워크를 통해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력되는 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하여 제2 프로세싱 결과를 획득하도록 구성되는 제2 디코딩 모듈을 포함한 다. 제6 양태에서, 본 개시의 일 실시예가 깊이 추정 디바이스를 제공하며, 그 디바이스는, 프로세싱될 이미지를 포 함하는 연속적인 적어도 두 개의 이미지들을 취득하도록 구성되는 연속 이미지 취득 모듈; 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하도록 구성되는 디스패리 티 정보 취득 모듈; 및 제1 디스패리티 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하도록 구 성되는 깊이 추정 모듈을 포함한다. 제7 양태에서, 본 개시의 일 실시예가 전자 디바이스를 제공하며, 그 디바이스는 메모리와 프로세서를 포함하며; 컴퓨터 프로그램들은 메모리에 저장되며; 프로세서는, 제1 양태의 실시예 또는 제1 양태의 임의의 선 택적인 실시예, 제2 양태의 실시예 또는 제2 양태의 임의의 선택적인 실시예, 제3 양태의 실시예 또는 제3 양태 의 임의의 선택적인 실시예에서 제공되는 방법을 구현하기 위해 컴퓨터 프로그램들을 실행하도록 구성된다. 제8 양태에서, 본 개시의 일 실시예는 컴퓨터 판독가능 저장 매체를 제공하고, 컴퓨터 프로그램들은 컴퓨터 판 독가능 저장 매체 상에 저장된다. 컴퓨터 프로그램들이 프로세서에 의해 실행될 때, 제1 양태의 실시예 또는 제 1 양태의 임의의 선택적인 실시예, 제2 양태의 실시예 또는 제2 양태의 임의의 선택적인 실시예, 제3 양태의 실 시예 또는 제3 양태의 임의의 선택적인 실시예에서 제공되는 방법이 구현된다. 본 개시에 의해 제공되는 기술적 해법에 의해 가져와지는 유익한 효과들은, 다음과 같다: 그 해법은 프로세싱될 이미지를 기설정된 평면에 매핑하여 기설정된 이미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 위치 정보를 획득하고, 깊이 추정 프로세스에서 기설정된 이미징 평면의 프로세싱될 이미지에서 의 각각의 화소의 위치 정보를 사용하여 깊이 추정 범위에 대한 카메라 파라미터들의 영향을 제거하여서, 동일 한 네트워크 모델은 상이한 카메라 파라미터들에 대응하는 프로세싱될 이미지의 깊이를 추정할 수 있다. 광범위 한 깊이 추정을 보장하면서, 그 해법은 컴퓨팅 자원들 및 저장 공간을 절약한다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 발명의 실시예들이 이하에서 상세히 설명될 것이다. 이들 실시예들의 예들은 동일하거나 또는 유사한 참조 번호들이 동일하거나 또는 유사한 요소들 또는 동일하거나 또는 유사한 기능들을 갖는 요소들을 참조하는 도면 들에서 예시되어 있다. 도면들을 참조하여 이하에서 설명되는 실시예들은 예시적이며, 본 발명을 설명하기 위해 사용된 것일 뿐이고 본 발명에 대한 임의의 제한으로서 간주되지 않아야 한다. \"a\", \"an\", \"the\", 및 \"said\"의 사용에 해당하는 단수 형은, 달리 언급되지 않는 한, 복수 형들을 포함하도록"}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "의도될 수 있다는 것이 본 기술분야의 통상의 기술자들에 의해 이해되어야 한다. 이 명세서에서 사용되는 \"포함 한다/포함하는\"라는 용어들은 언급된 특징들, 정수들, 단계들, 동작들, 요소들 및/또는 컴포넌트들의 존재를 명 시하지만, 하나 이상의 다른 특징들, 정수들, 단계들, 동작들, 요소들, 컴포넌트들, 및/또는 그 조합들의 존재 또는 추가를 배제하지 않는다는 것이 추가로 이해되어야 한다. 컴포넌트가 다른 컴포넌트에 \"연결되거나\" 또는 다른 컴포넌트에 \"결합링될\" 때, 이는 다른 요소들에 직접적으로 연결되거나 또는 결합될 수 있거나 또는 그것 들 사이에 개재하는 요소들이 제공될 수 있다는 것이 이해되어야 한다. 덧붙여서, 본 개시에서 사용되는 바와 같은 \"~에 연결되는\" 또는 \"~에 결합되는\"은 무선 연결 또는 결합을 포함할 수 있다. 본 개시에서 사용되는 바 와 같이, \"및/또는\"이란 용어는 하나 이상의 연관된 열거된 아이템들 또는 그 조합들 중 임의의 것 또는 그 전 부를 포함한다. 본 개시의 목적들, 기술적 해법들, 및 장점들을 더 명확하게 하기 위하여, 다음에서는 첨부 도면들을 참조하여 상세히 본 개시의 실시예들을 더 설명할 것이다. 자동 초점이 이미지들 및 비디오들을 촬영할 때 많은 스마트 디바이스들의 핵심 기능이다. 물체와 촬영 디바이 스 사이의 거리에 상관없이, 사용자는 촬영될 관심 물체가 선명하기를 바라지만, 현재 일부 스마트 디바이스들 (이를테면 모바일 폰들)은, 특히 너무 가까운 물체들에 대해 여전히 자동 초점 문제들을 가지고 초점을 맞출 수 없다. 도 1은 자동 초점 후 이미지를 오토-포커싱하고 취득하기 위한 스마트 디바이스(또는 촬영 디바이스)의 프로세싱 프로세스를 도시하는데, 좌측 이미지는 포커싱 전의 입력 이미지이며; 중간 이미지는 입력 이미지에 대응하는 깊이 추정의 결과 이미지(즉, 깊이 이미지)이며; 우측 이미지는 추정된 깊이 이미지에 따라 자동 초점 프로세싱을 한 후의 이미지이다. 많은 스마트 디바이스들, 이를테면 모바일 폰들이, 자동 초점을 성취하기 위해 요구되는 광범위한 깊이 추정을 하며, 이는 0.07m 내지 300m 범위를 커버하는 것을 요구한다. 다르게 말하면, 사용자가 스마트 디바이스들을 사용하여 사진들을 찍을 때, 그들 디바이스들은 단거리 대상들 및 장거리 대상들 양쪽 모두에 대해 자동 초점을 필요로 하며, 다시 말하면, 스마트 디바이스들은 광범위한 깊이 추정 능력들을 가질 필요가 있다. 입력 이미지에서의 각각의 화소의 깊이 값은 카메라 좌표계에서 물체에 대응하는 z-축 지점에서부터 카메라의 광학적 중심(원점)까지의 거리를 지칭한다. 현존 깊이 추정 기법의 원리는 카메라의 이미징 모델에 따라 이미징 평면 상의 이미지의 물체 영역의 사이즈를 사용함으로써 깊이를 추정하는 것이며, 다시 말하면, 촬영 물체와 촬 영 디바이스 사이의 z-축 거리를 추정하는 것이다. 도 2에 도시된 바와 같이, 동일한 실제 물체에 대해, 촬영 디바이스로부터의 거리가 d일 때(다시 말하면, 깊이 값이 d일 때), 상이한 카메라 초점 거리들(f1 및 f2)을 사 용하면, 이미징 평면 상에서 획득된 이미지들의 사이즈는 상이하다. 이미징 평면 상의 물체의 사이즈는 물체의 촬영 디바이스로부터의 거리와 촬영 디바이스의 파라미터들(이를테면 카메라 초점 거리, 주점 위치(또한 주점 좌표들이라 불림), 센서 사이즈 등)에 따라 달라진다. 현존 깊이 추정 기법들의 대부분은 위의 깊이 추정 원리에 기초한 인코더-디코더 네트워크 모델을 사용하여 입 력 이미지에 대응하는 깊이 이미지를 획득하여 깊이 추정을 완료한다. 도 3에 도시된 바와 같이, 깊이 추정 네 트워크 모델은 인코더와 디코더를 포함하며, 보통은 공공 데이터 세트들(NYUDv2 및 KITTI)을 사용하여 깊이 추 정 네트워크 모델을 훈련하고 테스트하고, 동일한 공공 데이터 세트에서 이미지 샘플들에 대응하는 촬영 장비의 카메라 파라미터들은 동일하다. 상이한 카메라 파라미터들에 대응하는 이미징 평면 상의 동일한 깊이 값의 사이 즈들이 상이하다. 다시 말하면, 상이한 카메라 파라미터들에 대응하는 물체의 깊이 값과 이미징 평면 상의 물체 의 사이즈 사이의 대응하는 관계가 상이하다. 따라서 동일한 데이터 세트에서의 이미지 샘플들을 사용하여 깊이 추정 네트워크를 훈련하고 테스트하는 것이 필요하다. 분명히, 위의 기법에 의해 훈련되는 깊이 추정 네트워크 모델은 훈련 데이터 세트에 대응하는 동일한 카메라 파라미터들을 이용하여 이미지들에 대한 깊이 추정을 수행 하는 데에만 사용될 수 있다. 다시 말하면, 훈련된 깊이 추정 네트워크 모델은 특정 데이터세트에 대응하는 특 정 카메라 파라미터들이 커버할 수 있는 깊이 범위만을 커버할 수 있다. 예를 들어, NYUDv2는 실내 장면 데이터 세트이고, 촬영할 때의 해당하는 깊이 범위가 0.5m~10m이면, NYUDv2 훈련을 사용하는 것에 의한 깊이 추정 네트워크 모델은 0.5m~10m의 깊이 범위만을 커버할 수 있다. 또한, KITTI가 실외 장면 데이터 세트이며, 촬영할 때 의 해당하는 깊이 범위가 1m~100m이면, KITTI를 사용하는 것에 의해 훈련된 깊이 추정 네트워크 모델은 1m~100m 의 깊이 범위만을 커버할 수 있다. 각각의 데이터 세트의 깊이 범위는 \"넓은 범위\"의 일부만을 커버할 수 있다. 그러므로, 상이한 훈련 세트들 상에서 훈련된 모델들의 경우, 그 모델들은 깊이 범위의 일부만을 추정할 수 있 다. 현존 깊이 추정 기법의 원리 및 네트워크 구조를 고려하면, 광범위한 깊이 추정을 성취하기 위하여, 스마트 디 바이스에서 상이한 데이터 세트들로부터 훈련된 다수의 깊이 추정 네트워크 모델들을 저장하는 것이 필요하며, 이는 많은 컴퓨팅 자원들 및 저장 공간들을 소비할 것이다. 위의 문제들의 견지에서, 본 개시의 실시예들은 깊 이 추정 방법을 제공하며, 이는 다음에서 추가로 설명될 것이다. 도 4는 본 개시의 일 실시예에 의해 제공되는 깊이 추정 방법의 개략적인 흐름도이다. 도 4에 도시된 바와 같이, 그 방법은, 단계 S401: 기설정된 평면에 프로세싱될 이미지를 매핑하고, 기설정된 평면 상에서 프로세싱될 이미지에서의 화 소들의 제1 위치 정보를 취득하는 단계; 및 단계 S402: 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단계를 포함할 수 있다. 여기서, 제1 위치 정보는 각각의 화소의 좌표들일 수 있다 구체적으로는, 상이한 카메라 파라미터들에 대응하는 다수의 프로세싱될 이미지들의 경우, 그 이미지들은 프로 세싱될 각각의 이미지에 대응하는 제1 위치 정보를 취득하기 위해 동일한 기설정된 평면(기설정된 평면은 기설 정된 이미징 평면이라고 또한 지칭될 수 있음)에 각각 매핑되며, 게다가 기설정된 이미징 평면 상의 프로세싱될 각각의 이미지에서의 물체의 사이즈는 제1 위치 정보에 따라 취득될 수 있고, 그러면 물체의 깊이 값은 그 사이 즈에 따라 추정된다. 프로세싱될 상이한 이미지들이 동일한 기설정된 이미징 평면에 매핑되기 때문에, 프로세싱 될 각각의 이미지에서의 물체 사이즈의 계산 스케일은 동일하므로, 물체 사이즈와 프로세싱될 각각의 이미지에 서의 물체 깊이 값 사이의 대응 관계는 일정하다. 다르게 말하면, 프로세싱될 이미지는 기설정된 이미징 평면에 매핑되고 그러면 깊이 추정은 기설정된 이미징 평면에서 프로세싱될 이미지에서의 물체의 사이즈를 사용하여 수 행되며, 이를 수행함으로써 동일한 깊이 추정 네트워크 모델의 깊이 추정 범위에 대한 카메라 파라미터들의 영 향은 제거된다. 다시 말하면, 동일한 깊이 추정 네트워크 모델은 상이한 카메라 파라미터들에 대응하는 프로세 싱될 이미지의 깊이 추정을 실현하는데 사용될 수 있다. 본 개시에 의해 제공되는 해법은 프로세싱될 이미지를 기설정된 평면에 매핑하여 기설정된 이미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 위치 정보를 획득하고, 깊이 추정 프로세스에서 기설정된 이미징 평면의 프 로세싱될 이미지에서의 각각의 화소의 위치 정보를 사용하여 깊이 추정 범위에 대한 카메라 파라미터들의 영향 을 제거하여, 동일한 네트워크 모델은 상이한 카메라 파라미터들에 대응하는 프로세싱될 이미지의 깊이를 추정 할 수 있다. 광범위한 깊이 추정을 보장하면서, 그 해법은 컴퓨팅 자원들 및 저장 공간을 절약한다. 본 개시의 선택적인 실시예에서, 기설정된 이미징 평면 상에서 프로세싱될 이미지에서의 화소들의 제1 위치 정 보를 취득하는 단계는, 현재 이미징 평면 상에서 프로세싱될 이미지에서의 화소들의 제2 위치 정보를 취득하고, 현재 이미징 평면에 대 응하는 제2 카메라 파라미터를 취득하는 단계; 및 제2 위치 정보, 기설정된 이미징 평면에 대응하는 제1 카메라 파라미터, 및 제2 카메라 파라미터에 기초하여 제 1 위치 정보를 취득하는 단계를 포함한다. 게다가, 제2 위치 정보, 기설정된 이미징 평면에 대응하는 제1 카메라 파라미터, 및 제2 카메라 파라미터에 기 초하여 제1 위치 정보를 취득하는 단계는, 제1 카메라 파라미터 및 제2 카메라 파라미터에 기초하여 제1 위치 정보와 제2 위치 정보 사이의 매핑 관계를 취득하는 단계; 및 제2 위치 정보 및 매핑 관계에 기초하여 제1 위치 정보를 취득하는 단계를 포함한다. 구체적으로는, 현재 이미징 평면에서부터 기설정된 이미징 평면으로 프로세싱될 이미지를 매핑하는 프로세스는 프로세싱될 이미지 내 각각의 화소의 좌표들을 변환하는, 다시 말하면, 현재 이미징 평면에서의 각각의 화소의 제2 위치 정보를 기설정된 이미징 평면에서의 제1 위치 정보로 변환하는 프로세스이다.구체적으로는, 도 5에 도시된 바와 같이, 기설정된 이미징 평면(U)에 대응하는 제1 카메라 파라미터들은, 초점 거리 , 센서 사이즈 를 포함하고, 이미징 평면(P)에 대응하는 제2 카메라 파라미터들은 초점 거리 , 센서 사이즈 , 및 주점 좌표들 을 포함한다. 위의 좌표 변환 프로세스는, 다음과 같은 과정을 포함할 수 있다. 먼저, 현재 이미징 평면(P)에서 프로세싱될 이미지의 각각의 화소의 좌표들(다시 말하면, 제2 위치 정보)을 결 정하는 것을 포함할 수 있으며, 그 수학식은 다음과 같다:"}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "여기서, 는 현재 이미징 평면(P)에서 화소 (i,j)의 x축 좌표이고, 는 현재 이미징 평면(P)에서 화소 (i,j)의 y축 좌표이다. 그러면, 삼각형의 닮음 원리에 따라, 현재 이미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 좌표들은 기 설정된 이미징 평면 상의 좌표들(다시 말하면, 제1 위치 정보)로 변환된다. 다시 말하면, 현재 이미징 평면과 기설정된 이미징 평면 사이의 매핑 관계는 삼각형의 닮음 원리에 따라 취득되고, 그러면 제1 위치 정보는 매핑 관계 및 제2 위치 정보에 따라 취득되고, 그 수학식은 다음과 같다:"}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "여기서, 는 기설정된 이미징 평면에서 화소 (i,j)의 x축 좌표이며, 는 기설정된 이미징 평면 (U)에서 화소 (i,j)의 y축 좌표이다. 이미지 매핑 모듈(IPM)은 프로세싱될 이미지에서 각각의 화소의 좌표 변환을 위한 위의 프로세싱 프로세스에 따 라 기설정될 수 있고, IPM의 입력은 프로세싱될 이미지 및 대응하는 카메라 파라미터들(즉, 제2 카메라 파라미 터)이며, 출력은 프로세싱될 이미지에 대응하는 제1 위치 정보라는 것이 이해될 수 있다. 도 6은 IPM 모듈의 입 력 및 출력의 개략도이다. 제2 카메라 파라미터들에 대응하는 x축 카메라 파라미터들 및 y축 카메라 파라미터들 이 입력되고, 제2 카메라 파라미터들에 대응하는 x축 카메라 파라미터들 및 y축 카메라 파라미터들이 IPM을 통 과한 후, 사이즈 W*H의 두 개의 좌표 행렬들이 출력된다(이는 W*H*2로서 기록될 수 있음). 이들 두 개의 좌표 행렬들에서의 각각의 요소는 기설정된 이미징 평면에서의 대응하는 화소의 x축 좌표 및 y축 좌표이다. 본 개시의 선택적인 실시예에서, 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단 계는, 미리 정의된 네트워크 모델을 통해, 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단계를 포함한다. 구체적으로는, 미리 정의된 네트워크 모델은, 제1 위치 정보와 조합하여, 프로세싱될 이미지에 대한 깊이 추정 을 수행하고, 해당하는 깊이 추정 결과를 출력하는데 사용된다. 구체적으로는, 미리 정의된 네트워크 모델은 IPM과 조합될 수 있고, 미리 정의된 네트워크 모델은 프로세싱될 이미지의 깊이 추정 프로세스 동안 IPM에 의해 출력되는 제1 위치 정보를 사용하여 해당하는 깊이 추정 결과를 출력한다. 본 개시의 선택적인 실시예에서, 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단 계는, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다 운샘플링을 수행하는 단계; 및제1 디코딩 네트워크를 통해, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 여기서, 획득된 깊이 추정 결과는 해당하는 깊이 이미지일 수 있다. 구체적으로는, 인코딩 네트워크에서, 특징 다운샘플링은 프로세싱될 이미지에 대해 여러 번 수행되고, 각각의 특징 다운샘플링은 해당하는 특징 맵을 생성할 것이고, 각각의 특징 다운샘플링에 의해 생성되는 특징 맵은 인 코딩 네트워크에 의해 출력된 특징 맵으로서 이해될 수 있다. 한편, 제1 디코딩 네트워크에서, 인코딩 네트워크 에 의해 출력된 특징 맵과 제1 위치 정보를 조합하여, 특징 업샘플링은 여러 번 인코딩 네트워크에 의해 출력된 특징 맵(다시 말하면, 인코딩 네트워크의 출력 특징 맵)에 대해 수행되어 프로세싱될 이미지에 대응하는 깊이 이미지를 획득하며, 각각의 특징 업샘플링은 인코딩 네트워크에서의 하나의 특징 다운샘플링에 대응한다. 도 7 에 도시된 바와 같이, 미리 정의된 네트워크 모델은 IPM과 조합된다. IPM에 의해 출력된 제1 위치 정보와 인코 딩 유닛(즉, 인코딩 네트워크)에서의 특징 다운샘플링에 대응하는 특징 맵은 디코딩 유닛(즉, 제1 디코딩 네트 워크)에 대응하는 특징 업샘플링에서 함께 사용된다. 본 개시의 선택적인 실시예에서, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제1 융합된 특징 맵을 획득하기 위해 제1 위치 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하는 단계; 및 적어도 하나의 제1 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하는 단계를 포함한다. 게다가, 적어도 하나의 제1 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘 플링을 수행하는 단계는, 해당하는 제2 융합된 특징 맵을 획득하기 위해 특징 업샘플링에 대응하는 제1 융합된 특징 맵 및 그것의 대응하 는 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제2 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 구체적으로는, 먼저, 제1 위치 정보와 적어도 한 번의 특징 다운샘플링에 대응하는 특징 맵은 특징 융합되어 적 어도 하나의 제1 융합된 특징 맵을 획득하며; 그 다음에 각각의 제1 융합된 특징 맵은 특징 다운샘플링에 대응 하는 해당하는 특징 업샘플링을 위해 사용된다. 구체적으로는, 특징 융합은 각각의 제1 융합 특징 및 해당하는 특징 업샘플링의 입력 특징 맵에 대해 수행되어 제2 융합된 특징 맵을 획득하고, 그 다음에 특징 업샘플링 프로 세싱은 제2 융합된 특징 맵에 대해 수행된다. 제1 융합된 특징 맵을 취득하는 프로세스에서, 제1 위치 정보와의 특징 융합을 위한 특징 맵은 실제 필요에 따 라 선택될 수 있다는 것에 주의해야 한다. 예를 들어, 하나의 특징 다운샘플링에 의해 출력된 특징 맵들이 선택 될 수 있거나, 또는 다수 회의 특징 다운샘플링에 의해 출력된 특징 맵들이 선택될 수 있다. 제2 융합된 특징 맵을 취득하는 후속 프로세스에서, 해당하는 특징 업샘플링의 입력 특징 맵 및 해당하는 제1 융합된 특징 맵을 특징 융합을 위해 선택하는 것이 필요하며, 다시 말하면, 제1 융합된 특징 맵을 대응하는 특징 업샘플링에 적용 하는 것이 필요하다. 많은 촬영 디바이스들에 대해, 미리보기 모드에서, 자동 노출 및 자동 초점은 동일한 시간에 이용 가능할 필요 가 있는 기능들이다. 종래 기술에서, 이들 두 개의 기능들은 성취할 상이한 네트워크 모델들을 요구하며, 도 8 에 도시된 바와 같이, 자동 노출 기능의 경우, 인간 파싱 및 노출 파라미터 설정들을 실현할 수 있는 네트워크 모델을 요구하며, 25ms를 소요한다. 자동 초점 기능의 경우, 깊이 추정을 구현할 수 있는 모델을 요구하며, 약 7ms 내지 10ms를 소요한다. 두 개의 네트워크 모델들이 따로따로 동작되면, 이는 많은 컴퓨팅 자원들과 시간을 소비할 것이며, 이는 실시간 성능에 영향을 미칠 것이다. 미리보기 모드에서, 인간 파싱 태스크를 위해 추출된 특징들과 깊이 추정을 위해 추출될 필요가 있는 특징들 사 이에 많은 유사점들이 있다는 것을 고려한다. 위의 인코딩 네트워크는 인간 파싱 태스크 및 깊이 추정 태스크에 의해 어느 정도 공유될 수 있다. 인간 파싱 태스크에 더하여, 다른 태스크들에 의해 추출된 특징들이 깊이 추정 태스크에 의해 사용될 수 있으면, 다른 태스크들은 깊이 추정 태스크와 인코딩 네트워크를 또한 공유할 수 있다는 것이 이해될 수 있다.예를 들어, 다른 태스크들은 또한 더 광범위하게 사용되는 의미론적 파싱 태스크일 수 있으며, 본 개시의 실시 예는 인간 파싱 태스크를 해법을 상세히 설명하기 위한 일 예로서만 사용하지만, 이에 한정되지 않는다. 본 개 시의 실시예에 의해 제공되는 해법은 미리보기 모드뿐만 아니라 비미리보기(non-preview) 모드에도 또한 적용될 수 있다는 것이 또한 이해될 수 있다. 인간 파싱의 효과를 보장하면서 깊이 추정을 수행하기 위하여, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들 을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈(이를테면 의미론적 파싱 인코딩 모듈과, 추가로, 이를테면 인간 파싱 인코딩 모듈)과 제2 특징 다운샘플링 모듈(즉, 깊이 추정 인코딩 모듈)을 포함한다. 제1 특징 다운샘플링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하 여 제1 특징 맵을 획득하고, 제2 특징 다운샘플링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다 운샘플링을 수행하여 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 결과를 출력한다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널들이 표준 컨볼루션 또는 점 방식 컨볼루션이면, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널은 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함한다. 이때, 제2 컨볼루션 커널의 값이 0이고, 제1 차원은 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수에 기초하여 결정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널에 기초하여 결정된다. 구체적으로는, 표준 컨볼루션 또는 점 방식 컨볼루션의 컨볼루션 커널은 일반적으로 3차원 컨볼루션 커널이다. 각각의 3차원 컨볼루션 커널은 다수의 2차원 컨볼루션 커널들의 중첩으로서 간주될 수 있다. 그러면 3차원 컨볼 루션 커널은 두 개의 부분들, 다시 말하면, 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널로 나누어질 수 있다. 예를 들어, a*b*c의 차원을 갖는 3차원 컨볼루션 커널이 a1*b*c(제1 차원)의 제1 컨볼루션 커널과 a2*b*c(제2 차원)의 제2 컨볼루션 커널로 나누어질 수 있고, a=a1+a2이며, a1 및 a2는 각각 제1 컨볼루 션 커널 및 제2 컨볼루션 커널의 높이라고 지칭될 수 있고, 높이는 이전의 특징 다운샘플링 유닛의 컨볼루션 커 널들의 수에 따라 결정될 수 있다. 구체적으로는, 제1 컨볼루션 커널의 높이는 이전의 특징 다운샘플링 유닛에 서의 제1 특징 다운샘플링 모듈의 컨볼루션 커널들의 수와 동일하고, 제2 컨볼루션 커널의 높이는 이전의 특징 다운샘플링 유닛에서의 제2 특징 다운샘플링 모듈의 컨볼루션 커널들의 수와 동일하고, 제2 컨볼루션 커널은 0 이며(다시 말하면, 제2 컨볼루션 커널에서의 모든 가중치들은 0이다). 위의 컨볼루션 커널 설정을 통해, 제1 특 징 다운샘플링 모듈은 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 출력된 특징 맵에서 특징 정보만을 추출하는 것이 보장될 수 있으며, 다시 말하면, 인간 파싱 모듈은 이전의 특징 다운샘플링 유닛 에서 인간 파싱 모듈에 의해 출력된 특징 맵에서 인간 파싱 특징 정보만을 추출하는 것을 보장할 수 있다. 덧붙 여서, 3차원 컨볼루션 커널에서 중첩된 각각의 2차원 컨볼루션 커널은 슬라이스라고 또한 불릴 수 있으며, 다시 말하면, 각각의 3차원 컨볼루션 커널은 다수의 슬라이스들로 구성되는데, 제1 컨볼루션 커널의 슬라이스들의 수 는 이전의 특징 다운샘플링 유닛에서의 제1 특징 다운샘플링 모듈의 컨볼루션 커널들의 수와 동일하고, 제2 컨 볼루션 커널의 슬라이스들의 수는 이전의 특징 다운샘플링 유닛에서의 제2 특징 다운샘플링 모듈의 컨볼루션 커 널들의 수와 동일하다. 제2 특징 다운샘플링 모듈의 컨볼루션 커널의 높이는 제1 특징 다운샘플링 모듈의 컨볼루션 커널의 높이와 동일 하다(이 컨볼루션 커널들의 차원들은 동일하다)는 것이 이해될 수 있다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널이 깊이 방식 컨볼루션 커널이면, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에 서 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 구체적으로는, 깊이 방식 컨볼루션을 위한 컨볼루션 커널은 일반적으로 2차원 컨볼루션 커널이다. 각각의 컨볼 루션 커널은 이전의 특징 다운샘플링 유닛에 의해 출력된 채널의 특징 맵에 대해 컨볼루션 연산을 수행하며, 그 러면 깊이 방식 컨볼루션을 위한 컨볼루션 커널은 이전의 특징 다운샘플링 유닛에서의 채널들의 수와 동일하다. 구체적으로는, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유 닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특징 다운샘플링 모 듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의 해 사용되는 컨볼루션 커널들의 수와 동일하다.본 개시의 선택적인 실시예에서, 그 방법은 추가로, 제2 디코딩 네트워크를 통해, 해당하는 프로세싱 결과를 획득하기 위해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력된 상기 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계; 및 제1 디코딩 네트워크를 통해, 해당하는 프로세싱 결과를 획득하기 위해 인코딩 네트워크에서 적어도 하나의 제2 다운샘플링 모듈에 의해 출력된 제2 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링을 수행하는 단계를 더 포함할 수 있다. 여기서, 제2 디코딩 네트워크에 의해 획득된 해당하는 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결 과일 수 있다. 제1 디코딩 네트워크에 의해 획득된 해당하는 프로세싱 결과는 프로세싱될 이미지의 깊이 추정 결과일 수 있다. 구체적으로는, 의미론적 파싱은 의미론적 세그먼트화와 유사하고, 두 가지의 기본 태스크는 각각의 화소에 카테 고리를 배정하는 것으로 간주될 수 있다. 일반적으로, 의미론적 파싱은 보통 의미론적 세그먼트화보다 더 상세 하다. 예를 들어, 의미론적 세그먼트화는 인간 신체, 청색 하늘, 잔디 등으로 나누어질 수 있으며; 의미론적 파 싱은 눈썹들, 코, 입 등으로 나누어질 수 있다. 다음으로, 위의 해법을 상세히 설명하기 위한 일 예로서 제1 특징 다운샘플링 모듈을 인간 파싱 인코딩 모듈로 서 계속 사용한다. 본 개시의 미리 정의된 네트워크 모델은 현존 인간 파싱 네트워크 모델의 확장에 의해 취득 된 것으로 이해될 수 있다. 도 9a에 도시된 바와 같이. 이는 인간 파싱 인코딩 모듈과 인간 파싱 디코딩 모듈을 포함하는 현존 인간 파싱 네트워크 모델의 네트워크 구조의 개략도이다. 구체적으로는, 도 9b에 도시된 바와 같이, 인간 파싱 네트워크 모델의 인간 파싱 인코딩 모듈 부분은 표준 일반 컨볼루션(즉, 표준 컨볼루션)과 채널 별(깊이 방식) 컨볼루션을 포함하는 다수의 컨볼루션 계층들로 구성된다. 입력 이미지가 다수의 컨볼루션 계층들을 통과한 후, 얕은 특징 맵들, 중간 특징 맵들, 및 깊은 특징 맵들이 획 득될 수 있다. 예를 들어, 입력 이미지는 C 개의 1*k*k 컨볼루션 커널들 뒤에 C 개의 얕은 특징 맵들을 얻고, 그 다음에 D 개의 표준 컨볼루션 커널들을 통하여 D 개의 중간 특징 맵들을 획득한 후, 그 다음에 D 개의 깊이 방식 컨볼루션 커널들(이는 채널 별 컨볼루션 커널이라고 또한 지칭될 수 있음)을 통하여 D 개의 깊은 특징 맵 들을 획득한다. 다음은 인간 파싱 디코딩 모듈(다시 말하면, 제2 디코딩 네트워크)이다. 인간 파싱 디코딩 모듈 은 다수의 컨볼루션 계층들로 또한 구성된다. 인간 파싱 디코딩 모듈은 인간 파싱 인코딩 모듈의 출력 이미지에 대해 특징 업샘플링을 수행하고, 최종 출력은 프로세싱될 이미지에 대응하는 인간 파싱 결과이다. 구체적으로는, 깊이 추정 인코딩 모듈(즉, 제2 특징 다운샘플링 모듈)이 인간 파싱 인코딩 모듈(즉, 제1 특징 다운샘플링 모듈)에 기초하여 확장된다. 다시 말하면, 본 개시의 실시예의 미리 정의된 네트워크 모델에서 인코 딩 네트워크가 획득된다. 한편, 깊이 추정 인코딩 유닛(즉, 제1 디코딩 네트워크)에 대응하는 깊이 추정 디코딩 유닛이 확장되며, 깊이 추정 디코딩 유닛과 인간 파싱 디코딩 유닛(즉, 제2 디코딩 네트워크)은 본 개시의 실시 예의 미리 정의된 네트워크 모델에서의 디코딩 네트워크를 구성한다. 미리 정의된 네트워크 모델의 네트워크 구 조의 개략도는 도 10a에 도시된다. 구체적으로는, 도 10b에 도시된 바와 같이, 인간 파싱 인코딩 모듈 부분을 확장하는 방법은: 인간 파싱 네트워 크 모델에서, 프로세싱될 이미지로부터 얕은 특징 맵이 출력될 때, 인간 파싱 태스크는 C 개의 1*k*k 컨볼루션 커널들을 요구하고, C' 개의 1*k*k 컨볼루션 커널들이 추가적으로 깊이 추정 태스크에 추가된다. 그러면, (C+C') 개 컨볼루션 커널들로 인해, 총 (C+C') 개 채널들의 얕은 특징 맵들이 획득된다. (인간 파싱 태스크는 C 개 채널들의 얕은 특징 맵들에 대응하고, 깊이 추정 태스크는 C' 개 채널들의 얕은 특징 맵들에 대응함) 그러면 (D+D') 개 채널들의 중간 특징 맵들(인간 파싱 태스크는 D 개 채널들의 중간 특징 맵들에 대응하고, 깊이 추정 태스크는 D' 개 채널들의 중간 특징 맵들에 대응함)이 중간 특징 맵들의 부가적으로 추가된 D' 개 (C+C')*k*k 채널들을 통과한 후 획득된다. 이후 (D+D') 개 채널들의 깊은 특징 맵들(인간 파싱 태스크는 D 개 채널들의 깊 은 특징 맵들에 대응하고, 깊이 추정 태스크는 깊은 D' 개 채널들의 특징 맵들에 대응함)이 (D+D') 개 1*k*k 깊 이 방식 커널들을 통과한 후 획득되고, (M+M') 개 채널들의 특징 맵들이 (M+M') 개 (D+D') *1*1 점 방식 컨볼루 션 커널의 계산 후에 획득된다. (인간 파싱 태스크는 M 개 채널들의 특징 맵들에 대응하고, 깊이 추정 태스크는 M' 개 채널들의 특징 맵에 대응한다) 다음은 인간 파싱 디코딩 모듈 부분과 깊이 추정 디코딩 모듈 부분이다. 인간 파싱 디코딩 모듈은 인간 파싱 인코딩 모듈에 대응하는 출력 특징 맵에 대해 특징 업샘플링을 수행하고, 깊이 추정 디코딩 모듈은 깊이 추정 인코딩 모듈에 대응하는 출력 특징에 대해 특징 업샘플링을 수행하고, 최종출력은 프로세싱될 이미지에 대응하는 인간 파싱 결과 및 깊이 추정 결과(즉, 깊이 이미지)이다. 위의 특징 다 운샘플링 프로세스로부터 알 수 있는 바와 같이, 표준 컨볼루션 동작 프로세스 및 점 방식 컨볼루션 동작 프로 세스에서, 인간 파싱 인코딩 모듈은 입력 특징 맵에서 인간 파싱 태스크에 대응하는 특징 맵 중 일부의 특징들 만을 추출하는 반면, 깊이 추정 인코딩 모델은 입력 특징 맵에서 인간 파싱 태스크에 대응하는 특징 맵 중 일부 의 특징들과 깊이 추정 태스크에 대응하는 특징 맵 중 일부의 특징을 동시에 추출한다. 본 개시의 실시예들에 의해 제공되는 인코딩 네트워크의 경우, 인간 파싱 태스크에서 추출된 특징들을 사용할 때, 위의 표준 컨볼루션 동작 방법만 또는 위의 조합 깊이 방식 컨볼루션 연산 및 점 방식 컨볼루션만이 사용될 수 있거나, 또는 위의 두 개의 방법들이 동시에 사용될 수 있다는 것에 주의해야 한다. 마찬가지로, 위의 \"얕은 특징 맵들\", \"중간 특징 맵들\" 및 \"깊은 특징 맵들\"은 예들일 뿐이며, 이로 제한되지 않으며, 다시 말하면, 컨 볼루션 계층에 의해 출력된 특징 맵들이 \"얕은 특징 맵들\"인지, \"중간 특징 맵\"인지 또는 \"깊은 특징 맵\"인지는 실제 필요에 따라 정의될 수 있다. 게다가, 깊이 추정 인코딩 모듈을 획득하기 위해 인간 파싱 인코딩 모듈을 확장하는 프로세스에서, 먼저, 깊이 추정에 대응하는 새로운 컨볼루션 커널을 결정하며; 그 다음에, 인간 파싱이 깊이 추정에 의해 추출된 특징들을 사용할 필요가 없기 때문에, 그리고 한편으로는 컴퓨테이션적으로 값비싼 병합 연산들을 사용하는 것을 피하기 위하여, 0들의 패딩이 인간 파싱 인코딩 모듈에 대응하는 컨볼루션 커널에 대해 수행되고, 제로 패딩의 차원은 추가된 컨볼루션 커널의 차원과 동일하며; 마지막으로, 깊이 추정이 인간 파싱에 의해 추출된 특징들을 사용할 수 있기 때문에, 추가된 컨볼루션 커널과 인간 파싱 모듈에 대응하는 제로 패딩 전의 컨볼루션 커널의 중첩은 깊이 추정 인코딩 모듈에 대응하는 컨볼루션 커널이다. 도 9b 및 도 10b를 다시 참조하면, 중간 특징 맵들을 획 득하기 위해 D 개 표준 컨볼루션들을 통과하는 얕은 특징 맵을 일 예를 들면, 차원 C*k*k를 갖는 D 개 컨볼루션 커널들은 인간 파싱 네트워크 모델에서 사용되고, 차원 (C+C')*k*k를 갖는 D 개 컨볼루션 커널들은 본 개시의 실시예의 미리 정의된 네트워크 모델의 인간 파싱 인코딩 모듈 부분에서 사용된다. 또한, 도 11a에 도시된 바와 같이, 그것의 처음 C차원 컨볼루션 커널(C의 높이를 갖는 컨볼루션 커널 부분)은 인간 파싱 네트워크 모델에서 의 컨볼루션 커널과 동일하고, 추가된 C' 차원 컨볼루션 커널(C'의 높이를 갖는 컨볼루션 커널 부분)은 0의 값 을 가져서, 인간 파싱에 영향을 미치는 일 없이 계산 비용이 많이 드는 병합 연산들을 사용하는 것을 피한다. 미리 정의된 네트워크 모델의 깊이 추정 인코딩 모듈 부분에서 사용되는 컨볼루션 커널의 차원은 또한 (C+C')*k*k이다. 전술한 바는 컨볼루션 커널을 확장하는 (다시 말하면, 인코딩 모듈을 확장하는) 방법이고, 상이한 확장 방법들 이 상이한 네트워크 분기들에서 사용될 수 있다는 것에 주의해야 한다. 예를 들어, 인간 파싱 인코딩 모듈로부 터의 깊이 방식 컨볼루션 커널들의 경우, 그것들이 깊이 방식 컨볼루션 계층들에 의해 사용되는 컨볼루션 커널 들이면, 그것들은 0들로 채워질 필요가 없다. 깊이 추정 인코딩 모듈로부터의 표준 컨볼루션 커널의 경우, 현재 계층이 인간 파싱에 의해 추출된 특징들을 사용할 필요가 없으면, 현재 계층 컨볼루션 커널은 도 11b에 도시된 바와 같이 0들로 채워질 필요가 있는데, C에 대응하는 차원의 컨볼루션 커널들은 모두 0이고, C'에 대응하는 차 원의 컨볼루션 커널은 깊이 추정 컨볼루션 커널이다. 인코딩 네트워크를 공유하는 인간 파싱 태스크 및 깊이 추정 태스크의 위의 기법에서, 한편으로는, 깊이 추정의 성능은 개선되고, 다른 한편으로는, 미리 정의된 네트워크 모델의 사이즈는 성능 개선을 전제로 제어된다. 특정 분석은 다음과 같이 나타내어진다: 성능: 인간 파싱의 특징들과 깊이 추정 태스크들의 특징들 사이에 많은 유사도들이 있고, 깊이 추정 태스크 들에서의 인간 파싱의 특징들의 재사용은 깊이 추정의 효과를 개선시킬 수 있다. 별도의 깊이 추정 태스크의 네 트워크와 비교하면, 이 기법에서 깊이 추정 태스크 네트워크의 인코딩 부분은 더 많은 의미론적 특징들을 획득 할 수 있으며, 특징들의 수가 더 많을 뿐만 아니라, 특징들의 의미가 더 풍부해지고, 더 많은 의미론적 특징들 이 깊이 추정의 성능을 개선시킬 수 있다. 예를 들어, 알려진 이미지에서 각각의 화소가 속한 범주에 대해, 예 를 들어, 특정한 부분은 인간 신체이며, 화소의 이 부분의 깊이 값은 유사해야 한다. 모델 사이즈: 두 개의 모델들이 별도의 인간 파싱 및 별도의 깊이 추정에 요구되고, 그 모델들은 비교적 큰 저장 공간을 점유할 것이다. 하지만 본 개시의 멀티 태스크 네트워크의 경우, 깊이 추정의 성능을 보장한다는 전제 하에, 깊이 추정에 요구되는 컨볼루션 커널들의 수가 별도의 깊이 추정 네트워크보다 훨씬 더 적기 때문에, 우리의 멀티 태스크 네트워크는 더 작은 저장 공간을 점유한다. 실시간: 인간 파싱 인코딩 네트워크를 재사용함으로써, 깊이 추정의 프로세싱 효율은 크게 개선될 수 있다. 본 개시의 실시예가 모바일 폰 카메라의 미리보기 모드에 적용되면, 미리보기 모드의 실시간 수요는 보장된다.미리보기 모드에서, 카메라를 보유하는 손의 움직임은 랜덤 각도 오프셋을 도입할 것이다. 예를 들어, 도 12에 도시된 바와 같이, 도면 (a)는 손 움직임에 의해 야기되는 수평 및 수직 오프셋을 보여주고, 오프셋의 분포는 거의 대칭적이다. 도 (b)는 각도 오프셋의 히스토그램 통계를 보여주고, 오프셋의 대부분은 비교적 작다. 미리 보기 모드에서, 손의 움직임 반응은 촬영 디바이스 상에서 보여지는데, 이미지에서 더 가까운 물체는 먼 물체보 다 더 큰 위치 오프셋을 가질 것이다. 이들 움직임들에 의해 야기된 오프셋은 물체들 사이의 깊이 값에서의 차 이를 반영한다. 예를 들어, 촬영 디바이스의 깊이 값에 더 가까운 물체는 더 큰 디스패리티 값을 가지고, 동일 한 이미지 상에서, 촬영 디바이스의 깊이 값에서 먼 물체는 더 작은 디스패리티 값을 가지며, 도 13에 도시된 바와 같이, 그 도면의 상부 부분은 연속적인 k 개 이미지들을 보여주고, 그 도면의 하부 부분은 모든 인접한 두 개의 이미지들 간에 획득된 k번째 이미지의 디스패리티 맵 및 깊이 맵을 보여준다. 본 개시의 선택적인 실시예에서, 그 방법은 추가로, 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하는 단계; 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계를 더 포함할 수 있다. 그러면, 프로세싱될 이미지에 대응하는 깊이 이미지를 획득하기 위해 제1 위치 정보에 기초하여 프로세싱될 이 미지에 대한 깊이 추정을 수행하는 단계는, 프로세싱될 이미지에 대응하는 깊이 이미지를 획득하기 위해 제1 디스패리티 정보 및 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단계를 포함한다. 게다가, 연속적인 적어도 두 개의 이미지들에 기초하여, 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계는, 연속적인 적어도 두 개의 이미지들에서 인접한 두 개의 이미지들 사이의 제2 디스패리티 정보를 취득하는 단계; 및 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계를 포함한다. 게다가, 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계는, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평 균 디스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하는 단계를 포함한다. 구체적으로는, 다시 도 13을 참조하면, 연속적인 k 개 이미지들에 대해, 디스패리티 맵(다시 말하면, 제2 디스 패리티 정보)은 모든 인접한 두 개의 이미지들에 대해 계산될 수 있고, 깊이 맵은 지속적으로 누적된 또는 평균 화된 (k-1) 개의 절대 디스패리티 맵들(즉, 제1 디스패리티 정보)에 기초하여 계산된다. 예를 들어, 제2 프레임 과 제1 프레임 사이의 수평 방향에서의 디스패리티 값은 이며, 수직 방향에서의 디스패리티 값은 이며; 제3 프레임과 제2 프레임 사이의 수평 방향에서의 디스패리티 값은 이며, 수직 방향에 서의 디스패리티 값은 이며; 유사하게, k번째 프레임과 (k-1)번째 프레임 사이의 수평 방향에서의 디 스패리티 값은 이고, 수직 방향에서의 디스패리티 값은 이다. 구체적으로는, 깊이 추정을 위해 단일 이미지를 사용할 때, 이는 강건하지 않은 것으로 추정하기 쉽다. 도 14에 도시된 바와 같이, k번째 이미지의 깊이 맵이 따로따로 추정될 때, 먼 물체들 및 가까운 물체들의 깊이 값들은 함께 혼합되며, 따라서 깊이 값의 추정은 정확하지 않다. 멀티 프레임 디스패리티 정보 융합을 사용한 후, 먼 물체들 및 가까운 물체들의 깊이 값들은 더 잘 구별될 수 있다. 먼 물체들의 디스패리티 값이 가까운 물체들의 디스패리티 값보다 더 작기 때문에, 추가 정보는 미리 정의된 네트워크 모델이 깊이 값을 더 잘 추정하고 깊이 추정의 강건도(robustness)를 개선하는데 도움이 될 수 있다. 멀티 프레임들 기반 디스패리티 모듈(MFbD)은 프로세싱될 이미지의 제1 디스패리티 정보를 획득하기 위해 위의 프로세싱 프로세스에 따라 기설정될 수 있고, MFbD의 입력은 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들(즉, 연속적인 k 개 이미지들)이고, 출력은 프로세싱될 이미지에 대응하는 제1 디스패리티 정보라 는 것이 이해될 수 있다. 도 15는 미리보기 모드에서 k 개 이미지들이 입력되고, MFbD 컨볼루션 커널을 통해 사이즈 W*H(이는 W*H*2로서 기록될 수 있음)의 두 개의 디스패리티 행렬들을 출력하는 MFbD의 입력 및 출력의 개 략도이다. 두 개의 디스패리티 행렬들에서의 각각의 요소는 대응하는 화소의 수평 및 수직 디스패리티 정보이다. 본 개시의 선택적인 실시예에서, 깊이 추정 결과를 획득하기 위해 특징 맵, 제1 위치 정보 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제3 융합된 특징 맵을 획득하기 위해 제1 위치 정보, 제1 디스패리티 정보, 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하는 단계; 및 적어도 하나의 제3 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 적어도 하나의 제3 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플 링에 대응하는 특징 업샘플링을 수행하는 단계는, 해당하는 제4 융합된 특징 맵을 획득하기 위해 해당하는 특징 업샘플링에 대응하는 제3 융합된 특징 맵 및 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제4 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 도 16 및 도 17에 도시된 바와 같이, IPM 같이, MFbD는 자체적으로 프로세싱될 이미지에 대해 깊이 추정을 수행 할 수 있거나 또는 위에서 설명된 미리 정의된 네트워크 모델과 함께, 프로세싱될 이미지에 대해 깊이 추정을 수행할 수 있다는 것에 주의해야 한다. 결합된 구조에서, MFbD 및 기설정된 네트워크의 연결 방법은 IPM 및 기 설정된 네트워크의 연결 방법과 동일하고, 미리 정의된 네트워크 모델의 깊이 추정에서 MFbD에 의해 출력된 제1 디스패리티 정보의 사용(usage)은 제1 위치 정보의 사용과 동일하고, 여기서 반복되지 않을 것이다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 5, "content": "요약하면, 사용자들에 의해 피드백된 문제들에 따르면, 일부 스마트 디바이스들의 자동 초점 기능은, 특히 근거 리 촬영에 대해, 충분하지 않으며, 본 개시에 의해 제공되는 기술적 해법은 후속 모바일 폰들의 자동 초점 기능 을 제공하기 위해 (마이크로 깊이를 커버하는) 넓은 범위의 깊이 맵 추정을 제공할 수 있다. 본 개시에 의해 제 공되는 해법에서 미리 정의된 네트워크 모델이 상이한 카메라들에 적응될 수 있기 때문에, 후면 카메라에서 메 인 카메라, 망원 렌즈 등에 사용될 수 있고, 전면 카메라 시리즈 그룹에서 상이한 카메라들에 또한 적용될 수 있다. 컴퓨팅 자원들을 절약하기 위하여, 본 개시에서 제공되는 해법은 하나의 모델을 사용하여 자동 노출 및 자동 초점의 기능들을 완성할 수 있다. 본 개시의 실시예들의 기술적 해법들을 적용함으로써, 물체의 거리에 상 관없이, 정확한 초점은 정확한 깊이 추정을 통해 성취됨으로써, 물체의 선명한 이미지를 획득할 수 있다. 본 개시의 실시예는 딥 러닝에 기초한 멀티 태스크 광범위한 깊이 추정 방법을 제안한다. 광범위한 깊이 추정은 자동 초점, AI(Artificial Intelligence) 카메라, AR(Augmented Reality)/VR(Virtual Reality) 및 다른 분야들 에서 매우 넓은 응용 가능성들을 가지고, 다양한 스마트 폰들, 로봇들 및 AR/VR 디바이스들에서 사용될 수 있다. 본 개시의 실시예는 이미지 평면 좌표 변환을 컨볼루션 계층에 통합함으로써 광범위한 깊이들에 대한 지 원을 획득하고, 멀티 이미지들의 정보를 융합함으로써 장거리 및 단거리의 더 강건한 깊이 추정을 실현하고, 깊 이 추정 및 멀티태스킹 방법에서의 다른 태스크들을 동일한 네트워크에 융합하여 효율적이고 광범위한 깊이 추 정을 성취하며, 이는 미리보기 모드에서 실시간 수요를 충족시킬 수 있다. 다음에서는 위의 프로세스에서 사용되는 미리 정의된 네트워크 모델의 훈련 프로세스를 설명한다. 이 부분은 훈 련 데이터의 취득, 손실 기능의 설계, 및 등급 기준들의 설계를 포함하며, 이는 구체적으로는 다음과 같이 표현 된다. 1. 훈련 데이터의 취득: 다른 카메라들에 의해 찍힌 이미지들(이를테면 NYUDV2, KITTI, DIODE 및 다른 데이터 세트들에서의 이미지 들)에 기초하여, 다음의 수학식을 사용하여 타겟 카메라(다시 말하면, 기설정된 이미징 평면에 대응하는 카메라 이고, 그것의 카메라 파라미터는 위의 제1 카메라 파라미터임)의 훈련 데이터를 계산한다: 여기서, 은 입력 이미지 상의 각각의 화소의 좌표이며, 은 현재 이미지를 찍은 카메라의 고유 파라미터 행렬(카메라 초점 거리, 이미지의 주점 좌표들, 센서 사이즈, 다시 말하면, 위에서 언급된 제2 카메라 파라미터를 포함함)의 역행렬이며, 는 타겟 카메라의 고유 파라미터 행렬이다. z는 현재 이미지의 각각의 화소의 깊이 값이며, 는 설정된 병진(translation) 깊이이고, R은 설정된 회전 행렬이다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 6, "content": "를 변경하여 요구된 마이크로 깊이 데이터를 획득하고, R을 변경하여 훈련 데이터를 확장한다. 단계 에서 생성된 이미지에 많은 홀들이 있을 것이다. 쌍선형(bilinear) 차이 값들을 사용하여 홀들을 채운다. 위의 단계들에서 획득된 이미지를 크로핑하고, 저해상도 및 저품질 영역들을 제거하여 훈련 데이터를 획득 한다. 도 18에 도시된 바와 같이, 좌측 이미지는 NYUDV2 데이터 세트에서 취한 이미지이고, 우측 이미지는 이미지를 변환함으로써 획득된 타겟 이미지(즉, 훈련 데이터)이다. 2. 손실 함수의 설계: 허용가능 에러 비용: 더 중요한 깊이 범위들에 대해 더 많은 패널티들이 있으며, 특정 손실 함수는 다음의 수학 식에서 보여진다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 7, "content": "여기서, D(i)는 i번째 깊이 값 간격의 비용 함수이다. 비용 함수는 실제 값과 추정된 깊이 값 사이의 에러를 측 정하는 L1, L2 또는 다른 함수들일 수 있으며, 는 허용가능 에러의 값을 나타내고, 는 i번째 깊이 값의 실제 값의 간격을 나타낸다. 는 t번째 데이터 배치(batch)에서의 이 의 간격에서 허용가능 에러 의 평균값을 나타내고, , 는 하이퍼 파라미터를 나타내며, L은 모든 간격들의 이동 평균 에러를 나타낸 다. 도 19는 NYUDv2 및 KITTI 데이터세트들의 깊이 값들의 히스토그램 통계 결과들을 보여준다. 표 1은 상이한 깊이 범위들에 대한 허용 가능 에러 값들을 보여준다. 예를 들어, 레벨 1의 경우, 해당하는 깊이 값 범위 [0.07m, 0.08m)에 대한 허용가능 에러 범위는 0.005m이고, 깊이 값 범위 [1.2m, 1.6m)에 대한 허용가 능 에러 범위는 0.2m이다. [표 1]"}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 8, "content": "3. 등급 기준들의 설계: 추정된 깊이 값(d)과 실제 깊이 값 사이의 차이 가 AE 미만이면, 이는 조건을 충족시키는 것으로 간주 된다. 조건들을 충족시키는 화소들의 수는 로서 기록되고, 테스트 세트 상의 모든 화소들의 수는 로서 기록되며, 그러면 정의된 평가 기준의 허용가능 에러는 다음으로서 표현된다: . 도 20은 본 개시의 일 실시예에 따른 이미지 프로세싱 방법의 개략적인 흐름도이다. 도 20에 도시된 바와 같이, 그 방법은, 단계 S2001, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하는 단계로서, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어 도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 제1 특 징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈은 각각 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다 운샘플링을 수행하여 제1 특징 맵 및 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 결과를 출 력하는, 상기 적어도 한 번의 특징 다운샘플링을 수행하는 단계; 단계 S2002, 제1 디코딩 네트워크를 통해, 제1 프로세싱 결과를 획득하기 위해 인코딩 네트워크에 의해 출력된 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계; 및 단계 S2003, 제2 디코딩 네트워크를 통해, 제2 프로세싱 결과를 획득하기 위해 인코딩 네트워크에서 적어도 하 나의 제1 다운샘플링 모듈에 의해 출력된 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단 계를 포함할 수 있다. 본 개시에 의해 제공된 해법에서, 이미지 프로세싱 프로세스에서, 인코딩 모듈은 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈을 통해 프로세싱될 이미지에 대해 특징 다운샘플링을 여러 번 수행하고, 해당하는 디 코딩 네트워크들을 통해 두 개의 특징 다운샘플링 모듈들에 의해 출력된 특징 맵들을 디코딩하여 해당하는 프로 세싱 결과를 획득한다. 두 개의 프로세싱 결과들이 하나의 모델에서 획득되지만, 인코딩 모듈에서의 제2 다운샘 플링 모듈이 다운샘플링을 수행할 때, 제1 다운샘플링 모듈에 의해 추출된 특징들은 디코딩 후에 획득된 프로세 싱 결과를 더 정확하게 만들기 위해서 재사용될 수 있다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널은 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함하는데, 제2 컨볼루션 커널의 값은 0이며, 여기서, 제1 차원은 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커 널들의 수에 기초하여 결정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의 해 사용되는 컨볼루션 커널들의 수에 기초하여 결정된다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특 징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특 징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운 샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 본 개시의 선택적인 실시예에서, 제1 프로세싱 결과는 프로세싱될 이미지의 깊이 추정 결과이고, 제2 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결과이다. 본 개시의 실시예에서의 깊이 추정 태스크 및 다른 관련 태스크들은 인코딩 네트워크를 공유하고, 인코딩 네트 워크에서의 깊이 추정 특징 다운샘플링 모듈은 관련된 태스크의 특징 다운샘플링 모듈에 의해 추출된 특징들을 재사용할 수 있다는 것에 주의해야 한다. 위의 해법의 특정 구현 프로세스에 대해, 앞서의 설명을 참조하고, 여 기서 반복되지 않을 것이다. 도 21에 도시된 바와 같이, 깊이 추정 태스크와 인간 파싱 태스크는 인코딩 네트워 크를 공유하고, 인코딩 네트워크는 인간 파싱 인코딩 모듈과 깊이 추정 모듈을 포함한다. 도 22는 본 개시의 일 실시예에 의해 제공되는 깊이 추정 방법의 개략적인 흐름도이다. 도 22에 도시된 바와 같 이, 그 방법은, 다음을 포함할 수 있다: 단계 S2201: 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하는 단계; 단계 S2202: 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보 를 취득하는 단계; 및 단계 S2203: 제1 디스패리티 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하는 단계. 본 개시에 의해 제공되는 해법은 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 통해 프로 세싱될 이미지의 디스패리티 정보를 취득하고, 프로세싱될 이미지의 디스패리티 정보를 결합하여 깊이 추정을 수행하며, 따라서 깊이 추정에 대해 프로세싱될 이미지들을 수집할 때 도입되는 디스패리티의 영향은 제거되고, 깊이 추정의 정확도 및 강건도는 개선된다. 본 개시의 선택적인 실시예에서, 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하는 단계는, 연속적인 적어도 두 개의 이미지들에서 인접한 두 개의 이미지들 사이의 제2 디스패리티 정보를 취득하는 단계; 및 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하는 단계는, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평 균 디스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 제1 디스패리티 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행 하는 단계는, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다 운샘플링을 수행하는 단계; 및 제1 디코딩 네트워크를 통해, 깊이 추정 결과를 획득하기 위해 특징 맵 및 제1 디스패리티 정보에 기초하여 적 어도 한 번의 특징 업샘플링을 수행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 특징 맵 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하는 단계는, 적어도 하나의 제5 융합된 특징 맵을 획득하기 위해 제1 디스패리티 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하는 단계; 및 적어도 하나의 제5 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하는 단계를 포함한다. 본 개시의 선택적인 실시예에서, 적어도 하나의 제5 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플 링에 대응하는 특징 업샘플링을 수행하는 단계는, 해당하는 제6 융합된 특징 맵을 획득하기 위해 특징 업샘플링에 대응하는 제5 융합된 특징 맵 및 그것의 대응하 는 입력 특징 맵에 대해 특징 융합을 수행하는 단계; 및 제6 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하는 단계를 포함한다. 도 23에 도시된 바와 같이, MFbD는 프로세싱될 이미지에 대한 깊이 추정을 수행하기 위해 위에서 설명된 미리 정의된 네트워크 모델과 결합된다는 것에 주의해야 한다. 결합된 구조에서, 깊이 추정을 위해 미리 정의된 네트 워크 모델에서 사용되는 MFbD에 의해 출력된 제1 디스패리티 정보의 사용은 제1 위치 정보의 사용과 동일하고, 여기서 반복되지 않을 것이다. 도 24는 본 개시의 일 실시예에 의해 제공되는 깊이 추정 디바이스의 구조적인 블록도이다. 도 24에 도시된 바 와 같이, 디바이스는 위치 정보 취득 모듈과 깊이 이미지 취득 모듈을 포함할 수 있으며, 여 기서, 위치 정보 취득 모듈은 프로세싱될 이미지를 기설정된 평면에 매핑하고, 기설정된 평면 상에서 프로세싱 될 이미지에서의 화소들의 제1 위치 정보를 취득하도록 구성되며; 그리고 깊이 이미지 취득 모듈은 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하도록 구성된다.본 개시에 의해 제공되는 해법은 프로세싱될 이미지를 기설정된 평면에 매핑하여 기설정된 이미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 위치 정보를 획득하고, 깊이 추정 프로세스에서 기설정된 이미징 평면의 프 로세싱될 이미지에서의 각각의 화소의 위치 정보를 사용하여 깊이 추정 범위에 대한 카메라 파라미터들의 영향 을 제거하여서, 동일한 네트워크 모델은 상이한 카메라 파라미터들에 대응하는 프로세싱될 이미지의 깊이를 추 정할 수 있다. 광범위한 깊이 추정을 보장하면서, 그 해법은 컴퓨팅 자원들 및 저장 공간을 절약한다. 본 개시의 선택적인 실시예에서, 제1 위치 정보 취득 모듈은 구체적으로는, 현재 이미징 평면에 대응하는 제2 카메라 파라미터에 기초하여 현재 이미징 평면 상에서 프로세싱될 이미지에서 의 화소들의 제2 위치 정보를 취득하며; 그리고 제2 위치 정보, 기설정된 평면에 대응하는 제1 카메라 파라미터, 및 제2 카메라 파라미터에 기초하여 제1 위치 정보를 취득하도록 구성된다. 본 개시의 선택적인 실시예에서, 카메라 파라미터는 카메라의 초점 거리, 주점(principal point)의 위치, 및 센 서의 사이즈 중 적어도 하나를 포함한다. 본 개시의 선택적인 실시예에서, 제1 위치 정보 취득 모듈은 추가로, 제1 카메라 파라미터 및 제2 카메라 파라미터에 기초하여 제1 위치 정보와 제2 위치 정보 사이의 매핑 관계를 취득하며; 그리고 제2 위치 정보 및 매핑 관계에 기초하여 제1 위치 정보를 취득하도록 구성된다. 본 개시의 선택적인 실시예에서, 깊이 이미지 취득 모듈은 특징 다운샘플링 서브모듈과 특징 업샘플링 서브모듈 을 포함하며, 여기서: 특징 다운샘플링 서브모듈은 인코딩 네트워크를 통해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플 링을 수행하여 해당하는 특징 맵을 획득하도록 구성되며; 그리고 제1 특징 업샘플링 서브모듈은 제1 디코딩 네트워크를 통해 특징 맵 및 제1 위치 정보에 기초하여 적어도 한 번 의 특징 업샘플링을 수행하여 깊이 추정 결과를 획득하도록 구성된다. 본 개시의 선택적인 실시예에서, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 여기서, 제1 특징 다운샘플링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하여 제1 특 징 맵을 획득하고, 제2 특징 다운샘플링 모듈은 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링 을 수행하여 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 결과를 출력한다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널은 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함하는데, 제2 컨볼루션 커널의 값은 0이며, 제1 차원은 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수 에 기초하여 결정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의해 사용되 는 컨볼루션 커널들의 수에 기초하여 결정된다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널들은 표준 컨볼루션 또는 점 방식 컨볼루션이다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특 징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특 징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운 샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈에 의해 사용되는 컨볼 루션 커널들은 깊이 방식 컨볼루션의 컨볼루션 커널들이다. 본 개시의 선택적인 실시예에서, 디바이스는 제2 특징 업샘플링 서브모듈을 더 포함할 수 있으며, 이 서브모듈 은, 제2 디코딩 네트워크를 통해, 해당하는 프로세싱 결과를 획득하기 위해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력된 상기 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하도록 구성 된다. 본 개시의 선택적인 실시예에서, 제2 디코딩 네트워크에 의해 획득되는 해당하는 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결과이다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 구체적으로는, 적어도 하나의 제1 융합된 특징 맵을 획득하기 위해 제1 위치 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하며; 그리고 적어도 하나의 제1 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 추가로, 해당하는 제2 융합된 특징 맵을 획득하기 위해 특징 업샘플링에 대응하는 제1 융합된 특징 맵 및 그것의 입력 특징 맵에 대해 특징 융합을 수행하며; 그리고 제2 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하도록 구성된다. 본 개시의 선택적인 실시예에서, 디바이스는 디스패리티 정보 취득 모듈을 더 포함할 수 있으며, 디스패리티 정 보 취득 모듈은, 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하며; 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하도 록 구성되며, 대응하여, 깊이 추정 모듈은 추가로, 특징 맵, 제1 위치 정보, 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수행하여, 깊이 추정 결과를 획득하도록 구성된다. 본 개시의 선택적인 실시예에서, 디스패리티 정보 취득 모듈은 구체적으로는, 적어도 두 개의 연속 프레임들에서 인접한 두 개의 이미지들 사이의 제2 디스패리티 정보를 취득하며; 그리고 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하도록 구성된다. 본 개시의 선택적인 실시예에서, 디스패리티 정보 취득 모듈은 추가로, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평 균 디스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 추가로, 적어도 하나의 제3 융합된 특징 맵을 획득하기 위해 제1 위치 정보, 제1 디스패리티 정보, 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하며; 그리고 적어도 하나의 제3 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 추가로, 해당하는 제4 융합된 특징 맵을 획득하기 위해 특징 업샘플링에 대응하는 제3 융합된 특징 맵 및 그것의 대응하 는 입력 특징 맵에 대해 특징 융합을 수행하며; 그리고 제4 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하도록 구성된다. 도 25는 본 개시의 일 실시예에 의해 제공되는 이미지 프로세싱 디바이스의 구조적인 블록도이다. 도 25에 도시 된 바와 같이, 디바이스는 인코딩 모듈, 제1 디코딩 모듈, 및 제2 디코딩 모듈을 포함 할 수 있으며, 여기서: 인코딩 모듈은 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적 어도 한 번의 특징 다운샘플링을 수행하는 구성되는데, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함 하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하 며, 제1 특징 다운샘플링 모듈 및 제2 특징 다운샘플링 모듈은 각각 프로세싱될 입력 이미지 또는 특징 맵에 대 해 특징 다운샘플링을 수행하여 제1 특징 맵 및 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵의 융합된 결과를 출력한다. 제1 디코딩 모듈은 제1 디코딩 네트워크를 통해 인코딩 네트워크에 의해 출력된 특징 맵에 기초하여 적어 도 한 번의 특징 업샘플링을 수행하여 제1 프로세싱 결과를 획득하도록 구성되며; 그리고 제2 디코딩 모듈은 제2 디코딩 네트워크를 통해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈 에 의해 출력되는 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하여 제2 프로세싱 결과를 획득 하도록 구성된다. 본 개시에 의해 제공되는 해법은 프로세싱될 이미지를 기설정된 평면에 매핑하여 기설정된 이 미징 평면 상에서 프로세싱될 이미지의 각각의 화소의 위치 정보를 획득하고, 깊이 추정 프로세스에서 기설정된 이미징 평면의 프로세싱될 이미지에서의 각각의 화소의 위치 정보를 사용하여 깊이 추정 범위에 대한 카메라 파 라미터들의 영향을 제거하여서, 동일한 네트워크 모델은 상이한 카메라 파라미터들에 대응하는 프로세싱될 이미 지의 깊이를 추정할 수 있다. 광범위한 깊이 추정을 보장하면서, 그 해법은 컴퓨팅 자원들 및 저장 공간을 절약 한다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널은 제1 차원의 제1 컨볼루션 커널과 제2 차원의 제2 컨볼루션 커널을 포함하는데, 제2 컨볼루션 커널의 값은 0이며, 제1 차원은 이전의 특징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수 에 기초하여 결정되고, 제2 차원은 이전의 특징 다운샘플링 유닛에서 제2 특징 다운샘플링 모듈에 의해 사용되 는 컨볼루션 커널들의 수에 기초하여 결정된다. 본 개시의 선택적인 실시예에서, 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특 징 다운샘플링 유닛에서 제1 특징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하고, 제2 특 징 다운샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수는 이전의 특징 다운샘플링 유닛에서 제2 특징 다운 샘플링 모듈에 의해 사용되는 컨볼루션 커널들의 수와 동일하다. 본 개시의 선택적인 실시예에서, 제1 프로세싱 결과는 프로세싱될 이미지의 깊이 추정 결과이고, 제2 프로세싱 결과는 프로세싱될 이미지의 의미론적 파싱 결과이다. 도 26은 본 개시의 일 실시예에 의해 제공되는 깊이 추정 디바이스의 구조적인 블록도이다. 도 26에 도시된 바 와 같이, 디바이스는 연속 이미지 취득 모듈, 디스패리티 정보 취득 모듈, 및 깊이 추정 모 듈을 포함할 수 있으며, 여기서: 연속 이미지 취득 모듈은 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하도록 구성되며; 디스패리티 정보 취득 모듈은 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하 는 제1 디스패리티 정보를 취득하도록 구성되며; 그리고 깊이 추정 모듈은 제1 디스패리티 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하도록 구 성된다. 본 개시의 선택적인 실시예에서, 제1 디스패리티 정보 취득 모듈은 구체적으로는, 적어도 두 개의 연속 프레임들에서 인접한 두 개의 이미지들 사이의 제2 디스패리티 정보를 취득하며; 그리고 제2 디스패리티 정보에 기초하여 제1 디스패리티 정보를 취득하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 디스패리티 정보 취득 모듈은 추가로, 제2 디스패리티 정보에 기초하여 해당하는 평균 디스패리티 정보 또는 누적된 디스패리티 정보를 취득하고, 평 균 디스패리티 정보 또는 누적된 디스패리티 정보를 제1 디스패리티 정보로서 사용하도록 구성된다. 본 개시의 선택적인 실시예에서, 깊이 추정 모듈은, 특징 다운샘플링 서브모듈과 특징 업샘플링 서브모듈을 포 함하며, 여기서: 특징 다운샘플링 서브모듈은 인코딩 네트워크를 통해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플 링을 수행하여 해당하는 특징 맵을 획득하도록 구성되며; 그리고 특징 업샘플링 서브모듈은 제1 디코딩 네트워크를 통해 특징 맵 및 제1 디스패리티 정보에 기초하여 적어도 한 번의 특징 업샘플링을 수행하여 깊이 추정 결과를 획득하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 구체적으로는, 적어도 하나의 제5 융합된 특징 맵을 획득하기 위해 제1 디스패리티 정보 및 적어도 한 번의 특징 다운샘플링에 의해 출력된 특징 맵에 대해 특징 융합을 수행하며; 그리고 적어도 하나의 제5 융합된 특징 맵에 기초하여 적어도 한 번의 특징 다운샘플링에 대응하는 특징 업샘플링을 수 행하도록 구성된다. 본 개시의 선택적인 실시예에서, 제1 특징 업샘플링 서브모듈은 추가로, 해당하는 제6 융합된 특징 맵을 획득하기 위해 특징 업샘플링에 대응하는 제5 융합된 특징 맵 및 그것의 입력 특징 맵에 대해 특징 융합을 수행하며; 그리고 제6 융합된 특징 맵에 기초하여 특징 업샘플링을 수행하고, 획득된 특징 맵을 출력하도록 구성된다. 도 27을 참조하면, 이는 본 개시의 실시예들을 구현하기에 적합한 전자 디바이스(예를 들어, 도 4, 도 20, 또는 도 22에 도시된 방법을 실행하는 단말 디바이스 또는 서버)의 개략적인 구조도를 도시한다. 본 개시의 실시예들에서의 전자 디바이스들은, 모바일 단말들(이를테면 모바일 폰들, 노트북 컴퓨터들, 디지털 브로드캐스 트 수신기들, PDA들(personal digital assistants), PAD들(tablet computers), PMP들(portable multimedia players), 차량 탑재 단말들(예를 들어, 자동차 내비게이션 단말들) 및 고정 단말들(이를테면 디지털 TV들, 데 스크톱 컴퓨터들 등)을 포함할 수 있지만 그것들로 제한되지 않는다. 도 27에 도시된 전자 디바이스는 단지 일 예이고, 본 개시의 실시예들의 기능들 및 사용 범위에 어떠한 제한도 부과하지 않아야 한다. 전자 디바이스는 메모리와 프로세서를 포함하는데, 메모리는 전술한 방법 실시예들에서 설명되는 방법들을 실행 하기 위한 프로그램들을 저장하도록 구성되며; 프로세서는 메모리에 저장되는 프로그램들을 실행하도록 구성된 다. 여기서, 프로세서는 아래에서 설명되는 프로세싱 디바이스라고 지칭될 수 있고, 메모리는, 구체적으 로는 다음과 같이 보여지는, 판독전용 메모리(read-only memory)(ROM), 랜덤 액세스 메모리(random- access memory)(RAM), 및 저장 디바이스 중 적어도 하나를 포함할 수 있다: 도 27에 도시된 바와 같이, 전자 디바이스는 프로세싱 디바이스(이를테면 중앙 프로세싱 유닛(central processing unit), 그래픽스 프로세서 등)를 포함할 수 있으며, 프로세싱 디바이스는 판독전용 메모리 (ROM)에 저장되는 프로그램들 또는 저장 디바이스로부터 랜덤 액세스 메모리(RAM) 안으로 로 딩되는 프로그램들에 따라 적절한 다양한 액션들 및 프로세싱들을 실행할 수 있다. RAM에서, 전자 디바이 스의 동작을 위해 요구되는 다양한 프로그램들 및 데이터가 또한 저장된다. 프로세싱 디바이스, ROM, 및 RAM은 버스를 통해 서로 연결된다. 입출력(I/O) 인터페이스가 버스에 또한 연결된다. 일반적으로, 다음의 디바이스들이 I/O 인터페이스에 연결될 수 있다: 입력 디바이스들 이를테면 터 치 스크린, 터치 패드, 키보드, 마우스, 카메라, 마이크로폰, 가속도계, 자이로스코프 등; 출력 디바이스 이를테면 액정 디스플레이(liquid crystal display)(LCD), 스피커, 진동; 저장 디바이스 이를테면 자기 테이프, 하드 디스크 등; 및 통신 디바이스. 통신 디바이스는 전자 디바이스가 다른 디바이 스들과 무선 또는 유선 통신을 수행하여 데이터를 교환하는 것을 허용할 수 있다. 도 27이 다양한 디바이스들을 갖는 전자 디바이스를 도시하지만, 모든 예시된 디바이스들을 구현하거나 또는 갖는 것이 필요하지 않다는 것이 이해되어야 한다. 이는 대안적으로는 더 많거나 더 적은 디바이스들로 구현되거나 제공될 수 있다. 특히, 본 개시의 실시예들에 따르면, 흐름도를 참조하여 위에서 설명된 프로세스는 컴퓨터 소프트웨어 프로그램 들로서 구현될 수 있다. 예를 들어, 본 개시의 실시예들은 컴퓨터 프로그램 제품을 포함하며, 그 제품은 비일시 적 컴퓨터 판독가능 매체 상에서 운반되는 컴퓨터 프로그램들을 포함하고, 컴퓨터 프로그램들은 흐름도에서 도 시된 방법을 실행하기 위한 프로그램 코드들을 포함한다. 이러한 실시예에서, 컴퓨터 프로그램들은 통신 디바이 스를 통해 네트워크로부터 다운로드되고 설치되거나, 또는 저장 디바이스로부터 설치되거나, 또는 ROM으로부터 설치될 수 있다. 컴퓨터 프로그램들이 프로세싱 디바이스에 의해 실행될 때, 이는 본 개시의 실시예의 방법에서 정의되는 위의 기능들을 실행한다.본 개시에서의 전술한 컴퓨터 판독가능 매체는 컴퓨터 판독가능 신호 매체 또는 컴퓨터 판독가능 저장 매체, 또 는 두 개의 임의의 조합일 수 있다는 것에 주의해야 한다. 컴퓨터 판독가능 저장 매체는, 예를 들어, 비제한적 으로, 전기, 자기, 광학적, 전자기, 적외선, 또는 반도체 시스템, 장치, 또는 디바이스, 또는 상기한 바의 임의 의 조합일 수 있다. 컴퓨터 판독가능 저장 매체의 더 많은 특정 예들은, 하나 이상의 와이어들을 갖는 전기 커 넥션들, 휴대용 컴퓨터 디스크들, 하드 디스크들, 랜덤 액세스 메모리(RAM), 판독 전용 메모리(ROM), 소거가능 프로그램가능 판독 전용 메모리(erasable Programmable read only memory)(EPROM 또는 플래시 메모리), 광섬유, 휴대용 콤팩트 디스크 판독 전용 메모리(compact disk read only memory)(CD-ROM), 광학적 저장 디바 이스, 자기 저장 디바이스, 또는 위에서 언급된 바의 임의의 적합한 조합을 비제한적으로 포함할 수 있다. 본 개시에서, 컴퓨터 판독가능 저장 매체는 프로그램을 포함 또는 저장하는 임의의 유형의 매체일 수 있고, 그 프 로그램은 명령 실행 시스템, 장치, 또는 디바이스에 의해 또는 그것과 조합하여 사용될 수 있다. 본 개시에서, 컴퓨터 판독가능 신호 매체는 기저대역에서 전파되는 또는 반송파의 일부로서의 데이터 신호를 포함할 수 있고, 컴퓨터 판독가능 프로그램 코드들은 그 안에서 포함된다. 이 전파되는 데이터 신호는 전자기 신호들, 광학적 신 호들 또는 전술한 바의 임의의 적합한 조합을 비제한적으로 포함하는 많은 형태를 취할 수 있다. 컴퓨터 판독가 능 신호 매체는 또한 컴퓨터 판독가능 저장 매체와는 다른 임의의 컴퓨터 판독가능 매체일 수 있다. 컴퓨터 판 독가능 신호 매체는 명령 실행 시스템, 장치, 또는 디바이스에 의한 또는 그러한 것과 조합한 사용을 위해 프로 그램을 전송, 전파, 또는 전달할 수 있다. 컴퓨터 판독가능 매체 상에 포함되는 프로그램 코드는 와이어, 광학 적 케이블, RF(radio frequency) 등, 또는 상기한 바의 임의의 적합한 조합을 비제한적으로 포함하는 임의의 적 합한 매체에 의해 송신될 수 있다. 일부 실시예들에서, 클라이언트와 서버는 HTTP(HyperText Transfer Protocol)와 같은 임의의 현재 알려진 또는 장차 개발될 네트워크 프로토콜과 통신할 수 있고, 디지털 데이터 통신의 임의의 형태 또는 매체(예를 들어, 통 신 네트워크)와 상호연결될 수 있다. 통신 네트워크들의 예들은 로컬 영역 네트워크들(local area networks)(\"LAN\"), 광역 네트워크들(wide area networks)(\"WAN\"), 인터넷(예를 들어, 인터넷)과, 단 대 단 네 트워크들(예를 들어, 애드 혹 단 대 단 네트워크들), 뿐만 아니라 임의의 현재 알려진 또는 장차 개발될 네트워 크를 포함한다. 위의 컴퓨터 판독가능 매체들은 위의 전자 디바이스에 포함될 수 있거나; 또는 전자 디바이스 안으로 어셈블되 는 일 없이 단독으로 존재할 수 있다. 위의 컴퓨터 판독가능 매체는 하나 이상의 프로그램들을 운반하고, 위의 하나 이상의 프로그램들이 전자 디바이 스에 의해 실행될 때, 전자 디바이스로 하여금, 기설정된 평면에 프로세싱될 이미지를 매핑하고, 기설정된 평면 상에서 프로세싱될 이미지에서의 화소들의 제1 위치 정보를 취득하게 하며; 그리고 제1 위치 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 하게 한 다. 또는, 인코딩 네트워크를 통해, 해당하는 특징 맵을 획득하기 위해 프로세싱될 이미지에 대해 적어도 한 번의 특징 다운샘플링을 수행하게 하며, 인코딩 네트워크는 여러 특징 다운샘플링 유닛들을 포함하고, 적어도 하나의 특징 다운샘플링 유닛은 제1 특징 다운샘플링 모듈과 제2 특징 다운샘플링 모듈을 포함하며, 제1 특징 다운샘플 링 모듈 및 제2 특징 다운샘플링 모듈은 각각 프로세싱될 입력 이미지 또는 특징 맵에 대해 특징 다운샘플링을 수행하여 제1 특징 맵 및 제2 특징 맵을 획득하고, 제1 특징 맵 및 제2 특징 맵을 융합하고 출력하며; 제1 디코 딩 네트워크를 통해, 제1 프로세싱 결과를 획득하기 위해 인코딩 네트워크에 의해 출력된 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하게 하며; 그리고 제2 디코딩 네트워크를 통해, 제2 프로세싱 결과를 획 득하기 위해 인코딩 네트워크에서 적어도 하나의 제1 다운샘플링 모듈에 의해 출력된 제1 특징 맵에 기초하여 적어도 한 번의 특징 업샘플링을 수행하게 한다. 또는, 프로세싱될 이미지를 포함하는 연속적인 적어도 두 개의 이미지들을 취득하게 하며; 연속적인 적어도 두 개의 이미지들에 기초하여 프로세싱될 이미지에 대응하는 제1 디스패리티 정보를 취득하게 하며; 그리고 제1 디 스패리티 정보에 기초하여 프로세싱될 이미지에 대한 깊이 추정을 수행하게 한다. 본 개시의 동작들을 수행하기 위한 컴퓨터 프로그램 코드는 하나 이상의 프로그래밍 언어들 또는 그것들의 조합 으로 기입될 수 있다. 위에서 언급된 프로그래밍 언어들은 객체 지향 프로그래밍 언어들 ― 이를테면 Java, Smalltalk, C++ ― 을 포함하고, 또한 기존의 절차적인 프로그래밍 언어 ― 이를테면 \"C\" 언어 또는 유사한 프 로그래밍 언어 ― 를 포함한다. 프로그램 코드는 사용자의 컴퓨터 상에서 전적으로, 사용자의 컴퓨터 상에서 부 분적으로 실행될 수 있거나, 독립적인 소프트웨어 패키지로서 실행될 수 있거나, 부분적으로는 사용자의 컴퓨터상에서 그리고 부분적으로는 원격 컴퓨터 상에서 실행될 수 있거나, 또는 전적으로 원격 컴퓨터 또는 서버 상에 서 실행될 수 있다. 원격 컴퓨터의 경우, 원격 컴퓨터는 로컬 영역 네트워크(LAN) 또는 광역 네트워크(WAN)를 포함하는 임의의 종류의 네트워크를 통해 사용자의 컴퓨터에 연결될 수 있거나, 또는 (예를 들어, 인터넷 서비 스 제공자를 사용하여 인터넷 연결을 통과하여) 외부 컴퓨터에 연결될 수 있다. 첨부 도면들에서의 흐름도들 및 블록도들은 본 개시의 다양한 실시예들에 따른 시스템, 방법, 및 컴퓨터 프로그 램 제품의 가능한 구현예 아키텍처, 기능들, 및 동작들을 예시한다. 이와 관련하여, 흐름도 또는 블록도에서의 각각의 블록은 모듈, 프로그램 세그먼트, 또는 코드의 일부를 나타낼 수 있고, 모듈, 프로그램 세그먼트, 또는 코드의 일부는 특정된 논리적 기능을 실현하기 위한 하나 이상의 실행가능 명령들을 포함한다. 일부 대안적 구 현예들에서, 블록들에서 표시된 기능들은 그 도면들에서 표시된 순서와는 다른 순서로 또한 일어날 수 있다는 것에 또한 주의해야 한다. 예를 들어, 연속하여 도시되는 두 개의 블록들은 실제로는 실질적으로 병행하여 실행 될 수 있거나, 또는 수반되는 기능들에 의존하여, 때때로 역순으로 실행될 수 있다. 블록도 및/또는 흐름도에서 의 각각의 블록, 블록도 및/또는 흐름도에서의 블록들의 조합은, 특정된 기능 또는 동작을 수행하는 전용 하드 웨어 기반 시스템에 의해 구현될 수 있거나, 또는 전용 하드웨어 및 컴퓨터 명령들의 조합에 의해 실현될 수 있 다는 것에 또한 주의해야 한다. 본 개시에서 설명되는 실시예들에 수반되는 모듈들 또는 유닛들은 소프트웨어 또는 하드웨어로 구현될 수 있다. 여기서, 모듈 또는 유닛의 이름은 특정한 상황에서 유닛 자체에 대한 제한을 구성하지 않는다. 예를 들어, 제1 위치 정보 취득 모듈은 \"제1 위치 정보를 취득하기 위한 모듈\"로서 또한 설명될 수 있다. 본 개시에서의 위의 기능들은 적어도 부분적으로는 하나 이상의 하드웨어 로직 컴포넌트들에 의해 수행될 수 있 다. 예를 들어, 제한 없이, 사용될 수 있는 예시적인 유형들의 하드웨어 로직 컴포넌트들은, FPGA(Field Programmable Gate Array), ASIC(Application Specific Integrated Circuit), ASSP(Application Specific Standard Product), SOC(System on Chip), CPLD(Complex Programmable Logical device) 등을 포함한다. 본 개시의 맥락에서, 머신 판독가능 매체는 유형의 매체일 수 있으며, 이는 명령 실행 시스템, 장치, 또는 디바 이스에 의한 또는 명령 실행 시스템, 장치, 또는 디바이스와 조합한 사용을 위해 프로그램을 포함 또는 저장할 수 있다. 머신 판독가능 매체는 머신 판독가능 신호 매체 또는 머신 판독가능 저장 매체일 수 있다. 머신 판독 가능 매체는 전자, 자기, 광학적, 전자기, 적외선, 또는 반도체 시스템, 장치 또는 디바이스, 또는 전술한 바의 임의의 적합한 조합을 비제한적으로 포함할 수 있다. 머신 판독가능 저장 매체의 더 많은 특정 예들은 하나 이 상의 와이어들에 기초한 전기 커넥션들, 휴대용 컴퓨터 디스크들, 하드 드라이브들, 랜덤 액세스 메모리(RAM), 판독전용 메모리(ROM), 소거가능 프로그램가능 판독전용 메모리(EPROM 또는 플래시 메모리), 광섬유, 휴대용 콤 팩트 디스크 판독 전용 메모리(CD-ROM), 광학적 저장 디바이스, 자기 저장 디바이스, 또는 상기한 바의 임의의 적합한 조합을 포함할 것이다. 본 개시의 실시예에서 제공되는 디바이스는 AI 모델을 통해 다수의 모듈들 중 적어도 하나를 구현할 수 있다. AI에 연관된 기능들은 비휘발성 메모리, 휘발성 메모리, 및 프로세서에 의해 수행될 수 있다. 프로세서는 하나 이상의 프로세서들을 포함할 수 있다. 이때, 하나 이상의 프로세서들은 일반 목적 프로세서들, 이를테면 중앙 프로세싱 유닛(CPU), 애플리케이션 프로세서(application processor)(AP) 등, 또는 순수 그래픽 프로세싱 유닛(graphics processing unit), 이를테면 그래픽 프로세싱 유닛(GPU), 시각적 프로세싱 유닛 (visual processing unit)(VPU), 및/또는 AI 전용 프로세서, 이를테면 신경 프로세싱 유닛(neural processing unit)(NPU)일 수 있다. 하나 이상의 프로세서들은 비휘발성 메모리 및 휘발성 메모리에 저장되는 미리 정의된 동작 규칙들 또는 인공지 능(AI) 모델들에 따라 입력 데이터의 프로세싱을 제어한다. 미리 정의된 동작 규칙들 또는 인공지능 모델들은 훈련 또는 학습을 통해 제공된다. 여기서, 학습에 의해 제공되는 것은 학습 알고리즘들을 다수의 학습 데이터에 적용함으로써 원하는 특성들을 갖 는 미리 정의된 동작 규칙들 또는 AI 모델들을 획득하는 것을 의미한다. 이 학습은 실시예에 따른 AI가 실행되 는 디바이스 자체에서 수행될 수 있으며, 그리고/또는 별도의 서버/시스템에 의해 실현될 수 있다. AI 모델이 다수의 신경망 계층들을 포함할 수 있다. 각각의 계층은 다수의 가중값 값들을 가지고, 하나의 계층 의 계산은 이전의 계층의 계산 결과와 현재 계층의 다수의 가중값들에 의해 수행된다. 신경망들의 예들은 CNN(Convolutional Neural Networks), DNN(Deep Neural Networks), RNN(Recurrent Neural Networks), RBM(Restricted Boltzmann Machines), DBN(Deep Belief Networks), BRDNN(Bidirectional Loops Deep NeuralNetwork), GAN(Generative Adversarial Network), 및 딥 Q 네트워크를 비제한적으로 포함한다. 학습 알고리즘은 타겟 디바이스가 결정 또는 예측을 하게 하거나, 결정 또는 예측을 하도록 허용하거나, 또는 결정 또는 예측을 하도록 제어하기 위해 복수의 학습 데이터를 사용하여 미리 결정된 타겟 디바이스(예를 들어, 로봇)를 훈련하는 방법이다. 학습 알고리즘들의 예들은 지도 학습(supervised learning), 자율 학습 (unsupervised learning), 반지도(semi-supervised) 학습, 또는 강화 학습을 포함하지만 그것들로 제한되지 않 는다."}
{"patent_id": "10-2021-0153377", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 9, "content": "본 기술분야의 통상의 기술자들은 설명의 편의 및 간결을 위해, 위에서 설명된 컴퓨터 판독가능 매체가 전자 디 바이스에 의해 실행될 때 구현되는 특정 방법이 전술한 방법 실시예에서의 대응하는 프로세스를 참조할 수 있으 며, 이는 여기서 반복되지 않을 것이다라는 것을 명확히 이해할 것이다."}
{"patent_id": "10-2021-0153377", "section": "도면", "subsection": "도면설명", "item": 1, "content": "본 개시의 실시예들의 기술적 해법들을 더 명확하게 설명하기 위하여, 다음에서는 본 개시의 실시예들의 설명에 서 사용될 필요가 있는 도면들을 간단히 소개할 것이다. 도 1은 스마트 디바이스에 의해 수행되는 오토 포커싱 프로세스의 개략도이다; 도 2는 상이한 이미징 평면들 상에서 물체들의 대응하는 사이즈들의 비교의 개략도이다; 도 3은 종래 기술에서 깊이 추정 네트워크 모델의 네트워크 구조의 개략도이다; 도 4는 본 개시의 일 실시예에 의해 제공되는 깊이 추정 방법의 개략적인 흐름도이디;도 5는 본 개시의 일 실시예에서 현재 이미징 평면으로부터 기설정된 이미징 평면으로의 좌표 변환의 개략도이 다; 도 6은 본 개시의 일 실시예의 일 예에서 제1 위치 정보를 취득하는 IPM(image mapping module)의 개략도이다; 도 7은 본 개시의 일 실시예의 일 예에서 깊이 추정을 위한 미리 정의된 네트워크 모델 및 IPM의 조합의 개략도 이다; 도 8은 종래 기술에서 자동 초점 기능 및 자동 노출 기능의 여러 인덱스들의 비교의 개략도이다; 도 9a는 본 개시의 일 실시예의 일 예에서 제공되는 인간 파싱을 위한 인간 파싱 네트워크 모델의 개략도이다; 도 9b는 도 9a에 도시된 인간 파싱 네트워크 모델의 네트워크 구조의 상세한 개략도이다; 도 10a는 본 개시의 일 실시예의 일 예에서 제공되는 인간 파싱 네트워크의 확장에 의해 획득된 미리 정의된 네 트워크 모델에 기초하여 깊이 추정을 수행하는 개략도이다; 도 10b는 도 10a에 도시된 미리 정의된 네트워크 모델의 네트워크 구조의 상세한 개략도이다; 도 11a는 본 개시의 일 실시예의 일 예에서 인간 파싱 인코딩 모듈에 대응하는 컨볼루션 커널에서의 제로 패딩 의 개략도이다; 도 11b는 본 개시의 일 실시예의 일 예에서 깊이 추정 인코딩 모듈에 대응하는 컨볼루션 커널에서의 제로 패딩 의 개략도이다; 도 12는 본 개시의 실시예의 일 예에서 촬상 디바이스를 보유하는 손의 움직임에 의해 도입되는 각도 오프셋 및 각도 오프셋의 히스토그램 통계의 개략도이다; 도 13은 본 개시의 일 실시예의 일 예에서 연속적인 k 개 이미지들에서의 인접한 두 개의 이미지들의 디스패리 티 도이다; 도 14는 본 개시의 일 실시예의 일 예에서 단일 이미지에 따라 그리고 연속적인 k 개 이미지들에 따라 각각 획 득된 깊이 이미지들의 비교의 개략도이다; 도 15는 본 개시의 일 실시예의 일 예에서 제1 디스패리티 정보를 취득하는 MFbD(multi-frames based disparity module)의 개략도이다; 도 16은 본 개시의 일 실시예의 일 예에서 깊이 추정을 위한 미리 정의된 네트워크 모델, IPM 및 MFbD의 조합의 개략도이다; 도 17은 도 16에 도시된 결합된 네트워크 구조의 상세한 개략도이다; 도 18은 본 개시의 일 실시예의 일 예에서 NYUDv2 데이터 세트에서의 하나의 이미지로부터 타겟 카메라의 이미 지를 생성하는 개략도이다; 도 19는 NYUDv2 및 KITTI 데이터 세트들의 깊이 값 히스토그램 통계이다; 도 20은 본 개시의 일 실시예에 의해 제공되는 이미지 프로세싱 방법의 개략적인 흐름도이다; 도 21은 본 개시의 일 실시예의 일 예에서 제공되는 인간 파싱 네트워크의 확장에 의해 획득된 다른 미리 정의 된 네트워크 모델에 기초하여 깊이 추정을 수행하는 개략도이다; 도 22는 본 개시의 일 실시예에 의해 제공되는 다른 깊이 추정 방법의 개략적인 흐름도이다; 도 23은 본 개시의 일 실시예의 일 예에서 제공되는 인간 파싱 네트워크의 확장에 의해 획득된 다른 미리 정의 된 네트워크 모델에 기초하여 깊이 추정을 수행하는 개략도이다; 도 24는 본 개시의 일 실시예에 의해 제공되는 깊이 추정 디바이스의 구조적인 블록도이다; 도 25는 본 개시의 일 실시예에 의해 제공되는 이미지 프로세싱 디바이스의 구조적인 블록도이다; 도 26은 본 개시의 일 실시예에 의해 제공되는 다른 깊이 추정 디바이스의 구조적인 블록도이다; 그리고 도 27은 본 개시의 일 실시예에 의해 제공되는 전자 디바이스의 개략도이다."}
