{"patent_id": "10-2024-0094631", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0032912", "출원번호": "10-2024-0094631", "발명의 명칭": "인공지능 모델을 학습하는 서버 장치와 전자 장치 및 그 방법", "출원인": "주식회사 크립토랩", "발명자": "이가람"}}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "전자 장치에 있어서,통신부;원본 데이터 및 생성형 AI 모델이 저장된 메모리; 및프로세서를 포함하며,상기 프로세서는,상기 원본 데이터를 상기 생성형 AI 모델에 입력하여, 상기 원본 데이터에 대응되는 가상의 합성 데이터를 획득하고, 상기 합성 데이터를 상기 통신부를 통해 서버 장치로 전송하며, 상기 원본 데이터를 동형 암호화하여 동형 암호문을 획득하고, 상기 동형 암호문을 상기 통신부를 통해 상기 서버 장치로 전송하는, 전자 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서,상기 프로세서는,상기 합성 데이터를 상기 통신부를 통해 상기 외부 장치로 전송한 후, 상기 외부 장치로부터 인공지능 모델의하이퍼파라미터 튜닝에 대한 완료 신호가 상기 통신부를 통해 수신되면, 상기 동형 암호문을 상기 통신부를 통해 상기 서버 장치로 전송하는, 전자 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서,상기 생성형 AI 모델은 데이터 타입 별로 마련된 복수의 모델을 포함하고,상기 프로세서는,상기 원본 데이터의 데이터 타입을 식별하고, 상기 복수의 모델 중에서 상기 데이터 타입에 대응되는 모델로 상기 원본 데이터를 입력하여 상기 원본 데이터와 동일한 데이터 타입을 가지는 상기 합성 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제1항에 있어서,상기 생성형 AI 모델은 데이터 타입 별로 마련된 복수의 모델을 포함하고,상기 프로세서는,상기 원본 데이터의 데이터 타입이 선택되면, 상기 복수의 모델 중에서 상기 선택된 데이터 타입에 대응되는 모델로 상기 원본 데이터를 입력하여 상기 합성 데이터를 획득하는, 전자 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제3항 또는 제4항에 있어서,상기 복수의 모델은, 정형 데이터(Tabular data)에 대응되는 제1 모델, 이미지에 대응되는 제2 모델, 텍스트에대응되는 제3 모델을 포함하며,상기 제1 모델은 Tabular GAN(TABGAN) 모델, Variational Autoencoders(VAEs) 모델, Copular-based methods 모공개특허 10-2025-0032912-3-델 중 적어도 하나를 포함하고,상기 제2 모델은 DCGAN(Deep Convolutional GAN) 모델, StyleGAN 모델, ProGAN 모델 중 적어도 하나를 포함하며,상기 제3 모델은 Generative Pretrained Transformer 모델을 포함하는, 전자 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "서버 장치에 있어서,통신부;평문용의 제1 인공지능 모델 및 암호문 용의 제2 인공지능 모델을 저장하는 메모리; 및프로세서를 포함하며,상기 프로세서는, 전자 장치에서 원본 데이터를 기초로 합성한 합성 데이터가 상기 통신부를 통해 수신되면, 상기 합성 데이터를이용하여 상기 제1 인공지능 모델의 하이퍼파라미터를 평문 상에서 튜닝하고, 상기 제2 인공지능 모델의 적어도하나의 하이퍼파라미터를 상기 제1 인공지능 모델의 하이퍼파라미터에 따라 튜닝하며, 상기 원본 데이터에 대한 동형암호문이 상기 통신부를 통해 수신되면, 상기 튜닝된 적어도 하나의 하이퍼파라미터를 가지는 상기 제2 인공지능 모델을 상기 동형암호문을 이용하여 학습하는, 서버 장치."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "인공지능모델을 학습하는 전자 장치의 방법에 있어서,원본 데이터를 생성형 AI 모델에 입력하여, 상기 원본 데이터에 대응되는 가상의 합성 데이터를 획득하는 단계;상기 서버 장치에서 인공지능 모델의 하이퍼파라미터 튜닝에 사용할 수 있도록, 상기 합성 데이터를 서버 장치로 전송하는 단계; 상기 원본 데이터를 동형 암호화하여 동형 암호문을 획득하여, 상기 동형 암호문을 상기 서버 장치로 전송하는단계;를 포함하는 방법."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제7항에 있어서,상기 동형 암호문을 상기 서버 장치로 전송하는 단계는,상기 합성 데이터를 상기 외부 장치로 전송한 후, 상기 외부 장치로부터 인공지능 모델의 하이퍼파라미터 튜닝에 대한 완료 신호가 수신되면, 상기 동형 암호문을 상기 서버 장치로 전송하는, 방법."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "제7항에 있어서,상기 가상의 합성 데이터를 획득하는 단계는, 상기 원본 데이터의 데이터 타입을 식별하는 단계;데이터 타입 별로 기 마련된 복수의 모델 중에서 상기 식별된 데이터 타입에 대응되는 모델로 상기 원본 데이터를 입력하여 상기 원본 데이터와 동일한 데이터 타입을 가지는 상기 합성 데이터를 획득하는 단계;를 포함하는,방법."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제7항에 있어서,상기 원본 데이터의 데이터 타입을 선택받기 위한 UI 화면을 디스플레이하는 단계를 더 포함하며, 공개특허 10-2025-0032912-4-상기 가상의 합성 데이터를 획득하는 단계는, 상기 UI 화면을 통해서 상기 원본 데이터의 데이터 타입이 선택되면, 데이터 타입 별로 기 마련된 복수의 모델중에서 상기 선택된 데이터 타입에 대응되는 모델로 상기 원본 데이터를 입력하여 상기 합성 데이터를획득하는, 방법."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항 또는 제10항에 있어서,상기 복수의 모델은, 정형 데이터(Tabular data)에 대응되는 제1 모델, 이미지에 대응되는 제2 모델, 텍스트에대응되는 제3 모델을 포함하며,상기 제1 모델은 Tabular GAN(TABGAN) 모델, Variational Autoencoders(VAEs) 모델, Copular-based methods 모델 중 적어도 하나를 포함하고,상기 제2 모델은 DCGAN(Deep Convolutional GAN) 모델, StyleGAN 모델, ProGAN 모델 중 적어도 하나를 포함하며,상기 제3 모델은 Generative Pretrained Transformer 모델을 포함하는, 방법."}
{"patent_id": "10-2024-0094631", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "인공지능모델을 학습하는 서버 장치의 방법에 있어서,전자 장치에서 원본 데이터를 기초로 합성한 합성 데이터가 수신되면, 평문용의 제1 인공지능 모델의 하이퍼파라미터를 상기 합성 데이터를 이용하여 평문 상에서 튜닝하는 단계;암호문 용의 제2 인공지능 모델의 적어도 하나의 하이퍼파라미터를 상기 제1 인공지능 모델의 하이퍼파라미터에따라 튜닝하는 단계;상기 원본 데이터에 대한 동형암호문이 수신되면, 상기 튜닝된 적어도 하나의 하이퍼파라미터를 가지는 상기 제2 인공지능 모델을 상기 동형암호문을 이용하여 학습하는 단계;를 포함하는, 방법."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "전자 장치가 개시된다. 본 장치는, 통신부, 원본 데이터 및 생성형 AI 모델이 저장된 메모리 및 프로세서를 포함 한다. 여기서, 프로세서는, 원본 데이터를 생성형 AI 모델에 입력하여, 원본 데이터에 대응되는 가상의 합성 데 이터를 획득하고, 합성 데이터를 상기 통신부를 통해 서버 장치로 전송하며, 원본 데이터를 동형 암호화하여 동 형 암호문을 획득하고, 동형 암호문을 통신부를 통해 서버 장치로 전송한다. 이에 따라, 원본 유출을 방지하면서, 인공지능 모델을 효과적으로 학습시킬 수 있다."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 인공지능 모델을 학습하는 장치 및 그 방법에 대한 것이다."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "인공지능 기술의 발달에 힘입어 다양한 분야에서 인공지능 모델이 사용되고 있다. 인공지능 모델은 데이터로부 터 패턴을 학습하여, 학습된 내용을 토대로 자체적으로 규칙을 찾아서, 결론 도출에 활용하는 인공지능 모델이 다. 사용자가 인공지능 모델을 이용하기 위해서는 데이터 학습이 필수적이다. 학습에 따른 성능을 최적화하기 위해 서는 정교한 하이퍼파라미터 튜닝(tuning)이 필수적이다. 하이퍼파라미터 튜닝은 정해져 있는 규칙이 있는 것은 아니며, 데이터의 성격 및 사용하는 모델에 따라 전문가의 경험을 바탕으로 수행한다. 한편, 인공지능 모델의 학습을 위하여 제공하는 데이터는 인공지능 모델을 보유하고 있는 보유자 또는 운영자에 게 전달되는 과정 또는 학습 과정에서 외부로 노출될 가능성이 있다. 따라서, 보안을 요하는 비밀 데이터인 경 우, 그 데이터 자체로 학습을 수행하기 어렵다는 문제점이 있었다. 이러한 문제를 해결하기 위하여, 암호화 상태에서도 연산이 가능한 동형암호문으로 암호화하여 제공하는 기술 을 고려할 수 있으나, 이 경우, 데이터가 전부 암호화되어 있기 때문에 복호화 키 없이는 학습된 모델의 성능을 서버에서 직접 확인할 수 없는 바, 적절한 하이퍼파라미터 튜닝이 불가능하게 된다. 따라서, 동형암호 기반에서는, 인공지능 모델이 최적화된 성능을 가지도록 학습하는 것이 어렵다는 문제점이 있"}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 2, "content": "었다.발명의 내용"}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 상술한 문제점을 해결하기 위한 것으로, 본 발명의 목적은 데이터 보안을 유지하면서 동형 암호 기반 에서 인공지능 모델을 효과적으로 학습하는 전자 장치와 서버 장치 및 그 방법들을 제공함에 있다."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "본 개시의 적어도 하나의 실시 예에 따르면, 전자 장치는, 통신부, 원본 데이터 및 생성형 AI 모델이 저장된 메 모리 및 프로세서를 포함하며, 상기 프로세서는, 상기 원본 데이터를 상기 생성형 AI 모델에 입력하여, 상기 원 본 데이터에 대응되는 가상의 합성 데이터를 획득하고, 상기 합성 데이터를 상기 통신부를 통해 서버 장치로 전 송하며, 상기 원본 데이터를 동형 암호화하여 동형 암호문을 획득하고, 상기 동형 암호문을 상기 통신부를 통해 상기 서버 장치로 전송한다. 본 개시의 적어도 하나의 실시 예에 따르면, 서버 장치는, 통신부, 평문용의 제1 인공지능 모델 및 암호문 용의 제2 인공지능 모델을 저장하는 메모리 및 프로세서를 포함하며, 상기 프로세서는, 전자 장치에서 원본 데이터를 기초로 합성한 합성 데이터가 상기 통신부를 통해 수신되면, 상기 합성 데이터를 이용하여 상기 제1 인공지능 모델의 하이퍼파라미터를 평문 상에서 튜닝하고, 상기 제2 인공지능 모델의 적어도 하나의 하이퍼파라미터를 상 기 제1 인공지능 모델의 하이퍼파라미터에 따라 튜닝하며, 상기 원본 데이터에 대한 동형암호문이 상기 통신부 를 통해 수신되면, 상기 튜닝된 적어도 하나의 하이퍼파라미터를 가지는 상기 제2 인공지능 모델을 상기 동형암 호문을 이용하여 학습한다. 본 개시의 적어도 일 실시 예에 따르면, 전자 장치에서 인공지능모델을 학습하는 방법은, 원본 데이터를 생성형 AI 모델에 입력하여, 상기 원본 데이터에 대응되는 가상의 합성 데이터를 획득하는 단계, 상기 서버 장치에서 인공지능 모델의 하이퍼파라미터 튜닝에 사용할 수 있도록, 상기 합성 데이터를 서버 장치로 전송하는 단계, 상 기 원본 데이터를 동형 암호화하여 동형 암호문을 획득하여, 상기 동형 암호문을 상기 서버 장치로 전송하는 단 계를 포함한다. 본 개시의 적어도 일 실시 예에 따르면, 서버 장치에서 인공지능모델을 학습하는 방법은, 전자 장치에서 원본 데이터를 기초로 합성한 합성 데이터가 수신되면, 평문용의 제1 인공지능 모델의 하이퍼파라미터를 상기 합성 데이터를 이용하여 튜닝하는 단계, 암호문 용의 제2 인공지능 모델의 적어도 하나의 하이퍼파라미터를 상기 제1 인공지능 모델의 하이퍼파라미터에 따라 튜닝하는 단계, 상기 원본 데이터에 대한 동형암호문이 수신되면, 상기 튜닝된 적어도 하나의 하이퍼파라미터를 가지는 상기 제2 인공지능 모델을 상기 동형암호문을 이용하여 학습하 는 단계를 포함한다."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "이상과 같은 본 개시의 다양한 실시 예들에 따르면, 인공지능 모델을 학습할 학습 데이터가 외부로 유출될 위험 성을 줄이면서도, 동형 암호 기반에서 인공지능 모델의 성능을 최적화하도록 학습할 수 있다."}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "본 개시에서 수행되는 정보(데이터) 전송 과정은 필요에 따라서 암호화/복호화가 적용될 수 있으며, 본 개시 및 특허청구범위에서 정보(데이터) 전송 과정을 설명하는 표현은 별도로 언급되지 않더라도 모두 암호화/복호화하 는 경우도 포함하는 것으로 해석되어야 한다. 본 개시에서 \"A로부터 B로 전송(전달)\" 또는 \"A가 B로부터 수신\" 과 같은 형태의 표현은 중간에 다른 매개체가 포함되어 전송(전달) 또는 수신되는 것도 포함하며, 반드시 A로부 터 B까지 직접 전송(전달) 또는 수신되는 것만을 표현하는 것은 아니다. 본 개시의 설명에 있어서 각 단계의 순서는 선행 단계가 논리적 및 시간적으로 반드시 후행 단계에 앞서서 수행 되어야 하는 경우가 아니라면 각 단계의 순서는 비제한적으로 이해되어야 한다. 즉, 위와 같은 예외적인 경우를 제외하고는 후행 단계로 설명된 과정이 선행단계로 설명된 과정보다 앞서서 수행되더라도 개시의 본질에는 영향 이 없으며 권리범위 역시 단계의 순서에 관계없이 정의되어야 한다. 그리고 본 명세서에서 \"A 또는 B\"라고 기재 한 것은 A와 B 중 어느 하나를 선택적으로 가리키는 것뿐만 아니라 A와 B 모두를 포함하는 것도 의미하는 것으 로 정의된다. 또한, 본 개시에서 \"포함\"이라는 용어는 포함하는 것으로 나열된 원소 이외에 추가로 다른 구성원 소를 더 포함하는 것도 포괄하는 의미를 가진다. 본 개시에서는 본 개시의 설명에 필요한 필수적인 구성원소만을 설명하며, 본 개시의 본질과 관계가 없는 구성 원소는 언급하지 아니한다. 그리고 언급되는 구성원소만을 포함하는 배타적인 의미로 해석되어서는 안 되며 다 른 구성원소도 포함할 수 있는 비배타적인 의미로 해석되어야 한다. 그리고 본 개시에서 \"값\"이라 함은 스칼라값뿐만 아니라 벡터도 포함하는 개념으로 정의된다. 후술하는 본 개시의 각 단계의 수학적 연산 및 산출은 해당 연산 또는 산출을 하기 위해 공지되어 있는 코딩 방 법 및/또는 본 개시에 적합하게 고안된 코딩에 의해서 컴퓨터 연산으로 구현될 수 있다. 이하에서 설명하는 구체적인 수학식은 가능한 여러 대안 중에서 예시적으로 설명되는 것이며, 본 개시의 권리 범위가 본 개시에 언급된 수학식에 제한되는 것으로 해석되어서는 아니된다. 설명의 편의를 위해서, 본 개시에서는 다음과 같이 표기를 정하기로 한다. a ← D : 분포(D)에 따라서 원소(a)를 선택함 s1, s2 ∈ R : S1, S2 각각은 R 집합에 속하는 원소이다. mod(q) : q 원소로 모듈(modular) 연산 : 내부 값을 반올림함 이하에서는 첨부된 도면을 이용하여 본 개시의 다양한 실시 예들에 대하여 구체적으로 설명한다. 도 1은 본 개시의 다양한 실시 예에 따른 인공지능 모델 학습 방법을 설명하기 위한 도면이다. 도 1에 따르면, 전자 장치, 서버 장치는 네트워크를 통해 서로 연결될 수 있다. 인공지능 모델의 학습은 서버 장 치에 의해 수행되며, 전자 장치는 인공지능 모델의 학습에 필요한 학습 데이터를 제공한다. 네트워크는 다양한 형태의 유무선 통신 네트워크, 방송 통신 네트워크, 광통신 네트워크, 클라우드 네트워 크 등으로 구현될 수 있다. 도 1에서는 각 장치들(100, 200)이 네트워크를 통해 간접적으로 서로 연결된 상 태를 도시하였으나, 이에 한정되는 것은 아니며, 각 장치들(100, 200)은 별도의 매개체 없이 와이파이, 블루투 스, NFC(Near Field Communication) 등과 같은 방식으로 연결될 수도 있다. 도 1에서 전자장치는 사용자가 사용하는 각종 단말 장치가 될 수 있다. 구체적으로, 전자장치는 PC, 랩탑 PC, 휴대폰, 태블릿 PC, 키오스크, TV, 홈서버, 기타 IoT 기능이 탑재된 전자 장치, 게임 플레이어 등 다 양한 형태로 구현될 수 있다. 전자장치에서 사용자가 입력하는 각종 데이터나, 전자장치의 연산 결과에 따라 산출되는 데이터, 전 자장치가 외부 장치로부터 수신한 데이터 등은, 서버장치에서 인공지능 모델을 학습하는데 필요한 학 습 데이터로 사용될 수 있다. 예를 들어, 학교에서 각 학생들의 과목별 시험 성적을 입력한 경우, 그 입력된 성 적 데이터가 학습 데이터가 될 수 있다. 또는, 관청에서 주민의 성별, 나이 등의 데이터를 입력한 경우, 그 신 상 데이터가 학습 데이터가 될 수도 있다. 서버장치는 전자 장치에서 제공하는 학습 데이터를 이용하여, 인공지능 모델을 학습하는 장치이다. 도 1에서는 전자장치 및 서버장치를 각각 하나씩 도시하였으나, 각 장치들(100, 200)은 복수개로 구 현될 수도 있다. 또한, 서버 장치는 전자장치 이외의 다양한 다른 장치들로부터 입력되는 학습 데이터도 함께 사용하여 학습을 수행할 수도 있다. 한편, 전자장치에서 제공하는 학습 데이터가 서버장치로 전달되는 과정에서 제3자에게 노출되거나, 서버장치의 관리자에게 노출될 가능성이 있다. 보안이 중요한 비밀 데이터의 경우, 노출로 인한 피해가 커 질 수 있다. 도 1의 전자장치는 이러한 위험성을 예방하기 위하여 학습 데이터를 동형 암호화하여, 동형 암호문을 획득 하고, 그 동형 암호문을 서버장치로 제공할 수 있다. 즉, 동형 암호문 자체가 학습 데이터가 될 수 있다. 동형 암호문이란 암호화된 상태에서 연산이 가능하도록 암호화한 암호문이다. 본 개시에서, 동형 암호문은, 추 후에 비밀키를 이용하여 복호화하였을 때 원본 데이터 및 에러 값을 포함하는 결과 값이 복원되는 형태로 생성 될 수 있다. 동형 암호문 형태로 서버 장치로 제공되는 경우, 서버 장치는 동형 암호문을 학습 데이터로 활용하여 인공지능 모델을 학습시킬 수 있다. 하지만, 동형 암호문의 특성 상, 연산은 가능하지만, 연산 결과를 서버 장 치에서 확인할 수는 없게 된다. 따라서, 인공지능 모델의 학습 성능을 개선하기 위한 하이퍼 파라미터의 튜닝이 불가능해진다. 인공지능 모델의 성능을 결정하는 주요 요소 중에는 파라미터 및 하이퍼 파라미터(hyper-parameter) 등이 있을 수 있다. 파라미터란 모델이 데이터로부터 학습하는 값들을 의미한다. 파라미터의 예로는, 선형 회귀 모델에서 의 가중치(weight)와 편향(bias) 등이 있다. 하이퍼 파라미터란 모델의 학습 방식을 결정하는 값들을 의미한다. 하이퍼 파라미터는 튜닝이 가능하다. 하이퍼 파라미터는 다양한 인공지능 모델들에 대하여 공통적으로 사용되는 하이퍼 파라미터와 모델 별로 고유하 게 사용되는 하이퍼 파라미터 등이 있을 수 있다. 예를 들어, 인공지능 모델은 크게 머신러닝 모델과 딥러닝 모델로 구분된다. 머신러닝 모델에는 선형 회귀 모델, 로지스틱 회귀 모델, SVM(Support vector machine) 모델, XGBoost(extreme gradient boosting) 모델 등 을 포함하고, 딥러닝 모델은 신경망(일반) 모델, CNN(Convolutional neural network) 모델, RNN/LSTM(Recurrent Neural Network/Long short term memory) 모델, 트랜스포머 모델, GAN(Generative Adversarial Network) 모델 등을 포함한다. 이 중, 선형 회귀 모델과 로지스틱 회귀 모델은 공통 하이퍼 파라미터로 러닝 레이트(learning rate), 에포크 수(number of epochs), 정규화 계수를 사용한다. 로지스틱 회귀 모델은 이에 추가하여 임계값이라는 하이퍼파라 미터도 사용한다. SVM 모델은 정규화 계수, 커널 유형, 감마 등의 하이퍼파라미터를 사용한다. XGBoost 모델은 러닝레이트, 트리계수, 최대 깊이, 감마, 서브샘플 비율 등과 같은 하이퍼파라미터를 사용한다. 딥러닝 모델들 은 공통 하이퍼파라미터로 러닝 레이트, 배치 크기, 에포크 수, 활성화 함수 등을 사용한다. 이 밖에, 개별 하 이퍼파라미터로는 신경망 모델의 경우 은닉층 수, 은닉층 노드 수 등을 사용하고, CNN 모델은 필터 크기, 필터 수, 풀링 크기, 스트라이드 등을 사용하며, RNN/LSTM 모델은 시퀀스 길이, 은닉 상태 크기 등을 사용한다. 트랜 스포머 모델은 어텐션 헤드 수, 인코더/디코더 층 수 등을 사용하고, GAN 모델은 생성자/판별자 구조, 노이즈 차원 등과 같은 하이퍼파라미터를 사용한다. 이 밖에 미니배치 크기(minibatch size)(옵티마이저(optimizer), 레귤러라이저(regularizer), 스케일러(scaler) 등), 결정 트리의 최대 깊이, kNN의 이웃 수(n_neighbors) 등과 같은 하이퍼 파라미터가 사용될 수도 있다. 이 중 러닝 레이트란 학습 과정에서 모델을 얼마나 빠르게 학습할지 결정하기 위한 하이퍼 파라미터이다. 미니배치 크기와 에포크 수는 학습 데이터의 처리 방법을 결정하기 위한 하이퍼 파라미터이다. 동형 암호문을 이용하여 인공지능 모델을 학습하는 경우, 학습 진행 과정을 서버 장치에서 모니터링하거나, 평가 세트(validation set)을 이용한 검증이 어렵다. 따라서, 하이퍼파라미터를 적절한 값으로 튜닝하기 어려우며, 이로 인해 인공지능 모델이 최적 성능을 가지도록 학습하기 어렵다는 문제가 있다. 전자 장치는 이러한 문제를 해결하기 위하여, 원본 데이터를 동형암호화하기 이전에 합성 데이터를 먼저 생성한다. 원본 데이터란 암호화되지 않은 평문 데이터이고, 합성 데이터(Synthetic data)는 원본 데이터와 상 이하지만, 통계적으로 유사한 성격을 가지는 가상의 데이터이다. 다르게는 재현 데이터, 재현 자료, 가상 데이 터, 모의 데이터 등으로 기재할 수도 있다. 전자 장치는 합성 데이터를 서버 장치로 전송한다. 서버 장치는 합성 데이터를 이용하여 인공지 능 모델의 하이퍼파라미터를 평문 상에서 튜닝한다. 전자 장치는 원본 데이터를 동형 암호화하여 동형 암호문을 서버 장치로 전송한다. 서버 장치는 튜닝된 하이퍼파라미터를 가지는 인공지능 모델을 동형 암호문으로 학습한다. 합성 데이터는 원본 데이터와 상 이하지만, 통계적으로 유사한 특징을 가지고 있기 때문에, 이를 이용하여 하이퍼 파라미터를 튜닝하게 되면, 튜 닝된 하이퍼 파라미터는 원본 데이터를 이용한 학습에서도 유효하게 작용할 수 있다. 또한, 동형 암호문 형태로 전송되기 때문에, 외부의 제3자가 원본 데이터를 확보할 위험을 예방할 수 있다. 도 2는 본 개시의 다양한 실시 예에 따른 전자 장치 및 서버 장치의 구성을 나타내는 블럭도이다. 도 2에 따르면, 전자장치는 통신부, 메모리, 프로세서를 포함한다. 통신부는 다양한 외부 장치와 통신을 수행하기 위한 구성이다. 통신부는 유/무선 LAN(Local Area Network), WAN(Wide Area Network), 이더넷(Ethernet), IEEE 1394, 블루투스(Bluetooth), AP 기반의 Wi-Fi(와 이파이, Wireless LAN 네트워크), 지그비(Zigbee), HDMI(High-Definition Multimedia Interface), USB(Universal Serial Bus), MHL(Mobile High-Definition Link), AES/EBU(Audio Engineering Society/ European Broadcasting Union), 옵티컬(Optical), 코액셜(Coaxial) 등과 같은 각종 유무선 통신 방식을 통해 각종 신호 및 데이터를 외부 장치와 송수신할 수 있다. 메모리는 전자장치의 동작에 필요한 각종 프로그램, 데이터, 인스트럭션 등을 저장하기 위한 구성이 다. 메모리는 RAM(dynamic RAM), SRAM(static RAM), SDRAM(synchronous dynamic RAM), OTPROM(one time programmable ROM), PROM(programmable ROM), EPROM(erasable and programmable ROM), EEPROM(electrically erasable and programmable ROM), mask ROM, flash ROM, 플래시 메모리, 하드 드라이브, 또는 솔리드 스테이트 드라이브(solid state drive(SSD)) 등 다양한 메모리들 중 적어도 하나로 구현될 수 있다. 프로세서는 전자장치의 동작을 전반적으로 제어하기 위한 구성이다. 프로세서는 메모리에 저장된 명령어, 프로그램, 데이터 등에 기초하여 다양한 동작을 수행할 수 있다. 프로세서는 디지털 신호를 처리하는 디지털 시그널 프로세서(digital signal processor(DSP)), 마이크로 프로세서(microprocessor)로 구현될 수 있다. 다만 이에 한정되는 것은 아니며, 중앙처리장치(central processing unit(CPU)), MCU(Micro Controller Unit), MPU(Micro Processing Unit), 컨트롤러(controller), 어플리케이션 프로세서(application processor(AP)), 또는 커뮤니케이션 프로세서(communication processor(CP)), ARM 프로세서, AI(Artificial Intelligence) 프로세서 중 하나 또는 그 이상을 포함하거나, 해당 용어로 정의될 수 있다. 또한, 프로세서는 프로세싱 알고리즘이 내장된 SoC(System on Chip), LSI(large scale integration)로 구현될 수도 있고, FPGA(Field Programmable gate array) 형태로 구현될 수도 있다. 프로세서는 메모리에 저장된 컴퓨터 실행가능 명령어(computer executable instructions)를 실행함으로써 다양한 기능을 수행할 수 있다. 구체적으로, 프로세서는 학습에 사용할 원본 데이터가 획득되면, 그 원본 데이터를 이용하여 합성 데이터 를 생성할 수 있다. 프로세서는 데이터 변환 룰(rule)이 존재하는 경우, 그 변환 룰에 기초하여 원본 데이 터를 합성 데이터로 변환할 수 있다. 또는, 프로세서는 생성형 AI 모델을 이용하여 합성 데이터를 생성할 수도 있다. 예를 들어, 메모리는 원본 데이터 및 생성형 AI 모델을 함께 저장할 수 있다. 생성형 AI 모델은 입력되는 데이터에 기초하여, 유사한 통계적 성질을 가지는 가상 데이터를 생성하도록 학습된 인공지능 모델이다. 생성형 AI 모델은 메모리에 하나만 저장될 수도 있으나, 이에 한정되지 않고, 데이터 타입 별로 마련된 복수의 모 델을 포함할 수 있다. 예를 들어, 원본 데이터는 표 등으로 정리된 정형 데이터(Tabular data), 이미지 데이터, 텍스트 데이터 등과 같이 다양한 타입으로 구분될 수 있다. 복수의 모델은, 정형 데이터(Tabular data)에 대응 되는 제1 모델, 이미지에 대응되는 제2 모델, 텍스트에 대응되는 제3 모델을 포함할 수 있다. 제1 모델로는 Generative Adversarial Networks(GANs)가 사용될 수 있다. 구체적으로, 제1 모델은 Tabular GAN(TABGAN) 모델, Variational Autoencoders(VAEs) 모델, Copular-based methods 모델 중 적어도 하나를 포함하고, 제2 모 델은 DCGAN(Deep Convolutional GAN) 모델, StyleGAN 모델, ProGAN 모델 중 적어도 하나를 포함할 수 있다. 제 3 모델은 Generative Pretrained Transformer 모델이나 기타 트랜스포머 기반의 생성 모델을 포함할 수 있다. 이상의 모델들은 예시에 불과하므로 이 밖에 다양한 생성형 AI 모델이 합성 데이터 생성에 사용될 수 있다. 프로세서는 원본 데이터의 데이터 타입을 식별하고, 복수의 모델 중에서 데이터 타입에 대응되는 모델로 원본 데이터를 입력하여, 원본 데이터와 동일한 데이터 타입을 가지는 합성 데이터를 획득한다. 원본 데이터의 데이터 타입은 데이터 파일의 확장자명에 기초하여 식별할 수 있다. 또는, 사용자가 데이터 타입을 직접 선택할 수도 있다. 예를 들어, 전자장치가 디스플레이(미도시)를 더 포함하고 있다면, 프로세서는 데이터 타입을 선택할 수 있는 UI 화면을 디스플레이 상에 표시한다. 사용자 가 UI 화면을 직접 터치하거나, 전자장치에 내장 또는 연결된 입력 장치를 이용하여 UI 화면 내의 하나의 데이터 타입을 선택하면, 프로세서는 선택된 데이터 타입에 대응되는 모델을 선택하여, 원본 데이터를 입 력한다. 이에 따라, 합성 데이터를 획득할 수 있다. 합성 데이터를 생성하는 것과 별개로, 프로세서는 동형 암호화를 위한 키를 생성하고, 그 키를 이용하여 원본 데이터를 동형 암호화하여 동형 암호문을 생성한다. 본 개시의 다양한 실시 예들에서 키 생성 및 동형 암 호문 생성 작업은 합성 데이터의 생성 이후에 수행하는 것으로 도시 및 설명하지만, 키 생성 및 동형 암호문 생 성 작업은 합성 데이터 생성 이전에 선행될 수도 있고, 병렬적으로 수행될 수도 있다. 프로세서는 동형 암호화를 위한 공개키 및 비밀키와, 연산을 위한 연산 키 등을 생성한다. 공개키는 동형 암호문을 생성하는데 사용되는 키이고, 비밀키는 동형 암호문을 복호화하는데 사용되는 키이다. 연산키는 동형 암호문을 기초로 하는 각종 연산(Evaluation or Computation)에 사용되는 키이다. 구체적으로, 연산키는 재선형 화 키(Relinearization key : rlk), 회전키(Rotation Key : rotKey) 등을 포함할 수 있다. 재선형화 키는 곱셈 연산에 사용되고, 회전 키는 회전 연산에 사용될 수 있다. 일 예로, 프로세서는, Ring-LWE 기법을 이용하여 공개키를 생성할 수 있다. 구체적으로 설명하면, 프로세 서는 먼저 각종 파라미터 및 링을 설정하여, 메모리에 저장할 수 있다. 여기서의 파라미터는 인공지 능 모델의 학습에 사용되는 파라미터가 아니라, 동형 암호화 기술에서 사용되는 파라미터이다. 파라미터의 예로 는 평문 메시지 비트의 길이, 공개키 및 비밀키의 크기 등이 있을 수 있다. 링은 다음과 같은 수학식으로 표현될 수 있다. [수학식 1]"}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "여기서 R은 링, Zq는 계수, f(x)는 n차 다항식이다. 링(Ring)이란 기 설정된 계수를 가지는 다항식의 집합으로, 원소들 사이에 덧셈과 곱셈이 정의되어 있으며 덧셈 과 곱셈에 대해서 닫혀 있는 집합을 의미한다. 일 예로, 링 R은 계수가 Zq인 n차 다항식의 집합을 의미한다. 구체적으로는, n이 Φ(N)일 때, N차 사이클로토믹 다항식 (N-th cyclotomic polynomial)으로 다항식을 나눈 나머지로 산출될 수 있는 다항식들을 의미한다. 수학식 1에서 (f(x))란 f(x)로 생성되는 Zq[x]의 이데알(ideal)을 나타낸다. Euler totient 함수 Φ(N)이란 N 과 서로소이고 N보다 작은 자연수의 개수를 의미한다. ΦN(x)를 N차 사이클로토믹 다항식으로 정의하면, 링은 다 음과 같은 수학식 2로도 표현될 수 있다. [수학식 2]"}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 3, "content": "한편, 상술한 수학식 2의 링 R은 평문 공간에서 바이너리 데이터를 가질 수 있다. 이와 같은 링이 설정되면, 프 로세서는 링으로부터 비밀키(sk)를 산출할 수 있다. 비밀키(sk)는 다음과 같이 표현될 수 있다. [수학식 3] sk ← (1, s(x)), s(x) ∈ R 여기서, s(x)는 작은 계수로 랜덤하게 생성한 다항식을 의미한다. 그리고 프로세서는 링으로부터 제1 랜덤 다항식(a(x))을 산출한다. 제1 랜덤 다항식은 다음과 같이 표현될 수 있다. [수학식 4] a(x) ← R 또한, 프로세서는 에러를 산출할 수 있다. 구체적으로, 프로세서는 이산 가우시안 분포 또는 그와 통 계적 거리가 가까운 분포로부터 에러를 추출할 수 있다. 이러한 에러는 다음과 같이 표현될 수 있다. [수학식 5] e(x) ←Dn αq 에러까지 산출되면, 프로세서는 제1 랜덤 다항식 및 비밀키에 에러를 모듈러 연산하여 제2 랜덤 다항식을 산출할 수 있다. 제2 랜덤 다항식은 다음과 같이 표현될 수 있다. [수학식 6] b(x) = -a(x)s(x) + e(x)(mod q) 최종적으로 공개키(pk)는 제1 랜덤 다항식 및 제2 랜덤 다항식을 포함하는 형태로 다음과 같이 설정된다. [수학식 7] pk = (b(x), a(x)) 상술한 키 생성 방법은 일 예에 불과하므로, 반드시 이에 한정되는 것은 아니며, 프로세서는 이 밖에 다른 방법으로 각종 키들을 생성할 수도 있음은 물론이다. 한편, 프로세서가 생성하는 동형 암호문은, 이후에 비밀키를 이용하여 복호화 하였을 때 다음과 같은 성질 을 만족하는 형태로 생성될 수 있다. [수학식 8] Dec(ct, sk) = <ct, sk> = M+e(mod q) 여기서 < , >는 내적 연산(usual inner product), ct는 암호문, sk는 비밀키, e는 암호화 에러 값, mod q는 암 호문의 모듈러스(Modulus)를 의미한다. M은 평문 메시지를 의미하며,본 개시의 다양한 실시 예들에서는 패스워 드가 될 수 있다. q는 스케일링 팩터(scaling factor)(Δ)가 메시지에 곱해진 결과 값 M보다 크게 선택되어야 한다. 에러 값 e의 절대값이 M에 비해서 충분히 작다면, 암호문의 복호화 값 M+e 는 유효 숫자 연산에서 원래의 메시지를 동일한 정밀도로 대체할 수 있는 값이다. 복호화된 데이터 중에서 에러는 최하위 비트(LSB) 측에 배치되고, M은 차하위 비트 측에 배치될 수 있다. 메시지의 크기가 너무 작거나 너무 큰 경우, 프로세서는 스케일링 팩터를 이용하여 그 크기를 조절할 수도 있다. 스케일링 팩터를 사용하게 되면, 정수 형태의 메시지뿐만 아니라 실수 형태의 메시지까지도 암호화할 수 있게 되므로, 활용성이 크게 증대할 수 있다. 또한, 스케일링 팩터를 이용하여 메시지의 크기를 조절함으로써, 연산이 이루어지고 난 이후의 암호문에서 메시지들이 존재하는 영역, 즉, 유효 영역의 크기도 조절될 수 있다. 실시 예에 따라, 암호문 모듈러스 q는 다양한 형태로 설정되어 사용될 수 있다. 일 예로, 암호문의 모듈러스는 스케일링 팩터 Δ의 지수승 q=ΔL 형태로 설정될 수 있다. Δ가 2라면, q=210 과 같은 값으로 설정될 수 있다. 그리고 본 개시에 따른 동형 암호문은 고정 소수점을 사용하는 것을 가정하여 설명하나, 부동 소수점을 사용하 는 경우에도 적용될 수 있다. 프로세서는 동형 암호화를 위하여 생성한 키들에 대한 정보를 메모리에 저장할 수 있다. 생성한 키들 중에서 연산키는 합성 데이터를 이용하여 인공지능 모델의 하이퍼파라미터를 튜닝하는데 사용될 수 있으므로, 합성 데이터 전송과 함께 또는 별개로 서버 장치로 전송하도록 통신부를 제어할 수 있다. 프로세서는 학습에 사용할 원본 데이터가 정해지면, 생성한 키들 중 공개키를 생성하여 원본 데이터를 동 형 암호화하여, 동형 암호문을 생성한다. 프로세서는 생성한 동형 암호문을 서버 장치로 전송하도록 통신부를 제어할 수 있다. 동형 암호문의 전송 시점은 다양하게 달라질 수 있다. 일 예로, 프로세서는 합성 데이터 및 연산키를 전송 한 후, 일정 시간이 경과되면, 동형 암호문의 전송을 전송할 수 있다. 다른 예로, 프로세서는 합성 데이터를 통신부를 통해 외부 장치, 즉, 서버 장치로 전송한 후, 서버 장치로부터 인공지능 모델의 하 이퍼 파라미터 튜닝에 대한 완료 신호가 통신부를 통해 수신되었을 때, 동형 암호문을 전송할 수도 있다. 도 2의 서버장치는 인공지능 모델을 학습하기 위한 장치이다. 도 2의 서버장치는 인공지능 모델을 자 체적으로 저장하고 있을 수도 있고, 인공지능 모델이 저장된 외부 장치와 연결되어 통신을 수행하는 장치일 수 도 있다. 도 2에 따르면, 서버 장치는 통신부, 메모리 및 프로세서를 포함한다. 각 구성의 구체적인 예시는 전자장치의 각 구성의 예시와 동일하므로 중복 설명은 생략한다. 통신부는 전자장치를 비롯한 다양한 외부 장치와 통신을 수행할 수 있다. 메모리는 인공지능 모델을 저장할 수 있다. 인공지능 모델은 평문용의 제1 인공지능 모델과, 암호문 용의 제2 인공지능 모델을 포함할 수 있다. 암호문 용의 제2 인공지능 모델이란 제1 인공지능 모델과 동일 또는 유사 한 구조로 설계되었으나, 동형암호문에서 가능한 연산을 이용하여 근사화한 결과값을 출력하도록 구현된 인공지 능 모델이다. 예를 들어, 동형암호문의 경우 더하기나 곱하기 연산이 가능하므로, 제2 인공지능 모델은 이 밖의 다른 연산을 수행해야 하는 경우에는 더하기 및 곱하기 연산을 활용하여 근사화된 결과값을 획득힌다. 제1 인공지능 모델 및 제2 인공지능 모델은 동일한 종류의 인공지능 모델로, 노드의 깊이나 개수 등의 구조가 동일하며, 동일한 종류의 하이퍼 파라미터 등을 사용하도록 설계될 수 있다. 이 경우, 제1 인공지능 모델을 이 용하여 튜닝한 하이퍼 파라미터를 제2 인공지능 모델에 그대로 적용할 수도 있다. 프로세서는 전자 장치에서 원본 데이터를 기초로 합성한 합성 데이터가 통신부를 통해 수신되면, 합 성 데이터를 이용하여 제1 인공지능 모델의 하이퍼파라미터를 평문 상에서 튜닝한다. 프로세서는 튜닝된 하이퍼파라미터를 메모리에 저장한다. 이 후, 프로세서는 제2 인공지능 모델의 적어도 하나의 하이퍼파라미터를 상기 제1 인공지능 모델의 하이 퍼파라미터에 따라 튜닝할 수 있다. 프로세서는 원본 데이터에 대한 동형암호문이 통신부를 통해 수신되면, 튜닝된 적어도 하나의 하이퍼 파라미터를 가지는 제2 인공지능 모델을 동형암호문을 이용하여 학습한다. 이에 따라, 동형암호문의 연산 결과 를 직접 확인하지 않더라도, 하이퍼 파라미터를 적절하게 튜닝하여 사용할 수 있다. 프로세서는 제1 인공지능 모델의 하이퍼 파라미터를 기준 값으로 설정한 상태에서 합성 데이터를 제1 인공 지능 모델에 입력한 후, 그 결과값을 확인하고, 하이퍼 파라미터를 일정 패턴으로 변경하고 다시 상술한 단계를 수행한다. 합성 데이터의 경우, 원본 데이터와 마찬가지로 암호화가 이루어지지 않은 평문이므로, 제1 인공지능 모델에 의해 출력되는 데이터의 내용을 프로세서가 식별할 수 있다. 프로세서는 식별 결과를 기초로 하이퍼 파라미터를 순차적으로 변경하면서 최적의 하이퍼 파라미터를 탐색하고, 그에 따라 하이퍼 파라미터의 값을 변경한다. 이러한 과정을 튜닝이라고 할 수 있다. 하이퍼 파라미터의 튜닝은 다양한 방식으로 수행할 수 있다. 일 예로, 프로세서는 그리드 탐색(Grid Search) 방식으로 튜닝할 수 있다. 그리드 탐색 방식이란, 사전에 정의된 하이퍼파라미터의 조합을 순차적으로 모두 시험해 보고, 그 시험 결과에 따라 최적의 하이퍼 파라미터 조합을 찾는 방식이다. 다른 예로, 프로세서는 랜덤 탐색 방식으로 튜닝할 수도 있다. 랜덤 탐색(Random Search) 방식이란 하이퍼 파라미터를 무작위로 조합하여 시험한 후, 그 결과값 중에서 최적의 조합을 찾는 방식이다. 또 다른 예로, 프로세서는 베이지안 최적화(Bayesian Optimization) 방식으로 튜닝할 수도 있다. 베이지안 최적화 방식이란 기존 입력값을 기초로 미지의 목적 함수에 대하여 확률적인 추정을 하는 서로게이트 모델 (surrogate model)을 이용하여 사전 정보를 수집하고, 그 사전 정보를 기초로 다음번에 탐색할 입력값 후보를 추천하는 획득 함수(Accuisition Function)를 이용하여 최적값을 탐색하는 방식이다. 하이퍼파라미터 튜닝은 인공지능 모델의 성능을 극대화하기 위한 것으로, 올바르게 튜닝된 하이퍼파라미터는 모 델의 예측 정확도를 높이고, 과적합의 위험을 줄일 수 있다. 이러한 튜닝 과정은 인공지능 모델이 실제 세계의 데이터에 더 잘 적용될 수 있도록 도와주며, 결과적으로 더 신뢰할 수 있는 예측을 제공하도록 한다. 상술한 바와 같이, 인공지능 모델의 종류에 따라 하이퍼파라미터의 종류도 달라질 수 있다. 즉, 프로세서 는, 러닝 레이트(learning rate), 에포크 수(number of epochs), 정규화 계수, 임계값, 커널 유형, 감마, 트리 계수, 최대 깊이, 서브샘플 비율, 배치(batch) 크기, 활성화 함수, 은닉층 수, 은닉층 노드 수, 필터 크기, 필터 수, 풀링 크기, 스트라이드, 시퀀스 길이, 은닉 상태 크기, 어텐션 헤드 수, 인코더/디코더 층 수, 생성자/ 판별자 구조, 노이즈 차원, 미니배치 크기(minibatch size)(옵티마이저(optimizer), 레귤러라이저 (regularizer), 스케일러(scaler) 등), 결정 트리의 최대 깊이, kNN의 이웃 수(n_neighbors) 중 적어도 하나의 하이퍼파라미터를 상술한 다양한 방식들 중 하나를 이용하여 튜닝할 수 있다, 프로세서는 튜닝한 하이퍼파라미터를 메모리에 저장하고, 제2 인공지능 모델의 적어도 하나의 하이퍼 파라미터를 제1 인공지능 모델의 하이퍼파라미터에 따라 튜닝한다. 프로세서는 원본 데이터에 대한 동형암호문이 통신부를 통해 수신되면, 튜닝된 적어도 하나의 하이퍼 파라미터를 가지는 제2 인공지능 모델을 동형암호문을 이용하여 학습한다. 상술한 바와 같이 합성 데이터는 원 본 데이터와 상이하지만 유사한 통계적 특징을 가지므로, 합성 데이터에 기초하여 최적값으로 튜닝한 하이퍼파 라미터는 원본 데이터의 동형 암호문을 이용하여 학습할 때에도 최적의 성능을 가질 수 있도록 작용한다. 결과 적으로, 전자장치는 원본 데이터가 외부로 유출될 가능성을 제거하면서도, 인공지능 모델을 최적 상태로 학습할 수 있게 된다. 도 3은 본 개시의 다양한 실시 예에 따른 전자 장치와 서버 장치의 동작을 설명하기 위한 타이밍도이다. 도 3에 따르면, 전자장치는 원본데이터가 획득되면(S310), 이를 기초로 합성 데이터를 획득한다(S320). 전자장치 는 원본데이터의 데이터 타입에 따라 상이한 생성형 AI 모델을 이용하여 합성 데이터를 획득할 수 있다. 이에 대해서는 상술한 부분에서 구체적으로 기재하였으므로 중복 설명은 생략한다. 전자장치는 합성 데이터를 생성하는 것과 별개로 공개키, 비밀키, 연산 키 등과 같은 키들을 생성한다 (S330). 전자장치는 합성데이터 및 연산키를 일괄적으로 또는 개별적으로 서버장치로 전송한다 (S340). 서버장치는 수신된 합성 데이터 및 연산키를 이용하여 제1 인공지능 모델을 학습하면서 그 결과를 모니터 링하여 제1 인공지능 모델의 하이퍼파라미터를 튜닝하고, 튜닝된 값을 저장한다(S350). 서버장치는 튜닝된 하이퍼파라미터를 제2 인공지능 모델에도 동일하게 적용한다. 실시 예에 따라서, 서버장치는 튜닝이 완료되었음을 알리는 완료 신호를 전자장치로 전송할 수 있으 나 반드시 이에 한정되는 것은 아니다. 전자장치는 공개키를 이용하여 원본 데이터를 동형 암호화하여 동형 암호문을 생성한다(S360). 전자장치 는 서버 장치로부터 완료 신호가 수신되거나, 합성 데이터 및 연산키를 전송한 이후에 일정 시간이 경과한 것으로 확인되면, 동형 암호문을 서버 장치로 전송한다(S370). 서버 장치는 튜닝된 하이퍼파라미터를 가지는 제2 인공지능 모델을 동형 암호문을 이용하여 학습한다 (S380). 도 3에서는 한번의 학습 과정을 설명하였으나, 인공지능 모델의 학습은 복수 회 반복 수행될 수 있다. 즉, 전자 장치는 원본 데이터가 획득 될 때마다 상술한 과정을 반복 수행하여, 인공지능 모델의 학습을 지원할 수 있다. 도 4는 본 개시의 적어도 하나의 실시 예에 따른 전자장치의 인공지능 모델 학습 방법을 설명하기 위한 흐름도 이다. 도 4에 따르면, 전자장치는 원본데이터에 대응되는 합성 데이터를 획득한 후(S410), 합성 데이터를 서버 장치로 전송한다(S420). 전자장치는 동형암호문의 연산에 사용되는 연산키를 합성 데이터와 함께 또는 별도로 전송할 수 있다. 이후, 전자장치는 원본데이터를 동형암호화한 동형암호문을 서버 장치로 전송한다(S430). 이러한 동작에 의해, 전자 장치는 원본 데이터가 외부로 유출되는 것을 방지하면서도 서버 장치가 인공지능 모 델을 최적 성능으로 학습하도록 지원할 수 있다. 도 5는 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 인공지능 모델 학습 방법을 설명하기 위한 흐름도 이다. 도 5에 따르면, 서버장치는 합성 데이터가 수신되면(S510), 제1 인공지능 모델의 하이퍼파라미터를 튜닝 하고 저장한다(S520). 서버 장치는 튜닝된 하이퍼파라미터를 제2 인공지능 모델에 대해서도 동일하게 적용하여, 제2 인공지능 모델의 하이퍼 파라미터를 튜닝한다(S530). 이 후, 서버 장치는 동형 암호문이 수신되면(S540), 튜닝된 하이퍼파라미터를 가지는 제2 인공지능 모델을 동형 암호문을 이용하여 학습한다(S550). 도 4의 전자 장치 및 도 5의 서버 장치는 도 2에서 설명한 구성을 가지는 전자 장치 및 서버 장치로 구현될 수 있으나 반드시 이에 한정되는 것은 아니며, 일부 구성이 추가 또는 제거되거나, 타 구성으로 대체된 형태의 장치에 의해 수행될 수도 있다. 이상의 다양한 실시 예들에서 설명한 내용은 각 실시 예 별로 단독으로 구현될 수도 있고, 본 개시의 다른 실시 예들 중 적어도 일부와 조합되어 구현될 수도 있다. 또한, 이상에서 설명된 다양한 실시예들은 기기(machine)(예: 컴퓨터)로 읽을 수 있는 저장 매체(machine- readable storage media)에 저장된 명령어를 포함하는 소프트웨어로 구현될 수 있다. 기기는, 저장 매체로부터 저장된 명령어를 호출하고, 호출된 명령어에 따라 동작이 가능한 장치로서, 개시된 실시예들에 따른 서버 장치 를 포함할 수 있다. 명령이 프로세서에 의해 실행될 경우, 프로세서가 직접, 또는 프로세서의 제어 하에 다른 구성요소들을 이용하여 명령에 해당하는 기능을 수행할 수 있다. 명령은 컴파일러 또는 인터프리터에 의해 생성 또는 실행되는 코드를 포함할 수 있다. 구체적으로는, 전자 장치에 의해 인공지능모델을 학습하는 방법을 수행하기 위한 프로그램은 비일시적 판독 가 능 기록 매체에 저장되어 제공될 수 있다. 여기서의 방법은, 원본 데이터를 생성형 AI 모델에 입력하여, 상기 원본 데이터에 대응되는 가상의 합성 데이터를 획득하는 단계, 상기 서버 장치에서 인공지능 모델의 하이퍼파라 미터 튜닝에 사용할 수 있도록, 상기 합성 데이터를 서버 장치로 전송하는 단계 및 상기 원본 데이터를 동형 암 호화하여 동형 암호문을 획득하여, 상기 동형 암호문을 상기 서버 장치로 전송하는 단계를 포함한다. 또는, 서버 장치에 의해 인공지능 모델을 학습하는 방법을 수행하기 위한 프로그램은, 전자 장치에서 원본 데이 터를 기초로 합성한 합성 데이터가 수신되면, 평문용의 제1 인공지능 모델의 하이퍼파라미터를 상기 합성 데이 터를 이용하여 튜닝하는 단계, 암호문 용의 제2 인공지능 모델의 적어도 하나의 하이퍼파라미터를 상기 제1 인 공지능 모델의 하이퍼파라미터에 따라 튜닝하는 단계, 상기 원본 데이터에 대한 동형암호문이 수신되면, 상기 튜닝된 적어도 하나의 하이퍼파라미터를 가지는 상기 제2 인공지능 모델을 상기 동형암호문을 이용하여 학습하 는 단계를 포함한다. 비일시적 판독 가능 기록 매체에서, '비일시적'은 저장 매체가 신호(signal)를 포함하지 않으며 실재(tangibl e)한다는 것을 의미할 뿐 데이터가 저장 매체에 반영구적 또는 임시적으로 저장됨을 구분하지 않는다. 또한, 이상에서 설명된 다양한 실시예들에 따른 방법은 컴퓨터 프로그램 제품(computer program product)에 포 함되어 제공될 수 있다. 컴퓨터 프로그램 제품은 상품으로서 판매자 및 구매자 간에 거래될 수 있다. 컴퓨터 프 로그램 제품은 기기로 읽을 수 있는 저장 매체(예: compact disc read only memory (CD-ROM))의 형태로, 또는 어플리케이션 스토어를 통해 온라인으로 배포될 수 있다. 온라인 배포의 경우에, 컴퓨터 프로그램 제품의 적어 도 일부는 제조사의 서버, 어플리케이션 스토어의 서버, 또는 중계 서버의 메모리와 같은 저장 매체에 적어도 일시 저장되거나, 임시적으로 생성될 수 있다. 상술한 다양한 실시예들에 따른 구성 요소(예: 모듈 또는 프로그램) 각각은 단수 또는 복수의 개체로 구성될 수 있으며, 전술한 해당 서브 구성 요소들 중 일부 서브 구성 요소가 생략되거나, 또는 다른 서브 구성 요소가 다 양한 실시예에 더 포함될 수 있다. 대체적으로 또는 추가적으로, 일부 구성 요소들(예: 모듈 또는 프로그램)은 하나의 개체로 통합되어, 통합되기 이전의 각각의 해당 구성 요소에 의해 수행되는 기능을 동일 또는 유사하게 수행할 수 있다. 다양한 실시예들에 따른, 모듈, 프로그램 또는 다른 구성 요소에 의해 수행되는 동작들은 순차 적, 병렬적, 반복적 또는 휴리스틱하게 실행되거나, 적어도 일부 동작이 다른 순서로 실행되거나, 생략되거나, 또는 다른 동작이 추가될 수 있다. 이상에서는 본 개시의 바람직한 실시예에 대하여 도시하고 설명하였지만, 본 개시는 상술한 특정의 실시예에 한"}
{"patent_id": "10-2024-0094631", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 4, "content": "정되지 아니하며, 청구범위에서 청구하는 본 개시의 요지를 벗어남이 없이 당해 개시에 속하는 기술분야에서 통 상의 지식을 가진 자에 의해 다양한 변형 실시가 가능한 것은 물론이고, 이러한 변형실시들은 본 개시의 기술적 사상으로부터 개별적으로 이해되어져서는 안될 것이다. 부호의 설명100 : 전자 장치 200 : 서버 장치 110 : 통신부 120 : 메모리 130 : 프로세서"}
{"patent_id": "10-2024-0094631", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 개시의 다양한 실시 예에 따른 인공지능 모델 학습 방법을 설명하기 위한 도면, 도 2는 본 개시의 다양한 실시 예에 따른 전자 장치 및 서버 장치의 구성을 나타내는 블럭도, 도 3은 본 개시의 다양한 실시 예에 따른 전자 장치 및 서버 장치의 동작을 설명하기 위한 타이밍도, 도 4는 본 개시의 적어도 하나의 실시 예에 따른 전자 장치의 인공지능 모델 학습 방법을 설명하기 위한 흐름도, 그리고, 도 5는 본 개시의 적어도 하나의 실시 예에 따른 서버 장치의 인공지능 모델 학습 방법을 설명하기 위한 흐름도 이다."}
