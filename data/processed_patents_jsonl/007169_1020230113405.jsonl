{"patent_id": "10-2023-0113405", "section": "특허_기본정보", "subsection": "특허정보", "content": {"공개번호": "10-2025-0031657", "출원번호": "10-2023-0113405", "발명의 명칭": "음성과 동조된 입모양 애니메이션 합성 방법 및 장치", "출원인": "포항공과대학교 산학협력단", "발명자": "서영주"}}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_1", "content": "메모리(memory)에 저장된 적어도 하나 이상의 명령을 실행하는 프로세서(processor)에 의하여 수행되는 방법으로서, 스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 단계; 상기 스피치 특징을 입력받아 프레임 별로 상기 스피치 데이터에 대응하는 블렌드셰이프 계수를 예측하는 단계;및 상기 예측된 블렌드셰이프 계수를 이용하여 상기 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를판별하는 단계;를 포함하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_2", "content": "제1항에 있어서, 상기 블렌드셰이프 계수를 예측하는 단계는, 상기 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타임-윈도우잉 셀프 어텐션 기법에 의하여 상기 블렌드셰이프 계수를 예측하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_3", "content": "제1항에 있어서, 상기 예측된 블렌드셰이프 계수와 상기 스피치 데이터에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차 손실함수를 이용하여 상기 블렌드셰이프 계수를 예측하는 단계를 훈련하는 단계; 를 더 포함하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_4", "content": "제3항에 있어서, 상기 블렌드셰이프 계수를 예측하는 단계를 훈련하는 단계는, 상기 스피치 데이터에 대한 입술모양 애니메이션 데이터를 상기 스피치 데이터에 대한 그라운드 트루쓰 데이터로 이용하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_5", "content": "제1항에 있어서, 상기 판별하는 단계는, 상기 예측된 블렌드셰이프 계수 및 상기 스피치 특징에 대한 조건부 연결 데이터를 이용하여 상기 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계;공개특허 10-2025-0031657-3-를 포함하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_6", "content": "제5항에 있어서, 상기 판별하는 단계는, 상기 조건부 연결 데이터를 복수개의 제1 1차원 합성곱 레이어에 전달하는 단계; 및 상기 복수개의 제1 1차원 합성곱 레이어의 출력을 제2 1차원 합성곱 레이어에 전달하는 단계;를 더 포함하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_7", "content": "제6항에 있어서, 상기 판별하는 단계는, 상기 제2 1차원 합성곱 레이어의 출력을 조건부 생성적 적대 신경망 손실함수를 이용하여 판별하는 단계;를 더 포함하는, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_8", "content": "제1항에 있어서, 상기 스피치 데이터는 스펙트로그램 데이터인, 음성과 동조된 입모양 합성 방법."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_9", "content": "스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 오디오 인코더; 상기 스피치 특징을 입력받아 프레임 별로 상기 스피치 데이터에 대응하는 블렌드셰이프 계수를 예측하는 블렌드셰이프 디코더; 및 상기 예측된 블렌드셰이프 계수를 이용하여 상기 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를판별하는 판별기;를 포함하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_10", "content": "제9항에 있어서, 상기 블렌드셰이프 디코더는, 상기 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타임-윈도우잉 셀프 어텐션 기법에 의하여 상기 블렌드셰이프 계수를 예측하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_11", "content": "제9항에 있어서, 공개특허 10-2025-0031657-4-상기 예측된 블렌드셰이프 계수와 상기 스피치 데이터에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차 손실함수를 이용하여 상기 블렌드셰이프 디코더가 훈련되는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_12", "content": "제11항에 있어서, 상기 블렌드셰이프 디코더의 훈련을 위하여 상기 스피치 데이터에 대한 입술모양 애니메이션 데이터를 상기 스피치 데이터에 대한 그라운드 트루쓰 데이터로 이용하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_13", "content": "제9항에 있어서, 상기 판별기는, 상기 예측된 블렌드셰이프 계수 및 상기 스피치 특징에 대한 조건부 연결 데이터를 생성하는 연결기(concatenator);를 포함하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_14", "content": "제13항에 있어서, 상기 판별기는, 상기 조건부 연결 데이터를 입력받는 복수개의 제1 1차원 합성곱 레이어; 및 상기 복수개의 제1 1차원 합성곱 레이어의 출력을 입력받는 제2 1차원 합성곱 레이어;를 더 포함하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_15", "content": "제14항에 있어서, 상기 판별기의 출력을 이용하여 상기 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하기위한 조건부 생성적 적대 신경망 손실함수;를 더 포함하는, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "청구범위", "subsection": "청구항", "claim_number": "청구항_16", "content": "제9항에 있어서, 상기 스피치 데이터는 스펙트로그램 데이터인, 음성과 동조된 입모양 합성 장치."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "요약", "paragraph": 1, "content": "본 발명의 일 실시예에 따른 방법은, 스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 단계; 스피치 특징을 입력받아 프레임 별로 스피치 데이터에 대응하는 블렌드셰이프 계수를 예측하는 단계; 및 예측된 블렌드셰이프 계수를 이용하여 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계를 포함한다."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "기술분야", "paragraph": 1, "content": "본 발명은 콘텐츠에 기반하여 영상 및/또는 애니메이션을 합성하는 기술에 관한 것으로, 특히 음성의 콘텐츠에 동조된 입모양 애니메이션을 합성하는 기술에 관한 것이다."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "배경기술", "paragraph": 1, "content": "이 부분에 기술된 내용은 단순히 본 실시예에 대한 배경 정보를 제공할 뿐 종래 기술을 구성하는 것은 아니다. 20 여년이 넘는 기간 동안 사실적인 3차원 얼굴 애니메이션을 제작하는 연구들이 그래픽스와 비전 학계에서 널 리 이루어져 왔다. 그래프 구조를 이용하여 입력 오디오에 대응되는 얼굴 모양 데이터를 매칭시켜서 블렌딩하여 얼굴 애니메이션을 처리하는 방법이 제안되었다. 또한, 세밀한 얼굴 움직임의 추적과 사실적인 결과를 얻기 위해 복잡한 캡쳐 장비를 이용한 연구 및 사람이 대 화할 때의 립싱크를 표현하기 위하여 오디오 신호를 사용하는 연구가 제안되었다. 오디오 신호에 기반하여 얼굴 애니메이션을 합성하는 기법으로서 동적 베이시안 네트워크(dynamic Bayesian network)를 이용하여 발성효과를 효과적으로 애니메이션화하는 연구가 제안되었다. 최근에는 딥러닝(deep learning) 기술에 기반하여 HMD(Head Mounted Display)를 착용한 사용자의 표정에서 가 려진 부분을 추정하여 사실적인 얼굴 애니메이션을 생성하는 연구가 제안되었으며, 딥러닝 기술에 기반하여 다 양한 언어로 대화하는 립싱크 애니메이션을 통합된 시스템에서 처리하는 연구가 제안되었다. 종래의 립싱크 애니메이션 연구들은 사람이 대화하는 환경의 입모양 움직임을 사실적으로 애니메이션화하는 방 법에 초점을 맞추어 왔으며, 대부분 사람이 대화하고 이야기하는 상황에서 자연스러운 립싱크 애니메이션을 처 리하는 방법들이 연구되어 왔다. 초기의 립싱크 애니메이션 연구들은 사실적인 얼굴 애니메이션을 표현하기 위하여 복잡한 얼굴 캡쳐 장비에 의 존하거나 매우 많은 얼굴 캡쳐 데이터를 기반으로 처리한 방법들을 다루어 왔다. 그리고 딥러닝처럼 최신의 인 공지능 기술을 이용한 립싱크 애니메이션 기술도 개발되었지만, 여전히 사람이 대화하는 환경에 초점을 맞춘 연 구들이 제안되었다. 하지만 사람이 노래를 부르는 환경에서는 입술과 머리의 움직임이 다양한 형태로 변화하며, 음운의 강도에 따라 입이 벌어지는 크기가 달라진다. 기존 방법으로는 가창 환경의 립싱크 애니메이션을 사실적 으로 재현하기에는 한계가 있다. 현재까지 제시된 연구들의 공통적인 문제점으로는, 오디오 신호만으로는 가창 환경에서 나타나는 입모양과 머리 의 움직임을 표현할 수 없는 단점이 있다. 특히 오디오의 콘텐츠에 대응하는 입모양을 효율적으로 표현하기 어 려운 점이 가장 큰 문제로 알려져 있다. 선행기술문헌 특허문헌 (특허문헌 0001) 한국등록특허공보 KR 10-2058783 B1 \"텍스트 기반 적응적 가창 립싱크 애니메이션 생성 장치 및 방법\" (2019년 12월 17일) (특허문헌 0002) 한국공개특허공보 KR 10-2022-0096067 A \"인공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치\" (2022년 7월 7일) 비특허문헌 (비특허문헌 0001) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. \"Image-to-image translation with conditional adversarial networks.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017. (2016년 11월 21일)"}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "해결하려는과제", "paragraph": 1, "content": "본 발명은 종래 기술의 문제점을 해결하기 위해 도출된 것으로, 언어에 구애받지 않고 음성과 동조된 입모양 애 니메이션을 합성하는 기술을 제안하는 것을 목적으로 한다. 본 발명은 음성과 동조된 입모양 애니메이션을 더욱 자연스럽게 표현하는 기술을 제안하는 것을 목적으로 한다. 본 발명은 이미지가 아닌 음성만으로도 입모양 애니메이션을 합성할 수 있는 기술을 제안하는 것을 또 다른 목 적으로 한다. 본 발명은 실시간 영상에서 마스크를 쓰고 있거나, 화자가 카메라를 향하지 않는 경우에도 음성에 기반하여 입모양 애니메이션을 합성할 수 있는 기술을 제안할 수 있다."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "과제의해결수단", "paragraph": 1, "content": "상기 목적을 달성하기 위한 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 방법은, 메모리(memory)에 저장된 적어도 하나 이상의 명령을 실행하는 프로세서(processor)에 의하여 수행되는 방법으로서, 스피치 데이 터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 단계; 스피치 특징을 입력받아 프레임 별로 스피치 데이터에 대응하는 블렌드셰이프 계수를 예측하는 단계; 및 예측된 블렌드셰이프 계수를 이용하여 예측된 블렌 드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계를 포함한다. 블렌드셰이프 계수를 예측하는 단계는, 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타 임-윈도우잉 셀프 어텐션 기법에 의하여 블렌드셰이프 계수를 예측할 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 방법은, 예측된 블렌드셰이프 계수와 스피치 데이터에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차(MSE) 손실함수를 이용하여 블렌드셰이프 계수를 예측하는 단계 를 훈련하는 단계를 더 포함할 수 있다. 블렌드셰이프 계수를 예측하는 단계를 훈련하는 단계는, 스피치 데이터에 대한 입술모양 애니메이션 데이터를 스피치 데이터에 대한 그라운드 트루쓰 데이터로 이용할 수 있다. 판별하는 단계는, 예측된 블렌드셰이프 계수 및 스피치 특징에 대한 조건부 연결 데이터를 이용하여 예측된 블 렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계를 포함할 수 있다. 판별하는 단계는, 조건부 연결 데이터를 복수개의 제1 1차원 합성곱 레이어에 전달하는 단계; 및 복수개의 제1 1차원 합성곱 레이어의 출력을 제2 1차원 합성곱 레이어에 전달하는 단계를 더 포함할 수 있다. 판별하는 단계는, 제2 1차원 합성곱 레이어의 출력을 조건부 생성적 적대 신경망 손실함수를 이용하여 판별하는 단계를 더 포함할 수 있다. 스피치 특징을 추출하기 위하여 입력받는 스피치 데이터는 스펙트로그램 데이터일 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 오디오 인코더; 스피치 특징을 입력받아 프레임 별로 스피치 데이터에 대응하는 블렌드 셰이프 계수를 예측하는 블렌드셰이프 디코더; 및 예측된 블렌드셰이프 계수를 이용하여 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 판별기를 포함한다. 블렌드셰이프 디코더는, 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타임-윈도우잉 셀 프 어텐션 기법에 의하여 블렌드셰이프 계수를 예측할 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치에서, 예측된 블렌드셰이프 계수와 스피치 데이터 에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차 (MSE) 손실함수를 이용하여 블렌드셰이프 디코더가 훈련될 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 블렌드셰이프 디코더의 훈련을 위하여 스피치 데이터에 대한 입술모양 애니메이션 데이터를 스피치 데이터에 대한 그라운드 트루쓰 데이터로 이용할 수 있다. 판별기는, 예측된 블렌드셰이프 계수 및 스피치 특징에 대한 조건부 연결 데이터를 생성하는 연결기 (concatenator)를 포함할 수 있다. 판별기는, 조건부 연결 데이터를 입력받는 복수개의 제1 1차원 합성곱 레이어; 및 복수개의 제1 1차원 합성곱 레이어의 출력을 입력받는 제2 1차원 합성곱 레이어를 더 포함할 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 판별기의 출력을 이용하여 예측된 블렌드셰이 프 계수가 진짜(real)인지 가짜(fake)인지를 판별하기 위한 조건부 생성적 적대 신경망 손실함수를 더 포함할 수 있다."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "발명의효과", "paragraph": 1, "content": "본 발명의 일 실시예에 따르면, 언어에 구애받지 않고 음성과 동조된 입모양 애니메이션을 합성할 수 있다. 본 발명의 일 실시예에 따르면, 음성과 동조된 입모양 애니메이션을 더욱 자연스럽게 표현할 수 있다."}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 1, "content": "상기 목적 외에 본 발명의 다른 목적 및 특징들은 첨부 도면을 참조한 실시예에 대한 설명을 통하여 명백히 드 러나게 될 것이다. 본 발명은 다양한 변경을 가할 수 있고 여러 가지 실시예를 가질 수 있는 바, 특정 실시예들을 도면에 예시하고 상세하게 설명하고자 한다. 그러나, 이는 본 발명을 특정한 실시 형태에 대해 한정하려는 것이 아니며, 본 발명 의 사상 및 기술 범위에 포함되는 모든 변경, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. 제1, 제2, A, B 등의 용어는 다양한 구성요소들을 설명하는데 사용될 수 있지만, 상기 구성요소들은 상기 용어 들에 의해 한정되어서는 안 된다. 상기 용어들은 하나의 구성요소를 다른 구성요소로부터 구별하는 목적으로만 사용된다. 예를 들어, 본 발명의 권리 범위를 벗어나지 않으면서 제1 구성요소는 제2 구성요소로 명명될 수 있 고, 유사하게 제2 구성요소도 제1 구성요소로 명명될 수 있다. '및/또는' 이라는 용어는 복수의 관련된 기재된 항목들의 조합 또는 복수의 관련된 기재된 항목들 중의 어느 항목을 포함한다. 본 출원의 실시예들에서, \"A 및 B 중에서 적어도 하나\"는 \"A 또는 B 중에서 적어도 하나\" 또는 \"A 및 B 중 하나 이상의 조합들 중에서 적어도 하나\"를 의미할 수 있다. 또한, 본 출원의 실시예들에서, \"A 및 B 중에서 하나 이 상\"은 \"A 또는 B 중에서 하나 이상\" 또는 \"A 및 B 중 하나 이상의 조합들 중에서 하나 이상\"을 의미할 수 있다. 어떤 구성요소가 다른 구성요소에 \"연결되어\" 있다거나 \"접속되어\" 있다고 언급된 때에는, 그 다른 구성요소에 직접적으로 연결되어 있거나 또는 접속되어 있을 수도 있지만, 중간에 다른 구성요소가 존재할 수도 있다고 이 해되어야 할 것이다. 반면에, 어떤 구성요소가 다른 구성요소에 \"직접 연결되어\" 있다거나 \"직접 접속되어\" 있 다고 언급된 때에는, 중간에 다른 구성요소가 존재하지 않는 것으로 이해되어야 할 것이다. 본 출원에서 사용한 용어는 단지 특정한 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 다르게 뜻하지 않는 한, 복수의 표현을 포함한다. 본 출원에서, \"포함 하다\" 또는 \"가지다\" 등의 용어는 명세서상에 기재된 특징, 숫자, 단계, 동작, 구성요소, 부품 또는 이들을 조 합한 것이 존재함을 지정하려는 것이지, 하나 또는 그 이상의 다른 특징들이나 숫자, 단계, 동작, 구성요소, 부 품 또는 이들을 조합한 것들의 존재 또는 부가 가능성을 미리 배제하지 않는 것으로 이해되어야 한다. 다르게 정의되지 않는 한, 기술적이거나 과학적인 용어를 포함해서 여기서 사용되는 모든 용어들은 본 발명이 속하는 기술 분야에서 통상의 지식을 가진 자에 의해 일반적으로 이해되는 것과 동일한 의미를 가지고 있다. 일 반적으로 사용되는 사전에 정의되어 있는 것과 같은 용어들은 관련 기술의 문맥 상 가지는 의미와 일치하는 의 미를 가진 것으로 해석되어야 하며, 본 출원에서 명백하게 정의하지 않는 한, 이상적이거나 과도하게 형식적인 의미로 해석되지 않는다. 한편 본 출원일 전에 공지된 기술이라 하더라도 필요 시 본 출원 발명의 구성의 일부로서 포함될 수 있으며, 이 에 대해서는 본 발명의 취지를 흐리지 않는 범위 내에서 본 명세서에서 설명한다. 다만 본 출원 발명의 구성을 설명함에 있어, 본 출원일 전에 공지된 기술로서 당업자가 자명하게 이해할 수 있는 사항에 대한 자세한 설명은본 발명의 취지를 흐릴 수 있으므로, 공지 기술에 대한 지나치게 자세한 사항의 설명은 생략한다. 예를 들어, 인공신경망, 특히 생성적 적대 신경망 (GAN, Generative Adversarial Network) 등을 이용하여 입모 양 애니메이션을 생성하는 기술, 얼굴 모양 데이터 및 각 항목의 변형량을 이용하여 얼굴모양 애니메이션을 생 성하는 기술 등은 본 발명의 출원 전 공지 기술을 이용할 수 있으며, 이들 공지 기술들 중 적어도 일부는 본 발 명을 실시하는 데에 필요한 요소 기술로서 적용될 수 있다. 예를 들어 본 발명의 구성의 일부를 실시하는 데에 필요한 요소 기술은 한국등록특허공보 KR 10-2058783 B1 \"텍스트 기반 적응적 가창 립싱크 애니메이션 생성 장 치 및 방법\" 및 한국공개특허공보 KR 10-2022-0096067 A \"인공신경망을 이용한 애니메이션 캐릭터 입모양 합성 장치\", Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. \"Image-to-image translation with conditional adversarial networks.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017. (이하 선행문헌 Isola) 등을 통하여 당업자에게 공지되었음을 알림으로써 설명에 갈음할 수 있다. 그러나 본 발명의 취지는 이들 공지 기술에 대한 권리를 주장하고자 하는 것이 아니며 공지 기술의 내용은 본 발명의 취지에 벗어나지 않는 범위 내에서 본 발명의 일부로서 포함될 수 있다. 이하, 첨부한 도면들을 참조하여, 본 발명의 바람직한 실시예를 보다 상세하게 설명하고자 한다. 본 발명을 설 명함에 있어 전체적인 이해를 용이하게 하기 위하여 도면상의 동일한 구성요소에 대해서는 동일한 참조부호를 사용하고 동일한 구성요소에 대해서 중복된 설명은 생략한다. 도 1은 본 발명의 일 실시예에 따른 음성과 동조된 입모양 애니메이션 합성 장치의 기능 블록을 도시하는 개념 도이다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 오디오 인코더 (Audio Encoder) ; 스피치 특징을 입력받아 프레임 별로 스피치 데 이터에 대응하는 블렌드셰이프 계수를 예측하는 블렌드셰이프 디코더 (Blendshape Decoder) ; 및 예측된 블렌드셰이프 계수를 이용하여 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 판별기 (Discriminator) 를 포함한다. Blendshape Decoder 의 기능은 평균제곱오차(MSE, Mean Square Error) 손실함수 를 이용하여 1차적 으로 훈련될 수 있다. Audio Encoder 및 Blendshape Decoder 는 생성적 적대 신경망 (GAN)의 생성기 (generator) 파트를 구성할 수 있다. 판별기 의 출력은 조건부 GAN (cGAN) 손실함수 를 이용하여 진짜(real)인지 가짜(fake)인지 판별될 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 애니메이션 합성 장치는 생성적 적대 신경망(GAN) 기법을 이 용할 수 있다. 이때 cGAN 손실함수 가 판별기의 기능을 훈련하는 데 이용되며, 판별기를 훈련하 기 전에 생성기 파트 또는 Blenndshape Decoder 를 MSE 손실함수 를 이용하여 먼저 훈련할 수 있다. 3D face animation은 메타버스, VR 등 가상세계가 주목을 받기 시작하면서 활발하게 연구되고 있다. 얼굴 애니 메이션을 생성하기 위해서는 다음과 같은 요소들이 구현될 것이 요구된다. Shape는 얼굴의 생김새로 특정 부위의 형태적 특성을 나타내고, Expression은 웃는 얼굴과 같이 주로 감정을 표 현하는 얼굴의 움직임을 의미한다. Motion은 소리에 따른 입술의 움직임이나 눈 깜빡임 같은 주기적 움직임 혹은 머리의 방향 등 같은 움직임을 의 미한다. 얼굴의 다양한 구성요소 중에서 특히 Lip motion은 speech와 강한 연관성을 가지는 것으로 알려져 있다. 음성으 로 얼굴의 형태나 눈 깜빡임과 같은 다른 움직임은 잡아낼 수 없지만 입모양은 음성으로부터 직접적으로 영향을 받는다. 영상을 활용해도 입모양을 완벽하게 구현할 수 있지만, 음성을 이용할 경우에 더욱 효과적으로 입모양을 구현할 수 있는 장점이 있다. 첫번째는 음성을 이용할 경우에는 얼굴을 가리는 것에 영향을 받지 않을 수 있다. 최근에는 팬데믹때문에 많은 사람들이 마스크를 끼고 있으므로, 카메라만을 이용하는 경우에는 마스크처럼 얼굴을 가리는 것이 있다면 제대 로 입모양을 생성할 수 없는 한계가 있다. 두번째는 소리는 공간의 제약을 받지 않는다는 점이다. 음성은 같은 공간에 있다면 어디에서든 들을 수 있기 때 문에, 사람이 뒤를 돌아보거나 카메라가 다른 것을 촬영하는 경우에도 음성에 기반한 입모양 애니메이션을 구현 할 수 있다. 실시예에 따라서는 가상현실과 같이 유저를 3D 아바타를 이용하여 구현할 때 카메라가 유저의 얼굴에 대한 실시 간 영상을 확보할 수 없는 경우에는 입모양을 표현하려면 음성 데이터가 필수적으로 요구된다. 음성을 기반으로 입모양 애니메이션을 합성하기 위해서 음성의 구성요소 중 입모양과 연관된 피쳐를 추출할 것 이 요구된다. 음성의 구성요소는 크게 4가지 요소로 알려져 있다. 리듬(rhythm)은 화자의 발화가 얼마나 빠르기를 나타내고, 피치는 음의 높낮이를 표현한다. 이를 통해 화자의 억양을 알 수 있다. 팀버(timber)는 음색으로, 목소리의 개성을 표현하는 인지적 특성이라고 할 수 있다. 이 팀버는 사람의 목소리 를 구분하는데 쓰이며 밝은 목소리 중후한 목소리 등으로 표현할 수 있다. 마지막으로 컨텐츠는 음성학적 피쳐이다. 즉 발화의 내용을 담고 있으며, 발화의 가장 기본 단위인 포님 (phoneme)으로 구분될 수 있다. 이 중에서 입모양과 가장 연관이 깊은 요소는 컨텐츠이며, 다른 요소를 제외하고 언어학적 특징을 적절히 추출 하는 것이 입모양 애니메이션을 합성하기 위하여 중요하다. 도 2는 본 발명의 일 실시예에 따른 오디오 인코더의 기능을 도시하는 개념도이다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치에서, 스피치 특징을 추출하기 위하여 입력받는 스 피치 데이터는 스펙트로그램 데이터일 수 있다. 스펙트로그램 데이터는 음성의 waveform을 그대로 이 용하기보다는 DeepSpeech2와 같이 알려진 Pretrained 음성인식 모델의 입력으로 이용될 수 있는 형태이다. Audio Encoder 는 입력된 스펙트로그램 데이터를 이용하여 ASR Pretrained Model 및 보간 (interpolate) 과정을 거쳐서 스피치 특징을 추출할 수 있다. 추출되는 스피치 특징은 Categorical Speech Feature 일 수 있다. Audio Encoder 는 두 가지 기술적인 특징에 기반하여 설계될 수 있다. 첫번째 특징은 전이 학습(transfer learning)을 통하여 speaker-independent speech feature를 추출할 수 있도 록 하는 것이고, 두번째 특징은 언어에 무관하게 발음된 소리의 phoneme feature를 포함할 수 있도록 스피치 특 징이 추출되는 것이다. Audio Encoder 는 speaker-independent speech feature를 입력 오디오 클립과 분리하기 위하여 pretrained ASR 모델인 DeepSpeech2 와 같은 모델을 이용하여 전이학습에 사용할 수 있다. DeepSpeech2는 스펙 트로그램을 입력받아 2개의 합성곱 레이어와 5개의 양방향 RNN 레이어를 거쳐 최종적으로 fully connected 레이 어를 경유하여 28 개의 영문 캐릭터를 출력한다. (28개인 이유는 알파벳 문자 26개와 어포스트로피, 블랭크 심 볼을 포함하기 때문) 이때 pre-trained ASR 모델은 스피치로부터 텍스트를 예측하도록 설계되므로, 출력은 소스 오디오의 화자와 무 관한 컨텐츠 정보만을 포함한다. 합성곱 레이어와 양방향 RNN 레이어의 가중치를 freezing하면, fully connedcted 계층만이 fine tuned되어 오 디오로부터 blendshape를 예측할 수 있도록 speaker-independent speech feature를 추출할 수 있다. DeepSpeech2의 마지막 fully connected 레이어는 64-dimensional 표현형을 출력하는 새로운 fully connected 레이어로 대체될 수 있다. 마지막 fully connected 레이어의 차원은 네트워크의 하이퍼-파라미터일 수 있다. Audio Encoder 의 목적은 ASR 모델에서처럼 알파벳 자체를 예측하는 것은 아니고, blendshape에 매핑될 수 있는 phoneme 정보를 보유하는 스피치 표현형을 창조하는 것이다. 일반적으로 phoneme의 구체적인 수는 언어 또는 방언에 의존하지만, 영어는 대체로 46 phoneme을 가지고, 한국 어는 40개를 가진다고 알려진다. 64-dimensional 표현형인 경우 heuristic하게 언어에 독립적으로 phonemic feature를 잡아낼 수 있는 수단을 제공할 수 있다. DeepSpeech2 모델에 이용되는 스펙트로그램은 초당 50 프레임인데 blendshape 를 위한 데이터셋은 60 fps로 구 성되는 경우가 있다. 이때 시간축 상에서 피처를 정렬하기 위하여, RNN 레이어의 출력은 60 fps로 리샘플될 수 있다. 이때 리샘플을 위하여 선형 interpolation이 이용될 수 있다. 도 3은 본 발명의 일 실시예에 따른 블렌드셰이프 디코더의 기능을 도시하는 개념도이다. 블렌드셰이프 디코더는, 도 2에서 추출된 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타임-윈도우잉 셀프 어텐션 기법에 의하여 블렌드셰이프 계수를 예측할 수 있다. Blendshape 계수는 일반적으로 애니메이션에서 얼굴 요소의 움직임을 나타내기 위하여 설정되는 key 값을 의미 한다. 3D vertex mesh와 비교해서 blendshape는 비용효율적이라는 장점을 가진다. 또한 blendshape이 정의되어 있는 모델이 있다면, blendshape coefficient 값만으로 움직임/애니메이션을 쉽게 합성할 수 있는 장점이 있다. 이러한 blendshape 기법을 이용하는 경우에 1대만의 카메라를 이용하여 데이터셋을 구축할 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치에서, 예측된 블렌드셰이프 계수와 스피치 데 이터에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차 (MSE) 손실함수를 이용하여 블렌드셰이프 디 코더가 훈련될 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 블렌드셰이프 디코더의 훈련을 위하여 스 피치 데이터에 대한 입술모양 애니메이션 데이터(또는 그 애니메이션을 생성할 수 있는 blendshape 계수)를 스 피치 데이터에 대한 그라운드 트루쓰 데이터로 이용할 수 있다. 스피치와 얼굴 표현, 즉 blendshape basis는 일대일 매핑되지 않는다. 동일한 phoneme이 서로 다른 blendshape basis 값으로 표현될 수 있고, 서로 다른 phoneme이 동일한 lip shape로 나타날 수 있다. Phoneme과 lip shape 가 일대일 매핑되지 않으며, 이는 혀로 내는 파열음의 경우, phoeneme이 lip shape을 결정하지 않기 때문이다. 또한 화자의 스피킹 스타일에 따라서도 lip shape는 달라질 수 있다. 예를 들어 입을 크게 벌리거나 작게 벌리 는 등의 스타일에 따라서도 lip shape는 영향받을 수 있다. 이러한 many-to-many 매핑으로 인하여 모델이 개별 프레임 별로 blendshape를 예측하는 경우 입술 애니메이션 내의 입술이 trembling되는 경향이 나타난다. 이러한 떨림 문제를 해결하기 위하여 time-windowing 기법이 이용될 수 있다. Time-windowing 기법은 W 프레임 을 관찰함으로써 현재 프레임 값을 추정하는 방식으로 구현될 수 있다. 스피치 특징의 W 개의 프레임이 셀프 어텐션 모듈을 통과할 수 있다. 셀프 어텐션 모듈은 W 개의 가중치를 결과 로 제공할 수 있도록, leaky ReLU 및 소프트맥스 계층과 5개의 1차원 CNN 계층을 포함할 수 있다. 이러한 time- windowing 셀프 어텐션 모듈의 최종 출력은 W개의 프레임의 스피치 특징과 가중치가 곱해져 얻어질 수 있다. 셀프 어텐션 모듈의 출력은 시간 상에서 압축된 정보를 포함하며, fully connected 계층의 입력으로 전달되어 blendshape 계수 값이 예측될 수 있다. 이때 시간 축 상에서 Aggregate 과정을 거쳐 blendshape 계수 값 에 대한 예측이 생성될 이때 디코더는 부자연스럽게 입술이 떨리는 현상(Lip shaking)을 방지하고 디코더는 시간 축 상에서 부드러운 움직임(좀 더 안정적인 움직임, temporal smoothness)을 생성하도록 time-windowing self-attention 방식을 이용하며, 인접한 8개의 frame을 이용할 수 있다. 도 4는 본 발명의 일 실시예에 따른 판별기(Discriminator)의 기능을 도시하는 개념도이다. 판별기는, 예측된 블렌드셰이프 계수 및 스피치 특징에 대한 조건부 연결 데이터를 생성하는 연결기 (concatenator)를 포함할 수 있다. 판별기는, 조건부 연결 데이터를 입력받는 복수개의 제1 1차원 합성곱 레이어; 및 복수개의 제1 1차 원 합성곱 레이어의 출력을 입력받는 제2 1차원 합성곱 레이어를 더 포함할 수 있다. 4개의 제1 1D 합성곱 레이어(커널 크기 4, 스트라이드 2 및 패딩 1)는 채널을 1, 64, 128, 256 및 512로 증가시킬 수 있다. 제2 1D 합성곱 레이어는 512개 채널을 1로 만드는 패처(patcher)일 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 장치는, 판별기의 출력을 이용하여 예측된 블렌드 셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하기 위한 조건부 생성적 적대 신경망 손실함수(46 0)를 더 포함할 수 있다. GAN 구조를 형성하는 discriminator는 모델의 예측값을 speech feature와 함께 관찰하면서 patch GAN 형 태로 일정 시간대 영역마다 진짜인지 가짜인지를 판별(진짜 음성 데이터인지, 생성자에 의하여 생성된 가짜 데 이터인지)할 수 있다. 본 발명의 실시예에서는 GAN 구조를 이용함으로써 웅얼거리는 것처럼 보이는 mumbling 현상을 줄이고 더 정확한 (실제에 가까운) 입 모양을 생성 모델이 생성하도록 훈련할 수 있다. 이때 판별기에 입력되는 그라운드 트루쓰 데이터 는 판별기의 학습을 위한 blendshape 데이터 (계수)일 수 있다. 설명의 편의상 판별기에 그라운드 트루쓰 데이터가 입력되는 경우에 판별기 의 출력을 Real patch로 명명하고, Decoder 에 의하여 생성된/예측된 blendshape 계수가 판별기 에 입력되는 경우에 판별기의 출력을 Fake patch로 명명할 수 있다. 판별기의 출력이 Real patch인지 Fake patch인지를 cGAN 손실함수에 의하여 판별할 수 있도록 판별기가 훈련되며, cGAN 손 실함수의 출력이 최소가 되도록 (즉, Fake patch가 Real patch와 구별되기 어렵도록) Decoder 와 판 별기가 GAN을 구성하여 훈련될 수 있다. GAN 네트워크에 의하여 Decoder 가 Real patch와 구별되기 어려운 Fake patch를 생성하는 기능이 학습될 수 있다. 앞서 설명한 phoenem과 lip shape 간의 many-to-many 매핑으로 인하여 예측된 blendshape 만으로 생성된 입술 애니메이션에서는 입술이 떨리는 현상이 나타난다. 이러한 첫번째 입술 trembling 원인 외에도 blendshape 기술이 특징 중 하나로, 비슷하게 입을 벌리는 형태가 서로 다른 blendshape key 값으로 생성될 수 있다는 이유로 인하여 입술이 떨리는 두번째 원인이 존재할 수 있 다. 입술의 모양은 크게 벌림(늘어남)과 닫힘(오므림)의 두 가지 형태가 있다. 특정 시점의 입술 모양을 나타내는 blendshape 계수 값의 분포는 정규분포가 아니라 두 개의 정규분포가 합쳐진 혼합분포이다. 이러한 특성 때문에 손실 함수가 단순히 실측값과 예측값의 차이에 기반한다면 모델은 평균값을 예측하는 경향이 있다. 또한 선행문헌 Isola에 따르면 L1 손실만 사용하여 이미지를 생성하면 흐릿한 결과가 생 성되는 경향이 나타난다. 모델은 고주파가 아닌 저주파의 정보만 캡처하므로, 입술 애니메이션의 경우 입술 움 직임이 중얼거리는 것처럼 표현될 수 있다. 본 발명의 실시예에서는 보다 정확한 입 모양을 만들기 위해 조건부 GAN (conditional GAN) 및 패치 GAN (patch-GAN)의 두 가지 기능이 있는 판별기를 이용할 수 있다. 판별기는 디코더의 출력을 입력받을 뿐만 아니라 스피치 특징도 조건으로 받을 수 있다. 이는 생성기 가 스피치 발화를 입술 애니메이션에 정확하게 매핑하는 데 도움이 될 수 있다. PatchGAN discriminator는 이미지를 N개의 패치로 나누고 각 패치가 진짜인지 가짜인지 분류하여 높은 주파수를 포착하는 능력을 향상시킬 수 있다. 선행문헌 Isola에 따르면 패치 외부의 픽셀이 독립적이라고 가정하고 이미지가 Markov 랜덤 필드로 모델링될 수 있다. 이미지와 달리 음성은 시간축에서만 Markov random field로 모델링될 수 있다. 따라서 패치를 N x N 형태 의 2차원 패치가 아닌 시간축 상의 1차원 패치로 구분하였다. Discriminator 의 후단(420, 430)는 스피치 특징과 blendshape 예측이 연결기에 의하여 연결된 텐서를 입력받아 T/16의 패치를 생성할 수 있다. 각 패치는 4개의 제1 1차원 합성곱 레이어(커널 크기 4 및 스트라이드 2)에 이어 인스턴스 정규화 (instance normalization) 및 Leaky ReLU가 적용되기 때문에 0.46s의 수용 필드(receptive field)를 가질 수 있다. 본 발명의 실시예에 따른 음성과 동조된 입모양 애니메이션을 합성하는 장치의 훈련 과정에서 이용되는 최종 손 실함수 L은 다음의 수학식 1과 같이 나타내어질 수 있다. [수학식 1]"}
{"patent_id": "10-2023-0113405", "section": "발명의_설명", "subsection": "발명을실시하기위한구체적인내용", "paragraph": 2, "content": "수학식 1에 나타내어진 것과 같이, 최종 손실함수 L은 조건부 GAN 손실함수 LcGAN (G, D) 및 평균제곱오차 손실 함수 LMSE (G) 의 가중치 합으로 주어질 수 있다. 이때 λ는 cGAN 손실함수 및 MSE 손실함수 간의 가중치 파라미터 (weighting parameter)이다. 조건부 GAN 손실함수 (cGAN Loss)는 상호 협력적이지 않은 상대인 Generator G, Discriminator D 간에 최적의 상태인 Nash Equilibrium에 도달하려고 노력하는 학습 과정을 나타내는 손실함수이다. 이 손실함수는 Generator G의 출력에 대한 Discriminator D의 판별 결과가 real일 확률에 대한 기대값과 fake일 확률에 대한 기대값을 이 용하며, 손실함수를 최대로 하는 Discriminator D 및 최소로 하는 Generator G를 탐색하는 손실함수이다. 구체적으로는 고정된 G의 출력에 대하여 손실함수를 최대화하는 최적의 Discriminator D를 얻고 다시 이를 전체 적으로 최소화하는 Generator G를 탐색하는 손실함수이다. 평균제곱오차 손실함수 (MSE (Mean-Square Error) Loss)는 각 프레임에서 정답 (GT, Ground Truth)과 Predicted Value (B, T) 간의 거리를 의미한다. 본 발명의 일 실시예에서는 Discriminator 없이 encoder-decoder 모델 또는 Decoder 의 MSE 손실함수를 구하여 encoder-decoder 모델 또는 Decoder 를 학습할 수도 있다. 그러나 이 경우 본 발명이 추구하는 lip movement의 역동성이 부족할 수 있다. 본 발명의 일 실시예에서는 conditional GAN을 함께 이용함으로써 더욱 정확하고 역동적인 lip animation 생성 모델을 학습할 수 있다. 또한 Discriminator 는 conditional GAN 및 patch-GAN 기법을 결합하여 이용할 수 있다. 본 발명의 일 실시예에서는 Discriminator 의 빠른 수렴으로 인하여 Decoder 가 불안정해지 는 것을 막기 위해, Decoder 를 먼저 학습하고 그 후에 Discriminator 의 훈련을 시작할 수 있다. 본 발명의 일 실시예에 따르면, 언어에 구애받지 않고 음성과 동조된 입모양 애니메이션 합성하는 기술 개발을 통해 해당 분야에서의 인공지능 기술의 글로벌 경쟁력 확보, 고품질의 얼굴 애니메이션 모션을 구축함으로써 AI 음성/영상 기술 개발을 활성화할 수 있다. 본 발명의 일 실시예에 따르면, 애니메이션 혹은 게임 등에서 캐릭터들이 더 정확한 입모양으로 말하게 된다면 청각 장애인들의 콘텐츠 이해도가 높아지고 접근성이 좋아져 다양한 문화생활을 영위할 수 있다. 도 5는 도 1 내지 도 4의 과정의 적어도 일부를 수행할 수 있는 일반화된 입모양 애니메이션 합성 장치 또는 컴 퓨팅 시스템의 예시를 도시하는 개념도이다. 예를 들어 도 1의 오디오 인코더, 블렌드셰이프 디코더, 판별기 각각은 도 5의 컴퓨팅 시스템과 같이 구현될 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 애니메이션 합성 방법의 적어도 일부의 과정은 도 5의 컴퓨 팅 시스템에 의하여 실행될 수 있다. 도 5를 참조하면, 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 프로세서, 메모리, 통신 인 터페이스, 저장 장치, 입력 인터페이스, 출력 인터페이스 및 버스(bus)를 포함 하여 구성될 수 있다. 본 발명의 일 실시예에 따른 컴퓨팅 시스템은, 적어도 하나의 프로세서(processor) 및 상기 적어도 하나의 프로세서가 적어도 하나의 단계를 수행하도록 지시하는 명령어들(instructions)을 저장하는 메모 리(memory)를 포함할 수 있다. 본 발명의 일 실시예에 따른 방법의 적어도 일부의 단계는 상기 적어도 하 나의 프로세서가 상기 메모리로부터 명령어들을 로드하여 실행함으로써 수행될 수 있다. 프로세서는 중앙 처리 장치(central processing unit, CPU), 그래픽 처리 장치(graphics processing unit, GPU), 또는 본 발명의 실시예들에 따른 방법들이 수행되는 전용의 프로세서를 의미할 수 있다. 메모리 및 저장 장치 각각은 휘발성 저장 매체 및 비휘발성 저장 매체 중에서 적어도 하나로 구성 될 수 있다. 예를 들어, 메모리는 읽기 전용 메모리(read only memory, ROM) 및 랜덤 액세스 메모리 (random access memory, RAM) 중에서 적어도 하나로 구성될 수 있다. 또한, 컴퓨팅 시스템은, 무선 네트워크를 통해 통신을 수행하는 통신 인터페이스를 포함할 수 있다. 또한, 컴퓨팅 시스템은, 저장 장치, 입력 인터페이스, 출력 인터페이스 등을 더 포함 할 수 있다. 또한, 컴퓨팅 시스템에 포함된 각각의 구성 요소들은 버스(bus)에 의해 연결되어 서로 통신을 수행 할 수 있다. 본 발명의 컴퓨팅 시스템의 예를 들면, 통신 가능한 데스크탑 컴퓨터(desktop computer), 랩탑 컴퓨터 (laptop computer), 노트북(notebook), 스마트폰(smart phone), 태블릿 PC(tablet PC), 모바일폰(mobile phone), 스마트 워치(smart watch), 스마트 글래스(smart glass), e-book 리더기, PMP(portable multimedia player), 휴대용 게임기, 네비게이션(navigation) 장치, 디지털 카메라(digital camera), DMB(digital multimedia broadcasting) 재생기, 디지털 음성 녹음기(digital audio recorder), 디지털 음성 재생기(digital audio player), 디지털 동영상 녹화기(digital video recorder), 디지털 동영상 재생기(digital video player), PDA(Personal Digital Assistant) 등일 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 방법은, 메모리(memory)에 저장된 적어도 하나 이상의 명령을 실행하는 프로세서(processor)에 의하여 수행되는 방법으로서, 스피치 데이터를 입력받아 오디오 인코딩하여 스피치 특징을 추출하는 단계; 스피치 특징을 입력받아 프레임 별로 스피치 데이터에 대응하 는 블렌드셰이프 계수를 예측하는 단계; 및 예측된 블렌드셰이프 계수를 이용하여 예측된 블렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계를 포함한다. 블렌드셰이프 계수를 예측하는 단계는, 스피치 특징에 대한 시간축 상의 n개의 슬라이딩 윈도우를 이용하는 타 임-윈도우잉 셀프 어텐션 기법에 의하여 블렌드셰이프 계수를 예측할 수 있다. 본 발명의 일 실시예에 따른 음성과 동조된 입모양 합성 방법은, 예측된 블렌드셰이프 계수와 스피치 데이터에 대한 그라운드 트루쓰 데이터 간의 평균제곱오차(MSE) 손실함수를 이용하여 블렌드셰이프 계수를 예측하는 단계 를 훈련하는 단계를 더 포함할 수 있다. 블렌드셰이프 계수를 예측하는 단계를 훈련하는 단계는, 스피치 데이터에 대한 입술모양 애니메이션 데이터를 스피치 데이터에 대한 그라운드 트루쓰 데이터로 이용할 수 있다. 판별하는 단계는, 예측된 블렌드셰이프 계수 및 스피치 특징에 대한 조건부 연결 데이터를 이용하여 예측된 블 렌드셰이프 계수가 진짜(real)인지 가짜(fake)인지를 판별하는 단계를 포함할 수 있다. 판별하는 단계는, 조건부 연결 데이터를 복수개의 제1 1차원 합성곱 레이어에 전달하는 단계; 및 복수개의 제1 1차원 합성곱 레이어의 출력을 제2 1차원 합성곱 레이어에 전달하는 단계를 더 포함할 수 있다. 판별하는 단계는, 제2 1차원 합성곱 레이어의 출력을 조건부 생성적 적대 신경망 손실함수를 이용하여 판별하는 단계를 더 포함할 수 있다. 스피치 특징을 추출하기 위하여 입력받는 스피치 데이터는 스펙트로그램 데이터일 수 있다. 본 발명의 실시예에 따른 방법의 동작은 컴퓨터로 읽을 수 있는 기록매체에 컴퓨터가 읽을 수 있는 프로그램 또 는 코드로서 구현하는 것이 가능하다. 컴퓨터가 읽을 수 있는 기록매체는 컴퓨터 시스템에 의해 읽힐 수 있는 정보가 저장되는 모든 종류의 기록장치를 포함한다. 또한 컴퓨터가 읽을 수 있는 기록매체는 네트워크로 연결된 컴퓨터 시스템에 분산되어 분산 방식으로 컴퓨터로 읽을 수 있는 프로그램 또는 코드가 저장되고 실행될 수 있 다. 또한, 컴퓨터가 읽을 수 있는 기록매체는 롬(rom), 램(ram), 플래시 메모리(flash memory) 등과 같이 프로그램 명령을 저장하고 수행하도록 특별히 구성된 하드웨어 장치를 포함할 수 있다. 프로그램 명령은 컴파일러 (compiler)에 의해 만들어지는 것과 같은 기계어 코드뿐만 아니라 인터프리터(interpreter) 등을 사용해서 컴퓨 터에 의해 실행될 수 있는 고급 언어 코드를 포함할 수 있다.본 발명의 일부 측면들은 장치의 문맥에서 설명되었으나, 그것은 상응하는 방법에 따른 설명 또한 나타낼 수 있 고, 여기서 블록 또는 장치는 방법 단계 또는 방법 단계의 특징에 상응한다. 유사하게, 방법의 문맥에서 설명된 측면들은 또한 상응하는 블록 또는 아이템 또는 상응하는 장치의 특징으로 나타낼 수 있다. 방법 단계들의 몇몇 또는 전부는 예를 들어, 마이크로프로세서, 프로그램 가능한 컴퓨터 또는 전자 회로와 같은 하드웨어 장치에 의 해(또는 이용하여) 수행될 수 있다. 몇몇의 실시 예에서, 가장 중요한 방법 단계들의 적어도 하나 이상은 이와 같은 장치에 의해 수행될 수 있다. 실시예들에서, 프로그램 가능한 로직 장치(예를 들어, 필드 프로그래머블 게이트 어레이)가 여기서 설명된 방법 들의 기능의 일부 또는 전부를 수행하기 위해 사용될 수 있다. 실시예들에서, 필드 프로그래머블 게이트 어레이 (field-programmable gate array)는 여기서 설명된 방법들 중 하나를 수행하기 위한 마이크로프로세서 (microprocessor)와 함께 작동할 수 있다. 일반적으로, 방법들은 어떤 하드웨어 장치에 의해 수행되는 것이 바 람직하다. 이상 본 발명의 바람직한 실시 예를 참조하여 설명하였지만, 해당 기술 분야의 숙련된 당업자는 하기의 특허 청 구의 범위에 기재된 본 발명의 사상 및 영역으로부터 벗어나지 않는 범위 내에서 본 발명을 다양하게 수정 및 변경시킬 수 있음을 이해할 수 있을 것이다."}
{"patent_id": "10-2023-0113405", "section": "도면", "subsection": "도면설명", "item": 1, "content": "도 1은 본 발명의 일 실시예에 따른 음성과 동조된 입모양 애니메이션 합성 장치의 기능 블록을 도시하는 개념 도이다. 도 2는 본 발명의 일 실시예에 따른 오디오 인코더의 기능을 도시하는 개념도이다. 도 3은 본 발명의 일 실시예에 따른 블렌드셰이프 디코더의 기능을 도시하는 개념도이다. 도 4는 본 발명의 일 실시예에 따른 판별기(Discriminator)의 기능을 도시하는 개념도이다. 도 5는 도 1 내지 도 4의 과정의 적어도 일부를 수행할 수 있는 일반화된 입모양 애니메이션 장치 또는 컴퓨팅 시스템의 예시를 도시하는 개념도이다."}
