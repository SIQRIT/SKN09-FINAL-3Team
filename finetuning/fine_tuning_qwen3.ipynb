{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f9bad5-f58b-4dcf-bcae-e7c8a68180c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install --upgrade bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe2db1-f238-4f40-8c7f-013c5b836804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = \"\"\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1c28d6-b235-4344-b236-c8dc62cf7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfde240e83d5441787510267665f5afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9670c7da61b4d4a91c852dbd3a433bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17501dab1a5a4f29984809636ac46a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89561367c57465fadb38e7a21b9ad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8924b2db334d08a3dc3130ef789166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023e6c2007644d6d8b3c37c954e02c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283931f70c7c4907b75e3f2f754bc696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6966e41424940e2b3faa485b32dcc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3415720bd5f140f2869f70fabd4beb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266d8295d14148f5aebd4cc9410e8da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1717b1d270a94ea0884248c472eb67cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a6d70b35fb426b843f0e499bfb54bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa205b26e514dfaa5e8cbe8c7a9868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fab867e3a74771a61cda269f3f9027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_dir = \"Qwen/Qwen3-8B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,   \n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a924a4-0a53-42c1-9fd7-815eb36bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is a fixed instruction that guides the assistant to write a Korean patent specification in full compliance with Korean Patent Law. \n",
    "The assistant must use the provided invention details to generate a structured patent section in Korean.\n",
    "Think step-by-step before responding\n",
    "The final response should be written in Korean and follow the 2007 model specification guidelines.\n",
    "**Respond in Korean.**\n",
    "\n",
    "### Instruction:\n",
    "당신은 대한민국 특허법에 따라 명세서를 작성하는 특허 어시스턴트입니다. 아래의 구성요소별로 정확하고 구조화된 문서를 생성해야 합니다. \n",
    "모든 항목은 특허법 제42조 및 시행규칙 제21조, 개정된 모범 명세서 작성법(2007.07.01. 이후 적용)을 철저히 준수해야 합니다.\n",
    "\n",
    "1. [발명의 명칭]\n",
    "- 발명의 내용을 간명하게 표현하는 명칭을 작성합니다.\n",
    "- 영문명을 {{}} 안에 함께 기재합니다.\n",
    "\n",
    "2. [기술분야]\n",
    "- 본 발명이 속하는 기술분야를 간결하게 설명합니다.\n",
    "\n",
    "3. [배경기술]\n",
    "- 종래 기술을 서술하고, 가능하면 문헌 인용을 포함합니다.\n",
    "\n",
    "4. [해결하려는 과제]\n",
    "- 기존 기술의 문제점과 해결하고자 하는 과제를 기술합니다.\n",
    "\n",
    "5. [과제의 해결 수단]\n",
    "- 본 발명이 과제를 어떻게 해결하는지를 구체적으로 기술하고, 청구항과 연결합니다.\n",
    "\n",
    "6. [발명의 효과]\n",
    "- 본 발명이 제공하는 구체적이고 비교 가능한 기술적 효과를 기술합니다.\n",
    "\n",
    "7. [발명을 실시하기 위한 구체적인 내용]\n",
    "- 실시예, 수단, 수치 등을 포함하여, 평균 기술자가 재현 가능하도록 작성합니다.\n",
    "\n",
    "8. [도면의 간단한 설명]\n",
    "- 도면이 있을 경우, 각 도면이 무엇을 나타내는지 기술합니다.\n",
    "\n",
    "9. [특허청구범위]\n",
    "- 독립항 및 종속항을 번호와 함께 명확히 구분하여 작성합니다.\n",
    "\n",
    "※ 모든 기재는 특허법 제42조 제2~5항, 시행규칙 제21조 및 모범명세서 가이드를 기반으로 작성해야 합니다.\n",
    "\n",
    "### Content:\n",
    "없음\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "{output}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e99543-566a-4aaa-8550-5247a9d6b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3528064ab3714a9a832544d68a1eab0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "# ✅ 데이터 로딩\n",
    "def load_jsonl_dataset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# ✅ 포맷팅 함수\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        if not out.endswith(EOS_TOKEN):\n",
    "            out += EOS_TOKEN\n",
    "        text = train_prompt_style.format(input=inp, output=out)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_jsonl_dataset(\"finetuning_data.jsonl\")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "# dataset[\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07427da0-5f73-4010-88e6-477d02704dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728e6602-c71a-423c-9ccd-9c278d52f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15453947-e297-4321-af2b-57f1c5943b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a0d9769e84469aae7280666cc3ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (169994 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def filter_too_long(example):\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=False, return_tensors=\"pt\")\n",
    "    return tokenized[\"input_ids\"].shape[1] <= 131072\n",
    "\n",
    "dataset = dataset.filter(filter_too_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bc151d-5b97-480e-9f4c-9ee8f9b9bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset 분할\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 2. Metrics 함수 (optional)\n",
    "import math\n",
    "def compute_metrics(eval_preds):\n",
    "    loss = eval_preds.loss\n",
    "    return {\n",
    "        \"eval_loss\": loss,\n",
    "        \"perplexity\": math.exp(loss) if loss < 100 else float(\"inf\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d83f1cc-7c6a-4532-9fab-20d45323937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294a6463130448adaa11dca3fcd32688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f468d5c6e495d8159da7fa0737b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd8d7c4d63440788520dc77f926d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa6276692674981a26a1dced359cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172c6c559df04f05b830b5227df8f018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a68364405f42bab9ebb7b5eb17f1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ef8e177667464d839969076742fb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa79fc1a9f43398c13e0e39797c91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    eval_strategy=\"steps\",   # ✅ 평가 실행 조건\n",
    "    eval_steps=50,                 # ✅ 50 스텝마다 평가\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef451e13-300e-48ea-adf9-f514fe1bff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1347' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1347 27:41 < 07:50, 0.63 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.479861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.464089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.458186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.452831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.450795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.448292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.446281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.442894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.432100</td>\n",
       "      <td>0.440909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.440521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.439892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.440186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.439406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.438324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.437698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.435618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.434652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.434187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.438424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.441737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.441208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1050, training_loss=0.4354999088105701, metrics={'train_runtime': 1662.9584, 'train_samples_per_second': 1.62, 'train_steps_per_second': 0.81, 'total_flos': 9.99031981473792e+16, 'train_loss': 0.4354999088105701})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cea42-4dae-4dc3-b6ae-b9a4bd2551ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분 이상함\n",
    "# 학습 완료 후 가장 좋은 모델 저장\n",
    "trainer.save_model(\"best_model\")  # 디렉토리 이름은 원하는 대로\n",
    "\n",
    "# tokenizer도 함께 저장 (권장)\n",
    "tokenizer.save_pretrained(\"best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4023486-37c5-4b13-a514-555aded1477a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
